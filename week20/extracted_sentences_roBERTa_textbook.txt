sentence
vary. or depending on performance operations, it may keep that value in cache or in a register which is local to that process – try dumping the -o2 optimized assembly of incrementing a variable. the problem comes in if two processors try to do it at the same time. the two processors could at the same time copy the value of the memory address, add one, and store the same result back, resulting in the value only being incremented once. that is why we have a special set of instructions on modern systems called atomic operations. if an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. with x86 this is done by the lock prefix [8, p. 1120].
interrupts are a important part of system programming. an interrupt is internally an electrical signal that is delivered to the processor when something happens – this is a hardware interrupt [3]. then the hardware decides if this is something that it should handle (i.e. handling keyboard or mouse input for older keyboard and mouses) or it should pass to the operating system. the operating system then decides if this is something that it should handle (i.e. paging a memory table from disk) or something the application should handle (i.e. a segfault). if the operating system decides that this is something that the process or program should take care of, it sends a software fault and that software fault is then propagated. the application then decides if it is an error
hyperthreading is a new technology and is in no way shape or form multithreading. hyperthreading allows one physical core to appear as many virtual cores to the operating system [8, p.51]. the operating system can then schedule processes on these virtual cores and one core will execute them. each core interleaves processes or threads. while the core is waiting for one memory access to complete, it may perform a few instructions of another process thread. the overall result is more instructions executed in a shorter time. this potentially means that you can divide the number of cores you need to power smaller devices.
ssh is short for the secure shell [2]. it is a network protocol that allows you to spawn a shell on a remote machine.
what is ‘git‘? git is a version control system. what that means is git stores the entire history of a directory. we refer to the directory as a repository. so what do you need to know is a few things. first, create your repository with the repo creator. if you haven’t already signed into enterprise github, make sure to do so otherwise your repository won’t be created for you. after that, that means your repository is created on the server. git is a decentralized version control system, meaning that you’ll need to get a repository onto your vm. we can do this with a clone. whatever you do, do not go through the readme.md tutorial.
vim is a text editor and a unix-like utility. you enter vim by typing vim [file], which takes you into the editor. there are three most commonly used modes: normal mode, insert mode, and command mode. you start off in normal mode. in this mode, you can move around with many keys with the most common ones being hjkl (corresponding to left, down, up, and right respectively). to run commands in vim, you can first type : and then a command after it. for instance, to quit vim, simply type :q (q stands for quit). if you have any unsaved edits, you must either save them :w, save and quit :wq, or quit and discard changes :q!. to make edits you can either type i to change you into insert mode or a to change to insert mode after the cursor. this is the basics when it comes to vim. in addition to the countless great resources out there on the internet, vim also has its own built-in tutorials set up for beginners. to access the interactive tutorial, enter vimtutor in the command line (not inside of vim), and you are all set!
the debug macro will disable all assertions, so don’t forget to set that once you finish debugging [1].
valgrind is a suite of tools designed to provide debugging and profiling tools to make your programs more correct and detect some runtime issues [4]. the most used of these tools is memcheck, which can detect many memory-related errors that are common in c and c++ programs and that can lead to crashes and unpredictable behavior (for example, unfreed memory buffers). to run valgrind on your program:
==29515== memcheck, a memory error detector ==29515== copyright (c) 2002-2015, and gnu gpl’d, by julian seward et al.
invalid write: it detected our heap block overrun, writing outside of an allocated block.
valgrind is a effective tool to check for errors at runtime. c is special when it comes to such behavior, so after compiling your program you can use valgrind to fix errors that your compiler may miss and that usually happens when your program is running.
threadsanitizer is a tool from google, built into clang and gcc, to help you detect race conditions in your code [5]. note, that running with tsan will slow your code down a bit. consider the following code.
gdb is short for the gnu debugger. gdb is a program that helps you track down errors by interactively debugging them [6]. it can start and stop your program, look around, and put in ad hoc constraints and checks. here are a few examples.
setting breakpoints programmatically a breakpoint is a line of code where you want the execution to stop and give control back to the debugger. a useful trick when debugging complex c programs with gdb is setting breakpoints in the source code.
what do you actually use to run your program? a shell! a shell is a programming language that is running inside your terminal. a terminal is merely a window to input commands. now, on posix we usually have one shell called sh that is linked to a posix compliant shell called dash. most of the time, you use a shell called bash that is somewhat posix compliant but has some nifty built-in features. if you want to be even more advanced, zsh has some more powerful features like tab complete on programs and fuzzy patterns.
clang provides a great drop-in replacement tools for compiling programs. if you want to see if there is an error that may cause a race condition, casting error, etc, all you need to do is the following.
strace and ltrace are two programs that trace the system calls and library calls respectively of a running program or command. these may be missing on your system so to install feel free to run the following.
"> ltrace ./a.out __libc_start_main(0x8048454, 1, 0xbfc19db4, 0x80484c0, 0x8048530 <unfinished ...> fopen(""i don’t exist"", ""r"") = 0x0 fwrite(""invalid write\n"", 1, 14, 0x0 <unfinished ...> --- sigsegv (segmentation fault) --+++ killed by sigsegv +++"
1. the #include directive takes the file stdio.h (which stands for standard input and output) located somewhere in your operating system, copies the text, and substitutes it where the #include was.
1. gcc is short for the gnu compiler collection which has a host of compilers ready for use. the compiler infers from the extension that you are trying to compile a .c file.
"2. ./main tells your shell to execute the program in the current directory called main. the program then prints out ""hello world""."
what is the preprocessor? preprocessing is a copy and paste operation that the compiler performs before actually compiling the program. the following is an example of substitution
there are side effects to the preprocessor though. one problem is that the preprocessor needs to be able to tokenize properly, meaning trying to redefine the internals of the c language with a preprocessor may be impossible. another problem is that they can’t be nested infinitely - there is a bounded depth where they need to stop. macros are also simple text substitutions, without semantics. for example, look at what can happen if a macro tries to perform an inline modification.
macros are simple text substitution so the above example expands to
1. break is a keyword that is used in case statements or looping statements. when used in a case statement, the program jumps to the end of the block.
2. const is a language level construct that tells the compiler that this data should remain constant. if one tries to change a const variable, the program will fail to compile. const works a little differently when put before the type, the compiler re-orders the first type and const. then the compiler uses a left associativity rule. meaning that whatever is left of the pointer is constant. this is known as const-correctness.
3. continue is a control flow statement that exists only in loop constructions. continue will skip the rest of the loop body and set the program counter back to the start of the loop before.
4. do {} while(); is another loop construct. these loops execute the body and then check the condition at the bottom of the loop. if the condition is zero, the next statement is executed – the program counter is set to the first instruction after the loop. otherwise, the loop body is executed.
5. enum is to declare an enumeration. an enumeration is a type that can take on many, finite values. if you have an enum and don’t specify any numerics, the c compiler will generate a unique number for that enum (within the context of the current enum) and use that for comparisons. the syntax to declare an instance of an enum is enum <type> varname. the added benefit to this is that the compiler can type check these expressions to make sure that you are only comparing alike types.
6. extern is a special keyword that tells the compiler that the variable may be defined in another object file or a library, so the program compiles on missing variable because the program will reference a variable in the system or another file.
7. for is a keyword that allows you to iterate with an initialization condition, a loop invariant, and an update condition. this is meant to be equivalent to a while loop, but with differing syntax.
8. goto is a keyword that allows you to do conditional jumps. do not use goto in your programs. the reason being is that it makes your code infinitely more hard to understand when strung together with multiple chains, which is called spaghetti code. it is acceptable to use in some contexts though, for example, error checking code in the linux kernel. the keyword is usually used in kernel contexts when adding another stack frame for cleanup isn’t a good idea. the canonical example of kernel cleanup is as below.
9. if else else-if are control flow keywords. there are a few ways to use these (1) a bare if (2) an if with an else (3) an if with an else-if (4) an if with an else if and else. note that an else is matched with the most recent if. a subtle bug related to a mismatched if and else statement, is the dangling else problem. the statements are always executed from the if to the else. if any of the intermediate conditions are true, the if block performs that action and goes to the end of that block.
"10. inline is a compiler keyword that tells the compiler it’s okay to moit the c function call procedure and ""paste"" the code in the callee. instead, the compiler is hinted at substituting the function body directly into the calling function. this is not always recommended explicitly as the compiler is usually smart enough to know when to inline a function for you."
11. restrict is a keyword that tells the compiler that this particular memory region shouldn’t overlap with all other memory regions. the use case for this is to tell users of the program that it is undefined behavior if the memory regions overlap. note that memcpy has undefined behavior when memory regions overlap. if this might be the case in your program, consider using memmove.
12. return is a control flow operator that exits the current function. if the function is void then it simply exits the functions. otherwise, another parameter follows as the return value.
13. signed is a modifier which is rarely used, but it forces a type to be signed instead of unsigned. the reason that this is so rarely used is because types are signed by default and need to have the unsigned modifier to make them unsigned but it may be useful in cases where you want the compiler to default to a signed type such as below.
14. sizeof is an operator that is evaluated at compile-time, which evaluates to the number of bytes that the expression contains. when the compiler infers the type the following code changes as follows.
15. static is a type specifier with three meanings.
(a) when used with a global variable or function declaration it means that the scope of the variable or the function is only limited to the file.
(b) when used with a function variable, that declares that the variable has static allocation – meaning that the variable is allocated once at program startup not every time the program is run, and its lifetime is extended to that of the program.
16. struct is a keyword that allows you to pair multiple types together into a new structure. c-structs are contiguous regions of memory that one can access specific elements of each memory as if they were separate variables. note that there might be padding between elements, such that each variable is memory-aligned (starts at a memory address that is a multiple of its size).
17. switch case default switches are essentially glorified jump statements. meaning that you take either a byte or an integer and the control flow of the program jumps to that location. note that, the various cases of a switch statement fall through. it means that if execution starts in one case, the flow of control will continue to all subsequent cases, until a break statement.
18. typedef declares an alias for a type. often used with structs to reduce the visual clutter of having to write ‘struct’ as part of the type.
this declares a function type comparator that accepts two void* params and returns an integer.
19. union is a new type specifier. a union is one piece of memory that many variables occupy. it is used to maintain consistency while having the flexibility to switch between types without maintaining functions to keep track of the bits. consider an example where we have different pixel values.
20. unsigned is a type modifier that forces unsigned behavior in the variables they modify. unsigned can only be used with primitive int types (like int and long). there is a lot of behavior associated with unsigned arithmetic. for the most part, unless your code involves bit shifting, it isn’t essential to know the difference in behavior with regards to unsigned and signed arithmetic.
21. void is a double meaning keyword. when used in terms of function or parameter definition, it means that the function explicitly returns no value or accepts no parameter, respectively. the following declares a function that accepts no parameters and returns nothing.
the other use of void is when you are defining an lvalue. a void * pointer is just a memory address. it is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. pointer arithmetic with this pointer is undefined behavior.
23. while represents the traditional while loop. there is a condition at the top of the loop, which is checked before every execution of the loop body. if the condition evaluates to a non-zero value, the loop body will be run.
1. char represents exactly one byte of data. the number of bits in a byte might vary. unsigned char and signed char means the exact same thing. this must be aligned on a boundary (meaning you cannot use bits in between two addresses). the rest of the types will assume 8 bits in a byte.
6. float represents an ieee-754 single precision floating point number tightly specified by ieee [1]. this will be four bytes aligned to a four byte boundary on most machines.
7. double represents an ieee-754 double precision floating point number specified by the same standard, which is aligned to the nearest eight byte boundary.
 -> is the structure dereference (or arrow) operator. if you have a pointer to a struct *p, you can use this to access one of its elements. p->element.
 +/-a is the unary plus and minus operator. they either keep or negate the sign, respectively, of the integer or float type underneath.
 &a is the address-of operator. this takes an element and returns its address.
 – is the decrement operator. this has the same semantics as the increment operator except that it decreases the value of the variable by one.
 sizeof is the sizeof operator, that is evaluated at the time of compilation. this is also mentioned in the keywords section.
 <=/>= are the greater than equal to/less than equal to, relational operators. they work as their name implies.
 </> are the greater than/less than relational operators. they again do as the name implies.
 && is the logical and operator. if the first operand is zero, the second won’t be evaluated and the expression will evaluate to 0. otherwise, it yields a 1-0 value of the second operand.
 & is the bitwise and operator. if a bit is set in both operands, it is set in the output. otherwise, it is not.
 | is the bitwise or operator. if a bit is set in either operand, it is set in the output. otherwise, it is not.
 ?: is the ternary / conditional operator. you put a boolean condition before the and if it evaluates to non-zero the element before the colon is returned otherwise the element after is. 1 ? a : b === a and 0 ? a : b === b.
 a, b is the comma operator. a is evaluated and then b is evaluated and b is returned. in a sequence of multiple statements delimited by commas, all statements are evaluated from left to right, and the right-most expression is returned.
a system call is an operation that the kernel carries out. first, the operating system prepares a system call.
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.
for performance, printf buffers data until its cache is full or a newline is printed. here is an example of printing things out.
the buffering semantics of printf is a little complicated. iso defines three types of streams [5, p. 278]  unbuffered, where the contents of the stream reach their destination as soon as possible.
 line buffered, where the contents of the stream reach their destination as soon as a newline is provided.
 fully buffered, where the contents of the stream reach their destination as soon as the buffer is full.
standard error is defined as “not fully buffered” [5, p. 279]. standard output and input are merely defined to be fully buffered if and only if the stream destination is not an interactive device. usually, standard error will be unbuffered, standard input and output will be line buffered if the output is a terminal otherwise fully buffered.
to print data into a c string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. we would use sprintf the size of the printed string is less than the provided buffer – think about printing an integer, it will never be more than 11 characters with the nul byte. if printf is dealing with variadic input, it is safer to use the former function as shown in the following snippet.
standard input or stdin oriented functions read from stdin directly. most of these functions have been deprecated due to them being poorly designed. these functions treat stdin as a file from which we can read bytes. one of the most notorious offenders is gets. gets is deprecated in c99 standard and has been removed from the latest c standard (c11). the reason that it was deprecated was that there is no way to control the length being read, therefore buffers could get overrun easily. when this is done maliciously to hijack program control flow, this is known as a buffer overflow.
in addition to those functions, we have perror that has a two-fold meaning. let’s say that a function call failed using the errno convention. perror(const char* message) will print the english version of the error to stderr.
to have a library function parse input in addition to reading it, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a c string, respectively. all of those functions will return how many items were parsed. it is a good idea to check if the number is equal to the amount expected. also naturally like printf, scanf functions require valid pointers. instead of pointing to valid memory, they need to also be writable. it’s a common source of error to pass in an incorrect pointer value. for example,
string.h functions are a series of functions that deal with how to manipulate and check pieces of memory. most of them deal with c-strings. a c-string is a series of bytes delimited by a nul character which is equal to the byte 0x00. more information about all of these functions. any behavior missing from the documentation, such as the result of strlen(null) is considered undefined behavior.
 int strlen(const char *s) returns the length of the string.
 int strcmp(const char *s1, const char *s2) returns an integer determining the lexicographic order of the strings. if s1 where to come before s2 in a dictionary, then a -1 is returned. if the two strings are equal, then 0. else, 1.
 char *strcpy(char *dest, const char *src) copies the string at src to dest. this function assumes dest has enough space for src otherwise undefined behavior  char *strcat(char *dest, const char *src) concatenates the string at src to the end of destination.
 char *strchr(const char *haystack, int needle) returns a pointer to the first occurrence of needle in the haystack. if none found, null is returned.
 char *strtok(const char *str, const char *delims) a dangerous but useful function strtok takes a string and tokenizes it. meaning that it will transform the strings into separate strings. this function has a lot of specs so please read the man pages a contrived example is below.
" void *memcpy(void *dest, const void *src, size_t n) moves n bytes starting at src to dest. be careful, there is undefined behavior when the memory regions overlap. this is one of the classic ""this works on my machine!"" examples because many times valgrind won’t be able to pick it up because it will look like it works on your machine. consider the safer version memmove."
in low-level terms, a struct is a piece of contiguous memory, nothing more. just like an array, a struct has enough space to keep all of its members. but unlike an array, it can store different types. consider the contact struct declared above.
the rest of the space is left untouched. we will assume that our machine is big endian. this means that the least significant byte is last.
string literals are character arrays stored in the code segment of the program, which is immutable. two string literals may share the same space in memory. an example follows.
char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. these following char arrays reside in different memory locations.
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.
a double free error is when a program accidentally attempt to free the same allocation twice.
automatic variables hold garbage or bit pattern that happened to be in memory or register. it is an error to assume that it will always be initialized to zero.
automatic (temporary variables) and heap allocations may contain random bytes or garbage.
these are a set of mistakes that may let the program compile but perform unintended functionality.
to understand what a process is, you need to understand what an operating system is. an operating system is a program that provides an interface between hardware and user software as well as providing a set of tools that the software can use. the operating system manages hardware and gives user programs a uniform way of interacting with hardware as long as the operating system can be installed on that hardware. although this idea sounds like it is the end-all, we know that there are many different operating systems with their own quirks and standards. as a solution to that, there is another layer of abstraction: posix or portable operating systems interface. this is a standard (or many standards now) that an operating system must implement to be posix compatible – most systems that we’ll be studying are almost posix compatible due more to political reasons.
before we talk about posix systems, we should understand what the idea of a kernel is generally. in an operating system (os), there are two spaces: kernel space and user space. kernel space is a power operating mode that allows the system to interact with the hardware and has the potential to destroy your machine. user space is where most applications run because they don’t need this level of power for every operation. when a user space program needs additional power, it interacts with the hardware through a system call that is conducted by the kernel. this adds a layer of security so that normal user programs can’t destroy your entire operating system.
1. the computer hardware executes code from read-only memory, called firmware.
2. the firmware executes a bootloader, which often conforms to the extensible firmware interface (efi), which is an interface between the system firmware and the operating system.
3. the bootloader’s boot manager loads the operating system kernels, based on the boot settings.
the kernel creates the first process init.d (an alternative is system.d). init.d boots up programs such as graphical user interfaces, terminals, etc – by default, this is the only process explicitly created by the system. all other processes are instantiated by using the system calls fork and exec from that single process.
notice that file descriptors may be reused between processes, but inside of a process, they are unique. file descriptors may have a notion of position. these are known as seekable streams. a program can read a file on disk completely because the os keeps track of the position in the file, an attribute that belongs to your process as well.
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.
 a stack the stack is the place where automatically allocated variables and function call return addresses are stored.
every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. this segment of the stack is writable but not executable. this behavior is controlled by the no-execute (nx) bit, sometimes called the wx̂ (write xor execute) bit, which helps prevent malicious code, such as shellcode from being run on the stack.
 a heap the heap is a contiguous, expanding region of memory [5]. if a program wants to allocate an object whose lifetime is manually controlled or whose size cannot be determined at compile-time, it would want to create a heap variable.
 a data segment this segment contains two parts, an initialized data segment, and an uninitialized segment. furthermore, the initialized data segment is divided into a readable and writable section.
– initialized data segment this contains all of a program’s globals and any other static variables.
processes could also contain the following information:  running state - whether a process is getting ready, running, stopped, terminated, etc. (more on this is covered in the chapter on scheduling).
 file descriptors - a list of mappings from integers to real devices (files, usb flash drives, sockets)  permissions - what user the file is running on and what group the process belongs to. the process can then only perform operations based on the permissions given to the user or group, such as accessing files.
there are tricks to make a program take a different user than who started the program i.e. sudo takes a program that a user starts and executes it as root. more specifically, a process has a real user id (identifies the owner of the process), an effective user id (used for non-privileged users trying to access files only accessible by superusers), and a saved user id (used when privileged users perform non-privileged actions).
 arguments - a list of strings that tell your program what parameters to run under.
 environment variables - a list of key-value pair strings in the form name=value that one can modify. these are often used to specify paths to libraries and binaries, program configuration settings, etc.
the fork system call clones the current process to create a new process, called a child process. this occurs by duplicating the state of the existing process with a few minor differences.
"to write code that is different for the parent and child process, check the return value of fork(). if fork() returns -1, that implies something went wrong in the process of creating a new child. one should check the value stored in errno to determine what kind of error occurred. common errors include eagain and enoent which are essentially ""try again – resource temporarily unavailable"", and ""no such file or directory""."
according to the posix standard, every process only has a single parent process.
a ‘fork bomb’ is what we warned you about earlier. this occurs when there is an attempt to create an infinite number of processes. this will often bring a system to a near-standstill, as it attempts to allocate cpu time and memory to a large number of processes that are ready to run. system administrators don’t like them and may set upper limits on the number of processes each user can have, or revoke login rights because they create disturbances in the force for other users’ programs. a program can limit the number of child processes created by using setrlimit().
a signal can be thought of as a software interrupt. this means that a process that receives a signal stops the execution of the current program and makes the program respond to the signal.
to note, it is generally poor programming practice to use signals in program logic, which is to send a signal to perform a certain operation. the reason: signals have no time frame of delivery and no assurance that they will be delivered. there are better ways to communicate between two processes.
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.
if the parent process wants to wait for the child to finish, it must use waitpid (or wait), both of which wait for a child to change process states, which can be one of the following: 1. the child terminated 2. the child was stopped by a signal 3. the child was resumed by a signal note that waitpid can be set to be non-blocking, which means they will return immediately, letting a program know if the child has exited.
wait is a simpler version of waitpid. wait accepts a pointer to an integer and waits on any child process.
2. the last parameter to waitpid is an option parameter. the options are listed below: 3. wnohang - return whetherthe searched process has exited 4. wnowait - wait, but leave the child wait-able by another wait call 5. wexited - wait for exited children 6. wstopped - wait for stopped children 7. wcontinued - wait for continued children exit statuses or the value stored in the integer pointer for both of the calls above are explained below.
it is good practice to wait on your process’ children. if a parent doesn’t wait on your children they become, what are called zombies. zombies are created when a child terminates and then takes up a spot in the kernel process table for your process. the process table keeps track of the following information about a process: pid, status, and how it was killed. the only way to get rid of a zombie is to wait on your children. if a long-running parent never waits for your children, it may lose the ability to fork.
having said that, a program doesn’t always need to wait for your children! your parent process can continue to execute code without having to wait for the child process. if a parent dies without waiting on its children, a process can orphan its children. once a parent process completes, any of its children will be assigned to init - the first process, whose pid is 1. therefore, these children would see getppid() return a value of 1. these orphans will eventually finish and for a brief moment become a zombie. the init process automatically waits for all of its children, thus removing these zombies from the system.
to make the child process execute another program, use one of the exec functions after forking. the exec set of functions replaces the process image with that of the specified program. this means that any lines of code after the exec call are replaced with those of the executed program. any other work a program wants the child process to do should be done before the exec call. the naming schemes can be shortened mnemonically.
3. p – uses the path environment variable to find the file named in the file argument to be executed.
2. chdir – change the current directory to /usr/include 3. execl – replace the program image with /bin/ls and call its main() method 4. perror – we don’t expect to get here - if we did then exec failed.
environment variables are variables that the system keeps for all processes to use. your system has these set up right now! in bash, some are already defined
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.
 realloc(void *space, size_t bytes) allows a program to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc) [4, p. 349]. the most common use of realloc is to resize memory used to hold an array of values. there are two gotchas with realloc. one, a new pointer may be returned. two, it can fail. a naive but readable version of realloc is suggested below with sample usage.
 calloc(size_t nmemb, size_t size) initializes memory contents to zero and also takes two arguments: the number of items and the size in bytes of each item. an advanced discussion of these limitations is in this article. programmers often use calloc rather than explicitly calling memset after malloc, to set the memory contents to zero because certain performance considerations are taken into account. note calloc(x,y) is identical to calloc(y,x), but you should follow the conventions of the manual. a naive implementation of calloc is below.
 free takes a pointer to the start of a piece of memory and makes it available for use in subsequent calls to the other allocation functions. this is important because we don’t want every process in our address space to take an enormous amount of memory. once we are done using memory, we stop using it with ‘free‘. a simple usage is below.
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.
these choices represent different placement strategies. whichever hole is chosen, the allocator will need to split the hole into two. the newly allocated space, which will be returned to the program and a smaller hole if there is spare space left over. a perfect-fit strategy finds the smallest hole that is of sufficient size (at least 2kib):
a worst-fit strategy finds the largest hole that is of sufficient size so break the 30kib hole into two:
figure 5.3: worst fit finds the worst match a first-fit strategy finds the first available hole that is of sufficient size so break the 16kib hole into two. we don’t even have to look through the entire heap!
in contrast, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way so a continuous block of that size is unavailable. in our previous example, of the 64kib of heap memory, 17kib is allocated, and 47kib is free. however, the largest available block is only 30kib because our available unallocated heap memory is fragmented into smaller pieces.
4. there are many placement strategies that we haven’t talked about, one is next-fit which is first fit on the next fit block. this adds deterministic randomness – pardon the oxymoron. you won’t be expected to know this algorithm, know as you are implementing a memory allocator as part of a machine problem, there are more than these.
our heap memory is a list of blocks where each block is either allocated or unallocated. thus there is conceptually a list of free blocks, but it is implicit in the form of block size information that we store as part of each block. let’s think of it in terms of a simple implementation.
to be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block’s size at the end of the block, too. these are called “boundary tags” [5]. these are knuth’s solution to the coalescing problem both ways. as the blocks are contiguous, the end of one block sits right next to the start of the next block. so the current block (apart from the first one) can look a few bytes further back to look up the size of the previous block. with this information, the allocator can now jump backward!
inserting in address order (“address ordered policy”) inserts deallocated blocks so that the blocks are visited in increasing address order. this policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. however, there is less fragmentation.
a segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. sizes are grouped into powers of two and each size is handled by a different sub-allocator and each size maintains its free list.
a well-known allocator of this type is the buddy allocator [6, p. 85]. we’ll discuss the binary buddy allocator which splits allocation into blocks of size 2n ; n = 1, 2, 3, ... times some base unit number of bytes, but others also exist like fibonacci split where the allocation is rounded up to the next fibonacci number. the basic concept is
the slub allocator is a slab allocator that serves different needs for the linux kernel slub. imagine you are creating an allocator for the kernel, what are your requirements? here is a hypothetical shortlist.
enter the slub allocator kmalloc. the slub allocator is a segregated list allocator with minimal splitting and coalescing. the difference here is that the segregated list focuses on more realistic allocation sizes, instead of powers of two. slub also focuses on a low overall memory footprint while keeping pages in the cache. there are blocks of different sizes and the kernel rounds up each allocation request to the lowest block size that satisfies it.
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.
your main function and other functions has automatic variables. we will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). if the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables.
 the third is a pointer to a function that we want to run  fourth is a pointer that will be given to our function the argument void *(*start_routine) (void *) is difficult to read! it means a pointer that takes a void * pointer and returns a void * pointer. it looks like a function declaration except that the name of the function is wrapped with (* .... )
 pthread_create. creates a new thread. every thread gets a new stack. if a program calls pthread_create twice, your process will contain three stacks - one for each thread. the first thread is created when the process start, the other two after the create. actually, there can be more stacks than this, but let’s keep it simple. the important idea is that each thread requires a stack because the stack contains automatic variables and the old cpu pc register, so that it can go back to executing the calling function after the function is finished.
 pthread_cancel stops a thread. note the thread may still continue. for example, it can be terminated
 pthread_exit(void *) stops the calling thread meaning the thread never returns after calling pthread_exit.
the pthread library will automatically finish the process if no other threads are running. pthread_exit(...) is equivalent to returning from the thread’s function; both finish the thread and also set the return value (void *pointer) for the thread. calling pthread_exit in the main thread is a common way for simple programs to ensure that all threads finish. for example, in the following program, the myfunc threads will probably not have time to get started. on the other hand exit() exits the entire process and sets the process’ exit value. this is equivalent to return (); in the main method. all threads inside the process are stopped. note the pthread_exit version creates thread zombies; however, this is not a long-running process, so we don’t care.
 pthread_join() waits for a thread to finish and records its return value. finished threads will continue to consume resources. eventually, if enough threads are created, pthread_create will fail. in practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits. this is equivalent to turning your children into zombies, so keep this in mind for long-running processes. in the exit example, we could also wait on all the threads.
 calling pthread_exit  canceling the thread with pthread_cancel  terminating the process through a signal.
race conditions are whenever the outcome of a program is determined by its sequence of events determined by the processor. this means that the execution of the code is non-deterministic. meaning that the same program can run multiple times and depending on how the kernel schedules the threads could produce inaccurate results.
figure 6.4: thread access - race condition this access pattern will cause the variable data to be 2. this is undefined behavior and a race condition. what we want is one thread to access the part of the code at a time.
 rendering of computer graphics. in computer animation, each frame may be rendered independently (see parallel rendering).
 discrete fourier transform where each harmonic is independently calculated.
 pthread life-cycle  each thread has a stack  capturing return values from a thread  using pthread_join  using pthread_create  using pthread_exit  under what conditions will a process exit
synchronization coordinates various tasks so that they all finishin the the correct state. in c, we have series of mechanisms to control what threads are allowed to perform at a given state. most of the time, the threads can progress without having to communicate, but every so often two or more threads may want to access a critical section. a critical section is a section of code that can only be executed by one thread at a time if the program is to function correctly. if two threads (or processes) were to execute code inside the critical section at the same time, it is possible that the program may no longer have the correct behavior.
as we said in the previous chapter, race conditions happen when an operation touches a piece of memory at the same time as another thread. if the memory location is only accessible by one thread, for example the automatic variable i below, then there is no possibility of a race condition and no critical section associated with i. however, the sum variable is a global variable and accessed by two threads. it is possible that two threads may attempt to increment the variable at the same time.
to ensure that only one thread at a time can access a global variable, use a mutex – short for mutual exclusion.
let’s think about a duck satisfying the mutex api. if someone has the duck then they are allowed to access a shared resource! we call it the mutex duck. everyone else has to waddle around and wait. once someone let’s go of the duck, they have to stop interacting with the resource and the next grabber can interact with the shared resource. now you know the origins of the duck.
the unlock function simply unlocks the mutex and returns. the lock function first checks to see if the lock is already locked. if it is currently locked, it will keep checking again until another thread has unlocked the mutex.
we might attempt to reduce the cpu overhead a little by calling pthread_yield() inside the loop - pthread_yield suggests to the operating system that the thread does not use the cpu for a short while, so the cpu may be assigned to threads that are waiting to run. this still leaves the race-condition. we need a better implementation.
atomic instructions are prone to spurious failures meaning that there are two versions to these atomic functions a strong and a weak part, strong guarantees the success or failure while weak may fail even when the operation succeeds. these are the same spurious failures that you’ll see in condition variables below. we are using weak because weak is faster, and we are in a loop! that means we are okay if it fails a little bit more often because we will keep spinning around anyway.
how does this guarantee mutual exclusion? when working with atomics we are unsure! but in this simple example, we can because the thread that can successfully expect the lock to be unlocked (0) and swap it to a locked (1) state is considered the winner. how do we implement unlock?
a semaphore is another synchronization primitive. it is initialized to some value. threads can either sem_wait or sem_post which lowers or increases the value. if the value reaches zero and a wait is called, the thread will be blocked until a post is called.
this becomes especially useful if you want to use a semaphore to implement a mutex. a mutex is a semaphore that always waits before it posts. some textbooks will refer to a mutex as a binary semaphore. you do have to be careful to never add more than one to a semaphore or otherwise your mutex abstraction breaks. that is usually why a mutex is used to implement a semaphore and vice versa.
but be warned, it isn’t the same! a mutex can handle what we call lock inversion well. meaning the following code breaks with a traditional mutex, but produces a race condition with threads.
occasionally, a waiting thread may appear to wake up for no reason. this is called a spurious wakeup. if you read the hardware implementation of a mutex section, this is similar to the atomic failure of the same name.
naturally, we want our data structures to be thread-safe as well! we can use mutexes and synchronization primitives to make that happen. first a few definitions. atomicity is when an operation is thread-safe. we have atomic instructions in hardware by providing the lock prefix
but atomicity also applies to higher orders of operations. we say a data structure operation is atomic if it happens all at once and successfully or not at all.
 there is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack) the last point can be fixed using counting semaphores. the implementation assumes a single stack. a more general-purpose version might include the mutex as part of the memory structure and use pthread_mutex_init to initialize the mutex. for example,
as already discussed, there are critical parts of our code that can only be executed by one thread at a time. we describe this requirement as ‘mutual exclusion’. only one thread (or process) may have access to the shared resource. in multi-threaded programs, we can wrap a critical section with mutex lock and unlock calls:
1. mutual exclusion. the thread/process gets exclusive access. others must wait until it exits the critical section.
candidate #3 satisfies mutual exclusion. each thread or process gets exclusive access to the critical section.
remember that pthread_cond_wait performs three actions. firstly, it atomically unlocks the mutex and then sleeps (until it is woken by pthread_cond_signal or pthread_cond_broadcast). thirdly, the awoken thread must re-acquire the mutex lock before returning. thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.
does this mean that a writer and read could read and write at the same time? no! first of all, remember cond_wait requires the thread re-acquire the mutex lock before returning. thus only one thread can be executing code inside the critical section (marked with **) at a time!
candidate #3 above suffers from starvation. if readers are constantly arriving then a writer will never be able to proceed (the ‘reading’ count never reduces to zero). this is known as starvation and would be discovered under heavy loads. our fix is to implement a bounded-wait for the writer. if a writer arrives they will still need to wait
a ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. as array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. as data is added (enqueued) to the front of the queue or removed (dequeued) from the tail of the queue, the current items in the buffer form a train that appears to circle the track a simple (single-threaded) implementation is shown below. note, enqueue and dequeue do not guard against underflow or overflow. it’s possible to add an item when the queue is full and possible to remove an item when the queue is empty. if we added 20 integers (1, 2, 3, . . . , 20) to the queue and did not dequeue any items then values, 17,18,19,20 would overwrite the 1,2,3,4. we won’t fix this problem right now, instead of when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.
void *buffer[16]; unsigned int in = 0, out = 0; void enqueue(void *value) {/* add one item to the front of the queue*/ buffer[in] = value; in++; /* advance the index for next time */ if (in == 16) in = 0; /* wrap around! */ } void *dequeue() {/* remove one item to the end of the queue.*/ void *result = buffer[out]; out++; if (out == 16) out = 0; return result; }
 the enqueue method waits and posts on the same semaphore (s1) and similarly with enqueue and (s2) i.e. we decrement the value and then immediately increment the value, so by the end of the function the semaphore value is unchanged!
deadlock is defined as when a system cannot make any forward progress. we define a system for the rest of the chapter as a set of rules by which a set of processes can move from one state to another, where a state is either working or waiting for a particular resource. forward progress is defined as if there is at least one process working or we can award a process waiting for a resource that resource. in a lot of systems, deadlock is avoided by ignoring the entire concept [4, p.237]. have you heard about turn it on and off again? for products where the stakes are low (user operating systems, phones), it may be more efficient to allow deadlock. but in the cases where “failure is not an option” - apollo 13, you need a system that tracks, breaks, or prevents deadlocks. apollo 13 didn’t fail because of deadlock, but it wouldn’t be good to restart the system on liftoff.
figure 8.1: resource allocation graph one such way is modeling the system with a resource allocation graph (rag). a resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. it is a simple yet powerful tool to illustrate how interacting processes can deadlock. if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node. if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will deadlock. for example, if process 1 holds resource a, process 2 holds resource b and process 1 is waiting for b and process 2 is waiting for a, then processes 1 and 2 will be deadlocked 8.1. we’ll make the distinction that the system is in deadlock by definition if all workers cannot perform an operation other than waiting. we can detect a deadlock by traversing the graph and searching for a cycle using a graph traversal algorithm, such as the depth first search (dfs). this graph is considered as a directed graph and we can treat both the processes and resources as nodes.
 mutual exclusion: no two processes can obtain a resource at the same time.
 circular wait: there exists a cycle in the resource allocation graph, or there exists a set of processes {p1, p2,. . . } such that p1 is waiting for resources held by p2, which is waiting for p3,. . . , which is waiting for p1.
 hold and wait: once a resource is obtained, a process keeps the resource locked.
 for contradiction, assume that there is no circular wait. if not then that means the resource allocation graph is acyclic, meaning that there is at least one process that is not waiting on any resource to be freed. since the system can move forward, the system is not deadlocked.
 for contradiction, assume that there is no mutual exclusion. if not, that means that no process is waiting on any other process for a resource. this breaks circular wait and the previous argument proves correctness.
if all processes have been surveyed and if all are requesting a resource and none can be granted a resource, consider it deadlocked. more formally, this system is deadlocked means if ∃t 0 , ∀t ≥ t 0 , ∀p ∈ p, w t (p) 6= satisfied and ∃q, q 6= p → h t (w t (p)) = q (which is what we need to prove).
if a system breaks any of them, it cannot have deadlock! consider the scenario where two students need to write both pen and paper and there is only one of each. breaking mutual exclusion means that the students share the pen and paper. breaking circular wait could be that the students agree to grab the pen then the paper. as proof by contradiction, say that deadlock occurs under the rule and the conditions. without loss of generality, that means a student would have to be waiting on a pen while holding the paper and the other waiting on a pen and holding the paper. we have contradicted ourselves because one student grabbed the paper without grabbing the pen, so deadlock fails to occur. breaking hold and wait could be that the students try to get the pen and then the paper and if a student fails to grab the paper then they release the pen. this introduces a new problem called livelock which will be discussed later. breaking preemption means that if the two students are in deadlock the teacher can come in and break up the deadlock by giving one of the students a held item or tell both students to put the items down.
livelock is generally harder to detect because the processes generally look like they are working to the outside operating system whereas in deadlock the operating system generally knows when two processes are waiting on a system-wide resource. another problem is that there are necessary conditions for livelock (i.e. deadlock fails to occur) but not sufficient conditions – meaning there is no set of rules where livelock has to occur. you must formally prove in a system by what is known as an invariant. one has to enumerate each of the steps of a system and if each of the steps eventually – after some finite number of steps – leads to forward progress, the system fails to livelock. there are even better systems that prove bounded waits; a system can only be livelocked for at most n cycles which may be important for something like stock exchanges.
problem then gets introduced that we could reach a livelock scenario if we preempt a set of resources again and again. the way around this is mostly probabilistic. the operating system chooses a random resource to break hold-and-wait. now even though a user can craft a program where breaking hold and wait on each resource will result in a livelock, this doesn’t happen as often on machines that run programs in practice or the livelock that does happen happens for a couple of cycles. these systems are good for products that need to maintain a non-deadlocked state but can tolerate a small chance of livelock for a short time.
in addition, we have the banker’s algorithm. which the basic premise is the bank never runs dry, which prevents livelock. feel free to check out the appendix for more details.
the memory management unit is part of the cpu, and it converts a virtual memory address into a physical address.
a page is a block of virtual memory. a typical block size on linux is 4kib or 212 addresses, though one can find examples of larger blocks. so rather than talking about individual bytes, we can talk about blocks of 4kibs, each block is called a page. we can also number our pages (“page 0” “page 1” etc). let’s do a sample calculation of how many pages are there assume page size of 4kib.
we also call this a frame or sometimes called a ‘page frame’ is a block of physical memory or ram – random access memory. a frame is the same number of bytes as a virtual page or 4kib on our machine. it stores the bytes of interest. to access a particular byte in a frame, an mmu goes from the start of the frame and adds the offset – discussed later.
a page table is a map from a number to a particular frame. for example page 1 might be mapped to frame 45, page 2 mapped to frame 30. other frames might be currently unused or assigned to other running processes or used internally by the operating system. implied from the name, imagine a page table as a table.
to overcome this overhead, the mmu includes an associative cache of recently-used virtual-page-to-frame lookups. this cache is called the tlb (“translation lookaside buffer”). every time a virtual address needs to be translated into a physical memory location, the tlb is queried in parallel to the page table. for most memory
frames can be shared between processes, and this is where the heart of the chapter comes into play. we can use these tables to communicate with processes. in addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. read-only frames can then be safely shared between multiple processes. for example, the c-library instruction code can be shared between all processes that dynamically load the code into the process memory. each process can only read that memory. meaning that if a program tries to write to a read-only page in memory, it will segfault. that is why sometimes memory accesses segfault and sometimes they don’t, it all depends on if your hardware says that a program can access.
1. the read-only bit marks the page as read-only. attempts to write to the page will cause a page fault. the page fault will then be handled by the kernel. two examples of the read-only page include sharing the c standard library between multiple processes for security you wouldn’t want to allow one process to modify the library and copy-on-write where the cost of duplicating a page can be delayed until the first write occurs.
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.
mmap does more than take a file and map it to memory. it is the general interface for creating shared memory among processes. currently it only supports regular files and posix shmem [2]. naturally, you can read all about it in the reference above, which references the current working group posix standard. some other options to note in the page will follow.
1. prot_read this means the process can read the memory. this isn’t the only flag that gives the process read permission, however! the underlying file descriptor, in this case, must be opened with read privileges.
2. prot_write this means the process can write to the memory. this has to be supplied for a process to write to a mapping. if this is supplied and prot_none is also supplied, the latter wins and no writes can be performed. the underlying file descriptor, in this case, must either be opened with write privileges or a private mapping must be supplied below 3. prot_exec this means the process can execute this piece of memory. although this is not stated in posix documents, this shouldn’t be supplied with write or none because that would make this invalid under the nx bit or not being able to execute (respectively) 4. prot_none this means the process can’t do anything with the mapping. this could be useful if you implement guard pages in terms of security. if you surround critical data with many more pages that can’t be accessed, that decreases the chance of various attacks.
6. map_private this mapping will only be visible to the process itself. useful to not thrash the operating system.
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.
1. null, this tells mmap we don’t need any particular address to start from 2. length + offset - page_offset, mmaps the “rest” of the file into memory (starting from offset) 3. prot_read, we want to read the file 4. map_private, tell the os, we don’t want to share our mapping 5. fd, object descriptor that we refer to 6. pa_offset, the page aligned offset to start from
you’ve seen the virtual memory way of ipc, but there are more standard versions of ipc that are provided by the kernel. one of the big utilities is posix pipes. a pipe simply takes in a stream of bytes and spits out a sequence of bytes.
what does the following code do? first, it lists the current directory. the -1 means that it outputs one entry per line. the cut command then takes everything before the first period. sort sorts all the input lines, uniq makes sure all the lines are unique. finally, tee outputs the contents to the file dir_contents and the terminal for your perusal. the important part is that bash creates 5 separate processes and connects their standard outs/stdins with pipes the trail looks something like this.
figure 9.8: pipe process filedescriptor redirection the numbers in the pipes are the file descriptors for each process and the arrow represents the redirect or where the output of the pipe is going. a posix pipe is almost like its real counterpart - a program can stuff bytes down one end and they will appear at the other end in the same order. unlike real pipes, however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. the pipe system call is used to create a pipe. these file descriptors can be used with read and write. a common method of using pipes is to create the pipe before forking to communicate with a child process
unnamed pipes live in memory and are a simple and efficient form of inter-process communication (ipc) that is useful for streaming data and simple messages. once all processes have closed, the pipe resources are freed.
this command takes the output of ls -1 which lists the content of the current directory on one line each and pipes it to cut. cut take a delimiter, in this case, a dot, and a field position, in our case 1, and outputs per line the nth field by each delimiter. at a high level, this grabs the file names without the extension of our current directory.
a named pipe mkfifo is a pipe that a program calls open(2) on with read and/or write permissions. this is useful if you want to have a pipe between two processes without one processing having to fork the other process.
 open takes a path to a file and creates a file descriptor entry in the process table. if the file is inaccessible, it errors out.
 read takes a certain number of bytes that the kernel has received and reads them into a user-space buffer.
 write outputs a certain number of bytes to a file descriptor. if the file is not open in write mode, this will break. this may be buffered internally.
 close removes a file descriptor from a process’ file descriptors. this always succeeds for a valid file descriptor.
 lseek takes a file descriptor and moves it to a certain position. it can fail if the seek is out of bounds.
 fcntl is the catch-all function for file descriptors. set file locks, read, write, edit permissions, etc.
 fopen opens a file and returns an object. null is returned if the program doesn’t have permission for the file.
 fread reads a certain number of bytes from a file. an error is returned if already at the end of the file when which the program must call feof() to check if the program attempted to read past the end of the file.
 fgetc/fgets get a char or a string from a file  fscanf read a format string from the file  fwrite write some objects to a file  fprintf write a formatted string to a file  fclose close a file handle  fflush take any buffered changes and flush them to a file but programs don’t get the expressiveness that linux gives with system calls. a program can convert back and forth between them with int fileno(file* stream) and file* fdopen(int fd...). also, c files are buffered meaning that their contents may be written to the backing after the call returns. you can change that with c options.
jumping around too much causes thrashing and loses all the benefits of using mmap. the other usage of mmap is for direct memory inter-process communication. this means that a program can store structures in a piece of mmap’ed memory and share them between two processes. python and ruby use this mapping all the time to utilize copy on write semantics.
cpu scheduling is the problem of efficiently selecting which process to run on a system’s cpu cores. in a busy system, there will be more ready-to-run processes than there are cpu cores, so the system kernel must evaluate which processes should be scheduled to run and which processes should be executed later. the system must also decide whetherit should take a particular process and pause its execution – along with any associated threads.
that means we will assume that the processes are in memory and ready to go. the other types of scheduling are long and medium term. long term schedulers act as gatekeepers to the processing world. when a process requests another process to be executed, it can either tell the process yes, no, or wait. the medium term scheduler deals with the caveats of moving a process from the paused state in memory to the paused state on disk when there are too many processes or some process are known to use an insignificant amount of cpu cycles. think about a process that only checks something once an hour.
10.1 high level scheduler overview schedulers are pieces of software programs. in fact, you can implement schedulers yourself! if you are given a list of commands to exec, a program can schedule them them with sigstop and sigcont. these are called user space schedulers. hadoop and python’s celery may do some sort of user space scheduling or deal with the operating system.
3. running is the state that we hope most of our processes are in, meaning they are doing useful work. a process could either get preempted, blocked, or terminate. preemption brings the process back to the ready state. if a process is blocked, that means it could be waiting on a mutex lock, or it could’ve called sleep – either way, it willingly gave up control.
10.2 measurements scheduling affects the performance of the system, specifically the latency and throughput of the system. the throughput might be measured by a system value, for example, the i/o throughput - the number of bits written per second, or the number of small processes that can complete per unit time. the latency might be measured by the response time – elapse time before a process can start to send a response – or wait time or turnaround time –the elapsed time to complete a task. different schedulers offer different optimization trade-offs that may be appropriate for desired use. there is no optimal scheduler for all possible environments and goals. for example, shortest job first will minimize total wait time across all jobs but in interactive (ui) environments it would be preferable to minimize response time at the expense of some throughput, while fcfs seems intuitively fair and easy to implement but suffers from the convoy effect. arrival time is the time at which a process first arrives at the ready queue, and is ready to start executing. if a cpu is idle, the arrival time would also be the starting time of execution.
10.3 measures of efficiency first some definitions 1. start_time is the wall-clock start time of the process (cpu starts working on it) 2. end_time is the end wall-clock of the process (cpu finishes the process) 3. run_time is the total amount of cpu time required 4. arrival_time is the time the process enters the scheduler (cpu may start working on it) here are measures of efficiency and their mathematical equations 1. turnaround time is the total time from when the process arrives to when it ends. end_time - arrival_time 2. response time is the total latency (time) that it takes from when the process arrives to when the cpu actually starts working on it. start_time - arrival_time
3. wait time is the total wait time or the total time that a process is on the ready queue. a common mistake is to believe it is only the initial waiting time in the ready queue. if a cpu intensive process with no i/o takes 7 minutes of cpu time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. for those 2 minutes, the process was ready to run but had no cpu assigned. it does not matter when the job was waiting, the wait time is 2 minutes.
10.3.1 convoy effect the convoy effect is when a process takes up a lot of the cpu time, leaving all other processes with potentially smaller resource needs following like a convoy behind them.
10.4.2 preemptive shortest job first (psjf) preemptive shortest job first is like shortest job first but if a new job comes in with a shorter runtime than the total runtime of the current job, it is run instead. if it is equal like our example our algorithm can choose. the scheduler uses the total runtime of the process. if the scheduler wants to compare the shortest remaining time left, that is a variant of psjf called shortest remaining time first (srtf).
10.4.4 round robin (rr) processes are scheduled in order of their arrival in the ready queue. after a small time step though, a running process will be forcibly removed from the running state and placed back on the ready queue. this ensures long-running processes refrain from starving all other processes from running. the maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. as the time quanta approaches to infinity, round robin will be equivalent to fcfs.
10.4.5 priority processes are scheduled in the order of priority value. for example, a navigation process might be more important to execute than a logging process.
11.1 the osi model the open source interconnection 7 layer model (osi model) is a sequence of segments that define standards for both infrastructure and protocols for forms of radio communication, in our case the internet. the 7 layer model is as follows 1. layer 1: the physical layer. these are the actual waves that carry the bauds across the wire. as an aside, bits don’t cross the wire because in most mediums you can alter two characteristics of a wave – the amplitude and the frequency – and get more bits per clock cycle.
2. layer 2: the link layer. this is how each of the agents reacts to certain events (error detection, noisy channels, etc). this is where ethernet and wifi live.
3. layer 3: the network layer. this is the heart of the internet. the bottom two protocols deal with communication between two different computers that are directly connected. this layer deals with routing packets from one endpoint to another.
4. layer 4: the transport layer. this layer specifies how the slices of data are received. the bottom three layers make no guarantee about the order that packets are received and what happens when a packet is dropped. using different protocols, this layer can.
5. layer 5: the session layer. this layer makes sure that if a connection in the previous layers is dropped, a new connection in the lower layers can be established, and it looks like nothing happened to the end-user.
6. layer 6: the presentation layer. this layer deals with encryption, compression, and data translation. for example, portability between different operating systems like translating newlines to windows newlines.
this book won’t cover networking in depth. we will focus on some aspects of layers 3, 4, and 7 because they are essential to know if you are going to be doing something with the internet, which at some point in your career you will be. as for another definition, a protocol is a set of specifications put forward by the internet engineering task force that govern how implementers of a protocol have their program or circuit behave under specific circumstances.
11.2 layer 3: the internet protocol the following is a short introduction to internet protocol (ip), the primary way to send datagrams of information from one machine to another. “ip4”, or more precisely, ipv4 is version 4 of the internet protocol that describes how to send packets of information across a network from one machine to another. even as of 2018, ipv4 still dominates internet traffic, but google reports that 24 countries now supply 15% of their traffic through ipv6 [2].
there are special ip addresses. one such in ipv4 is 127.0.0.1, ipv6 as 0:0:0:0:0:0:0:1 or ::1 also known as localhost. packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine. there are a lot of others that are denoted by certain octets being zeros or 255, the maximum value. you won’t need to know all the terminology, keep in mind that the actual number of ip addresses that a machine can have globally over the internet is smaller than the number of “raw” addresses. this book covers how ip deals with routing, fragmenting, and reassembling upper-level protocols. a more in-depth aside follows.
figure 11.2: extra: tcp header specification most services on the internet today use tcp because it efficiently hides the complexity of the lower, packet-level nature of the internet. tcp or transport control protocol is a connection-based protocol that is built on top of ipv4 and ipv6 and therefore can be described as “tcp/ip” or “tcp over ip”. tcp creates a pipe between two machines and abstracts away the low-level packet-nature of the internet. thus, under most conditions, bytes sent over a tcp connection delivered and uncorrupted. high performance and error-prone code won’t even assume that!
7. congestion control. congestion control is performed on the sender’s side. congestion control is to avoid a sender from flooding the network with too many packets. this is important to make sure that each tcp connection is treated fairly. meaning that two connections leaving a computer to google and youtube receive the same bandwidth and ping as each other. one can easily define a protocol that takes all the bandwidth and leaves other protocols in the dust, but this tends to be malicious because many times limiting a computer to a single tcp connection will yield the same result.
8. connection-oriented/life cycle oriented. you can imagine a tcp connection as a series of bytes sent through a pipe. there is a “lifecycle” to a tcp connection though. tcp handles setting up the connection through syn syn-ack ack. this means the client will send a synchronization packet that tells tcp what starting sequence to start on. then the receiver will send a syn-ack message acknowledging the synchronization number. then the client will acknowledge that with one last packet. the connection is now open for both reading and writing on both ends tcp will send data and the receiver of the data will acknowledge that it received a packet. then every so often if a packet is not sent, tcp will trade zero-length packets to make sure the connection is still alive. at any point, the client and server can send a fin packet meaning that the server will not transmit. this packet can be altered with bits that only close the read or write end of a particular connection. when all ends are closed then the connection is over.
4. delimiting requests. tcp is naturally connection-oriented. applications that are communicating over tcp need to find a unique way of telling each other that this request or response is over. http delimits the header through two carriage returns and uses either a length field or one keeps listening until the connection closes
htons(xyz) returns the 16-bit unsigned integer ‘short’ value xyz in network byte order. htonl(xyz) returns the 32-bit unsigned integer ‘long’ value xyz in network byte order. any longer integers need to have the computers specify the order.
1. int getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, struct addri the getaddrinfo call if successful, creates a linked-list of addrinfo structs and sets the given pointer to point to the first one.
2. int socket(int domain, int socket_type, int protocol); the socket call creates a network socket and returns a descriptor that can be used with read and write. in this sense, it is the network analog of open that opens a file stream – except that we haven’t connected the socket to anything yet!
sockets are created with a domain af_inet for ipv4 or af_inet6 for ipv6, socket_type is whether to use udp, tcp, or other some other socket type, the protocol is an optional choice of protocol configuration for our examples this we can leave this as 0 for default. this call creates a socket object in the kernel with which one can communicate with the outside world/network. you can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.
tcp sockets are similar to pipes and are often used in situations that require ipc. we don’t mention it in the previous chapters because it is overkill using a device suited for networks to simply communicate between processes on a single thread.
3. connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); finally, the connect call attempts the connection to the remote machine. we pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. there are different kinds of socket address structures that can require more memory. so in addition to passing the pointer, the size of the structure is also passed. to help identify errors and mistakes it is good practice to check the return value of all networking calls, including connect
there is an old function gethostbyname is deprecated. it’s the old way convert a hostname into an ip address.
"2. ""connection: close"" means that as soon as the request is over, please close the connection. this line won’t be used for any other connections. this is a little redundant given that http 1.0 doesn’t allow you to send multiple requests, but it is better to be explicit given there are non-conformant technologies."
11.4 layer 4: tcp server the four system calls required to create a minimal tcp server are socket, bind, listen, and accept. each has a specific purpose and should be called in roughly the above order 1. int socket(int domain, int socket_type, int protocol) to create an endpoint for networking communication. a new socket by itself is stores bytes. though we’ve specified either a packet or stream-based connections, it is unbound to a particular network interface or port. instead, socket returns a network descriptor that can be used with later calls to bind, listen and accept.
as one gotcha, these sockets must be declared passive. passive server sockets wait for another host to connect. instead, they wait for incoming connections. additionally, server sockets remain open when the peer disconnects. instead, the client communicates with a separate active socket on the server that is specific to that connection.
2. int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); the bind call associates an abstract socket with an actual network interface and port. it is possible to call bind on a tcp client. the port information used by bind can be set manually (many older ipv4-only c code examples do this), or be created using getaddrinfo.
by default, a port is released after some time when the server socket is closed. instead, the port enters a “timed-wait” state. this can lead to significant confusion during development because the timeout can make valid networking code appear to fail.
3. int listen(int sockfd, int backlog); the listen call specifies the queue size for the number of incoming, unhandled connections. there are the connections unassigned to a file descriptor by accept. typical values for a high-performance server are 128 or more.
one can use the macros ni_maxhost to denote the maximum length of a hostname, and ni_maxserv to denote the maximum length of a port. ni_numerichost gets the hostname as a numeric ip address and similarly for ni_numericserv although the port is usually numeric, to begin with. the open bsd man pages have more information 5. int close(int fd) and int shutdown(int fd, int how) use the shutdown call when you no longer need to read any more data from the socket, write more data, or have finished doing both. when you call shutdown on socket on the read and/or write ends, that information is also sent to the other end of the connection. if you shut down the socket for further writing at the server end, then a moment later, a blocked read call could return 0 to indicate that no more bytes are expected. similarly, a write to a tcp connection that has been shut down for reading will generate a sigpipe use close when your process no longer needs the socket file descriptor.
11.5 layer 4: udp udp is a connectionless protocol that is built on top of ipv4 and ipv6. it’s simple to use. decide the destination address and port and send your data packet! however, the network makes no guarantee about whether the packets will arrive. packets may be dropped if the network is congested. packets may be duplicated or arrive out of order.
11.5.2 udp client udp clients are pretty versatile below is a simple client that sends a packet to a server specified through the command line. note that this client sends a packet and doesn’t wait for an acknowledgment. it fires and forgets.
the addr struct will hold the sender (source) information about the arriving packet. note the sockaddr_storage type is sufficiently large enough to hold all possible types of socket addresses – ipv4, ipv6 or any other internet protocol. the full udp server code is below.
11.6 layer 7: http layer 7 of the osi layer deals with application-level interfaces. meaning that you can ignore everything below this layer and treat the internet as a way of communicating with another computer than can be secure and the session may reconnect. common layer 7 protocols are the following 1. http(s) - hypertext transfer protocol. sends arbitrary data and executes remote actions on a web server.
2. ftp - file transfer protocol. transfers a file from one computer to another 3. tftp - trivial file transfer protocol. same as above but using udp.
4. dns - domain name service. translates hostnames to ip addresses 5. smtp - simple mail transfer protocol. allows one to send plain text emails to an email server 6. ssh - secure shell. allows one computer to connect to another computer and execute commands remotely.
7. bitcoin - decentralized cryptocurrency 8. bittorrent - peer to peer file sharing protocol 9. ntp - network time protocol. this protocol helps keep your computer’s clock synced with the outside world
for example, a coffee shop internet connection could easily subvert your dns requests and send back different ip addresses for a particular domain. the way this is usually subverted is that after the ip address is obtained then a connection is usually made over https. https uses what is called the tls (formerly known as ssl) to secure transmissions and verify that the hostname is recognized by a certificate authority. certificate authorities often get hacked so be careful of equating a green lock to secure. even with this added layer of security, the united states government has recently issued a request for everyone to upgrade their dns to dnssec which includes additional security-focused technologies to verify with high probability that an ip address is truly associated with a hostname.
digression aside, dns works like this in a nutshell 1. send a udp packet to your dns server 2. if that dns server has the packet cached return the result 3. if not, ask higher-level dns servers for the answer. cache and send the result 4. if either packet is not answered from within a guessed timeout, resend the request.
posix lets you set a flag on a file descriptor such that any call to read() on that file descriptor will return immediately, whether it has finished or not. with your file descriptor in this mode, your call to read() will start the read operation, and while it’s working you can do other useful work. this is called “non-blocking” mode since the call to read() doesn’t block.
2. writefds - a file descriptor in writefds is ready when a call to write() will succeed.
select() returns the total number of ready file descriptors. if none of them become ready during the time defined by timeout, it will return 0. after select() returns, the caller will need to loop through the file descriptors in readfds and/or writefds to see which ones are ready. as readfds and writefds act as both input and output parameters, when select() indicates that there are ready file descriptors, it would have overwritten them to reflect only the ready file descriptors. unless the caller intends to call select() only once, it would be a good idea to save a copy of readfds and writefds before calling it. here is a comprehensive snippet.
to wait for some of the file descriptors to become ready, use epoll_wait(). the epoll_event struct that it fills out will contain the data you provided in event.data when you added this file descriptor. this makes it easy for you to look up your data associated with this file descriptor.
to unsubscribe one file descriptor from epoll while leaving others active, use epoll_ctl() with the epoll_ctl_del option.
1. there are two modes. level triggered and edge-triggered. level triggered means that while the file descriptor
5. epoll has the epolloneshot flag which will remove a file descriptor after it has been returned in epoll_wait 6. epoll using level-triggered mode could starve certain file descriptors because it is unknown how much data the application will read from each descriptor.
11.8 remote procedure calls rpc or remote procedure call is the idea that we can execute a procedure on a different machine. in practice, the procedure may execute on the same machine. however, it may be in a different context. for example, the operation under a different user with different permissions and different lifecycles.
11.8.1 privilege separation the remote code will execute under a different user and with different privileges from the caller. in practice, the remote call may execute with more or fewer privileges than the caller. this in principle can be used to improve the security of a system by ensuring components operate with the least privilege. unfortunately, security concerns need to be carefully assessed to ensure that rpc mechanisms cannot be subverted to perform unwanted actions.
google protocol buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low cpu overhead and minimal memory copying. this means client and server stub code in multiple languages can be generated from the .proto specification file to marshal data to and from a binary stream.
some terminology before we begin this chapter. a filesystem, as we’ll define more concretely later, is anything that satisfies the api of a filesystem. a filesystem is backed by a storage medium, such as a hard disk drive, solid state drive, ram, etc. a disk is either a hard disk drive (hdd) which includes a spinning metallic platter and a head which can zap the platter to encode a 1 or a 0, or a solid-state drive (ssd) that can flip certain nand gates on a chip or standalone drive to store a 1 or a 0. as of 2019, ssds are an order of magnitude faster than the standard hdd. these are typical backings for a filesystem. a filesystem is implemented on top of this backing, meaning that we can either implement something like ext, minixfs, ntfs, fat32, etc. on a commercially available hard disk. this filesystem tells the operating system how to organize the 1s and 0s to store file information as well as directory information, but more on that later. to avoid being pedantic, we’ll say that a filesystem like ext or ntfs implements the filesystem api directly (open, close, etc). often, operating systems will add a layer of abstraction and require that the operating system satisfy its api instead (think imaginary functions linux_open, linux_close etc). the two benefits are that one filesystem can be implemented for multiple operating system apis and adding a new os filesystem call doesn’t require all of the underlying file systems to change their api. for example, in the next iteration of linux if there was a new system call to create a backup of a file, the os can implement that with the internal api rather than requiring all filesystem drivers to change their code.
a filesystem is an implementation of the file interface. in this chapter, we will be exploring the various callbacks a filesystem provides, some typical functionality and associated implementation details. in this class, we will mostly talk about filesystems that serve to allow users to access data on disk, which are integral to modern computers.
here are some common features of a filesystem: 1. they deal with both storing local files and handle special devices that allow for safe communication between the kernel and user space.
before we dive into the details of a filesystem, let’s take a look at some examples. to clarify, a mount point is simply a mapping of a directory to a filesystem represented in the kernel.
2. procfs usually mounted at /proc, provides information and control over processes.
4. tmpfs mounted at /tmp in some systems, an in-memory filesystem to hold temporary files.
"as you may have noticed, some filesystems provide an interface to things that aren’t ""files"". filesystems such as procfs are usually referred to as virtual filesystems, since they don’t provide data access in the same sense as a traditional filesystem would. technically, all filesystems in the kernel are represented by virtual filesystems, but we will differentiate virtual filesystems as filesystems that actually don’t store anything on a hard disk."
12.1.1 the file api a filesystem must provide callback functions to a variety of actions. some of them are listed below:  open opens a file for io  read read contents of a file  write write to a file  close close a file and free associated resources  chmod modify permissions of a file  ioctl interact with device parameters of character devices such as terminals not every filesystem supports all the possible callback functions. for example, many filesystems omit ioctl or link. many filesystems aren’t seekable meaning that they exclusively provide sequential access. a program cannot move to an arbitrary point in the file. this is analogous to seekable streams. in this chapter, we will not be examining each filesystem callback. if you would like to learn more about this interface, try looking at the documentation for filesystems at the user space level (fuse).
1. disk block a disk block is a portion of the disk that is reserved for storing the contents of a file or a directory.
2. inode an inode is a file or directory. this means that an inode contains metadata about the file as well as pointers to disk blocks so that the file can actually be written to or read from.
a filesystem has a special block denoted as a superblock that stores metadata about the filesystem such as a journal (which logs changes to the filesystem), a table of inodes, the location of the first inode on disk, etc. the important thing about a superblock is that it is in a known location on disk. if not, your computer may fail to boot! consider a simple rom programmed into your motherboard. if your processor can’t tell the motherboard to start reading and decipher a disk block to start the boot sequence, you are out of luck.
12.2.1 file contents from wikipedia: in a unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be various things including a file or a directory. each inode stores the attributes and disk block location(s) of the filesystem object’s data. filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).
it is common to think of the file name as the ‘actual’ file. it’s not! instead, consider the inode as the file. the inode holds the meta-information (last accessed, ownership, size) and points to the disk blocks used to hold the file contents. however, the inode does not usually store a filename. filenames are usually only stored in directories (see below).
to solve this problem, we introduce indirect blocks. a single indirect block is a block that stores pointers to more data blocks. similarly, a double indirect block stores pointers to single indirect blocks, and the concept can be generalized to arbitrary levels of indirection. this is a important concept, as inodes are stored in the superblock, or some other structure in a well known location with a constant amount of space, indirection allows exponential increases in the amount of space an inode can keep track of.
12.2.2 directory implementation a directory is a mapping of names to inode numbers. it is typically a normal file, but with some special bits set in its inode and a specific structure for its contents. posix provides a small set of functions to read the filename and inode number for each entry, which we will talk about in depth later in this chapter.
there are two main gotchas and one consideration. the readdir function returns “.” (current directory) and “..” (parent directory). the other is programs need to explicity exclude subdirectories from a search, otherwise the search may take a long time.
while modeling the filesystem as a tree would imply that every inode has a unique parent directory, links allow inodes to present themselves as files in multiple places, potentially with different names, thus leading to an inode having multiple parent directories. there are two kinds of links: 1. hard links a hard link is simply an entry in a directory assigning some name to an inode number that already has a different name and mapping in either the same directory or a different one. if we already have a file on a file system we can create another link to the same inode using the ‘ln’ command:
2. soft links the second kind of link is called a soft link, symbolic link, or symlink. a symbolic link is different because it is a file with a special bit set and stores a path to another file. quite simply, without the special bit, it is nothing more than a text file with a file path inside. note when people generally talk about a link without specifying hard or soft, they are referring to a hard link.
to create a symbolic link in the shell, use ln -s. to read the contents of the link as a file, use readlink.
once the archive area has a copy of a particular file, then future archives can re-use these archive files rather than creating a duplicate file. this is called an incremental backup. apple’s “time machine” software does this.
"12.2.6 pathing now that we have definitions, and have talked about directories, we come across the concept of a path. a path is a sequence of directories that provide one with a ""path"" in the graph that is a filesystem. however, there are some nuances. it is possible to have a path called a/b/../c/./. since .. and . are special entries in directories, this is a valid path that actually refers to a/c. most filesystem functions will allow uncompressed paths to be passed in."
lstat() is identical to stat(), except that if pathname is a symbolic link, then it returns information about the link itself, not the file that it refers to.
struct stat { dev_t st_dev; /* id of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* file type and mode */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user id of owner */ gid_t st_gid; /* group id of owner */ dev_t st_rdev; /* device id (if special file) */ off_t st_size; /* total size, in bytes */ blksize_t st_blksize; /* block size for filesystem i/o */ blkcnt_t st_blocks; /* number of 512b blocks allocated */ struct timespec st_atim; /* time of last access */ struct timespec st_mtim; /* time of last modification */ struct timespec st_ctim; /* time of last status change */ };
12.3 permissions and bits permissions are a key part of the way unix systems provide security in a filesystem. you may have noticed that the st_mode field in struct stat contains more than the file type. it also contains the mode, a description detailing what a user can and can’t do with a given file. there are usually three sets of permissions for any file.
chmod takes a number and a file and changes the permission bits. however, before we can discuss chmod in detail, we must also understand the user id (uid) and group id (gid) as well.
12.3.1 user id / group id every user in a unix system has a user id. this is a unique number that can identify a user. similarly, users can be added to collections called groups, and every group also has a unique identifying number. groups have a variety of uses on unix systems. they can be assigned capabilities - a way of describing the level of control a user has over a system. for example, a group you may have run into is the sudoers group, a set of trusted users who are allowed to use the command sudo to temporarily gain higher privileges. we’ll talk more about how sudo works in this chapter. every file, upon creation, an owner, the creator of the file. this owner’s user id (uid) can be found inside the st_mode file of a struct stat with a call to stat. similarly, the group id (gid) is set as well.
the base-8 (‘octal’) digits describe the permissions for each role: the user who owns the file, the group and everyone else. the octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1) example: chmod 755 myfile 1. r + w + x = digit * user has 4+2+1, full permission 2. group has 4+0+1, read and execute permission 3. all users have 4+0+1, read and execute permission
12.3.3 understanding the ‘umask’ the umask subtracts (reduces) permission bits from 777 and is used when new files and new directories are created by open, mkdir etc. by default, the umask is set to 022 (octal), which means that group and other privileges will be exclusively readable. each process has a current umask value. when forking, the child inherits the parent’s umask value.
it indicated that when run, the program will set the uid of the user to that of the owner of the file. similar, there is a setgid bit which sets the gid of the executor to the gid of the owner. the canonical example of a program with setuid set is sudo.
 getuid returns the real user id (zero if logged in as root)  geteuid returns the effective user id (zero if acting as root, e.g. due to the setuid flag set on a program) these functions can allow one to write a program that can only be run by a privileged user by checking geteuid or go a step further and ensure that the only user who can run the code is root by using getuid.
12.3.5 the ‘sticky’ bit sticky bits as we use them today serve a different purpose from initial introduction. sticky bits were a bit that could be set on an executable file that would allow a program’s text segment to remain in swap even after the end of the program’s execution. this made subsequent executions of the same program faster. today, this behavior is no longer supported and the sticky bit only holds meaning when set on a directory, when a directory’s sticky bit is set only the file’s owner, the directory’s owner, and the root user can rename or delete the file. this is useful when multiple users have write access to a common directory. a common use of the sticky bit is for the shared and writable /tmp directory where many users’ files may be stored, but users should not be able to access files belonging to other users.
12.4.2 obtaining random data /dev/random is a file that contains a random number generator where the entropy is determined from environmental noise. random will block/wait until enough entropy is collected from the environment.
both the input and output files in the example above are virtual - they don’t exist on a disk. this means the speed of the transfer is unaffected by hardware power.
dd is also commonly used to make a copy of a disk or an entire filesystem to create images that can either be burned on to other disks or to distribute data to other users.
an example use of touch is to force make to recompile a file that is unchanged after modifying the compiler options inside the makefile. remember that make is ‘lazy’ - it will compare the modified time of the source file with the corresponding output file to see if the file needs to be recompiled.
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.
the iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. cdroms).
mmap takes a file and maps its contents into memory. this allows a user to treat the entire file as a buffer in memory for easier semantics while programming, and to avoid having to read a file as discrete chunks explicitly.
1. mmap requires a file descriptor, so we need to open the file first 2. we seek to our desired size and write one byte to ensure that the file is sufficient length 3. when finished call munmap to unmap the file from memory.
the prot_read | prot_write options specify the virtual memory protection. the option prot_exec (not used here) can be set to allow cpu execution of instructions in memory.
for efficiency, the kernel caches recently used disk blocks. for writing, we have to choose a trade-off between performance and reliability. disk writes can also be cached (“write-back cache”) where modified disk blocks are stored in memory until evicted. alternatively, a ‘write-through cache’ policy can be employed where disk writes are sent immediately to the disk. the latter is safer as filesystem modifications are quickly stored to persistent media but slower than a write-back cache. if writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block. note, this is a simplified description because solid state drives (ssds) can be used as a secondary write-back cache.
12.6.1 raid - redundant array of inexpensive disks one way to protect against this is to store the data twice! this is the main principle of a “raid-1” disk array. by duplicating the writes to a disk with writes to another backup disk, there are exactly two copies of the data. if one disk fails, the other disk serves as the only copy until it can be re-cloned. reading data is faster since data can be requested from either disk, but writes are potentially twice as slow because now two write commands need to be issued for every disk block write. compared to using a single disk, the cost of storage per byte has doubled.
another common raid scheme is raid-0, meaning that a file could be split up among two disks, but if any disk fails then the files are irrecoverable. this has the benefit of halving write times because one part of the file could be writing to hard disk one and another part to hard disk two.
it is also common to combine these systems. if you have a lot of hard disks, consider raid-10. this is where you have two systems of raid-1, but the systems are hooked up in raid-0 to each other. this means you would get roughly the same speed from the slowdowns but now any one disk can fail and you can recover that disk. if two disks from opposing raid partitions fail, there is a chance that you can recover though we don’t could on it most of the time.
12.6.2 higher levels of raid raid-3 uses parity codes instead of mirroring the data. for each n-bits written, we will write one extra bit, the ‘parity bit’ that ensures the total number of 1s written is even. the parity bit is written to an additional disk. if any disk including the parity disk is lost, then its contents can still be computed using the contents of the other disks.
so, what does our hypothetical filesystem look like? we will base it off of the minixfs, a simple filesystem that happens to be the first filesystem that linux ran on. it is laid out sequentially on disk, and the first section is the superblock. the superblock stores important metadata about the entire filesystem. since we want to be able to read this block before we know anything else about the data on disk, this needs to be in a well-known location so the start of the disk is a good choice. after the superblock, we’ll keep a map of which inodes are being used. the nth bit is set if the nth inode – 0 being the inode root – is being used. similarly, we store a map recording which data blocks are used. finally, we have an array of inodes followed by the rest of the disk - implicitly partitioned into data blocks. one data block may be identical to the next from the perspective of the hardware components of the disk. thinking about the disk as an array of data blocks is simply something we do so that we have a way to describe where files live on disk.
note that a file will fill up each of its data blocks completely before requesting an additional data block. we will refer to this property as the file being compact. the file presented above is interesting since it uses all of its direct blocks, one of the entries for its indirect block and partially uses another indirect block.
 “ls -l” shows the size of each file in a directory. is the size stored in the directory or in the file’s inode?
signals are a convenient way to deliver low-priority information and for users to interact with their programs when other ways don’t work (for example standard input being frozen). they allow a program to clean up or perform an action in the case of an event. sometimes, a program can choose to ignore events which is supported.
13.1 the deep dive of signals a signal allows one process to asynchronously send an event or message to another process. if that process wants to accept the signal, it can, and then, for most signals, decide what to do with that signal.
first, a bit of terminology. a signal disposition is a per-process attribute that determines how a signal is handled after it is delivered. think of it as a table of signal-action pairs. the full discussion is in the man page.
the actions are 1. term, terminates the process 2. ign, ignore 3. core, generate a core dump 4. stop, stops a process 5. cont, continues a process
"2. a signal that is created is in a ""generated"" state."
3. the time between when a signal is generated and the kernel can apply the mask rules is called the pending state.
6. otherwise, the kernel delivers the signal by stopping whatever a particular thread is doing currently, and jumps that thread to the signal handler. the signal is now in the delivered phase. more signals can be generated now, but they can’t be delivered until the signal handler is complete which is when the delivered phase is over.
7. finally, we consider a signal caught if the process remains intact after the signal was delivered.
name usual use sigint stop a process nicely sigquit stop a process harshly sigterm stop a process even more harshly sigstop suspends a process sigcont starts after a stop sigkill you want the process gone
send sigkill (terminate the process) kill -sigkill 4409 kill -9 4409 use kill all instead to kill a process by executable name killall -l firefox
for non-root processes, signals can only be sent to processes of the same user. you can’t sigkill any process!
13.3 handling signals there are strict limitations on the executable code inside a signal handler. most library and system calls are async-signal-unsafe, meaning they may not be used inside a signal handler because they are not re-entrant.
re-entrant safety means that your function can be frozen at any point and executed again, can you guarantee that your function wouldn’t fail? let’s take the following
the above code might appear to be correct on paper. however, we need to provide a hint to the compiler and the cpu core that will execute the main() loop. we need to prevent compiler optimization. the expression pleasestop doesn’t get changed in the body of the loop, so some compilers will optimize it to true todo: citation needed. secondly, we need to ensure that the value of pleasestop is uncached using a cpu register and instead always read from and written to main memory. the sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptible operation. it is impossible to read a value that is composed of some new bit values and old bit values.
you can also choose a handle pending signals asynchronously or synchronously. to install a signal handler to asynchronously handle signals, use sigaction. to synchronously catch a pending signal use sigwait which blocks until a signal is delivered or signalfd which also blocks and provides a file descriptor that can be read() to retrieve pending signals.
you can use system call sigaction to set the current handler and disposition for a signal or read the current signal handler for a particular signal.
 sig_block. the set of blocked signals is the union of the current set and the set argument.
 sig_unblock. the signals in set are removed from the current set of blocked signals. it is permissible to attempt to unblock a signal which is not blocked.
 sig_setmask. the set of blocked signals is set to the argument set.
the sigset type behaves as a set. it is a common error to forget to initialize the signal set before adding to the set.
13.4.1 sigwait sigwait can be used to read one pending signal at a time. sigwait is used to synchronously wait for signals, rather than handle them in a callback. a typical use of sigwait in a multi-threaded program is shown below. notice that the thread signal mask is set first (and will be inherited by new threads). the mask prevents signals from being delivered so they will remain in a pending state until sigwait is called. also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is used as the set of signals that sigwait can catch and return.
pthread_sigmask(sig_setmask, &set, null) - replace the thread’s mask with given signal set pthread_sigmask(sig_block, &set, null) - add the signal set to the thread’s mask pthread_sigmask(sig_unblock, &set, null) - remove the signal set from the thread’s mask
computer security is the protection of hardware and software from unauthorized access or modification. even if you don’t work directly in the computer security field, the concepts are important to learn because all systems will have attackers given enough time. even though this is introduced as a different chapter, it is important to note that most of these concepts and code examples have already been introduced at different points in the course. we won’t go in depth about all of the common ways of attack and defense nor will we go into how to perform all of these attacks in an arbitrary system. our goal is to introduce you to the field of making programs do what you want to do.
14.1 security terminology and ethics there is some terminology that needs to be explained to get someone who has little to no experience in computer security up to speed 1. an attacker is typically the user who is trying to break into the system. breaking into the system means performing an action that the developer of the system didn’t intend. it could also mean accessing a system you shouldn’t have access to.
2. a defender is typically the user who is preventing the attacker from breaking into the system. this may be the developer of the system.
3. there are different types of attackers. there are white hat hackers who attempt to hack a defender with their consent. this is commonly a form of pre-emptive testing – in case a not-so-friendly attack comes along.
the black hat hackers are hackers who hack without permission and the intent to use the information obtained for any purpose. gray hat hacking differs because the hacker’s intent is to inform the defender of the vulnerability – though this can be hard to judge at times.
non-authorized use of a ‘protected computer’ of a computer as a felony. since most computers are involved in some interstate/international commerce (the internet) most computers fall under this category. it is important to think about your actions and have some ladder of accountability before executing any attack or defense. to be more concrete, make sure supervisors in your organization have given you their blessing before trying to execute an attack.
2. determine whetheryou need to “hack” the system. a hack is defined generally as trying to use a system unintendedly. first, you should determine if your use is intended or unintended or somewhere in the middle – get a decision for them. if you can’t get that, make a reasonable judgement as to what the intended use.
1. information confidentiality means that only authorized parties are allowed to see a piece of information 2. information integrity means that only authorized parties are allowed the modify a piece of information, regardless of whether they are allowed to see it. it ensures that information remains in complete during transit.
3. information availability means information, or a service, is available when it is needed.
one of the most prominent bugs concerning this is spectre [2]. spectre is a bug where instructions that otherwise wouldn’t be executed are speculatively executed due to out-of-order instruction execution. the following snippet is a high-level proof of concept.
3. address space layout randomization (aslr). aslr causes the address spaces of important sections of a process, including the base address of the executable and the positions of the stack, heap and libraries, to start at randomized values, on every run. this is so that an attacker with a running executable has to randomly guess where sensitive information could be hidden. for example, an attacker may use this to easily perform a return-to-libc attack.
5. write xor execute, also known as data execution prevention (dep). this is a protection that was covered in the ipc section that distinguishes code from data. a page can either be written to or executed but not both. this is to prevent buffer overflows where attackers write arbitrary code, often stored on the stack or heap, and execute with the user’s permissions.
7. apparmor. apparmor is a suite of operating system tools at the userspace level to restrict applications to certain operations.
2. unveil. unveil is a system call that restricts the access of a current program to a few directories. those permissions apply to all forked programs as well. this means if you have a suspicious executable that you want to run whose description is “creates a new file and outputs random words” one could use this call to restrict access to a safe subdirectory and watch it receive the sigkill signal if it tries to access system files in the root directory, for example. this could be useful for your program as well. if you want to ensure that no user data is lost during an update (which is what happened with a steam system update), then the system could only reveal the program’s installation directory. if an attacker manages to find an exploit in the executable, it can only compromise the installation directory.
3. sudo. sudo is an openbsd project that runs everywhere! before to run commands as root, one would have to drop to a root shell. some times that would also mean giving users scary system capabilities. sudo gives you access to perform commands as root for one-offs without giving a long list of capabilities to all of your users.
14.2.5 virtualization security virtualization is the act of creating a virtual version of an environment for a program to run on. though that definition might be bent a little with the advent of new-age bare metal virtual machines, the abstraction is still there.
one can imagine a single operating system per motherboard. virtualization in the software sense is providing “virtual” motherboard features like usb ports or monitors that another program (the bridge) communicates with the actual hardware to perform a task. a simple example is running a virtual machine on your host desktop!
one can spin up an entirely different operating system whose instructions are fed through another program and executed on the host system. there are many forms of virtualization that we use today. we will discuss two popular forms below. one form is virtual machines. these programs emulate all forms of motherboard peripherals to create a full machine. another form is containers. virtual machines are good but are often bulky and programs only need a certain level of protection. containers are virtual machines that don’t emulate all motherboard peripherals and instead share with the host operating system, adding in additional layers of security.
now, you can’t have proper virtualization without security. one of the reasons to have virtualization is to ensure that the virtualized environment doesn’t maliciously leak back into the host environment. we say maliciously because there are intended ways of communication that we want to keep in check. here are some simple examples of security provided through virtualization 1. chroot is a contrived way of creating a virtualization environment. chroot is short for change root. this changes where a program believes that (/) is mounted on the system. for example with chroot, one can make a hello world program believe /home/user/ is actually the root directory. this is useful because no other files are exposed. this is contrived because linux still needs additional tools (think the c standard library) to come from different directories such as /usr/lib which means those could still be vulnerable.
2. namespaces are linux’s better way to create a virtualization environment. we won’t go into this too much, just know that they exist.
3. hardware virtualization technology. hardware vendors have become increasingly aware that physical protections are needed when emulating instructions. as such, there can be switches enabled by the user that allows the operating system to flip into a virtualization mode where instructions are run as normal but are monitored for malicious activity. this helps the performance and increases the security of virtualized environments.
14.3.1 security at the tcp level 1. encryption. tcp is unencrypted! this means any data that is sent over a tcp connection is in plain text. if one needs to send encrypted data, one needs to use a higher level protocol such as https or develop their own.
2. identity verification. in tcp, there is no way to verify the identity of who the program is connecting to.
4. syn-flood. before the first synchronization packet is acknowledged, there is no connection. that means a malicious attacker can write a bad tcp implementation that sends out a flood of syn packets to a hapless server. the syn flood is easily mitigated by using iptables or another netfilter module to drop all incoming connections from an ip address after a certain volume of traffic is reached for a certain period.
5. denial of service, distributed denial of service is the hardest form of attack to stop. companies today are still trying to find good ways to ease these attacks. this involves sending all sorts of network traffic forward to servers in the hopes that the traffic will clog them up and slow down the servers. in big systems, this can lead to cascading failures. if a system is engineered poorly, one server’s failure causes all the other servers to pick up more work which increases the probability that they will fail and so on and so forth.
7. complete the function pointer typedef to declare a pointer to a function that takes a void* argument and returns a void*. name your type ‘pthread_callback’
14. your startup offers path planning using the latest traffic information. your overpaid intern has created a thread unsafe data structure with two functions: shortest (which uses but does not modify the graph) and set_edge (which modifies the graph).
6. use posix calls fork pipe dup2 and close to implement an autograding program. capture the standard output of a child process into a pipe. the child process should exec the program ./test with no additional arguments (other than the process name). in the parent process read from the pipe: exit the parent process as soon as the captured output contains the ! character. before exiting the parent process send sigkill to the child process. exit 0 if the output contained a !. otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. be sure to close the unused ends of the pipe in the parent and child process 7. this advanced challenge uses pipes to get an “ai player” to play itself until the game is complete. the program tic tac toe accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. a turn is specified using two characters. for example “a1” and “c3” are two opposite corner positions. the string b2a1a3 is a game of 3 turns/plys. a valid response is b2a1a3c1 (the c1 response blocks the diagonal b2 a3 threat). the output line may also include a suffix -i win -you win -invalid or -draw use pipes to control the input and output of each child process created. when the output contains a -, print the final output line (the entire game sequence and the result) and exit.
6. create a simple tcp echo server. this is a server that reads bytes from a client until it closes and echoes the bytes back to the client.
39. implement a udp server that listens on port 2000. reserve a buffer of 200 bytes. listen for an arriving packet. valid packets are 200 bytes or less and start with four bytes 0x65 0x66 0x67 0x68. ignore invalid packets. for valid packets add the value of the fifth byte as an unsigned value to a running total and print the total so far. if the running total is greater than 255 then exit.
16.1 the linux kernel throughout the course of cs 241, you become familiar with system calls - the userspace interface to interacting with the kernel. how does this kernel actually work? what is a kernel? in this section, we will explore these questions in more detail and shed some light on various black boxes that you have encountered in this course. we will mostly be focusing on the linux kernel in this chapter, so please assume that all examples pertain to the linux kernel unless otherwise specified.
as it stands, most of you are probably familiar with the linux kernel, at least in terms of interacting with it via system calls. some of you may also have explored the windows kernel, which we won’t talk about too much in this chapter. or darwin, the unix-like kernel for macos (a derivative of bsd). those of you who might have done a bit more digging might have also encountered projects such a gnu hurd or zircon.
kernels can generally be classified into one of two categories, a monolithic kernel or a micro-kernel. a monolithic kernel is essentially a kernel and all of it’s associated services as a single program. a micro-kernel on the other hand is designed to have a main component which provides the bare-minimum functionality that a kernel needs. this involves setting up important device drivers, the root filesystem, paging or other functionality that is imperative for other higher-level features to be implemented. the higher-level features (such as a networking stack, other filesystems, and non-critical device drivers) are then implemented as separate programs that can interact with the kernel by some form of ipc, typically rpc. as a result of this design, micro-kernels have traditionally been slower than monolithic kernels due to the ipc overhead.
16.1.2 system calls demystified system calls use an instruction that can be run by a program operating in userspace that traps to the kernel (by use of a signal) to complete the call. this includes actions such as writing data to disk, interacting directly with hardware in general or operations related to gaining or relinquishing privileges (e.g. becoming the root user and gaining all capabilities).
"in order to fulfill a user’s request, the kernel will rely on kernel calls. kernel calls are essentially the ""public"" functions of the kernel - functions implemented by other developers for use in other parts of the kernel. here is a snippet for a kernel call man page:"
gfp_t flags the type of memory to allocate.
description kmalloc is the normal method of allocating memory for objects smaller than page size in the kernel.
gfp_kernel - allocate normal kernel ram. may sleep.
a container is almost like a virtual machine. in some senses, containers are to virtual machines as threads are to processes. a container is a lightweight environment that shares resources and a kernel with a host machine, while isolating itself from other containers or processes on the host. you may have encountered containers while working with technologies such as docker, perhaps the most well-known implementation of containers out there.
it is ready for your next command! you can type in a lot of unix utilities like ls, echo hello and the shell will execute them and give you the result. some of these are what are known as shell-builtins meaning that the code is in the shell program itself. some of these are compiled programs that you run. the shell only looks through a special variable called path which contains a list of colon separated paths to search for an executable with your name, here is an example path.
17.1.1 shell tricks and tips  the up arrow will get you your most recent command  ctrl-r will search commands that you previously ran  ctrl-c will interrupt your shell’s process  !! will execute the last command  !<num> goes back that many commands and runs that  !<prefix> runs the last command that has that prefix  !$ is the last arg of the previous command  !* is all args of the previous command  p̂atŝub takes the last command and substitutes the pattern pat for the substitution sub  cd - goes to the previous directory  pushd <dir> pushes the current directory on a stack and cds  popd cds to the directory at the top of the stack
a terminal is an application that displays the output from the shell. you can have your default terminal, a quake based terminal, terminator, the options are endless!
2. diff tells you the difference between the two files. if nothing is printed, then zero is returned meaning the files are the same byte for byte. otherwise, the longest common subsequence difference is printed
3. grep tells you which lines in a file or standard input match a posix pattern.
4. ls tells you which files are in the current directory.
7. make executes programs according to a makefile.
17.1.4 syntactic shells have many useful utilities like saving some output to a file using redirection >. this overwrites the file from the beginning. if you only meant to append to the file, you can use ». unix also allows file descriptor swapping.
this means that you can take the output going to one file descriptor and make it seem like it’s coming out of another. the most common one is 2>1 which means take the stderr and make it seem like it is coming out of standard out. this is important because when you use > and » they only write the standard output of the file.
the pipe operator has a fascinating history. the unix philosophy is writing small programs and chaining them together to do new and interesting things. back in the early days, hard disk space was limited and write times were slow. brian kernighan wanted to maintain the philosophy while omitting intermediate files that take up hard drive space. so, the unix pipe was born. a pipe takes the stdout of the program on its left and feeds it to the stdin of the program on its write. consider the command tee. it can be used as a replacement for the redirection operators because tee will both write to a file and output to standard out. it also has the added benefit that it doesn’t need to be the last command in the list. meaning, that you can write an intermediate result and continue your piping.
the and || operator are operators that execute a command sequentially. only executes a command if the previous command succeeds, and || always executes the next command.
17.2 stack smashing each thread uses a stack memory. the stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. stack memory includes non-static automatic (temporary) variables, parameter values, and the return address. if a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. the precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent.
2. parsing: the compiler parses the text file for function declarations, variable declarations, etc.
4. assembling: the assembler turns the assembly into 0s and 1s and creates an object file. this object file maps names to pieces of code.
5. static linking: the linker then takes a series of objects and static libraries and resolves references of variables and functions from one object file to another. the linker then finds the main method and makes that the entry point for the function. the linker also notices when a function is meant to be dynamically linked. the compiler also creates a section in the executable that tells the operating system that these functions need addresses right before running.
6. dynamic linking: as the program is getting ready to be executed, the operating system looks at what libraries that the program needs and links those functions to the dynamic library.
further classes will teach you about parsing and assembly – preprocessing is an extension of parsing. most classes won’t teach you about the two different types of linking though. static linking a library is similar to combining object files. to create a static library, a compiler combines different object files to create one executable.
a static library is literally is an archive of object files. these libraries are useful when you want your executable to be secure, you know all the code that is being included into your executable, and portable, all the code is bundled with your executable meaning no additional installs.
the other type is a dynamic library. typically, dynamic libraries are installed user-wide or system-wide and are accessible by most programs. dynamic libraries’ functions are filled in right before they are run. there are a number of benefits to this.
what this means is that if we don’t follow posix to the letter when using two file descriptors that refer to the same description across processes, we get undefined behavior. to be technical, the file descriptor must have a “position” meaning that it needs to have a beginning and an end like a file, not like an arbitrary stream of bytes. posix then goes on to introduce the idea of an active handle, where a handle may be a file descriptor or a file* pointer. file handles don’t have a flag called “active”. an active file descriptor is one that is currently being used for reading and writing and other operations (such as exit). the standard says that before a fork that the application or your code must execute a series of steps to prepare the state of the file. in simplified terms, the descriptor needs to be closed, flushed, or read to its entirety – the gory details are explained later.
"why does this work? well at the start we are in a safe state – defined by we have enough money to suit at least one person. each of these ""loans"" results in a safe state. if we have exhausted our reserve, one person is working and will give us money greater than or equal to our previous ""loan"", thus putting us in a safe state again."
17.5 clean/dirty forks (chandy/misra solution) there are many more advanced solutions. one such solution is by chandy and misra [? ]. this is not a true solution to the dining philosophers problem because it has the requirement that philosophers can speak to each other. it is a solution that ensures fairness for some notion of fairness. in essence, it defines a series of rounds that a philosopher must eat in a given round before going to the next one.
17.6 actor model the actor model is another form of synchronization that doesn’t have to do anything with negotiating locks or waiting. the idea is simple. each actor can either perform work, create more actors, send messages, or respond to messages. any time an actor needs something from another actor, it sends a message. most importantly, an actor is only responsible for one thing. if we were implementing a real-world application, we may have an actor that handles the database, one that handles the incoming connections, one that services the connections, etc. these actors would pass messages to each other like “there is a new connection” from the incoming connection actor to the servicing actor. the servicing actor may send a data request message to the database actor and a data response message comes back.
 static scheduling breaks up the problems into fixed-size chunks (predetermined) and have each thread work on each of the chunks. this works well when each of the subproblems takes roughly the same time because there is no additional overhead. all you need to do is write a loop and give the map function to each sub-array.
 dynamic scheduling as a new problem becomes available to have a thread serve it. this is useful when you don’t know how long the scheduling will take  guided scheduling this is a mix of the above with a mix of the benefits and tradeoffs. you start with static scheduling and move slowly to dynamic if needed  runtime scheduling you have absolutely no idea how long the problems are going to take. instead of deciding it yourself, let the program decide what to do!
no need to memorize any of the scheduling routines though. openmp is a standard that is an alternative to pthreads. for example, here is how to parallelize a for loop
 data integrity. file systems use journaling and sometimes checksums to ensure that the data written to is valid. journalling is a simple invention where the file system writes an operation in a journal. if the filesystem crashes before the operation is complete, it can resume the operation when booted up again using the partial journal.
 efficient backups. many of us have data that we can’t store on the cloud for one reason or another. it is useful that when a filesystems is either being used as a backup medium or is the source to the backup that it is able to calculate what has changed efficiently, compress files, and sync between the external drive.
storemi is a hardware microcontroller that analyzes how the operating system accesses files and moves files/blocks around to speed up the load time. a common usage can be imagined as having a fast, but small capacity ssd and a slower, large capcity hdd. to make it seem like all the files are on an ssd, the storemi matches the pattern of file access. if you are starting up windows, windows will often access many files in the same order.
 nice values are the kernel’s way of giving priority to certain processes, the lower nice value the higher priority.
but there is a happy ending. modern hardware addresses these issues using ‘memory fences’ also known as a memory barrier. this prevents instructions from getting ordered before or after the barrier. there is a performance loss, but it is needed for correct programs!
what is the note about (dynamic)? in the pthread man pages, wait creates a runtime binding to a mutex. this means that after the first call is called, a mutex is associated with a condition variable while there is still a thread waiting on that condition variable. each new thread coming in must have the same mutex, and it must be locked.
5. broadcast iterates through and frees all of the threads.
17.14.1 sequentially consistent sequentially consistent is the simplest, least error-prone and most expensive model. this model says that any change that happens, all changes before it will be synchronized between all threads.
17.14.2 relaxed relaxed is a simple memory order providing for more optimizations. this means that only a particular operation needs to be atomic. one can have stale reads and writes, but after reading the new value, it won’t become old.
17.15 actor model and goroutines there are a lot of other methods of concurrency than described in this book. posix threads are the finest grained thread construct, allowing for tight control of the threads and the cpu. other languages have their abstractions.
this actually creates what is known as a goroutine. a goroutine can be thought of as a lightweight thread.
main actor that will be performing the main instruction set. the other actor will be the counter. the counter is responsible for adding numbers to an internal variable. we’ll send messages between the threads when we want to add and see the value.
17.16 scheduling conceptually this section could be useful for those that like to analyze these algorithms mathematically if your co-worker asked you what scheduling algorithm to use, you may not have the tools to analyze each algorithm. so, let’s think about scheduling algorithms at a high level and break them down by their times. we will be evaluating this in the context of a random process timing, meaning that each process takes a random but finite amount of time to finish.
1. queueing theory involves a random variable controlling the interarrival time – or the time between two different processes arriving. we won’t name this random variable, but we will assume that (1) it has a mean of λ and (2) it is distributed as a poisson random variable. this means the probability of getting a process exp(−λ) t units after getting another process is λ t ∗ t! where t! can be approximated by the gamma function when dealing with real values.
2. we will be denoting the service time s, and deriving the waiting time w , and the response time r; more specifically the expected values of all of those variables e[s] deriving turnaround time is simply s + w . for clarity, we will introduce another variable n that is the number of people currently in the queue. a famous result in queueing theory is little’s law which states e[n ] = λe[w ] meaning that the number of people waiting is the arrival rate times the expected waiting time (assuming the queue is in a steady state).
5. we will assume that there is one processor. this is known as an m/g/1 queue in queueing theory.
2. next is the expected response time e[r] = e[n ] ∗ e[s] = λ ∗ e[w ] ∗ e[s] the response time is simple to calculate, it is the expected number of people ahead of the process in the queue times the expected time to service each of those processes. from little’s law above, we can substitute that for this. since we already know the value of the waiting time, we can reason about the response time as well.
interestingly, we don’t have to worry about the convoy effect or any new processes coming in. the total wait time remains bounded by the number of people in the queue. for those of you familiar with tail inequalities since processes arrive according to a poisson distribution, the probability that we’ll get many processes drops off exponentially due to chernoff bounds (all arrivals are independent of other arrivals). meaning roughly we can assume low variance on the number of processes. as long as the service time is reasonable on average, the wait time will be too.
which says that the scheduler needs to wait for all jobs with a higher priority and the same to go before a process can go. imagine a series of fcfs queues that a process needs to wait your turn. using little’s law for different colored jobs and the formula above we can simplify this
3. as for a comparison with fcfs in the average case, it usually does better assuming that we have a smooth probability distribution – i.e. the probability of getting any particular priority is zero. in all of our formulas, we still have some probability mass to put on lower priority processes, bringing the expectation down. this statement doesn’t hold for all smooth distributions but for most real-world smoothed distributions (which tend to be smooth) they do.
4. this isn’t even to mention the idea of utility. utility means that if we gain an amount of happiness by having certain jobs finish, priority and preemptive priority maximize that while balancing out other measures of efficiency.
we incur the same cost on response time and then we have to suffer an additional cost based on what the probabilities are of lower priority jobs coming in and taking this job out. that is what we call the average interruption time. this follows the same laws as before. since we have a variadic, pyramid summation if we have a lot of jobs with small service times then the wait time goes down for both additive pieces. it can be analytically shown that this is better given certain probability distributions. for example, try with the uniform versus fcfs or the non preemptive version. what happens? as always the proof is left to the reader.
4. the next two are identification number. ip handles taking packets that are too big to be sent over the physical wire and chunks them up. as such, this number identifies what datagram this originally belonged to.
"7. the next octet is time to live. so this is the number of ""hops"" (travels over a wire) a packet is allowed to go. this is set because different routing protocols could cause packets to go in circles, the packets must be dropped at some point."
9. the next two octets is an internet checksum. this is a crc that is calculated to make sure that a wide variety of bit errors are detected.
10. the source address is what people generally refer to as the ip address. there is no verification of this, so one host can pretend to be any ip address possible 11. the destination address is where you want the packet to be sent to. destinations are crucial to the routing process.
13. footer: a bit of padding to make sure your data is a multiple of 4 octets.
"17.17.2 routing the internet protocol routing is an amazing intersection of theory and application. we can imagine the entire internet as a set of graphs. most peers are connected to what we call ""peering points"" – these are the wifi routers and ethernet ports that one finds at home, at work, and in public. these peering points are then connected to a wired network of routers, switches, and servers that all route themselves. at a high level there are two types of routing 1. internal routing protocols. internal protocols are routing designed for within an isp’s network. these protocols are meant to be fast and more trusting because all computers, switches, and routers are part of an isp. communication between two routers."
"2. external routing protocols. these typically happen to be isp to isp protocol. certain routers are designated as border routers. these routers talk to routers from isps who have different policies from accepting or receiving packets. if an evil isp is trying to dump all network traffic onto your isp, these routers would deal with that. these protocols also deal with gathering information about the outside world to each router. in most routing protocols using link state or ospf, a router must necessarily calculate the shortest path to the destination. this means it needs information about the ""foreign"" routers which is disseminated according to these protocols."
17.17.4 ip multicast a little known feature is that using the ip protocol one can send a datagram to all devices connected to a router in what is called a multicast. multicasts can also be configured with groups, so one can efficiently slice up all connected routers and send a piece of information to all of them efficiently. to access this in a higher protocol, you need to use udp and specify a few more options. note that this will cause undue stress on the network, so a series of multicasts could flood the network fast.
openbsd and freebsd have an arguably better model of asynchronous io from the kqueue model. kqueue is a system call that is exclusive the bsds and macos. it allows you to modify file descriptor events and read file descriptors all in a single call under a unified interface. so what are the benefits?
3. the unified system call for all types. kqueue is the truest sense of underlying descriptor agnostic. one can add files, sockets, pipes to it and get full or near full performance. you can add the same to epoll, but linux’s whole ecosystem with async file input-output has been messed up with aio, meaning that since there is no unified interface, you run into weird edge cases.
feature test macro requirements for glibc (see feature_test_macros(7)): reallocarray(): _gnu_source description the malloc() function allocates size bytes and returns a pointer to the allocated memory. the memory is not initialized.
the free() function frees the memory space pointed to by ptr, which must have been returned by a previous call to malloc(),
the calloc() function allocates memory for an array of nmemb elements of size bytes each and returns a pointer to the allocated memory. the memory is set to zero. if nmemb or size is 0, then calloc() returns either null, or a unique pointer value that can later be successfully passed to free().
the realloc() function changes the size of the memory block pointed to by ptr to size bytes. the contents will be unchanged in the range from the start of the region up to the minimum of the old and new sizes. if the new size is larger than the old size, the added memory will not be initialized. if ptr is null, then the call is equivalent to malloc(size), for all values of size; if size is equal to zero, and ptr is not null, then the call is equivalent to free(ptr). unless ptr is null, it must have been returned by an earlier call to malloc(), calloc(), or realloc(). if the area pointed to was moved, a free(ptr) is done.
the reallocarray() function changes the size of the memory block pointed to by ptr to be large enough for an array of nmemb elements, each of which is size bytes. it is equivalent to the call realloc(ptr, nmemb * size); however, unlike that realloc() call, reallocarray() fails safely in the case where the multiplication would overflow.
the realloc() function returns a pointer to the newly allocated memory, which is suitably aligned for any built-in type and may be different from ptr, or null if the request fails. if size was equal to 0, either null or a pointer suitable to be passed to free() is returned. if realloc() fails, the original block is left untouched; it is not freed or moved.
on success, the reallocarray() function returns a pointer to the newly allocated memory. on failure, it returns null and the original block of memory is left untouched.
notes by default, linux follows an optimistic memory allocation strategy. this means that when malloc() returns non-null there is no guarantee that the memory is available. in case it turns out that the system is out of memory, one or more processes will be killed by the oom killer. for more information, see the description of /proc/sys/vm/overcommit_memory and /proc/sys/vm/oom_adj in proc(5), and the linux kernel source file documentation/vm/overcommit-accounting.
normally, malloc() allocates memory from the heap, and adjusts the size of the heap as required, using sbrk(2). when allocating blocks of memory larger than mmap_threshold bytes, the glibc malloc() implementation allocates the memory as a private anonymous mapping using mmap(2). mmap_threshold is 128 kb by default, but is adjustable using mallopt(3). prior to linux 4.7 allocations performed using mmap(2) were unaffected by the rlimit_data resource limit; since linux 4.7, this limit is also enforced for allocations performed using mmap(2).
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.
18.2 heartbleed required: intro to c to put it simply, there were no bounds on buffer checking. the ssl heartbeat is super simple. a server sends a string of a certain length, and the second server is supposed to send the string of the length back. the problem is someone can maliciously change the size of the request to larger than what they sent (i.e. send “cat” but request 500 bytes) and get crucial information like passwords from the server. there is a relevant xkcd on it.
18.3 dirty cow required: processes/virtual memory dirty cow a process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. dirty cow is a vulnerability where a bunch of threads attempts to access the same piece of memory at the same time hoping that one of the threads flips the nx bit and the writable bit. after that, an attacker can modify the page. this can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell.
18.6 mars pathfinder required sections: synchronization and a bit of scheduling pathfinder link the mars pathfinder was a mission that tried to collect climate data on mars. the finder uses a single bus to communicate with different parts. since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. the architecture was pretty simple. there was a thread that controlled data along the information bus, communications thread,
18.8 year 2038 required sections: intro to c 2038 this is issue that hasn’t happened yet. unix timestamps are kept as the number of seconds from a particular day (jan 1st 1970). this is stored as a 32 bit signed integer. in march of 2038, this number will overflow. this isn’t a problem for most modern operating system who store 64 bit signed integers which is enough to keep us going until the end of time, but it is a problem for embedded devices that we can’t change the internal hardware to. stay tuned to see what happens.
18.9 northeast blackout of 2003 required sections: synchronization 2003 a race condition triggered a series of undefined events in a system that caused the blackout of most of the northeastern part of north america for quite some time. this bug also turned off or caused the backup system and logging systems to fail so people didn’t even know of the bug for an hour. the exact bits that were flipped are unknown, but patches have been put into place.
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.
18.15 appnexus double free required sections: intro to c/malloc double free appnexus uses an asynchronous garbage collector that reclaims different parts of the heap when it believes that objects are unused. the architecture is that an element is in the unavailable list and then it is taken out to a to-be-freed list. after a certain time if that element was unused, it is freed and added to the free list. this is fine until two thread try to delete the same object at once, adding to the list twice. after less time, one of the objects was deleted, the delete was announced to other computers.
