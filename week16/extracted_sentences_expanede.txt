sentence	source	start
2 background	background	2
2.1 systems architecture	system	4
this section is a short review of system architecture topics that you’ll need for system programming.	this section	0
this section is a short review of system architecture topics that you’ll need for system programming.	system	34
this section is a short review of system architecture topics that you’ll need for system programming.	section	5
what is assembly? assembly is the lowest that you’ll get to machine language without writing 1’s and 0’s. each computer has an architecture, and that architecture has an associated assembly language. each assembly command has a 1:1 mapping to a set of 1’s and 0’s that tell the computer exactly what to do. for example, the following in the widely used x86 assembly language add one to the memory address 20 [13] – you can also look in [8] section 2a under the add instruction though it is more verbose.	the following	320
what is assembly? assembly is the lowest that you’ll get to machine language without writing 1’s and 0’s. each computer has an architecture, and that architecture has an associated assembly language. each assembly command has a 1:1 mapping to a set of 1’s and 0’s that tell the computer exactly what to do. for example, the following in the widely used x86 assembly language add one to the memory address 20 [13] – you can also look in [8] section 2a under the add instruction though it is more verbose.	address	397
what is assembly? assembly is the lowest that you’ll get to machine language without writing 1’s and 0’s. each computer has an architecture, and that architecture has an associated assembly language. each assembly command has a 1:1 mapping to a set of 1’s and 0’s that tell the computer exactly what to do. for example, the following in the widely used x86 assembly language add one to the memory address 20 [13] – you can also look in [8] section 2a under the add instruction though it is more verbose.	memory	390
what is assembly? assembly is the lowest that you’ll get to machine language without writing 1’s and 0’s. each computer has an architecture, and that architecture has an associated assembly language. each assembly command has a 1:1 mapping to a set of 1’s and 0’s that tell the computer exactly what to do. for example, the following in the widely used x86 assembly language add one to the memory address 20 [13] – you can also look in [8] section 2a under the add instruction though it is more verbose.	section	440
why do we mention this? because it is important that although you are going to be doing most of this class in c, that this is what the code is translated into. serious implications arise for race conditions and atomic operations.	code	135
an operation is atomic if no other processor should interrupt it. take for example the above assembly code to add one to a register. in the architecture, it may actually have a few different steps on the circuit. the operation may start by fetching the value of the memory from the stick of ram, then storing it in the cache or a register, and then finally writing back [12] – under the description for fetch-and-add though your micro-architecture may 7	code	102
an operation is atomic if no other processor should interrupt it. take for example the above assembly code to add one to a register. in the architecture, it may actually have a few different steps on the circuit. the operation may start by fetching the value of the memory from the stick of ram, then storing it in the cache or a register, and then finally writing back [12] – under the description for fetch-and-add though your micro-architecture may 7	memory	266
vary. or depending on performance operations, it may keep that value in cache or in a register which is local to that process – try dumping the -o2 optimized assembly of incrementing a variable. the problem comes in if two processors try to do it at the same time. the two processors could at the same time copy the value of the memory address, add one, and store the same result back, resulting in the value only being incremented once. that is why we have a special set of instructions on modern systems called atomic operations. if an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. with x86 this is done by the lock prefix [8, p. 1120].	thread	602
vary. or depending on performance operations, it may keep that value in cache or in a register which is local to that process – try dumping the -o2 optimized assembly of incrementing a variable. the problem comes in if two processors try to do it at the same time. the two processors could at the same time copy the value of the memory address, add one, and store the same result back, resulting in the value only being incremented once. that is why we have a special set of instructions on modern systems called atomic operations. if an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. with x86 this is done by the lock prefix [8, p. 1120].	address	336
vary. or depending on performance operations, it may keep that value in cache or in a register which is local to that process – try dumping the -o2 optimized assembly of incrementing a variable. the problem comes in if two processors try to do it at the same time. the two processors could at the same time copy the value of the memory address, add one, and store the same result back, resulting in the value only being incremented once. that is why we have a special set of instructions on modern systems called atomic operations. if an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. with x86 this is done by the lock prefix [8, p. 1120].	memory	329
vary. or depending on performance operations, it may keep that value in cache or in a register which is local to that process – try dumping the -o2 optimized assembly of incrementing a variable. the problem comes in if two processors try to do it at the same time. the two processors could at the same time copy the value of the memory address, add one, and store the same result back, resulting in the value only being incremented once. that is why we have a special set of instructions on modern systems called atomic operations. if an instruction is atomic, it makes sure that only one processor or thread performs any intermediate step at a time. with x86 this is done by the lock prefix [8, p. 1120].	system	498
if a particular address is already in the cache when reading or writing, the processor will perform the operation on the cache such as adding and update the actual memory later because updating memory is slow [9, section 3.4]. if it isn’t, the processor requests a chunk of memory from the memory chip and stores it in the cache, kicking out the least recently used page – this depends on caching policy, but intel’s does use this. this is done because the l3 processor cache is roughly three times faster to reach than the memory in terms of time [11, p. 22] though exact speeds will vary based on the clock speed and architecture. naturally, this leads to problems because there are two different copies of the same value, in the cited paper this refers to an unshared line. this isn’t a class about caching, know how this could impact your code. a short but non-complete list could be 1. race conditions! if a value is stored in two different processor caches, then that value should be accessed by a single thread.	thread	1011
if a particular address is already in the cache when reading or writing, the processor will perform the operation on the cache such as adding and update the actual memory later because updating memory is slow [9, section 3.4]. if it isn’t, the processor requests a chunk of memory from the memory chip and stores it in the cache, kicking out the least recently used page – this depends on caching policy, but intel’s does use this. this is done because the l3 processor cache is roughly three times faster to reach than the memory in terms of time [11, p. 22] though exact speeds will vary based on the clock speed and architecture. naturally, this leads to problems because there are two different copies of the same value, in the cited paper this refers to an unshared line. this isn’t a class about caching, know how this could impact your code. a short but non-complete list could be 1. race conditions! if a value is stored in two different processor caches, then that value should be accessed by a single thread.	code	843
if a particular address is already in the cache when reading or writing, the processor will perform the operation on the cache such as adding and update the actual memory later because updating memory is slow [9, section 3.4]. if it isn’t, the processor requests a chunk of memory from the memory chip and stores it in the cache, kicking out the least recently used page – this depends on caching policy, but intel’s does use this. this is done because the l3 processor cache is roughly three times faster to reach than the memory in terms of time [11, p. 22] though exact speeds will vary based on the clock speed and architecture. naturally, this leads to problems because there are two different copies of the same value, in the cited paper this refers to an unshared line. this isn’t a class about caching, know how this could impact your code. a short but non-complete list could be 1. race conditions! if a value is stored in two different processor caches, then that value should be accessed by a single thread.	address	16
if a particular address is already in the cache when reading or writing, the processor will perform the operation on the cache such as adding and update the actual memory later because updating memory is slow [9, section 3.4]. if it isn’t, the processor requests a chunk of memory from the memory chip and stores it in the cache, kicking out the least recently used page – this depends on caching policy, but intel’s does use this. this is done because the l3 processor cache is roughly three times faster to reach than the memory in terms of time [11, p. 22] though exact speeds will vary based on the clock speed and architecture. naturally, this leads to problems because there are two different copies of the same value, in the cited paper this refers to an unshared line. this isn’t a class about caching, know how this could impact your code. a short but non-complete list could be 1. race conditions! if a value is stored in two different processor caches, then that value should be accessed by a single thread.	memory	164
if a particular address is already in the cache when reading or writing, the processor will perform the operation on the cache such as adding and update the actual memory later because updating memory is slow [9, section 3.4]. if it isn’t, the processor requests a chunk of memory from the memory chip and stores it in the cache, kicking out the least recently used page – this depends on caching policy, but intel’s does use this. this is done because the l3 processor cache is roughly three times faster to reach than the memory in terms of time [11, p. 22] though exact speeds will vary based on the clock speed and architecture. naturally, this leads to problems because there are two different copies of the same value, in the cited paper this refers to an unshared line. this isn’t a class about caching, know how this could impact your code. a short but non-complete list could be 1. race conditions! if a value is stored in two different processor caches, then that value should be accessed by a single thread.	section	213
2. speed. with a cache, your program may look faster mysteriously. just assume that reads and writes that either happened recently or are next to each other in memory are fast.	memory	160
interrupts are a important part of system programming. an interrupt is internally an electrical signal that is delivered to the processor when something happens – this is a hardware interrupt [3]. then the hardware decides if this is something that it should handle (i.e. handling keyboard or mouse input for older keyboard and mouses) or it should pass to the operating system. the operating system then decides if this is something that it should handle (i.e. paging a memory table from disk) or something the application should handle (i.e. a segfault). if the operating system decides that this is something that the process or program should take care of, it sends a software fault and that software fault is then propagated. the application then decides if it is an error	memory	471
interrupts are a important part of system programming. an interrupt is internally an electrical signal that is delivered to the processor when something happens – this is a hardware interrupt [3]. then the hardware decides if this is something that it should handle (i.e. handling keyboard or mouse input for older keyboard and mouses) or it should pass to the operating system. the operating system then decides if this is something that it should handle (i.e. paging a memory table from disk) or something the application should handle (i.e. a segfault). if the operating system decides that this is something that the process or program should take care of, it sends a software fault and that software fault is then propagated. the application then decides if it is an error	system	35
(segfault) or not (sigpipe for example) and reports to the user. applications can also send signals to the kernel and to the hardware as well. this is an oversimplification because there are certain hardware faults that can’t be ignored or masked away, but this class isn’t about teaching you to build an operating system.	system	315
an important application of this is this is how system calls are served! there is a well-established set of registers that the arguments go in according to the kernel as well as a system call “number” again defined by the kernel. then the operating system triggers an interrupt which the kernel catches and serves the system call [7].	system	48
an important application of this is this is how system calls are served! there is a well-established set of registers that the arguments go in according to the kernel as well as a system call “number” again defined by the kernel. then the operating system triggers an interrupt which the kernel catches and serves the system call [7].	a system call	178
operating system developers and instruction set developers alike didn’t like the overhead of causing an interrupt on a system call. now, systems use sysenter and sysexit which has a cleaner way of transferring control safely to the kernel and safely back. what safely means is obvious out of the scope for this class, but it persists.	system	10
operating system developers and instruction set developers alike didn’t like the overhead of causing an interrupt on a system call. now, systems use sysenter and sysexit which has a cleaner way of transferring control safely to the kernel and safely back. what safely means is obvious out of the scope for this class, but it persists.	a system call	117
2.1.5 optional: hyperthreading	thread	21
2.1.5 optional: hyperthreading	hyperthreading	16
hyperthreading is a new technology and is in no way shape or form multithreading. hyperthreading allows one physical core to appear as many virtual cores to the operating system [8, p.51]. the operating system can then schedule processes on these virtual cores and one core will execute them. each core interleaves processes or threads. while the core is waiting for one memory access to complete, it may perform a few instructions of another process thread. the overall result is more instructions executed in a shorter time. this potentially means that you can divide the number of cores you need to power smaller devices.	thread	5
hyperthreading is a new technology and is in no way shape or form multithreading. hyperthreading allows one physical core to appear as many virtual cores to the operating system [8, p.51]. the operating system can then schedule processes on these virtual cores and one core will execute them. each core interleaves processes or threads. while the core is waiting for one memory access to complete, it may perform a few instructions of another process thread. the overall result is more instructions executed in a shorter time. this potentially means that you can divide the number of cores you need to power smaller devices.	memory	371
hyperthreading is a new technology and is in no way shape or form multithreading. hyperthreading allows one physical core to appear as many virtual cores to the operating system [8, p.51]. the operating system can then schedule processes on these virtual cores and one core will execute them. each core interleaves processes or threads. while the core is waiting for one memory access to complete, it may perform a few instructions of another process thread. the overall result is more instructions executed in a shorter time. this potentially means that you can divide the number of cores you need to power smaller devices.	system	171
hyperthreading is a new technology and is in no way shape or form multithreading. hyperthreading allows one physical core to appear as many virtual cores to the operating system [8, p.51]. the operating system can then schedule processes on these virtual cores and one core will execute them. each core interleaves processes or threads. while the core is waiting for one memory access to complete, it may perform a few instructions of another process thread. the overall result is more instructions executed in a shorter time. this potentially means that you can divide the number of cores you need to power smaller devices.	hyperthreading	0
there be dragons here though. with hyperthreading, you must be wary of optimizations. a famous hyperthreading bug that caused programs to crash if at least two processes were scheduled on a physical core, using specific registers, in a tight loop. the actual problem is better explained through an architecture lens. but, the actual application was found through systems programmers working on ocaml’s mainline [10].	thread	40
there be dragons here though. with hyperthreading, you must be wary of optimizations. a famous hyperthreading bug that caused programs to crash if at least two processes were scheduled on a physical core, using specific registers, in a tight loop. the actual problem is better explained through an architecture lens. but, the actual application was found through systems programmers working on ocaml’s mainline [10].	system	363
there be dragons here though. with hyperthreading, you must be wary of optimizations. a famous hyperthreading bug that caused programs to crash if at least two processes were scheduled on a physical core, using specific registers, in a tight loop. the actual problem is better explained through an architecture lens. but, the actual application was found through systems programmers working on ocaml’s mainline [10].	optimizations	71
there be dragons here though. with hyperthreading, you must be wary of optimizations. a famous hyperthreading bug that caused programs to crash if at least two processes were scheduled on a physical core, using specific registers, in a tight loop. the actual problem is better explained through an architecture lens. but, the actual application was found through systems programmers working on ocaml’s mainline [10].	hyperthreading	35
ssh is short for the secure shell [2]. it is a network protocol that allows you to spawn a shell on a remote machine.	a shell	89
if you don’t want to type your password out every time, you can generate an ssh key that uniquely identifies your machine. if you already have a key pair, you can skip to the copy id stage.	type	21
ssh-keygen -t rsa -b 4096 do whatever keygen tells you don’t feel like you need a passcode if your login password is secure ssh-copy-id netid@sem-cs241-vm.cs.illinois.edu enter your password for maybe the final time ssh netid@sem-cs241-vm.cs.illinois.edu	code	86
if you still think that that is too much typing, you can always alias hosts. you may need to restart your vm or reload sshd for this to take effect. the config file is available on linux and mac distros. for windows, you’ll have to use the windows linux subsystem or configure any aliases in putty	system	257
what is ‘git‘? git is a version control system. what that means is git stores the entire history of a directory. we refer to the directory as a repository. so what do you need to know is a few things. first, create your repository with the repo creator. if you haven’t already signed into enterprise github, make sure to do so otherwise your repository won’t be created for you. after that, that means your repository is created on the server. git is a decentralized version control system, meaning that you’ll need to get a repository onto your vm. we can do this with a clone. whatever you do, do not go through the readme.md tutorial.	system	40
vim is a text editor and a unix-like utility. you enter vim by typing vim [file], which takes you into the editor. there are three most commonly used modes: normal mode, insert mode, and command mode. you start off in normal mode. in this mode, you can move around with many keys with the most common ones being hjkl (corresponding to left, down, up, and right respectively). to run commands in vim, you can first type : and then a command after it. for instance, to quit vim, simply type :q (q stands for quit). if you have any unsaved edits, you must either save them :w, save and quit :wq, or quit and discard changes :q!. to make edits you can either type i to change you into insert mode or a to change to insert mode after the cursor. this is the basics when it comes to vim. in addition to the countless great resources out there on the internet, vim also has its own built-in tutorials set up for beginners. to access the interactive tutorial, enter vimtutor in the command line (not inside of vim), and you are all set!	resources	817
vim is a text editor and a unix-like utility. you enter vim by typing vim [file], which takes you into the editor. there are three most commonly used modes: normal mode, insert mode, and command mode. you start off in normal mode. in this mode, you can move around with many keys with the most common ones being hjkl (corresponding to left, down, up, and right respectively). to run commands in vim, you can first type : and then a command after it. for instance, to quit vim, simply type :q (q stands for quit). if you have any unsaved edits, you must either save them :w, save and quit :wq, or quit and discard changes :q!. to make edits you can either type i to change you into insert mode or a to change to insert mode after the cursor. this is the basics when it comes to vim. in addition to the countless great resources out there on the internet, vim also has its own built-in tutorials set up for beginners. to access the interactive tutorial, enter vimtutor in the command line (not inside of vim), and you are all set!	type	414
2.2.4 clean code	code	12
make your code modular using helper functions. if there is a repeated task (getting the pointers to contiguous blocks in the malloc mp, for example), make them helper functions. and make sure each function does one thing well so that you don’t have to debug twice. let’s say that we are doing selection sort by finding the minimum element each iteration like so,	code	10
make your code modular using helper functions. if there is a repeated task (getting the pointers to contiguous blocks in the malloc mp, for example), make them helper functions. and make sure each function does one thing well so that you don’t have to debug twice. let’s say that we are doing selection sort by finding the minimum element each iteration like so,	block	111
make your code modular using helper functions. if there is a repeated task (getting the pointers to contiguous blocks in the malloc mp, for example), make them helper functions. and make sure each function does one thing well so that you don’t have to debug twice. let’s say that we are doing selection sort by finding the minimum element each iteration like so,	pointer	88
many can see the bug in the code, but it can help to refactor the above method into	code	28
and the error is specifically in one function. in the end, this class is about writing system programs, not a class about refactoring/debugging your code. in fact, most kernel code is so atrocious that you don’t want to read it – the defense there is that it needs to be. but for the sake of debugging, it may benefit you in the long run to adopt some of these practices.	code	149
and the error is specifically in one function. in the end, this class is about writing system programs, not a class about refactoring/debugging your code. in fact, most kernel code is so atrocious that you don’t want to read it – the defense there is that it needs to be. but for the sake of debugging, it may benefit you in the long run to adopt some of these practices.	system	87
use assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. for example, if your data structure is a doubly-linked list, you can do something like assert(node == node->next->prev) to assert that the next node has a pointer to the current node. you can also check the pointer is pointing to an expected range of memory address, non-null, ->size is reasonable, etc.	a struct	148
use assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. for example, if your data structure is a doubly-linked list, you can do something like assert(node == node->next->prev) to assert that the next node has a pointer to the current node. you can also check the pointer is pointing to an expected range of memory address, non-null, ->size is reasonable, etc.	code	33
use assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. for example, if your data structure is a doubly-linked list, you can do something like assert(node == node->next->prev) to assert that the next node has a pointer to the current node. you can also check the pointer is pointing to an expected range of memory address, non-null, ->size is reasonable, etc.	address	382
use assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. for example, if your data structure is a doubly-linked list, you can do something like assert(node == node->next->prev) to assert that the next node has a pointer to the current node. you can also check the pointer is pointing to an expected range of memory address, non-null, ->size is reasonable, etc.	memory	375
use assertions to make sure your code works up to a certain point – and importantly, to make sure you don’t break it later. for example, if your data structure is a doubly-linked list, you can do something like assert(node == node->next->prev) to assert that the next node has a pointer to the current node. you can also check the pointer is pointing to an expected range of memory address, non-null, ->size is reasonable, etc.	pointer	279
here is a quick example with an assert. let’s say that we are writing code using memcpy. we would want to put an assert before that checks whether my two memory regions overlap. if they do overlap, memcpy runs into undefined behavior, so we want to catch that problem than later.	code	70
here is a quick example with an assert. let’s say that we are writing code using memcpy. we would want to put an assert before that checks whether my two memory regions overlap. if they do overlap, memcpy runs into undefined behavior, so we want to catch that problem than later.	memory	154
2.3 valgrind	valgrind	4
valgrind is a suite of tools designed to provide debugging and profiling tools to make your programs more correct and detect some runtime issues [4]. the most used of these tools is memcheck, which can detect many memory-related errors that are common in c and c++ programs and that can lead to crashes and unpredictable behavior (for example, unfreed memory buffers). to run valgrind on your program:	memory	214
valgrind is a suite of tools designed to provide debugging and profiling tools to make your programs more correct and detect some runtime issues [4]. the most used of these tools is memcheck, which can detect many memory-related errors that are common in c and c++ programs and that can lead to crashes and unpredictable behavior (for example, unfreed memory buffers). to run valgrind on your program:	valgrind	0
valgrind --leak-check=full --show-leak-kinds=all myprogram arg1 arg2	valgrind	0
this program compiles and runs with no errors. let’s see what valgrind will output.	valgrind	62
==29515== memcheck, a memory error detector ==29515== copyright (c) 2002-2015, and gnu gpl’d, by julian seward et al.	memory	22
==29515== using valgrind-3.11.0 and libvex; rerun with -h for copyright info ==29515== command: ./a ==29515== ==29515== invalid write of size 4 ==29515== at 0x400544: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== address 0x5203068 is 0 bytes after a block of size 40 alloc’d ==29515== at 0x4c2db8f: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==29515== by 0x400537: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== ==29515== ==29515== heap summary: ==29515== in use at exit: 40 bytes in 1 blocks ==29515== total heap usage: 1 allocs, 0 frees, 40 bytes allocated ==29515== ==29515== leak summary: ==29515== definitely lost: 40 bytes in 1 blocks ==29515== indirectly lost: 0 bytes in 0 blocks ==29515== possibly lost: 0 bytes in 0 blocks ==29515== still reachable: 0 bytes in 0 blocks ==29515== suppressed: 0 bytes in 0 blocks ==29515== rerun with --leak-check=full to see details of leaked memory ==29515== ==29515== for counts of detected and suppressed errors, rerun with: -v ==29515== error summary: 1 errors from 1 contexts (suppressed: 0 from 0)	address	288
==29515== using valgrind-3.11.0 and libvex; rerun with -h for copyright info ==29515== command: ./a ==29515== ==29515== invalid write of size 4 ==29515== at 0x400544: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== address 0x5203068 is 0 bytes after a block of size 40 alloc’d ==29515== at 0x4c2db8f: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==29515== by 0x400537: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== ==29515== ==29515== heap summary: ==29515== in use at exit: 40 bytes in 1 blocks ==29515== total heap usage: 1 allocs, 0 frees, 40 bytes allocated ==29515== ==29515== leak summary: ==29515== definitely lost: 40 bytes in 1 blocks ==29515== indirectly lost: 0 bytes in 0 blocks ==29515== possibly lost: 0 bytes in 0 blocks ==29515== still reachable: 0 bytes in 0 blocks ==29515== suppressed: 0 bytes in 0 blocks ==29515== rerun with --leak-check=full to see details of leaked memory ==29515== ==29515== for counts of detected and suppressed errors, rerun with: -v ==29515== error summary: 1 errors from 1 contexts (suppressed: 0 from 0)	memory	1056
==29515== using valgrind-3.11.0 and libvex; rerun with -h for copyright info ==29515== command: ./a ==29515== ==29515== invalid write of size 4 ==29515== at 0x400544: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== address 0x5203068 is 0 bytes after a block of size 40 alloc’d ==29515== at 0x4c2db8f: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==29515== by 0x400537: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== ==29515== ==29515== heap summary: ==29515== in use at exit: 40 bytes in 1 blocks ==29515== total heap usage: 1 allocs, 0 frees, 40 bytes allocated ==29515== ==29515== leak summary: ==29515== definitely lost: 40 bytes in 1 blocks ==29515== indirectly lost: 0 bytes in 0 blocks ==29515== possibly lost: 0 bytes in 0 blocks ==29515== still reachable: 0 bytes in 0 blocks ==29515== suppressed: 0 bytes in 0 blocks ==29515== rerun with --leak-check=full to see details of leaked memory ==29515== ==29515== for counts of detected and suppressed errors, rerun with: -v ==29515== error summary: 1 errors from 1 contexts (suppressed: 0 from 0)	block	325
==29515== using valgrind-3.11.0 and libvex; rerun with -h for copyright info ==29515== command: ./a ==29515== ==29515== invalid write of size 4 ==29515== at 0x400544: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== address 0x5203068 is 0 bytes after a block of size 40 alloc’d ==29515== at 0x4c2db8f: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so) ==29515== by 0x400537: dummy_function (in /home/rafi/projects/exocpp/a) ==29515== by 0x40055a: main (in /home/rafi/projects/exocpp/a) ==29515== ==29515== ==29515== heap summary: ==29515== in use at exit: 40 bytes in 1 blocks ==29515== total heap usage: 1 allocs, 0 frees, 40 bytes allocated ==29515== ==29515== leak summary: ==29515== definitely lost: 40 bytes in 1 blocks ==29515== indirectly lost: 0 bytes in 0 blocks ==29515== possibly lost: 0 bytes in 0 blocks ==29515== still reachable: 0 bytes in 0 blocks ==29515== suppressed: 0 bytes in 0 blocks ==29515== rerun with --leak-check=full to see details of leaked memory ==29515== ==29515== for counts of detected and suppressed errors, rerun with: -v ==29515== error summary: 1 errors from 1 contexts (suppressed: 0 from 0)	valgrind	16
invalid write: it detected our heap block overrun, writing outside of an allocated block.	block	36
definitely lost: memory leak — you probably forgot to free a memory block.	memory	17
definitely lost: memory leak — you probably forgot to free a memory block.	block	68
valgrind is a effective tool to check for errors at runtime. c is special when it comes to such behavior, so after compiling your program you can use valgrind to fix errors that your compiler may miss and that usually happens when your program is running.	valgrind	0
threadsanitizer is a tool from google, built into clang and gcc, to help you detect race conditions in your code [5]. note, that running with tsan will slow your code down a bit. consider the following code.	the following	188
threadsanitizer is a tool from google, built into clang and gcc, to help you detect race conditions in your code [5]. note, that running with tsan will slow your code down a bit. consider the following code.	thread	0
threadsanitizer is a tool from google, built into clang and gcc, to help you detect race conditions in your code [5]. note, that running with tsan will slow your code down a bit. consider the following code.	code	108
threadsanitizer is a tool from google, built into clang and gcc, to help you detect race conditions in your code [5]. note, that running with tsan will slow your code down a bit. consider the following code.	threadsanitizer	0
we can see that there is a race condition on the variable global. both the main thread and created thread will try to change the value at the same time. but, does threadsantizer catch it?	thread	80
$ ./a.out ================== warning: threadsanitizer: data race (pid=28888) read of size 4 at 0x7f73ed91c078 by thread t1: #0 thread1 /home/zmick2/simple_race.c:7 (exe+0x000000000a50) #1 :0 (libtsan.so.0+0x00000001b459) previous write of size 4 at 0x7f73ed91c078 by main thread: #0 main /home/zmick2/simple_race.c:14 (exe+0x000000000ac8) thread t1 (tid=28889, running) created by main thread at: #0 :0 (libtsan.so.0+0x00000001f6ab) #1 main /home/zmick2/simple_race.c:13 (exe+0x000000000ab8)	thread	38
$ ./a.out ================== warning: threadsanitizer: data race (pid=28888) read of size 4 at 0x7f73ed91c078 by thread t1: #0 thread1 /home/zmick2/simple_race.c:7 (exe+0x000000000a50) #1 :0 (libtsan.so.0+0x00000001b459) previous write of size 4 at 0x7f73ed91c078 by main thread: #0 main /home/zmick2/simple_race.c:14 (exe+0x000000000ac8) thread t1 (tid=28889, running) created by main thread at: #0 :0 (libtsan.so.0+0x00000001f6ab) #1 main /home/zmick2/simple_race.c:13 (exe+0x000000000ab8)	threadsanitizer	38
summary: threadsanitizer: data race /home/zmick2/simple_race.c:7 thread1 ================== threadsanitizer: reported 1 warnings	thread	9
summary: threadsanitizer: data race /home/zmick2/simple_race.c:7 thread1 ================== threadsanitizer: reported 1 warnings	threadsanitizer	9
2.4 gdb	gdb	4
gdb is short for the gnu debugger. gdb is a program that helps you track down errors by interactively debugging them [6]. it can start and stop your program, look around, and put in ad hoc constraints and checks. here are a few examples.	gdb	0
setting breakpoints programmatically a breakpoint is a line of code where you want the execution to stop and give control back to the debugger. a useful trick when debugging complex c programs with gdb is setting breakpoints in the source code.	code	63
setting breakpoints programmatically a breakpoint is a line of code where you want the execution to stop and give control back to the debugger. a useful trick when debugging complex c programs with gdb is setting breakpoints in the source code.	a breakpoint	37
setting breakpoints programmatically a breakpoint is a line of code where you want the execution to stop and give control back to the debugger. a useful trick when debugging complex c programs with gdb is setting breakpoints in the source code.	gdb	198
$ gcc main.c -g -o main $ gdb --args ./main (gdb) r [...] program received signal sigtrap, trace/breakpoint trap.	gdb	26
main () at main.c:6 6 val = 7; (gdb) p val $1 = 42	gdb	32
$ gcc main.c -g -o main $ gdb --args ./main (gdb) break main.c:4 [...] (gdb) p val $1 = 42	gdb	26
checking memory content we can also use gdb to check the content of different pieces of memory. for example,	memory	9
checking memory content we can also use gdb to check the content of different pieces of memory. for example,	gdb	40
int main() { char bad_string[3] = {’c’, ’a’, ’t’}; printf("%s", bad_string); }	string	22
int main() { char bad_string[3] = {’c’, ’a’, ’t’}; printf("%s", bad_string); }	printf	51
we can now use gdb to look at specific bytes of the string and reason about when the program should’ve stopped running	string	52
we can now use gdb to look at specific bytes of the string and reason about when the program should’ve stopped running	gdb	15
(gdb) l 1 #include <stdio.h> 2 int main() { 3 char bad_string[3] = {’c’, ’a’, ’t’}; 4 printf("%s", bad_string); 5 } (gdb) b 4 breakpoint 1 at 0x100000f57: file main.c, line 4.	string	55
(gdb) l 1 #include <stdio.h> 2 int main() { 3 char bad_string[3] = {’c’, ’a’, ’t’}; 4 printf("%s", bad_string); 5 } (gdb) b 4 breakpoint 1 at 0x100000f57: file main.c, line 4.	gdb	1
(gdb) l 1 #include <stdio.h> 2 int main() { 3 char bad_string[3] = {’c’, ’a’, ’t’}; 4 printf("%s", bad_string); 5 } (gdb) b 4 breakpoint 1 at 0x100000f57: file main.c, line 4.	printf	86
(gdb) r [...] breakpoint 1, main () at main.c:4 4 printf("%s", bad_string);	string	67
(gdb) r [...] breakpoint 1, main () at main.c:4 4 printf("%s", bad_string);	gdb	1
(gdb) r [...] breakpoint 1, main () at main.c:4 4 printf("%s", bad_string);	printf	50
(gdb) x/16xb bad_string 0x7fff5fbff9cd: 0x63 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7fff5fbff9d5: 0x7f 0x00 0x00 0xfd 0xb5 0x23 0x89 0xff (gdb)	string	17
(gdb) x/16xb bad_string 0x7fff5fbff9cd: 0x63 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7fff5fbff9d5: 0x7f 0x00 0x00 0xfd 0xb5 0x23 0x89 0xff (gdb)	gdb	1
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	the following	157
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	address	86
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	string	122
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	memory	79
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	parameter	34
here, by using the x command with parameters 16xb, we can see that starting at memory address 0x7fff5fbff9c (value of bad_string), printf would actually see the following sequence of bytes as a string because we provided a malformed string without a null terminator.	printf	131
2.4.1 involved gdb example	gdb	15
here is how one of your tas would go through and debug a simple program that is going wrong. first, the source code of the program. if you can see the error immediately, please bear with us.	code	111
#include <stdio.h> double convert_to_radians(int deg); int main(){ for (int deg = 0; deg > 360; ++deg){ double radians = convert_to_radians(deg); printf("%d. %f\n", deg, radians); } return 0; } double convert_to_radians(int deg){ return ( 31415 / 1000 ) * deg / 180; }	printf	146
how can we use gdb to debug? first we ought to load gdb.	gdb	15
$ gdb --args ./main (gdb) layout src; # if you want a gui type (gdb) run (gdb)	type	58
$ gdb --args ./main (gdb) layout src; # if you want a gui type (gdb) run (gdb)	gdb	2
(gdb) l 1 #include <stdio.h>	gdb	1
2 3 double convert_to_radians(int deg); 4 5 int main(){ 6 for (int deg = 0; deg > 360; ++deg){ 7 double radians = convert_to_radians(deg); 8 printf("%d. %f\n", deg, radians); 9 } 10 return 0; (gdb) break 7 # break <file>:line or break <file>:function (gdb) run (gdb)	gdb	193
2 3 double convert_to_radians(int deg); 4 5 int main(){ 6 for (int deg = 0; deg > 360; ++deg){ 7 double radians = convert_to_radians(deg); 8 printf("%d. %f\n", deg, radians); 9 } 10 return 0; (gdb) break 7 # break <file>:line or break <file>:function (gdb) run (gdb)	printf	141
from running the code, the breakpoint didn’t even trigger, meaning the code never got to that point. thats because of the comparison! okay, flip the sign it should work now right?	code	17
(gdb) run 350. 60.000000 351. 60.000000 352. 60.000000 353. 60.000000 354. 60.000000 355. 61.000000 356. 61.000000 357. 61.000000 358. 61.000000 359. 61.000000	gdb	1
(gdb) break 14 if deg == 359 # let’s check the last iteration only (gdb) run ...	gdb	1
(gdb) print/x deg # print the hex value of degree $1 = 0x167 (gdb) print (31415/1000) $2 = 0x31 (gdb) print (31415/1000.0) $3 = 201.749 (gdb) print (31415.0/10000.0) $4 = 3.1414999999999999	gdb	1
that was only the bare minimum, though most of you will get by with that. there are a whole load more resources on the web, here are a few specific ones that can help you get started.	resources	102
1. introduction to gdb 2. memory content 3. cppcon 2015: greg law give me 15 minutes i’ll change your view of gdb	memory	26
1. introduction to gdb 2. memory content 3. cppcon 2015: greg law give me 15 minutes i’ll change your view of gdb	gdb	19
what do you actually use to run your program? a shell! a shell is a programming language that is running inside your terminal. a terminal is merely a window to input commands. now, on posix we usually have one shell called sh that is linked to a posix compliant shell called dash. most of the time, you use a shell called bash that is somewhat posix compliant but has some nifty built-in features. if you want to be even more advanced, zsh has some more powerful features like tab complete on programs and fuzzy patterns.	a terminal	127
what do you actually use to run your program? a shell! a shell is a programming language that is running inside your terminal. a terminal is merely a window to input commands. now, on posix we usually have one shell called sh that is linked to a posix compliant shell called dash. most of the time, you use a shell called bash that is somewhat posix compliant but has some nifty built-in features. if you want to be even more advanced, zsh has some more powerful features like tab complete on programs and fuzzy patterns.	a shell	46
2.4.3 undefined behavior sanitizer	behavior sanitizer	16
clang provides a great drop-in replacement tools for compiling programs. if you want to see if there is an error that may cause a race condition, casting error, etc, all you need to do is the following.	the following	188
strace and ltrace are two programs that trace the system calls and library calls respectively of a running program or command. these may be missing on your system so to install feel free to run the following.	the following	194
strace and ltrace are two programs that trace the system calls and library calls respectively of a running program or command. these may be missing on your system so to install feel free to run the following.	system	50
int main() { file *fp = fopen("i don’t exist", "r"); fprintf(fp, "a"); fclose(fp); return 0; }	printf	54
strace on the other hand could modify your program. debugging with strace is amazing. the basic usage is running strace with a program, and itll get you a complete list of system call parameters.	parameter	184
strace on the other hand could modify your program. debugging with strace is amazing. the basic usage is running strace with a program, and itll get you a complete list of system call parameters.	system	172
2.4.6 printfs	printf	6
when all else fails, print! each of your functions should have an idea of what it is going to do. you want to test that each of your functions is doing what it set out to do and see exactly where your code breaks. in the case with race conditions, tsan may be able to help, but having each thread print out data at certain times could help you identify the race condition.	thread	290
when all else fails, print! each of your functions should have an idea of what it is going to do. you want to test that each of your functions is doing what it set out to do and see exactly where your code breaks. in the case with race conditions, tsan may be able to help, but having each thread print out data at certain times could help you identify the race condition.	code	201
to make printfs useful, try to have a macro that fills in the context by which the printf was called – a log statement if you will. a simple useful but untested log statement could be as follows. try to make a test and figure out something that is going wrong, then log the state of your variables.	printf	8
and then use as appropriately. check out the compiling and linking section in the appendix if you have any questions on how a c program gets translated to machine code.	code	163
and then use as appropriately. check out the compiling and linking section in the appendix if you have any questions on how a c program gets translated to machine code.	section	67
char q[] = "do you wanna build a c99 program?"; #define or "go debugging with gdb?" static unsigned int i = sizeof(or) != strlen(or); char* ptr = "lathe"; size_t come = fprintf(stdout,"%s door", ptr+2); int away = ! (int) * ""; int* shared = mmap(null, sizeof(int*), prot_read | prot_write, map_shared | map_anonymous, -1, 0); munmap(shared,sizeof(int*)); if(!fork()) { execlp("man","man","-3","ftell", (char*)0); perror("failed"); } if(!fork()) { execlp("make","make", "snowman", (char*)0); execlp("make","make", (char*)0)); } exit(0);	ftell	395
char q[] = "do you wanna build a c99 program?"; #define or "go debugging with gdb?" static unsigned int i = sizeof(or) != strlen(or); char* ptr = "lathe"; size_t come = fprintf(stdout,"%s door", ptr+2); int away = ! (int) * ""; int* shared = mmap(null, sizeof(int*), prot_read | prot_write, map_shared | map_anonymous, -1, 0); munmap(shared,sizeof(int*)); if(!fork()) { execlp("man","man","-3","ftell", (char*)0); perror("failed"); } if(!fork()) { execlp("make","make", "snowman", (char*)0); execlp("make","make", (char*)0)); } exit(0);	mmap	242
char q[] = "do you wanna build a c99 program?"; #define or "go debugging with gdb?" static unsigned int i = sizeof(or) != strlen(or); char* ptr = "lathe"; size_t come = fprintf(stdout,"%s door", ptr+2); int away = ! (int) * ""; int* shared = mmap(null, sizeof(int*), prot_read | prot_write, map_shared | map_anonymous, -1, 0); munmap(shared,sizeof(int*)); if(!fork()) { execlp("man","man","-3","ftell", (char*)0); perror("failed"); } if(!fork()) { execlp("make","make", "snowman", (char*)0); execlp("make","make", (char*)0)); } exit(0);	gdb	78
char q[] = "do you wanna build a c99 program?"; #define or "go debugging with gdb?" static unsigned int i = sizeof(or) != strlen(or); char* ptr = "lathe"; size_t come = fprintf(stdout,"%s door", ptr+2); int away = ! (int) * ""; int* shared = mmap(null, sizeof(int*), prot_read | prot_write, map_shared | map_anonymous, -1, 0); munmap(shared,sizeof(int*)); if(!fork()) { execlp("man","man","-3","ftell", (char*)0); perror("failed"); } if(!fork()) { execlp("make","make", "snowman", (char*)0); execlp("make","make", (char*)0)); } exit(0);	printf	170
2.5.1 so you want to master system programming? and get a better	system	28
int main(int argc, char** argv) { puts("great! we have plenty of useful resources for you, but it’s up to you to"); puts(" be an active learner and learn how to solve problems and debug code."); puts("bring your near-completed answers the problems below"); puts(" to the first lab to show that you’ve been working on this."); printf("a few \"don’t knows\" or \"unsure\" is fine for lab 1.\n"); puts("warning: you and your peers will work hard in this class."); puts("this is not cs225; you will be pushed much harder to");	code	186
int main(int argc, char** argv) { puts("great! we have plenty of useful resources for you, but it’s up to you to"); puts(" be an active learner and learn how to solve problems and debug code."); puts("bring your near-completed answers the problems below"); puts(" to the first lab to show that you’ve been working on this."); printf("a few \"don’t knows\" or \"unsure\" is fine for lab 1.\n"); puts("warning: you and your peers will work hard in this class."); puts("this is not cs225; you will be pushed much harder to");	resources	72
int main(int argc, char** argv) { puts("great! we have plenty of useful resources for you, but it’s up to you to"); puts(" be an active learner and learn how to solve problems and debug code."); puts("bring your near-completed answers the problems below"); puts(" to the first lab to show that you’ve been working on this."); printf("a few \"don’t knows\" or \"unsure\" is fine for lab 1.\n"); puts("warning: you and your peers will work hard in this class."); puts("this is not cs225; you will be pushed much harder to");	printf	326
puts(" work things out on your own."); fprintf(stdout,"this homework is a stepping stone to all future assignments.\n"); char p[] = "so, you will want to clear up any confusions or misconceptions.\n"; write(1, p, strlen(p) ); char buffer[1024]; sprintf(buffer,"for grading purposes, this homework 0 will be graded as part of your lab %d work.\n", 1); write(1, buffer, strlen(buffer)); printf("press return to continue\n"); read(0, buffer, sizeof(buffer)); return 0; }	printf	40
2.5.2 watch the videos and write up your answers to the following questions	the following	52
in which our intrepid hero battles standard out, standard error, file descriptors and writing to files 1. hello, world! (system call style) write a program that uses write() to print out "hi! my name is <your name>".	system	121
4. not everything is a system call take your program from "writing to files" and replace write() with printf(). make sure to print to the file instead of standard out!	system	23
4. not everything is a system call take your program from "writing to files" and replace write() with printf(). make sure to print to the file instead of standard out!	printf	102
4. not everything is a system call take your program from "writing to files" and replace write() with printf(). make sure to print to the file instead of standard out!	a system call	21
5. what are some differences between write() and printf()?	printf	49
sizing up c types and their limits, int and char arrays, and incrementing pointers 1. how many bits are there in a byte?	type	12
sizing up c types and their limits, int and char arrays, and incrementing pointers 1. how many bits are there in a byte?	pointer	74
3. how many bytes the following are on your machine? int, double, float, long, and long long 4. on a machine with 8 byte integers, the declaration for the variable data is int data[8]. if the address of data is 0x7fbd9d40, then what is the address of data+2?	the following	18
3. how many bytes the following are on your machine? int, double, float, long, and long long 4. on a machine with 8 byte integers, the declaration for the variable data is int data[8]. if the address of data is 0x7fbd9d40, then what is the address of data+2?	address	192
5. what is data[3] equivalent to in c? hint: what does c convert data[3] to before dereferencing the address? remember, the type of a string constant "abc" is an array.	address	101
5. what is data[3] equivalent to in c? hint: what does c convert data[3] to before dereferencing the address? remember, the type of a string constant "abc" is an array.	string	134
5. what is data[3] equivalent to in c? hint: what does c convert data[3] to before dereferencing the address? remember, the type of a string constant "abc" is an array.	type	124
program arguments, environment variables, and working with character arrays (strings)	string	77
3. where are the pointers to environment variables stored (on the stack, the heap, somewhere else)?	the heap	73
3. where are the pointers to environment variables stored (on the stack, the heap, somewhere else)?	pointer	17
4. on a machine where pointers are 8 bytes, and with the following code:	the following	53
4. on a machine where pointers are 8 bytes, and with the following code:	code	67
4. on a machine where pointers are 8 bytes, and with the following code:	pointer	22
5. what data structure manages the lifetime of automatic variables?	a struct	11
heap and stack memory, and working with structs 1. if i want to use data after the lifetime of the function it was created in ends, where should i put it? how do i put it there?	memory	15
2. what are the differences between heap and stack memory?	memory	51
3. are there other kinds of memory in a process?	a process	38
3. are there other kinds of memory in a process?	memory	28
7. what is wrong with this code snippet?	code	27
7. what is wrong with this code snippet?	snippet	32
8. what is wrong with this code snippet?	code	27
8. what is wrong with this code snippet?	snippet	32
free(ptr); printf("%s\n", ptr);	printf	11
10. create a struct that represents a person. then make a typedef, so that struct person can be replaced with a single word. a person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to persons).	a struct	11
10. create a struct that represents a person. then make a typedef, so that struct person can be replaced with a single word. a person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to persons).	the following	149
10. create a struct that represents a person. then make a typedef, so that struct person can be replaced with a single word. a person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to persons).	string	190
10. create a struct that represents a person. then make a typedef, so that struct person can be replaced with a single word. a person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to persons).	type	58
10. create a struct that represents a person. then make a typedef, so that struct person can be replaced with a single word. a person should contain the following information: their name (a string), their age (an integer), and a list of their friends (stored as a pointer to an array of pointers to persons).	pointer	264
11. now, make two persons on the heap, "agent smith" and "sonny moore", who are 128 and 256 years old respectively and are friends with each other. create functions to create and destroy a person (person’s and their names should live on the heap).	the heap	29
12. create() should take a name and age. the name should be copied onto the heap. use malloc to reserve sufficient memory for everyone having up to ten friends. be sure initialize all fields (why?).	the heap	72
12. create() should take a name and age. the name should be copied onto the heap. use malloc to reserve sufficient memory for everyone having up to ten friends. be sure initialize all fields (why?).	memory	115
13. destroy() should free up both the memory of the person struct and all of its attributes that are stored on the heap. destroying one person keeps other people in tact any other.	the heap	111
13. destroy() should free up both the memory of the person struct and all of its attributes that are stored on the heap. destroying one person keeps other people in tact any other.	memory	38
3. write code that parses the string "hello 5 world" and initializes 3 variables to "hello", 5, and "world".	code	9
3. write code that parses the string "hello 5 world" and initializes 3 variables to "hello", 5, and "world".	string	30
2. you fix a problem in the makefile and type make again. explain why this may be insufficient to generate a new build.	type	41
 convert a song lyrics into system programming and c code covered in this wiki book and share on piazza.	code	53
 convert a song lyrics into system programming and c code covered in this wiki book and share on piazza.	system	28
 find, in your opinion, the best and worst c code on the web and post the link to piazza.	code	45
 do you have any cool/disastrous system programming bugs you’ve heard about? feel free to share with your peers and the course staff on piazza.	system	33
7. did i try commenting out, printing, and/or stepping through parts of the code bit by bit to find out precisely where the error occurs?	code	76
8. did i commit my code to git in case the tas need more context?	code	19
9. did i include the console/gdb/valgrind output **and** code surrounding the bug in my piazza post?	code	57
9. did i include the console/gdb/valgrind output **and** code surrounding the bug in my piazza post?	valgrind	33
9. did i include the console/gdb/valgrind output **and** code surrounding the bug in my piazza post?	gdb	29
11. am i following good programming practice? (i.e. encapsulation, functions to limit repetition, etc) the biggest tip that we can give you when asking a question on piazza if you want a swift answer is to ask your question like you were trying to answer it. like before you ask a question, try to answer it yourself. if you are thinking about posting hi, my code got a 50 sounds good and courteous, but course staff would much much prefer a post resembling the following	the following	458
11. am i following good programming practice? (i.e. encapsulation, functions to limit repetition, etc) the biggest tip that we can give you when asking a question on piazza if you want a swift answer is to ask your question like you were trying to answer it. like before you ask a question, try to answer it yourself. if you are thinking about posting hi, my code got a 50 sounds good and courteous, but course staff would much much prefer a post resembling the following	code	359
hi, i recently failed test x, y, z which is about half the tests on this current assignment. i noticed that they all have something to do with networking and epoll, but couldn’t figure out what was linking them together, or i may be completely off track. so to test my idea, i tried spawning 1000 clients with various get and put requests and verifying the files matched their originals. i couldn’t get it to fail while running normally, the debug build, or valgrind or tsan. i have no warnings and none of the pre-syntax checks showed me anything. could you tell me if my understanding of the failure is correct and what i could do to modify my tests to better reflect x, y, z? netid: bvenkat2 you don’t need to be as courteous, though we’d appreciate it, this will get a faster response time hand over foot. if you were trying to answer this question, you’d have everything you need in the question body.	valgrind	458
[5] threadsanitizercppmanual, dec 2018.	thread	4
[5] threadsanitizercppmanual, dec 2018.	threadsanitizer	4
threadsanitizercppmanual.	thread	0
threadsanitizercppmanual.	threadsanitizer	0
com/systemcallinlinux2_6.html.	system	4
[8] part guide. intel® 64 and ia-32 architectures software developers manual. volume 3b: system programming guide, part, 2, 2011.	system	89
if you want to teach systems, don’t drum up the programmers, sort the issues, and make prs. instead, teach them to yearn for the vast and endless c.	system	21
c is the de-facto programming language to do serious system serious programming. why? most kernels have their api accessible through c. the linux kernel [7] and the xnu kernel [4] of which macos is based on are written in c and have c api - application programming interface. the windows kernel uses c++, but doing system programming on that is much harder on windows that unix for novice system programmers. c doesn’t have abstractions like classes and resource acquisition is initialization (raii) to clean up memory. c also gives you much more of an opportunity to shoot yourself in the foot, but it lets you do things at a much more fine-grained level.	memory	512
c is the de-facto programming language to do serious system serious programming. why? most kernels have their api accessible through c. the linux kernel [7] and the xnu kernel [4] of which macos is based on are written in c and have c api - application programming interface. the windows kernel uses c++, but doing system programming on that is much harder on windows that unix for novice system programmers. c doesn’t have abstractions like classes and resource acquisition is initialization (raii) to clean up memory. c also gives you much more of an opportunity to shoot yourself in the foot, but it lets you do things at a much more fine-grained level.	system	53
c was developed by dennis ritchie and ken thompson at bell labs back in 1973 [8]. back then, we had gems of programming languages like fortran, algol, and lisp. the goal of c was two-fold. firstly, it was made to target the most popular computers at the time, such as the pdp-7. secondly, it tried to remove some of the lower-level constructs (managing registers, and programming assembly for jumps), and create a language that had the power to express programs procedurally (as opposed to mathematically like lisp) with readable code. all this while still having the ability to interface with the operating system. it sounded like a tough feat. at first, it was only used internally at bell labs along with the unix operating system.	code	530
c was developed by dennis ritchie and ken thompson at bell labs back in 1973 [8]. back then, we had gems of programming languages like fortran, algol, and lisp. the goal of c was two-fold. firstly, it was made to target the most popular computers at the time, such as the pdp-7. secondly, it tried to remove some of the lower-level constructs (managing registers, and programming assembly for jumps), and create a language that had the power to express programs procedurally (as opposed to mathematically like lisp) with readable code. all this while still having the ability to interface with the operating system. it sounded like a tough feat. at first, it was only used internally at bell labs along with the unix operating system.	system	608
 speed. there is little separating a program and the system.	system	53
 manual memory management. c gives a program the ability to manage its memory. however, this can be a downside if a program has memory errors.	memory	8
 ubiquity. through foreign function interfaces (ffi) and language bindings of various types, most other languages can call c functions and vice versa. the standard library is also everywhere. c has stood the test of time as a popular language, and it doesn’t look like it is going anywhere.	type	86
#include <stdio.h> int main(void) { printf("hello world\n"); return 0; }	printf	36
1. the #include directive takes the file stdio.h (which stands for standard input and output) located somewhere in your operating system, copies the text, and substitutes it where the #include was.	system	130
2. the int main(void) is a function declaration. the first word int tells the compiler the return type of the function. the part before the parenthesis (main) is the function name. in c, no two functions can have the same name in a single compiled program, although shared libraries may be able. then, the parameter list comes after. when we provide the parameter list for regular functions (void) that means that the compiler should produce an error if the function is called with a non-zero number of arguments. for regular functions having a declaration like void func() means that the function can be called like func(1, 2, 3), because there is no delimiter. main is a special function. there are many ways of declaring main but the standard ones are int main(void), int main(), and int main(int argc, char *argv[]).	parameter	306
2. the int main(void) is a function declaration. the first word int tells the compiler the return type of the function. the part before the parenthesis (main) is the function name. in c, no two functions can have the same name in a single compiled program, although shared libraries may be able. then, the parameter list comes after. when we provide the parameter list for regular functions (void) that means that the compiler should produce an error if the function is called with a non-zero number of arguments. for regular functions having a declaration like void func() means that the function can be called like func(1, 2, 3), because there is no delimiter. main is a special function. there are many ways of declaring main but the standard ones are int main(void), int main(), and int main(int argc, char *argv[]).	type	98
3. printf("hello world"); is what a function call. printf is defined as a part of stdio.h. the function has been compiled and lives somewhere else on our machine - the location of the c standard library. just remember to include the header and call the function with the appropriate parameters (a string literal "hello world"). if the newline isn’t included, the buffer will not be flushed (i.e. the write will not complete immediately).	string	297
3. printf("hello world"); is what a function call. printf is defined as a part of stdio.h. the function has been compiled and lives somewhere else on our machine - the location of the c standard library. just remember to include the header and call the function with the appropriate parameters (a string literal "hello world"). if the newline isn’t included, the buffer will not be flushed (i.e. the write will not complete immediately).	parameter	283
3. printf("hello world"); is what a function call. printf is defined as a part of stdio.h. the function has been compiled and lives somewhere else on our machine - the location of the c standard library. just remember to include the header and call the function with the appropriate parameters (a string literal "hello world"). if the newline isn’t included, the buffer will not be flushed (i.e. the write will not complete immediately).	the header	229
3. printf("hello world"); is what a function call. printf is defined as a part of stdio.h. the function has been compiled and lives somewhere else on our machine - the location of the c standard library. just remember to include the header and call the function with the appropriate parameters (a string literal "hello world"). if the newline isn’t included, the buffer will not be flushed (i.e. the write will not complete immediately).	printf	3
if systems programming was as easy as writing hello world though, our jobs would be much easier.	system	3
what is the preprocessor? preprocessing is a copy and paste operation that the compiler performs before actually compiling the program. the following is an example of substitution	the following	136
what is the preprocessor? preprocessing is a copy and paste operation that the compiler performs before actually compiling the program. the following is an example of substitution	preprocessing	26
#define min(a,b) a < b ? a : b int main() { int x = 4; if(min(x++, 5)) printf("%d is six", x); return 0; }	printf	71
there are also logical problems with the flexibility of certain parameters. one common source of confusion is with static arrays and the sizeof operator.	parameter	64
what is wrong with the macro? well, it works if a static array is passed in because sizeof a static array returns the number of bytes that array takes up and dividing it by the sizeof(an_element) would give the number of entries. but if passed a pointer to a piece of memory, taking the sizeof the pointer and dividing it by the size of the first entry won’t always give us the size of the array.	memory	268
what is wrong with the macro? well, it works if a static array is passed in because sizeof a static array returns the number of bytes that array takes up and dividing it by the sizeof(an_element) would give the number of entries. but if passed a pointer to a piece of memory, taking the sizeof the pointer and dividing it by the size of the first entry won’t always give us the size of the array.	pointer	246
1. break is a keyword that is used in case statements or looping statements. when used in a case statement, the program jumps to the end of the block.	block	144
puts("1"); break; /* jumps to the end of the block */ case 2: /* ignores this program */ puts("2"); break; } /* continues here */	block	45
in the context of a loop, using it breaks out of the inner-most loop. the loop can be either a for, while, or do-while construct	the loop	70
2. const is a language level construct that tells the compiler that this data should remain constant. if one tries to change a const variable, the program will fail to compile. const works a little differently when put before the type, the compiler re-orders the first type and const. then the compiler uses a left associativity rule. meaning that whatever is left of the pointer is constant. this is known as const-correctness.	type	230
2. const is a language level construct that tells the compiler that this data should remain constant. if one tries to change a const variable, the program will fail to compile. const works a little differently when put before the type, the compiler re-orders the first type and const. then the compiler uses a left associativity rule. meaning that whatever is left of the pointer is constant. this is known as const-correctness.	pointer	372
but, it is important to know that this is a compiler imposed restriction only. there are ways of getting around this, and the program will run fine with defined behavior. in systems programming, the only type of memory that you can’t write to is system write-protected memory.	memory	212
but, it is important to know that this is a compiler imposed restriction only. there are ways of getting around this, and the program will run fine with defined behavior. in systems programming, the only type of memory that you can’t write to is system write-protected memory.	system	174
but, it is important to know that this is a compiler imposed restriction only. there are ways of getting around this, and the program will run fine with defined behavior. in systems programming, the only type of memory that you can’t write to is system write-protected memory.	type	204
3. continue is a control flow statement that exists only in loop constructions. continue will skip the rest of the loop body and set the program counter back to the start of the loop before.	the loop	111
4. do {} while(); is another loop construct. these loops execute the body and then check the condition at the bottom of the loop. if the condition is zero, the next statement is executed – the program counter is set to the first instruction after the loop. otherwise, the loop body is executed.	the loop	120
int i = 1; do { printf("%d\n", i--); } while (i > 10) /* only executed once */	printf	16
5. enum is to declare an enumeration. an enumeration is a type that can take on many, finite values. if you have an enum and don’t specify any numerics, the c compiler will generate a unique number for that enum (within the context of the current enum) and use that for comparisons. the syntax to declare an instance of an enum is enum <type> varname. the added benefit to this is that the compiler can type check these expressions to make sure that you are only comparing alike types.	type	58
6. extern is a special keyword that tells the compiler that the variable may be defined in another object file or a library, so the program compiles on missing variable because the program will reference a variable in the system or another file.	system	222
as of the c89 standard, one cannot declare variables inside the for loop initialization block. this is because there was a disagreement in the standard for how the scoping rules of a variable defined in the loop would work. it has since been resolved with more recent standards, so people can use the for loop that they know and love today	block	88
as of the c89 standard, one cannot declare variables inside the for loop initialization block. this is because there was a disagreement in the standard for how the scoping rules of a variable defined in the loop would work. it has since been resolved with more recent standards, so people can use the for loop that they know and love today	the loop	203
(b) check the invariant. if false, terminate the loop and execute the next statement. if true, continue to the body of the loop.	the loop	45
(c) perform the body of the loop.	the loop	24
8. goto is a keyword that allows you to do conditional jumps. do not use goto in your programs. the reason being is that it makes your code infinitely more hard to understand when strung together with multiple chains, which is called spaghetti code. it is acceptable to use in some contexts though, for example, error checking code in the linux kernel. the keyword is usually used in kernel contexts when adding another stack frame for cleanup isn’t a good idea. the canonical example of kernel cleanup is as below.	code	135
9. if else else-if are control flow keywords. there are a few ways to use these (1) a bare if (2) an if with an else (3) an if with an else-if (4) an if with an else if and else. note that an else is matched with the most recent if. a subtle bug related to a mismatched if and else statement, is the dangling else problem. the statements are always executed from the if to the else. if any of the intermediate conditions are true, the if block performs that action and goes to the end of that block.	block	438
10. inline is a compiler keyword that tells the compiler it’s okay to moit the c function call procedure and "paste" the code in the callee. instead, the compiler is hinted at substituting the function body directly into the calling function. this is not always recommended explicitly as the compiler is usually smart enough to know when to inline a function for you.	code	121
11. restrict is a keyword that tells the compiler that this particular memory region shouldn’t overlap with all other memory regions. the use case for this is to tell users of the program that it is undefined behavior if the memory regions overlap. note that memcpy has undefined behavior when memory regions overlap. if this might be the case in your program, consider using memmove.	memory	71
12. return is a control flow operator that exits the current function. if the function is void then it simply exits the functions. otherwise, another parameter follows as the return value.	parameter	150
13. signed is a modifier which is rarely used, but it forces a type to be signed instead of unsigned. the reason that this is so rarely used is because types are signed by default and need to have the unsigned modifier to make them unsigned but it may be useful in cases where you want the compiler to default to a signed type such as below.	type	63
14. sizeof is an operator that is evaluated at compile-time, which evaluates to the number of bytes that the expression contains. when the compiler infers the type the following code changes as follows.	the following	164
14. sizeof is an operator that is evaluated at compile-time, which evaluates to the number of bytes that the expression contains. when the compiler infers the type the following code changes as follows.	code	178
14. sizeof is an operator that is evaluated at compile-time, which evaluates to the number of bytes that the expression contains. when the compiler infers the type the following code changes as follows.	type	159
char a = 0; printf("%zu", sizeof(a++));	printf	12
char a = 0; printf("%zu", 1);	printf	12
which then the compiler is allowed to operate on further. the compiler must have a complete definition of the type at compile-time - not link time - or else you may get an odd error. consider the following	the following	192
which then the compiler is allowed to operate on further. the compiler must have a complete definition of the type at compile-time - not link time - or else you may get an odd error. consider the following	type	110
this code will not compile because sizeof is not able to compile file.c without knowing the full declaration of the person struct. that is typically why programmers either put the full declaration in a header file or we abstract the creation and the interaction away so that users cannot access the internals of our struct.	code	5
additionally, if the compiler knows the full length of an array object, it will use that in the expression instead of having it decay into a pointer.	pointer	141
be careful, using sizeof for the length of a string!	string	45
15. static is a type specifier with three meanings.	type	16
16. struct is a keyword that allows you to pair multiple types together into a new structure. c-structs are contiguous regions of memory that one can access specific elements of each memory as if they were separate variables. note that there might be padding between elements, such that each variable is memory-aligned (starts at a memory address that is a multiple of its size).	address	339
16. struct is a keyword that allows you to pair multiple types together into a new structure. c-structs are contiguous regions of memory that one can access specific elements of each memory as if they were separate variables. note that there might be padding between elements, such that each variable is memory-aligned (starts at a memory address that is a multiple of its size).	memory	130
16. struct is a keyword that allows you to pair multiple types together into a new structure. c-structs are contiguous regions of memory that one can access specific elements of each memory as if they were separate variables. note that there might be padding between elements, such that each variable is memory-aligned (starts at a memory address that is a multiple of its size).	type	57
one of the more famous examples of this is duff’s device which allows for loop unrolling. you don’t need to understand this code for the purposes of this class, but it is fun to look at [2].	code	124
this piece of code highlights that switch statements are goto statements, and you can put any code on the other end of a switch case. most of the time it doesn’t make sense, some of the time it just makes too much sense.	code	14
18. typedef declares an alias for a type. often used with structs to reduce the visual clutter of having to write ‘struct’ as part of the type.	type	4
in this class, we regularly typedef functions. a typedef for a function can be this for example	type	28
typedef int (*comparator)(void*,void*); int greater_than(void* a, void* b){ return a > b; } comparator gt = greater_than;	type	0
this declares a function type comparator that accepts two void* params and returns an integer.	type	25
19. union is a new type specifier. a union is one piece of memory that many variables occupy. it is used to maintain consistency while having the flexibility to switch between types without maintaining functions to keep track of the bits. consider an example where we have different pixel values.	memory	59
19. union is a new type specifier. a union is one piece of memory that many variables occupy. it is used to maintain consistency while having the flexibility to switch between types without maintaining functions to keep track of the bits. consider an example where we have different pixel values.	type	19
20. unsigned is a type modifier that forces unsigned behavior in the variables they modify. unsigned can only be used with primitive int types (like int and long). there is a lot of behavior associated with unsigned arithmetic. for the most part, unless your code involves bit shifting, it isn’t essential to know the difference in behavior with regards to unsigned and signed arithmetic.	code	259
20. unsigned is a type modifier that forces unsigned behavior in the variables they modify. unsigned can only be used with primitive int types (like int and long). there is a lot of behavior associated with unsigned arithmetic. for the most part, unless your code involves bit shifting, it isn’t essential to know the difference in behavior with regards to unsigned and signed arithmetic.	type	18
21. void is a double meaning keyword. when used in terms of function or parameter definition, it means that the function explicitly returns no value or accepts no parameter, respectively. the following declares a function that accepts no parameters and returns nothing.	the following	188
21. void is a double meaning keyword. when used in terms of function or parameter definition, it means that the function explicitly returns no value or accepts no parameter, respectively. the following declares a function that accepts no parameters and returns nothing.	parameter	72
the other use of void is when you are defining an lvalue. a void * pointer is just a memory address. it is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. pointer arithmetic with this pointer is undefined behavior.	address	92
the other use of void is when you are defining an lvalue. a void * pointer is just a memory address. it is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. pointer arithmetic with this pointer is undefined behavior.	memory	85
the other use of void is when you are defining an lvalue. a void * pointer is just a memory address. it is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. pointer arithmetic with this pointer is undefined behavior.	type	134
the other use of void is when you are defining an lvalue. a void * pointer is just a memory address. it is specified as an incomplete type meaning that you cannot dereference it but it can be promoted to any time to any other type. pointer arithmetic with this pointer is undefined behavior.	pointer	67
22. volatile is a compiler keyword. this means that the compiler should not optimize its value out. consider the following simple function.	the following	109
the compiler may, since the internals of the while loop have nothing to do with the flag, optimize it to the following even though a function may alter the data.	the following	105
if you use the volatile keyword, the compiler is forced to keep the variable in and perform that check. this is useful for cases where you are doing multi-process or multi-threaded programs so that we can affect the running of one sequence of execution with another.	thread	172
23. while represents the traditional while loop. there is a condition at the top of the loop, which is checked before every execution of the loop body. if the condition evaluates to a non-zero value, the loop body will be run.	the loop	84
3.3.2 c data types	type	13
there are many data types in c. as you may realize, all of them are either integers or floating point numbers and other types are variations of these.	type	20
1. char represents exactly one byte of data. the number of bits in a byte might vary. unsigned char and signed char means the exact same thing. this must be aligned on a boundary (meaning you cannot use bits in between two addresses). the rest of the types will assume 8 bits in a byte.	address	223
1. char represents exactly one byte of data. the number of bits in a byte might vary. unsigned char and signed char means the exact same thing. this must be aligned on a boundary (meaning you cannot use bits in between two addresses). the rest of the types will assume 8 bits in a byte.	type	251
2. short (short int) must be at least two bytes. this is aligned on a two byte boundary, meaning that the address must be divisible by two.	address	106
if you want a fixed width integer type, for more portable code, you may use the types defined in stdint.h, which are of the form [u]intwidth_t, where u (which is optional) represents the signedness, and width is any of 8, 16, 32, and 64.	code	58
if you want a fixed width integer type, for more portable code, you may use the types defined in stdint.h, which are of the form [u]intwidth_t, where u (which is optional) represents the signedness, and width is any of 8, 16, 32, and 64.	type	34
 [] is the subscript operator. a[n] == (a + n)* where n is a number type and a is a pointer type.	type	68
 [] is the subscript operator. a[n] == (a + n)* where n is a number type and a is a pointer type.	pointer	84
 -> is the structure dereference (or arrow) operator. if you have a pointer to a struct *p, you can use this to access one of its elements. p->element.	a struct	79
 -> is the structure dereference (or arrow) operator. if you have a pointer to a struct *p, you can use this to access one of its elements. p->element.	pointer	68
 +/-a is the unary plus and minus operator. they either keep or negate the sign, respectively, of the integer or float type underneath.	type	119
 *a is the dereference operator. if you have a pointer *p, you can use this to access the element located at this memory address. if you are reading, the return value will be the size of the underlying type. if you are writing, the value will be written with an offset.	address	121
 *a is the dereference operator. if you have a pointer *p, you can use this to access the element located at this memory address. if you are reading, the return value will be the size of the underlying type. if you are writing, the value will be written with an offset.	memory	114
 *a is the dereference operator. if you have a pointer *p, you can use this to access the element located at this memory address. if you are reading, the return value will be the size of the underlying type. if you are writing, the value will be written with an offset.	type	202
 *a is the dereference operator. if you have a pointer *p, you can use this to access the element located at this memory address. if you are reading, the return value will be the size of the underlying type. if you are writing, the value will be written with an offset.	pointer	47
 &a is the address-of operator. this takes an element and returns its address.	address	11
 sizeof is the sizeof operator, that is evaluated at the time of compilation. this is also mentioned in the keywords section.	section	117
 a <mop> b where <mop> in {+, -, *, %, /} are the arithmetic binary operators. if the operands are both number types, then the operations are plus, minus, times, modulo, and division respectively. if the left operand is a pointer and the right operand is an integer type, then only plus or minus may be used and the rules for pointer arithmetic are invoked.	type	111
 a <mop> b where <mop> in {+, -, *, %, /} are the arithmetic binary operators. if the operands are both number types, then the operations are plus, minus, times, modulo, and division respectively. if the left operand is a pointer and the right operand is an integer type, then only plus or minus may be used and the rules for pointer arithmetic are invoked.	pointer	222
 a <mop> b where <mop> in {+, -, *, %, /} are the arithmetic binary operators. if the operands are both number types, then the operations are plus, minus, times, modulo, and division respectively. if the left operand is a pointer and the right operand is an integer type, then only plus or minus may be used and the rules for pointer arithmetic are invoked.	operand	86
 »/« are the bit shift operators. the operand on the right has to be an integer type whose signedness is ignored unless it is signed negative in which case the behavior is undefined. the operator on the left decides a lot of semantics. if we are left shifting, there will always be zeros introduced on the right. if we are right shifting there are a few different cases – if the operand on the left is signed, then the integer is sign-extended. this means that if the number has the sign bit set, then any shift right will introduce ones on the left. if the number does not have the sign bit set, any shift right will introduce zeros on the left.	type	80
 »/« are the bit shift operators. the operand on the right has to be an integer type whose signedness is ignored unless it is signed negative in which case the behavior is undefined. the operator on the left decides a lot of semantics. if we are left shifting, there will always be zeros introduced on the right. if we are right shifting there are a few different cases – if the operand on the left is signed, then the integer is sign-extended. this means that if the number has the sign bit set, then any shift right will introduce ones on the left. if the number does not have the sign bit set, any shift right will introduce zeros on the left.	operand	38
– if the operand is unsigned, zeros will be introduced on the left either way.	operand	9
 && is the logical and operator. if the first operand is zero, the second won’t be evaluated and the expression will evaluate to 0. otherwise, it yields a 1-0 value of the second operand.	operand	46
 || is the logical or operator. if the first operand is not zero, then second won’t be evaluated and the expression will evaluate to 1. otherwise, it yields a 1-0 value of the second operand.	operand	45
 ! is the logical not operator. if the operand is zero, then this will return 1. otherwise, it will return 0.	operand	39
 & is the bitwise and operator. if a bit is set in both operands, it is set in the output. otherwise, it is not.	operand	56
 | is the bitwise or operator. if a bit is set in either operand, it is set in the output. otherwise, it is not.	operand	57
up until this point, we’ve covered c’s language fundamentals. we’ll now be focusing our attention to c and the posix variety of functions available to us to interact with the operating systems. we will talk about portable functions, for example fwrite printf. we will be evaluating the internals and scrutinizing them under the posix models and more specifically gnu/linux. there are several things to that philosophy that makes the rest of this easier to know, so we’ll put those things here.	system	185
up until this point, we’ve covered c’s language fundamentals. we’ll now be focusing our attention to c and the posix variety of functions available to us to interact with the operating systems. we will talk about portable functions, for example fwrite printf. we will be evaluating the internals and scrutinizing them under the posix models and more specifically gnu/linux. there are several things to that philosophy that makes the rest of this easier to know, so we’ll put those things here.	printf	252
and operations on those objects are done through system calls. one last thing to note before we move on is that the file descriptors are merely pointers. imagine that each of the file descriptors in the example actually refers to an entry in a table of objects that the operating system picks and chooses from (that is, the file descriptor table).	system	49
and operations on those objects are done through system calls. one last thing to note before we move on is that the file descriptors are merely pointers. imagine that each of the file descriptors in the example actually refers to an entry in a table of objects that the operating system picks and chooses from (that is, the file descriptor table).	pointer	144
objects can be allocated and deallocated, closed and opened, etc. the program interacts with these objects by using the api specified through system calls, and library functions.	system	142
3.4.2 system calls	system	6
before we dive into common c functions, we need to know what a system call is. if you are a student and have completed hw0, feel free to gloss over this section.	this section	148
before we dive into common c functions, we need to know what a system call is. if you are a student and have completed hw0, feel free to gloss over this section.	system	63
before we dive into common c functions, we need to know what a system call is. if you are a student and have completed hw0, feel free to gloss over this section.	section	153
before we dive into common c functions, we need to know what a system call is. if you are a student and have completed hw0, feel free to gloss over this section.	a system call	61
a system call is an operation that the kernel carries out. first, the operating system prepares a system call.	system	2
a system call is an operation that the kernel carries out. first, the operating system prepares a system call.	a system call	0
next, the kernel executes the system call to the best of its ability in kernel space and is a privileged operation.	system	30
in the previous example, we got access to a file descriptor object. we can now also write some bytes to the file descriptor object that represents a file, and the operating system will do its best to get the bytes written to the disk.	system	173
when we say the kernel tries its best, this includes the possibility that the operation could fail for several reasons. some of them are: the file is no longer valid, the hard drive failed, the system was interrupted etc. the way that a programmer communicates with the outside system is with system calls. an important thing to note is that system calls are expensive. their cost in terms of time and cpu cycles has recently been decreased, but try to use them as sparingly as possible.	system	194
3.4.3 c system calls	system	8
many c functions that will be discussed in the next sections are abstractions that call the correct underlying system call, based on the current platform. their windows implementation, for example, may be entirely different from that of other operating systems. nevertheless, we will be studying these in the context of their linux implementation.	system	111
many c functions that will be discussed in the next sections are abstractions that call the correct underlying system call, based on the current platform. their windows implementation, for example, may be entirely different from that of other operating systems. nevertheless, we will be studying these in the context of their linux implementation.	section	52
to find more information about any functions, please use the man pages. note the man pages are organized into sections. section 2 are system calls. section 3 are c libraries. on the web, google man 7 open. in the shell, man -s2 open or man -s3 printf	system	134
to find more information about any functions, please use the man pages. note the man pages are organized into sections. section 2 are system calls. section 3 are c libraries. on the web, google man 7 open. in the shell, man -s2 open or man -s3 printf	printf	244
to find more information about any functions, please use the man pages. note the man pages are organized into sections. section 2 are system calls. section 3 are c libraries. on the web, google man 7 open. in the shell, man -s2 open or man -s3 printf	section	110
whatever the pros/cons are, we use the former because of backwards compatibility with languages like fortran [3, p. 84]. each thread will get a copy of errno because it is stored at the top of each thread’s stack – more on threads later. one makes a call to a function that could return an error and if that function returns an error according to the man pages, it is up to the programmer to check errno.	thread	126
#include <errno.h> file *f = fopen("/does/not/exist.txt", "r"); if (null == f) { fprintf(stderr, "errno is %d\n", errno); fprintf(stderr, "description is %s\n", strerror(errno)); }	printf	82
there is a shortcut function perror that prints the english description of errno. also, a function may return the error code in the return value itself.	code	120
int s = getnameinfo(...); if (0 != s) { fprintf(stderr, "getnameinfo: %s\n", gai_strerror(s)); }	printf	41
be sure to check the man page for return code characteristics.	code	41
in this section we will cover all the basic input and output functions in the standard library with references to system calls. every process has three streams of data when it starts execution: standard input (for program input), standard output (for program output), and standard error (for error and debug messages). usually, standard input is sourced from the terminal in which the program is being run in, and standard out is the same terminal.	this section	3
in this section we will cover all the basic input and output functions in the standard library with references to system calls. every process has three streams of data when it starts execution: standard input (for program input), standard output (for program output), and standard error (for error and debug messages). usually, standard input is sourced from the terminal in which the program is being run in, and standard out is the same terminal.	system	114
in this section we will cover all the basic input and output functions in the standard library with references to system calls. every process has three streams of data when it starts execution: standard input (for program input), standard output (for program output), and standard error (for error and debug messages). usually, standard input is sourced from the terminal in which the program is being run in, and standard out is the same terminal.	section	8
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	the following	322
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	address	526
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	string	231
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	memory	519
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	parameter	209
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	pointer	375
stdout oriented streams standard output or stdout oriented streams are streams whose only options are to write to stdout. printf is the function with which most people are familiar in this category. the first parameter is a format string that includes placeholders for the data to be printed. common format specifiers are the following 1. %s treat the argument as a c string pointer, keep printing all characters until the null-character is reached 2. %d prints the argument as an integer 3. %p print the argument as a memory address.	printf	122
for performance, printf buffers data until its cache is full or a newline is printed. here is an example of printing things out.	printf	17
from the previous section, printf calls the system call write. printf is a c library function, while write is a system call system.	system	44
from the previous section, printf calls the system call write. printf is a c library function, while write is a system call system.	printf	27
from the previous section, printf calls the system call write. printf is a c library function, while write is a system call system.	section	18
from the previous section, printf calls the system call write. printf is a c library function, while write is a system call system.	a system call	110
the buffering semantics of printf is a little complicated. iso defines three types of streams [5, p. 278]  unbuffered, where the contents of the stream reach their destination as soon as possible.	type	77
the buffering semantics of printf is a little complicated. iso defines three types of streams [5, p. 278]  unbuffered, where the contents of the stream reach their destination as soon as possible.	printf	27
standard error is defined as “not fully buffered” [5, p. 279]. standard output and input are merely defined to be fully buffered if and only if the stream destination is not an interactive device. usually, standard error will be unbuffered, standard input and output will be line buffered if the output is a terminal otherwise fully buffered.	a terminal	306
this relates to printf because printf merely uses the abstraction provided by the file interface and uses the above semantics to determine when to write. one can force a write by calling fflush() on the stream.	printf	16
to print strings and single characters, use puts(char *name ) and putchar(char c )	string	9
other streams to print to other file streams, use fprintf( _file_ , "hello %s, score: %d", name, score); where _file_ is either predefined (‘stdout’ or ‘stderr’) or a file pointer that was returned by fopen or fdopen. there is a printf equivalent that works with file descriptors, called dprintf. just use dprintf(int fd, char* format_string, ...);.	string	335
other streams to print to other file streams, use fprintf( _file_ , "hello %s, score: %d", name, score); where _file_ is either predefined (‘stdout’ or ‘stderr’) or a file pointer that was returned by fopen or fdopen. there is a printf equivalent that works with file descriptors, called dprintf. just use dprintf(int fd, char* format_string, ...);.	pointer	172
other streams to print to other file streams, use fprintf( _file_ , "hello %s, score: %d", name, score); where _file_ is either predefined (‘stdout’ or ‘stderr’) or a file pointer that was returned by fopen or fdopen. there is a printf equivalent that works with file descriptors, called dprintf. just use dprintf(int fd, char* format_string, ...);.	printf	51
to print data into a c string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. we would use sprintf the size of the printed string is less than the provided buffer – think about printing an integer, it will never be more than 11 characters with the nul byte. if printf is dealing with variadic input, it is safer to use the former function as shown in the following snippet.	the following	418
to print data into a c string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. we would use sprintf the size of the printed string is less than the provided buffer – think about printing an integer, it will never be more than 11 characters with the nul byte. if printf is dealing with variadic input, it is safer to use the former function as shown in the following snippet.	string	23
to print data into a c string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. we would use sprintf the size of the printed string is less than the provided buffer – think about printing an integer, it will never be more than 11 characters with the nul byte. if printf is dealing with variadic input, it is safer to use the former function as shown in the following snippet.	snippet	432
to print data into a c string, use sprintf or better snprintf. snprintf returns the number of characters written excluding the terminating byte. we would use sprintf the size of the printed string is less than the provided buffer – think about printing an integer, it will never be more than 11 characters with the nul byte. if printf is dealing with variadic input, it is safer to use the former function as shown in the following snippet.	printf	36
note that, unlike gets, fgets copies the newline into the buffer. on the other hand, one of the advantages of getline is that will automatically allocate and reallocate a buffer on the heap of sufficient size.	the heap	181
to have a library function parse input in addition to reading it, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a c string, respectively. all of those functions will return how many items were parsed. it is a good idea to check if the number is equal to the amount expected. also naturally like printf, scanf functions require valid pointers. instead of pointing to valid memory, they need to also be writable. it’s a common source of error to pass in an incorrect pointer value. for example,	string	174
to have a library function parse input in addition to reading it, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a c string, respectively. all of those functions will return how many items were parsed. it is a good idea to check if the number is equal to the amount expected. also naturally like printf, scanf functions require valid pointers. instead of pointing to valid memory, they need to also be writable. it’s a common source of error to pass in an incorrect pointer value. for example,	memory	430
to have a library function parse input in addition to reading it, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a c string, respectively. all of those functions will return how many items were parsed. it is a good idea to check if the number is equal to the amount expected. also naturally like printf, scanf functions require valid pointers. instead of pointing to valid memory, they need to also be writable. it’s a common source of error to pass in an incorrect pointer value. for example,	pointer	391
to have a library function parse input in addition to reading it, use scanf (or fscanf or sscanf) to get input from the default input stream, an arbitrary file stream or a c string, respectively. all of those functions will return how many items were parsed. it is a good idea to check if the number is equal to the amount expected. also naturally like printf, scanf functions require valid pointers. instead of pointing to valid memory, they need to also be writable. it’s a common source of error to pass in an incorrect pointer value. for example,	printf	353
we wanted to write the character value into c and the integer value into the malloc’d memory. however, we passed the address of the data pointer, not what the pointer is pointing to! so sscanf will change the pointer itself. the pointer will now point to address 10 so this code will later fail when free(data) is called.	code	274
we wanted to write the character value into c and the integer value into the malloc’d memory. however, we passed the address of the data pointer, not what the pointer is pointing to! so sscanf will change the pointer itself. the pointer will now point to address 10 so this code will later fail when free(data) is called.	address	117
we wanted to write the character value into c and the integer value into the malloc’d memory. however, we passed the address of the data pointer, not what the pointer is pointing to! so sscanf will change the pointer itself. the pointer will now point to address 10 so this code will later fail when free(data) is called.	memory	86
we wanted to write the character value into c and the integer value into the malloc’d memory. however, we passed the address of the data pointer, not what the pointer is pointing to! so sscanf will change the pointer itself. the pointer will now point to address 10 so this code will later fail when free(data) is called.	pointer	137
now, scanf will keep reading characters until the string ends. to stop scanf from causing a buffer overflow, use a format specifier. make sure to pass one less than the size of the buffer.	string	50
one last thing to note is if system calls are expensive, the scanf family is much more expensive due to compatibility reasons. since it needs to be able to process all of the printf specifiers correctly, the code isn’t efficient todo: citation needed. for highly performant programs, one should write the parsing themselves. if it is a one-off program or script, feel free to use scanf.	code	208
one last thing to note is if system calls are expensive, the scanf family is much more expensive due to compatibility reasons. since it needs to be able to process all of the printf specifiers correctly, the code isn’t efficient todo: citation needed. for highly performant programs, one should write the parsing themselves. if it is a one-off program or script, feel free to use scanf.	system	29
one last thing to note is if system calls are expensive, the scanf family is much more expensive due to compatibility reasons. since it needs to be able to process all of the printf specifiers correctly, the code isn’t efficient todo: citation needed. for highly performant programs, one should write the parsing themselves. if it is a one-off program or script, feel free to use scanf.	printf	175
3.5.4 string.h	string	6
string.h functions are a series of functions that deal with how to manipulate and check pieces of memory. most of them deal with c-strings. a c-string is a series of bytes delimited by a nul character which is equal to the byte 0x00. more information about all of these functions. any behavior missing from the documentation, such as the result of strlen(null) is considered undefined behavior.	string	0
string.h functions are a series of functions that deal with how to manipulate and check pieces of memory. most of them deal with c-strings. a c-string is a series of bytes delimited by a nul character which is equal to the byte 0x00. more information about all of these functions. any behavior missing from the documentation, such as the result of strlen(null) is considered undefined behavior.	memory	98
 int strlen(const char *s) returns the length of the string.	string	53
 int strcmp(const char *s1, const char *s2) returns an integer determining the lexicographic order of the strings. if s1 where to come before s2 in a dictionary, then a -1 is returned. if the two strings are equal, then 0. else, 1.	string	106
 char *strcpy(char *dest, const char *src) copies the string at src to dest. this function assumes dest has enough space for src otherwise undefined behavior  char *strcat(char *dest, const char *src) concatenates the string at src to the end of destination.	string	54
this function assumes that there is enough space for src at the end of destination including the nul byte  char *strdup(const char *dest) returns a malloc’d copy of the string.	string	169
 char *strchr(const char *haystack, int needle) returns a pointer to the first occurrence of needle in the haystack. if none found, null is returned.	pointer	58
 char *strstr(const char *haystack, const char *needle) same as above but this time a string!	string	86
 char *strtok(const char *str, const char *delims) a dangerous but useful function strtok takes a string and tokenizes it. meaning that it will transform the strings into separate strings. this function has a lot of specs so please read the man pages a contrived example is below.	string	98
#include <stdio.h> #include <string.h>	string	29
int main(){ char* upped = strdup("strtok,is,tricky,!!"); char* start = strtok(upped, ","); do{ printf("%s\n", start); }while((start = strtok(null, ","))); return 0; }	printf	95
why is it tricky? well what happens when upped is changed to the following?	the following	61
 for integer parsing use long int strtol(const char *nptr, char **endptr, int base); or long long int strt what these functions do is take the pointer to your string *nptr and a base (i.e. binary, octal, decimal, hexadecimal etc) and an optional pointer endptr and returns a parsed value.	string	159
 for integer parsing use long int strtol(const char *nptr, char **endptr, int base); or long long int strt what these functions do is take the pointer to your string *nptr and a base (i.e. binary, octal, decimal, hexadecimal etc) and an optional pointer endptr and returns a parsed value.	pointer	143
be careful though! error handling is tricky because the function won’t return an error code. if passed an invalid number string, it will return 0. the caller has to be careful from a valid 0 and an error. this often involves an errno trampoline as shown below.	code	87
be careful though! error handling is tricky because the function won’t return an error code. if passed an invalid number string, it will return 0. the caller has to be careful from a valid 0 and an error. this often involves an errno trampoline as shown below.	string	121
 void *memcpy(void *dest, const void *src, size_t n) moves n bytes starting at src to dest. be careful, there is undefined behavior when the memory regions overlap. this is one of the classic "this works on my machine!" examples because many times valgrind won’t be able to pick it up because it will look like it works on your machine. consider the safer version memmove.	memory	141
 void *memcpy(void *dest, const void *src, size_t n) moves n bytes starting at src to dest. be careful, there is undefined behavior when the memory regions overlap. this is one of the classic "this works on my machine!" examples because many times valgrind won’t be able to pick it up because it will look like it works on your machine. consider the safer version memmove.	valgrind	248
 void *memmove(void *dest, const void *src, size_t n) does the same thing as above, but if the memory regions overlap then it is guaranteed that all the bytes will get copied over correctly. memcpy and memmove both in string.h?	string	218
 void *memmove(void *dest, const void *src, size_t n) does the same thing as above, but if the memory regions overlap then it is guaranteed that all the bytes will get copied over correctly. memcpy and memmove both in string.h?	memory	95
3.6 c memory model	memory	6
the c memory model is probably unlike most that you’ve seen before. instead of allocating an object with type safety, we either use an automatic variable or request a sequence of bytes with malloc or another family member and later we free it.	memory	6
the c memory model is probably unlike most that you’ve seen before. instead of allocating an object with type safety, we either use an automatic variable or request a sequence of bytes with malloc or another family member and later we free it.	type	105
in low-level terms, a struct is a piece of contiguous memory, nothing more. just like an array, a struct has enough space to keep all of its members. but unlike an array, it can store different types. consider the contact struct declared above.	a struct	20
in low-level terms, a struct is a piece of contiguous memory, nothing more. just like an array, a struct has enough space to keep all of its members. but unlike an array, it can store different types. consider the contact struct declared above.	memory	54
in low-level terms, a struct is a piece of contiguous memory, nothing more. just like an array, a struct has enough space to keep all of its members. but unlike an array, it can store different types. consider the contact struct declared above.	type	194
we will often use the following typedef, so we can write use the struct name as the full type.	the following	18
we will often use the following typedef, so we can write use the struct name as the full type.	type	32
typedef struct contact contact;	type	0
contact person; typedef struct optional_name { ...	type	16
if you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.	code	19
if you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.	address	85
if you compile the code without any optimizations and reordering, you can expect the addresses of each of the variables to look like this.	optimizations	36
all your compiler does is say "reserve this much space". whenever a read or write occurs in the code, the compiler will calculate the offsets of the variable. the offsets are where the variable starts at. the phone variables starts at the 0x128th bytes and continues for sizeof(int) bytes with this compiler. offsets don’t determine where the variable ends though. consider the following hack seen in a lot of kernel code.	the following	374
all your compiler does is say "reserve this much space". whenever a read or write occurs in the code, the compiler will calculate the offsets of the variable. the offsets are where the variable starts at. the phone variables starts at the 0x128th bytes and continues for sizeof(int) bytes with this compiler. offsets don’t determine where the variable ends though. consider the following hack seen in a lot of kernel code.	code	96
currently, our memory looks like the following image. there is nothing in those boxes	the following	33
currently, our memory looks like the following image. there is nothing in those boxes	memory	15
figure 3.2: struct pointing to 11 boxes, 4 filled with 0006, 7 junk now, we can write a string to the end of our struct with the following call.	the following	125
figure 3.2: struct pointing to 11 boxes, 4 filled with 0006, 7 junk now, we can write a string to the end of our struct with the following call.	string	88
figure 3.3: struct pointing to 11 boxes, 4 filled with 0006, 7 the stirng “person” we can even do a sanity check to make sure that the strings are equal.	string	135
what that zero length array does is point to the end of the struct this means that the compiler will leave room for all of the elements calculated with respect to their size on the operating system (ints, chars, etc). the zero length array will take up no bytes of space. since structs are continuous pieces of memory, we can allocate more space than required and use the extra space as a place to store extra bytes. although this seems like a parlor trick,	memory	311
what that zero length array does is point to the end of the struct this means that the compiler will leave room for all of the elements calculated with respect to their size on the operating system (ints, chars, etc). the zero length array will take up no bytes of space. since structs are continuous pieces of memory, we can allocate more space than required and use the extra space as a place to store extra bytes. although this seems like a parlor trick,	system	191
it is an important optimization because to have a variable length string any other way, one would need to have two different memory allocation calls. this is highly inefficient for doing something as common in programming as is string manipulation.	string	66
it is an important optimization because to have a variable length string any other way, one would need to have two different memory allocation calls. this is highly inefficient for doing something as common in programming as is string manipulation.	memory	125
it is an important optimization because to have a variable length string any other way, one would need to have two different memory allocation calls. this is highly inefficient for doing something as common in programming as is string manipulation.	memory allocation	125
3.6.2 strings in c	string	6
in c, we have null terminated strings rather than length prefixed for historical reasons. for everyday programmers, remember to nul terminate your string! a string in c is defined as a bunch of bytes ended by ‘’ or the nul byte.	string	30
3.6.3 places for strings	string	17
whenever you define a string literal - one in the form char* str = "constant" – that string is stored in the data section. depending on your architecture, it is read-only, meaning that any attempt to modify the string will cause a segfault. one can also declare strings to be either in the writable data segment or the stack. to do so, specify a length for the string or put brackets instead of a pointer char str[] = "mutable" and put in the global scope or the function scope for the data segment or the stack respectively. if one, however, malloc’s space, one can change that string to be whatever they want. forgetting to nul terminate a string has a big effect on the strings! bounds checking is important. the heartbleed bug mentioned earlier in the book is partially because of this.	string	22
whenever you define a string literal - one in the form char* str = "constant" – that string is stored in the data section. depending on your architecture, it is read-only, meaning that any attempt to modify the string will cause a segfault. one can also declare strings to be either in the writable data segment or the stack. to do so, specify a length for the string or put brackets instead of a pointer char str[] = "mutable" and put in the global scope or the function scope for the data segment or the stack respectively. if one, however, malloc’s space, one can change that string to be whatever they want. forgetting to nul terminate a string has a big effect on the strings! bounds checking is important. the heartbleed bug mentioned earlier in the book is partially because of this.	pointer	397
whenever you define a string literal - one in the form char* str = "constant" – that string is stored in the data section. depending on your architecture, it is read-only, meaning that any attempt to modify the string will cause a segfault. one can also declare strings to be either in the writable data segment or the stack. to do so, specify a length for the string or put brackets instead of a pointer char str[] = "mutable" and put in the global scope or the function scope for the data segment or the stack respectively. if one, however, malloc’s space, one can change that string to be whatever they want. forgetting to nul terminate a string has a big effect on the strings! bounds checking is important. the heartbleed bug mentioned earlier in the book is partially because of this.	section	114
strings in c are represented as characters in memory. the end of the string includes a nul (0) byte. so "abc" requires four(4) bytes. the only way to find out the length of a c string is to keep reading memory until you find the nul byte. c characters are always exactly one byte each.	string	0
strings in c are represented as characters in memory. the end of the string includes a nul (0) byte. so "abc" requires four(4) bytes. the only way to find out the length of a c string is to keep reading memory until you find the nul byte. c characters are always exactly one byte each.	memory	46
string literals are constant a string literal is naturally constant. any write will cause the operating system to produce a segfault.	string	0
string literals are constant a string literal is naturally constant. any write will cause the operating system to produce a segfault.	system	104
string literals are character arrays stored in the code segment of the program, which is immutable. two string literals may share the same space in memory. an example follows.	code	51
string literals are character arrays stored in the code segment of the program, which is immutable. two string literals may share the same space in memory. an example follows.	string	0
string literals are character arrays stored in the code segment of the program, which is immutable. two string literals may share the same space in memory. an example follows.	memory	148
the strings pointed to by str1 and str2 may actually reside in the same location in memory.	string	4
the strings pointed to by str1 and str2 may actually reside in the same location in memory.	memory	84
char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. these following char arrays reside in different memory locations.	code	79
char arrays, however, contain the literal value which has been copied from the code segment into either the stack or static memory. these following char arrays reside in different memory locations.	memory	124
here are some common ways to initialize a string include. where do they reside in memory?	string	42
here are some common ways to initialize a string include. where do they reside in memory?	memory	82
we can also print out the pointer and the contents of a c-string easily. here is some boilerplate code to illustrate this.	code	98
we can also print out the pointer and the contents of a c-string easily. here is some boilerplate code to illustrate this.	string	58
we can also print out the pointer and the contents of a c-string easily. here is some boilerplate code to illustrate this.	pointer	26
unlike the array, however, we can change ptr to point to another piece of memory,	memory	74
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	the heap	53
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	address	27
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	string	86
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	memory	122
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	pointer	7
unlike pointers, that hold addresses to variables on the heap, or stack, char arrays (string literals) point to read-only memory located in the data section of the program. this means that pointers are more flexible than arrays, even though the name of an array is a pointer to its starting address.	section	149
in a more common case, pointers will point to heap memory in which case the memory referred to by the pointer can be modified.	memory	51
in a more common case, pointers will point to heap memory in which case the memory referred to by the pointer can be modified.	heap memory	46
in a more common case, pointers will point to heap memory in which case the memory referred to by the pointer can be modified.	pointer	23
3.7 pointers	pointer	4
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.	this section	178
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.	address	33
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.	memory	159
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.	pointer	0
pointers are variables that hold addresses. these addresses have a numeric value, but usually, programmers are interested in the value of the contents at that memory address. in this section, we will try to take you through a basic introduction to pointers.	section	183
3.7.1 pointer basics	pointer	6
declaring a pointer a pointer refers to a memory address. the type of the pointer is useful – it tells the compiler how many bytes need to be read/written and delineates the semantics for pointer arithmetic (addition and subtraction).	address	49
declaring a pointer a pointer refers to a memory address. the type of the pointer is useful – it tells the compiler how many bytes need to be read/written and delineates the semantics for pointer arithmetic (addition and subtraction).	memory	42
declaring a pointer a pointer refers to a memory address. the type of the pointer is useful – it tells the compiler how many bytes need to be read/written and delineates the semantics for pointer arithmetic (addition and subtraction).	type	62
declaring a pointer a pointer refers to a memory address. the type of the pointer is useful – it tells the compiler how many bytes need to be read/written and delineates the semantics for pointer arithmetic (addition and subtraction).	pointer	12
due to c’s syntax, an int* or any pointer is not actually its own type. you have to precede each pointer variable with an asterisk. as a common gotcha, the following	the following	152
due to c’s syntax, an int* or any pointer is not actually its own type. you have to precede each pointer variable with an asterisk. as a common gotcha, the following	type	66
due to c’s syntax, an int* or any pointer is not actually its own type. you have to precede each pointer variable with an asterisk. as a common gotcha, the following	pointer	34
will only declare *ptr3 as a pointer. ptr4 will actually be a regular int variable. to fix this declaration, ensure the * precedes the pointer.	pointer	29
keep this in mind for structs as well. if one declares without a typedef, then the pointer goes after the type.	type	65
keep this in mind for structs as well. if one declares without a typedef, then the pointer goes after the type.	pointer	83
reading / writing with pointers let’s say that int *ptr was declared. for the sake of discussion, let us assume that ptr contains the memory address 0x1000. to write to the pointer, it must be dereferenced and assigned a value.	address	141
reading / writing with pointers let’s say that int *ptr was declared. for the sake of discussion, let us assume that ptr contains the memory address 0x1000. to write to the pointer, it must be dereferenced and assigned a value.	memory	134
reading / writing with pointers let’s say that int *ptr was declared. for the sake of discussion, let us assume that ptr contains the memory address 0x1000. to write to the pointer, it must be dereferenced and assigned a value.	pointer	23
what c does is take the type of the pointer which is an int and write sizeof(int) bytes from the start of the pointer, meaning that bytes 0x1000, 0x1001, 0x1002, 0x1003 will all be zero. the number of bytes written depends on the pointer type. it is the same for all primitive types but structs are a little different.	type	24
what c does is take the type of the pointer which is an int and write sizeof(int) bytes from the start of the pointer, meaning that bytes 0x1000, 0x1001, 0x1002, 0x1003 will all be zero. the number of bytes written depends on the pointer type. it is the same for all primitive types but structs are a little different.	pointer	36
reading and writing to non-primitive types gets tricky. the compilation unit - usually the file or a header needs to have the size of the data structure readily available. this means that opaque data structures can’t be copied. here is an example of assigning a struct pointer:	a struct	141
reading and writing to non-primitive types gets tricky. the compilation unit - usually the file or a header needs to have the size of the data structure readily available. this means that opaque data structures can’t be copied. here is an example of assigning a struct pointer:	type	37
reading and writing to non-primitive types gets tricky. the compilation unit - usually the file or a header needs to have the size of the data structure readily available. this means that opaque data structures can’t be copied. here is an example of assigning a struct pointer:	pointer	269
reading and writing to non-primitive types gets tricky. the compilation unit - usually the file or a header needs to have the size of the data structure readily available. this means that opaque data structures can’t be copied. here is an example of assigning a struct pointer:	the data structure	134
#include <stdio.h> typedef struct { int a1; int a2; } pair; int main() { pair obj; pair zeros; zeros.a1 = 0; zeros.a2 = 0; pair *ptr = &obj; obj.a1 = 1; obj.a2 = 2; *ptr = zeros; printf("a1: %d, a2: %d\n", ptr->a1, ptr->a2); return 0; }	type	19
#include <stdio.h> typedef struct { int a1; int a2; } pair; int main() { pair obj; pair zeros; zeros.a1 = 0; zeros.a2 = 0; pair *ptr = &obj; obj.a1 = 1; obj.a2 = 2; *ptr = zeros; printf("a1: %d, a2: %d\n", ptr->a1, ptr->a2); return 0; }	printf	179
as for reading structure pointers, don’t do it directly. instead, programmers create abstractions for creating, copying, and destroying structs. if this sounds familiar, it is what c++ originally intended to do before the standards committee went off the deep end.	pointer	25
3.7.2 pointer arithmetic	pointer	6
in addition to adding to an integer, pointers can be added to. however, the pointer type is used to determine how much to increment the pointer. a pointer is moved over by the value added times the size of the underlying type.	type	84
in addition to adding to an integer, pointers can be added to. however, the pointer type is used to determine how much to increment the pointer. a pointer is moved over by the value added times the size of the underlying type.	pointer	37
for char pointers, this is trivial because characters are always one byte.	pointer	9
notice how only ’efgh’ is printed. why is that? well as mentioned above, when performing ’bna+=1’ we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte) because pointer arithmetic in c is always automatically scaled by the size of the type that is pointed to, posix standards forbid arithmetic on void pointers. having said that, compilers will often treat the underlying type as char. here is a machine translation. the following two pointer arithmetic operations are equal	the following	518
notice how only ’efgh’ is printed. why is that? well as mentioned above, when performing ’bna+=1’ we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte) because pointer arithmetic in c is always automatically scaled by the size of the type that is pointed to, posix standards forbid arithmetic on void pointers. having said that, compilers will often treat the underlying type as char. here is a machine translation. the following two pointer arithmetic operations are equal	system	177
notice how only ’efgh’ is printed. why is that? well as mentioned above, when performing ’bna+=1’ we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte) because pointer arithmetic in c is always automatically scaled by the size of the type that is pointed to, posix standards forbid arithmetic on void pointers. having said that, compilers will often treat the underlying type as char. here is a machine translation. the following two pointer arithmetic operations are equal	type	336
notice how only ’efgh’ is printed. why is that? well as mentioned above, when performing ’bna+=1’ we are increasing the **integer** pointer by 1, (translates to 4 bytes on most systems) which is equivalent to 4 characters (each character is only 1 byte) because pointer arithmetic in c is always automatically scaled by the size of the type that is pointed to, posix standards forbid arithmetic on void pointers. having said that, compilers will often treat the underlying type as char. here is a machine translation. the following two pointer arithmetic operations are equal	pointer	132
every time you do pointer arithmetic, take a deep breath and make sure that you are shifting over the number of bytes you think you are shifting over.	pointer	18
3.7.3 so what is a void pointer?	pointer	24
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.	code	125
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.	address	229
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.	memory	222
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.	type	38
a void pointer is a pointer without a type. void pointers are used when either the datatype is unknown or when interfacing c code with other programming languages without apis. you can think of this as a raw pointer, or a memory address. malloc by default returns a void pointer that can be safely promoted to any other type.	pointer	7
void *give_me_space = malloc(10); char *string = give_me_space;	string	40
c automatically promotes void* to its appropriate type. gcc and clang are not totally iso c compliant, meaning that they will permit arithmetic on a void pointer. they will treat it as a char pointer. do not do this because it is not portable - it is not guaranteed to work with all compilers!	type	50
c automatically promotes void* to its appropriate type. gcc and clang are not totally iso c compliant, meaning that they will permit arithmetic on a void pointer. they will treat it as a char pointer. do not do this because it is not portable - it is not guaranteed to work with all compilers!	pointer	154
what’s wrong with this code?	code	23
in the above code it simply changes the dest pointer to point to source string. also the nul bytes are not copied. here is a better version -	code	13
in the above code it simply changes the dest pointer to point to source string. also the nul bytes are not copied. here is a better version -	string	72
in the above code it simply changes the dest pointer to point to source string. also the nul bytes are not copied. here is a better version -	pointer	45
note that it is also common to see the following kind of implementation, which does everything inside the expression test, including copying the nul byte. however, this is bad style, as a result of doing multiple operations in the same line.	the following	35
the fix is first to write correct programs! secondly, it is a good habit to set pointers to null, once the memory has been freed. this ensures that the pointer cannot be used incorrectly without the program crashing.	memory	107
the fix is first to write correct programs! secondly, it is a good habit to set pointers to null, once the memory has been freed. this ensures that the pointer cannot be used incorrectly without the program crashing.	pointer	80
3.8.3 returning pointers to automatic variables	pointer	16
automatic variables are bound to stack memory only for the lifetime of the function. after the function returns, it is an error to continue to use the memory.	memory	39
3.8.4 insufficient memory allocation	memory	19
3.8.4 insufficient memory allocation	memory allocation	19
struct user { char name[100]; }; typedef struct user user_t; user_t *user = (user_t *) malloc(sizeof(user));	type	33
in the above example, we needed to allocate enough bytes for the struct. instead, we allocated enough bytes to hold a pointer. once we start using the user pointer we will corrupt memory. the correct code is shown below.	code	200
in the above example, we needed to allocate enough bytes for the struct. instead, we allocated enough bytes to hold a pointer. once we start using the user pointer we will corrupt memory. the correct code is shown below.	memory	180
in the above example, we needed to allocate enough bytes for the struct. instead, we allocated enough bytes to hold a pointer. once we start using the user pointer we will corrupt memory. the correct code is shown below.	pointer	118
struct user { char name[100]; }; typedef struct user user_t; user_t * user = (user_t *) malloc(sizeof(user_t));	type	33
a famous example: heart bleed performed a memcpy into a buffer that was of insufficient size. a simple example: implement a strcpy and forget to add one to strlen, when determining the size of the memory required.	memory	197
c fails to check if pointers are valid. the above example writes into array[10] which is outside the array bounds. this can cause memory corruption because that memory location is probably being used for something else. in practice, this can be harder to spot because the overflow/underflow may occur in a library call. here is our old friend gets.	memory	130
c fails to check if pointers are valid. the above example writes into array[10] which is outside the array bounds. this can cause memory corruption because that memory location is probably being used for something else. in practice, this can be harder to spot because the overflow/underflow may occur in a library call. here is our old friend gets.	pointer	20
3.8.6 strings require strlen(s)+1 bytes	string	6
every string must have a nul byte after the last characters. to store the string “hi” it takes 3 bytes: [h] [i] [\0].	string	6
char *strdup(const char *input) {/* return a copy of ’input’ */ char *copy; copy = malloc(sizeof(char*)); /* nope! this allocates space for a pointer, not a string */ copy = malloc(strlen(input)); /* almost...but what about the null terminator? */ copy = malloc(strlen(input) + 1); /* that’s right. */ strcpy(copy, input); /* strcpy will provide the null terminator */ return copy; }	string	157
char *strdup(const char *input) {/* return a copy of ’input’ */ char *copy; copy = malloc(sizeof(char*)); /* nope! this allocates space for a pointer, not a string */ copy = malloc(strlen(input)); /* almost...but what about the null terminator? */ copy = malloc(strlen(input) + 1); /* that’s right. */ strcpy(copy, input); /* strcpy will provide the null terminator */ return copy; }	pointer	142
automatic variables hold garbage or bit pattern that happened to be in memory or register. it is an error to assume that it will always be initialized to zero.	memory	71
3.8.8 assuming uninitialized memory will be zeroed	memory	29
if (answer = 42) {printf("the answer is %d", answer);}	printf	18
if (42 = answer) {printf("the answer is %d", answer);}	printf	18
this piece of code calls getline, and assigns the return value or the number of bytes read to nread. it also in the same line checks if that value is -1 and if so terminates the loop. it is always good practice to put parentheses around any assignment condition.	code	14
this piece of code calls getline, and assigns the return value or the number of bytes read to nread. it also in the same line checks if that value is -1 and if so terminates the loop. it is always good practice to put parentheses around any assignment condition.	the loop	174
3.9.2 undeclared or incorrectly prototyped functions	type	37
some snippets of code may do the following.	the following	29
some snippets of code may do the following.	code	17
some snippets of code may do the following.	snippet	5
the system function ‘time’ actually takes a parameter a pointer to some memory that can receive the time_t structure or null. the compiler fails to catch this error because the programmer omitted the valid function prototype by including time.h.	memory	72
the system function ‘time’ actually takes a parameter a pointer to some memory that can receive the time_t structure or null. the compiler fails to catch this error because the programmer omitted the valid function prototype by including time.h.	parameter	44
the system function ‘time’ actually takes a parameter a pointer to some memory that can receive the time_t structure or null. the compiler fails to catch this error because the programmer omitted the valid function prototype by including time.h.	system	4
the system function ‘time’ actually takes a parameter a pointer to some memory that can receive the time_t structure or null. the compiler fails to catch this error because the programmer omitted the valid function prototype by including time.h.	type	220
the system function ‘time’ actually takes a parameter a pointer to some memory that can receive the time_t structure or null. the compiler fails to catch this error because the programmer omitted the valid function prototype by including time.h.	pointer	56
more confusingly this could compile, work for decades and then crash. the reason for that is that time would be found at link time, not compile-time in the c standard library which almost surely is already in memory. since a parameter isn’t being passed, we are hoping the arguments on the stack (any garbage) is zeroed out because if it isn’t, time will try to write the result of the function to that garbage which will cause the program to segfault.	memory	209
more confusingly this could compile, work for decades and then crash. the reason for that is that time would be found at link time, not compile-time in the c standard library which almost surely is already in memory. since a parameter isn’t being passed, we are hoping the arguments on the stack (any garbage) is zeroed out because if it isn’t, time will try to write the result of the function to that garbage which will cause the program to segfault.	parameter	225
however, the following code is perfectly ok.	the following	9
however, the following code is perfectly ok.	code	23
for(int i = 0; i < 5; i++){ printf("%d\n", i);;;;;;;;;;;;; }	printf	28
it is ok to have this kind of code because the c language uses semicolons (;) to separate statements. if there is no statement in between semicolons, then there is nothing to do and the compiler moves on to the next statement to save a lot of confusion, always use braces. it increases the number of lines of code, which is a great productivity metric.	code	30
 c-strings representation  c-strings as pointers  char p[]vs char* p  simple c string functions (strcmp, strcat, strcpy)	string	3
 c-strings representation  c-strings as pointers  char p[]vs char* p  simple c string functions (strcmp, strcat, strcpy)	pointer	40
 sizeof char  sizeof x vs x*  heap memory lifetime  calls to heap allocation  dereferencing pointers  address-of operator  pointer arithmetic  string duplication  string truncation  double-free error  string literals  print formatting.	address	102
 sizeof char  sizeof x vs x*  heap memory lifetime  calls to heap allocation  dereferencing pointers  address-of operator  pointer arithmetic  string duplication  string truncation  double-free error  string literals  print formatting.	string	143
 sizeof char  sizeof x vs x*  heap memory lifetime  calls to heap allocation  dereferencing pointers  address-of operator  pointer arithmetic  string duplication  string truncation  double-free error  string literals  print formatting.	memory	35
 sizeof char  sizeof x vs x*  heap memory lifetime  calls to heap allocation  dereferencing pointers  address-of operator  pointer arithmetic  string duplication  string truncation  double-free error  string literals  print formatting.	heap memory	30
 sizeof char  sizeof x vs x*  heap memory lifetime  calls to heap allocation  dereferencing pointers  address-of operator  pointer arithmetic  string duplication  string truncation  double-free error  string literals  print formatting.	pointer	92
 memory out of bounds errors  static memory  file input / output. posix vs. c library  c input output: fprintf and printf  posix file io (read, write, open)  buffering of stdout	memory	1
 memory out of bounds errors  static memory  file input / output. posix vs. c library  c input output: fprintf and printf  posix file io (read, write, open)  buffering of stdout	printf	104
 what does the following print out?	the following	11
int main(){ fprintf(stderr, fprintf(stdout, fprintf(stderr, fprintf(stdout, return 0; }	printf	13
 what are the differences between the following two declarations? what does sizeof return for one of them?	the following	34
 what is a string in c?	string	11
 code up a simple my_strcmp. how about my_strcat, my_strcpy, or my_strdup? bonus: code the functions while only going through the strings once.	code	1
 code up a simple my_strcmp. how about my_strcat, my_strcpy, or my_strdup? bonus: code the functions while only going through the strings once.	string	130
 what should each of the following lines usually return?	the following	21
 what is malloc? how is it different from calloc. once memory is allocated how can we use realloc?	memory	55
 pointer arithmetic. assume the following addresses. what are the following shifts?	the following	28
 pointer arithmetic. assume the following addresses. what are the following shifts?	address	42
 pointer arithmetic. assume the following addresses. what are the following shifts?	pointer	1
 what is the printf specifier to print a string, int, or char?	string	41
 what is the printf specifier to print a string, int, or char?	printf	13
 is the following code valid? why? where is output located?	the following	4
 is the following code valid? why? where is output located?	code	18
char *foo(int var){ static char output[20]; snprintf(output, 20, "%d", var); return output; }	printf	46
 write a function that accepts a path as a string, and opens that file, prints the file contents 40 bytes at a time but, every other print reverses the string (try using the posix api for this).	string	43
3.12 rapid fire: pointer arithmetic	pointer	17
pointer arithmetic is important! take a deep breath and figure out how many bytes each operation moves a pointer. the following is a rapid fire section. we’ll use the following definitions:	the following	114
pointer arithmetic is important! take a deep breath and figure out how many bytes each operation moves a pointer. the following is a rapid fire section. we’ll use the following definitions:	pointer	0
pointer arithmetic is important! take a deep breath and figure out how many bytes each operation moves a pointer. the following is a rapid fire section. we’ll use the following definitions:	section	144
how many bytes are moved over from the following additions?	the following	35
museum.uq.edu.au/pdf/dec-10-afdo-d%20decsystem10%20fortran%20iv%20programmer%27s% 20reference%20manual.pdf.	system	40
intel marketing on meltdown and spectre	spectre	32
to understand what a process is, you need to understand what an operating system is. an operating system is a program that provides an interface between hardware and user software as well as providing a set of tools that the software can use. the operating system manages hardware and gives user programs a uniform way of interacting with hardware as long as the operating system can be installed on that hardware. although this idea sounds like it is the end-all, we know that there are many different operating systems with their own quirks and standards. as a solution to that, there is another layer of abstraction: posix or portable operating systems interface. this is a standard (or many standards now) that an operating system must implement to be posix compatible – most systems that we’ll be studying are almost posix compatible due more to political reasons.	a process	19
to understand what a process is, you need to understand what an operating system is. an operating system is a program that provides an interface between hardware and user software as well as providing a set of tools that the software can use. the operating system manages hardware and gives user programs a uniform way of interacting with hardware as long as the operating system can be installed on that hardware. although this idea sounds like it is the end-all, we know that there are many different operating systems with their own quirks and standards. as a solution to that, there is another layer of abstraction: posix or portable operating systems interface. this is a standard (or many standards now) that an operating system must implement to be posix compatible – most systems that we’ll be studying are almost posix compatible due more to political reasons.	system	74
before we talk about posix systems, we should understand what the idea of a kernel is generally. in an operating system (os), there are two spaces: kernel space and user space. kernel space is a power operating mode that allows the system to interact with the hardware and has the potential to destroy your machine. user space is where most applications run because they don’t need this level of power for every operation. when a user space program needs additional power, it interacts with the hardware through a system call that is conducted by the kernel. this adds a layer of security so that normal user programs can’t destroy your entire operating system.	system	27
before we talk about posix systems, we should understand what the idea of a kernel is generally. in an operating system (os), there are two spaces: kernel space and user space. kernel space is a power operating mode that allows the system to interact with the hardware and has the potential to destroy your machine. user space is where most applications run because they don’t need this level of power for every operation. when a user space program needs additional power, it interacts with the hardware through a system call that is conducted by the kernel. this adds a layer of security so that normal user programs can’t destroy your entire operating system.	a system call	512
for the purposes of our class, we’ll talk about single machine multiple user operating systems. this is where there is a central clock on a standard laptop or desktop. other oses relax the central clock requirement (distributed) or the “standardness” of the hardware (embedded systems). other invariants make sure events happen at particular times too.	system	87
the operating system is made up of many different pieces. there may be a program running to handle incoming usb connections, another one to stay connected to the network, etc. the most important one is the kernel – although it might be a set of processes – which is the heart of the operating system. the kernel has many important tasks. the first of which is booting.	system	14
1. the computer hardware executes code from read-only memory, called firmware.	code	34
1. the computer hardware executes code from read-only memory, called firmware.	memory	54
2. the firmware executes a bootloader, which often conforms to the extensible firmware interface (efi), which is an interface between the system firmware and the operating system.	system	138
3. the bootloader’s boot manager loads the operating system kernels, based on the boot settings.	system	53
 scheduling processes and threads  handling synchronization primitives (futexes, mutexes, semaphores, etc.)  providing system calls such as write or read  managing virtual memory and low-level binary devices such as usb drivers  managing filesystems  handling communication over networks  handling communication between processes  dynamically linking libraries  the list goes on and on.	thread	26
 scheduling processes and threads  handling synchronization primitives (futexes, mutexes, semaphores, etc.)  providing system calls such as write or read  managing virtual memory and low-level binary devices such as usb drivers  managing filesystems  handling communication over networks  handling communication between processes  dynamically linking libraries  the list goes on and on.	memory	172
 scheduling processes and threads  handling synchronization primitives (futexes, mutexes, semaphores, etc.)  providing system calls such as write or read  managing virtual memory and low-level binary devices such as usb drivers  managing filesystems  handling communication over networks  handling communication between processes  dynamically linking libraries  the list goes on and on.	system	119
the kernel creates the first process init.d (an alternative is system.d). init.d boots up programs such as graphical user interfaces, terminals, etc – by default, this is the only process explicitly created by the system. all other processes are instantiated by using the system calls fork and exec from that single process.	system	63
the kernel keeps track of the file descriptors and what they point to. later we will learn two things: that file descriptors point to more than files and that the operating system keeps track of them.	system	173
notice that file descriptors may be reused between processes, but inside of a process, they are unique. file descriptors may have a notion of position. these are known as seekable streams. a program can read a file on disk completely because the os keeps track of the position in the file, an attribute that belongs to your process as well.	a process	76
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.	the following	239
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.	resources	88
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.	a process	0
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.	system	297
a process is an instance of a computer program that may be running. processes have many resources at their disposal. at the start of each program, a program gets one process, but each program can make more processes. a program consists of the following:  a binary format: this tells the operating system about the various sections of bits in the binary – which parts are executable, which parts are constants, which libraries to include etc.	section	322
 a number denoting which instruction to start from  constants  libraries to link and where to fill in the address of those libraries processes are powerful, but they are isolated!	address	106
this is important because in complex systems (like the university of illinois engineering workstations), it is likely that different processes will have different privileges. one certainly doesn’t want the average user to be able to bring down the entire system, by either purposely or accidentally modifying a process. as most of you have realized by now, if you stuck the following code snippet into a program, the variables are unshared between two parallel invocations of the program.	the following	370
this is important because in complex systems (like the university of illinois engineering workstations), it is likely that different processes will have different privileges. one certainly doesn’t want the average user to be able to bring down the entire system, by either purposely or accidentally modifying a process. as most of you have realized by now, if you stuck the following code snippet into a program, the variables are unshared between two parallel invocations of the program.	code	384
this is important because in complex systems (like the university of illinois engineering workstations), it is likely that different processes will have different privileges. one certainly doesn’t want the average user to be able to bring down the entire system, by either purposely or accidentally modifying a process. as most of you have realized by now, if you stuck the following code snippet into a program, the variables are unshared between two parallel invocations of the program.	a process	309
this is important because in complex systems (like the university of illinois engineering workstations), it is likely that different processes will have different privileges. one certainly doesn’t want the average user to be able to bring down the entire system, by either purposely or accidentally modifying a process. as most of you have realized by now, if you stuck the following code snippet into a program, the variables are unshared between two parallel invocations of the program.	system	37
this is important because in complex systems (like the university of illinois engineering workstations), it is likely that different processes will have different privileges. one certainly doesn’t want the average user to be able to bring down the entire system, by either purposely or accidentally modifying a process. as most of you have realized by now, if you stuck the following code snippet into a program, the variables are unshared between two parallel invocations of the program.	snippet	389
int secrets; secrets++; printf("%d\n", secrets);	printf	24
on two different terminals, they would both print out 1 not 2. even if we changed the code to attempt to affect other process instances, there would be no way to change another process’ state unintentionally. however, there are other intentional ways to change the program states of other processes.	code	86
4.3.1 memory layout	memory	6
when a process starts, it gets its own address space. each process gets the following.	the following	72
when a process starts, it gets its own address space. each process gets the following.	address	39
when a process starts, it gets its own address space. each process gets the following.	a process	5
 a stack the stack is the place where automatically allocated variables and function call return addresses are stored.	address	97
every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. this segment of the stack is writable but not executable. this behavior is controlled by the no-execute (nx) bit, sometimes called the wx̂ (write xor execute) bit, which helps prevent malicious code, such as shellcode from being run on the stack.	code	309
every time a new variable is declared, the program moves the stack pointer down to reserve space for the variable. this segment of the stack is writable but not executable. this behavior is controlled by the no-execute (nx) bit, sometimes called the wx̂ (write xor execute) bit, which helps prevent malicious code, such as shellcode from being run on the stack.	pointer	67
if the stack grows too far – meaning that it either grows beyond a preset boundary or intersects the heap – the program will stack overflow error, most likely resulting in a segfault. the stack is statically allocated by default; there is only a certain amount of space to which one can write.	the heap	97
 a heap the heap is a contiguous, expanding region of memory [5]. if a program wants to allocate an object whose lifetime is manually controlled or whose size cannot be determined at compile-time, it would want to create a heap variable.	the heap	8
 a heap the heap is a contiguous, expanding region of memory [5]. if a program wants to allocate an object whose lifetime is manually controlled or whose size cannot be determined at compile-time, it would want to create a heap variable.	memory	54
the heap starts at the top of the text segment and grows upward, meaning malloc may push the heap boundary – called the program break – upward.	the heap	0
we will explore this in more depth in our chapter on memory allocation. this area is also writable but not executable. one can run out of heap memory if the system is constrained or if a program run out of addresses, a phenomenon that is more common on a 32-bit system.	address	206
we will explore this in more depth in our chapter on memory allocation. this area is also writable but not executable. one can run out of heap memory if the system is constrained or if a program run out of addresses, a phenomenon that is more common on a 32-bit system.	memory	53
we will explore this in more depth in our chapter on memory allocation. this area is also writable but not executable. one can run out of heap memory if the system is constrained or if a program run out of addresses, a phenomenon that is more common on a 32-bit system.	heap memory	138
we will explore this in more depth in our chapter on memory allocation. this area is also writable but not executable. one can run out of heap memory if the system is constrained or if a program run out of addresses, a phenomenon that is more common on a 32-bit system.	system	157
we will explore this in more depth in our chapter on memory allocation. this area is also writable but not executable. one can run out of heap memory if the system is constrained or if a program run out of addresses, a phenomenon that is more common on a 32-bit system.	memory allocation	53
 a data segment this segment contains two parts, an initialized data segment, and an uninitialized segment. furthermore, the initialized data segment is divided into a readable and writable section.	section	190
this section starts at the end of the text segment and starts at a constant size because the number of globals is known at compile time. the end of the data segment is called the program break and can be extended via the use of brk / sbrk.	this section	0
this section starts at the end of the text segment and starts at a constant size because the number of globals is known at compile time. the end of the data segment is called the program break and can be extended via the use of brk / sbrk.	section	5
this section is writable [10, p. 124]. most notably, this section contains variables that were initialized with a static initializer, as follows:	this section	0
this section is writable [10, p. 124]. most notably, this section contains variables that were initialized with a static initializer, as follows:	section	5
– uninitialized data segment / bss bss stands for an old assembler operator known as block started by symbol.	block	85
this variable will be zeroed; otherwise, we would have a security risk involving isolation from other processes. they get put in a different section to speed up process start up time. this section starts at the end of the data segment and is also static in size because the amount of globals is known at compile time. currently, both the initialized and bss data segments are combined and referred to as the data segment [10, p. 124], despite being somewhat different in purpose.	this section	184
this variable will be zeroed; otherwise, we would have a security risk involving isolation from other processes. they get put in a different section to speed up process start up time. this section starts at the end of the data segment and is also static in size because the amount of globals is known at compile time. currently, both the initialized and bss data segments are combined and referred to as the data segment [10, p. 124], despite being somewhat different in purpose.	section	141
 a text segment this is where all executable instructions are stored, and is readable (function pointers) but not writable.	pointer	96
the program counter moves through this segment executing instructions one after the other. it is important to note that this is the only executable section of the program, by default. if a program’s code while it’s running, the program most likely will segfault. there are ways around it, but we will not be exploring these in this course. why doesn’t it always start at zero? this is because of a security feature called address space layout randomization. the reasons for and explanation about this is outside the scope of this class, but it is good to know about its existence. having said that, this address can be made constant, if a program is compiled with the debug flag.	code	199
the program counter moves through this segment executing instructions one after the other. it is important to note that this is the only executable section of the program, by default. if a program’s code while it’s running, the program most likely will segfault. there are ways around it, but we will not be exploring these in this course. why doesn’t it always start at zero? this is because of a security feature called address space layout randomization. the reasons for and explanation about this is outside the scope of this class, but it is good to know about its existence. having said that, this address can be made constant, if a program is compiled with the debug flag.	address	422
the program counter moves through this segment executing instructions one after the other. it is important to note that this is the only executable section of the program, by default. if a program’s code while it’s running, the program most likely will segfault. there are ways around it, but we will not be exploring these in this course. why doesn’t it always start at zero? this is because of a security feature called address space layout randomization. the reasons for and explanation about this is outside the scope of this class, but it is good to know about its existence. having said that, this address can be made constant, if a program is compiled with the debug flag.	section	148
heap data text figure 4.1: process address space	address	35
to keep track of all these processes, your operating system gives each process a number called the process id (pid). processes are also given the pid of their parent process, called parent process id (ppid). every process has a parent, that parent could be init.d.	system	53
processes could also contain the following information:  running state - whether a process is getting ready, running, stopped, terminated, etc. (more on this is covered in the chapter on scheduling).	the following	29
processes could also contain the following information:  running state - whether a process is getting ready, running, stopped, terminated, etc. (more on this is covered in the chapter on scheduling).	a process	81
there are tricks to make a program take a different user than who started the program i.e. sudo takes a program that a user starts and executes it as root. more specifically, a process has a real user id (identifies the owner of the process), an effective user id (used for non-privileged users trying to access files only accessible by superusers), and a saved user id (used when privileged users perform non-privileged actions).	a process	175
 arguments - a list of strings that tell your program what parameters to run under.	string	23
 arguments - a list of strings that tell your program what parameters to run under.	parameter	59
 environment variables - a list of key-value pair strings in the form name=value that one can modify. these are often used to specify paths to libraries and binaries, program configuration settings, etc.	string	50
according to the posix specification, a process only needs a thread and address space, but most kernel developers and users know that only these aren’t enough [6].	thread	61
according to the posix specification, a process only needs a thread and address space, but most kernel developers and users know that only these aren’t enough [6].	address	72
according to the posix specification, a process only needs a thread and address space, but most kernel developers and users know that only these aren’t enough [6].	a process	38
according to the posix specification, a process only needs a thread and address space, but most kernel developers and users know that only these aren’t enough [6].	a thread	59
process forking is a powerful and dangerous tool. if you make a mistake resulting in a fork bomb, you can bring down an entire system. to reduce the chances of this, limit your maximum number of processes to a small number e.g. 40 by typing ulimit -u 40 into a command line. note, this limit is only for the user, which means if you fork bomb, then you won’t be able to kill all created process since calling killall requires your shell to fork(). quite unfortunate. one solution is to spawn another shell instance as another user (for example root) beforehand and kill processes from there.	system	127
process forking is a powerful and dangerous tool. if you make a mistake resulting in a fork bomb, you can bring down an entire system. to reduce the chances of this, limit your maximum number of processes to a small number e.g. 40 by typing ulimit -u 40 into a command line. note, this limit is only for the user, which means if you fork bomb, then you won’t be able to kill all created process since calling killall requires your shell to fork(). quite unfortunate. one solution is to spawn another shell instance as another user (for example root) beforehand and kill processes from there.	process forking	0
finally, you could reboot the system, but you only have one shot at this with the exec function.	system	30
when testing fork() code, ensure that you have either root and/or physical access to the machine involved. if you must work on fork() code remotely, remember that kill -9 -1 will save you in the event of an emergency. fork can be extremely dangerous if you aren’t prepared for it. you have been warned.	code	20
the fork system call clones the current process to create a new process, called a child process. this occurs by duplicating the state of the existing process with a few minor differences.	system	9
 just as a side remark, in older unix systems, the entire address space of the parent process was directly copied regardless of whether the resource was modified or not. the current behavior is for the kernel to perform a copy-on-write, which saves a lot of resources, while being time efficient [7, copy-on-write section].	resources	258
 just as a side remark, in older unix systems, the entire address space of the parent process was directly copied regardless of whether the resource was modified or not. the current behavior is for the kernel to perform a copy-on-write, which saves a lot of resources, while being time efficient [7, copy-on-write section].	address	58
 just as a side remark, in older unix systems, the entire address space of the parent process was directly copied regardless of whether the resource was modified or not. the current behavior is for the kernel to perform a copy-on-write, which saves a lot of resources, while being time efficient [7, copy-on-write section].	system	38
 just as a side remark, in older unix systems, the entire address space of the parent process was directly copied regardless of whether the resource was modified or not. the current behavior is for the kernel to perform a copy-on-write, which saves a lot of resources, while being time efficient [7, copy-on-write section].	section	314
printf("this line twice!\n");	printf	0
here is a simple example of this address space cloning. the following program may print out 42 twice - but the fork() is after the printf!? why?	the following	56
here is a simple example of this address space cloning. the following program may print out 42 twice - but the fork() is after the printf!? why?	address	33
here is a simple example of this address space cloning. the following program may print out 42 twice - but the fork() is after the printf!? why?	printf	131
#include <unistd.h> /*fork declared here*/ #include <stdio.h> /* printf declared here*/ int main() {	printf	65
int answer = 84 >> 1; printf("answer: %d", answer); fork(); return 0; }	printf	22
the printf line is executed only once however notice that the printed contents are not flushed to standard out. there’s no newline printed, we didn’t call fflush, or change the buffering mode. the output text is therefore still in process memory waiting to be sent. when fork() is executed the entire process memory is duplicated including the buffer. thus, the child process starts with a non-empty output buffer which may be flushed when the program exits. we say may because the contents may be unwritten given a bad program exit as well.	memory	239
the printf line is executed only once however notice that the printed contents are not flushed to standard out. there’s no newline printed, we didn’t call fflush, or change the buffering mode. the output text is therefore still in process memory waiting to be sent. when fork() is executed the entire process memory is duplicated including the buffer. thus, the child process starts with a non-empty output buffer which may be flushed when the program exits. we say may because the contents may be unwritten given a bad program exit as well.	printf	4
to write code that is different for the parent and child process, check the return value of fork(). if fork() returns -1, that implies something went wrong in the process of creating a new child. one should check the value stored in errno to determine what kind of error occurred. common errors include eagain and enoent which are essentially "try again – resource temporarily unavailable", and "no such file or directory".	code	9
while (--argc && (id=fork())) { waitpid(id,&status,0); /* wait for child*/ } printf("%d:%s\n", argc, argv[argc]); return 0; }	printf	77
another example is below. this is the amazing parallel apparent-o(n) sleepsort is today’s silly winner. first published on 4chan in 2011. a version of this awful but amusing sorting algorithm is shown below. this sorting algorithm may fail to produce the correct output.	sleepsort	69
int main(int c, char **v) { while (--c > 1 && !fork()); int val = atoi(v[c]); sleep(val); printf("%d\n", val); return 0; }	printf	90
figure 4.2: timing of sorting 1, 3, 2, 4 the algorithm isn’t actually o(n) because of how the system scheduler works. in essence, this program outsources the actual sorting to the operating system.	system	94
a ‘fork bomb’ is what we warned you about earlier. this occurs when there is an attempt to create an infinite number of processes. this will often bring a system to a near-standstill, as it attempts to allocate cpu time and memory to a large number of processes that are ready to run. system administrators don’t like them and may set upper limits on the number of processes each user can have, or revoke login rights because they create disturbances in the force for other users’ programs. a program can limit the number of child processes created by using setrlimit().	memory	224
a ‘fork bomb’ is what we warned you about earlier. this occurs when there is an attempt to create an infinite number of processes. this will often bring a system to a near-standstill, as it attempts to allocate cpu time and memory to a large number of processes that are ready to run. system administrators don’t like them and may set upper limits on the number of processes each user can have, or revoke login rights because they create disturbances in the force for other users’ programs. a program can limit the number of child processes created by using setrlimit().	system	155
a signal can be thought of as a software interrupt. this means that a process that receives a signal stops the execution of the current program and makes the program respond to the signal.	a process	68
there are various signals defined by the operating system, two of which you may already know: sigsegv and sigint. the first is caused by an illegal memory access, and the second is sent by a user wanting to terminate a	memory	148
there are various signals defined by the operating system, two of which you may already know: sigsegv and sigint. the first is caused by an illegal memory access, and the second is sent by a user wanting to terminate a	system	51
a signal has four stages in its life cycle: generated, pending, blocked, and received state. these refer to when a process generates a signal, the kernel is about to deliver a signal, the signal is blocked, and when the kernel delivers a signal, each of which requires some time to complete. read more in the introduction to the signals chapter.	a process	113
a signal has four stages in its life cycle: generated, pending, blocked, and received state. these refer to when a process generates a signal, the kernel is about to deliver a signal, the signal is blocked, and when the kernel delivers a signal, each of which requires some time to complete. read more in the introduction to the signals chapter.	block	64
4. the process will be created with one thread (more on that later. the general consensus is to not create processes and threads at the same time).	thread	40
5. since we have copy on write (cow), read-only memory addresses are shared between processes.	address	55
5. since we have copy on write (cow), read-only memory addresses are shared between processes.	memory	48
6. if a program sets up certain regions of memory, they can be shared between processes.	memory	43
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	a struct	282
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	the following	592
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	a filesystem	317
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	a process	514
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	system	323
there are some tricky edge cases when it comes to using file and forking. first, we have to make a technical distinction. a file description is the struct that a file descriptor points to. file descriptors can point to many different structs, but for our purposes, they’ll point to a struct that represents a file on a filesystem. this file description contains elements like paths, how far the descriptor has read into the file, etc. a file descriptor points to a file description. this is important because when a process is forked, only the file descriptor is cloned, not the description. the following snippet contains only one description.	snippet	606
one process will read one part of the file, the other process will read another part of the file. in the following example, there are two descriptions caused by two different file handles.	the following	101
take a look at this code, what does it do?	code	20
size_t buffer_cap = 0; char * buffer = null; ssize_t nread; file * file = fopen("test.txt", "r"); int count = 0; while((nread = getline(&buffer, &buffer_cap, file) != -1) { printf("%s", buffer); if(fork() == 0) { exit(0); } wait(null); }	printf	173
5. if the file descriptor is prepared, it must unactive in the parent process if the child process is using it or vice versa. a process is using it if it is read or written or if that process for whatever reason calls exit. if a process uses it when the other process is as well, the whole application’s behavior is undefined.	a process	126
so how would we fix the code? we would have to flush the file before forking and refrain from using it until after the wait call – more on the specifics of this next section.	code	24
so how would we fix the code? we would have to flush the file before forking and refrain from using it until after the wait call – more on the specifics of this next section.	section	166
size_t buffer_cap = 0; char * buffer = null; ssize_t nread; file * file = fopen("test.txt", "r"); int count = 0; while((nread = getline(&buffer, &buffer_cap, file) != -1) { printf("%s", buffer);	printf	173
if the parent process wants to wait for the child to finish, it must use waitpid (or wait), both of which wait for a child to change process states, which can be one of the following: 1. the child terminated 2. the child was stopped by a signal 3. the child was resumed by a signal note that waitpid can be set to be non-blocking, which means they will return immediately, letting a program know if the child has exited.	the following	169
if the parent process wants to wait for the child to finish, it must use waitpid (or wait), both of which wait for a child to change process states, which can be one of the following: 1. the child terminated 2. the child was stopped by a signal 3. the child was resumed by a signal note that waitpid can be set to be non-blocking, which means they will return immediately, letting a program know if the child has exited.	block	321
wait is a simpler version of waitpid. wait accepts a pointer to an integer and waits on any child process.	pointer	53
2. the last parameter to waitpid is an option parameter. the options are listed below: 3. wnohang - return whetherthe searched process has exited 4. wnowait - wait, but leave the child wait-able by another wait call 5. wexited - wait for exited children 6. wstopped - wait for stopped children 7. wcontinued - wait for continued children exit statuses or the value stored in the integer pointer for both of the calls above are explained below.	parameter	12
2. the last parameter to waitpid is an option parameter. the options are listed below: 3. wnohang - return whetherthe searched process has exited 4. wnowait - wait, but leave the child wait-able by another wait call 5. wexited - wait for exited children 6. wstopped - wait for stopped children 7. wcontinued - wait for continued children exit statuses or the value stored in the integer pointer for both of the calls above are explained below.	pointer	387
a process can only have 256 return values, the rest of the bits are informational, and the information is extracted with bit shifting. however, the kernel has an internal way of keeping track of signaled, exited, or stopped processes. this api is abstracted so that that the kernel developers are free to change it at will. remember: these macros only make sense if the precondition is met. for example, a process’ exit status won’t be defined if the process isn’t signaled. the macros will not do the checking for the program, so it’s up to the programmer to make sure the logic is correct. as an example above, the program should use the wifstopped to check if a process was stopped and then the wstopsig to find the signal that stopped it. as such, there is no need to memorize the following. this is a high-level overview of how information is stored inside the status variables. from the sys/wait.h of an old berkeley standard distribution(bsd) kernel [1]:	the following	781
a process can only have 256 return values, the rest of the bits are informational, and the information is extracted with bit shifting. however, the kernel has an internal way of keeping track of signaled, exited, or stopped processes. this api is abstracted so that that the kernel developers are free to change it at will. remember: these macros only make sense if the precondition is met. for example, a process’ exit status won’t be defined if the process isn’t signaled. the macros will not do the checking for the program, so it’s up to the programmer to make sure the logic is correct. as an example above, the program should use the wifstopped to check if a process was stopped and then the wstopsig to find the signal that stopped it. as such, there is no need to memorize the following. this is a high-level overview of how information is stored inside the status variables. from the sys/wait.h of an old berkeley standard distribution(bsd) kernel [1]:	a process	0
there is a convention about exit codes. if the process exited normally and everything was successful, then a zero should be returned. beyond that, there aren’t too many widely accepted conventions. if a program specifies return codes to mean certain conditions, it may be able to make more sense of the 256 error codes. for example, a program could return 1 if the program went to stage 1 (like writing to a file) 2 if it did something else, etc.	code	33
it is good practice to wait on your process’ children. if a parent doesn’t wait on your children they become, what are called zombies. zombies are created when a child terminates and then takes up a spot in the kernel process table for your process. the process table keeps track of the following information about a process: pid, status, and how it was killed. the only way to get rid of a zombie is to wait on your children. if a long-running parent never waits for your children, it may lose the ability to fork.	the following	283
it is good practice to wait on your process’ children. if a parent doesn’t wait on your children they become, what are called zombies. zombies are created when a child terminates and then takes up a spot in the kernel process table for your process. the process table keeps track of the following information about a process: pid, status, and how it was killed. the only way to get rid of a zombie is to wait on your children. if a long-running parent never waits for your children, it may lose the ability to fork.	a process	315
having said that, a program doesn’t always need to wait for your children! your parent process can continue to execute code without having to wait for the child process. if a parent dies without waiting on its children, a process can orphan its children. once a parent process completes, any of its children will be assigned to init - the first process, whose pid is 1. therefore, these children would see getppid() return a value of 1. these orphans will eventually finish and for a brief moment become a zombie. the init process automatically waits for all of its children, thus removing these zombies from the system.	code	119
having said that, a program doesn’t always need to wait for your children! your parent process can continue to execute code without having to wait for the child process. if a parent dies without waiting on its children, a process can orphan its children. once a parent process completes, any of its children will be assigned to init - the first process, whose pid is 1. therefore, these children would see getppid() return a value of 1. these orphans will eventually finish and for a brief moment become a zombie. the init process automatically waits for all of its children, thus removing these zombies from the system.	a process	220
having said that, a program doesn’t always need to wait for your children! your parent process can continue to execute code without having to wait for the child process. if a parent dies without waiting on its children, a process can orphan its children. once a parent process completes, any of its children will be assigned to init - the first process, whose pid is 1. therefore, these children would see getppid() return a value of 1. these orphans will eventually finish and for a brief moment become a zombie. the init process automatically waits for all of its children, thus removing these zombies from the system.	system	613
warning: this section uses signals which are partially introduced. the parent gets the signal sigchld when a child completes, so the signal handler can wait for the process. a slightly simplified version is shown below.	this section	9
warning: this section uses signals which are partially introduced. the parent gets the signal sigchld when a child completes, so the signal handler can wait for the process. a slightly simplified version is shown below.	section	14
1. more than one child may have finished but the parent will only get one sigchld signal (signals are not queued) 2. sigchld signals can be sent for other reasons (e.g. a child process has temporarily stopped) 3. it uses the deprecated signal code, instead of the more portable sigaction.	code	243
a more robust code to reap zombies is shown below.	code	14
to make the child process execute another program, use one of the exec functions after forking. the exec set of functions replaces the process image with that of the specified program. this means that any lines of code after the exec call are replaced with those of the executed program. any other work a program wants the child process to do should be done before the exec call. the naming schemes can be shortened mnemonically.	code	214
1. e – an array of pointers to environment variables is explicitly passed to the new process image.	pointer	19
4. v – command-line arguments are passed to the function as an array (vector) of pointers.	pointer	81
an example of this code is below. this code executes ls	code	19
<unistd.h> <sys/types.h> <sys/wait.h> <stdlib.h> <stdio.h>	type	16
try to decode the following example	the following	14
try to decode the following example	code	9
the example writes "captain’s log" to a file then prints everything in /usr/include to the same file. there’s no error checking in the above code (we assume close, open, chdir etc. work as expected).	code	141
posix details all of the semantics that exec needs to cover [3]. note the following 1. file descriptors are preserved after an exec. that means if a program open a file and doesn’t to close it, it remains open in the child. this is a problem because usually the child doesn’t know about those file descriptors. nevertheless, they take up a slot in the file descriptor table and could possibly prevent other processes from accessing the file. the one exception to this is if the file descriptor has the close-on-exec flag set (o_cloexec) – we will go over setting flags later.	the following	70
3. environment variables are preserved unless using an environ version of exec 4. the operating system may open up 0, 1, 2 – stdin, stdout, stderr, if they are closed after exec, most of the time they leave them closed.	system	96
system pre-packs the above code [9, p. 371]. the following is a snippet of how to use system.	the following	45
system pre-packs the above code [9, p. 371]. the following is a snippet of how to use system.	code	27
system pre-packs the above code [9, p. 371]. the following is a snippet of how to use system.	system	0
system pre-packs the above code [9, p. 371]. the following is a snippet of how to use system.	snippet	64
the system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. this also means that system is a blocking call. the parent process can’t continue until the process started by system exits. also, system actually creates a shell that is then given the string, which is more overhead than using exec directly. the standard shell will use the path environment variable to search for a filename that matches the command. using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern, so we encourage you to learn and use fork exec and waitpid instead. it also tends to be a huge security risk. by allowing someone to access a shell version of the environment, the program can run into all sorts of problems:	a shell	284
the system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. this also means that system is a blocking call. the parent process can’t continue until the process started by system exits. also, system actually creates a shell that is then given the string, which is more overhead than using exec directly. the standard shell will use the path environment variable to search for a filename that matches the command. using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern, so we encourage you to learn and use fork exec and waitpid instead. it also tends to be a huge security risk. by allowing someone to access a shell version of the environment, the program can run into all sorts of problems:	string	315
the system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. this also means that system is a blocking call. the parent process can’t continue until the process started by system exits. also, system actually creates a shell that is then given the string, which is more overhead than using exec directly. the standard shell will use the path environment variable to search for a filename that matches the command. using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern, so we encourage you to learn and use fork exec and waitpid instead. it also tends to be a huge security risk. by allowing someone to access a shell version of the environment, the program can run into all sorts of problems:	block	162
the system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. this also means that system is a blocking call. the parent process can’t continue until the process started by system exits. also, system actually creates a shell that is then given the string, which is more overhead than using exec directly. the standard shell will use the path environment variable to search for a filename that matches the command. using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern, so we encourage you to learn and use fork exec and waitpid instead. it also tends to be a huge security risk. by allowing someone to access a shell version of the environment, the program can run into all sorts of problems:	parameter	57
the system call will fork, execute the command passed by parameter and the original parent process will wait for this to finish. this also means that system is a blocking call. the parent process can’t continue until the process started by system exits. also, system actually creates a shell that is then given the string, which is more overhead than using exec directly. the standard shell will use the path environment variable to search for a filename that matches the command. using system will usually be sufficient for many simple run-this-command problems but can quickly become limiting for more complex or subtle problems, and it hides the mechanics of the fork-exec-wait pattern, so we encourage you to learn and use fork exec and waitpid instead. it also tends to be a huge security risk. by allowing someone to access a shell version of the environment, the program can run into all sorts of problems:	system	4
int main(int argc, char**argv) { char *to_exec = asprintf("ls %s", argv[1]); system(to_exec); }	system	77
int main(int argc, char**argv) { char *to_exec = asprintf("ls %s", argv[1]); system(to_exec); }	printf	51
why not execute ls directly? the reason is that now we have a monitor program – our parent that can do other things. it can proceed and execute another function, or it can also modify the state of the system or read the output of the function call.	system	201
environment variables are variables that the system keeps for all processes to use. your system has these set up right now! in bash, some are already defined	system	45
 what signal is sent when press ctrl-c at a terminal?	a terminal	42
 process memory isolation.	memory	9
 process memory layout (where is the heap, stack etc; invalid memory addresses).	the heap	33
 process memory layout (where is the heap, stack etc; invalid memory addresses).	address	69
 process memory layout (where is the heap, stack etc; invalid memory addresses).	memory	9
 what is the difference between execs with a p and without a p? what does the operating system  how does a program pass in command line arguments to execl*? how about execv*? what should be the first command line argument by convention?	system	88
 what is the int *status pointer passed into wait? when does wait fail?	pointer	25
 my terminal is anchored to pid = 1337 and has become unresponsive. write me the terminal command and the c code to send sigquit to it.	code	108
 can one process alter another processes memory through normal means? why?	memory	41
 where is the heap, stack, data, and text segment? which segments can a program write to? what are invalid memory addresses?	the heap	10
 where is the heap, stack, data, and text segment? which segments can a program write to? what are invalid memory addresses?	address	114
 where is the heap, stack, data, and text segment? which segments can a program write to? what are invalid memory addresses?	memory	107
 code up a fork bomb in c (please don’t run it).	code	1
5 memory allocators	memory	2
memory memory everywhere but not an allocation to be made a fragmented heap	memory	0
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	the heap	129
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	address	182
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	memory	0
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	heap memory	60
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	system	145
memory allocation is important! allocating and deallocating heap memory is one of the most common operations in any application. the heap at the system level is contiguous series of addresses that the program can expand or contract and use as its accord [2]. in posix, this is called the system break. we use sbrk to move the system break. most programs don’t interact directly with this call, they use a memory allocation system around it to handle chunking up and keeping track of which memory is allocated and which is freed.	memory allocation	0
we will mainly be looking into simple allocators. just know that there are other ways of dividing up memory like with mmap or other allocation schemes and methods like jemalloc.	mmap	118
we will mainly be looking into simple allocators. just know that there are other ways of dividing up memory like with mmap or other allocation schemes and methods like jemalloc.	memory	101
5.2 c memory allocation api	memory	6
5.2 c memory allocation api	memory allocation	6
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.	code	440
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.	address	560
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.	memory	87
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.	block	78
 malloc(size_t bytes) is a c library call and is used to reserve a contiguous block of memory that may be uninitialized [4, p. 348]. unlike stack memory, the memory remains allocated until free is called with the same pointer. if malloc can either return a pointer to at least that much free space requested or null. that means that malloc can return null even if there is some space. robust programs should check the return value. if your code assumes malloc succeeds, and it does not, then your program will likely crash (segfault) when it tries to write to address 0. also, malloc leaves garbage in memory because of performance – check your code to make sure that a program all program values are initialized.	pointer	218
 realloc(void *space, size_t bytes) allows a program to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc) [4, p. 349]. the most common use of realloc is to resize memory used to hold an array of values. there are two gotchas with realloc. one, a new pointer may be returned. two, it can fail. a naive but readable version of realloc is suggested below with sample usage.	the heap	126
 realloc(void *space, size_t bytes) allows a program to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc) [4, p. 349]. the most common use of realloc is to resize memory used to hold an array of values. there are two gotchas with realloc. one, a new pointer may be returned. two, it can fail. a naive but readable version of realloc is suggested below with sample usage.	memory	75
 realloc(void *space, size_t bytes) allows a program to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc) [4, p. 349]. the most common use of realloc is to resize memory used to hold an array of values. there are two gotchas with realloc. one, a new pointer may be returned. two, it can fail. a naive but readable version of realloc is suggested below with sample usage.	pointer	312
 realloc(void *space, size_t bytes) allows a program to resize an existing memory allocation that was previously allocated on the heap (via malloc, calloc, or realloc) [4, p. 349]. the most common use of realloc is to resize memory used to hold an array of values. there are two gotchas with realloc. one, a new pointer may be returned. two, it can fail. a naive but readable version of realloc is suggested below with sample usage.	memory allocation	75
the above code is fragile. if realloc fails then the program leaks memory. robust code checks for the return value and only reassigns the original pointer if not null.	code	10
the above code is fragile. if realloc fails then the program leaks memory. robust code checks for the return value and only reassigns the original pointer if not null.	memory	67
the above code is fragile. if realloc fails then the program leaks memory. robust code checks for the return value and only reassigns the original pointer if not null.	pointer	147
 calloc(size_t nmemb, size_t size) initializes memory contents to zero and also takes two arguments: the number of items and the size in bytes of each item. an advanced discussion of these limitations is in this article. programmers often use calloc rather than explicitly calling memset after malloc, to set the memory contents to zero because certain performance considerations are taken into account. note calloc(x,y) is identical to calloc(y,x), but you should follow the conventions of the manual. a naive implementation of calloc is below.	memory	47
 free takes a pointer to the start of a piece of memory and makes it available for use in subsequent calls to the other allocation functions. this is important because we don’t want every process in our address space to take an enormous amount of memory. once we are done using memory, we stop using it with ‘free‘. a simple usage is below.	address	203
 free takes a pointer to the start of a piece of memory and makes it available for use in subsequent calls to the other allocation functions. this is important because we don’t want every process in our address space to take an enormous amount of memory. once we are done using memory, we stop using it with ‘free‘. a simple usage is below.	memory	49
 free takes a pointer to the start of a piece of memory and makes it available for use in subsequent calls to the other allocation functions. this is important because we don’t want every process in our address space to take an enormous amount of memory. once we are done using memory, we stop using it with ‘free‘. a simple usage is below.	pointer	14
if a program uses a piece of memory after it is freed - that is undefined behavior.	memory	29
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	the heap	0
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	thread	437
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	address	346
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	memory	32
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	heap memory	59
the heap is part of the process memory and varies in size. heap memory allocation is performed by the c library when a program calls malloc (calloc, realloc) and free. by calling sbrk the c library can increase the size of the heap as your program demands more heap memory. as the heap and stack need to grow, we put them at opposite ends of the address space. stacks don’t grow like a heap, new parts of the stack are allocated for new threads. for typical architectures, the heap will grow upwards and the stack grows downwards.	memory allocation	64
nowadays, modern operating system memory allocators no longer need sbrk. instead, they can request independent regions of virtual memory and maintain multiple memory regions. for example, gibibyte requests may be placed in a different memory region than small allocation requests. however, this detail is an unwanted complexity.	this detail	290
nowadays, modern operating system memory allocators no longer need sbrk. instead, they can request independent regions of virtual memory and maintain multiple memory regions. for example, gibibyte requests may be placed in a different memory region than small allocation requests. however, this detail is an unwanted complexity.	memory	34
nowadays, modern operating system memory allocators no longer need sbrk. instead, they can request independent regions of virtual memory and maintain multiple memory regions. for example, gibibyte requests may be placed in a different memory region than small allocation requests. however, this detail is an unwanted complexity.	system	27
programs don’t need to call brk or sbrk typically, though calling sbrk(0) can be interesting because it tells a program where your heap currently ends. instead programs use malloc, calloc, realloc and free which are part of the c library. the internal implementation of these functions may call sbrk when additional heap memory is required.	memory	321
programs don’t need to call brk or sbrk typically, though calling sbrk(0) can be interesting because it tells a program where your heap currently ends. instead programs use malloc, calloc, realloc and free which are part of the c library. the internal implementation of these functions may call sbrk when additional heap memory is required.	heap memory	316
note that the memory that was newly obtained by the operating system must be zeroed out. if the operating system left the contents of physical ram as-is, it might be possible for one process to learn about the memory of another process that had previously used the memory. this would be a security leak. unfortunately, this means that for malloc requests before any memory has been freed is often zero. this is unfortunate because many programmers mistakenly write c programs that assume allocated memory will always be zero.	memory	14
note that the memory that was newly obtained by the operating system must be zeroed out. if the operating system left the contents of physical ram as-is, it might be possible for one process to learn about the memory of another process that had previously used the memory. this would be a security leak. unfortunately, this means that for malloc requests before any memory has been freed is often zero. this is unfortunate because many programmers mistakenly write c programs that assume allocated memory will always be zero.	system	62
 system calls are slow compared to library calls. we should reserve a large amount of memory and only occasionally ask for more from the system.	memory	86
 system calls are slow compared to library calls. we should reserve a large amount of memory and only occasionally ask for more from the system.	system	1
 no reuse of freed memory. our program never re-uses heap memory - it keeps asking for a bigger heap.	memory	19
 no reuse of freed memory. our program never re-uses heap memory - it keeps asking for a bigger heap.	heap memory	53
if this allocator was used in a typical program, the process would quickly exhaust all available memory.	memory	97
instead, we need an allocator that can efficiently use heap space and only ask for more memory when necessary.	memory	88
some programs use this type of allocator. consider a video game allocating objects to load the next scene. it is considerably faster to do the above and throw the entire block of memory away than it is to do the following placement strategies.	the following	208
some programs use this type of allocator. consider a video game allocating objects to load the next scene. it is considerably faster to do the above and throw the entire block of memory away than it is to do the following placement strategies.	memory	179
some programs use this type of allocator. consider a video game allocating objects to load the next scene. it is considerably faster to do the above and throw the entire block of memory away than it is to do the following placement strategies.	block	170
some programs use this type of allocator. consider a video game allocating objects to load the next scene. it is considerably faster to do the above and throw the entire block of memory away than it is to do the following placement strategies.	type	23
during program execution, memory is allocated and deallocated, so there will be a gap in the heap memory that can be re-used for future memory requests. the memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available. suppose our current heap size is 64k. let’s say that our heap looks like the following table.	the heap	89
during program execution, memory is allocated and deallocated, so there will be a gap in the heap memory that can be re-used for future memory requests. the memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available. suppose our current heap size is 64k. let’s say that our heap looks like the following table.	the following	353
during program execution, memory is allocated and deallocated, so there will be a gap in the heap memory that can be re-used for future memory requests. the memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available. suppose our current heap size is 64k. let’s say that our heap looks like the following table.	memory	26
during program execution, memory is allocated and deallocated, so there will be a gap in the heap memory that can be re-used for future memory requests. the memory allocator needs to keep track of which parts of the heap are currently allocated and which are parts are available. suppose our current heap size is 64k. let’s say that our heap looks like the following table.	heap memory	93
figure 5.1: empty heap blocks if a new malloc request for 2kib is executed (malloc(2048)), where should malloc reserve the memory? it could use the last 2kib hole, which happens to be the perfect size! or it could split one of the other two free holes.	memory	123
figure 5.1: empty heap blocks if a new malloc request for 2kib is executed (malloc(2048)), where should malloc reserve the memory? it could use the last 2kib hole, which happens to be the perfect size! or it could split one of the other two free holes.	block	23
figure 5.4: first fit finds the first match one thing to keep in mind is those placement strategies don’t need to replace the block. for example, our first fit allocator could’ve returned the original block unbroken. notice that this would lead to about 14kib of space to be unused by the user and the allocator. we call this internal fragmentation.	block	126
in contrast, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way so a continuous block of that size is unavailable. in our previous example, of the 64kib of heap memory, 17kib is allocated, and 47kib is free. however, the largest available block is only 30kib because our available unallocated heap memory is fragmented into smaller pieces.	the heap	81
in contrast, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way so a continuous block of that size is unavailable. in our previous example, of the 64kib of heap memory, 17kib is allocated, and 47kib is free. however, the largest available block is only 30kib because our available unallocated heap memory is fragmented into smaller pieces.	memory	71
in contrast, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way so a continuous block of that size is unavailable. in our previous example, of the 64kib of heap memory, 17kib is allocated, and 47kib is free. however, the largest available block is only 30kib because our available unallocated heap memory is fragmented into smaller pieces.	block	137
in contrast, external fragmentation is that even though we have enough memory in the heap, it may be divided up in a way so a continuous block of that size is unavailable. in our previous example, of the 64kib of heap memory, 17kib is allocated, and 47kib is free. however, the largest available block is only 30kib because our available unallocated heap memory is fragmented into smaller pieces.	heap memory	213
 need to minimize fragmentation (i.e. maximize memory utilization)  need high performance  fiddly implementation – lots of pointer manipulation using linked lists and pointer arithmetic.	memory	47
 need to minimize fragmentation (i.e. maximize memory utilization)  need high performance  fiddly implementation – lots of pointer manipulation using linked lists and pointer arithmetic.	pointer	123
 the allocator doesn’t know the program’s memory allocation requests in advance. even if we did, this is the knapsack problem which is known to be np-hard!	memory	42
 the allocator doesn’t know the program’s memory allocation requests in advance. even if we did, this is the knapsack problem which is known to be np-hard!	memory allocation	42
different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).	memory	54
different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).	heap memory	49
different strategies affect the fragmentation of heap memory in non-obvious ways, which only are discovered by mathematical analysis or careful simulations under real-world conditions (for example simulating the memory allocation requests of a database or webserver).	memory allocation	212
first, we will have a more mathematical, one-shot approach to each of these algorithms [3]. the paper describes a scenario where you have a certain number of bins and a certain number of allocations, and you are trying to fit the allocations in as few bins as possible, hence using as little memory as possible. the paper discusses theoretical implications and puts a nice limit on the ratios in the long run between the ideal memory usage and the actual memory usage. for those who are interested, the paper concludes that actual memory usage over ideal memory usage as the number of bins increases – the bins can have any distribution – is about 1.7 for first-fit and lower bounded by 1.7 for best fit. the problem with this analysis is that few real-world applications need this type of one-shot allocation. video game object allocations will typically designate a different subheap for each level and fill up that subheap if they need a quick memory allocation scheme that they can throw away.	memory	292
first, we will have a more mathematical, one-shot approach to each of these algorithms [3]. the paper describes a scenario where you have a certain number of bins and a certain number of allocations, and you are trying to fit the allocations in as few bins as possible, hence using as little memory as possible. the paper discusses theoretical implications and puts a nice limit on the ratios in the long run between the ideal memory usage and the actual memory usage. for those who are interested, the paper concludes that actual memory usage over ideal memory usage as the number of bins increases – the bins can have any distribution – is about 1.7 for first-fit and lower bounded by 1.7 for best fit. the problem with this analysis is that few real-world applications need this type of one-shot allocation. video game object allocations will typically designate a different subheap for each level and fill up that subheap if they need a quick memory allocation scheme that they can throw away.	type	782
first, we will have a more mathematical, one-shot approach to each of these algorithms [3]. the paper describes a scenario where you have a certain number of bins and a certain number of allocations, and you are trying to fit the allocations in as few bins as possible, hence using as little memory as possible. the paper discusses theoretical implications and puts a nice limit on the ratios in the long run between the ideal memory usage and the actual memory usage. for those who are interested, the paper concludes that actual memory usage over ideal memory usage as the number of bins increases – the bins can have any distribution – is about 1.7 for first-fit and lower bounded by 1.7 for best fit. the problem with this analysis is that few real-world applications need this type of one-shot allocation. video game object allocations will typically designate a different subheap for each level and fill up that subheap if they need a quick memory allocation scheme that they can throw away.	memory allocation	947
in practice, we’ll be using the result from a more rigorous survey conducted in 2005 [7]. the survey makes sure to note that memory allocation is a moving target. a good allocation scheme to one program may not be a good allocation scheme for another program. programs don’t uniformly follow the distribution of allocations. the survey talks about all the allocation schemes that we have introduced as well as a few extra ones. here are some summarized takeaways 1. best fit may have problems when a block is chosen that is almost the right size, and the remaining space is split so small that a program probably won’t use it. a way to get around this could be to set a threshold for splitting. this small splitting isn’t observed as frequently under a regular workload. also, the worst-case behavior of best-fit is bad, but it doesn’t usually happen [p. 43].	memory	125
in practice, we’ll be using the result from a more rigorous survey conducted in 2005 [7]. the survey makes sure to note that memory allocation is a moving target. a good allocation scheme to one program may not be a good allocation scheme for another program. programs don’t uniformly follow the distribution of allocations. the survey talks about all the allocation schemes that we have introduced as well as a few extra ones. here are some summarized takeaways 1. best fit may have problems when a block is chosen that is almost the right size, and the remaining space is split so small that a program probably won’t use it. a way to get around this could be to set a threshold for splitting. this small splitting isn’t observed as frequently under a regular workload. also, the worst-case behavior of best-fit is bad, but it doesn’t usually happen [p. 43].	block	500
in practice, we’ll be using the result from a more rigorous survey conducted in 2005 [7]. the survey makes sure to note that memory allocation is a moving target. a good allocation scheme to one program may not be a good allocation scheme for another program. programs don’t uniformly follow the distribution of allocations. the survey talks about all the allocation schemes that we have introduced as well as a few extra ones. here are some summarized takeaways 1. best fit may have problems when a block is chosen that is almost the right size, and the remaining space is split so small that a program probably won’t use it. a way to get around this could be to set a threshold for splitting. this small splitting isn’t observed as frequently under a regular workload. also, the worst-case behavior of best-fit is bad, but it doesn’t usually happen [p. 43].	memory allocation	125
2. the survey also talks about an important distinction of first-fit. there are multiple notions of first. first could be ordered in terms of the time of ‘free‘’ing, or it could be ordered through the addresses of the start of the block, or it could be ordered by the time of last free – first being least recently used. the survey didn’t go too in-depth into the performance of each but did make a note that address-ordered and least recently used (lru) lists ended up with better performance than the most recently used first.	address	201
2. the survey also talks about an important distinction of first-fit. there are multiple notions of first. first could be ordered in terms of the time of ‘free‘’ing, or it could be ordered through the addresses of the start of the block, or it could be ordered by the time of last free – first being least recently used. the survey didn’t go too in-depth into the performance of each but did make a note that address-ordered and least recently used (lru) lists ended up with better performance than the most recently used first.	block	231
3. the survey concludes by first saying that under simulated random (assuming uniform at random) workloads, best fit and first fit do as well. even in practice, both best and address ordered first fit do about as equally as well with a splitting threshold and coalescing. the reasons why aren’t entirely known.	address	175
some additional notes we make 1. best fit may take less time than a full heap scan. when a block of perfect size or perfect size within a threshold is found, that can be returned, depending on what edge-case policy you have.	block	91
2. worst fit follows this as well. your heap could be represented with the max-heap data structure and each allocation call could simply pop the top off, re-heapify, and possibly insert a split memory block. using fibonacci heaps, however, could be extremely inefficient.	a struct	87
2. worst fit follows this as well. your heap could be represented with the max-heap data structure and each allocation call could simply pop the top off, re-heapify, and possibly insert a split memory block. using fibonacci heaps, however, could be extremely inefficient.	memory	194
2. worst fit follows this as well. your heap could be represented with the max-heap data structure and each allocation call could simply pop the top off, re-heapify, and possibly insert a split memory block. using fibonacci heaps, however, could be extremely inefficient.	block	201
3. first-fit needs to have a block order. most of the time programmers will default to linked lists which is a fine choice. there aren’t too many improvements you can make with a least recently used and most recently used linked list policy, but with address ordered linked lists you can speed up insertion from o(n) to o(log(n)) by using a randomized skip-list in conjunction with your singly-linked list. an insert would use the skip list as shortcuts to find the right place to insert the block and removal would go through the list as normal.	address	251
3. first-fit needs to have a block order. most of the time programmers will default to linked lists which is a fine choice. there aren’t too many improvements you can make with a least recently used and most recently used linked list policy, but with address ordered linked lists you can speed up insertion from o(n) to o(log(n)) by using a randomized skip-list in conjunction with your singly-linked list. an insert would use the skip list as shortcuts to find the right place to insert the block and removal would go through the list as normal.	block	29
4. there are many placement strategies that we haven’t talked about, one is next-fit which is first fit on the next fit block. this adds deterministic randomness – pardon the oxymoron. you won’t be expected to know this algorithm, know as you are implementing a memory allocator as part of a machine problem, there are more than these.	memory	262
4. there are many placement strategies that we haven’t talked about, one is next-fit which is first fit on the next fit block. this adds deterministic randomness – pardon the oxymoron. you won’t be expected to know this algorithm, know as you are implementing a memory allocator as part of a machine problem, there are more than these.	block	120
5.4 memory allocator tutorial	memory	4
a memory allocator needs to keep track of which bytes are currently allocated and which are available for use.	memory	2
this section introduces the implementation and conceptual details of building an allocator, or the actual code that implements malloc and free.	this section	0
this section introduces the implementation and conceptual details of building an allocator, or the actual code that implements malloc and free.	code	106
this section introduces the implementation and conceptual details of building an allocator, or the actual code that implements malloc and free.	section	5
conceptually, we are thinking about creating linked lists and lists of blocks! please enjoy the following ascii art. bt is short for boundary tag.	the following	92
conceptually, we are thinking about creating linked lists and lists of blocks! please enjoy the following ascii art. bt is short for boundary tag.	block	71
figure 5.5: 3 adjacent memory blocks we will have implicit pointers in our next block, meaning that we can get from one block to another using addition. this is in contrast to an explicit metadata *next field in our meta block.	memory	23
figure 5.5: 3 adjacent memory blocks we will have implicit pointers in our next block, meaning that we can get from one block to another using addition. this is in contrast to an explicit metadata *next field in our meta block.	block	30
figure 5.5: 3 adjacent memory blocks we will have implicit pointers in our next block, meaning that we can get from one block to another using addition. this is in contrast to an explicit metadata *next field in our meta block.	pointer	59
figure 5.6: malloc addition one can grab the next block by finding the end of the current one. that is what we mean by “implicit list”.	block	50
the actual spacing may be different. the metadata can contain different things. a minimal metadata implementation would simply have the size of the block.	block	148
since we write integers and pointers into memory that we already control, we can later consistently hop from one address to the next. this internal information represents some overhead. meaning even if we had requested 1024 kib of contiguous memory from the system, we an allocation of that size will fail.	address	113
since we write integers and pointers into memory that we already control, we can later consistently hop from one address to the next. this internal information represents some overhead. meaning even if we had requested 1024 kib of contiguous memory from the system, we an allocation of that size will fail.	memory	42
since we write integers and pointers into memory that we already control, we can later consistently hop from one address to the next. this internal information represents some overhead. meaning even if we had requested 1024 kib of contiguous memory from the system, we an allocation of that size will fail.	system	258
since we write integers and pointers into memory that we already control, we can later consistently hop from one address to the next. this internal information represents some overhead. meaning even if we had requested 1024 kib of contiguous memory from the system, we an allocation of that size will fail.	pointer	28
our heap memory is a list of blocks where each block is either allocated or unallocated. thus there is conceptually a list of free blocks, but it is implicit in the form of block size information that we store as part of each block. let’s think of it in terms of a simple implementation.	memory	9
our heap memory is a list of blocks where each block is either allocated or unallocated. thus there is conceptually a list of free blocks, but it is implicit in the form of block size information that we store as part of each block. let’s think of it in terms of a simple implementation.	block	29
our heap memory is a list of blocks where each block is either allocated or unallocated. thus there is conceptually a list of free blocks, but it is implicit in the form of block size information that we store as part of each block. let’s think of it in terms of a simple implementation.	heap memory	4
we could navigate from one block to the next block by adding the block’s size.	block	27
p + sizeof(metadata) + p->block_size + sizeof(btag)	block	26
the calling program never sees these values. they are internal to the implementation of the memory allocator.	memory	92
header data. the allocator would need to find an unallocated space of at least 88 bytes. after updating the heap data it would return a pointer to the block. however, the returned pointer points to the usable space, not the internal data! instead, we would return the start of the block + 8 bytes. in the implementation, remember that pointer arithmetic depends on type. for example, p += 8 adds 8 * sizeof(p), not necessarily 8 bytes!	the heap	104
header data. the allocator would need to find an unallocated space of at least 88 bytes. after updating the heap data it would return a pointer to the block. however, the returned pointer points to the usable space, not the internal data! instead, we would return the start of the block + 8 bytes. in the implementation, remember that pointer arithmetic depends on type. for example, p += 8 adds 8 * sizeof(p), not necessarily 8 bytes!	block	151
header data. the allocator would need to find an unallocated space of at least 88 bytes. after updating the heap data it would return a pointer to the block. however, the returned pointer points to the usable space, not the internal data! instead, we would return the start of the block + 8 bytes. in the implementation, remember that pointer arithmetic depends on type. for example, p += 8 adds 8 * sizeof(p), not necessarily 8 bytes!	type	365
header data. the allocator would need to find an unallocated space of at least 88 bytes. after updating the heap data it would return a pointer to the block. however, the returned pointer points to the usable space, not the internal data! instead, we would return the start of the block + 8 bytes. in the implementation, remember that pointer arithmetic depends on type. for example, p += 8 adds 8 * sizeof(p), not necessarily 8 bytes!	pointer	136
5.4.1 implementing a memory allocator	memory	21
the simplest implementation uses first-fit. start at the first block, assuming it exists, and iterate until a block that represents an unallocated space of sufficient size is found, or we’ve checked all the blocks. if no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. for this class, we will try to serve every memory request until the operating system tells us we are going to run out of heap space. other applications may limit themselves to a certain heap size and cause requests to intermittently fail. besides, a fast implementation might extend it a significant amount so that we will not need to request more heap memory soon.	the heap	312
the simplest implementation uses first-fit. start at the first block, assuming it exists, and iterate until a block that represents an unallocated space of sufficient size is found, or we’ve checked all the blocks. if no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. for this class, we will try to serve every memory request until the operating system tells us we are going to run out of heap space. other applications may limit themselves to a certain heap size and cause requests to intermittently fail. besides, a fast implementation might extend it a significant amount so that we will not need to request more heap memory soon.	memory	365
the simplest implementation uses first-fit. start at the first block, assuming it exists, and iterate until a block that represents an unallocated space of sufficient size is found, or we’ve checked all the blocks. if no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. for this class, we will try to serve every memory request until the operating system tells us we are going to run out of heap space. other applications may limit themselves to a certain heap size and cause requests to intermittently fail. besides, a fast implementation might extend it a significant amount so that we will not need to request more heap memory soon.	block	63
the simplest implementation uses first-fit. start at the first block, assuming it exists, and iterate until a block that represents an unallocated space of sufficient size is found, or we’ve checked all the blocks. if no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. for this class, we will try to serve every memory request until the operating system tells us we are going to run out of heap space. other applications may limit themselves to a certain heap size and cause requests to intermittently fail. besides, a fast implementation might extend it a significant amount so that we will not need to request more heap memory soon.	heap memory	670
the simplest implementation uses first-fit. start at the first block, assuming it exists, and iterate until a block that represents an unallocated space of sufficient size is found, or we’ve checked all the blocks. if no suitable block is found, it’s time to call sbrk() again to sufficiently extend the size of the heap. for this class, we will try to serve every memory request until the operating system tells us we are going to run out of heap space. other applications may limit themselves to a certain heap size and cause requests to intermittently fail. besides, a fast implementation might extend it a significant amount so that we will not need to request more heap memory soon.	system	400
when a free block is found, it may be larger than the space we need. if so, we will create two entries in our implicit list. the first entry is the allocated block, the second entry is the remaining space. there are ways to do this if the program wants to keep the overhead small. we recommend first for going with readability.	block	12
typedef struct { unsigned int block_size : 7; unsigned int is_free : 1; } size_free; typedef struct { size_free info; char data[0]; } block;	block	30
typedef struct { unsigned int block_size : 7; unsigned int is_free : 1; } size_free; typedef struct { size_free info; char data[0]; } block;	type	0
the compiler will handle the shifting. after setting up your fields then it becomes simply looping through each of the blocks and checking the appropriate fields here is a visual representation of what happens. if we assume that we have a block that looks like this, we want to spit if the allocation is let’s say 16 bytes the split we’ll have to do is the following.	the following	353
the compiler will handle the shifting. after setting up your fields then it becomes simply looping through each of the blocks and checking the appropriate fields here is a visual representation of what happens. if we assume that we have a block that looks like this, we want to spit if the allocation is let’s say 16 bytes the split we’ll have to do is the following.	block	119
many architectures expect multibyte primitives to be aligned to some multiple of 2 (4, 16, etc). for example, it’s common to require 4-byte types to be aligned to 4-byte boundaries and 8-byte types on 8-byte boundaries. if multi-byte primitives are stored on an unreasonable boundary, the performance can be significantly impacted because it may require an additional memory read. on some architectures the penalty is even greater - the program will crash with a bus error. most of you have experienced this in your architecture classes if there was no memory protection.	memory	368
many architectures expect multibyte primitives to be aligned to some multiple of 2 (4, 16, etc). for example, it’s common to require 4-byte types to be aligned to 4-byte boundaries and 8-byte types on 8-byte boundaries. if multi-byte primitives are stored on an unreasonable boundary, the performance can be significantly impacted because it may require an additional memory read. on some architectures the penalty is even greater - the program will crash with a bus error. most of you have experienced this in your architecture classes if there was no memory protection.	type	140
as malloc does not know how the user will use the allocated memory, the pointer returned to the program needs to be aligned for the worst case, which is architecture-dependent.	memory	60
as malloc does not know how the user will use the allocated memory, the pointer returned to the program needs to be aligned for the worst case, which is architecture-dependent.	pointer	72
from glibc documentation, the glibc malloc uses the following heuristic [1] the block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. on gnu systems, the address is always a multiple of eight on most systems and a multiple of 16 on 64-bit systems." for example, if you need to calculate how many 16 byte units are required, don’t forget to round up.	the following	48
from glibc documentation, the glibc malloc uses the following heuristic [1] the block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. on gnu systems, the address is always a multiple of eight on most systems and a multiple of 16 on 64-bit systems." for example, if you need to calculate how many 16 byte units are required, don’t forget to round up.	address	194
from glibc documentation, the glibc malloc uses the following heuristic [1] the block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. on gnu systems, the address is always a multiple of eight on most systems and a multiple of 16 on 64-bit systems." for example, if you need to calculate how many 16 byte units are required, don’t forget to round up.	block	80
from glibc documentation, the glibc malloc uses the following heuristic [1] the block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. on gnu systems, the address is always a multiple of eight on most systems and a multiple of 16 on 64-bit systems." for example, if you need to calculate how many 16 byte units are required, don’t forget to round up.	system	181
from glibc documentation, the glibc malloc uses the following heuristic [1] the block that malloc gives you is guaranteed to be aligned so that it can hold any type of data. on gnu systems, the address is always a multiple of eight on most systems and a multiple of 16 on 64-bit systems." for example, if you need to calculate how many 16 byte units are required, don’t forget to round up.	type	160
the additional constant ensures incomplete units are rounded up. note, real code is more likely to symbol sizes e.g. sizeof(x) - 1, rather than coding numerical constant 15. here’s a great article on memory alignment, if you are further interested another added effect could be internal fragmentation happens when the given block is larger than their allocation size. let’s say that we have a free block of size 16b (not including metadata). if they allocate 7 bytes,	code	76
the additional constant ensures incomplete units are rounded up. note, real code is more likely to symbol sizes e.g. sizeof(x) - 1, rather than coding numerical constant 15. here’s a great article on memory alignment, if you are further interested another added effect could be internal fragmentation happens when the given block is larger than their allocation size. let’s say that we have a free block of size 16b (not including metadata). if they allocate 7 bytes,	memory	200
the additional constant ensures incomplete units are rounded up. note, real code is more likely to symbol sizes e.g. sizeof(x) - 1, rather than coding numerical constant 15. here’s a great article on memory alignment, if you are further interested another added effect could be internal fragmentation happens when the given block is larger than their allocation size. let’s say that we have a free block of size 16b (not including metadata). if they allocate 7 bytes,	block	324
the allocator may want to round up to 16b and return the entire block. this gets sinister when implementing coalescing and splitting. if the allocator doesn’t implement either, it may end up returning a block of size 64b for a 7b allocation! there is a lot of overhead for that allocation which is what we are trying to avoid.	block	64
when free is called we need to re-apply the offset to get back to the ‘real’ start of the block – to where we stored the size information. a naive implementation would simply mark the block as unused. if we are storing the block allocation status in a bitfield, then we need to clear the bit:	block	90
however, we have a bit more work to do. if the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block. similarly, we also need to check the previous block, too. if that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.	memory	266
however, we have a bit more work to do. if the current block and the next block (if it exists) are both free we need to coalesce these blocks into a single block. similarly, we also need to check the previous block, too. if that exists and represents an unallocated memory, then we need to coalesce the blocks into a single large block.	block	55
to be able to coalesce a free block with a previous free block we will also need to find the previous block, so we store the block’s size at the end of the block, too. these are called “boundary tags” [5]. these are knuth’s solution to the coalescing problem both ways. as the blocks are contiguous, the end of one block sits right next to the start of the next block. so the current block (apart from the first one) can look a few bytes further back to look up the size of the previous block. with this information, the allocator can now jump backward!	block	30
take for example a double coalesce. if we wanted to free the middle block we need to turn the surrounding blocks into one big blocks	block	68
with the above description, it’s possible to build a memory allocator. its main advantage is simplicity - at least simple compared to other allocators! allocating memory is a worst-case linear time operation – search linked lists for a sufficiently large free block. de-allocation is constant time. no more than 3 blocks will need to coalesce into a single block, and using a most recently used block scheme only one linked list entry.	memory	53
with the above description, it’s possible to build a memory allocator. its main advantage is simplicity - at least simple compared to other allocators! allocating memory is a worst-case linear time operation – search linked lists for a sufficiently large free block. de-allocation is constant time. no more than 3 blocks will need to coalesce into a single block, and using a most recently used block scheme only one linked list entry.	block	260
using this allocator it is possible to experiment with different placement strategies. for example, the allocator could start searching from the last deallocated block. if the allocator stores pointers to blocks, it needs to update the pointers so that they always remain valid.	block	162
using this allocator it is possible to experiment with different placement strategies. for example, the allocator could start searching from the last deallocated block. if the allocator stores pointers to blocks, it needs to update the pointers so that they always remain valid.	pointer	193
better performance can be achieved by implementing an explicit doubly-linked list of free nodes. in that case, we can immediately traverse to the next free block and the previous free block. this can reduce the search time because the linked list only includes unallocated blocks. a second advantage is that we now have some control over the ordering of the linked list. for example, when a block is deallocated, we could choose to insert it into the beginning of the linked list rather than always between its neighbors. we may update our struct to look like this	block	156
typedef struct { size_t info; struct block *next; char data[0]; } block;	block	37
typedef struct { size_t info; struct block *next; char data[0]; } block;	type	0
figure 5.9: free list where do we store the pointers of our linked list? a simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block, though you have to ensure that the free blocks are always sufficiently large to hold two pointers. we still need to implement boundary tags, so we can correctly free blocks and coalesce them with their two neighbors. consequently, explicit free lists require more code and complexity. with explicitly linked lists a fast and simple ‘find-first’ algorithm is used to find the first sufficiently large link. however, since the link order can be modified, this corresponds to different placement strategies. if the links are maintained from largest to smallest, then this produces a ‘worst-fit’ placement strategy.	code	469
figure 5.9: free list where do we store the pointers of our linked list? a simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block, though you have to ensure that the free blocks are always sufficiently large to hold two pointers. we still need to implement boundary tags, so we can correctly free blocks and coalesce them with their two neighbors. consequently, explicit free lists require more code and complexity. with explicitly linked lists a fast and simple ‘find-first’ algorithm is used to find the first sufficiently large link. however, since the link order can be modified, this corresponds to different placement strategies. if the links are maintained from largest to smallest, then this produces a ‘worst-fit’ placement strategy.	block	111
figure 5.9: free list where do we store the pointers of our linked list? a simple trick is to realize that the block itself is not being used and store the next and previous pointers as part of the block, though you have to ensure that the free blocks are always sufficiently large to hold two pointers. we still need to implement boundary tags, so we can correctly free blocks and coalesce them with their two neighbors. consequently, explicit free lists require more code and complexity. with explicitly linked lists a fast and simple ‘find-first’ algorithm is used to find the first sufficiently large link. however, since the link order can be modified, this corresponds to different placement strategies. if the links are maintained from largest to smallest, then this produces a ‘worst-fit’ placement strategy.	pointer	44
figure 5.10: free list good and bad coalesce we recommend when trying to implement malloc that you draw out all the cases conceptually and then write the code.	code	154
explicit linked list insertion policy the newly deallocated block can be inserted easily into two possible positions: at the beginning or in address order. inserting at the beginning creates a lifo (last-in, first-out) policy. the most recently deallocated spaces will be reused. studies suggest fragmentation is worse than using address order [7].	address	141
explicit linked list insertion policy the newly deallocated block can be inserted easily into two possible positions: at the beginning or in address order. inserting at the beginning creates a lifo (last-in, first-out) policy. the most recently deallocated spaces will be reused. studies suggest fragmentation is worse than using address order [7].	block	60
inserting in address order (“address ordered policy”) inserts deallocated blocks so that the blocks are visited in increasing address order. this policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. however, there is less fragmentation.	address	13
inserting in address order (“address ordered policy”) inserts deallocated blocks so that the blocks are visited in increasing address order. this policy required more time to free a block because the boundary tags (size data) must be used to find the next and previous unallocated blocks. however, there is less fragmentation.	block	74
a segregated allocator is one that divides the heap into different areas that are handled by different sub-allocators dependent on the size of the allocation request. sizes are grouped into powers of two and each size is handled by a different sub-allocator and each size maintains its free list.	the heap	43
a well-known allocator of this type is the buddy allocator [6, p. 85]. we’ll discuss the binary buddy allocator which splits allocation into blocks of size 2n ; n = 1, 2, 3, ... times some base unit number of bytes, but others also exist like fibonacci split where the allocation is rounded up to the next fibonacci number. the basic concept is	block	141
a well-known allocator of this type is the buddy allocator [6, p. 85]. we’ll discuss the binary buddy allocator which splits allocation into blocks of size 2n ; n = 1, 2, 3, ... times some base unit number of bytes, but others also exist like fibonacci split where the allocation is rounded up to the next fibonacci number. the basic concept is	type	31
simple: if there are no free blocks of size 2n , go to the next level and steal that block and split it into two. if two neighboring blocks of the same size become unallocated, they can coalesce together into a single large block of twice the size.	block	29
buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the deallocated block’s address, rather than traversing the size tags. ultimate performance often requires a small amount of assembler code to use a specialized cpu instruction to find the lowest non-zero bit.	code	232
buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the deallocated block’s address, rather than traversing the size tags. ultimate performance often requires a small amount of assembler code to use a specialized cpu instruction to find the lowest non-zero bit.	address	121
buddy allocators are fast because the neighboring blocks to coalesce with can be calculated from the deallocated block’s address, rather than traversing the size tags. ultimate performance often requires a small amount of assembler code to use a specialized cpu instruction to find the lowest non-zero bit.	block	50
the main disadvantage of the buddy allocator is that they suffer from internal fragmentation because allocations are rounded up to the nearest block size. for example, a 68-byte allocation will require a 128-byte block.	block	143
the slub allocator is a slab allocator that serves different needs for the linux kernel slub. imagine you are creating an allocator for the kernel, what are your requirements? here is a hypothetical shortlist.	the slub allocator	0
1. first and foremost is you want a low memory footprint to have the kernel be able to be installed on all types of hardware: embedded, desktop, supercomputer, etc.	memory	40
1. first and foremost is you want a low memory footprint to have the kernel be able to be installed on all types of hardware: embedded, desktop, supercomputer, etc.	type	107
2. then, you want the actual memory to be as contiguous as possible to make use of caching. every time a system call is performed, the kernel’s pages need to get loaded into memory. this means that if they are all contiguous, the processor will be able to cache them more efficiently 3. lastly, you want your allocations to be fast.	memory	29
2. then, you want the actual memory to be as contiguous as possible to make use of caching. every time a system call is performed, the kernel’s pages need to get loaded into memory. this means that if they are all contiguous, the processor will be able to cache them more efficiently 3. lastly, you want your allocations to be fast.	system	105
2. then, you want the actual memory to be as contiguous as possible to make use of caching. every time a system call is performed, the kernel’s pages need to get loaded into memory. this means that if they are all contiguous, the processor will be able to cache them more efficiently 3. lastly, you want your allocations to be fast.	a system call	103
enter the slub allocator kmalloc. the slub allocator is a segregated list allocator with minimal splitting and coalescing. the difference here is that the segregated list focuses on more realistic allocation sizes, instead of powers of two. slub also focuses on a low overall memory footprint while keeping pages in the cache. there are blocks of different sizes and the kernel rounds up each allocation request to the lowest block size that satisfies it.	the slub allocator	6
enter the slub allocator kmalloc. the slub allocator is a segregated list allocator with minimal splitting and coalescing. the difference here is that the segregated list focuses on more realistic allocation sizes, instead of powers of two. slub also focuses on a low overall memory footprint while keeping pages in the cache. there are blocks of different sizes and the kernel rounds up each allocation request to the lowest block size that satisfies it.	memory	276
enter the slub allocator kmalloc. the slub allocator is a segregated list allocator with minimal splitting and coalescing. the difference here is that the segregated list focuses on more realistic allocation sizes, instead of powers of two. slub also focuses on a low overall memory footprint while keeping pages in the cache. there are blocks of different sizes and the kernel rounds up each allocation request to the lowest block size that satisfies it.	block	337
one of the big differences between this allocator and the others is that it usually conforms to page sizes. we’ll talk about virtual memory and pages in another chapter, but the kernel will be working with direct memory pages in spans of 4kib or 4096 bytes.	memory	133
guiding questions  is malloc’ed memory initialized? how about calloc’ed or realloc’ed memory?	memory	32
 slab allocation  buddy memory allocation	memory	24
 slab allocation  buddy memory allocation	memory allocation	24
 best fit  worst fit  first fit  buddy allocator  internal fragmentation  external fragmentation  sbrk  natural alignment  boundary tag  coalescing  splitting  slab allocation/memory pool	memory	176
bibliography [1] virtual memory allocation and paging, may 2001.	memory	25
bibliography [1] virtual memory allocation and paging, may 2001.	memory allocation	25
[7] paul r. wilson, mark s. johnstone, michael neely, and david boles. dynamic storage allocation: a survey and critical review. in henry g. baler, editor, memory management, pages 1–116, berlin, heidelberg, 1995.	memory	156
6 threads	thread	2
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	thread	2
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	address	450
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	a process	283
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	parameter	221
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	system	519
a thread is short for ‘thread-of-execution’. it represents the sequence of instructions that the cpu has and will execute. to remember how to return from function calls, and to store the values of automatic variables and parameters a thread uses a stack. almost weirdly, a thread is a process, meaning that creating a thread is similar to fork, except there is no copying meaning no copy on write. what this allows is for a process to share the same address space, variables, heap, file descriptors and etc. the actual system call to create a thread is similar to fork.	a thread	0
it’s clone. we won’t go into the specifics, but you can read the man pages keeping in mind that it is outside the direct scope of this course. lwp or lightweight processes or threads are preferred to forking for a lot of scenarios because there is a lot less overhead creating them. but in some cases, notably python uses this, multiprocessing is the way to make your code faster.	thread	175
it’s clone. we won’t go into the specifics, but you can read the man pages keeping in mind that it is outside the direct scope of this course. lwp or lightweight processes or threads are preferred to forking for a lot of scenarios because there is a lot less overhead creating them. but in some cases, notably python uses this, multiprocessing is the way to make your code faster.	code	368
6.1 processes vs threads	thread	17
 when you are running into synchronization primitives and each process is operating on something in the system.	system	104
 when you have too many threads – the kernel tries to schedule all the threads near each other which could cause more harm than good.	thread	24
 when you don’t want to worry about race conditions  if one thread blocks in a task (say io) then all threads block. processes don’t have that same restriction.	thread	60
 when you don’t want to worry about race conditions  if one thread blocks in a task (say io) then all threads block. processes don’t have that same restriction.	block	67
on the other hand, creating threads is more useful when	thread	28
 you want to leverage the power of a multi-core system to do one task  when you can’t deal with the overhead of processes  when you want communication between the processes simplified  when you want to threads to be part of the same process	thread	202
 you want to leverage the power of a multi-core system to do one task  when you can’t deal with the overhead of processes  when you want communication between the processes simplified  when you want to threads to be part of the same process	system	48
6.2 thread internals	thread	4
your main function and other functions has automatic variables. we will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). if the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables.	thread	204
your main function and other functions has automatic variables. we will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). if the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables.	memory	86
your main function and other functions has automatic variables. we will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). if the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables.	parameter	298
your main function and other functions has automatic variables. we will store them in memory using a stack and keep track of how large the stack is by using a simple pointer (the “stack pointer”). if the thread calls another function, we move our stack pointer down, so that we have more space for parameters and automatic variables.	pointer	166
once it returns from a function, we can move the stack pointer back up to its previous value. we keep a copy of the old stack pointer value - on the stack! this is why returning from a function is quick. it’s easy to ‘free’ the memory used by automatic variables because the program needs to change the stack pointer.	memory	228
once it returns from a function, we can move the stack pointer back up to its previous value. we keep a copy of the old stack pointer value - on the stack! this is why returning from a function is quick. it’s easy to ‘free’ the memory used by automatic variables because the program needs to change the stack pointer.	pointer	55
in a multi-threaded program, there are multiple stacks but only one address space. the pthread library allocates some stack space and uses the clone function call to start the thread at that stack address.	thread	11
in a multi-threaded program, there are multiple stacks but only one address space. the pthread library allocates some stack space and uses the clone function call to start the thread at that stack address.	address	68
1st thread's stack	thread	4
2nd thread's stack	thread	4
figure 6.1: thread stack visualization a program can have more than one thread running inside a process. the programget the first thread for free!	thread	12
figure 6.1: thread stack visualization a program can have more than one thread running inside a process. the programget the first thread for free!	a process	94
it runs the code you write inside ‘main’. if a program need more threads, it can call pthread_create to create a new thread using the pthread library. you’ll need to pass a pointer to a function so that the thread knows where to start.	thread	65
it runs the code you write inside ‘main’. if a program need more threads, it can call pthread_create to create a new thread using the pthread library. you’ll need to pass a pointer to a function so that the thread knows where to start.	code	12
it runs the code you write inside ‘main’. if a program need more threads, it can call pthread_create to create a new thread using the pthread library. you’ll need to pass a pointer to a function so that the thread knows where to start.	pointer	173
the threads all live inside the same virtual memory because they are part of the same process. thus they can all see the heap, the global variables, and the program code.	the heap	117
the threads all live inside the same virtual memory because they are part of the same process. thus they can all see the heap, the global variables, and the program code.	thread	4
the threads all live inside the same virtual memory because they are part of the same process. thus they can all see the heap, the global variables, and the program code.	code	165
the threads all live inside the same virtual memory because they are part of the same process. thus they can all see the heap, the global variables, and the program code.	memory	45
1st thread's stack	thread	4
2nd thread's stack	thread	4
figure 6.2: threads pointing to the same place in the heap thus, a program can have two (or more) cpus working on your program at the same time and inside the same process. it’s up to the operating system to assign the threads to cpus. if a program has more active threads than cpus, the kernel will assign the thread to a cpu for a short duration or until it runs out of things to do and then will automatically switch the cpu to work on another thread. for example, one cpu might be processing the game ai while another thread is computing the graphics output.	the heap	50
figure 6.2: threads pointing to the same place in the heap thus, a program can have two (or more) cpus working on your program at the same time and inside the same process. it’s up to the operating system to assign the threads to cpus. if a program has more active threads than cpus, the kernel will assign the thread to a cpu for a short duration or until it runs out of things to do and then will automatically switch the cpu to work on another thread. for example, one cpu might be processing the game ai while another thread is computing the graphics output.	thread	12
figure 6.2: threads pointing to the same place in the heap thus, a program can have two (or more) cpus working on your program at the same time and inside the same process. it’s up to the operating system to assign the threads to cpus. if a program has more active threads than cpus, the kernel will assign the thread to a cpu for a short duration or until it runs out of things to do and then will automatically switch the cpu to work on another thread. for example, one cpu might be processing the game ai while another thread is computing the graphics output.	system	198
to use pthreads, include pthread.h and compile and link with -pthread or -lpthread compiler option. this option tells the compiler that your program requires threading support. to create a thread, use the function pthread_create. this function takes four arguments:	thread	8
to use pthreads, include pthread.h and compile and link with -pthread or -lpthread compiler option. this option tells the compiler that your program requires threading support. to create a thread, use the function pthread_create. this function takes four arguments:	a thread	187
int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);	thread	5
 the first is a pointer to a variable that will hold the id of the newly created thread.	thread	81
 the first is a pointer to a variable that will hold the id of the newly created thread.	pointer	16
 the second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.	thread	107
 the second is a pointer to attributes that we can use to tweak and tune some of the advanced features of pthreads.	pointer	17
 the third is a pointer to a function that we want to run  fourth is a pointer that will be given to our function the argument void *(*start_routine) (void *) is difficult to read! it means a pointer that takes a void * pointer and returns a void * pointer. it looks like a function declaration except that the name of the function is wrapped with (* .... )	pointer	16
in the above example, the result will be null because the busy function returned null. we need to pass the address-of result because pthread_join will be writing into the contents of our pointer.	thread	134
in the above example, the result will be null because the busy function returned null. we need to pass the address-of result because pthread_join will be writing into the contents of our pointer.	address	107
in the above example, the result will be null because the busy function returned null. we need to pass the address-of result because pthread_join will be writing into the contents of our pointer.	pointer	187
in the man pages, it warns that programmers should use pthread_t as an opaque type and not look at the internals. we do ignore that often, though.	thread	56
in the man pages, it warns that programmers should use pthread_t as an opaque type and not look at the internals. we do ignore that often, though.	type	78
6.4 pthread functions	thread	5
here are some common pthread functions.	thread	22
 pthread_create. creates a new thread. every thread gets a new stack. if a program calls pthread_create twice, your process will contain three stacks - one for each thread. the first thread is created when the process start, the other two after the create. actually, there can be more stacks than this, but let’s keep it simple. the important idea is that each thread requires a stack because the stack contains automatic variables and the old cpu pc register, so that it can go back to executing the calling function after the function is finished.	thread	2
 pthread_cancel stops a thread. note the thread may still continue. for example, it can be terminated	thread	2
 pthread_cancel stops a thread. note the thread may still continue. for example, it can be terminated	a thread	22
when the thread makes an operating system call (e.g. write). in practice, pthread_cancel is rarely used because a thread won’t clean up open resources like files. an alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.	thread	9
when the thread makes an operating system call (e.g. write). in practice, pthread_cancel is rarely used because a thread won’t clean up open resources like files. an alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.	resources	141
when the thread makes an operating system call (e.g. write). in practice, pthread_cancel is rarely used because a thread won’t clean up open resources like files. an alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.	system	35
when the thread makes an operating system call (e.g. write). in practice, pthread_cancel is rarely used because a thread won’t clean up open resources like files. an alternative implementation is to use a boolean (int) variable whose value is used to inform other threads that they should finish and clean up.	a thread	112
 pthread_exit(void *) stops the calling thread meaning the thread never returns after calling pthread_exit.	thread	2
the pthread library will automatically finish the process if no other threads are running. pthread_exit(...) is equivalent to returning from the thread’s function; both finish the thread and also set the return value (void *pointer) for the thread. calling pthread_exit in the main thread is a common way for simple programs to ensure that all threads finish. for example, in the following program, the myfunc threads will probably not have time to get started. on the other hand exit() exits the entire process and sets the process’ exit value. this is equivalent to return (); in the main method. all threads inside the process are stopped. note the pthread_exit version creates thread zombies; however, this is not a long-running process, so we don’t care.	the following	376
the pthread library will automatically finish the process if no other threads are running. pthread_exit(...) is equivalent to returning from the thread’s function; both finish the thread and also set the return value (void *pointer) for the thread. calling pthread_exit in the main thread is a common way for simple programs to ensure that all threads finish. for example, in the following program, the myfunc threads will probably not have time to get started. on the other hand exit() exits the entire process and sets the process’ exit value. this is equivalent to return (); in the main method. all threads inside the process are stopped. note the pthread_exit version creates thread zombies; however, this is not a long-running process, so we don’t care.	thread	5
the pthread library will automatically finish the process if no other threads are running. pthread_exit(...) is equivalent to returning from the thread’s function; both finish the thread and also set the return value (void *pointer) for the thread. calling pthread_exit in the main thread is a common way for simple programs to ensure that all threads finish. for example, in the following program, the myfunc threads will probably not have time to get started. on the other hand exit() exits the entire process and sets the process’ exit value. this is equivalent to return (); in the main method. all threads inside the process are stopped. note the pthread_exit version creates thread zombies; however, this is not a long-running process, so we don’t care.	pointer	224
 pthread_join() waits for a thread to finish and records its return value. finished threads will continue to consume resources. eventually, if enough threads are created, pthread_create will fail. in practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits. this is equivalent to turning your children into zombies, so keep this in mind for long-running processes. in the exit example, we could also wait on all the threads.	thread	2
 pthread_join() waits for a thread to finish and records its return value. finished threads will continue to consume resources. eventually, if enough threads are created, pthread_create will fail. in practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits. this is equivalent to turning your children into zombies, so keep this in mind for long-running processes. in the exit example, we could also wait on all the threads.	resources	117
 pthread_join() waits for a thread to finish and records its return value. finished threads will continue to consume resources. eventually, if enough threads are created, pthread_create will fail. in practice, this is only an issue for long-running processes but is not an issue for simple, short-lived processes as all thread resources are automatically freed when the process exits. this is equivalent to turning your children into zombies, so keep this in mind for long-running processes. in the exit example, we could also wait on all the threads.	a thread	26
there are many ways to exit threads. here is a non-complete list.	thread	28
 returning from the thread function	thread	20
 calling pthread_exit  canceling the thread with pthread_cancel  terminating the process through a signal.	thread	10
 calling exit() or abort()  returning from main  executing another program  unplugging your computer  some undefined behavior can terminate your threads, it is undefined behavior	thread	145
race conditions are whenever the outcome of a program is determined by its sequence of events determined by the processor. this means that the execution of the code is non-deterministic. meaning that the same program can run multiple times and depending on how the kernel schedules the threads could produce inaccurate results.	thread	286
race conditions are whenever the outcome of a program is determined by its sequence of events determined by the processor. this means that the execution of the code is non-deterministic. meaning that the same program can run multiple times and depending on how the kernel schedules the threads could produce inaccurate results.	code	160
the following is the canonical race condition.	the following	0
void *thread_main(void *p) { int *p_int = (int*) p; int x = *p_int; x += x; *p_int = x; return null; } int main() { int data = 1; pthread_t one, two; pthread_create(&one, null, thread_main, &data); pthread_create(&two, null, thread_main, &data); pthread_join(one, null); pthread_join(two, null); printf("%d\n", data); return 0; }	thread	6
void *thread_main(void *p) { int *p_int = (int*) p; int x = *p_int; x += x; *p_int = x; return null; } int main() { int data = 1; pthread_t one, two; pthread_create(&one, null, thread_main, &data); pthread_create(&two, null, thread_main, &data); pthread_join(one, null); pthread_join(two, null); printf("%d\n", data); return 0; }	printf	296
breaking down the assembly there are many different accesses of the code. we will assume that data is stored in the eax register. the code to increment is the following with no optimization (assume int_ptr contains eax).	the following	155
breaking down the assembly there are many different accesses of the code. we will assume that data is stored in the eax register. the code to increment is the following with no optimization (assume int_ptr contains eax).	code	68
thread 2	thread	0
thread 1	thread	0
figure 6.3: thread access - not a race condition this access pattern will cause the variable data to be 4. the problem is when the instructions are executed in parallel.	thread	12
thread 2	thread	0
thread 1	thread	0
figure 6.4: thread access - race condition this access pattern will cause the variable data to be 2. this is undefined behavior and a race condition. what we want is one thread to access the part of the code at a time.	thread	12
figure 6.4: thread access - race condition this access pattern will cause the variable data to be 2. this is undefined behavior and a race condition. what we want is one thread to access the part of the code at a time.	code	203
but when compiled with -o2, assembly output is a single instruction.	assembly output	28
a day at the races here is another small race condition. the following code is supposed to start ten threads with the integers 0 through 9 inclusive. however, when run prints out 1 7 8 8 8 8 8 8 8 10! or seldom does it print out what we expect. can you see why?	the following	57
a day at the races here is another small race condition. the following code is supposed to start ten threads with the integers 0 through 9 inclusive. however, when run prints out 1 7 8 8 8 8 8 8 8 10! or seldom does it print out what we expect. can you see why?	thread	101
a day at the races here is another small race condition. the following code is supposed to start ten threads with the integers 0 through 9 inclusive. however, when run prints out 1 7 8 8 8 8 8 8 8 10! or seldom does it print out what we expect. can you see why?	code	71
the above code suffers from a race condition - the value of i is changing. the new threads start later in the example output the last thread starts after the loop has finished. to overcome this race-condition, we will give each thread a pointer to its own data area. for example, for each thread we may want to store the id, a starting value and an output value. we will instead treat i as a pointer and cast it by value.	thread	83
the above code suffers from a race condition - the value of i is changing. the new threads start later in the example output the last thread starts after the loop has finished. to overcome this race-condition, we will give each thread a pointer to its own data area. for example, for each thread we may want to store the id, a starting value and an output value. we will instead treat i as a pointer and cast it by value.	code	10
the above code suffers from a race condition - the value of i is changing. the new threads start later in the example output the last thread starts after the loop has finished. to overcome this race-condition, we will give each thread a pointer to its own data area. for example, for each thread we may want to store the id, a starting value and an output value. we will instead treat i as a pointer and cast it by value.	pointer	237
the above code suffers from a race condition - the value of i is changing. the new threads start later in the example output the last thread starts after the loop has finished. to overcome this race-condition, we will give each thread a pointer to its own data area. for example, for each thread we may want to store the id, a starting value and an output value. we will instead treat i as a pointer and cast it by value.	the loop	154
race conditions aren’t in our code. they can be in provided code some functions like asctime, getenv, strtok, strerror not thread-safe. let’s look at a simple function that is also not ‘thread-safe’. the result buffer could be stored in global memory. this is good in a single-threaded program. we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. if two threads were to use it at the same time, one would corrupt the other.	thread	123
race conditions aren’t in our code. they can be in provided code some functions like asctime, getenv, strtok, strerror not thread-safe. let’s look at a simple function that is also not ‘thread-safe’. the result buffer could be stored in global memory. this is good in a single-threaded program. we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. if two threads were to use it at the same time, one would corrupt the other.	code	30
race conditions aren’t in our code. they can be in provided code some functions like asctime, getenv, strtok, strerror not thread-safe. let’s look at a simple function that is also not ‘thread-safe’. the result buffer could be stored in global memory. this is good in a single-threaded program. we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. if two threads were to use it at the same time, one would corrupt the other.	address	346
race conditions aren’t in our code. they can be in provided code some functions like asctime, getenv, strtok, strerror not thread-safe. let’s look at a simple function that is also not ‘thread-safe’. the result buffer could be stored in global memory. this is good in a single-threaded program. we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. if two threads were to use it at the same time, one would corrupt the other.	memory	244
race conditions aren’t in our code. they can be in provided code some functions like asctime, getenv, strtok, strerror not thread-safe. let’s look at a simple function that is also not ‘thread-safe’. the result buffer could be stored in global memory. this is good in a single-threaded program. we wouldn’t want to return a pointer to an invalid address on the stack, but there’s only one result buffer in the entire memory. if two threads were to use it at the same time, one would corrupt the other.	pointer	324
char *to_message(int num) { static char result [256]; if (num < 10) sprintf(result, "%d : blah blah" , num); else strcpy(result, "unknown"); return result; }	printf	69
there are ways around this like using synchronization locks, but first let’s do this by design. how would you fix the function above? you can change any of the parameters and any return types. here is one valid solution.	parameter	160
there are ways around this like using synchronization locks, but first let’s do this by design. how would you fix the function above? you can change any of the parameters and any return types. here is one valid solution.	type	186
int to_message_r(int num, char *buf, size_t nbytes) { size_t written; if (num < 10) { written = snprintf(buf, nbtytes, "%d : blah blah" , num); } else { strncpy(buf, "unknown", nbytes); buf[nbytes] = ’\0’; written = strlen(buf) + 1; } return written <= nbytes; }	printf	98
instead of making the function responsible for the memory, we made the caller responsible! a lot of programs, and hopefully your programs, have minimal communication needed. often a malloc call is less work than locking a mutex or sending a message to another thread.	thread	260
instead of making the function responsible for the memory, we made the caller responsible! a lot of programs, and hopefully your programs, have minimal communication needed. often a malloc call is less work than locking a mutex or sending a message to another thread.	a mutex	220
instead of making the function responsible for the memory, we made the caller responsible! a lot of programs, and hopefully your programs, have minimal communication needed. often a malloc call is less work than locking a mutex or sending a message to another thread.	memory	51
a program can fork inside a process with multiple threads! however, the child process only has a single thread, which is a clone of the thread that called fork. we can see this as a simple example, where the background threads never print out a second message in the child process.	thread	50
a program can fork inside a process with multiple threads! however, the child process only has a single thread, which is a clone of the thread that called fork. we can see this as a simple example, where the background threads never print out a second message in the child process.	a process	26
a program can fork inside a process with multiple threads! however, the child process only has a single thread, which is a clone of the thread that called fork. we can see this as a simple example, where the background threads never print out a second message in the child process.	background	208
#include <pthread.h> #include <stdio.h> #include <unistd.h>	thread	11
static pid_t child = -2; void *sleepnprint(void *arg) { printf("%d:%s starting up...\n", getpid(), (char *) arg); while (child == -2) {sleep(1);} /* later we will use condition variables */ printf("%d:%s finishing...\n",getpid(), (char*)arg); return null; } int main() { pthread_t tid1, tid2; pthread_create(&tid1,null, sleepnprint, "new thread one"); pthread_create(&tid2,null, sleepnprint, "new thread two"); child = fork(); printf("%d:%s\n",getpid(), "fork()ing complete"); sleep(3); printf("%d:%s\n",getpid(), "main thread finished"); pthread_exit(null); return 0; /* never executes */ }	thread	272
static pid_t child = -2; void *sleepnprint(void *arg) { printf("%d:%s starting up...\n", getpid(), (char *) arg); while (child == -2) {sleep(1);} /* later we will use condition variables */ printf("%d:%s finishing...\n",getpid(), (char*)arg); return null; } int main() { pthread_t tid1, tid2; pthread_create(&tid1,null, sleepnprint, "new thread one"); pthread_create(&tid2,null, sleepnprint, "new thread two"); child = fork(); printf("%d:%s\n",getpid(), "fork()ing complete"); sleep(3); printf("%d:%s\n",getpid(), "main thread finished"); pthread_exit(null); return 0; /* never executes */ }	printf	56
8970:new thread one starting up...	thread	9
8970:fork()ing complete 8973:fork()ing complete 8970:new thread two starting up...	thread	57
8970:new thread two finishing...	thread	9
8970:new thread one finishing...	thread	9
8970:main thread finished 8973:main thread finished in practice, creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. another thread might have locked a mutex like by calling malloc and never unlock it again. advanced users may find pthread_atfork useful however we suggest a program avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.	thread	10
8970:main thread finished 8973:main thread finished in practice, creating threads before forking can lead to unexpected errors because (as demonstrated above) the other threads are immediately terminated when forking. another thread might have locked a mutex like by calling malloc and never unlock it again. advanced users may find pthread_atfork useful however we suggest a program avoid creating threads before forking unless you fully understand the limitations and difficulties of this approach.	a mutex	251
with your new understanding of threads, all you need to do is create a thread for the left half, and one for the right half. given that your cpu has multiple real cores, you will see a speedup following amdahl’s law. the time complexity analysis gets interesting here as well. the parallel algorithm runs in o(log3 (n)) running time because we have the analysis assumes that we have a lot of cores.	thread	31
with your new understanding of threads, all you need to do is create a thread for the left half, and one for the right half. given that your cpu has multiple real cores, you will see a speedup following amdahl’s law. the time complexity analysis gets interesting here as well. the parallel algorithm runs in o(log3 (n)) running time because we have the analysis assumes that we have a lot of cores.	a thread	69
in practice though, we typically do two changes. one, once the array gets small enough, we ditch the parallel merge sort algorithm and do conventional sort that works fast on small arrays, usually cache coherency rules at this level. the other thing that we know is that cpus don’t have infinite cores. to get around that, we typically keep a worker pool. you won’t see the speedup right away because of things like cache coherency and scheduling extra threads. over the bigger pieces of code though, you will start to see speedups.	thread	453
in practice though, we typically do two changes. one, once the array gets small enough, we ditch the parallel merge sort algorithm and do conventional sort that works fast on small arrays, usually cache coherency rules at this level. the other thing that we know is that cpus don’t have infinite cores. to get around that, we typically keep a worker pool. you won’t see the speedup right away because of things like cache coherency and scheduling extra threads. over the bigger pieces of code though, you will start to see speedups.	code	488
in practice though, we typically do two changes. one, once the array gets small enough, we ditch the parallel merge sort algorithm and do conventional sort that works fast on small arrays, usually cache coherency rules at this level. the other thing that we know is that cpus don’t have infinite cores. to get around that, we typically keep a worker pool. you won’t see the speedup right away because of things like cache coherency and scheduling extra threads. over the bigger pieces of code though, you will start to see speedups.	a thread	451
since none of the elements depend on any other element, how would you go about parallelizing this? what do you think would be the best way to split up the work between threads.	thread	168
check out thread scheduling in the appendix for more ways to schedule.	thread	10
 notable real-world examples include distributed.net and proof-of-work systems used in cryptocurrency.	system	71
 blast searches in bioinformatics for multiple queries (but not for individual large queries)  large scale facial recognition systems that compare thousands of arbitrary acquired faces (e.g., a security or surveillance video via closed-circuit television) with a similarly large number of previously stored faces (e.g., a rogues gallery or similar watch list).	system	126
in the beginning of the chapter, we mentioned that threads are processes. what do we mean by that? you can create a thread like a process take a look at the example code below	thread	51
in the beginning of the chapter, we mentioned that threads are processes. what do we mean by that? you can create a thread like a process take a look at the example code below	code	165
in the beginning of the chapter, we mentioned that threads are processes. what do we mean by that? you can create a thread like a process take a look at the example code below	a process	128
in the beginning of the chapter, we mentioned that threads are processes. what do we mean by that? you can create a thread like a process take a look at the example code below	a thread	114
it seems pretty simple right? why not use this functionality? first, there is a decent bit of boilerplate code.	code	106
in addition, pthreads are part of the posix standard and have defined functionality. pthreads let a program set various attributes – some that resemble the option in clone – to customize your thread. but as we mentioned earlier, with each later of abstraction for portability reasons we lose some functionality. clone can do some neat things like keeping different parts of your heap the same while creating copies of other pages. a program has finer control of scheduling because it is a process with the same mappings.	thread	14
in addition, pthreads are part of the posix standard and have defined functionality. pthreads let a program set various attributes – some that resemble the option in clone – to customize your thread. but as we mentioned earlier, with each later of abstraction for portability reasons we lose some functionality. clone can do some neat things like keeping different parts of your heap the same while creating copies of other pages. a program has finer control of scheduling because it is a process with the same mappings.	a process	487
guiding questions  what is the first argument to pthread create?	thread	50
 what is the start routing in pthread create? how about arg?	thread	31
 why might pthread create fail?	thread	12
 what are a few things that threads share in a process? what are a few things that threads have different?	thread	28
 what are a few things that threads share in a process? what are a few things that threads have different?	a process	45
 how can a thread uniquely identify itself?	thread	11
 how can a thread uniquely identify itself?	a thread	9
 what are some examples of non thread safe library functions? why might they not be thread safe?	thread	31
 how can a program stop a thread?	thread	26
 how can a program stop a thread?	a thread	24
 how can a program get back a thread’s "return value"?	thread	30
 how can a program get back a thread’s "return value"?	a thread	28
 man page  pthread reference guide  concise third party sample code explaining create, join and exit	thread	12
 man page  pthread reference guide  concise third party sample code explaining create, join and exit	code	63
 pthread life-cycle  each thread has a stack  capturing return values from a thread  using pthread_join  using pthread_create  using pthread_exit  under what conditions will a process exit	thread	2
 pthread life-cycle  each thread has a stack  capturing return values from a thread  using pthread_join  using pthread_create  using pthread_exit  under what conditions will a process exit	a process	174
 pthread life-cycle  each thread has a stack  capturing return values from a thread  using pthread_join  using pthread_create  using pthread_exit  under what conditions will a process exit	a thread	75
 what happens when a pthread gets created?	thread	22
 where is each thread’s stack?	thread	15
 how does a program get a return value given a pthread_t? what are the ways a thread can set that return value? what happens if a program discards the return value?	thread	48
 how does a program get a return value given a pthread_t? what are the ways a thread can set that return value? what happens if a program discards the return value?	a thread	76
 why is pthread_join important (think stack space, registers, return values)?	thread	9
 what does pthread_exit do if it is not the last thread? what other functions are called when after calling pthread_exit?	thread	12
 give me three conditions under which a multi-threaded process will exit. are there any more?	thread	46
bibliography [1] part guide. intel® 64 and ia-32 architectures software developers manual. volume 3b: system programming guide, part, 2, 2011.	system	102
when multithreading gets interesting	thread	10
synchronization coordinates various tasks so that they all finishin the the correct state. in c, we have series of mechanisms to control what threads are allowed to perform at a given state. most of the time, the threads can progress without having to communicate, but every so often two or more threads may want to access a critical section. a critical section is a section of code that can only be executed by one thread at a time if the program is to function correctly. if two threads (or processes) were to execute code inside the critical section at the same time, it is possible that the program may no longer have the correct behavior.	thread	142
synchronization coordinates various tasks so that they all finishin the the correct state. in c, we have series of mechanisms to control what threads are allowed to perform at a given state. most of the time, the threads can progress without having to communicate, but every so often two or more threads may want to access a critical section. a critical section is a section of code that can only be executed by one thread at a time if the program is to function correctly. if two threads (or processes) were to execute code inside the critical section at the same time, it is possible that the program may no longer have the correct behavior.	code	378
synchronization coordinates various tasks so that they all finishin the the correct state. in c, we have series of mechanisms to control what threads are allowed to perform at a given state. most of the time, the threads can progress without having to communicate, but every so often two or more threads may want to access a critical section. a critical section is a section of code that can only be executed by one thread at a time if the program is to function correctly. if two threads (or processes) were to execute code inside the critical section at the same time, it is possible that the program may no longer have the correct behavior.	section	334
as we said in the previous chapter, race conditions happen when an operation touches a piece of memory at the same time as another thread. if the memory location is only accessible by one thread, for example the automatic variable i below, then there is no possibility of a race condition and no critical section associated with i. however, the sum variable is a global variable and accessed by two threads. it is possible that two threads may attempt to increment the variable at the same time.	the sum variable	341
as we said in the previous chapter, race conditions happen when an operation touches a piece of memory at the same time as another thread. if the memory location is only accessible by one thread, for example the automatic variable i below, then there is no possibility of a race condition and no critical section associated with i. however, the sum variable is a global variable and accessed by two threads. it is possible that two threads may attempt to increment the variable at the same time.	thread	131
as we said in the previous chapter, race conditions happen when an operation touches a piece of memory at the same time as another thread. if the memory location is only accessible by one thread, for example the automatic variable i below, then there is no possibility of a race condition and no critical section associated with i. however, the sum variable is a global variable and accessed by two threads. it is possible that two threads may attempt to increment the variable at the same time.	memory	96
as we said in the previous chapter, race conditions happen when an operation touches a piece of memory at the same time as another thread. if the memory location is only accessible by one thread, for example the automatic variable i below, then there is no possibility of a race condition and no critical section associated with i. however, the sum variable is a global variable and accessed by two threads. it is possible that two threads may attempt to increment the variable at the same time.	section	305
a typical output of the above code is argggh sum is <some number less than expected> because there is a race condition. the code allows two threads to read and write sum at the same time. for example, both threads copy the current value of sum into cpu that runs each thread (let’s pick 123). both threads increment one to their own copy. both threads write back the value (124). if the threads had accessed the sum at different times then the count would have been 125. a few of the possible different orderings are below.	thread	140
a typical output of the above code is argggh sum is <some number less than expected> because there is a race condition. the code allows two threads to read and write sum at the same time. for example, both threads copy the current value of sum into cpu that runs each thread (let’s pick 123). both threads increment one to their own copy. both threads write back the value (124). if the threads had accessed the sum at different times then the count would have been 125. a few of the possible different orderings are below.	code	30
thread 1 load addr, add 1 (i=1 locally) store (i=1 globally) ...	thread	0
table 7.1: good thread access pattern thread 2 ...	thread	16
thread 1 load addr, add 1 (i=1 locally) store (i=1 globally) ...	thread	0
table 7.2: bad thread access pattern thread 2 ...	thread	15
thread 1 load addr, add 1 (i=1 locally) store (i=1 globally)	thread	0
table 7.3: horrible thread access pattern thread 2 load addr, add 1 (i=1 locally) store (i=1 globally)	thread	20
we would like the first pattern of the code being mutually exclusive. which leads us to our first synchronization primitive, a mutex.	a mutex	125
we would like the first pattern of the code being mutually exclusive. which leads us to our first synchronization primitive, a mutex.	code	39
to ensure that only one thread at a time can access a global variable, use a mutex – short for mutual exclusion.	thread	24
to ensure that only one thread at a time can access a global variable, use a mutex – short for mutual exclusion.	a mutex	75
if one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. a mutex isn’t a primitive in the truest sense, though it is one of the smallest that has useful threading api. a mutex also isn’t a data structure. it is an abstract data type.	a struct	261
if one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. a mutex isn’t a primitive in the truest sense, though it is one of the smallest that has useful threading api. a mutex also isn’t a data structure. it is an abstract data type.	thread	7
if one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. a mutex isn’t a primitive in the truest sense, though it is one of the smallest that has useful threading api. a mutex also isn’t a data structure. it is an abstract data type.	a mutex	126
if one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. a mutex isn’t a primitive in the truest sense, though it is one of the smallest that has useful threading api. a mutex also isn’t a data structure. it is an abstract data type.	type	297
if one thread is currently inside a critical section we would like another thread to wait until the first thread is complete. a mutex isn’t a primitive in the truest sense, though it is one of the smallest that has useful threading api. a mutex also isn’t a data structure. it is an abstract data type.	section	45
there are many ways to implement a mutex, and we’ll give a few in this chapter. for right now let’s use the black box that the pthread library gives us. here is how we declare a mutex.	thread	128
there are many ways to implement a mutex, and we’ll give a few in this chapter. for right now let’s use the black box that the pthread library gives us. here is how we declare a mutex.	a mutex	33
there are a few ways of initializing a mutex. a program can use the macro pthread_mutex_initializer only for global (‘static’) variables. m = pthread_mutex_initializer is functionally equivalent to the more general purpose pthread_mutex_init(m,null). the init version includes options to trade performance for additional error-checking and advanced sharing options. the init version also makes sure that the mutex is correctly initialized after the call, global mutexes are initialized on the first lock. a program can also call the init function inside of a program for a mutex located on the heap.	the heap	590
there are a few ways of initializing a mutex. a program can use the macro pthread_mutex_initializer only for global (‘static’) variables. m = pthread_mutex_initializer is functionally equivalent to the more general purpose pthread_mutex_init(m,null). the init version includes options to trade performance for additional error-checking and advanced sharing options. the init version also makes sure that the mutex is correctly initialized after the call, global mutexes are initialized on the first lock. a program can also call the init function inside of a program for a mutex located on the heap.	thread	75
there are a few ways of initializing a mutex. a program can use the macro pthread_mutex_initializer only for global (‘static’) variables. m = pthread_mutex_initializer is functionally equivalent to the more general purpose pthread_mutex_init(m,null). the init version includes options to trade performance for additional error-checking and advanced sharing options. the init version also makes sure that the mutex is correctly initialized after the call, global mutexes are initialized on the first lock. a program can also call the init function inside of a program for a mutex located on the heap.	a mutex	37
once we are finished with the mutex we should also call pthread_mutex_destroy(m) too. note, a program can only destroy an unlocked mutex, destroy on a locked mutex is undefined behavior. things to keep in mind about init and destroy a program doesn’t need to destroy a mutex created with the global initializer.	thread	57
once we are finished with the mutex we should also call pthread_mutex_destroy(m) too. note, a program can only destroy an unlocked mutex, destroy on a locked mutex is undefined behavior. things to keep in mind about init and destroy a program doesn’t need to destroy a mutex created with the global initializer.	a mutex	267
1. multiple threads init/destroy has undefined behavior 2. destroying a locked mutex has undefined behavior 3. keep to the pattern of one and only one thread initializing a mutex.	thread	12
1. multiple threads init/destroy has undefined behavior 2. destroying a locked mutex has undefined behavior 3. keep to the pattern of one and only one thread initializing a mutex.	a mutex	171
4. copying the bytes of the mutex to a new memory location and then using the copy is not supported. to reference a mutex, a program must to have a pointer to that memory address.	a mutex	114
4. copying the bytes of the mutex to a new memory location and then using the copy is not supported. to reference a mutex, a program must to have a pointer to that memory address.	address	171
4. copying the bytes of the mutex to a new memory location and then using the copy is not supported. to reference a mutex, a program must to have a pointer to that memory address.	memory	43
4. copying the bytes of the mutex to a new memory location and then using the copy is not supported. to reference a mutex, a program must to have a pointer to that memory address.	pointer	148
how does one use a mutex? here is a complete example in the spirit of the earlier piece of code.	a mutex	17
how does one use a mutex? here is a complete example in the spirit of the earlier piece of code.	code	91
in the code above, the thread gets the lock to the counting house before entering. the critical section is only the sum+=1 so the following version is also correct.	the following	126
in the code above, the thread gets the lock to the counting house before entering. the critical section is only the sum+=1 so the following version is also correct.	thread	23
in the code above, the thread gets the lock to the counting house before entering. the critical section is only the sum+=1 so the following version is also correct.	code	7
in the code above, the thread gets the lock to the counting house before entering. the critical section is only the sum+=1 so the following version is also correct.	section	96
for (i = 0; i < 10000000; i++) { pthread_mutex_lock(&m); sum += 1; pthread_mutex_unlock(&m); } return null; }	thread	34
this process runs slower because we lock and unlock the mutex a million times, which is expensive - at least compared with incrementing a variable. in this simple example, we didn’t need threads - we could have added up twice! a faster multi-thread example would be to add one million using an automatic (local) variable and only then adding it to a shared total after the calculation loop has finished:	thread	187
int local = 0; for (i = 0; i < 10000000; i++) { local += 1; } pthread_mutex_lock(&m); sum += local; pthread_mutex_unlock(&m); return null; }	thread	63
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	a struct	92
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	the following	418
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	thread	167
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	a mutex	69
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	code	119
starting with the gotchas. firstly, c mutexes do not lock variables. a mutex is a simple data structure. it works with code, not data. if a mutex is locked, the other threads will continue. it’s only when a thread attempts to lock a mutex that is already locked, will the thread have to wait. as soon as the original thread unlocks the mutex, the second (waiting) thread will acquire the lock and be able to continue. the following code creates a mutex that does effectively nothing.	a thread	205
here are some other gotchas in no particular order 1. don’t cross the streams! if using threads, don’t fork in the middle of your program. this means any time after your mutexes have been initialized.	thread	88
2. the thread that locks a mutex is the only thread that can unlock it.	thread	7
2. the thread that locks a mutex is the only thread that can unlock it.	a mutex	25
3. each program can have multiple mutex locks. a thread safe design might include a lock with each data structure, one lock per heap, or one lock per set of data structures if a program has only one lock, then there may be significant contention for the lock. if two threads were updating two different counters, it isn’t necessary to use the same lock.	a struct	102
3. each program can have multiple mutex locks. a thread safe design might include a lock with each data structure, one lock per heap, or one lock per set of data structures if a program has only one lock, then there may be significant contention for the lock. if two threads were updating two different counters, it isn’t necessary to use the same lock.	thread	49
3. each program can have multiple mutex locks. a thread safe design might include a lock with each data structure, one lock per heap, or one lock per set of data structures if a program has only one lock, then there may be significant contention for the lock. if two threads were updating two different counters, it isn’t necessary to use the same lock.	a thread	47
4. locks are only tools. they don’t spot critical sections!	section	50
5. there will always be a small amount of overhead of calling pthread_mutex_lock and pthread_mutex_unlock.	thread	63
6. not unlocking a mutex due to an early return during an error condition 7. resource leak (not calling pthread_mutex_destroy) 8. using an uninitialized mutex or using a mutex that has already been destroyed 9. locking a mutex twice on a thread without unlocking first 10. deadlock	thread	105
6. not unlocking a mutex due to an early return during an error condition 7. resource leak (not calling pthread_mutex_destroy) 8. using an uninitialized mutex or using a mutex that has already been destroyed 9. locking a mutex twice on a thread without unlocking first 10. deadlock	a mutex	17
6. not unlocking a mutex due to an early return during an error condition 7. resource leak (not calling pthread_mutex_destroy) 8. using an uninitialized mutex or using a mutex that has already been destroyed 9. locking a mutex twice on a thread without unlocking first 10. deadlock	a thread	236
so we have this cool data structure. how do we implement it? a naive, incorrect implementation is shown below.	a struct	24
the unlock function simply unlocks the mutex and returns. the lock function first checks to see if the lock is already locked. if it is currently locked, it will keep checking again until another thread has unlocked the mutex.	thread	196
for the time being, we’ll avoid the condition that other threads are able to unlock a lock they don’t own and focus on the mutual exclusion aspect.	thread	57
version 1 uses ‘busy-waiting’ unnecessarily wasting cpu resources. however, there is a more serious problem.	resources	56
we have a race-condition! if two threads both called lock concurrently, it is possible that both threads would read m_locked as zero. thus both threads would believe they have exclusive access to the lock and both threads will continue.	thread	33
we might attempt to reduce the cpu overhead a little by calling pthread_yield() inside the loop - pthread_yield suggests to the operating system that the thread does not use the cpu for a short while, so the cpu may be assigned to threads that are waiting to run. this still leaves the race-condition. we need a better implementation.	thread	65
we might attempt to reduce the cpu overhead a little by calling pthread_yield() inside the loop - pthread_yield suggests to the operating system that the thread does not use the cpu for a short while, so the cpu may be assigned to threads that are waiting to run. this still leaves the race-condition. we need a better implementation.	system	138
we might attempt to reduce the cpu overhead a little by calling pthread_yield() inside the loop - pthread_yield suggests to the operating system that the thread does not use the cpu for a short while, so the cpu may be assigned to threads that are waiting to run. this still leaves the race-condition. we need a better implementation.	the loop	87
we will talk about this later in the critical section part of this chapter. for now, we will talk about semaphores.	section	46
7.1.4 advanced: implementing a mutex with hardware	a mutex	29
first the data structure and initialization code.	a struct	13
first the data structure and initialization code.	code	44
first the data structure and initialization code.	the data structure	6
this is the initialization code, nothing fancy here. we set the state of the mutex to unlocked and set the owner to locked.	code	27
what does this code do? it initializes a variable that we will keep as the unlocked state. atomic compare and exchange is an instruction supported by most modern architectures (on x86 it’s lock cmpxchg). the pseudocode for this operation looks like this	code	15
what does this code do? it initializes a variable that we will keep as the unlocked state. atomic compare and exchange is an instruction supported by most modern architectures (on x86 it’s lock cmpxchg). the pseudocode for this operation looks like this	exchange	110
int atomic_compare_exchange_pseudo(int* addr1, int* addr2, int val){ if(*addr1 == *addr2){ *addr1 = val; return 1; }else{ *addr2 = *addr1; return 0; } }	exchange	19
when we wake up we try to grab the lock again. once we successfully swap, we are in the critical section! we set the mutex’s owner to the current thread for the unlock method and return successfully.	thread	146
when we wake up we try to grab the lock again. once we successfully swap, we are in the critical section! we set the mutex’s owner to the current thread for the unlock method and return successfully.	section	97
how does this guarantee mutual exclusion? when working with atomics we are unsure! but in this simple example, we can because the thread that can successfully expect the lock to be unlocked (0) and swap it to a locked (1) state is considered the winner. how do we implement unlock?	thread	130
to satisfy the api, a thread can’t unlock the mutex unless the thread is the one who owns it. then we unassign the mutex owner, because critical section is over after the atomic. we want a strong exchange because we don’t want to block. we expect the mutex to be locked, and we swap it to unlock. if the swap was successful, we unlocked the mutex. if the swap wasn’t, that means that the mutex was unlocked and we tried to switch it from unlocked to unlocked, preserving the behavior of unlock.	thread	22
to satisfy the api, a thread can’t unlock the mutex unless the thread is the one who owns it. then we unassign the mutex owner, because critical section is over after the atomic. we want a strong exchange because we don’t want to block. we expect the mutex to be locked, and we swap it to unlock. if the swap was successful, we unlocked the mutex. if the swap wasn’t, that means that the mutex was unlocked and we tried to switch it from unlocked to unlocked, preserving the behavior of unlock.	block	230
to satisfy the api, a thread can’t unlock the mutex unless the thread is the one who owns it. then we unassign the mutex owner, because critical section is over after the atomic. we want a strong exchange because we don’t want to block. we expect the mutex to be locked, and we swap it to unlock. if the swap was successful, we unlocked the mutex. if the swap wasn’t, that means that the mutex was unlocked and we tried to switch it from unlocked to unlocked, preserving the behavior of unlock.	a thread	20
to satisfy the api, a thread can’t unlock the mutex unless the thread is the one who owns it. then we unassign the mutex owner, because critical section is over after the atomic. we want a strong exchange because we don’t want to block. we expect the mutex to be locked, and we swap it to unlock. if the swap was successful, we unlocked the mutex. if the swap wasn’t, that means that the mutex was unlocked and we tried to switch it from unlocked to unlocked, preserving the behavior of unlock.	exchange	196
to satisfy the api, a thread can’t unlock the mutex unless the thread is the one who owns it. then we unassign the mutex owner, because critical section is over after the atomic. we want a strong exchange because we don’t want to block. we expect the mutex to be locked, and we swap it to unlock. if the swap was successful, we unlocked the mutex. if the swap wasn’t, that means that the mutex was unlocked and we tried to switch it from unlocked to unlocked, preserving the behavior of unlock.	section	145
what is this memory order business? we were talking about memory fences earlier, here it is! we won’t go into detail because it is outside the scope of this course but in the scope of this article. we need consistency to make sure no loads or stores are ordered before or after. a program need to create dependency chains for more efficient ordering.	memory	13
a semaphore is another synchronization primitive. it is initialized to some value. threads can either sem_wait or sem_post which lowers or increases the value. if the value reaches zero and a wait is called, the thread will be blocked until a post is called.	thread	83
a semaphore is another synchronization primitive. it is initialized to some value. threads can either sem_wait or sem_post which lowers or increases the value. if the value reaches zero and a wait is called, the thread will be blocked until a post is called.	block	227
using a semaphore is as easy as using a mutex. first, decide if on the initial value, for example the number of remaining spaces in an array. unlike pthread mutex there are no shortcuts to creating a semaphore - use sem_init.	thread	150
using a semaphore is as easy as using a mutex. first, decide if on the initial value, for example the number of remaining spaces in an array. unlike pthread mutex there are no shortcuts to creating a semaphore - use sem_init.	a mutex	38
when using a semaphore, wait and post can be called from different threads! unlike a mutex, the increment and decrement can be from different threads.	thread	67
when using a semaphore, wait and post can be called from different threads! unlike a mutex, the increment and decrement can be from different threads.	a mutex	83
this becomes especially useful if you want to use a semaphore to implement a mutex. a mutex is a semaphore that always waits before it posts. some textbooks will refer to a mutex as a binary semaphore. you do have to be careful to never add more than one to a semaphore or otherwise your mutex abstraction breaks. that is usually why a mutex is used to implement a semaphore and vice versa.	a mutex	75
 replace pthread_mutex_lock with sem_wait  replace pthread_mutex_unlock with sem_post	thread	10
but be warned, it isn’t the same! a mutex can handle what we call lock inversion well. meaning the following code breaks with a traditional mutex, but produces a race condition with threads.	the following	95
but be warned, it isn’t the same! a mutex can handle what we call lock inversion well. meaning the following code breaks with a traditional mutex, but produces a race condition with threads.	thread	182
but be warned, it isn’t the same! a mutex can handle what we call lock inversion well. meaning the following code breaks with a traditional mutex, but produces a race condition with threads.	a mutex	34
but be warned, it isn’t the same! a mutex can handle what we call lock inversion well. meaning the following code breaks with a traditional mutex, but produces a race condition with threads.	code	109
also, binary semaphores are different than mutexes because one thread can unlock a mutex from a different thread.	thread	63
also, binary semaphores are different than mutexes because one thread can unlock a mutex from a different thread.	a mutex	81
signal safety also, sem_post is one of a handful of functions that can be correctly used inside a signal handler pthread_mutex_unlock is not. we can release a waiting thread that can now make all of the calls that we disallowed to call inside the signal handler itself e.g. printf. here is some code that utilizes this;	thread	114
signal safety also, sem_post is one of a handful of functions that can be correctly used inside a signal handler pthread_mutex_unlock is not. we can release a waiting thread that can now make all of the calls that we disallowed to call inside the signal handler itself e.g. printf. here is some code that utilizes this;	code	295
signal safety also, sem_post is one of a handful of functions that can be correctly used inside a signal handler pthread_mutex_unlock is not. we can release a waiting thread that can now make all of the calls that we disallowed to call inside the signal handler itself e.g. printf. here is some code that utilizes this;	printf	274
<stdio.h> <pthread.h> <signal.h> <semaphore.h> <unistd.h>	thread	12
sem_t s; void handler(int signal) { sem_post(&s); /* release the kraken! */ } void *singsong(void *param) { sem_wait(&s); printf("waiting until a signal releases...\n"); } int main() { int ok = sem_init(&s, 0, 0 /* initial value of zero*/); if (ok == -1) { perror("could not create unnamed semaphore");	printf	122
other uses for semaphores are keeping track of empty spaces in arrays. we will discuss these in the thread-safe data structures section.	a struct	115
other uses for semaphores are keeping track of empty spaces in arrays. we will discuss these in the thread-safe data structures section.	thread	100
other uses for semaphores are keeping track of empty spaces in arrays. we will discuss these in the thread-safe data structures section.	section	128
condition variables allow a set of threads to sleep until woken up. the api allows either one or all threads to be woken up. if a program only wakes one thread, the operating system will decide which thread to wake up.	thread	35
condition variables allow a set of threads to sleep until woken up. the api allows either one or all threads to be woken up. if a program only wakes one thread, the operating system will decide which thread to wake up.	system	175
threads don’t wake threads other directly like by id. instead, a thread ‘signal’s the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.	thread	0
threads don’t wake threads other directly like by id. instead, a thread ‘signal’s the condition variable, which then will wake up one (or all) threads that are sleeping inside the condition variable.	a thread	63
condition variables are also used with a mutex and with a loop, so when woken up they have to check a condition in a critical section. if a thread needs to be woken up outside of a critical section, there are other ways to do this in posix. threads sleeping inside a condition variable are woken up by calling pthread_cond_broadcast (wake up all) or pthread_cond_signal (wake up one). note despite the function name, this has nothing to do with posix signals!	thread	140
condition variables are also used with a mutex and with a loop, so when woken up they have to check a condition in a critical section. if a thread needs to be woken up outside of a critical section, there are other ways to do this in posix. threads sleeping inside a condition variable are woken up by calling pthread_cond_broadcast (wake up all) or pthread_cond_signal (wake up one). note despite the function name, this has nothing to do with posix signals!	a mutex	39
condition variables are also used with a mutex and with a loop, so when woken up they have to check a condition in a critical section. if a thread needs to be woken up outside of a critical section, there are other ways to do this in posix. threads sleeping inside a condition variable are woken up by calling pthread_cond_broadcast (wake up all) or pthread_cond_signal (wake up one). note despite the function name, this has nothing to do with posix signals!	a thread	138
condition variables are also used with a mutex and with a loop, so when woken up they have to check a condition in a critical section. if a thread needs to be woken up outside of a critical section, there are other ways to do this in posix. threads sleeping inside a condition variable are woken up by calling pthread_cond_broadcast (wake up all) or pthread_cond_signal (wake up one). note despite the function name, this has nothing to do with posix signals!	section	126
occasionally, a waiting thread may appear to wake up for no reason. this is called a spurious wakeup. if you read the hardware implementation of a mutex section, this is similar to the atomic failure of the same name.	thread	24
occasionally, a waiting thread may appear to wake up for no reason. this is called a spurious wakeup. if you read the hardware implementation of a mutex section, this is similar to the atomic failure of the same name.	a mutex	145
occasionally, a waiting thread may appear to wake up for no reason. this is called a spurious wakeup. if you read the hardware implementation of a mutex section, this is similar to the atomic failure of the same name.	section	153
why do spurious wakeups happen? for performance. on multi-cpu systems, it is possible that a race condition could cause a wake-up (signal) request to be unnoticed. the kernel may not detect this lost wake-up call but can detect when it might occur. to avoid the potentially lost signal, the thread is woken up so that the program code can test the condition again. if you want to know why, check out the appendix.	thread	291
why do spurious wakeups happen? for performance. on multi-cpu systems, it is possible that a race condition could cause a wake-up (signal) request to be unnoticed. the kernel may not detect this lost wake-up call but can detect when it might occur. to avoid the potentially lost signal, the thread is woken up so that the program code can test the condition again. if you want to know why, check out the appendix.	code	330
why do spurious wakeups happen? for performance. on multi-cpu systems, it is possible that a race condition could cause a wake-up (signal) request to be unnoticed. the kernel may not detect this lost wake-up call but can detect when it might occur. to avoid the potentially lost signal, the thread is woken up so that the program code can test the condition again. if you want to know why, check out the appendix.	system	62
7.3 thread-safe data structures	a struct	19
7.3 thread-safe data structures	thread	4
naturally, we want our data structures to be thread-safe as well! we can use mutexes and synchronization primitives to make that happen. first a few definitions. atomicity is when an operation is thread-safe. we have atomic instructions in hardware by providing the lock prefix	a struct	26
naturally, we want our data structures to be thread-safe as well! we can use mutexes and synchronization primitives to make that happen. first a few definitions. atomicity is when an operation is thread-safe. we have atomic instructions in hardware by providing the lock prefix	thread	45
but atomicity also applies to higher orders of operations. we say a data structure operation is atomic if it happens all at once and successfully or not at all.	a struct	71
as such, we can use synchronization primitives to make our data structures thread-safe. for the most part, we will be using mutexes because they carry more semantic meaning than a binary semaphore. note, this is an introduction. writing high-performance thread-safe data structures requires its own book! take for example the following thread-unsafe stack.	a struct	62
as such, we can use synchronization primitives to make our data structures thread-safe. for the most part, we will be using mutexes because they carry more semantic meaning than a binary semaphore. note, this is an introduction. writing high-performance thread-safe data structures requires its own book! take for example the following thread-unsafe stack.	the following	322
as such, we can use synchronization primitives to make our data structures thread-safe. for the most part, we will be using mutexes because they carry more semantic meaning than a binary semaphore. note, this is an introduction. writing high-performance thread-safe data structures requires its own book! take for example the following thread-unsafe stack.	thread	75
version 1 of the stack is thread-unsafe because if two threads call push or pop at the same time then the results or the stack can be inconsistent. for example, imagine if two threads call pop at the same time then both threads may read the same value, both may read the original count value.	thread	26
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	a struct	35
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	thread	20
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	a mutex	521
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	code	96
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	memory	266
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	a thread	18
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	the data structure	349
to turn this into a thread-safe data structure we need to identify the critical sections of our code, meaning we need to ask which section(s) of the code must only have one thread at a time. in the above example the push, pop, and is_empty functions access the same memory and all critical sections for the stack. while push (and pop) is executing, the data structure is an inconsistent state, for example the count may not have been written to, so it may still contain the original value. by wrapping these methods with a mutex we can ensure that only one thread at a time can update (or read) the stack. a candidate ‘solution’ is shown below. is it correct? if not, how will it fail?	section	80
values[count++] = v; pthread_mutex_unlock(&m1); } double pop() { pthread_mutex_lock(&m2); double v = values[--count]; pthread_mutex_unlock(&m2); return v; } int is_empty() { pthread_mutex_lock(&m1); return count == 0; pthread_mutex_unlock(&m1); }	thread	22
if three threads called push() at the same time, the lock m1 ensures that only one thread at time manipulates the stack on push or is_empty – two threads will need to wait until the first thread completes a similar argument applies to concurrent calls to pop. however, version 2 does not prevent push and pop from running at the same time because push and pop use two different mutex locks. the fix is simple in this case - use the same mutex lock for both the push and pop functions.	thread	9
the code has a second error. is_empty returns after the comparison and leaves the mutex unlocked. however, the error would not be spotted immediately. for example, suppose one thread calls is_empty and a second thread later calls push. this thread would mysteriously stop. using debugger, you can discover that the thread is stuck at the lock() method inside the push method because the lock was never unlocked by the earlier is_empty call.	thread	176
the code has a second error. is_empty returns after the comparison and leaves the mutex unlocked. however, the error would not be spotted immediately. for example, suppose one thread calls is_empty and a second thread later calls push. this thread would mysteriously stop. using debugger, you can discover that the thread is stuck at the lock() method inside the push method because the lock was never unlocked by the earlier is_empty call.	code	4
thus an oversight in one thread led to problems much later in time in an arbitrary other thread. let’s try to rectify these problems	thread	25
return v; } int is_empty() { pthread_mutex_lock(&m); int result = count == 0; pthread_mutex_unlock(&m); return result; }	thread	30
version 3 is thread-safe. we have ensured mutual exclusion for all of the critical sections. there are a few things to note.	thread	13
version 3 is thread-safe. we have ensured mutual exclusion for all of the critical sections. there are a few things to note.	section	83
 is_empty is thread-safe but its result may already be out-of-date. the stack may no longer be empty by the time the thread gets the result! this is usually why in thread-safe data structures, functions that return sizes are removed or deprecated.	a struct	179
 is_empty is thread-safe but its result may already be out-of-date. the stack may no longer be empty by the time the thread gets the result! this is usually why in thread-safe data structures, functions that return sizes are removed or deprecated.	thread	13
 there is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack) the last point can be fixed using counting semaphores. the implementation assumes a single stack. a more general-purpose version might include the mutex as part of the memory structure and use pthread_mutex_init to initialize the mutex. for example,	thread	313
 there is no protection against underflow (popping on an empty stack) or overflow (pushing onto an already-full stack) the last point can be fixed using counting semaphores. the implementation assumes a single stack. a more general-purpose version might include the mutex as part of the memory structure and use pthread_mutex_init to initialize the mutex. for example,	memory	287
void push(stack_t *s, double v) { pthread_mutex_lock(&s->m);	thread	35
s->values[(s->count)++] = v; pthread_mutex_unlock(&s->m); } double pop(stack_t *s) { pthread_mutex_lock(&s->m); double v = s->values[--(s->count)]; pthread_mutex_unlock(&s->m); return v; } int is_empty(stack_t *s) { pthread_mutex_lock(&s->m); int result = s->count == 0; pthread_mutex_unlock(&s->m); return result; } int main() { stack_t *s1 = stack_create(10 /* max capacity*/); stack_t *s2 = stack_create(10); push(s1, 3.141); push(s2, pop(s1)); stack_destroy(s2); stack_destroy(s1); }	thread	30
before we fix the problems with semaphores. how would we fix the problems with condition variables? try it out before you look at the code in the previous section. we need to wait in push and pop if our stack is full or empty respectively. attempted solution:	code	134
before we fix the problems with semaphores. how would we fix the problems with condition variables? try it out before you look at the code in the previous section. we need to wait in push and pop if our stack is full or empty respectively. attempted solution:	section	155
does the following solution work? take a second before looking at the answer to spot the errors.	the following	5
2. we only have if statement checks. wait() could spuriously wake up 3. we never signal any of the threads! threads could get stuck waiting indefinitely.	thread	99
void push(stack_t *s, double v) { pthread_mutex_lock(&s->m); while(s->count == capacity) pthread_cond_wait(&s->cv, &s->m); s->values[(s->count)++] = v; pthread_mutex_unlock(&s->m); pthread_cond_signal(&s->cv); } double pop(stack_t *s) { pthread_mutex_lock(&s->m); while(s->count == 0) pthread_cond_wait(&s->cv, &s->m); double v = s->values[--(s->count)]; pthread_cond_broadcast(&s->cv); pthread_mutex_unlock(&s->m); return v; }	thread	35
now, how would we use counting semaphores to prevent over and underflow? let’s discuss it in the next section.	section	102
let’s use a counting semaphore to keep track of how many spaces remain and another semaphore to track the number of items in the stack. we will call these two semaphores sremain and sitems. remember sem_wait will wait if the semaphore’s count has been decremented to zero (by another thread calling sem_post).	thread	284
sketch #2 has implemented the post too early. another thread waiting in push can erroneously attempt to write into a full stack. similarly, a thread waiting in the pop() is allowed to continue too early.	thread	54
sketch #2 has implemented the post too early. another thread waiting in push can erroneously attempt to write into a full stack. similarly, a thread waiting in the pop() is allowed to continue too early.	a thread	140
sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. however, there is no mutual exclusion. two threads can be in the critical section at the same time, which would corrupt the data structure or least lead to data loss. the fix is to wrap a mutex around the critical section:	a struct	213
sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. however, there is no mutual exclusion. two threads can be in the critical section at the same time, which would corrupt the data structure or least lead to data loss. the fix is to wrap a mutex around the critical section:	thread	129
sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. however, there is no mutual exclusion. two threads can be in the critical section at the same time, which would corrupt the data structure or least lead to data loss. the fix is to wrap a mutex around the critical section:	a mutex	272
sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. however, there is no mutual exclusion. two threads can be in the critical section at the same time, which would corrupt the data structure or least lead to data loss. the fix is to wrap a mutex around the critical section:	the data structure	206
sketch 3 correctly enforces buffer full and buffer empty conditions using semaphores. however, there is no mutual exclusion. two threads can be in the critical section at the same time, which would corrupt the data structure or least lead to data loss. the fix is to wrap a mutex around the critical section:	section	160
double pop() { pthread_mutex_lock(&m); sem_wait(&sitems); double v= values[--count]; pthread_mutex_unlock(&m); sem_post(&sremain); return v; } void push(double v) { sem_wait(&sremain); pthread_mutex_lock(&m); values[count++] = v; pthread_mutex_unlock(&m); sem_post(&sitems); }	thread	16
7.4 software solutions to the critical section	section	39
as already discussed, there are critical parts of our code that can only be executed by one thread at a time. we describe this requirement as ‘mutual exclusion’. only one thread (or process) may have access to the shared resource. in multi-threaded programs, we can wrap a critical section with mutex lock and unlock calls:	thread	92
as already discussed, there are critical parts of our code that can only be executed by one thread at a time. we describe this requirement as ‘mutual exclusion’. only one thread (or process) may have access to the shared resource. in multi-threaded programs, we can wrap a critical section with mutex lock and unlock calls:	code	54
as already discussed, there are critical parts of our code that can only be executed by one thread at a time. we describe this requirement as ‘mutual exclusion’. only one thread (or process) may have access to the shared resource. in multi-threaded programs, we can wrap a critical section with mutex lock and unlock calls:	section	282
pthread_mutex_lock(p_mutex_t *m) { while(m->lock) ; m->lock = 1; } pthread_mutex_unlock(p_mutex_t *m) { m->lock = 0; }	thread	1
as we touched on earlier, this implementation does not satisfy mutual exclusion even considering that threads can unlock other threads locks. let’s take a close look at this ‘implementation’ from two threads running around the same time.	thread	102
to simplify the discussion, we consider only two threads. note these arguments work for threads and processes and the classic cs literature discusses these problems in terms of two processes that need exclusive access to a critical section or shared resource. raising a flag represents a thread/process’s intention to enter the critical section.	thread	49
to simplify the discussion, we consider only two threads. note these arguments work for threads and processes and the classic cs literature discusses these problems in terms of two processes that need exclusive access to a critical section or shared resource. raising a flag represents a thread/process’s intention to enter the critical section.	a thread	286
to simplify the discussion, we consider only two threads. note these arguments work for threads and processes and the classic cs literature discusses these problems in terms of two processes that need exclusive access to a critical section or shared resource. raising a flag represents a thread/process’s intention to enter the critical section.	section	232
there are three main desirable properties that we desire in a solution to the critical section problem.	section	87
1. mutual exclusion. the thread/process gets exclusive access. others must wait until it exits the critical section.	thread	25
1. mutual exclusion. the thread/process gets exclusive access. others must wait until it exits the critical section.	section	108
2. bounded wait. a thread/process cannot get superseded by another thread infinite amounts of time.	thread	19
2. bounded wait. a thread/process cannot get superseded by another thread infinite amounts of time.	a thread	17
3. progress. if no thread/process is inside the critical section, the thread/process should be able to proceed without having to wait.	thread	19
3. progress. if no thread/process is inside the critical section, the thread/process should be able to proceed without having to wait.	section	57
with these ideas in mind, let’s examine another candidate solution that uses a turn-based flag only if two threads both required access at the same time.	thread	107
remember that the pseudo-code outlined below is part of a larger program. the thread or process will typically need to enter the critical section many times during the lifetime of the process. so, imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.	thread	78
remember that the pseudo-code outlined below is part of a larger program. the thread or process will typically need to enter the critical section many times during the lifetime of the process. so, imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.	code	25
remember that the pseudo-code outlined below is part of a larger program. the thread or process will typically need to enter the critical section many times during the lifetime of the process. so, imagine each example as wrapped inside a loop where for a random amount of time the thread or process is working on something else.	section	138
answer: candidate solution #1 also suffers from a race condition because both threads/processes could read each other’s flag value as lowered and continue.	thread	78
this suggests we should raise the flag before checking the other thread’s flag, which is candidate solution #2 below.	thread	65
candidate #2 satisfies mutual exclusion. it is impossible for two threads to be inside the critical section at the same time. however, this code suffers from deadlock! suppose two threads wish to enter the critical section at the same time.	thread	66
candidate #2 satisfies mutual exclusion. it is impossible for two threads to be inside the critical section at the same time. however, this code suffers from deadlock! suppose two threads wish to enter the critical section at the same time.	code	140
candidate #2 satisfies mutual exclusion. it is impossible for two threads to be inside the critical section at the same time. however, this code suffers from deadlock! suppose two threads wish to enter the critical section at the same time.	section	100
thread 1 raise flag wait	thread	0
thread 2 raise flag wait	thread	0
both processes are now waiting for the other one to lower their flags. neither one will enter the critical section as both are now stuck forever! this suggests we should use a turn-based variable to try to resolve who should proceed.	section	107
the following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue	the following	0
the following candidate solution #3 uses a turn-based variable to politely allow one thread and then the other to continue	thread	85
candidate #3 satisfies mutual exclusion. each thread or process gets exclusive access to the critical section.	thread	46
candidate #3 satisfies mutual exclusion. each thread or process gets exclusive access to the critical section.	section	102
however, both threads/processes must take a strict turn-based approach to use the critical section. they are forced into an alternating critical section access pattern. if thread 1 wishes to read a hash table every millisecond, but another thread writes to a hash table every second, then the reading thread would have to wait another 999ms before being able to read from the hash table again. this ‘solution’ is ineffective because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.	thread	14
however, both threads/processes must take a strict turn-based approach to use the critical section. they are forced into an alternating critical section access pattern. if thread 1 wishes to read a hash table every millisecond, but another thread writes to a hash table every second, then the reading thread would have to wait another 999ms before being able to read from the hash table again. this ‘solution’ is ineffective because our threads should be able to make progress and enter the critical section if no other thread is currently in the critical section.	section	91
is the following a correct solution to csp?	the following	3
candidate #4 fails because a thread does not wait until the other thread lowers its flag. after some thought or inspiration, the following scenario can be created to demonstrate how mutual exclusion is not satisfied.	the following	125
candidate #4 fails because a thread does not wait until the other thread lowers its flag. after some thought or inspiration, the following scenario can be created to demonstrate how mutual exclusion is not satisfied.	thread	29
candidate #4 fails because a thread does not wait until the other thread lowers its flag. after some thought or inspiration, the following scenario can be created to demonstrate how mutual exclusion is not satisfied.	a thread	27
imagine the first thread runs this code twice. the turn flag now points to the second thread. while the first thread is still inside the critical section, the second thread arrives. the second thread can immediately continue into the critical section!	thread	18
imagine the first thread runs this code twice. the turn flag now points to the second thread. while the first thread is still inside the critical section, the second thread arrives. the second thread can immediately continue into the critical section!	code	35
imagine the first thread runs this code twice. the turn flag now points to the second thread. while the first thread is still inside the critical section, the second thread arrives. the second thread can immediately continue into the critical section!	section	146
notice how the process’s flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. further, the flag can be interpreted as an immediate intent to enter the critical section. only if the other process has also raised the flag will one process defer, lower their intent flag and wait. let’s check the conditions.	section	67
notice how the process’s flag is always raised during the critical section no matter if the loop is iterated zero, once or more times. further, the flag can be interpreted as an immediate intent to enter the critical section. only if the other process has also raised the flag will one process defer, lower their intent flag and wait. let’s check the conditions.	the loop	88
1. mutual exclusion. let’s try to sketch a simple proof. the loop invariant is that at the start of checking the condition, your flag has to be raised – this is by exhaustion. since the only way that a thread can leave the loop is by having the condition be false, the flag must be raised for the entirety of the critical section. since the loop prevents a thread from exiting while the other thread’s flag is raised and a thread has its flag raised in the critical section, the other thread can’t enter the critical section at the same time.	thread	202
1. mutual exclusion. let’s try to sketch a simple proof. the loop invariant is that at the start of checking the condition, your flag has to be raised – this is by exhaustion. since the only way that a thread can leave the loop is by having the condition be false, the flag must be raised for the entirety of the critical section. since the loop prevents a thread from exiting while the other thread’s flag is raised and a thread has its flag raised in the critical section, the other thread can’t enter the critical section at the same time.	a thread	200
1. mutual exclusion. let’s try to sketch a simple proof. the loop invariant is that at the start of checking the condition, your flag has to be raised – this is by exhaustion. since the only way that a thread can leave the loop is by having the condition be false, the flag must be raised for the entirety of the critical section. since the loop prevents a thread from exiting while the other thread’s flag is raised and a thread has its flag raised in the critical section, the other thread can’t enter the critical section at the same time.	section	322
1. mutual exclusion. let’s try to sketch a simple proof. the loop invariant is that at the start of checking the condition, your flag has to be raised – this is by exhaustion. since the only way that a thread can leave the loop is by having the condition be false, the flag must be raised for the entirety of the critical section. since the loop prevents a thread from exiting while the other thread’s flag is raised and a thread has its flag raised in the critical section, the other thread can’t enter the critical section at the same time.	the loop	57
2. bounded wait. assuming that the critical section ends in finite time, a thread once it has left the critical section cannot then get the critical section back. the reason being is the turn variable is set to the other thread, meaning that that thread now has priority. that means a thread cannot be superseded infinitely by another thread.	thread	75
2. bounded wait. assuming that the critical section ends in finite time, a thread once it has left the critical section cannot then get the critical section back. the reason being is the turn variable is set to the other thread, meaning that that thread now has priority. that means a thread cannot be superseded infinitely by another thread.	a thread	73
2. bounded wait. assuming that the critical section ends in finite time, a thread once it has left the critical section cannot then get the critical section back. the reason being is the turn variable is set to the other thread, meaning that that thread now has priority. that means a thread cannot be superseded infinitely by another thread.	section	44
3. progress. if the other thread isn’t in the critical section, it will simply continue with a simple check. we didn’t make any statement about if threads are randomly stopped by the system scheduler. this is an idealized scenario where threads will keep executing instructions.	thread	26
3. progress. if the other thread isn’t in the critical section, it will simply continue with a simple check. we didn’t make any statement about if threads are randomly stopped by the system scheduler. this is an idealized scenario where threads will keep executing instructions.	system	183
3. progress. if the other thread isn’t in the critical section, it will simply continue with a simple check. we didn’t make any statement about if threads are randomly stopped by the system scheduler. this is an idealized scenario where threads will keep executing instructions.	section	55
this solution satisfies mutual exclusion, bounded wait and progress. if thread #2 has set turn to 2 and is currently inside the critical section. thread #1 arrives, sets the turn back to 1 and now waits until thread 2 lowers the flag.	thread	72
this solution satisfies mutual exclusion, bounded wait and progress. if thread #2 has set turn to 2 and is currently inside the critical section. thread #1 arrives, sets the turn back to 1 and now waits until thread 2 lowers the flag.	section	137
1. mutual exclusion. let’s try to sketch a simple proof again. a thread doesn’t get into the critical section until the turn variable is yours or the other thread’s flag isn’t up. if the other thread’s flag isn’t up, it isn’t trying to enter the critical section. that is the first action the thread does and the last action the thread undoes. if the turn variable is set to this thread, that means that the other thread has given the control to this thread.	thread	65
1. mutual exclusion. let’s try to sketch a simple proof again. a thread doesn’t get into the critical section until the turn variable is yours or the other thread’s flag isn’t up. if the other thread’s flag isn’t up, it isn’t trying to enter the critical section. that is the first action the thread does and the last action the thread undoes. if the turn variable is set to this thread, that means that the other thread has given the control to this thread.	a thread	63
1. mutual exclusion. let’s try to sketch a simple proof again. a thread doesn’t get into the critical section until the turn variable is yours or the other thread’s flag isn’t up. if the other thread’s flag isn’t up, it isn’t trying to enter the critical section. that is the first action the thread does and the last action the thread undoes. if the turn variable is set to this thread, that means that the other thread has given the control to this thread.	section	102
since my flag is raised and the turn variable is set, the other thread has to wait in the loop until the current thread is done.	thread	64
since my flag is raised and the turn variable is set, the other thread has to wait in the loop until the current thread is done.	the loop	86
2. bounded wait. after one thread lowers, a thread waiting in the while loop will leave because the first condition is broken. this means that threads cannot win all the time.	thread	27
2. bounded wait. after one thread lowers, a thread waiting in the while loop will leave because the first condition is broken. this means that threads cannot win all the time.	a thread	42
3. progress. if no other thread is contesting, other thread’s flags are not up. that means that a thread can go past the while loop and do critical section items.	thread	25
3. progress. if no other thread is contesting, other thread’s flags are not up. that means that a thread can go past the while loop and do critical section items.	a thread	96
3. progress. if no other thread is contesting, other thread’s flags are not up. that means that a thread can go past the while loop and do critical section items.	section	148
now that we have a solution to the critical section problem, we can reasonably implement a mutex. how would we implement other synchronization primitives? let’s start with a semaphore. to implement a semaphore with efficient cpu usage, we will say that we have implemented a condition variable. implementing an o(1) space condition variable using only a mutex is not trivial, or at least an o(1) heap condition variable is not trivial. we don’t want to call malloc while implementing a primitive, or we may deadlock!	a mutex	89
now that we have a solution to the critical section problem, we can reasonably implement a mutex. how would we implement other synchronization primitives? let’s start with a semaphore. to implement a semaphore with efficient cpu usage, we will say that we have implemented a condition variable. implementing an o(1) space condition variable using only a mutex is not trivial, or at least an o(1) heap condition variable is not trivial. we don’t want to call malloc while implementing a primitive, or we may deadlock!	section	44
 each semaphore needs a count, a condition variable and a mutex	a mutex	56
typedef struct sem_t { ssize_t count; pthread_mutex_t m; pthread_condition_t cv; } sem_t;	thread	39
typedef struct sem_t { ssize_t count; pthread_mutex_t m; pthread_condition_t cv; } sem_t;	type	0
s->count = value; pthread_mutex_init(&s->m, null); pthread_cond_init(&s->cv, null); return 0; }	thread	19
our implementation of sem_post needs to increment the count. we will also wake up any threads sleeping inside the condition variable. notice we lock and unlock the mutex so only one thread can be inside the critical section at a time.	thread	86
our implementation of sem_post needs to increment the count. we will also wake up any threads sleeping inside the condition variable. notice we lock and unlock the mutex so only one thread can be inside the critical section at a time.	section	216
void sem_post(sem_t *s) { pthread_mutex_lock(&s->m); s->count++; pthread_cond_signal(&s->cv); /* a woken thread must acquire the lock, so it will also have to wait until we call unlock*/ pthread_mutex_unlock(&s->m); }	thread	27
our implementation of sem_wait may need to sleep if the semaphore’s count is zero. just like sem_post, we wrap the critical section using the lock, so only one thread can be executing our code at a time. notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter sem_post and awaken us from our sleep!	thread	160
our implementation of sem_wait may need to sleep if the semaphore’s count is zero. just like sem_post, we wrap the critical section using the lock, so only one thread can be executing our code at a time. notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter sem_post and awaken us from our sleep!	code	188
our implementation of sem_wait may need to sleep if the semaphore’s count is zero. just like sem_post, we wrap the critical section using the lock, so only one thread can be executing our code at a time. notice if the thread does need to wait then the mutex will be unlocked, allowing another thread to enter sem_post and awaken us from our sleep!	section	124
also notice that even if a thread is woken up before it returns from pthread_cond_wait, it must re-acquire the lock, so it will have to wait until sem_post finishes.	thread	27
also notice that even if a thread is woken up before it returns from pthread_cond_wait, it must re-acquire the lock, so it will have to wait until sem_post finishes.	a thread	25
void sem_wait(sem_t *s) { pthread_mutex_lock(&s->m); while (s->count == 0) { pthread_cond_wait(&s->cv, &s->m); /*unlock mutex, wait, relock mutex*/ } s->count--; pthread_mutex_unlock(&s->m); }	thread	27
that is a complete implementation of a counting semaphore notice that we are calling sem_post every single time. in practice, this means sem_post would unnecessary call pthread_cond_signal even if there are no waiting threads. a more efficient implementation would only call pthread_cond_signal when necessary i.e.	thread	170
/* did we increment from zero to one- time to signal a thread sleeping inside sem_post */ if (s->count == 1) /* wake up one waiting thread!*/ pthread_cond_signal(&s->cv);	thread	55
/* did we increment from zero to one- time to signal a thread sleeping inside sem_post */ if (s->count == 1) /* wake up one waiting thread!*/ pthread_cond_signal(&s->cv);	a thread	53
 a production semaphore implementation may include a queue to ensure fairness and priority. meaning, we wake up the highest-priority and/or longest sleeping thread.	thread	157
 an advanced use of sem_init allows semaphores to be shared across processes. our implementation only works for threads inside the same process. we could fix this by setting the condition variable and mutex attributes.	thread	112
implementing a condition variable with a mutex is complex, so we’ve left that in the appendix.	a mutex	39
suppose we wanted to perform a multi-threaded calculation that has two stages, but we don’t want to advance to the second stage until the first stage is completed. we could use a synchronization method called a barrier.	thread	37
when a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they’ll all proceed together.	thread	7
when a thread reaches a barrier, it will wait at the barrier until all the threads reach the barrier, and then they’ll all proceed together.	a thread	5
pthreads has a function pthread_barrier_wait() that implements this. you’ll need to declare a pthread_barrier_t variable and initialize it with pthread_barrier_init(). pthread_barrier_init() takes the number of threads that will be participating in the barrier as an argument. here is a sample program using barriers.	thread	1
#define _gnu_source #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <pthread.h> #include <time.h> #define thread_count 4 pthread_barrier_t mybarrier; void* threadfn(void *id_ptr) { int thread_id = *(int*)id_ptr;	thread	90
int wait_sec = 1 + rand() % 5; printf("thread %d: wait for %d seconds.\n", thread_id, wait_sec); sleep(wait_sec); printf("thread %d: i’m ready...\n", thread_id); pthread_barrier_wait(&mybarrier); printf("thread %d: going!\n", thread_id); return null; }	thread	39
int wait_sec = 1 + rand() % 5; printf("thread %d: wait for %d seconds.\n", thread_id, wait_sec); sleep(wait_sec); printf("thread %d: i’m ready...\n", thread_id); pthread_barrier_wait(&mybarrier); printf("thread %d: going!\n", thread_id); return null; }	printf	31
int main() { int i; pthread_t ids[thread_count]; int short_ids[thread_count]; srand(time(null)); pthread_barrier_init(&mybarrier, null, thread_count + 1); for (i=0; i < thread_count; i++) { short_ids[i] = i; pthread_create(&ids[i], null, threadfn, &short_ids[i]); } printf("main() is ready.\n"); pthread_barrier_wait(&mybarrier); printf("main() is going!\n"); for (i=0; i < thread_count; i++) { pthread_join(ids[i], null); } pthread_barrier_destroy(&mybarrier); return 0; }	thread	21
int main() { int i; pthread_t ids[thread_count]; int short_ids[thread_count]; srand(time(null)); pthread_barrier_init(&mybarrier, null, thread_count + 1); for (i=0; i < thread_count; i++) { short_ids[i] = i; pthread_create(&ids[i], null, threadfn, &short_ids[i]); } printf("main() is ready.\n"); pthread_barrier_wait(&mybarrier); printf("main() is going!\n"); for (i=0; i < thread_count; i++) { pthread_join(ids[i], null); } pthread_barrier_destroy(&mybarrier); return 0; }	printf	266
now let’s implement our own barrier and use it to keep all the threads in sync in a large calculation. here is our thought process, 1. threads do first calculation (use and change values in data) 2. barrier! wait for all threads to finish first calculation before continuing 3. threads do second calculation (use and change values in data)	thread	63
the thread function has four main parts-	thread	4
our main thread will create the 16 threads, and we will divide each calculation into 16 separate pieces. each thread will be given a unique value (0,1,2,..15), so it can work on its own block. since a (void*) type can hold small integers, we will pass the value of i by casting it to a void pointer.	thread	9
our main thread will create the 16 threads, and we will divide each calculation into 16 separate pieces. each thread will be given a unique value (0,1,2,..15), so it can work on its own block. since a (void*) type can hold small integers, we will pass the value of i by casting it to a void pointer.	block	186
our main thread will create the 16 threads, and we will divide each calculation into 16 separate pieces. each thread will be given a unique value (0,1,2,..15), so it can work on its own block. since a (void*) type can hold small integers, we will pass the value of i by casting it to a void pointer.	type	209
our main thread will create the 16 threads, and we will divide each calculation into 16 separate pieces. each thread will be given a unique value (0,1,2,..15), so it can work on its own block. since a (void*) type can hold small integers, we will pass the value of i by casting it to a void pointer.	pointer	291
note, we will never dereference this pointer value as an actual memory location.	memory	64
note, we will never dereference this pointer value as an actual memory location.	pointer	37
after calculation 1 completes, we need to wait for the slower threads unless we are the last thread! so, keep track of the number of threads that have arrived at our barrier ‘checkpoint’.	thread	62
however, the code has a few flaws. one is two threads might try to decrement remain. the other is the loop is a busy loop. we can do better! let’s use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.	thread	46
however, the code has a few flaws. one is two threads might try to decrement remain. the other is the loop is a busy loop. we can do better! let’s use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.	code	13
however, the code has a few flaws. one is two threads might try to decrement remain. the other is the loop is a busy loop. we can do better! let’s use a condition variable and then we will use a broadcast/signal functions to wake up the sleeping threads.	the loop	98
a reminder, a condition variable is similar to a house! threads go there to sleep (pthread_cond_wait). a threa can choose to wake up one thread (pthread_cond_signal) or all of them (pthread_cond_broadcast).	thread	56
if there are no threads currently waiting then these two calls have no effect.	thread	16
a condition variable version is usually similar to a busy loop incorrect solution - as we will show next. first, let’s add a mutex and condition global variables and don’t forget to initialize them in main.	a mutex	123
we will use the mutex to ensure that only one thread modifies remain at a time. the last arriving thread needs to wake up all sleeping threads - so we will use pthread_cond_broadcast(cv) not pthread_cond_signal	thread	46
pthread_mutex_lock(&m); remain--; if (remain == 0) { pthread_cond_broadcast(&cv); } else { while(remain != 0) { pthread_cond_wait(&cv, &m); } } pthread_mutex_unlock(&m);	thread	1
when a thread enters pthread_cond_wait, it releases the mutex and sleeps. after, the thread will be woken up. once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.	thread	7
when a thread enters pthread_cond_wait, it releases the mutex and sleeps. after, the thread will be woken up. once we bring a thread back from its sleep, before returning it must wait until it can lock the mutex. notice that even if a sleeping thread wakes up early, it will check the while loop condition and re-enter wait if necessary.	a thread	5
the above barrier is not reusable. meaning that if we stick it into any old calculation loop there is a good chance that the code will encounter a condition where the barrier either deadlocks or thread races ahead one iteration faster. why is that? because of the ambitious thread.	thread	195
the above barrier is not reusable. meaning that if we stick it into any old calculation loop there is a good chance that the code will encounter a condition where the barrier either deadlocks or thread races ahead one iteration faster. why is that? because of the ambitious thread.	code	125
we will assume that one thread is much faster than all the other threads. with the barrier api, this thread should be waiting, but it may not be. to make it concrete, let’s look at this code	thread	24
we will assume that one thread is much faster than all the other threads. with the barrier api, this thread should be waiting, but it may not be. to make it concrete, let’s look at this code	code	186
what happens if a thread becomes ambitious. well 1. many other threads wait on the condition variable 2. the last thread broadcasts.	thread	18
what happens if a thread becomes ambitious. well 1. many other threads wait on the condition variable 2. the last thread broadcasts.	a thread	16
3. a single thread leaves the while loop.	thread	12
4. this single thread performs its calculation before any other threads even wake up 5. reset the number of remaining threads and goes back to sleep.	thread	15
all the other threads who should’ve woken up never do and our implementation deadlocks. how would you go about solving this? hint: if multiple threads call barrier_wait in a loop then one can guarantee that they are on the same iteration.	thread	14
imagine you had a key-value map data structure that is used by many threads. multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. the writers are not so gregarious. to avoid data corruption, only one thread at a time may modify (write) the data structure and no readers may be reading at that time.	a struct	35
imagine you had a key-value map data structure that is used by many threads. multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. the writers are not so gregarious. to avoid data corruption, only one thread at a time may modify (write) the data structure and no readers may be reading at that time.	thread	68
imagine you had a key-value map data structure that is used by many threads. multiple threads should be able to look up (read) values at the same time provided the data structure is not being written to. the writers are not so gregarious. to avoid data corruption, only one thread at a time may modify (write) the data structure and no readers may be reading at that time.	the data structure	160
an incorrect attempt is shown below (“lock” is a shorthand for pthread_mutex_lock):	thread	64
our second attempt suffers from a race condition. imagine if two threads both called read and write or both called write at the same time. both threads would be able to proceed! secondly, we can have multiple readers and multiple writers, so let’s keep track of the total number of readers or writers which brings us to attempt #3.	thread	65
remember that pthread_cond_wait performs three actions. firstly, it atomically unlocks the mutex and then sleeps (until it is woken by pthread_cond_signal or pthread_cond_broadcast). thirdly, the awoken thread must re-acquire the mutex lock before returning. thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.	thread	15
remember that pthread_cond_wait performs three actions. firstly, it atomically unlocks the mutex and then sleeps (until it is woken by pthread_cond_signal or pthread_cond_broadcast). thirdly, the awoken thread must re-acquire the mutex lock before returning. thus only one thread can actually be running inside the critical section defined by the lock and unlock() methods.	section	324
does this mean that a writer and read could read and write at the same time? no! first of all, remember cond_wait requires the thread re-acquire the mutex lock before returning. thus only one thread can be executing code inside the critical section (marked with **) at a time!	thread	127
does this mean that a writer and read could read and write at the same time? no! first of all, remember cond_wait requires the thread re-acquire the mutex lock before returning. thus only one thread can be executing code inside the critical section (marked with **) at a time!	code	216
does this mean that a writer and read could read and write at the same time? no! first of all, remember cond_wait requires the thread re-acquire the mutex lock before returning. thus only one thread can be executing code inside the critical section (marked with **) at a time!	section	241
candidate #3 above also uses pthread_cond_signal. this will only wake up one thread. if many readers are waiting for the writer to complete, only one sleeping reader will be awoken from their slumber. the reader and writer should use cond_broadcast so that all threads should wake up and check their while-loop condition.	thread	30
the “holding pen” can be implemented using a variable and a condition variable so that we can wake up the threads once the writer has finished.	thread	106
can you identify any improvements? for example, how would you improve the code so that we only woke up readers or one writer?	code	74
a ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. as array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. as data is added (enqueued) to the front of the queue or removed (dequeued) from the tail of the queue, the current items in the buffer form a train that appears to circle the track a simple (single-threaded) implementation is shown below. note, enqueue and dequeue do not guard against underflow or overflow. it’s possible to add an item when the queue is full and possible to remove an item when the queue is empty. if we added 20 integers (1, 2, 3, . . . , 20) to the queue and did not dequeue any items then values, 17,18,19,20 would overwrite the 1,2,3,4. we won’t fix this problem right now, instead of when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.	thread	520
a ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. as array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. as data is added (enqueued) to the front of the queue or removed (dequeued) from the tail of the queue, the current items in the buffer form a train that appears to circle the track a simple (single-threaded) implementation is shown below. note, enqueue and dequeue do not guard against underflow or overflow. it’s possible to add an item when the queue is full and possible to remove an item when the queue is empty. if we added 20 integers (1, 2, 3, . . . , 20) to the queue and did not dequeue any items then values, 17,18,19,20 would overwrite the 1,2,3,4. we won’t fix this problem right now, instead of when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.	a ring buffer	0
a ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. as array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. as data is added (enqueued) to the front of the queue or removed (dequeued) from the tail of the queue, the current items in the buffer form a train that appears to circle the track a simple (single-threaded) implementation is shown below. note, enqueue and dequeue do not guard against underflow or overflow. it’s possible to add an item when the queue is full and possible to remove an item when the queue is empty. if we added 20 integers (1, 2, 3, . . . , 20) to the queue and did not dequeue any items then values, 17,18,19,20 would overwrite the 1,2,3,4. we won’t fix this problem right now, instead of when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.	memory	83
a ring buffer is a simple, usually fixed-sized, storage mechanism where contiguous memory is treated as if it is circular, and two index counters keep track of the current beginning and end of the queue. as array indexing is not circular, the index counters must wrap around to zero when moved past the end of the array. as data is added (enqueued) to the front of the queue or removed (dequeued) from the tail of the queue, the current items in the buffer form a train that appears to circle the track a simple (single-threaded) implementation is shown below. note, enqueue and dequeue do not guard against underflow or overflow. it’s possible to add an item when the queue is full and possible to remove an item when the queue is empty. if we added 20 integers (1, 2, 3, . . . , 20) to the queue and did not dequeue any items then values, 17,18,19,20 would overwrite the 1,2,3,4. we won’t fix this problem right now, instead of when we create the multi-threaded version we will ensure enqueue-ing and dequeue-ing threads are blocked while the ring buffer is full or empty respectively.	block	1027
it’s tempting to write the enqueue or dequeue method in the following compact form.	the following	56
this buffer does not yet prevent overwrites. for that, we’ll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.	thread	79
this buffer does not yet prevent overwrites. for that, we’ll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.	block	106
this buffer does not yet prevent overwrites. for that, we’ll turn to our multi-threaded attempt that will block a thread until there is space or there is at least one item to remove.	a thread	112
7.8.2 multithreaded correctness	thread	11
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity, pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	the following	0
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity, pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	thread	200
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity, pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	code	14
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity, pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	block	97
before reading on, see how many mistakes you can find. then determine what would happen if threads called the enqueue and dequeue methods.	thread	91
 the initial value of s1 is 16, so the semaphore will never be reduced to zero - enqueue will not block if the ring buffer is full - so overflow is possible.	block	98
 the initial value of s2 is zero, so calls to dequeue will always block and never return!	block	66
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	the following	0
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	thread	199
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	code	14
the following code is an incorrect implementation. what will happen? will enqueue and/or dequeue block? is mutual exclusion satisfied? can the buffer underflow? can the buffer overflow? for clarity pthread_mutex is shortened to p_m and we assume sem_wait cannot be interrupted.	block	97
 the initial value of s2 is 0. thus enqueue will block on the first call to sem_wait even though the buffer is empty!	block	49
 the initial value of s1 is 16. thus dequeue will not block on the first call to sem_wait even though the buffer is empty - underflow! the dequeue method will return invalid data.	block	54
 the code does not satisfy mutual exclusion. two threads can modify in or out at the same time! the code appears to use mutex lock. unfortunately, the lock was never initialized with pthread_mutex_init() or pthread_mutex_initializer - so the lock may not work (pthread_mutex_lock may simply do nothing)	thread	49
 the code does not satisfy mutual exclusion. two threads can modify in or out at the same time! the code appears to use mutex lock. unfortunately, the lock was never initialized with pthread_mutex_init() or pthread_mutex_initializer - so the lock may not work (pthread_mutex_lock may simply do nothing)	code	5
7.8.5 correct implementation of a ring buffer	a ring buffer	32
as the mutex lock is stored in global (static) memory it can be initialized with pthread_mutex_initializer. if we had allocated space for the mutex on the heap, then we would have used pthread_mutex_init(ptr, null)	the heap	151
as the mutex lock is stored in global (static) memory it can be initialized with pthread_mutex_initializer. if we had allocated space for the mutex on the heap, then we would have used pthread_mutex_init(ptr, null)	thread	82
as the mutex lock is stored in global (static) memory it can be initialized with pthread_mutex_initializer. if we had allocated space for the mutex on the heap, then we would have used pthread_mutex_init(ptr, null)	memory	47
1. the lock is only held during the critical section (access to the data structure).	a struct	71
1. the lock is only held during the critical section (access to the data structure).	the data structure	64
1. the lock is only held during the critical section (access to the data structure).	section	45
 what would happen if the order of pthread_mutex_unlock and sem_post calls were swapped?	thread	36
 what would happen if the order of sem_wait and pthread_mutex_lock calls were swapped?	thread	49
you thought that you were using different processes, so you don’t have to synchronize? think again! you may not have race conditions within a process but what if your process needs to interact with the system around it? let’s consider a motivating example	a process	140
you thought that you were using different processes, so you don’t have to synchronize? think again! you may not have race conditions within a process but what if your process needs to interact with the system around it? let’s consider a motivating example	system	202
void write_string(const char *data) { int fd = open("my_file.txt", o_wronly); write(fd, data, strlen(data)); close(fd); } int main() { if(!fork()) { write_string("key1: value1"); wait(null); } else { write_string("key2: value2"); } return 0; }	string	11
if none of the system calls fail then we should get something that looks like this given the file was empty to begin with.	system	15
but, there is a hidden nuance. most system calls can be interrupted meaning that the operating system can stop an ongoing system call because it needs to stop the process. so barring fork wait open and close from failing – they typically go to completion – what happens if write fails? if write fails and no bytes are written, we can get something like key1: value1 or key2: value2. this is data loss which is incorrect but won’t corrupt the file. what happens if write gets interrupted after a partial write? we get all sorts of madness. for example,	system	36
a program can create a mutex before fork-ing - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other. advanced note: there are advanced options using shared memory that allow a child and parent to share a mutex if it’s created with the correct options and uses a shared memory segment. see stackoverflow example so what should we do? we should use a shared mutex! consider the following code.	the following	442
a program can create a mutex before fork-ing - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other. advanced note: there are advanced options using shared memory that allow a child and parent to share a mutex if it’s created with the correct options and uses a shared memory segment. see stackoverflow example so what should we do? we should use a shared mutex! consider the following code.	a mutex	21
a program can create a mutex before fork-ing - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other. advanced note: there are advanced options using shared memory that allow a child and parent to share a mutex if it’s created with the correct options and uses a shared memory segment. see stackoverflow example so what should we do? we should use a shared mutex! consider the following code.	code	456
a program can create a mutex before fork-ing - however the child and parent process will not share virtual memory and each one will have a mutex independent of the other. advanced note: there are advanced options using shared memory that allow a child and parent to share a mutex if it’s created with the correct options and uses a shared memory segment. see stackoverflow example so what should we do? we should use a shared mutex! consider the following code.	memory	107
pthread_mutex_t * mutex = null; pthread_mutexattr_t attr; void write_string(const char *data) { pthread_mutex_lock(mutex); int fd = open("my_file.txt", o_wronly); int bytes_to_write = strlen(data), written = 0; while(written < bytes_to_write) { written += write(fd, data + written, bytes_to_write - written); } close(fd); pthread_mutex_unlock(mutex); } int main() { pthread_mutexattr_init(&attr); pthread_mutexattr_setpshared(&attr, pthread_process_shared); pmutex = mmap (null, sizeof(pthread_mutex_t), prot_read|prot_write, map_shared|map_anon, -1, 0); pthread_mutex_init(pmutex, &attrmutex); if(!fork()) { write_string("key1: value1"); wait(null); pthread_mutex_destroy(pmutex); pthread_mutexattr_destroy(&attrmutex);	thread	1
pthread_mutex_t * mutex = null; pthread_mutexattr_t attr; void write_string(const char *data) { pthread_mutex_lock(mutex); int fd = open("my_file.txt", o_wronly); int bytes_to_write = strlen(data), written = 0; while(written < bytes_to_write) { written += write(fd, data + written, bytes_to_write - written); } close(fd); pthread_mutex_unlock(mutex); } int main() { pthread_mutexattr_init(&attr); pthread_mutexattr_setpshared(&attr, pthread_process_shared); pmutex = mmap (null, sizeof(pthread_mutex_t), prot_read|prot_write, map_shared|map_anon, -1, 0); pthread_mutex_init(pmutex, &attrmutex); if(!fork()) { write_string("key1: value1"); wait(null); pthread_mutex_destroy(pmutex); pthread_mutexattr_destroy(&attrmutex);	mmap	467
pthread_mutex_t * mutex = null; pthread_mutexattr_t attr; void write_string(const char *data) { pthread_mutex_lock(mutex); int fd = open("my_file.txt", o_wronly); int bytes_to_write = strlen(data), written = 0; while(written < bytes_to_write) { written += write(fd, data + written, bytes_to_write - written); } close(fd); pthread_mutex_unlock(mutex); } int main() { pthread_mutexattr_init(&attr); pthread_mutexattr_setpshared(&attr, pthread_process_shared); pmutex = mmap (null, sizeof(pthread_mutex_t), prot_read|prot_write, map_shared|map_anon, -1, 0); pthread_mutex_init(pmutex, &attrmutex); if(!fork()) { write_string("key1: value1"); wait(null); pthread_mutex_destroy(pmutex); pthread_mutexattr_destroy(&attrmutex);	string	69
munmap((void *)pmutex, sizeof(*pmutex)); } else { write_string("key2: value2"); } return 0; }	string	56
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	thread	254
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	code	9
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	mmap	132
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	a process	41
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	memory	88
what the code does in main is initialize a process shared mutex using a piece of shared memory. you will find out what this call to mmap does later – assume for the time being that it creates memory that is shared between processes. we can initialize a pthread_mutex_t in that special piece of memory and use it as normal. to counter write failing, we have put the write call inside a while loop that keeps writing so long as there are bytes left to write. now if all the other system calls function, there should be more race conditions.	system	478
most programs try to avoid this problem entirely by writing to separate files, but it is good to know that there are mutexes across processes, and they are useful. a program can use all of the primitives that were mentioned previouslty! barriers, semaphores, and condition variables can all be initialized on a shared piece of memory and used in similar ways to their multithreading counterparts.	thread	373
most programs try to avoid this problem entirely by writing to separate files, but it is good to know that there are mutexes across processes, and they are useful. a program can use all of the primitives that were mentioned previouslty! barriers, semaphores, and condition variables can all be initialized on a shared piece of memory and used in similar ways to their multithreading counterparts.	memory	327
 you don’t have to worry about arbitrary memory addresses becoming race condition candidates. only areas that specifically mapped are in danger.	address	48
 you don’t have to worry about arbitrary memory addresses becoming race condition candidates. only areas that specifically mapped are in danger.	memory	41
 you get the nice isolation of processes so if one process fails the system can maintain intact.	system	69
 when you have a lot of threads, creating a process might ease the system load there are other ways to synchronize as well, check out goroutines or higher orders of synchronization in the appendix.	thread	24
 when you have a lot of threads, creating a process might ease the system load there are other ways to synchronize as well, check out goroutines or higher orders of synchronization in the appendix.	a process	42
 when you have a lot of threads, creating a process might ease the system load there are other ways to synchronize as well, check out goroutines or higher orders of synchronization in the appendix.	system	67
7.10 external resources	resources	14
 why would a mutex lock fail? what’s an example?	a mutex	11
 what happens if a thread tries to destroy a locked mutex?	thread	19
 what happens if a thread tries to destroy a locked mutex?	a thread	17
 can a thread copy the underlying bytes of a mutex instead of using a pointer?	thread	7
 can a thread copy the underlying bytes of a mutex instead of using a pointer?	a mutex	43
 can a thread copy the underlying bytes of a mutex instead of using a pointer?	a thread	5
 can a thread copy the underlying bytes of a mutex instead of using a pointer?	pointer	70
 pthread_mutex_lock man page  pthread_mutex_init man page  sem_init  sem_wait  sem_post  sem_destroy	thread	2
 atomic operations  critical section  producer consumer problem  using condition variables  using counting semaphore  implementing a barrier  implementing a ring buffer  using pthread_mutex  implementing producer consumer  analyzing multi-threaded coded	thread	177
 atomic operations  critical section  producer consumer problem  using condition variables  using counting semaphore  implementing a barrier  implementing a ring buffer  using pthread_mutex  implementing producer consumer  analyzing multi-threaded coded	code	248
 atomic operations  critical section  producer consumer problem  using condition variables  using counting semaphore  implementing a barrier  implementing a ring buffer  using pthread_mutex  implementing producer consumer  analyzing multi-threaded coded	a ring buffer	155
 atomic operations  critical section  producer consumer problem  using condition variables  using counting semaphore  implementing a barrier  implementing a ring buffer  using pthread_mutex  implementing producer consumer  analyzing multi-threaded coded	section	29
 why will the following not work in parallel code	the following	10
 why will the following not work in parallel code	code	45
 what is the critical section?	section	22
 once you have identified a critical section, what is one way of assuring that only one thread will be in the section at a time?	thread	88
 once you have identified a critical section, what is one way of assuring that only one thread will be in the section at a time?	section	37
 identify the critical section here	section	23
 how tight can you make the critical section?	section	37
 what is a producer consumer problem? how might the above be a producer consumer problem be used in the above section? how is a producer consumer problem related to a reader writer problem?	section	110
 why is this code dangerous?	code	13
if(not_ready){ pthread_cond_wait(&cv, &mtx); }	thread	16
 what is a thread barrier?	thread	11
 what is a thread barrier?	a thread	9
 give me an implementation of a reader-writer lock with condition variables, make a struct with whatever you need, it needs to be able to support the following functions	a struct	82
 give me an implementation of a reader-writer lock with condition variables, make a struct with whatever you need, it needs to be able to support the following functions	the following	146
typedef struct {	type	0
 write code to implement a producer consumer using only three counting semaphores. assume there can be more than one thread calling enqueue and dequeue. determine the initial value of each semaphore.	thread	117
 write code to implement a producer consumer using only three counting semaphores. assume there can be more than one thread calling enqueue and dequeue. determine the initial value of each semaphore.	code	7
 write code to implement a producer consumer using condition variables and a mutex. assume there can be more than one thread calling enqueue and dequeue.	thread	118
 write code to implement a producer consumer using condition variables and a mutex. assume there can be more than one thread calling enqueue and dequeue.	a mutex	75
 write code to implement a producer consumer using condition variables and a mutex. assume there can be more than one thread calling enqueue and dequeue.	code	7
 use cvs to implement add(unsigned int) and subtract(unsigned int) blocking functions that never allow the global value to be greater than 100.	block	67
 use cvs to implement a barrier for 15 threads.	thread	39
 what does the following code do?	the following	11
 what does the following code do?	code	25
pthread_mutex_t mutex; pthread_cond_t cond; pthread_mutex_init(&mutex, null); pthread_cond_init(&cond, null); pthread_cond_broadcast(&cond); pthread_cond_wait(&cond,&mutex); return 0; }	thread	1
 is the following code correct? if it isn’t, could you fix it?	the following	4
 is the following code correct? if it isn’t, could you fix it?	code	18
extern int money; void deposit(int amount) { pthread_mutex_lock(&m); money += amount; pthread_mutex_unlock(&m); } void withdraw(int amount) { if (money < amount) { pthread_cond_wait(&cv); } pthread_mutex_lock(&m); money -= amount; pthread_mutex_unlock(&m); }	thread	46
 sketch how to use a binary semaphore as a mutex. remember in addition to mutual exclusion, a mutex can only ever be unlocked by the thread who called it.	thread	133
 sketch how to use a binary semaphore as a mutex. remember in addition to mutual exclusion, a mutex can only ever be unlocked by the thread who called it.	a mutex	41
 how many of the following statements are true?	the following	13
deadlock is defined as when a system cannot make any forward progress. we define a system for the rest of the chapter as a set of rules by which a set of processes can move from one state to another, where a state is either working or waiting for a particular resource. forward progress is defined as if there is at least one process working or we can award a process waiting for a resource that resource. in a lot of systems, deadlock is avoided by ignoring the entire concept [4, p.237]. have you heard about turn it on and off again? for products where the stakes are low (user operating systems, phones), it may be more efficient to allow deadlock. but in the cases where “failure is not an option” - apollo 13, you need a system that tracks, breaks, or prevents deadlocks. apollo 13 didn’t fail because of deadlock, but it wouldn’t be good to restart the system on liftoff.	a process	358
deadlock is defined as when a system cannot make any forward progress. we define a system for the rest of the chapter as a set of rules by which a set of processes can move from one state to another, where a state is either working or waiting for a particular resource. forward progress is defined as if there is at least one process working or we can award a process waiting for a resource that resource. in a lot of systems, deadlock is avoided by ignoring the entire concept [4, p.237]. have you heard about turn it on and off again? for products where the stakes are low (user operating systems, phones), it may be more efficient to allow deadlock. but in the cases where “failure is not an option” - apollo 13, you need a system that tracks, breaks, or prevents deadlocks. apollo 13 didn’t fail because of deadlock, but it wouldn’t be good to restart the system on liftoff.	system	30
mission-critical operating systems need this guarantee formally because playing the odds with people’s lives isn’t a good idea. okay so how do we do this? we model the problem. even though it is a common statistical phrase that all models are wrong, the more accurate the model is to the system the higher the chance the method will work.	system	27
figure 8.1: resource allocation graph one such way is modeling the system with a resource allocation graph (rag). a resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. it is a simple yet powerful tool to illustrate how interacting processes can deadlock. if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node. if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will deadlock. for example, if process 1 holds resource a, process 2 holds resource b and process 1 is waiting for b and process 2 is waiting for a, then processes 1 and 2 will be deadlocked 8.1. we’ll make the distinction that the system is in deadlock by definition if all workers cannot perform an operation other than waiting. we can detect a deadlock by traversing the graph and searching for a cycle using a graph traversal algorithm, such as the depth first search (dfs). this graph is considered as a directed graph and we can treat both the processes and resources as nodes.	resources	1234
figure 8.1: resource allocation graph one such way is modeling the system with a resource allocation graph (rag). a resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. it is a simple yet powerful tool to illustrate how interacting processes can deadlock. if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node. if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will deadlock. for example, if process 1 holds resource a, process 2 holds resource b and process 1 is waiting for b and process 2 is waiting for a, then processes 1 and 2 will be deadlocked 8.1. we’ll make the distinction that the system is in deadlock by definition if all workers cannot perform an operation other than waiting. we can detect a deadlock by traversing the graph and searching for a cycle using a graph traversal algorithm, such as the depth first search (dfs). this graph is considered as a directed graph and we can treat both the processes and resources as nodes.	a process	345
figure 8.1: resource allocation graph one such way is modeling the system with a resource allocation graph (rag). a resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. it is a simple yet powerful tool to illustrate how interacting processes can deadlock. if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node. if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will deadlock. for example, if process 1 holds resource a, process 2 holds resource b and process 1 is waiting for b and process 2 is waiting for a, then processes 1 and 2 will be deadlocked 8.1. we’ll make the distinction that the system is in deadlock by definition if all workers cannot perform an operation other than waiting. we can detect a deadlock by traversing the graph and searching for a cycle using a graph traversal algorithm, such as the depth first search (dfs). this graph is considered as a directed graph and we can treat both the processes and resources as nodes.	system	67
figure 8.1: resource allocation graph one such way is modeling the system with a resource allocation graph (rag). a resource allocation graph tracks which resource is held by which process and which process is waiting for a resource of a particular type. it is a simple yet powerful tool to illustrate how interacting processes can deadlock. if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node. if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will deadlock. for example, if process 1 holds resource a, process 2 holds resource b and process 1 is waiting for b and process 2 is waiting for a, then processes 1 and 2 will be deadlocked 8.1. we’ll make the distinction that the system is in deadlock by definition if all workers cannot perform an operation other than waiting. we can detect a deadlock by traversing the graph and searching for a cycle using a graph traversal algorithm, such as the depth first search (dfs). this graph is considered as a directed graph and we can treat both the processes and resources as nodes.	type	249
there are four necessary and sufficient conditions for deadlock – meaning if these conditions hold then there is a non-zero probability that the system will deadlock at any given iteration. these are known as the coffman conditions [1].	system	145
 circular wait: there exists a cycle in the resource allocation graph, or there exists a set of processes {p1, p2,. . . } such that p1 is waiting for resources held by p2, which is waiting for p3,. . . , which is waiting for p1.	resources	150
 hold and wait: once a resource is obtained, a process keeps the resource locked.	a process	45
→ if the system is deadlocked, the four coffman conditions are apparent.	system	9
 for contradiction, assume that there is no circular wait. if not then that means the resource allocation graph is acyclic, meaning that there is at least one process that is not waiting on any resource to be freed. since the system can move forward, the system is not deadlocked.	system	226
 for contradiction, assume that processes don’t hold and wait but our system still deadlocks. since we have circular wait from the first condition at least one process must be waiting on another process. if that and processes don’t hold and wait, that means one process must let go of a resource. since the system has moved forward, it cannot be deadlocked.	system	70
 for contradiction, assume that we have preemption, but the system cannot be un-deadlocked. have one process, or create one process, that recognizes the circular wait that must be apparent from above and break one of the links. by the first branch, we must not have deadlocked.	system	60
← if the four conditions are apparent, the system is deadlocked. we will prove that if the system is not deadlocked, the four conditions are not apparent. though this proof is not formal, let us build a system with the three requirements not including circular wait. let assume that there is a set of processes p = {p1 , p2 , ..., pn } and there is a set of resources r = {r1 , r2 , ..., rm }. for simplicity, a process can only request one resource at a time but the proof can be generalized to multiple. let assume that the system is a state at time t. let us assume that the state of the system is a tuple (h t , w t ) where there are two functions h t : r → p ∪ {unassigned} that maps resources to the processes that own them (this is a function, meaning that we have mutual exclusion) and or unassigned and w t : p → r ∪ {satisfied} that maps the requests that each process makes to a resource or if the process is satisfied. if the process is satisfied, we consider the work trivial and the process exits, releasing all resources – this can also be generalized. let l t ⊆ p × r be a set of lists of requests that a process uses to release a resource at any given time. the evolution of the system is at each step at every time.	resources	358
← if the four conditions are apparent, the system is deadlocked. we will prove that if the system is not deadlocked, the four conditions are not apparent. though this proof is not formal, let us build a system with the three requirements not including circular wait. let assume that there is a set of processes p = {p1 , p2 , ..., pn } and there is a set of resources r = {r1 , r2 , ..., rm }. for simplicity, a process can only request one resource at a time but the proof can be generalized to multiple. let assume that the system is a state at time t. let us assume that the state of the system is a tuple (h t , w t ) where there are two functions h t : r → p ∪ {unassigned} that maps resources to the processes that own them (this is a function, meaning that we have mutual exclusion) and or unassigned and w t : p → r ∪ {satisfied} that maps the requests that each process makes to a resource or if the process is satisfied. if the process is satisfied, we consider the work trivial and the process exits, releasing all resources – this can also be generalized. let l t ⊆ p × r be a set of lists of requests that a process uses to release a resource at any given time. the evolution of the system is at each step at every time.	a process	410
← if the four conditions are apparent, the system is deadlocked. we will prove that if the system is not deadlocked, the four conditions are not apparent. though this proof is not formal, let us build a system with the three requirements not including circular wait. let assume that there is a set of processes p = {p1 , p2 , ..., pn } and there is a set of resources r = {r1 , r2 , ..., rm }. for simplicity, a process can only request one resource at a time but the proof can be generalized to multiple. let assume that the system is a state at time t. let us assume that the state of the system is a tuple (h t , w t ) where there are two functions h t : r → p ∪ {unassigned} that maps resources to the processes that own them (this is a function, meaning that we have mutual exclusion) and or unassigned and w t : p → r ∪ {satisfied} that maps the requests that each process makes to a resource or if the process is satisfied. if the process is satisfied, we consider the work trivial and the process exits, releasing all resources – this can also be generalized. let l t ⊆ p × r be a set of lists of requests that a process uses to release a resource at any given time. the evolution of the system is at each step at every time.	system	43
 release all resources in l t .	resources	13
 find a process that is requesting a resource  if that resource is available give it to that process, generating a new (h t+1 , w t+1 ) and exit the current iteration.	a process	6
if all processes have been surveyed and if all are requesting a resource and none can be granted a resource, consider it deadlocked. more formally, this system is deadlocked means if ∃t 0 , ∀t ≥ t 0 , ∀p ∈ p, w t (p) 6= satisfied and ∃q, q 6= p → h t (w t (p)) = q (which is what we need to prove).	system	153
mutual exclusion and no pre-emption are encoded into the system. circular wait implies the second condition, a resource is owned by another process which is owned by another process meaning at this state	code	42
mutual exclusion and no pre-emption are encoded into the system. circular wait implies the second condition, a resource is owned by another process which is owned by another process meaning at this state	system	57
∀p ∈ p, ∃q 6= p → h t (w t (p)) = q. circular wait also implies that at this current state, no process is satisfied, meaning at this state ∀p ∈ p, w t (p) 6= satisfied. hold and wait simply proves the condition that from this point onward, the system will not change, which is all the conditions that we needed to show.	system	244
if a system breaks any of them, it cannot have deadlock! consider the scenario where two students need to write both pen and paper and there is only one of each. breaking mutual exclusion means that the students share the pen and paper. breaking circular wait could be that the students agree to grab the pen then the paper. as proof by contradiction, say that deadlock occurs under the rule and the conditions. without loss of generality, that means a student would have to be waiting on a pen while holding the paper and the other waiting on a pen and holding the paper. we have contradicted ourselves because one student grabbed the paper without grabbing the pen, so deadlock fails to occur. breaking hold and wait could be that the students try to get the pen and then the paper and if a student fails to grab the paper then they release the pen. this introduces a new problem called livelock which will be discussed later. breaking preemption means that if the two students are in deadlock the teacher can come in and break up the deadlock by giving one of the students a held item or tell both students to put the items down.	system	5
livelock is generally harder to detect because the processes generally look like they are working to the outside operating system whereas in deadlock the operating system generally knows when two processes are waiting on a system-wide resource. another problem is that there are necessary conditions for livelock (i.e. deadlock fails to occur) but not sufficient conditions – meaning there is no set of rules where livelock has to occur. you must formally prove in a system by what is known as an invariant. one has to enumerate each of the steps of a system and if each of the steps eventually – after some finite number of steps – leads to forward progress, the system fails to livelock. there are even better systems that prove bounded waits; a system can only be livelocked for at most n cycles which may be important for something like stock exchanges.	system	123
livelock is generally harder to detect because the processes generally look like they are working to the outside operating system whereas in deadlock the operating system generally knows when two processes are waiting on a system-wide resource. another problem is that there are necessary conditions for livelock (i.e. deadlock fails to occur) but not sufficient conditions – meaning there is no set of rules where livelock has to occur. you must formally prove in a system by what is known as an invariant. one has to enumerate each of the steps of a system and if each of the steps eventually – after some finite number of steps – leads to forward progress, the system fails to livelock. there are even better systems that prove bounded waits; a system can only be livelocked for at most n cycles which may be important for something like stock exchanges.	exchange	847
ignoring deadlock is the most obvious approach. quite humorously, the name for this approach is called the ostrich algorithm. though there is no apparent source, the idea for the algorithm comes from the concept of an ostrich sticking its head in the sand. when the operating system detects deadlock, it does nothing out of the ordinary, and any deadlock usually goes away. an operating system preempts processes when stopping them for context switches. the operating system can interrupt any system call, potentially breaking a deadlock scenario.	system	276
the os also makes some files read-only thus making the resource shareable. what the algorithm refers to is that if there is an adversary that specifically crafts a program – or equivalently a user who poorly writes a program – that the os deadlocks. for everyday life, this tends to be fine. when it is not we can turn to the following method.	the following	322
deadlock detection allows the system to enter a deadlocked state. after entering, the system uses the information to break deadlock. as an example, consider multiple processes accessing files. the operating system can keep track of all of the files/resources through file descriptors at some level either abstracted through an api or directly. if the operating system detects a directed cycle in the operating system file descriptor table it may break one process’ hold through scheduling for example and let the system proceed. why this is a popular choice in this realm is that there is no way of knowing which resources a program will select without running the program. this is an extension of rice’s theorem [3] that says that we cannot know any semantic feature without running the program (semantic meaning like what files it tries to open). so theoretically, it is sound. the	resources	249
deadlock detection allows the system to enter a deadlocked state. after entering, the system uses the information to break deadlock. as an example, consider multiple processes accessing files. the operating system can keep track of all of the files/resources through file descriptors at some level either abstracted through an api or directly. if the operating system detects a directed cycle in the operating system file descriptor table it may break one process’ hold through scheduling for example and let the system proceed. why this is a popular choice in this realm is that there is no way of knowing which resources a program will select without running the program. this is an extension of rice’s theorem [3] that says that we cannot know any semantic feature without running the program (semantic meaning like what files it tries to open). so theoretically, it is sound. the	system	30
problem then gets introduced that we could reach a livelock scenario if we preempt a set of resources again and again. the way around this is mostly probabilistic. the operating system chooses a random resource to break hold-and-wait. now even though a user can craft a program where breaking hold and wait on each resource will result in a livelock, this doesn’t happen as often on machines that run programs in practice or the livelock that does happen happens for a couple of cycles. these systems are good for products that need to maintain a non-deadlocked state but can tolerate a small chance of livelock for a short time.	resources	92
problem then gets introduced that we could reach a livelock scenario if we preempt a set of resources again and again. the way around this is mostly probabilistic. the operating system chooses a random resource to break hold-and-wait. now even though a user can craft a program where breaking hold and wait on each resource will result in a livelock, this doesn’t happen as often on machines that run programs in practice or the livelock that does happen happens for a couple of cycles. these systems are good for products that need to maintain a non-deadlocked state but can tolerate a small chance of livelock for a short time.	system	178
the dining philosophers problem is a classic synchronization problem. imagine we invite n (let’s say 6) philosophers to a meal. we will sit them at a table with 6 chopsticks, one between each philosopher. a philosopher alternates between wanting to eat or think. to eat the philosopher must pick up the two chopsticks either side of their position. the original problem required each philosopher to have two forks, but one can eat with a single fork so we rule this out. however, these chopsticks are shared with his neighbor.	the dining philosophers problem	0
resources	resources	0
pthread_mutex_t* left_fork = phil_info->left_fork; pthread_mutex_t* right_fork = phil_info->right_fork; while(phil_info->simulation){ pthread_mutex_lock(left_fork); pthread_mutex_lock(right_fork); eat(left_fork, right_fork); pthread_mutex_unlock(left_fork); pthread_mutex_unlock(right_fork); } }	thread	1
this looks good but. what if everyone picks up their left fork and is waiting on their right fork? we have deadlocked the program. it is important to note that deadlock doesn’t happen all the time and the probability that this solution deadlock goes down as the number of philosophers goes up. what is important to note is that eventually that this solution will deadlock, letting threads starve which is bad. here is a simple resource allocation graph that shows how the system could be deadlocked	thread	381
this looks good but. what if everyone picks up their left fork and is waiting on their right fork? we have deadlocked the program. it is important to note that deadlock doesn’t happen all the time and the probability that this solution deadlock goes down as the number of philosophers goes up. what is important to note is that eventually that this solution will deadlock, letting threads starve which is bad. here is a simple resource allocation graph that shows how the system could be deadlocked	system	472
void* philosopher(void* forks){ info phil_info = forks; pthread_mutex_t* left_fork = phil_info->left_fork; pthread_mutex_t* right_fork = phil_info->right_fork; while(phil_info->simulation){ int left_succeed = pthread_mutex_trylock(left_fork); if (!left_succeed) { sleep(); continue; } int right_succeed = pthread_mutex_trylock(right_fork); if (!right_succeed) { pthread_mutex_unlock(left_fork); sleep(); continue;	thread	57
} eat(left_fork, right_fork); pthread_mutex_unlock(left_fork); pthread_mutex_unlock(right_fork); } }	thread	31
now our philosopher picks up the left fork and tries to grab the right. if it’s available, they eat. if it’s not available, they put the left fork down and try again. no deadlock! but, there is a problem. what if all the philosophers pick up their left at the same time, try to grab their right, put their left down, pick up their left, try to grab their right and so on. here is what a time evolution of the system would look like.	system	409
the naive arbitrator solution has one arbitrator a mutex for example. have each of the philosophers ask the arbitrator for permission to eat or trylock an arbitrator mutex. this solution allows one philosopher to eat at a time. when they are done, another philosopher can ask for permission to eat. this prevents deadlock because there is no circular wait! no philosopher has to wait for any other philosopher. the advanced arbitrator solution is to implement a class that determines if the philosopher’s forks are in the arbitrator’s possession. if they are, they give them to the philosopher, let him eat, and take the forks back. this has the bonus of being able to have multiple philosophers eat at the same time.	a mutex	49
failure. assuming that all the philosophers are good-willed, the arbitrator needs to be fair. in practical systems, the arbitrator tends to give forks to the same processes because of scheduling or pseudo-randomness. another important thing to note is that this prevents deadlock for the entire system. but in our model of dining philosophers, the philosopher has to release the lock themselves. then, you can consider the case of the malicious philosopher (let’s say descartes because of his evil demons) could hold on to the arbitrator forever. he would make forward progress and the system would make forward progress but there is no way of ensuring that each process makes forward progress without assuming something about the processes or having true preemption – meaning that a higher authority (let’s say steve jobs) tells them to stop eating forcibly.	system	107
proof: the arbitrator solution doesn’t deadlock the proof is about as simple as it gets. only one philosopher can request resources at a time. there is no way to make a cycle in the resource allocation graph with only one philosopher acting in pickup the left then the right fork which is what we needed to show.	resources	122
stallings’ [5, p. 280] solution removes philosophers from the table until deadlock is not possible – think about what the magic number of philosophers at the table. the way to do this in the actual system is through semaphores and letting a certain number of philosophers through. this has the benefit that multiple philosophers can be eating.	system	198
there is also no reliable way to know the number of resources beforehand. in the dining philosophers case, this is solved because everything is known but trying to specify an operating system where a system doesn’t know which file is going to get opened by what process can lead to a faulty solution. and again since semaphores are system constructs, they obey system timing clocks which means that the same processes tend to get added back into the queue again. now if a philosopher becomes evil, then the problem becomes that there is no preemption. a	resources	52
there is also no reliable way to know the number of resources beforehand. in the dining philosophers case, this is solved because everything is known but trying to specify an operating system where a system doesn’t know which file is going to get opened by what process can lead to a faulty solution. and again since semaphores are system constructs, they obey system timing clocks which means that the same processes tend to get added back into the queue again. now if a philosopher becomes evil, then the problem becomes that there is no preemption. a	system	185
philosopher can eat for as long as they want and the system will continue to function but that means the fairness of this solution can be low in the worst case. this works best with timeouts or forced context switches to ensure bounded wait times.	system	53
proof: stallings’ solution doesn’t deadlock. let’s number the philosophers {p0 , p1 , .., pn−1 } and the resources {r0 , r1 , .., rn−1 }. a philosopher pi needs resource ri−1 mod n and ri+1 mod n . without loss of generality, let us take pi out of the picture. each resource had exactly two philosophers that could use it. now resources ri−1 mod n and ri+1 mod n only have on philosopher waiting on it. even if hold and wait, no preemption, and mutual exclusion or present, the resources can never enter a state where one philosopher requests them and they are held by another philosopher because only one philosopher can request them. since there is no way to generate a cycle otherwise, circular wait cannot hold. since circular wait cannot hold, deadlock cannot happen.	resources	105
here is a visualization of the worst-case. the system is about to deadlock, but the approach resolves it.	system	47
some problems are that an entity either needs to know the finite set of resources in advance or be able to produce a consistent partial order such that circular wait cannot happen. this also implies that there needs to be some entity, either the operating system or another process, deciding on the number and all of the philosophers need to agree on the number as new resources come in. as we have also seen with previous solutions, this relies on context switching. this prioritizes philosophers that have already eaten but can be made fairer by introducing random sleeps and waits.	resources	72
some problems are that an entity either needs to know the finite set of resources in advance or be able to produce a consistent partial order such that circular wait cannot happen. this also implies that there needs to be some entity, either the operating system or another process, deciding on the number and all of the philosophers need to agree on the number as new resources come in. as we have also seen with previous solutions, this relies on context switching. this prioritizes philosophers that have already eaten but can be made fairer by introducing random sleeps and waits.	system	256
proof: dijkstra’s solution doesn’t deadlock the proof is similar to the previous proof. let’s number the philosophers {p0 , p1 , .., pn−1 } and the resources {r0 , r1 , .., rn−1 }. a philosopher pi needs resource ri−1 mod n and ri+1 mod n . each philosopher will grab ri−1 mod n then ri+1 mod n but the last philosopher will grab in the reverse order. even if hold and wait, no preemption, and mutual exclusion or present. since the last philosopher will grab rn−1 then r0 there are two cases either the philosopher has the first lock or the philosopher doesn’t.	resources	148
if the last philosopher pn−1 holds the first lock meaning the previous philosopher pn−2 is waiting on rn−1 meaning rn−2 is available. since no other blockers, the philosopher previous pn−3 will grab her first lock.	block	149
this is now a reduction to the previous proof of stalling because we now have n resources but only n − 1 philosophers, meaning this cannot deadlock.	resources	80
if the philosopher doesn’t obtain that first lock, then we have a reduction to stalling’s proof above because now have n − 1 philosophers vying for n resources. since we can’t reach deadlock in either case, this solution cannot deadlock which is what we needed to show.	resources	150
 which coffman condition is unsatisfied in the following snippet?	the following	43
 which coffman condition is unsatisfied in the following snippet?	snippet	57
 the following calls are made	the following	1
what happens and why? what happens if a third thread calls pthread_mutex_lock(m1) ?	thread	46
 how many processes are blocked? as usual, assume that a process can complete if it can acquire all of the resources listed below.	resources	107
 how many processes are blocked? as usual, assume that a process can complete if it can acquire all of the resources listed below.	a process	55
 how many processes are blocked? as usual, assume that a process can complete if it can acquire all of the resources listed below.	block	24
bibliography [1] edward g coffman, melanie elphick, and arie shoshani. system deadlocks. acm computing surveys (csur), 3(2):67–78, 1971.	system	71
[4] a. silberschatz, p.b. galvin, and g. gagne. operating system principles, 7th ed. wiley student edition.	system	58
[5] william stallings. operating systems: internals and design principles 7th ed. by stallings (international economy edition).	system	33
9 virtual memory and interprocess communication	memory	10
in simple embedded systems and early computers, processes directly access memory – “address 1234” corresponds to a particular byte stored in a particular part of physical memory. for example, the ibm 709 had to read and write directly to tape with no level of abstraction [3, p. 65]. even in systems after that, it was hard to adopt virtual memory because virtual memory required the whole fetch cycle to be altered through hardware – a change many manufacturers still thought was expensive. in the pdp-10, a workaround was used by using different registers for each process and then virtual memory was added later [1]. in modern systems, this is no longer the case. instead, each process is isolated, and there is a translation process between the address of a particular cpu instruction or piece of data of a process and the actual byte of physical memory (“ram”). memory addresses no longer map to physical addresses the process runs inside virtual memory. virtual memory keeps processes safe because one process cannot directly read or modify another process’s memory. virtual memory also allows the system to efficiently allocate and reallocate portions of memory to different processes. the modern process of translating memory is as follows.	address	84
in simple embedded systems and early computers, processes directly access memory – “address 1234” corresponds to a particular byte stored in a particular part of physical memory. for example, the ibm 709 had to read and write directly to tape with no level of abstraction [3, p. 65]. even in systems after that, it was hard to adopt virtual memory because virtual memory required the whole fetch cycle to be altered through hardware – a change many manufacturers still thought was expensive. in the pdp-10, a workaround was used by using different registers for each process and then virtual memory was added later [1]. in modern systems, this is no longer the case. instead, each process is isolated, and there is a translation process between the address of a particular cpu instruction or piece of data of a process and the actual byte of physical memory (“ram”). memory addresses no longer map to physical addresses the process runs inside virtual memory. virtual memory keeps processes safe because one process cannot directly read or modify another process’s memory. virtual memory also allows the system to efficiently allocate and reallocate portions of memory to different processes. the modern process of translating memory is as follows.	a process	809
in simple embedded systems and early computers, processes directly access memory – “address 1234” corresponds to a particular byte stored in a particular part of physical memory. for example, the ibm 709 had to read and write directly to tape with no level of abstraction [3, p. 65]. even in systems after that, it was hard to adopt virtual memory because virtual memory required the whole fetch cycle to be altered through hardware – a change many manufacturers still thought was expensive. in the pdp-10, a workaround was used by using different registers for each process and then virtual memory was added later [1]. in modern systems, this is no longer the case. instead, each process is isolated, and there is a translation process between the address of a particular cpu instruction or piece of data of a process and the actual byte of physical memory (“ram”). memory addresses no longer map to physical addresses the process runs inside virtual memory. virtual memory keeps processes safe because one process cannot directly read or modify another process’s memory. virtual memory also allows the system to efficiently allocate and reallocate portions of memory to different processes. the modern process of translating memory is as follows.	memory	74
in simple embedded systems and early computers, processes directly access memory – “address 1234” corresponds to a particular byte stored in a particular part of physical memory. for example, the ibm 709 had to read and write directly to tape with no level of abstraction [3, p. 65]. even in systems after that, it was hard to adopt virtual memory because virtual memory required the whole fetch cycle to be altered through hardware – a change many manufacturers still thought was expensive. in the pdp-10, a workaround was used by using different registers for each process and then virtual memory was added later [1]. in modern systems, this is no longer the case. instead, each process is isolated, and there is a translation process between the address of a particular cpu instruction or piece of data of a process and the actual byte of physical memory (“ram”). memory addresses no longer map to physical addresses the process runs inside virtual memory. virtual memory keeps processes safe because one process cannot directly read or modify another process’s memory. virtual memory also allows the system to efficiently allocate and reallocate portions of memory to different processes. the modern process of translating memory is as follows.	system	19
1. a process makes a memory request 2. the circuit first checks the translation lookaside buffer (tlb) if the address page is cached into memory. it skips to the reading from/writing to phase if found otherwise the request goes to the mmu.	address	110
1. a process makes a memory request 2. the circuit first checks the translation lookaside buffer (tlb) if the address page is cached into memory. it skips to the reading from/writing to phase if found otherwise the request goes to the mmu.	a process	3
1. a process makes a memory request 2. the circuit first checks the translation lookaside buffer (tlb) if the address page is cached into memory. it skips to the reading from/writing to phase if found otherwise the request goes to the mmu.	memory	21
3. the memory management unit (mmu) performs the address translation. if the translation succeeds, the page gets pulled from ram – conceptually the entire page isn’t loaded up. the result is cached in the tlb.	address	49
3. the memory management unit (mmu) performs the address translation. if the translation succeeds, the page gets pulled from ram – conceptually the entire page isn’t loaded up. the result is cached in the tlb.	memory	7
4. the cpu performs the operation by either reading from the physical address or writing to the address.	address	70
9.1 translating addresses	address	16
the memory management unit is part of the cpu, and it converts a virtual memory address into a physical address.	address	80
the memory management unit is part of the cpu, and it converts a virtual memory address into a physical address.	memory	4
first, we’ll talk about what the virtual memory abstraction is and how to translate addresses to illustrate, consider a 32-bit machine, meaning pointers are 32-bits. they can address 232 different locations or 4gb of memory where one address is one byte. imagine we had a large table for every possible address where we will store the ‘real’ i.e. physical address. each physical address will need 4 bytes – to hold the 32-bits.	address	84
first, we’ll talk about what the virtual memory abstraction is and how to translate addresses to illustrate, consider a 32-bit machine, meaning pointers are 32-bits. they can address 232 different locations or 4gb of memory where one address is one byte. imagine we had a large table for every possible address where we will store the ‘real’ i.e. physical address. each physical address will need 4 bytes – to hold the 32-bits.	memory	41
first, we’ll talk about what the virtual memory abstraction is and how to translate addresses to illustrate, consider a 32-bit machine, meaning pointers are 32-bits. they can address 232 different locations or 4gb of memory where one address is one byte. imagine we had a large table for every possible address where we will store the ‘real’ i.e. physical address. each physical address will need 4 bytes – to hold the 32-bits.	pointer	144
naturally, this scheme would require 16 billion bytes to store all of the entries. it should be painfully obvious that our lookup scheme would consume all of the memory that we could buy for our 4gb machine. our lookup table should be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data. the solution is to chunk memory into small regions called ‘pages’ and ‘frames’ and use a lookup table for each page.	memory	162
naturally, this scheme would require 16 billion bytes to store all of the entries. it should be painfully obvious that our lookup scheme would consume all of the memory that we could buy for our 4gb machine. our lookup table should be smaller than the memory we have otherwise we will have no space left for our actual programs and operating system data. the solution is to chunk memory into small regions called ‘pages’ and ‘frames’ and use a lookup table for each page.	system	342
a page is a block of virtual memory. a typical block size on linux is 4kib or 212 addresses, though one can find examples of larger blocks. so rather than talking about individual bytes, we can talk about blocks of 4kibs, each block is called a page. we can also number our pages (“page 0” “page 1” etc). let’s do a sample calculation of how many pages are there assume page size of 4kib.	address	82
a page is a block of virtual memory. a typical block size on linux is 4kib or 212 addresses, though one can find examples of larger blocks. so rather than talking about individual bytes, we can talk about blocks of 4kibs, each block is called a page. we can also number our pages (“page 0” “page 1” etc). let’s do a sample calculation of how many pages are there assume page size of 4kib.	a page	0
a page is a block of virtual memory. a typical block size on linux is 4kib or 212 addresses, though one can find examples of larger blocks. so rather than talking about individual bytes, we can talk about blocks of 4kibs, each block is called a page. we can also number our pages (“page 0” “page 1” etc). let’s do a sample calculation of how many pages are there assume page size of 4kib.	memory	29
a page is a block of virtual memory. a typical block size on linux is 4kib or 212 addresses, though one can find examples of larger blocks. so rather than talking about individual bytes, we can talk about blocks of 4kibs, each block is called a page. we can also number our pages (“page 0” “page 1” etc). let’s do a sample calculation of how many pages are there assume page size of 4kib.	block	12
for a 32-bit machine, 232 address/212 (address/page) = 220 pages.	address	26
for a 64-bit machine, 264 address/212 (address/page) = 252 pages ≈ 1015 pages.	address	26
we also call this a frame or sometimes called a ‘page frame’ is a block of physical memory or ram – random access memory. a frame is the same number of bytes as a virtual page or 4kib on our machine. it stores the bytes of interest. to access a particular byte in a frame, an mmu goes from the start of the frame and adds the offset – discussed later.	memory	84
we also call this a frame or sometimes called a ‘page frame’ is a block of physical memory or ram – random access memory. a frame is the same number of bytes as a virtual page or 4kib on our machine. it stores the bytes of interest. to access a particular byte in a frame, an mmu goes from the start of the frame and adds the offset – discussed later.	block	66
a page table is a map from a number to a particular frame. for example page 1 might be mapped to frame 45, page 2 mapped to frame 30. other frames might be currently unused or assigned to other running processes or used internally by the operating system. implied from the name, imagine a page table as a table.	a page	0
a page table is a map from a number to a particular frame. for example page 1 might be mapped to frame 45, page 2 mapped to frame 30. other frames might be currently unused or assigned to other running processes or used internally by the operating system. implied from the name, imagine a page table as a table.	system	248
a page table is a map from a number to a particular frame. for example page 1 might be mapped to frame 45, page 2 mapped to frame 30. other frames might be currently unused or assigned to other running processes or used internally by the operating system. implied from the name, imagine a page table as a table.	a page table	0
now to go through the actual calculations. we will assume that a 32-bit machine has 4kib pages naturally, to address all the possible entries, there are 220 frames. since there are 220 possible frames, we will need 20 bits to number all of the possible frames meaning frame number must be 2.5 bytes long. in practice, we’ll round that up to 4 bytes and do something interesting with the rest of the bits. with 4 bytes per entry x 220 entries = 4 mib of physical memory are required to hold the entire page table for a process.	address	109
now to go through the actual calculations. we will assume that a 32-bit machine has 4kib pages naturally, to address all the possible entries, there are 220 frames. since there are 220 possible frames, we will need 20 bits to number all of the possible frames meaning frame number must be 2.5 bytes long. in practice, we’ll round that up to 4 bytes and do something interesting with the rest of the bits. with 4 bytes per entry x 220 entries = 4 mib of physical memory are required to hold the entire page table for a process.	a process	516
now to go through the actual calculations. we will assume that a 32-bit machine has 4kib pages naturally, to address all the possible entries, there are 220 frames. since there are 220 possible frames, we will need 20 bits to number all of the possible frames meaning frame number must be 2.5 bytes long. in practice, we’ll round that up to 4 bytes and do something interesting with the rest of the bits. with 4 bytes per entry x 220 entries = 4 mib of physical memory are required to hold the entire page table for a process.	memory	462
remember our page table maps pages to frames, but each frame is a block of contiguous addresses. how do we calculate which particular byte to use inside a particular frame? the solution is to re-use the lowest bits of the virtual memory address directly. for example, suppose our process is reading the following addressvirtualaddress = 11110000111100001111000010101010 (binary) so to give an example say we have the virtual address above. how would we split it up using a one-page table to frame scheme?	the following	299
remember our page table maps pages to frames, but each frame is a block of contiguous addresses. how do we calculate which particular byte to use inside a particular frame? the solution is to re-use the lowest bits of the virtual memory address directly. for example, suppose our process is reading the following addressvirtualaddress = 11110000111100001111000010101010 (binary) so to give an example say we have the virtual address above. how would we split it up using a one-page table to frame scheme?	address	86
remember our page table maps pages to frames, but each frame is a block of contiguous addresses. how do we calculate which particular byte to use inside a particular frame? the solution is to re-use the lowest bits of the virtual memory address directly. for example, suppose our process is reading the following addressvirtualaddress = 11110000111100001111000010101010 (binary) so to give an example say we have the virtual address above. how would we split it up using a one-page table to frame scheme?	memory	230
remember our page table maps pages to frames, but each frame is a block of contiguous addresses. how do we calculate which particular byte to use inside a particular frame? the solution is to re-use the lowest bits of the virtual memory address directly. for example, suppose our process is reading the following addressvirtualaddress = 11110000111100001111000010101010 (binary) so to give an example say we have the virtual address above. how would we split it up using a one-page table to frame scheme?	block	66
figure 9.2: splitting address we can imagine the steps to dereference as one process. in general, it looks like the following.	the following	112
figure 9.2: splitting address we can imagine the steps to dereference as one process. in general, it looks like the following.	address	22
figure 9.3: one level dereference the way to read from a particular address above is visualized below.	address	68
figure 9.4: one level dereference example and if we were reading from it, ’return’ that value. this sounds like a perfect solution. take each address and map it to a virtual address in sequential order. the process will believe that the address looks continuous, but the top 20 bits are used to figure out page_num, which will allow us to find the frame number, find the frame, add the offset – derived from the last 12 bits – and do the read or write.	address	142
we do have a problem with 64-bit operating systems. for a 64-bit machine with 4kib pages, each entry needs 52 bits. meaning we need roughly with 252 entries, that’s 255 bytes (roughly 40 petabytes). so our page table is too large. in 64-bit architecture, memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used. we’ll take about this below. there is one last piece of terminology that needs to be covered.	address	262
we do have a problem with 64-bit operating systems. for a 64-bit machine with 4kib pages, each entry needs 52 bits. meaning we need roughly with 252 entries, that’s 255 bytes (roughly 40 petabytes). so our page table is too large. in 64-bit architecture, memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used. we’ll take about this below. there is one last piece of terminology that needs to be covered.	memory	255
we do have a problem with 64-bit operating systems. for a 64-bit machine with 4kib pages, each entry needs 52 bits. meaning we need roughly with 252 entries, that’s 255 bytes (roughly 40 petabytes). so our page table is too large. in 64-bit architecture, memory addresses are sparse, so we need a mechanism to reduce the page table size, given that most of the entries will never be used. we’ll take about this below. there is one last piece of terminology that needs to be covered.	system	43
multi-level pages are one solution to the page table size issue for 64-bit architectures. we’ll look at the simplest implementation - a two-level page table. each table is a list of pointers that point to the next level of tables, som sub-tables may be omitted. an example, a two-level page table for a 32-bit architecture is shown below.	each table	158
multi-level pages are one solution to the page table size issue for 64-bit architectures. we’ll look at the simplest implementation - a two-level page table. each table is a list of pointers that point to the next level of tables, som sub-tables may be omitted. an example, a two-level page table for a 32-bit architecture is shown below.	pointer	182
figure 9.5: three way address split	address	22
so what is the intuition for dereferencing an address? first, the mmu takes the top-level page table and find the index1’th entry. that will contain a number that will lead the mmu to the appropriate sub-table then go to the index2’th entry of that table. that will contain a frame number. this is the good old fashioned 4kib ram that we were talking about earlier. then, the mmu adds the offset and do the read or write.	address	46
visualizing the dereference in one diagram, the dereference looks like the following image.	the following	71
if 2 bytes are used for each entry in the top-level table and there are only 210 entries, we only need 2kib to store this entire first level page table. each subtable will point to physical frames, and each of their entries needs to be the required 4 bytes to be able to address all the frames as mentioned earlier. however, for processes with only tiny memory needs, we only need to specify entries for low memory addresses for the heap and program code and high memory addresses for the stack.	the heap	429
if 2 bytes are used for each entry in the top-level table and there are only 210 entries, we only need 2kib to store this entire first level page table. each subtable will point to physical frames, and each of their entries needs to be the required 4 bytes to be able to address all the frames as mentioned earlier. however, for processes with only tiny memory needs, we only need to specify entries for low memory addresses for the heap and program code and high memory addresses for the stack.	code	450
if 2 bytes are used for each entry in the top-level table and there are only 210 entries, we only need 2kib to store this entire first level page table. each subtable will point to physical frames, and each of their entries needs to be the required 4 bytes to be able to address all the frames as mentioned earlier. however, for processes with only tiny memory needs, we only need to specify entries for low memory addresses for the heap and program code and high memory addresses for the stack.	address	271
if 2 bytes are used for each entry in the top-level table and there are only 210 entries, we only need 2kib to store this entire first level page table. each subtable will point to physical frames, and each of their entries needs to be the required 4 bytes to be able to address all the frames as mentioned earlier. however, for processes with only tiny memory needs, we only need to specify entries for low memory addresses for the heap and program code and high memory addresses for the stack.	memory	354
thus, the total memory overhead for our multi-level page table has shrunk from 4mib for the single-level implementation to three page tables of memory or 2kib for the top-level and 4kib for the two intermediate levels of size 10kib. here’s why. we need at least one frame for the high-level directory and two frames for two sub-tables. one sub-table is necessary for the low addresses – program code, constants and possibly a tiny heap.	code	395
thus, the total memory overhead for our multi-level page table has shrunk from 4mib for the single-level implementation to three page tables of memory or 2kib for the top-level and 4kib for the two intermediate levels of size 10kib. here’s why. we need at least one frame for the high-level directory and two frames for two sub-tables. one sub-table is necessary for the low addresses – program code, constants and possibly a tiny heap.	address	375
thus, the total memory overhead for our multi-level page table has shrunk from 4mib for the single-level implementation to three page tables of memory or 2kib for the top-level and 4kib for the two intermediate levels of size 10kib. here’s why. we need at least one frame for the high-level directory and two frames for two sub-tables. one sub-table is necessary for the low addresses – program code, constants and possibly a tiny heap.	memory	16
the other sub-table is for higher addresses used by the environment and stack. in practice, real programs will likely need more sub-table entries, as each subtable can only reference 1024*4kib = 4mib of address space.	address	34
the main point still stands. we have significantly reduced the memory overhead required to perform page table lookups.	memory	63
there are lots of problems with page tables – one of the big problems is that they are slow. for a single page table, our machine is now twice as slow! two memory accesses are required. for a two-level page table, memory access is now three times as slow – three memory accesses are required.	memory	156
to overcome this overhead, the mmu includes an associative cache of recently-used virtual-page-to-frame lookups. this cache is called the tlb (“translation lookaside buffer”). every time a virtual address needs to be translated into a physical memory location, the tlb is queried in parallel to the page table. for most memory	address	197
to overcome this overhead, the mmu includes an associative cache of recently-used virtual-page-to-frame lookups. this cache is called the tlb (“translation lookaside buffer”). every time a virtual address needs to be translated into a physical memory location, the tlb is queried in parallel to the page table. for most memory	memory	244
accesses of most programs, there is a significant chance that the tlb has cached the results. however, if a program has inadequate cache coherence, the address will be missing in the tlb, meaning the mmu must use the much slower page table translation.	address	152
there is a sort of pseudocode associated with the mmu. we will assume that this is for a single-level page table.	code	25
1. receive address 2. try to translate address according to the programmed scheme 3. if the translation fails, report an invalid address 4. otherwise, (a) if the tlb contains the physical memory, get the physical frame from the tlb and perform the read and write.	address	11
1. receive address 2. try to translate address according to the programmed scheme 3. if the translation fails, report an invalid address 4. otherwise, (a) if the tlb contains the physical memory, get the physical frame from the tlb and perform the read and write.	memory	188
(b) if the page exists in memory, check if the process has permissions to perform the operation on the page meaning the process has access to the page, and it is reading from the page/writing to a page that it has permission to do so.	a page	195
(b) if the page exists in memory, check if the process has permissions to perform the operation on the page meaning the process has access to the page, and it is reading from the page/writing to a page that it has permission to do so.	memory	26
i. if so then do the dereference provide the address, cache the results in the tlb ii. otherwise, trigger a hardware interrupt. the kernel will most likely send a sigsegv or a segmentation violation.	address	45
(c) if the page doesn’t exist in memory, generate an interrupt.	memory	33
frames can be shared between processes, and this is where the heart of the chapter comes into play. we can use these tables to communicate with processes. in addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. read-only frames can then be safely shared between multiple processes. for example, the c-library instruction code can be shared between all processes that dynamically load the code into the process memory. each process can only read that memory. meaning that if a program tries to write to a read-only page in memory, it will segfault. that is why sometimes memory accesses segfault and sometimes they don’t, it all depends on if your hardware says that a program can access.	code	403
frames can be shared between processes, and this is where the heart of the chapter comes into play. we can use these tables to communicate with processes. in addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. read-only frames can then be safely shared between multiple processes. for example, the c-library instruction code can be shared between all processes that dynamically load the code into the process memory. each process can only read that memory. meaning that if a program tries to write to a read-only page in memory, it will segfault. that is why sometimes memory accesses segfault and sometimes they don’t, it all depends on if your hardware says that a program can access.	a process	240
frames can be shared between processes, and this is where the heart of the chapter comes into play. we can use these tables to communicate with processes. in addition to storing the frame number, the page table can be used to store whether a process can write or only read a particular frame. read-only frames can then be safely shared between multiple processes. for example, the c-library instruction code can be shared between all processes that dynamically load the code into the process memory. each process can only read that memory. meaning that if a program tries to write to a read-only page in memory, it will segfault. that is why sometimes memory accesses segfault and sometimes they don’t, it all depends on if your hardware says that a program can access.	memory	492
also, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. it is an important distinction that we are talking about mmap and not memory-mapped io in general. the mmap system call can’t reliably be used to do other memory-mapped operations like communicate with gpus and write pixels to the screen – this is mainly hardware dependent.	address	148
also, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. it is an important distinction that we are talking about mmap and not memory-mapped io in general. the mmap system call can’t reliably be used to do other memory-mapped operations like communicate with gpus and write pixels to the screen – this is mainly hardware dependent.	a page	26
also, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. it is an important distinction that we are talking about mmap and not memory-mapped io in general. the mmap system call can’t reliably be used to do other memory-mapped operations like communicate with gpus and write pixels to the screen – this is mainly hardware dependent.	mmap	64
also, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. it is an important distinction that we are talking about mmap and not memory-mapped io in general. the mmap system call can’t reliably be used to do other memory-mapped operations like communicate with gpus and write pixels to the screen – this is mainly hardware dependent.	memory	277
also, processes can share a page with a child process using the mmap system call. mmap is an interesting call because instead of tying each virtual address to a physical frame, it ties it to something else. it is an important distinction that we are talking about mmap and not memory-mapped io in general. the mmap system call can’t reliably be used to do other memory-mapped operations like communicate with gpus and write pixels to the screen – this is mainly hardware dependent.	system	69
1. the read-only bit marks the page as read-only. attempts to write to the page will cause a page fault. the page fault will then be handled by the kernel. two examples of the read-only page include sharing the c standard library between multiple processes for security you wouldn’t want to allow one process to modify the library and copy-on-write where the cost of duplicating a page can be delayed until the first write occurs.	a page	91
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.	the heap	286
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.	code	235
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.	a page	46
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.	memory	796
2. the execution bit defines whether bytes in a page can be executed as cpu instructions. processors may merge these bits into one and deem a page either writable or executable. this bit is useful because it prevents stack overflow or code injection attacks when writing user data into the heap or the stack because those are not read-only and thus not executable. further reading: background 3. the dirty bit allows for performance optimization. a page exclusively read from can be discarded without syncing to disk, since the page hasn’t changed. however, if the page was written to after it’s paged in, its dirty bit will be set, indicating that the page must be written back to the backing store. this strategy requires that the backing store retain a copy of the page after it is paged into memory. when a dirty bit is omitted, the backing store need only be as large as the instantaneous total size of all paged-out pages at any moment.	background	382
when a dirty bit is used, at all times some pages will exist in both physical memory and the backing store.	memory	78
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	address	51
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	a page	0
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	a process	29
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	memory	81
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	system	295
a page fault may happen when a process accesses an address in a frame missing in memory. there are three types of page faults 1. minor if there is no mapping yet for the page, but it is a valid address. this could be memory asked for by sbrk(2) but not written to yet meaning that the operating system can wait for the first write before allocating space – if it was read from, the operating system could short circuit the operation to read 0. the os simply makes the page, loads it into memory, and moves on.	type	105
2. major if the mapping to the page is exclusively on disk. the operating system will swap the page into memory and swap another page out. if this happens frequently enough, your program is said to thrash the mmu.	memory	105
2. major if the mapping to the page is exclusively on disk. the operating system will swap the page into memory and swap another page out. if this happens frequently enough, your program is said to thrash the mmu.	system	74
3. invalid when a program tries to write to a non-writable memory address or read to a non-readable memory address. the mmu generates an invalid fault and the os will usually generate a sigsegv meaning segmentation violation meaning that the program wrote outside the segment that it could write to.	address	66
3. invalid when a program tries to write to a non-writable memory address or read to a non-readable memory address. the mmu generates an invalid fault and the os will usually generate a sigsegv meaning segmentation violation meaning that the program wrote outside the segment that it could write to.	memory	59
what does this have to do with ipc? before, you knew that processes had isolation. one, you didn’t know how that isolation mapped. two, you may not know how you can break this isolation. to break any memory level isolation you have two avenues. one is to ask the kernel to provide some kind of interface. the other is to ask the kernel to map two pages of memory to the same virtual memory area and handle all the synchronization yourself.	memory	200
9.2 mmap	mmap	4
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	the following	327
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	code	341
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	a page	56
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	mmap	0
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	memory	27
mmap is a trick of virtual memory of instead of mapping a page to a frame, that frame can be backed by a file on disk, or the frame can be shared among processes. we can use that to read from a file on disk efficiently or sync changes to the file. one of the big optimizations is a file may be lazily allocated to memory. take the following code for example.	optimizations	263
the kernel sees that the program wants to mmap the file into memory, so it will reserve some space in your address space that is the length of the file. that means when the program writes to addr[0] that it writes to the first byte of the file. the kernel can do some optimizations too. instead of loading the whole file into memory, it may only load pages at a time. a program may only access 3 or 4 pages making loading the entire file a waste of time. page faults are so powerful because let the operating system take control of when a file is used.	address	107
the kernel sees that the program wants to mmap the file into memory, so it will reserve some space in your address space that is the length of the file. that means when the program writes to addr[0] that it writes to the first byte of the file. the kernel can do some optimizations too. instead of loading the whole file into memory, it may only load pages at a time. a program may only access 3 or 4 pages making loading the entire file a waste of time. page faults are so powerful because let the operating system take control of when a file is used.	mmap	42
the kernel sees that the program wants to mmap the file into memory, so it will reserve some space in your address space that is the length of the file. that means when the program writes to addr[0] that it writes to the first byte of the file. the kernel can do some optimizations too. instead of loading the whole file into memory, it may only load pages at a time. a program may only access 3 or 4 pages making loading the entire file a waste of time. page faults are so powerful because let the operating system take control of when a file is used.	memory	61
the kernel sees that the program wants to mmap the file into memory, so it will reserve some space in your address space that is the length of the file. that means when the program writes to addr[0] that it writes to the first byte of the file. the kernel can do some optimizations too. instead of loading the whole file into memory, it may only load pages at a time. a program may only access 3 or 4 pages making loading the entire file a waste of time. page faults are so powerful because let the operating system take control of when a file is used.	system	509
the kernel sees that the program wants to mmap the file into memory, so it will reserve some space in your address space that is the length of the file. that means when the program writes to addr[0] that it writes to the first byte of the file. the kernel can do some optimizations too. instead of loading the whole file into memory, it may only load pages at a time. a program may only access 3 or 4 pages making loading the entire file a waste of time. page faults are so powerful because let the operating system take control of when a file is used.	optimizations	268
9.2.1 mmap definitions	mmap	6
mmap does more than take a file and map it to memory. it is the general interface for creating shared memory among processes. currently it only supports regular files and posix shmem [2]. naturally, you can read all about it in the reference above, which references the current working group posix standard. some other options to note in the page will follow.	mmap	0
mmap does more than take a file and map it to memory. it is the general interface for creating shared memory among processes. currently it only supports regular files and posix shmem [2]. naturally, you can read all about it in the reference above, which references the current working group posix standard. some other options to note in the page will follow.	memory	46
the first option is that the flags argument of mmap can take many options.	mmap	47
1. prot_read this means the process can read the memory. this isn’t the only flag that gives the process read permission, however! the underlying file descriptor, in this case, must be opened with read privileges.	memory	49
2. prot_write this means the process can write to the memory. this has to be supplied for a process to write to a mapping. if this is supplied and prot_none is also supplied, the latter wins and no writes can be performed. the underlying file descriptor, in this case, must either be opened with write privileges or a private mapping must be supplied below 3. prot_exec this means the process can execute this piece of memory. although this is not stated in posix documents, this shouldn’t be supplied with write or none because that would make this invalid under the nx bit or not being able to execute (respectively) 4. prot_none this means the process can’t do anything with the mapping. this could be useful if you implement guard pages in terms of security. if you surround critical data with many more pages that can’t be accessed, that decreases the chance of various attacks.	a process	90
2. prot_write this means the process can write to the memory. this has to be supplied for a process to write to a mapping. if this is supplied and prot_none is also supplied, the latter wins and no writes can be performed. the underlying file descriptor, in this case, must either be opened with write privileges or a private mapping must be supplied below 3. prot_exec this means the process can execute this piece of memory. although this is not stated in posix documents, this shouldn’t be supplied with write or none because that would make this invalid under the nx bit or not being able to execute (respectively) 4. prot_none this means the process can’t do anything with the mapping. this could be useful if you implement guard pages in terms of security. if you surround critical data with many more pages that can’t be accessed, that decreases the chance of various attacks.	memory	54
6. map_private this mapping will only be visible to the process itself. useful to not thrash the operating system.	system	107
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.	address	206
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.	mmap	37
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.	memory	317
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.	parameter	415
remember that once a program is done mmapping that the program must munmap to tell the operating system that it is no longer using the pages allocated, so the os can write it back to disk and give back the addresses in case another mmap needs to occur. there are accompanying calls msync that take a piece of mmap’ed memory and sync the changes back to the filesystem though we won’t cover that in-depth. the other parameters to mmap are described in the annotated walkthrough below.	system	97
9.2.2 annotated mmap walkthrough	mmap	16
below is an annotated walkthrough of the example code in the man pages. our command-line utility will take a file, offset, and length to print. we can assume that these are initialized correctly and the offset + length is less than the length of the file.	code	49
we’ll assume that all system calls succeed. first, we have to open the file and get the size.	system	22
then, we need to introduce another variable known as page_offset. mmap doesn’t let the program pass in any value as an offset, it needs to be a multiple of the page size. in our case, we will round down.	mmap	66
then, we make the call to mmap, here is the order of arguments.	mmap	26
1. null, this tells mmap we don’t need any particular address to start from 2. length + offset - page_offset, mmaps the “rest” of the file into memory (starting from offset) 3. prot_read, we want to read the file 4. map_private, tell the os, we don’t want to share our mapping 5. fd, object descriptor that we refer to 6. pa_offset, the page aligned offset to start from	address	54
1. null, this tells mmap we don’t need any particular address to start from 2. length + offset - page_offset, mmaps the “rest” of the file into memory (starting from offset) 3. prot_read, we want to read the file 4. map_private, tell the os, we don’t want to share our mapping 5. fd, object descriptor that we refer to 6. pa_offset, the page aligned offset to start from	mmap	20
1. null, this tells mmap we don’t need any particular address to start from 2. length + offset - page_offset, mmaps the “rest” of the file into memory (starting from offset) 3. prot_read, we want to read the file 4. map_private, tell the os, we don’t want to share our mapping 5. fd, object descriptor that we refer to 6. pa_offset, the page aligned offset to start from	memory	144
char * addr = mmap(null, length + offset - page_offset, prot_read, map_private, fd, page_offset);	mmap	14
now, we can interact with the address as if it were a normal buffer. after, we have to unmap the file and close the file descriptor to make sure other system resources are freed.	resources	158
now, we can interact with the address as if it were a normal buffer. after, we have to unmap the file and close the file descriptor to make sure other system resources are freed.	address	30
now, we can interact with the address as if it were a normal buffer. after, we have to unmap the file and close the file descriptor to make sure other system resources are freed.	system	151
9.2.3 mmap communication	mmap	6
so how would we use mmap to communicate across processes? conceptually, it would be the same as using threading. let’s go through a broken down example. first, we need to allocate some space. we can do that with the mmap call. we’ll also allocate space for 100 integers	thread	102
so how would we use mmap to communicate across processes? conceptually, it would be the same as using threading. let’s go through a broken down example. first, we need to allocate some space. we can do that with the mmap call. we’ll also allocate space for 100 integers	mmap	20
int size = 100 * sizeof(int); void *addr = mmap(0, size, prot_read | prot_write, map_shared | map_anonymous, -1, 0); int *shared = addr;	mmap	43
now, there is no assurance that the values will be communicated because the process used sleep, not a mutex.	a mutex	100
<stdio.h> <stdlib.h> <sys/types.h> <sys/stat.h> <sys/mman.h> /* mmap() is defined in this header */	mmap	64
<stdio.h> <stdlib.h> <sys/types.h> <sys/stat.h> <sys/mman.h> /* mmap() is defined in this header */	type	26
<fcntl.h> <unistd.h> <errno.h> <string.h>	string	32
this piece of code allocates space for a 100 integers and creates a piece of memory that is shared between all processes. the code then forks. the parent process writes two integers to the first two slots. to avoid a data race, the child sleeps for a second and then prints out the stored values. this is an imperfect way to protect against data races. we could use a mutex across the processes mentioned in the synchronization section. but for this simple example, it works fine. note that each process should call munmap when done using the piece of memory.	a mutex	366
this piece of code allocates space for a 100 integers and creates a piece of memory that is shared between all processes. the code then forks. the parent process writes two integers to the first two slots. to avoid a data race, the child sleeps for a second and then prints out the stored values. this is an imperfect way to protect against data races. we could use a mutex across the processes mentioned in the synchronization section. but for this simple example, it works fine. note that each process should call munmap when done using the piece of memory.	code	14
this piece of code allocates space for a 100 integers and creates a piece of memory that is shared between all processes. the code then forks. the parent process writes two integers to the first two slots. to avoid a data race, the child sleeps for a second and then prints out the stored values. this is an imperfect way to protect against data races. we could use a mutex across the processes mentioned in the synchronization section. but for this simple example, it works fine. note that each process should call munmap when done using the piece of memory.	memory	77
this piece of code allocates space for a 100 integers and creates a piece of memory that is shared between all processes. the code then forks. the parent process writes two integers to the first two slots. to avoid a data race, the child sleeps for a second and then prints out the stored values. this is an imperfect way to protect against data races. we could use a mutex across the processes mentioned in the synchronization section. but for this simple example, it works fine. note that each process should call munmap when done using the piece of memory.	section	428
sharing anonymous memory is an efficient form of inter-process communication because there is no copying, system call, or disk-access overhead - the two processes share the same physical frame of main memory. on the other hand, shared memory, like in a multithreading context, creates room for data races. processes that share writable memory might need to use synchronization primitives like mutexes to prevent these from happening.	thread	258
sharing anonymous memory is an efficient form of inter-process communication because there is no copying, system call, or disk-access overhead - the two processes share the same physical frame of main memory. on the other hand, shared memory, like in a multithreading context, creates room for data races. processes that share writable memory might need to use synchronization primitives like mutexes to prevent these from happening.	memory	18
sharing anonymous memory is an efficient form of inter-process communication because there is no copying, system call, or disk-access overhead - the two processes share the same physical frame of main memory. on the other hand, shared memory, like in a multithreading context, creates room for data races. processes that share writable memory might need to use synchronization primitives like mutexes to prevent these from happening.	system	106
you’ve seen the virtual memory way of ipc, but there are more standard versions of ipc that are provided by the kernel. one of the big utilities is posix pipes. a pipe simply takes in a stream of bytes and spits out a sequence of bytes.	memory	24
small, portable programs that did one thing well and could be composed. as such, pipes were invented to take the output of one program and feed it to the input of another program though they have other uses today – you can read more at the wikipedia page consider if you type the following into your terminal.	the following	276
small, portable programs that did one thing well and could be composed. as such, pipes were invented to take the output of one program and feed it to the input of another program though they have other uses today – you can read more at the wikipedia page consider if you type the following into your terminal.	a page	248
small, portable programs that did one thing well and could be composed. as such, pipes were invented to take the output of one program and feed it to the input of another program though they have other uses today – you can read more at the wikipedia page consider if you type the following into your terminal.	type	271
what does the following code do? first, it lists the current directory. the -1 means that it outputs one entry per line. the cut command then takes everything before the first period. sort sorts all the input lines, uniq makes sure all the lines are unique. finally, tee outputs the contents to the file dir_contents and the terminal for your perusal. the important part is that bash creates 5 separate processes and connects their standard outs/stdins with pipes the trail looks something like this.	the following	10
what does the following code do? first, it lists the current directory. the -1 means that it outputs one entry per line. the cut command then takes everything before the first period. sort sorts all the input lines, uniq makes sure all the lines are unique. finally, tee outputs the contents to the file dir_contents and the terminal for your perusal. the important part is that bash creates 5 separate processes and connects their standard outs/stdins with pipes the trail looks something like this.	code	24
figure 9.8: pipe process filedescriptor redirection the numbers in the pipes are the file descriptors for each process and the arrow represents the redirect or where the output of the pipe is going. a posix pipe is almost like its real counterpart - a program can stuff bytes down one end and they will appear at the other end in the same order. unlike real pipes, however, the flow is always in the same direction, one file descriptor is used for reading and the other for writing. the pipe system call is used to create a pipe. these file descriptors can be used with read and write. a common method of using pipes is to create the pipe before forking to communicate with a child process	system	492
the problem with using a pipe in this fashion is that writing to a pipe can block meaning the pipe only has a limited buffering capacity. the maximum size of the buffer is system-dependent; typical values from 4kib up to 128kib though they can be changed.	block	76
the problem with using a pipe in this fashion is that writing to a pipe can block meaning the pipe only has a limited buffering capacity. the maximum size of the buffer is system-dependent; typical values from 4kib up to 128kib though they can be changed.	system	172
int main() { int fh[2]; pipe(fh); int b = 0; #define mesg "..............................." while(1) { printf("%d\n",b); write(fh[1], mesg, sizeof(mesg)) b+=sizeof(mesg); } return 0; }	printf	103
the parent sends the bytes h,i,(space),c...! into the pipe. the child starts reading the pipe one byte at a time. in the above case, the child process will read and print each character. however, it never leaves the while loop! when there are no characters left to read it simply blocks and waits for more unless all the writers close their ends another solution could also exit the loop by checking for an end-of-message marker,	block	280
the parent sends the bytes h,i,(space),c...! into the pipe. the child starts reading the pipe one byte at a time. in the above case, the child process will read and print each character. however, it never leaves the while loop! when there are no characters left to read it simply blocks and waits for more unless all the writers close their ends another solution could also exit the loop by checking for an end-of-message marker,	the loop	379
we know that when a process tries to read from a pipe where there are still writers, the process blocks. if no pipe has no writers, read returns 0. if a process tries to write with some reader’s read goes through, or fails – partially or completely – if the pipe is full. why happens when a process tries to write when there are no readers left?	a process	18
we know that when a process tries to read from a pipe where there are still writers, the process blocks. if no pipe has no writers, read returns 0. if a process tries to write with some reader’s read goes through, or fails – partially or completely – if the pipe is full. why happens when a process tries to write when there are no readers left?	block	97
the mistake in the above code is that there is still a reader for the pipe! the child still has the pipe’s first file descriptor open and remember the specification? all readers must be closed when forking, it is common practice to close the unnecessary (unused) end of each pipe in the child and parent process. for example, the parent might close the reading end and the child might close the writing end.	code	25
unnamed pipes live in memory and are a simple and efficient form of inter-process communication (ipc) that is useful for streaming data and simple messages. once all processes have closed, the pipe resources are freed.	resources	198
unnamed pipes live in memory and are a simple and efficient form of inter-process communication (ipc) that is useful for streaming data and simple messages. once all processes have closed, the pipe resources are freed.	memory	22
if the program already has a file descriptor, it can ‘wrap’ it yourself into a file pointer using fdopen.	pointer	84
#include <sys/types.h> #include <sys/stat.h> #include <fcntl.h>	type	14
int main() { char *name="fred"; int score = 123; int filedes = open("mydata.txt", "w", o_creat, s_iwusr | s_irusr); file *f = fdopen(filedes, "w"); fprintf(f, "name:%s score:%d\n", name, score); fclose(f);	printf	149
#include <unistd.h> #include <stdlib.h> #include <stdio.h> int main() { int fh[2]; pipe(fh); file *reader = fdopen(fh[0], "r"); file *writer = fdopen(fh[1], "w"); pid_t p = fork(); if (p > 0) { int score; fscanf(reader, "score %d", &score); printf("the child says the score is %d\n", score); } else { fprintf(writer, "score %d", 10 + 10); fflush(writer); } return 0; }	printf	241
change: fprintf(writer, "score %d", 10 + 10); to: fprintf(writer, "score %d\n", 10 + 10);	printf	9
if you want your bytes to be sent to the pipe immediately, you’ll need to fflush! remember back to the introduction section that shows the difference between terminal vs non-terminal outputs of stdout.	section	116
even though we have a section on it, it is highly not recommended to use the file descriptor api for non-seekable files. the reason being is that while we get conveniences we also get annoyances like the buffering example we mentioned, caching, etc. the basic c library motto is that any device a program can properly fseek or move to an arbitrary position, it should be able to fdopen. files satisfy this behavior, shared memory also, terminals, etc. when it comes to pipes, sockets, epoll objects, etc, don’t do it.	memory	423
even though we have a section on it, it is highly not recommended to use the file descriptor api for non-seekable files. the reason being is that while we get conveniences we also get annoyances like the buffering example we mentioned, caching, etc. the basic c library motto is that any device a program can properly fseek or move to an arbitrary position, it should be able to fdopen. files satisfy this behavior, shared memory also, terminals, etc. when it comes to pipes, sockets, epoll objects, etc, don’t do it.	section	22
an alternative to unnamed pipes is named pipes created using mkfifo. from the command line: mkfifo from c: int mkfifo(const char *pathname, mode_t mode); you give it the pathname and the operation mode, it will be ready to go! named pipes take up virtually no space on a file system. this means the actual contents of the pipe aren’t printed to the file and read from that same file. what the operating system tells you when you have a named pipe is that it will create an unnamed pipe that refers to the named pipe, and that’s it! there is no additional magic. this is for programming convenience if processes are started without forking meaning that there would be no way to get the file descriptor to the child process for an unnamed pipe.	system	276
1$ mkfifo fifo 1$ echo hello > fifo # this will hang until the following command is run on another terminal or another process 2$ cat fifo hello	the following	59
any open is called on a named pipe the kernel blocks until another process calls the opposite open. meaning, echo calls open(.., o_rdonly) but that blocks until cat calls open(.., o_wronly), then the programs are allowed to continue.	block	46
what is wrong with the following program?	the following	19
this may never print hello because of a race condition. since a program opened the pipe in the first process under both permissions, open won’t wait for a reader because the program told the operating system that it is a reader! sometimes it looks like it works because the execution of the code looks something like this.	code	291
this may never print hello because of a race condition. since a program opened the pipe in the first process under both permissions, open won’t wait for a reader because the program told the operating system that it is a reader! sometimes it looks like it works because the execution of the code looks something like this.	system	201
open(o_rdonly) (blocks indefinitely)	block	16
 close removes a file descriptor from a process’ file descriptors. this always succeeds for a valid file descriptor.	a process	38
the linux interface is powerful and expressive, but sometimes we need portability for example if we are writing for a macintosh or windows. this is where c’s abstraction comes into play. on different operating systems, c uses the low-level functions to create a wrapper around files used everywhere, meaning that c on linux uses the above calls.	system	210
 fgetc/fgets get a char or a string from a file  fscanf read a format string from the file  fwrite write some objects to a file  fprintf write a formatted string to a file  fclose close a file handle  fflush take any buffered changes and flush them to a file but programs don’t get the expressiveness that linux gives with system calls. a program can convert back and forth between them with int fileno(file* stream) and file* fdopen(int fd...). also, c files are buffered meaning that their contents may be written to the backing after the call returns. you can change that with c options.	string	29
 fgetc/fgets get a char or a string from a file  fscanf read a format string from the file  fwrite write some objects to a file  fprintf write a formatted string to a file  fclose close a file handle  fflush take any buffered changes and flush them to a file but programs don’t get the expressiveness that linux gives with system calls. a program can convert back and forth between them with int fileno(file* stream) and file* fdopen(int fd...). also, c files are buffered meaning that their contents may be written to the backing after the call returns. you can change that with c options.	system	323
 fgetc/fgets get a char or a string from a file  fscanf read a format string from the file  fwrite write some objects to a file  fprintf write a formatted string to a file  fclose close a file handle  fflush take any buffered changes and flush them to a file but programs don’t get the expressiveness that linux gives with system calls. a program can convert back and forth between them with int fileno(file* stream) and file* fdopen(int fd...). also, c files are buffered meaning that their contents may be written to the backing after the call returns. you can change that with c options.	printf	130
danger with portability you lose something important, the ability to tell an error. a program can fopen a file descriptor and get a file* object but it won’t be the same as a file meaning that certain calls will fail or act weirdly. the c api reduces this weirdness, but for example a program cannot fseek to a part of the file, or perform any operations with its buffering. the problem is the api won’t give a lot of warning because c needs to maintain compatibility with other operating systems. to keep things simple, use the c api of files when dealing with a file on disk, which will work fine. otherwise, be in for a rough ride for portability’s sake.	system	489
for files less than the size of a long, using fseek and ftell is a simple way to accomplish this. move to the end of the file and find out the current position.	ftell	56
fseek(f, 0, seek_end); long pos = ftell(f);	ftell	34
all future reads and writes in the parent or child processes will honor this position. note writing or reading from the file will change the current position. see the man pages for fseek and ftell for more information.	ftell	191
this only works on some architectures and compilers that quirk is that longs only need to be 4 bytes big meaning that the maximum size that ftell can return is a little under 2 gibibytes. nowadays, our files could be hundreds of gibibytes or even terabytes on a distributed file system. what should we do instead? use stat! we will cover stat in a later part but here is some code that will tell a program the size of the file	ftell	140
this only works on some architectures and compilers that quirk is that longs only need to be 4 bytes big meaning that the maximum size that ftell can return is a little under 2 gibibytes. nowadays, our files could be hundreds of gibibytes or even terabytes on a distributed file system. what should we do instead? use stat! we will cover stat in a later part but here is some code that will tell a program the size of the file	code	376
this only works on some architectures and compilers that quirk is that longs only need to be 4 bytes big meaning that the maximum size that ftell can return is a little under 2 gibibytes. nowadays, our files could be hundreds of gibibytes or even terabytes on a distributed file system. what should we do instead? use stat! we will cover stat in a later part but here is some code that will tell a program the size of the file	system	279
buf.st_size is of type off_t which is big enough for large files.	type	18
files are used almost all the time as a form of ipc. hadoop is a great example where processes will write to append-only tables and then other processes will read from those tables. we generally use files under a few cases.	hadoop	53
one case is if we want to save the intermediate results of an operation to a file for future use. another case is if putting it in memory would cause an out of memory error. on linux, file operations are generally pretty cheap, so most programmers use it for larger intermediate storage.	memory	131
mmap is used for two scenarios. one is a linear or near-linear read through of the file. meaning, a program reads the file front to back or back to front. the key is that the program doesn’t jump around too much.	mmap	0
jumping around too much causes thrashing and loses all the benefits of using mmap. the other usage of mmap is for direct memory inter-process communication. this means that a program can store structures in a piece of mmap’ed memory and share them between two processes. python and ruby use this mapping all the time to utilize copy on write semantics.	mmap	77
jumping around too much causes thrashing and loses all the benefits of using mmap. the other usage of mmap is for direct memory inter-process communication. this means that a program can store structures in a piece of mmap’ed memory and share them between two processes. python and ruby use this mapping all the time to utilize copy on write semantics.	memory	121
1. virtual memory 2. page table 3. mmu/tlb 4. address translation 5. page faults 6. frames/pages 7. single-level vs multi-level page table 8. calculating offsets for multi-level page table 9. pipes 10. pipe read write ends 11. writing to a zero reader pipe 12. reading from a zero writer pipe 13. named pipe and unnamed pipes 14. buffer size/atomicity 15. scheduling algorithms 16. measures of efficiency	address	46
1. virtual memory 2. page table 3. mmu/tlb 4. address translation 5. page faults 6. frames/pages 7. single-level vs multi-level page table 8. calculating offsets for multi-level page table 9. pipes 10. pipe read write ends 11. writing to a zero reader pipe 12. reading from a zero writer pipe 13. named pipe and unnamed pipes 14. buffer size/atomicity 15. scheduling algorithms 16. measures of efficiency	memory	11
1. what is virtual memory?	memory	19
2. what are the following and what is their purpose?	the following	12
(a) translation lookaside buffer (b) physical address (c) memory management unit. multilevel page table. frame number. page number and page offset.	address	46
(a) translation lookaside buffer (b) physical address (c) memory management unit. multilevel page table. frame number. page number and page offset.	memory	58
(d) the dirty bit (e) the nx bit 3. what is a page table? how about a physical frame? does a page always need to point to a physical frame?	a page	44
(d) the dirty bit (e) the nx bit 3. what is a page table? how about a physical frame? does a page always need to point to a physical frame?	a page table	44
4. what is a page fault? what are the types? when does it result in a segfault?	a page	11
4. what is a page fault? what are the types? when does it result in a segfault?	type	38
6. what does a multi-leveled table look like in memory?	memory	48
8. given a 64-bit address space, 4kb pages and frames, and a 3 level page table, how many bits are the virtual page number 1, vpn2, vpn3 and the offset?	address	18
10. when is sigpipe delivered to a process?	a process	33
11. under what conditions will calling read() on a pipe block? under what conditions will read() immediately return 0 12. what is the difference between a named pipe and an unnamed pipe?	block	56
13. is a pipe thread-safe?	thread	14
14. write a function that uses fseek and ftell to replace the middle character of a file with an ’x’ 15. write a function that creates a pipe and uses write to send 5 bytes, "hello" to the pipe. return the read file descriptor of the pipe.	ftell	41
16. what happens when you mmap a file?	mmap	26
17. why is getting the file size with ftell not recommended? how should you do it instead?	ftell	38
cpu scheduling is the problem of efficiently selecting which process to run on a system’s cpu cores. in a busy system, there will be more ready-to-run processes than there are cpu cores, so the system kernel must evaluate which processes should be scheduled to run and which processes should be executed later. the system must also decide whetherit should take a particular process and pause its execution – along with any associated threads.	thread	434
cpu scheduling is the problem of efficiently selecting which process to run on a system’s cpu cores. in a busy system, there will be more ready-to-run processes than there are cpu cores, so the system kernel must evaluate which processes should be scheduled to run and which processes should be executed later. the system must also decide whetherit should take a particular process and pause its execution – along with any associated threads.	system	81
the additional complexity of multi-threaded and multiple cpu cores are considered a distraction to this initial exposition so are ignored here. another gotcha for non-native speakers is the dual meaning of “time”: the word “time” can be used in both clock and elapsed duration context. for example “the arrival time of the first process was 9:00am.” and, “the running time of the algorithm is 3 seconds”.	thread	35
that means we will assume that the processes are in memory and ready to go. the other types of scheduling are long and medium term. long term schedulers act as gatekeepers to the processing world. when a process requests another process to be executed, it can either tell the process yes, no, or wait. the medium term scheduler deals with the caveats of moving a process from the paused state in memory to the paused state on disk when there are too many processes or some process are known to use an insignificant amount of cpu cycles. think about a process that only checks something once an hour.	a process	202
that means we will assume that the processes are in memory and ready to go. the other types of scheduling are long and medium term. long term schedulers act as gatekeepers to the processing world. when a process requests another process to be executed, it can either tell the process yes, no, or wait. the medium term scheduler deals with the caveats of moving a process from the paused state in memory to the paused state on disk when there are too many processes or some process are known to use an insignificant amount of cpu cycles. think about a process that only checks something once an hour.	memory	52
that means we will assume that the processes are in memory and ready to go. the other types of scheduling are long and medium term. long term schedulers act as gatekeepers to the processing world. when a process requests another process to be executed, it can either tell the process yes, no, or wait. the medium term scheduler deals with the caveats of moving a process from the paused state in memory to the paused state on disk when there are too many processes or some process are known to use an insignificant amount of cpu cycles. think about a process that only checks something once an hour.	type	86
10.1 high level scheduler overview schedulers are pieces of software programs. in fact, you can implement schedulers yourself! if you are given a list of commands to exec, a program can schedule them them with sigstop and sigcont. these are called user space schedulers. hadoop and python’s celery may do some sort of user space scheduling or deal with the operating system.	system	367
10.1 high level scheduler overview schedulers are pieces of software programs. in fact, you can implement schedulers yourself! if you are given a list of commands to exec, a program can schedule them them with sigstop and sigcont. these are called user space schedulers. hadoop and python’s celery may do some sort of user space scheduling or deal with the operating system.	hadoop	271
at the operating system level, you generally have this type of flowchart, described in words first below. note, please don’t memorize all the states.	system	17
at the operating system level, you generally have this type of flowchart, described in words first below. note, please don’t memorize all the states.	type	55
1. new is the initial state. a process has been requested to schedule. all process requests come from fork or clone. at this point the operating system knows it needs to create a new process.	a process	29
1. new is the initial state. a process has been requested to schedule. all process requests come from fork or clone. at this point the operating system knows it needs to create a new process.	system	145
2. a process moves from the new state to the ready. this means any structs in the kernel are allocated. from there, it can go into ready suspended or running.	a process	3
3. running is the state that we hope most of our processes are in, meaning they are doing useful work. a process could either get preempted, blocked, or terminate. preemption brings the process back to the ready state. if a process is blocked, that means it could be waiting on a mutex lock, or it could’ve called sleep – either way, it willingly gave up control.	a mutex	278
3. running is the state that we hope most of our processes are in, meaning they are doing useful work. a process could either get preempted, blocked, or terminate. preemption brings the process back to the ready state. if a process is blocked, that means it could be waiting on a mutex lock, or it could’ve called sleep – either way, it willingly gave up control.	a process	103
3. running is the state that we hope most of our processes are in, meaning they are doing useful work. a process could either get preempted, blocked, or terminate. preemption brings the process back to the ready state. if a process is blocked, that means it could be waiting on a mutex lock, or it could’ve called sleep – either way, it willingly gave up control.	block	141
4. on the blocked state the operating system can either turn the process ready or it can go into a deeper state called blocked suspended.	block	10
4. on the blocked state the operating system can either turn the process ready or it can go into a deeper state called blocked suspended.	system	38
5. there are so-called deep slumber states called blocked suspended and blocked ready. you don’t need to worry about these.	block	50
we will try to pick a scheme that decides when a process should move to the running state, and when it should be moved back to the ready state. we won’t make much mention of how to factor in voluntarily blocked states and when to switch to deep slumber states.	a process	47
we will try to pick a scheme that decides when a process should move to the running state, and when it should be moved back to the ready state. we won’t make much mention of how to factor in voluntarily blocked states and when to switch to deep slumber states.	block	203
10.2 measurements scheduling affects the performance of the system, specifically the latency and throughput of the system. the throughput might be measured by a system value, for example, the i/o throughput - the number of bits written per second, or the number of small processes that can complete per unit time. the latency might be measured by the response time – elapse time before a process can start to send a response – or wait time or turnaround time –the elapsed time to complete a task. different schedulers offer different optimization trade-offs that may be appropriate for desired use. there is no optimal scheduler for all possible environments and goals. for example, shortest job first will minimize total wait time across all jobs but in interactive (ui) environments it would be preferable to minimize response time at the expense of some throughput, while fcfs seems intuitively fair and easy to implement but suffers from the convoy effect. arrival time is the time at which a process first arrives at the ready queue, and is ready to start executing. if a cpu is idle, the arrival time would also be the starting time of execution.	a process	386
10.2 measurements scheduling affects the performance of the system, specifically the latency and throughput of the system. the throughput might be measured by a system value, for example, the i/o throughput - the number of bits written per second, or the number of small processes that can complete per unit time. the latency might be measured by the response time – elapse time before a process can start to send a response – or wait time or turnaround time –the elapsed time to complete a task. different schedulers offer different optimization trade-offs that may be appropriate for desired use. there is no optimal scheduler for all possible environments and goals. for example, shortest job first will minimize total wait time across all jobs but in interactive (ui) environments it would be preferable to minimize response time at the expense of some throughput, while fcfs seems intuitively fair and easy to implement but suffers from the convoy effect. arrival time is the time at which a process first arrives at the ready queue, and is ready to start executing. if a cpu is idle, the arrival time would also be the starting time of execution.	system	60
10.2 measurements scheduling affects the performance of the system, specifically the latency and throughput of the system. the throughput might be measured by a system value, for example, the i/o throughput - the number of bits written per second, or the number of small processes that can complete per unit time. the latency might be measured by the response time – elapse time before a process can start to send a response – or wait time or turnaround time –the elapsed time to complete a task. different schedulers offer different optimization trade-offs that may be appropriate for desired use. there is no optimal scheduler for all possible environments and goals. for example, shortest job first will minimize total wait time across all jobs but in interactive (ui) environments it would be preferable to minimize response time at the expense of some throughput, while fcfs seems intuitively fair and easy to implement but suffers from the convoy effect. arrival time is the time at which a process first arrives at the ready queue, and is ready to start executing. if a cpu is idle, the arrival time would also be the starting time of execution.	fcfs	875
without preemption, processes will run until they are unable to utilize the cpu any further. for example the following conditions would remove a process from the cpu and the cpu would be available to be scheduled for other processes. the process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally. thus once a process is scheduled it will continue even if another process with a high priority appears on the ready queue.	the following	105
without preemption, processes will run until they are unable to utilize the cpu any further. for example the following conditions would remove a process from the cpu and the cpu would be available to be scheduled for other processes. the process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally. thus once a process is scheduled it will continue even if another process with a high priority appears on the ready queue.	a process	143
without preemption, processes will run until they are unable to utilize the cpu any further. for example the following conditions would remove a process from the cpu and the cpu would be available to be scheduled for other processes. the process terminates due to a signal, is blocked waiting for concurrency primitive, or exits normally. thus once a process is scheduled it will continue even if another process with a high priority appears on the ready queue.	block	277
any scheduler that doesn’t use some form of preemption can result in starvation because earlier processes may never be scheduled to run (assigned a cpu). for example with sjf, longer jobs may never be scheduled if the system continues to have many short jobs to schedule. it all depends on the type of scheduler.	system	218
any scheduler that doesn’t use some form of preemption can result in starvation because earlier processes may never be scheduled to run (assigned a cpu). for example with sjf, longer jobs may never be scheduled if the system continues to have many short jobs to schedule. it all depends on the type of scheduler.	type	294
10.2.2 why might a process (or thread) be placed on the ready queue?	thread	31
10.2.2 why might a process (or thread) be placed on the ready queue?	a process	17
a process is placed on the ready queue when it can use a cpu. some examples include:  a process was blocked waiting for a read from storage or socket to complete and data is now available.	a process	0
a process is placed on the ready queue when it can use a cpu. some examples include:  a process was blocked waiting for a read from storage or socket to complete and data is now available.	block	100
 a process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.	thread	11
 a process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.	a process	1
 a process thread was blocked on a synchronization primitive (condition variable, semaphore, mutex lock) but is now able to continue.	block	22
 a process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.	a process	1
 a process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.	block	14
 a process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.	system	36
 a process is blocked waiting for a system call to complete but a signal has been delivered and the signal handler needs to run.	a system call	34
3. wait time is the total wait time or the total time that a process is on the ready queue. a common mistake is to believe it is only the initial waiting time in the ready queue. if a cpu intensive process with no i/o takes 7 minutes of cpu time to complete but required 9 minutes of wall-clock time to complete we can conclude that it was placed on the ready-queue for 2 minutes. for those 2 minutes, the process was ready to run but had no cpu assigned. it does not matter when the job was waiting, the wait time is 2 minutes.	a process	59
10.3.1 convoy effect the convoy effect is when a process takes up a lot of the cpu time, leaving all other processes with potentially smaller resource needs following like a convoy behind them.	a process	47
suppose the cpu is currently assigned to a cpu intensive task and there is a set of i/o intensive processes that are in the ready queue. these processes require a tiny amount of cpu time but they are unable to proceed because they are waiting for the cpu-intensive task to be removed from the processor. these processes are starved until the cpu bound process releases the cpu. but, the cpu will rarely be released. for example, in the case of an fcfs scheduler, we must wait until the process is blocked due to an i/o request. the i/o intensive process can now finally satisfy their cpu needs, which they can do quickly because their cpu needs are small and the cpu is assigned back to the cpu-intensive process again. thus the i/o performance of the whole system suffers through an indirect effect of starvation of cpu needs of all processes.	block	497
suppose the cpu is currently assigned to a cpu intensive task and there is a set of i/o intensive processes that are in the ready queue. these processes require a tiny amount of cpu time but they are unable to proceed because they are waiting for the cpu-intensive task to be removed from the processor. these processes are starved until the cpu bound process releases the cpu. but, the cpu will rarely be released. for example, in the case of an fcfs scheduler, we must wait until the process is blocked due to an i/o request. the i/o intensive process can now finally satisfy their cpu needs, which they can do quickly because their cpu needs are small and the cpu is assigned back to the cpu-intensive process again. thus the i/o performance of the whole system suffers through an indirect effect of starvation of cpu needs of all processes.	system	758
suppose the cpu is currently assigned to a cpu intensive task and there is a set of i/o intensive processes that are in the ready queue. these processes require a tiny amount of cpu time but they are unable to proceed because they are waiting for the cpu-intensive task to be removed from the processor. these processes are starved until the cpu bound process releases the cpu. but, the cpu will rarely be released. for example, in the case of an fcfs scheduler, we must wait until the process is blocked due to an i/o request. the i/o intensive process can now finally satisfy their cpu needs, which they can do quickly because their cpu needs are small and the cpu is assigned back to the cpu-intensive process again. thus the i/o performance of the whole system suffers through an indirect effect of starvation of cpu needs of all processes.	fcfs	447
this effect is usually discussed in the context of fcfs scheduler; however, a round robin scheduler can also exhibit the convoy effect for long time-quanta.	fcfs	51
advantages 1. shorter jobs tend to get run first 2. on average wait times and response times are down disadvantages 1. needs algorithm to be omniscient 2. need to estimate the burstiness of a process which is harder than let’s say a computer network	a process	190
10.4.3 first come first served (fcfs)	fcfs	32
figure 10.3: first come first serve scheduling  p2 at 0ms  p1 at 1000ms  p5 at 3000ms  p4 at 4000ms  p3 at 5000ms processes are scheduled in the order of arrival. one advantage of fcfs is that scheduling algorithm is simple the ready queue is a fifo (first in first out) queue. fcfs suffers from the convoy effect. here p2 arrives, then p1 arrives, then p5, then p4, then p3. you can see the convoy effect for p5.	fcfs	180
10.4.4 round robin (rr) processes are scheduled in order of their arrival in the ready queue. after a small time step though, a running process will be forcibly removed from the running state and placed back on the ready queue. this ensures long-running processes refrain from starving all other processes from running. the maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. as the time quanta approaches to infinity, round robin will be equivalent to fcfs.	a process	352
10.4.4 round robin (rr) processes are scheduled in order of their arrival in the ready queue. after a small time step though, a running process will be forcibly removed from the running state and placed back on the ready queue. this ensures long-running processes refrain from starving all other processes from running. the maximum amount of time that a process can execute before being returned to the ready queue is called the time quanta. as the time quanta approaches to infinity, round robin will be equivalent to fcfs.	fcfs	519
if you need a math-y way of comparing scheduling algorithms, please check out the appendix and the section conceptually scheduling	section	99
bibliography [1] a. silberschatz, p.b. galvin, and g. gagne. operating system concepts. wiley, 2005. isbn 9780471694663.	system	71
that means that when we refer to the si prefixes of kilo-, mega-, etc, then we are always referring to a power of 10. a kilobyte is one thousand bytes, a megabyte is a thousand kilobytes and so on. if we need to refer to 1024 bytes, we will use the more accurate term kibibyte. mibibyte and gibibyte are the analogs of megabyte and gigabyte respectively. we make this distinction to make sure that we aren’t off by 24. the reasons for this misnomer will be explained in the filesystems chapter.	a megabyte	152
that means that when we refer to the si prefixes of kilo-, mega-, etc, then we are always referring to a power of 10. a kilobyte is one thousand bytes, a megabyte is a thousand kilobytes and so on. if we need to refer to 1024 bytes, we will use the more accurate term kibibyte. mibibyte and gibibyte are the analogs of megabyte and gigabyte respectively. we make this distinction to make sure that we aren’t off by 24. the reasons for this misnomer will be explained in the filesystems chapter.	system	478
6. layer 6: the presentation layer. this layer deals with encryption, compression, and data translation. for example, portability between different operating systems like translating newlines to windows newlines.	system	158
this book won’t cover networking in depth. we will focus on some aspects of layers 3, 4, and 7 because they are essential to know if you are going to be doing something with the internet, which at some point in your career you will be. as for another definition, a protocol is a set of specifications put forward by the internet engineering task force that govern how implementers of a protocol have their program or circuit behave under specific circumstances.	a protocol	263
11.2 layer 3: the internet protocol the following is a short introduction to internet protocol (ip), the primary way to send datagrams of information from one machine to another. “ip4”, or more precisely, ipv4 is version 4 of the internet protocol that describes how to send packets of information across a network from one machine to another. even as of 2018, ipv4 still dominates internet traffic, but google reports that 24 countries now supply 15% of their traffic through ipv6 [2].	the following	36
a significant limitation of ipv4 is that source and destination addresses are limited to 32 bits. ipv4 was designed at a time when the idea of 4 billion devices connected to the same network was unthinkable or at least not worth making the packet size larger. ipv4 addresses are written typically in a sequence of four octets delimited by periods "255.255.255.0" for example.	address	64
each ipv4 datagram includes a small header - typically 20 octets, that includes a source and destination address. conceptually the source and destination addresses can be split into two: a network number the upper bits and lower bits represent a particular host number on that network.	address	105
a newer packet protocol ipv6 solves many of the limitations of ipv4 like making routing tables simpler and 128-bit addresses. however, little web traffic is ipv6 based on comparison as of 2018 [2] we write ipv6 addresses in a sequence of eight, four hexadecimal delimiters like "1f45:0000:0000:0000:0000:0000:0000:0000". since that can get unruly, we can omit the zeros "1f45::". a machine can have an ipv6 address and an ipv4 address.	address	115
there are special ip addresses. one such in ipv4 is 127.0.0.1, ipv6 as 0:0:0:0:0:0:0:1 or ::1 also known as localhost. packets sent to 127.0.0.1 will never leave the machine; the address is specified to be the same machine. there are a lot of others that are denoted by certain octets being zeros or 255, the maximum value. you won’t need to know all the terminology, keep in mind that the actual number of ip addresses that a machine can have globally over the internet is smaller than the number of “raw” addresses. this book covers how ip deals with routing, fragmenting, and reassembling upper-level protocols. a more in-depth aside follows.	address	21
source address	address	7
destination address	address	12
figure 11.1: ipv6 datagram divisibility one of the big features of ipv6 is the address space. the world ran out of ip addresses a while ago and has been using hacks to get around that. with ipv6 there are enough internal and external addresses so even if we discover alien civilizations, we probably won’t run out. the other benefit is that these addresses are leased not bought, meaning that if something drastic happens in let’s say the internet of things and there needs to be a change in the block addressing scheme, it can be done.	address	79
figure 11.1: ipv6 datagram divisibility one of the big features of ipv6 is the address space. the world ran out of ip addresses a while ago and has been using hacks to get around that. with ipv6 there are enough internal and external addresses so even if we discover alien civilizations, we probably won’t run out. the other benefit is that these addresses are leased not bought, meaning that if something drastic happens in let’s say the internet of things and there needs to be a change in the block addressing scheme, it can be done.	block	496
another big feature is security through ipsec. ipv4 was designed with little to no security in mind. as such, now there is a key exchange similar to tls in higher layers that allows you to encrypt communication.	exchange	129
another feature is simplified processing. to make the internet fast, ipv4 and ipv6 headers are verified in hardware. that means that all header options are processed in circuits as they come in. the problem is that as the ipv4 spec grew to include a copious amount of headers, the hardware had to become more and more advanced to support those headers. ipv6 reorders the headers so that packets can be dropped and routed with fewer hardware cycles. in the case of the internet, every cycle matters when trying to route the world’s traffic.	the header	367
11.2.2 what’s my address?	address	17
to obtain a linked list of ip addresses of the current machine use getifaddrs which will return a linked list of ipv4 and ipv6 ip addresses among other interfaces as well. we can examine each entry and use getnameinfo to print the host’s ip address. the ifaddrs struct includes the family but does not include the sizeof the struct.	address	30
the complete code is shown below.	code	13
to get your ip address from the command line use ifconfig or windows’ ipconfig.	address	15
to grab the ip address of a remote website, the function getaddrinfo can convert a human-readable domain name (e.g. www.illinois.edu) into an ipv4 and ipv6 address. it will return a linked-list of addrinfo structs:	address	15
struct addrinfo { int ai_flags; int ai_family; int ai_socktype; int ai_protocol; socklen_t ai_addrlen; struct sockaddr *ai_addr; char *ai_canonname; struct addrinfo *ai_next; };	type	58
for example, suppose you wanted to find out the numeric ipv4 address of a web server at www.bbc.com. we do this in two stages. first, use getaddrinfo to build a linked-list of possible connections. secondly, use getnameinfo to convert the binary address of one of those into a readable form.	address	61
<stdio.h> <stdlib.h> <sys/types.h> <sys/socket.h> <netdb.h>	type	26
one can specify ipv4 or ipv6 with af_unspec. just replace the ai_family attribute in the above code with the following.	the following	105
one can specify ipv4 or ipv6 with af_unspec. just replace the ai_family attribute in the above code with the following.	code	95
if you are wondering how the computer maps hostnames to addresses, we will talk about that in layer 7.	address	56
spoiler: it is a service called dns. before we move onto the next section, it is important to note that a single website can have multiple ip addresses. this may be to be efficient with machines. if google or facebook has a single server routing all of their incoming requests to other computers, they’d have to spend massive amounts of money on that computer or data center. instead, they can give different regions different ip addresses and have a computer pick. it isn’t bad to access a website through the non-preferred ip address. the page may load slower.	address	142
spoiler: it is a service called dns. before we move onto the next section, it is important to note that a single website can have multiple ip addresses. this may be to be efficient with machines. if google or facebook has a single server routing all of their incoming requests to other computers, they’d have to spend massive amounts of money on that computer or data center. instead, they can give different regions different ip addresses and have a computer pick. it isn’t bad to access a website through the non-preferred ip address. the page may load slower.	section	66
window urgent pointer padding	pointer	14
figure 11.2: extra: tcp header specification most services on the internet today use tcp because it efficiently hides the complexity of the lower, packet-level nature of the internet. tcp or transport control protocol is a connection-based protocol that is built on top of ipv4 and ipv6 and therefore can be described as “tcp/ip” or “tcp over ip”. tcp creates a pipe between two machines and abstracts away the low-level packet-nature of the internet. thus, under most conditions, bytes sent over a tcp connection delivered and uncorrupted. high performance and error-prone code won’t even assume that!	code	574
figure 11.2: extra: tcp header specification most services on the internet today use tcp because it efficiently hides the complexity of the lower, packet-level nature of the internet. tcp or transport control protocol is a connection-based protocol that is built on top of ipv4 and ipv6 and therefore can be described as “tcp/ip” or “tcp over ip”. tcp creates a pipe between two machines and abstracts away the low-level packet-nature of the internet. thus, under most conditions, bytes sent over a tcp connection delivered and uncorrupted. high performance and error-prone code won’t even assume that!	transport control protocol	191
3. out of order packets. packets may get routed more favorably due to various reasons in ip. if a later packet arrives before another packet, the protocol should detect and reorder them.	the protocol	142
4. duplicate packets. packets can arrive twice. packets can arrive twice. as such, a protocol needs to be able to differentiate between two packets given a sequence number subject to overflow.	a protocol	83
7. congestion control. congestion control is performed on the sender’s side. congestion control is to avoid a sender from flooding the network with too many packets. this is important to make sure that each tcp connection is treated fairly. meaning that two connections leaving a computer to google and youtube receive the same bandwidth and ping as each other. one can easily define a protocol that takes all the bandwidth and leaves other protocols in the dust, but this tends to be malicious because many times limiting a computer to a single tcp connection will yield the same result.	a protocol	384
1. security. connecting to an ip address claiming to be a certain website does not verify the claim (like in tls). you could be sending packets to a malicious computer.	address	33
4. delimiting requests. tcp is naturally connection-oriented. applications that are communicating over tcp need to find a unique way of telling each other that this request or response is over. http delimits the header through two carriage returns and uses either a length field or one keeps listening until the connection closes	the header	208
on your machine! it depends on the actual architecture of the host running the code. if the architecture happens to be the same as network ordering then the functions return identical integers. for x86 machines, the host and network order are different.	code	79
unless agreed otherwise, whenever you read or write the low-level c network structures, i.e. port and address information, remember to use the above functions to ensure correct conversion to/from a machine format.	address	102
this doesn’t apply to protocols that negotiate the endianness before-hand. if two computers are cpu bound by converting the messages between network orders – this happens with rpcs in high-performance systems – it may be worth it to negotiate if they are on similar endianness to send in little-endian order.	system	201
why is network order defined to be big-endian? the simple answer is that rfc1700 says so [5]. if you want more information, we’ll cite the famous article located that argued for a particular version [3]. the most important part is that it is standard. what happens when we don’t have one standard? we have 4 different usb plug types (regular, micro, mini, and usb-c) that don’t interact well with each other. include relevant xkcd here standards.	type	327
11.3.2 tcp client there are three basic system calls to connect to a remote machine.	system	40
1. int getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, struct addri the getaddrinfo call if successful, creates a linked-list of addrinfo structs and sets the given pointer to point to the first one.	pointer	198
also, you can use the hints struct to only grab certain entries like certain ip protocols, etc. the addrinfo structure that is passed into getaddrinfo to define the kind of connection you’d like. for example, to specify stream-based protocols over ipv6, you can use the following snippet.	the following	266
also, you can use the hints struct to only grab certain entries like certain ip protocols, etc. the addrinfo structure that is passed into getaddrinfo to define the kind of connection you’d like. for example, to specify stream-based protocols over ipv6, you can use the following snippet.	snippet	280
error handling with getaddrinfo is a little different. the return value is the error code. to convert to a human-readable error use gai_strerror to get the equivalent short english error text.	code	85
2. int socket(int domain, int socket_type, int protocol); the socket call creates a network socket and returns a descriptor that can be used with read and write. in this sense, it is the network analog of open that opens a file stream – except that we haven’t connected the socket to anything yet!	type	37
sockets are created with a domain af_inet for ipv4 or af_inet6 for ipv6, socket_type is whether to use udp, tcp, or other some other socket type, the protocol is an optional choice of protocol configuration for our examples this we can leave this as 0 for default. this call creates a socket object in the kernel with which one can communicate with the outside world/network. you can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.	address	405
sockets are created with a domain af_inet for ipv4 or af_inet6 for ipv6, socket_type is whether to use udp, tcp, or other some other socket type, the protocol is an optional choice of protocol configuration for our examples this we can leave this as 0 for default. this call creates a socket object in the kernel with which one can communicate with the outside world/network. you can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.	the protocol	146
sockets are created with a domain af_inet for ipv4 or af_inet6 for ipv6, socket_type is whether to use udp, tcp, or other some other socket type, the protocol is an optional choice of protocol configuration for our examples this we can leave this as 0 for default. this call creates a socket object in the kernel with which one can communicate with the outside world/network. you can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.	parameter	439
sockets are created with a domain af_inet for ipv4 or af_inet6 for ipv6, socket_type is whether to use udp, tcp, or other some other socket type, the protocol is an optional choice of protocol configuration for our examples this we can leave this as 0 for default. this call creates a socket object in the kernel with which one can communicate with the outside world/network. you can use the result of getaddressinfo to fill in the socket parameters, or provide them manually.	type	80
tcp sockets are similar to pipes and are often used in situations that require ipc. we don’t mention it in the previous chapters because it is overkill using a device suited for networks to simply communicate between processes on a single thread.	thread	239
3. connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); finally, the connect call attempts the connection to the remote machine. we pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. there are different kinds of socket address structures that can require more memory. so in addition to passing the pointer, the size of the structure is also passed. to help identify errors and mistakes it is good practice to check the return value of all networking calls, including connect	address	204
3. connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); finally, the connect call attempts the connection to the remote machine. we pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. there are different kinds of socket address structures that can require more memory. so in addition to passing the pointer, the size of the structure is also passed. to help identify errors and mistakes it is good practice to check the return value of all networking calls, including connect	memory	348
3. connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen); finally, the connect call attempts the connection to the remote machine. we pass the original socket descriptor and also the socket address information which is stored inside the addrinfo structure. there are different kinds of socket address structures that can require more memory. so in addition to passing the pointer, the size of the structure is also passed. to help identify errors and mistakes it is good practice to check the return value of all networking calls, including connect	pointer	386
4. (optional) to clean up code call freeaddrinfo(struct addrinfo *ai) on the first level addrinfo struct.	code	26
there is an old function gethostbyname is deprecated. it’s the old way convert a hostname into an ip address.	address	101
the port address still needs to be manually set using htons function. it’s much easier to write code to support ipv4 and ipv6 using the newer getaddrinfo this is all that is needed to create a simple tcp client. however, network communications offer many different levels of abstraction and several attributes and options that can be set at each level. for example, we haven’t talked about setsockopt which can manipulate options for the socket. you can also mess around with lower protocols as the kernel provides primitives that contribute to this. note that you need to be root to create a raw socket. also, you need to have a lot of “set up” or starter code, be prepared to have your datagrams be dropped due to bad form as well. for more information see this guide.	code	96
the port address still needs to be manually set using htons function. it’s much easier to write code to support ipv4 and ipv6 using the newer getaddrinfo this is all that is needed to create a simple tcp client. however, network communications offer many different levels of abstraction and several attributes and options that can be set at each level. for example, we haven’t talked about setsockopt which can manipulate options for the socket. you can also mess around with lower protocols as the kernel provides primitives that contribute to this. note that you need to be root to create a raw socket. also, you need to have a lot of “set up” or starter code, be prepared to have your datagrams be dropped due to bad form as well. for more information see this guide.	address	9
a simple http client that sends a request to a compliant url is below. first, we’ll start with the boring stuff and the parsing code.	code	128
the code that sends the request is below. the first thing that we have to do is connect to an address.	code	4
the code that sends the request is below. the first thing that we have to do is connect to an address.	address	94
struct addrinfo current, *result; memset(&current, 0, sizeof(struct addrinfo)); current.ai_family = af_inet; current.ai_socktype = sock_stream; getaddrinfo(info->hostname, info->port, &current, &result); connect(sock_fd, result->ai_addr, result->ai_addrlen) freeaddrinfo(result);	type	124
the next piece of code sends the request. here is what each header means.	code	18
a more robust piece of code would also check if the write fails or if the call was interrupted.	code	23
char *buffer; asprintf(&buffer, "get %s http/1.0\r\n" "connection: close\r\n" "accept: */*\r\n\r\n", info->resource); write(sock_fd, buffer, strlen(buffer)); free(buffer);	printf	16
the last piece of code is the driver code that sends the request. feel free to use the following code if you want to open the file descriptor as a file object for convenience functions. just be careful not to forget to set the buffering to zero otherwise you may double buffer the input, which would lead to performance problems.	the following	83
the last piece of code is the driver code that sends the request. feel free to use the following code if you want to open the file descriptor as a file object for convenience functions. just be careful not to forget to set the buffering to zero otherwise you may double buffer the input, which would lead to performance problems.	code	18
2. the resource. “/” “/index.html” “/image.png” 3. the protocol “http/1.0” 4. a new line (rn). requests always have a carriage return.	the protocol	51
5. any other knobs or switch parameters 6. the actual body of the request delimited by two new lines. the body of the request is either if the size is specified or until the receiver closes their connection.	parameter	29
the server’s first response line describes the http version used and whether the request is successful using a 3 digit response code.	code	128
if the client had requested a non-existent path, e.g. get /nosuchfile.html http/1.0 then the first line includes the response code is the well-known 404 response code.	code	126
11.4 layer 4: tcp server the four system calls required to create a minimal tcp server are socket, bind, listen, and accept. each has a specific purpose and should be called in roughly the above order 1. int socket(int domain, int socket_type, int protocol) to create an endpoint for networking communication. a new socket by itself is stores bytes. though we’ve specified either a packet or stream-based connections, it is unbound to a particular network interface or port. instead, socket returns a network descriptor that can be used with later calls to bind, listen and accept.	system	34
11.4 layer 4: tcp server the four system calls required to create a minimal tcp server are socket, bind, listen, and accept. each has a specific purpose and should be called in roughly the above order 1. int socket(int domain, int socket_type, int protocol) to create an endpoint for networking communication. a new socket by itself is stores bytes. though we’ve specified either a packet or stream-based connections, it is unbound to a particular network interface or port. instead, socket returns a network descriptor that can be used with later calls to bind, listen and accept.	type	238
since a tcp connection is defined by the sender address and port along with a receiver address and port, a particular server port there can be one passive server socket but multiple active sockets. one for each currently open connection. the server’s operating system maintains a lookup table that associates a unique tuple with active sockets so that incoming packets can be correctly routed to the correct socket.	address	48
since a tcp connection is defined by the sender address and port along with a receiver address and port, a particular server port there can be one passive server socket but multiple active sockets. one for each currently open connection. the server’s operating system maintains a lookup table that associates a unique tuple with active sockets so that incoming packets can be correctly routed to the correct socket.	system	261
2. int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen); the bind call associates an abstract socket with an actual network interface and port. it is possible to call bind on a tcp client. the port information used by bind can be set manually (many older ipv4-only c code examples do this), or be created using getaddrinfo.	code	283
by default, a port is released after some time when the server socket is closed. instead, the port enters a “timed-wait” state. this can lead to significant confusion during development because the timeout can make valid networking code appear to fail.	code	232
4. int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen); once the server socket has been initialized the server calls accept to wait for new connections. unlike socket bind and listen, this call will block, unless the nonblocking option has been set. if there are no new connections, this call will block and only return when a new client connects. the returned tcp socket is associated with a particular tuple (client ip, client port, server ip, server port) and will be used for all future incoming and outgoing tcp packets that match this tuple.	block	213
note the accept call returns a new file descriptor. this file descriptor is specific to a particular client. it is a common programming mistake to use the original server socket descriptor for the server i/o and then wonder why networking code has failed.	code	239
the accept system call can optionally provide information about the remote client, by passing in a sockaddr struct. different protocols have different variants of the struct sockaddr, which are different sizes. the simplest struct to use is the sockaddr_storage which is sufficiently large to represent all possible types of sockaddr. notice that c does not have any model of inheritance. therefore we need to explicitly cast our struct to the ‘base type’ struct sockaddr.	system	11
the accept system call can optionally provide information about the remote client, by passing in a sockaddr struct. different protocols have different variants of the struct sockaddr, which are different sizes. the simplest struct to use is the sockaddr_storage which is sufficiently large to represent all possible types of sockaddr. notice that c does not have any model of inheritance. therefore we need to explicitly cast our struct to the ‘base type’ struct sockaddr.	type	316
we’ve already seen getaddrinfo that can build a linked list of addrinfo entries and each one of these can include socket configuration data. what if we wanted to turn socket data into ip and port addresses?	address	196
enter getnameinfo that can be used to convert local or remote socket information into a domain name or numeric ip. similarly, the port number can be represented as a service name. for example, port 80 is commonly used as the incoming connection port for incoming http requests. in the example below, we request numeric versions for the client ip address and client port number.	address	346
one can use the macros ni_maxhost to denote the maximum length of a hostname, and ni_maxserv to denote the maximum length of a port. ni_numerichost gets the hostname as a numeric ip address and similarly for ni_numericserv although the port is usually numeric, to begin with. the open bsd man pages have more information 5. int close(int fd) and int shutdown(int fd, int how) use the shutdown call when you no longer need to read any more data from the socket, write more data, or have finished doing both. when you call shutdown on socket on the read and/or write ends, that information is also sent to the other end of the connection. if you shut down the socket for further writing at the server end, then a moment later, a blocked read call could return 0 to indicate that no more bytes are expected. similarly, a write to a tcp connection that has been shut down for reading will generate a sigpipe use close when your process no longer needs the socket file descriptor.	address	182
one can use the macros ni_maxhost to denote the maximum length of a hostname, and ni_maxserv to denote the maximum length of a port. ni_numerichost gets the hostname as a numeric ip address and similarly for ni_numericserv although the port is usually numeric, to begin with. the open bsd man pages have more information 5. int close(int fd) and int shutdown(int fd, int how) use the shutdown call when you no longer need to read any more data from the socket, write more data, or have finished doing both. when you call shutdown on socket on the read and/or write ends, that information is also sent to the other end of the connection. if you shut down the socket for further writing at the server end, then a moment later, a blocked read call could return 0 to indicate that no more bytes are expected. similarly, a write to a tcp connection that has been shut down for reading will generate a sigpipe use close when your process no longer needs the socket file descriptor.	block	727
if you fork-ed after creating a socket file descriptor, all processes need to close the socket before the socket resources can be reused. if you shut down a socket for further read, all processes are affected because you’ve changed the socket, not the file descriptor. well written code will shutdown a socket before calling close it.	code	282
if you fork-ed after creating a socket file descriptor, all processes need to close the socket before the socket resources can be reused. if you shut down a socket for further read, all processes are affected because you’ve changed the socket, not the file descriptor. well written code will shutdown a socket before calling close it.	resources	113
 not initializing the unused struct entries  the bind call will fail if the port is currently in use. ports are per machine – not per process or user. in other words, you cannot use port 1234 while another process is using that port. worse, ports are by default ‘tied up’ after a process has finished.	a process	278
11.4.1 example server a working simple server example is shown below. note: this example is incomplete. for example, the socket file descriptor remains open and memory created by getaddrinfo remains allocated. first, we get the address info for our current machine.	address	228
11.4.1 example server a working simple server example is shown below. note: this example is incomplete. for example, the socket file descriptor remains open and memory created by getaddrinfo remains allocated. first, we get the address info for our current machine.	memory	161
struct addrinfo hints, *result; memset(&hints, 0, sizeof(struct addrinfo)); hints.ai_family = af_inet; hints.ai_socktype = sock_stream; hints.ai_flags = ai_passive; int s = getaddrinfo(null, "1234", &hints, &result); if (s != 0) { fprintf(stderr, "getaddrinfo: %s\n", gai_strerror(s)); exit(1); }	type	116
struct addrinfo hints, *result; memset(&hints, 0, sizeof(struct addrinfo)); hints.ai_family = af_inet; hints.ai_socktype = sock_stream; hints.ai_flags = ai_passive; int s = getaddrinfo(null, "1234", &hints, &result); if (s != 0) { fprintf(stderr, "getaddrinfo: %s\n", gai_strerror(s)); exit(1); }	printf	232
printf("connection made: client_fd=%d\n", client_fd);	printf	0
11.4.2 sorry to interrupt one concept that we need to make clear is that you need to handle interrupts in your networking code. that means that the sockets or accepted file descriptors that you read to or write to may have their calls interrupted – most of the time you will get an interrupt or two. in reality, any of your system calls could get interrupted. the reason we bring this up now is that you are usually waiting for the network. which is an order of magnitude slower than processes. meaning a higher probability of getting interrupted.	code	122
11.4.2 sorry to interrupt one concept that we need to make clear is that you need to handle interrupts in your networking code. that means that the sockets or accepted file descriptors that you read to or write to may have their calls interrupted – most of the time you will get an interrupt or two. in reality, any of your system calls could get interrupted. the reason we bring this up now is that you are usually waiting for the network. which is an order of magnitude slower than processes. meaning a higher probability of getting interrupted.	system	324
we can assure you that the following code experience errors. can you see why? on the surface, it does restart a call after a read or write. but what else happens when the error is eintr? are the contents of the buffer correct?	the following	23
we can assure you that the following code experience errors. can you see why? on the surface, it does restart a call after a read or write. but what else happens when the error is eintr? are the contents of the buffer correct?	code	37
11.5 layer 4: udp udp is a connectionless protocol that is built on top of ipv4 and ipv6. it’s simple to use. decide the destination address and port and send your data packet! however, the network makes no guarantee about whether the packets will arrive. packets may be dropped if the network is congested. packets may be duplicated or arrive out of order.	address	133
 simple the udp protocol is supposed to have much less fluff than tcp. meaning that for tcp there are a lot of configurable parameters and a lot of edge cases in the implementation. udp is fire and forget.	parameter	124
 stateless/transaction the udp protocol is stateless. this makes the protocol more simple and lets the protocol represent simple transactions like requesting or responding to queries. there is also less overhead to sending a udp message because there is no three-way handshake.	the protocol	65
the previous code grabs an entry hostent that matches by hostname. even though this isn’t portable, it gets the job done. first is to connect to it and make it reusable – the same as a tcp socket. note that we pass sock_dgram instead of sock_stream.	code	13
then a final useful part of udp is that we can time out receiving a packet as opposed to tcp because udp isn’t connection-oriented. the snippet to do that is below.	snippet	136
now, the socket is connected and ready to use. we can use sendto to send a packet. we should also check the return value. note that we won’t get an error if the packet isn’t delivered because that is a part of the udp protocol. we will, however, get error codes for invalid structs, bad addresses, etc.	code	256
now, the socket is connected and ready to use. we can use sendto to send a packet. we should also check the return value. note that we won’t get an error if the packet isn’t delivered because that is a part of the udp protocol. we will, however, get error codes for invalid structs, bad addresses, etc.	address	287
the above code simply sends “hello” through a udp. there is no idea of if the packet arrives, is processed, etc.	code	10
memset(&hints, 0, sizeof(hints)); hints.ai_family = af_inet6; hints.ai_socktype = sock_dgram; hints.ai_flags = ai_passive;	type	75
next, use getaddrinfo to specify the port number. we don’t need to specify a host as we are creating a server socket, not sending a packet to a remote host. be careful not to send “localhost” or any other synonym for the loop-back address. we may end up trying to passively listen to ourselves and resulting in bind errors.	address	231
next, use getaddrinfo to specify the port number. we don’t need to specify a host as we are creating a server socket, not sending a packet to a remote host. be careful not to send “localhost” or any other synonym for the loop-back address. we may end up trying to passively listen to ourselves and resulting in bind errors.	the loop	217
getaddrinfo(null, "300", &hints, &res); sockfd = socket(res->ai_family, res->ai_socktype, res->ai_protocol); bind(sockfd, res->ai_addr, res->ai_addrlen);	type	84
the addr struct will hold the sender (source) information about the arriving packet. note the sockaddr_storage type is sufficiently large enough to hold all possible types of socket addresses – ipv4, ipv6 or any other internet protocol. the full udp server code is below.	code	257
the addr struct will hold the sender (source) information about the arriving packet. note the sockaddr_storage type is sufficiently large enough to hold all possible types of socket addresses – ipv4, ipv6 or any other internet protocol. the full udp server code is below.	address	182
the addr struct will hold the sender (source) information about the arriving packet. note the sockaddr_storage type is sufficiently large enough to hold all possible types of socket addresses – ipv4, ipv6 or any other internet protocol. the full udp server code is below.	type	111
#include <string.h> #include <stdio.h> #include <stdlib.h>	string	10
<sys/types.h> <sys/socket.h> <netdb.h> <unistd.h> <arpa/inet.h>	type	5
11.6 layer 7: http layer 7 of the osi layer deals with application-level interfaces. meaning that you can ignore everything below this layer and treat the internet as a way of communicating with another computer than can be secure and the session may reconnect. common layer 7 protocols are the following 1. http(s) - hypertext transfer protocol. sends arbitrary data and executes remote actions on a web server.	the following	291
4. dns - domain name service. translates hostnames to ip addresses 5. smtp - simple mail transfer protocol. allows one to send plain text emails to an email server 6. ssh - secure shell. allows one computer to connect to another computer and execute commands remotely.	address	57
remember when we were talking before about converting a website to an ip address? a system called “dns” (domain name service) is used. if the ip address is missing form a machine’s cache then it sends a udp packet to a local dns server. this server may query other upstream dns servers.	address	73
remember when we were talking before about converting a website to an ip address? a system called “dns” (domain name service) is used. if the ip address is missing form a machine’s cache then it sends a udp packet to a local dns server. this server may query other upstream dns servers.	system	84
remember when we were talking before about converting a website to an ip address? a system called “dns” (domain name service) is used. if the ip address is missing form a machine’s cache then it sends a udp packet to a local dns server. this server may query other upstream dns servers.	a system call	82
for example, a coffee shop internet connection could easily subvert your dns requests and send back different ip addresses for a particular domain. the way this is usually subverted is that after the ip address is obtained then a connection is usually made over https. https uses what is called the tls (formerly known as ssl) to secure transmissions and verify that the hostname is recognized by a certificate authority. certificate authorities often get hacked so be careful of equating a green lock to secure. even with this added layer of security, the united states government has recently issued a request for everyone to upgrade their dns to dnssec which includes additional security-focused technologies to verify with high probability that an ip address is truly associated with a hostname.	address	113
if you want the full bits and pieces, feel free to look at the wikipedia page. in essence, there is a hierarchy of dns servers. first, there is the dot hierarchy. this hierarchy first resolves top-level domains .edu .gov etc.	a page	71
11.7 non-blocking io when you call read() if the data is unavailable, it will wait until the data is ready before the function returns.	block	9
posix lets you set a flag on a file descriptor such that any call to read() on that file descriptor will return immediately, whether it has finished or not. with your file descriptor in this mode, your call to read() will start the read operation, and while it’s working you can do other useful work. this is called “non-blocking” mode since the call to read() doesn’t block.	block	321
to set a file descriptor to be non-blocking.	block	35
for a socket, you can create it in non-blocking mode by adding sock_nonblock to the second argument to socket():	block	39
fd = socket(af_inet, sock_stream | sock_nonblock, 0);	block	43
when a file is in non-blocking mode and you call read(), it will return immediately with whatever bytes are available. say 100 bytes have arrived from the server at the other end of your socket and you call read(fd, buf, 150).	block	22
‘read‘ will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for. say you tried to read the remaining data with a call to read(fd, buf+100, 50), but the last 50 bytes still hadn’t arrived yet. read() would return -1 and set the global error variable errno to either eagain or ewouldblock. that’s the system’s way of telling you the data isn’t ready yet.	block	320
‘read‘ will return immediately with a value of 100, meaning it read 100 of the 150 bytes you asked for. say you tried to read the remaining data with a call to read(fd, buf+100, 50), but the last 50 bytes still hadn’t arrived yet. read() would return -1 and set the global error variable errno to either eagain or ewouldblock. that’s the system’s way of telling you the data isn’t ready yet.	system	338
write() also works in non-blocking mode. say you want to send 40,000 bytes to a remote server using a socket. the system can only send so many bytes at a time. in non-blocking mode, write(fd, buf, 40000) would return the number of bytes it was able to send immediately, or about 23,000. if you called write() right away again, it would return -1 and set errno to eagain or ewouldblock. that’s the system’s way of telling you that it’s still busy sending the last chunk of data and isn’t ready to send more yet.	block	26
write() also works in non-blocking mode. say you want to send 40,000 bytes to a remote server using a socket. the system can only send so many bytes at a time. in non-blocking mode, write(fd, buf, 40000) would return the number of bytes it was able to send immediately, or about 23,000. if you called write() right away again, it would return -1 and set errno to eagain or ewouldblock. that’s the system’s way of telling you that it’s still busy sending the last chunk of data and isn’t ready to send more yet.	system	114
3. exceptfds - system-specific, not well-defined. just pass null for this.	system	15
select() returns the total number of ready file descriptors. if none of them become ready during the time defined by timeout, it will return 0. after select() returns, the caller will need to loop through the file descriptors in readfds and/or writefds to see which ones are ready. as readfds and writefds act as both input and output parameters, when select() indicates that there are ready file descriptors, it would have overwritten them to reflect only the ready file descriptors. unless the caller intends to call select() only once, it would be a good idea to save a copy of readfds and writefds before calling it. here is a comprehensive snippet.	parameter	335
select() returns the total number of ready file descriptors. if none of them become ready during the time defined by timeout, it will return 0. after select() returns, the caller will need to loop through the file descriptors in readfds and/or writefds to see which ones are ready. as readfds and writefds act as both input and output parameters, when select() indicates that there are ready file descriptors, it would have overwritten them to reflect only the ready file descriptors. unless the caller intends to call select() only once, it would be a good idea to save a copy of readfds and writefds before calling it. here is a comprehensive snippet.	snippet	645
fd_set readfds, writefds; fd_zero(&readfds); fd_zero(&writefds); for (int i=0; i < read_fd_count; i++) fd_set(my_read_fds[i], &readfds); for (int i=0; i < write_fd_count; i++) fd_set(my_write_fds[i], &writefds); struct timeval timeout; timeout.tv_sec = 3; timeout.tv_usec = 0; int num_ready = select(fd_setsize, &readfds, &writefds, null, &timeout); if (num_ready < 0) { perror("error in select()"); } else if (num_ready == 0) { printf("timeout\n"); } else { for (int i=0; i < read_fd_count; i++) if (fd_isset(my_read_fds[i], &readfds)) printf("fd %d is ready for reading\n", my_read_fds[i]); for (int i=0; i < write_fd_count; i++) if (fd_isset(my_write_fds[i], &writefds)) printf("fd %d is ready for writing\n", my_write_fds[i]); }	printf	429
it will tell you exactly which descriptors are ready. it even gives you a way to store a small amount of data with each descriptor, like an array index or a pointer, making it easier to access your data associated with that descriptor.	pointer	157
for each file descriptor that you want to monitor with epoll, you’ll need to add it to the epoll data structures using epoll_ctl() with the epoll_ctl_add option. you can add any number of file descriptors to it.	a struct	100
int num_ready = epoll_wait(epfd, &event, 1, timeout_milliseconds); if (num_ready > 0) { mydata *mypointer = (mydata*) event.data.ptr; printf("ready to write on %d\n", mypointer->fd); }	pointer	98
int num_ready = epoll_wait(epfd, &event, 1, timeout_milliseconds); if (num_ready > 0) { mydata *mypointer = (mydata*) event.data.ptr; printf("ready to write on %d\n", mypointer->fd); }	printf	134
say you were waiting to write data to a file descriptor, but now you want to wait to read data from it. just use epoll_ctl() with the epoll_ctl_mod option to change the type of operation you’re monitoring.	type	169
event.data.ptr = mypointer; epoll_ctl(epfd, epoll_ctl_mod, mypointer->fd, &event);	pointer	19
epoll_ctl(epfd, epoll_ctl_del, mypointer->fd, null);	pointer	33
also to non-blocking read() and write(), any calls to connect() on a non-blocking socket will also be non-blocking. to wait for the connection to complete, use select() or epoll to wait for the socket to be writable.	block	12
11.7.2 epoll example let’s break down the epoll code in the man page. we’ll assume that we have a prepared tcp server socket int listen_sock. the first thing we have to do is create the epoll device.	code	48
the function above is missing some error checking for brevity as well. note that this code is performant because we added the server socket in level-triggered mode and we add each of the client file descriptors in edge-triggered. edge triggered mode leaves more calculations on the part of the application – the application must keep reading or writing until the file descriptor is out of bytes – but it prevents starvation. a more efficient implementation would also add the listening socket in edge-triggered to clear out the backlog of connections as well.	code	86
has events on it, it will be returned by epoll when calling the ctl function. in edge-triggered, the caller will only get the file descriptor once it goes from zero events to an event. this means if you forget to read, write, accept etc on the file descriptor until you get a ewouldblock, that file descriptor will be dropped.	block	282
3. you can add an epoll object to epoll. edge triggered and level-triggered modes are the same because ctl will reset the state to zero events 4. depending on the conditions, you may get a file descriptor that was closed from epoll. this isn’t a bug. the reason that this happens is epoll works on the kernel object level, not the file descriptor level. if the kernel object lives longer and the right flags are set, a process could get a closed file descriptor. this also means that if you close the file descriptor, there is no way to remove the kernel object.	a process	417
read more at man 7 epoll or check out a better version called kqueue in the appendix.	kqueue	62
an example of this is you may send a remote procedure call to a docker daemon to change the state of the container. not every application needs to have access to the entire system machine, but they should have access to containers that they’ve created.	system	173
11.8.1 privilege separation the remote code will execute under a different user and with different privileges from the caller. in practice, the remote call may execute with more or fewer privileges than the caller. this in principle can be used to improve the security of a system by ensuring components operate with the least privilege. unfortunately, security concerns need to be carefully assessed to ensure that rpc mechanisms cannot be subverted to perform unwanted actions.	code	39
11.8.1 privilege separation the remote code will execute under a different user and with different privileges from the caller. in practice, the remote call may execute with more or fewer privileges than the caller. this in principle can be used to improve the security of a system by ensuring components operate with the least privilege. unfortunately, security concerns need to be carefully assessed to ensure that rpc mechanisms cannot be subverted to perform unwanted actions.	system	274
11.8.2 stub code and marshaling the stub code is the necessary code to hide the complexity of performing a remote procedure call. one of the roles of the stub code is to marshal the necessary data into a format that can be sent as a byte stream to a remote server.	code	12
using a string format may be a little inefficient. a good example of this marshaling is golang’s grpc or google rpc. there is a version in c as well if you want to check that out.	string	8
the server stub code will receive the request, unmarshal the request into a valid in-memory data call the underlying implementation and send the result back to the caller. often the underlying library will do this for you.	code	16
the server stub code will receive the request, unmarshal the request into a valid in-memory data call the underlying implementation and send the result back to the caller. often the underlying library will do this for you.	memory	85
2. ascii, unicode text format 8, some other encoding?	code	13
to marshal a struct, decide which fields need to be serialized. it may be unnecessary to send all data items.	a struct	11
to marshal a linked list, it is unnecessary to send the link pointers, stream the values. as part of unmarshaling, the server can recreate a linked list structure from the byte sequence.	pointer	61
by starting at the head node/vertex, a simple tree can be recursively visited to create a serialized version of the data. a cyclic graph will usually require additional memory to ensure that each edge and vertex is processed exactly once.	memory	169
11.8.3 interface description language writing stub code by hand is painful, tedious, error-prone, difficult to maintain and difficult to reverse engineer the wire protocol from the implemented code. a better approach is to specify the data objects, messages, and services to automatically generate the client and server code. a modern example of an interface description language is google’s protocol buffer .proto files.	code	51
an rpc must marshal data into a wire-compatible format. this may require multiple passes through the data structure, temporary memory allocation, and transformation of the data representation.	a struct	104
an rpc must marshal data into a wire-compatible format. this may require multiple passes through the data structure, temporary memory allocation, and transformation of the data representation.	memory	127
an rpc must marshal data into a wire-compatible format. this may require multiple passes through the data structure, temporary memory allocation, and transformation of the data representation.	memory allocation	127
an rpc must marshal data into a wire-compatible format. this may require multiple passes through the data structure, temporary memory allocation, and transformation of the data representation.	the data structure	97
robust rpc stub code must intelligently handle network failures and versioning. for example, a server may have to process requests from clients that are still running an early version of the stub code.	code	16
a secure rpc will need to implement additional security checks including authentication and authorization, validate data and encrypt communication between the client and host. a lot of the time, the rpc system can do this efficiently for you. consider if you have both an rpc client and server on the same machine. starting up a thrift or google rpc server could validate and route the request to a local socket which wouldn’t be sent over the network.	system	203
11.8.4 transferring structured data let’s examine three methods of transferring data using 3 different formats - json, xml, and google protocol buffers. json and xml are text-based protocols. examples of json and xml messages are below.	protocol buffers	135
google protocol buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low cpu overhead and minimal memory copying. this means client and server stub code in multiple languages can be generated from the .proto specification file to marshal data to and from a binary stream.	code	201
google protocol buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low cpu overhead and minimal memory copying. this means client and server stub code in multiple languages can be generated from the .proto specification file to marshal data to and from a binary stream.	memory	151
google protocol buffers is an open-source efficient binary protocol that places a strong emphasis on high throughput with low cpu overhead and minimal memory copying. this means client and server stub code in multiple languages can be generated from the .proto specification file to marshal data to and from a binary stream.	protocol buffers	7
google protocol buffers reduces the versioning problem by ignoring unknown fields that are present in a message. see the introduction to protocol buffers for more information.	protocol buffers	7
the general chain is to abstract away the actual business logic and the various marshaling code. if your application ever becomes cpu bound parsing xml, json or yaml, switch to protocol buffers!	code	91
the general chain is to abstract away the actual business logic and the various marshaling code. if your application ever becomes cpu bound parsing xml, json or yaml, switch to protocol buffers!	protocol buffers	177
 packet loss/connection based  get address info  dns  tcp client calls  tcp server calls  shutdown  recvfrom  epoll vs select  rpc	address	35
 what is a remote procedure call? when should one use it versus http or running code locally?	code	80
12 filesystems	system	7
filesystems are important because they allow you to persist data after a computer is shut down, crashes, or has memory corruption. back in the day, filesystems were expensive to use. writing to the filesystem (fs) involved writing to magnetic tape and reading from that tape [1]. it was slow, heavy, and prone to errors.	memory	112
filesystems are important because they allow you to persist data after a computer is shut down, crashes, or has memory corruption. back in the day, filesystems were expensive to use. writing to the filesystem (fs) involved writing to magnetic tape and reading from that tape [1]. it was slow, heavy, and prone to errors.	system	4
nowadays most of our files are stored on disk – though not all of them! the disk is still slower than memory by an order of magnitude at the least.	memory	102
some terminology before we begin this chapter. a filesystem, as we’ll define more concretely later, is anything that satisfies the api of a filesystem. a filesystem is backed by a storage medium, such as a hard disk drive, solid state drive, ram, etc. a disk is either a hard disk drive (hdd) which includes a spinning metallic platter and a head which can zap the platter to encode a 1 or a 0, or a solid-state drive (ssd) that can flip certain nand gates on a chip or standalone drive to store a 1 or a 0. as of 2019, ssds are an order of magnitude faster than the standard hdd. these are typical backings for a filesystem. a filesystem is implemented on top of this backing, meaning that we can either implement something like ext, minixfs, ntfs, fat32, etc. on a commercially available hard disk. this filesystem tells the operating system how to organize the 1s and 0s to store file information as well as directory information, but more on that later. to avoid being pedantic, we’ll say that a filesystem like ext or ntfs implements the filesystem api directly (open, close, etc). often, operating systems will add a layer of abstraction and require that the operating system satisfy its api instead (think imaginary functions linux_open, linux_close etc). the two benefits are that one filesystem can be implemented for multiple operating system apis and adding a new os filesystem call doesn’t require all of the underlying file systems to change their api. for example, in the next iteration of linux if there was a new system call to create a backup of a file, the os can implement that with the internal api rather than requiring all filesystem drivers to change their code.	code	378
some terminology before we begin this chapter. a filesystem, as we’ll define more concretely later, is anything that satisfies the api of a filesystem. a filesystem is backed by a storage medium, such as a hard disk drive, solid state drive, ram, etc. a disk is either a hard disk drive (hdd) which includes a spinning metallic platter and a head which can zap the platter to encode a 1 or a 0, or a solid-state drive (ssd) that can flip certain nand gates on a chip or standalone drive to store a 1 or a 0. as of 2019, ssds are an order of magnitude faster than the standard hdd. these are typical backings for a filesystem. a filesystem is implemented on top of this backing, meaning that we can either implement something like ext, minixfs, ntfs, fat32, etc. on a commercially available hard disk. this filesystem tells the operating system how to organize the 1s and 0s to store file information as well as directory information, but more on that later. to avoid being pedantic, we’ll say that a filesystem like ext or ntfs implements the filesystem api directly (open, close, etc). often, operating systems will add a layer of abstraction and require that the operating system satisfy its api instead (think imaginary functions linux_open, linux_close etc). the two benefits are that one filesystem can be implemented for multiple operating system apis and adding a new os filesystem call doesn’t require all of the underlying file systems to change their api. for example, in the next iteration of linux if there was a new system call to create a backup of a file, the os can implement that with the internal api rather than requiring all filesystem drivers to change their code.	a filesystem	47
some terminology before we begin this chapter. a filesystem, as we’ll define more concretely later, is anything that satisfies the api of a filesystem. a filesystem is backed by a storage medium, such as a hard disk drive, solid state drive, ram, etc. a disk is either a hard disk drive (hdd) which includes a spinning metallic platter and a head which can zap the platter to encode a 1 or a 0, or a solid-state drive (ssd) that can flip certain nand gates on a chip or standalone drive to store a 1 or a 0. as of 2019, ssds are an order of magnitude faster than the standard hdd. these are typical backings for a filesystem. a filesystem is implemented on top of this backing, meaning that we can either implement something like ext, minixfs, ntfs, fat32, etc. on a commercially available hard disk. this filesystem tells the operating system how to organize the 1s and 0s to store file information as well as directory information, but more on that later. to avoid being pedantic, we’ll say that a filesystem like ext or ntfs implements the filesystem api directly (open, close, etc). often, operating systems will add a layer of abstraction and require that the operating system satisfy its api instead (think imaginary functions linux_open, linux_close etc). the two benefits are that one filesystem can be implemented for multiple operating system apis and adding a new os filesystem call doesn’t require all of the underlying file systems to change their api. for example, in the next iteration of linux if there was a new system call to create a backup of a file, the os can implement that with the internal api rather than requiring all filesystem drivers to change their code.	system	53
the last piece of background is an important one. in this chapter, we will refer to sizes of files in the iso-compliant kib or kibibyte. the *ib family is short for power of two storage. that means the following:	the following	198
the last piece of background is an important one. in this chapter, we will refer to sizes of files in the iso-compliant kib or kibibyte. the *ib family is short for power of two storage. that means the following:	background	18
the standard notational prefixes mean the following:	the following	38
we will do this in the book and in the networking chapter for the sake of consistency and to not confuse anyone. confusingly in the real world, there is a different convention. that convention is that when a file is displayed in the operating system, kb is the same as kib. when we are talking about computer networks, cds, other storage kb is not the same as kib and is the iso / metric definition above. this is a historical quirk was brought by a clash between network developers and memory/hard storage developers. hard storage and memory developers found that if a bit could take one of two states, it would be natural to call a kilo- prefix 1024 because it was about 1000. network developers had to deal with bits, real-time signal processing, and various other factors, so they went with the already accepted convention that kilo- means 1000 of something [1]. what you need to know is if you see kb in the wild, that it may be 1024 based on the context. if any time in this class you see kb or any of the family refer to a filesystems question, you can safely infer that they are referring to 1024 as the base unit. though when you are pushing production code, make sure to ask about the difference!	code	1162
we will do this in the book and in the networking chapter for the sake of consistency and to not confuse anyone. confusingly in the real world, there is a different convention. that convention is that when a file is displayed in the operating system, kb is the same as kib. when we are talking about computer networks, cds, other storage kb is not the same as kib and is the iso / metric definition above. this is a historical quirk was brought by a clash between network developers and memory/hard storage developers. hard storage and memory developers found that if a bit could take one of two states, it would be natural to call a kilo- prefix 1024 because it was about 1000. network developers had to deal with bits, real-time signal processing, and various other factors, so they went with the already accepted convention that kilo- means 1000 of something [1]. what you need to know is if you see kb in the wild, that it may be 1024 based on the context. if any time in this class you see kb or any of the family refer to a filesystems question, you can safely infer that they are referring to 1024 as the base unit. though when you are pushing production code, make sure to ask about the difference!	a filesystem	1028
we will do this in the book and in the networking chapter for the sake of consistency and to not confuse anyone. confusingly in the real world, there is a different convention. that convention is that when a file is displayed in the operating system, kb is the same as kib. when we are talking about computer networks, cds, other storage kb is not the same as kib and is the iso / metric definition above. this is a historical quirk was brought by a clash between network developers and memory/hard storage developers. hard storage and memory developers found that if a bit could take one of two states, it would be natural to call a kilo- prefix 1024 because it was about 1000. network developers had to deal with bits, real-time signal processing, and various other factors, so they went with the already accepted convention that kilo- means 1000 of something [1]. what you need to know is if you see kb in the wild, that it may be 1024 based on the context. if any time in this class you see kb or any of the family refer to a filesystems question, you can safely infer that they are referring to 1024 as the base unit. though when you are pushing production code, make sure to ask about the difference!	memory	487
we will do this in the book and in the networking chapter for the sake of consistency and to not confuse anyone. confusingly in the real world, there is a different convention. that convention is that when a file is displayed in the operating system, kb is the same as kib. when we are talking about computer networks, cds, other storage kb is not the same as kib and is the iso / metric definition above. this is a historical quirk was brought by a clash between network developers and memory/hard storage developers. hard storage and memory developers found that if a bit could take one of two states, it would be natural to call a kilo- prefix 1024 because it was about 1000. network developers had to deal with bits, real-time signal processing, and various other factors, so they went with the already accepted convention that kilo- means 1000 of something [1]. what you need to know is if you see kb in the wild, that it may be 1024 based on the context. if any time in this class you see kb or any of the family refer to a filesystems question, you can safely infer that they are referring to 1024 as the base unit. though when you are pushing production code, make sure to ask about the difference!	system	243
12.1 what is a filesystem?	a filesystem	13
12.1 what is a filesystem?	system	19
you may have encountered the old unix adage, "everything is a file". in most unix systems, file operations provide an interface to abstract many different operations. network sockets, hardware devices, and data on the disk are all represented by file-like objects. a file-like object must follow the following conventions: 1. it must present itself to the filesystem.	the following	296
you may have encountered the old unix adage, "everything is a file". in most unix systems, file operations provide an interface to abstract many different operations. network sockets, hardware devices, and data on the disk are all represented by file-like objects. a file-like object must follow the following conventions: 1. it must present itself to the filesystem.	system	82
2. it must support common filesystem operations, such as open, read, write. at a minimum, it needs to be opened and closed.	system	30
a filesystem is an implementation of the file interface. in this chapter, we will be exploring the various callbacks a filesystem provides, some typical functionality and associated implementation details. in this class, we will mostly talk about filesystems that serve to allow users to access data on disk, which are integral to modern computers.	a filesystem	0
a filesystem is an implementation of the file interface. in this chapter, we will be exploring the various callbacks a filesystem provides, some typical functionality and associated implementation details. in this class, we will mostly talk about filesystems that serve to allow users to access data on disk, which are integral to modern computers.	system	6
here are some common features of a filesystem: 1. they deal with both storing local files and handle special devices that allow for safe communication between the kernel and user space.	a filesystem	33
here are some common features of a filesystem: 1. they deal with both storing local files and handle special devices that allow for safe communication between the kernel and user space.	system	39
before we dive into the details of a filesystem, let’s take a look at some examples. to clarify, a mount point is simply a mapping of a directory to a filesystem represented in the kernel.	a filesystem	35
before we dive into the details of a filesystem, let’s take a look at some examples. to clarify, a mount point is simply a mapping of a directory to a filesystem represented in the kernel.	system	41
1. ext4 usually mounted at / on linux systems, this is the filesystem that usually provides disk access as you’re used to.	system	38
4. tmpfs mounted at /tmp in some systems, an in-memory filesystem to hold temporary files.	memory	48
4. tmpfs mounted at /tmp in some systems, an in-memory filesystem to hold temporary files.	system	33
it tells you what filesystem directory-based system calls resolve to. for example, / is resolved by the ext4 filesystem in our case, but /proc/2 is resolved by the procfs system even though it contains / as a subsystem.	system	22
as you may have noticed, some filesystems provide an interface to things that aren’t "files". filesystems such as procfs are usually referred to as virtual filesystems, since they don’t provide data access in the same sense as a traditional filesystem would. technically, all filesystems in the kernel are represented by virtual filesystems, but we will differentiate virtual filesystems as filesystems that actually don’t store anything on a hard disk.	system	34
12.1.1 the file api a filesystem must provide callback functions to a variety of actions. some of them are listed below:  open opens a file for io  read read contents of a file  write write to a file  close close a file and free associated resources  chmod modify permissions of a file  ioctl interact with device parameters of character devices such as terminals not every filesystem supports all the possible callback functions. for example, many filesystems omit ioctl or link. many filesystems aren’t seekable meaning that they exclusively provide sequential access. a program cannot move to an arbitrary point in the file. this is analogous to seekable streams. in this chapter, we will not be examining each filesystem callback. if you would like to learn more about this interface, try looking at the documentation for filesystems at the user space level (fuse).	a filesystem	20
12.1.1 the file api a filesystem must provide callback functions to a variety of actions. some of them are listed below:  open opens a file for io  read read contents of a file  write write to a file  close close a file and free associated resources  chmod modify permissions of a file  ioctl interact with device parameters of character devices such as terminals not every filesystem supports all the possible callback functions. for example, many filesystems omit ioctl or link. many filesystems aren’t seekable meaning that they exclusively provide sequential access. a program cannot move to an arbitrary point in the file. this is analogous to seekable streams. in this chapter, we will not be examining each filesystem callback. if you would like to learn more about this interface, try looking at the documentation for filesystems at the user space level (fuse).	resources	240
12.1.1 the file api a filesystem must provide callback functions to a variety of actions. some of them are listed below:  open opens a file for io  read read contents of a file  write write to a file  close close a file and free associated resources  chmod modify permissions of a file  ioctl interact with device parameters of character devices such as terminals not every filesystem supports all the possible callback functions. for example, many filesystems omit ioctl or link. many filesystems aren’t seekable meaning that they exclusively provide sequential access. a program cannot move to an arbitrary point in the file. this is analogous to seekable streams. in this chapter, we will not be examining each filesystem callback. if you would like to learn more about this interface, try looking at the documentation for filesystems at the user space level (fuse).	parameter	314
12.1.1 the file api a filesystem must provide callback functions to a variety of actions. some of them are listed below:  open opens a file for io  read read contents of a file  write write to a file  close close a file and free associated resources  chmod modify permissions of a file  ioctl interact with device parameters of character devices such as terminals not every filesystem supports all the possible callback functions. for example, many filesystems omit ioctl or link. many filesystems aren’t seekable meaning that they exclusively provide sequential access. a program cannot move to an arbitrary point in the file. this is analogous to seekable streams. in this chapter, we will not be examining each filesystem callback. if you would like to learn more about this interface, try looking at the documentation for filesystems at the user space level (fuse).	system	26
12.2 storing data on disk to understand how a filesystem interacts with data on disk, there are three key terms we will be using.	a filesystem	44
12.2 storing data on disk to understand how a filesystem interacts with data on disk, there are three key terms we will be using.	system	50
1. disk block a disk block is a portion of the disk that is reserved for storing the contents of a file or a directory.	block	8
2. inode an inode is a file or directory. this means that an inode contains metadata about the file as well as pointers to disk blocks so that the file can actually be written to or read from.	block	128
2. inode an inode is a file or directory. this means that an inode contains metadata about the file as well as pointers to disk blocks so that the file can actually be written to or read from.	pointer	111
3. superblock a superblock contains metadata about the inodes and disk blocks. an example superblock can store how full each disk block is, which inodes are being used etc. modern filesystems may actually contain multiple superblocks and a sort-of super-super block that keeps track of which sectors are governed by which superblocks. this tends to help with fragmentation.	block	8
3. superblock a superblock contains metadata about the inodes and disk blocks. an example superblock can store how full each disk block is, which inodes are being used etc. modern filesystems may actually contain multiple superblocks and a sort-of super-super block that keeps track of which sectors are governed by which superblocks. this tends to help with fragmentation.	system	184
it may seem overwhelming, but by the end of this chapter, we will be able to make sense of every part of the filesystem.	system	113
to reason about data on some form of storage – spinning disks, solid state drives, magnetic tape – it is common practice to first consider the medium of storage as a collection of blocks. a block can be thought of as a contiguous region on disk. while its size is sometimes determined by some property of the underlying hardware, it is more frequently determined based on the size of a page of memory for a given system, so that data from the disk can be cached in memory for faster access – a important feature of many filesystems.	a page	384
to reason about data on some form of storage – spinning disks, solid state drives, magnetic tape – it is common practice to first consider the medium of storage as a collection of blocks. a block can be thought of as a contiguous region on disk. while its size is sometimes determined by some property of the underlying hardware, it is more frequently determined based on the size of a page of memory for a given system, so that data from the disk can be cached in memory for faster access – a important feature of many filesystems.	memory	394
to reason about data on some form of storage – spinning disks, solid state drives, magnetic tape – it is common practice to first consider the medium of storage as a collection of blocks. a block can be thought of as a contiguous region on disk. while its size is sometimes determined by some property of the underlying hardware, it is more frequently determined based on the size of a page of memory for a given system, so that data from the disk can be cached in memory for faster access – a important feature of many filesystems.	block	180
to reason about data on some form of storage – spinning disks, solid state drives, magnetic tape – it is common practice to first consider the medium of storage as a collection of blocks. a block can be thought of as a contiguous region on disk. while its size is sometimes determined by some property of the underlying hardware, it is more frequently determined based on the size of a page of memory for a given system, so that data from the disk can be cached in memory for faster access – a important feature of many filesystems.	system	413
a filesystem has a special block denoted as a superblock that stores metadata about the filesystem such as a journal (which logs changes to the filesystem), a table of inodes, the location of the first inode on disk, etc. the important thing about a superblock is that it is in a known location on disk. if not, your computer may fail to boot! consider a simple rom programmed into your motherboard. if your processor can’t tell the motherboard to start reading and decipher a disk block to start the boot sequence, you are out of luck.	a filesystem	0
a filesystem has a special block denoted as a superblock that stores metadata about the filesystem such as a journal (which logs changes to the filesystem), a table of inodes, the location of the first inode on disk, etc. the important thing about a superblock is that it is in a known location on disk. if not, your computer may fail to boot! consider a simple rom programmed into your motherboard. if your processor can’t tell the motherboard to start reading and decipher a disk block to start the boot sequence, you are out of luck.	block	27
a filesystem has a special block denoted as a superblock that stores metadata about the filesystem such as a journal (which logs changes to the filesystem), a table of inodes, the location of the first inode on disk, etc. the important thing about a superblock is that it is in a known location on disk. if not, your computer may fail to boot! consider a simple rom programmed into your motherboard. if your processor can’t tell the motherboard to start reading and decipher a disk block to start the boot sequence, you are out of luck.	system	6
the inode is the most important structure for our filesystem as it represents a file. before we explore it in-depth, let’s list out the key information we need to have a usable file.	system	54
12.2.1 file contents from wikipedia: in a unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be various things including a file or a directory. each inode stores the attributes and disk block location(s) of the filesystem object’s data. filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).	a struct	125
12.2.1 file contents from wikipedia: in a unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be various things including a file or a directory. each inode stores the attributes and disk block location(s) of the filesystem object’s data. filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).	a filesystem	155
12.2.1 file contents from wikipedia: in a unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be various things including a file or a directory. each inode stores the attributes and disk block location(s) of the filesystem object’s data. filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).	block	279
12.2.1 file contents from wikipedia: in a unix-style file system, an index node, informally referred to as an inode, is a data structure used to represent a filesystem object, which can be various things including a file or a directory. each inode stores the attributes and disk block location(s) of the filesystem object’s data. filesystem object attributes may include manipulation metadata (e.g. change, access, modify time), as well as owner and permission data (e.g. group-id, user-id, permissions).	system	58
the superblock may store an array of inodes, each of which stores direct, and potentially several kinds of indirect pointers to disk blocks. since inodes are stored in the superblock, most filesystems have a limit on how many inodes can exist. since each inode corresponds to a file, this is also a limit on how many files that filesystem can have. trying to overcome this problem by storing inodes in some other location greatly increases the complexity of the filesystem. trying to reallocate space for the inode table is also infeasible since every byte following the end of the inode array would have to be shifted, a highly expensive operation. this isn’t to say there aren’t any solutions at all, although typically there is no need to increase the number of inodes since the number of inodes is usually sufficiently high.	block	9
the superblock may store an array of inodes, each of which stores direct, and potentially several kinds of indirect pointers to disk blocks. since inodes are stored in the superblock, most filesystems have a limit on how many inodes can exist. since each inode corresponds to a file, this is also a limit on how many files that filesystem can have. trying to overcome this problem by storing inodes in some other location greatly increases the complexity of the filesystem. trying to reallocate space for the inode table is also infeasible since every byte following the end of the inode array would have to be shifted, a highly expensive operation. this isn’t to say there aren’t any solutions at all, although typically there is no need to increase the number of inodes since the number of inodes is usually sufficiently high.	system	193
the superblock may store an array of inodes, each of which stores direct, and potentially several kinds of indirect pointers to disk blocks. since inodes are stored in the superblock, most filesystems have a limit on how many inodes can exist. since each inode corresponds to a file, this is also a limit on how many files that filesystem can have. trying to overcome this problem by storing inodes in some other location greatly increases the complexity of the filesystem. trying to reallocate space for the inode table is also infeasible since every byte following the end of the inode array would have to be shifted, a highly expensive operation. this isn’t to say there aren’t any solutions at all, although typically there is no need to increase the number of inodes since the number of inodes is usually sufficiently high.	pointer	116
it is common to think of the file name as the ‘actual’ file. it’s not! instead, consider the inode as the file. the inode holds the meta-information (last accessed, ownership, size) and points to the disk blocks used to hold the file contents. however, the inode does not usually store a filename. filenames are usually only stored in directories (see below).	block	205
for example, to read the first few bytes of the file, follow the first direct block pointer to the first direct block and read the first few bytes. writing follows the same process. if a program wants to read the entire file, keep reading direct blocks until you’ve read several bytes equal to the size of the file. if the total size of the file is less than that of the number of direct blocks multiplied by the size of a block, then unused block pointers will be undefined. similarly, if the size of a file is not a multiple of the size of a block, data past the end of the last byte in the last block will be garbage.	block	78
for example, to read the first few bytes of the file, follow the first direct block pointer to the first direct block and read the first few bytes. writing follows the same process. if a program wants to read the entire file, keep reading direct blocks until you’ve read several bytes equal to the size of the file. if the total size of the file is less than that of the number of direct blocks multiplied by the size of a block, then unused block pointers will be undefined. similarly, if the size of a file is not a multiple of the size of a block, data past the end of the last byte in the last block will be garbage.	pointer	84
what if a file is bigger than the maximum space addressable by its direct blocks? to that, we present a motto programmers take too seriously.	address	48
what if a file is bigger than the maximum space addressable by its direct blocks? to that, we present a motto programmers take too seriously.	block	74
to solve this problem, we introduce indirect blocks. a single indirect block is a block that stores pointers to more data blocks. similarly, a double indirect block stores pointers to single indirect blocks, and the concept can be generalized to arbitrary levels of indirection. this is a important concept, as inodes are stored in the superblock, or some other structure in a well known location with a constant amount of space, indirection allows exponential increases in the amount of space an inode can keep track of.	block	45
to solve this problem, we introduce indirect blocks. a single indirect block is a block that stores pointers to more data blocks. similarly, a double indirect block stores pointers to single indirect blocks, and the concept can be generalized to arbitrary levels of indirection. this is a important concept, as inodes are stored in the superblock, or some other structure in a well known location with a constant amount of space, indirection allows exponential increases in the amount of space an inode can keep track of.	pointer	100
as a worked example, suppose we divide the disk into 4kib blocks and we want to address up to 232 blocks.	address	80
as a worked example, suppose we divide the disk into 4kib blocks and we want to address up to 232 blocks.	block	58
the maximum disk size is 4k ib ∗ 232 = 16t ib remember 210 = 1024. a disk block can store 4k4bib possible pointers or 1024 pointers. four byte wide pointers are needed because we want to address 32 bits worth of blocks. each pointer refers to a 4kib disk block, so you can refer up to 1024 ∗ 4k ib = 4m ib of data. for the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. thus a double-indirect block can refer up to 1024 ∗ 4m ib = 4gib of data. similarly, a triple indirect block can refer up to 4tib of data. this is three times as slow for reading between blocks, due to increased levels of indirection. the actual intra-block reading times don’t change.	address	187
the maximum disk size is 4k ib ∗ 232 = 16t ib remember 210 = 1024. a disk block can store 4k4bib possible pointers or 1024 pointers. four byte wide pointers are needed because we want to address 32 bits worth of blocks. each pointer refers to a 4kib disk block, so you can refer up to 1024 ∗ 4k ib = 4m ib of data. for the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. thus a double-indirect block can refer up to 1024 ∗ 4m ib = 4gib of data. similarly, a triple indirect block can refer up to 4tib of data. this is three times as slow for reading between blocks, due to increased levels of indirection. the actual intra-block reading times don’t change.	block	74
the maximum disk size is 4k ib ∗ 232 = 16t ib remember 210 = 1024. a disk block can store 4k4bib possible pointers or 1024 pointers. four byte wide pointers are needed because we want to address 32 bits worth of blocks. each pointer refers to a 4kib disk block, so you can refer up to 1024 ∗ 4k ib = 4m ib of data. for the same disk configuration, a double indirect block stores 1024 pointers to 1024 indirection tables. thus a double-indirect block can refer up to 1024 ∗ 4m ib = 4gib of data. similarly, a triple indirect block can refer up to 4tib of data. this is three times as slow for reading between blocks, due to increased levels of indirection. the actual intra-block reading times don’t change.	pointer	106
let’s think about what directories looks like in the actual file system. theoretically, they are files. the disk blocks will contain directory entries or dirents. what that means is that our disk block can look like this	block	113
let’s think about what directories looks like in the actual file system. theoretically, they are files. the disk blocks will contain directory entries or dirents. what that means is that our disk block can look like this	system	65
| inode_num | name | | ----------- | ------ | | 2043567 | hi.txt | | ... | each directory entry could either be a fixed size, or a variable length c-string. it depends on how the particular filesystem implements it at the lower level. to see a mapping of filenames to inode numbers on a posix system, from a shell, use ls with the -i option	a shell	306
| inode_num | name | | ----------- | ------ | | 2043567 | hi.txt | | ... | each directory entry could either be a fixed size, or a variable length c-string. it depends on how the particular filesystem implements it at the lower level. to see a mapping of filenames to inode numbers on a posix system, from a shell, use ls with the -i option	string	149
| inode_num | name | | ----------- | ------ | | 2043567 | hi.txt | | ... | each directory entry could either be a fixed size, or a variable length c-string. it depends on how the particular filesystem implements it at the lower level. to see a mapping of filenames to inode numbers on a posix system, from a shell, use ls with the -i option	system	194
12.2.3 unix directory conventions in standard unix filesystems, the following entries are specially added on requests to read a directory.	the following	64
12.2.3 unix directory conventions in standard unix filesystems, the following entries are specially added on requests to read a directory.	system	55
additional facts about name-related conventions: 1. ~ is usually expanded to the home directory by the shell 2. files that start with ’.’ (a period) on disk are conventionally considered ’hidden’ and will be omitted by programs like ls without additional flags (-a). this is not a feature of the filesystem, and programs may choose to ignore this.	system	300
3. some files may also start with a nul byte. these are usually abstract unix sockets and are used to prevent cluttering up the filesystem since they will be effectively hidden by any unexpecting program. they will, however, be listed by tools that detail information about sockets, so this is not a feature providing security.	system	132
12.2.4 directory api while interacting with a file in c is typically done by using open to open the file and then read or write to interact with the file before calling close to release resources, directories have special calls such as, opendir, closedir and readdir. there is no function writedir since typically that implies creating a file or link. the program would use something like open or mkdir.	resources	186
to explore these functions, let’s write a program to search the contents of a directory for a particular file. the code below has a bug, try to spot it!	code	115
did you find the bug? it leaks resources! if a matching filename is found then ‘closedir’ is never called as part of the early return. any file descriptors opened and any memory allocated by opendir are never released. this means eventually the process will run out of resources and an open or opendir call will fail.	resources	31
did you find the bug? it leaks resources! if a matching filename is found then ‘closedir’ is never called as part of the early return. any file descriptors opened and any memory allocated by opendir are never released. this means eventually the process will run out of resources and an open or opendir call will fail.	memory	171
the fix is to ensure we free up resources in every possible code path.	code	60
the fix is to ensure we free up resources in every possible code path.	resources	32
in the above code, this means calling closedir before return 1. forgetting to release resources is a common c programming bug because there is no support in the c language to ensure resources are always released with all code paths.	code	13
in the above code, this means calling closedir before return 1. forgetting to release resources is a common c programming bug because there is no support in the c language to ensure resources are always released with all code paths.	resources	86
the following code attempts to list all files in a directory recursively. as an exercise, try to identify the bugs it introduces.	the following	0
the following code attempts to list all files in a directory recursively. as an exercise, try to identify the bugs it introduces.	code	14
void dirlist(char *path) { struct dirent *dp; dir *dirp = opendir(path); while ((dp = readdir(dirp)) != null) { char newpath[strlen(path) + strlen(dp->d_name) + 1]; sprintf(newpath,"%s/%s", newpath, dp->d_name); printf("%s\n", dp->d_name); dirlist(newpath); } } int main(int argc, char **argv) { dirlist(argv[1]); return 0; }	printf	166
one final note of caution. readdir is not thread-safe! you shouldn’t use the re-entrant version of the function.	thread	42
synchronizing the filesystem within a process is important, so use locks around readdir.	a process	36
synchronizing the filesystem within a process is important, so use locks around readdir.	system	22
12.2.5 linking links are what force us to model a filesystem as a graph rather than a tree.	a filesystem	48
12.2.5 linking links are what force us to model a filesystem as a graph rather than a tree.	system	54
while modeling the filesystem as a tree would imply that every inode has a unique parent directory, links allow inodes to present themselves as files in multiple places, potentially with different names, thus leading to an inode having multiple parent directories. there are two kinds of links: 1. hard links a hard link is simply an entry in a directory assigning some name to an inode number that already has a different name and mapping in either the same directory or a different one. if we already have a file on a file system we can create another link to the same inode using the ‘ln’ command:	system	23
for simplicity, the above examples made hard links inside the same directory. hard links can be created anywhere inside the same filesystem.	system	133
some advantages of symbolic links are  can refer to files that don’t exist yet  unlike hard links, can refer to directories as well as regular files  can refer to files (and directories) that exist outside of the current file system	system	226
however, symlinks have a key disadvantage, they as slower than regular files and directories. when the link’s contents are read, they must be interpreted as a new path to the target file, resulting in an additional call to open and read since the real file must be opened and read. another disadvantage is that posix forbids hard linking directories where as soft links are allowed. the ln command will only allow root to do this and only if you provide the -d option. however, even root may not be able to perform this because most filesystems prevent it!	system	537
the integrity of the file system assumes the directory structure is an acyclic tree that is reachable from the root directory. it becomes expensive to enforce or verify this constraint if directory linking is allowed. breaking these assumptions can leave file integrity tools unable to repair the file system. recursive searches potentially never terminate and directories can have more than one parent but “..” can only refer to a single parent. all in all, a bad idea. soft links are merely ignored, which is why we can use them to reference directories.	system	26
the integrity of the file system assumes the directory structure is an acyclic tree that is reachable from the root directory. it becomes expensive to enforce or verify this constraint if directory linking is allowed. breaking these assumptions can leave file integrity tools unable to repair the file system. recursive searches potentially never terminate and directories can have more than one parent but “..” can only refer to a single parent. all in all, a bad idea. soft links are merely ignored, which is why we can use them to reference directories.	the directory structure	41
an example use of hard links is to efficiently create multiple archives of a file system at different points in time.	system	82
12.2.6 pathing now that we have definitions, and have talked about directories, we come across the concept of a path. a path is a sequence of directories that provide one with a "path" in the graph that is a filesystem. however, there are some nuances. it is possible to have a path called a/b/../c/./. since .. and . are special entries in directories, this is a valid path that actually refers to a/c. most filesystem functions will allow uncompressed paths to be passed in.	a filesystem	206
12.2.6 pathing now that we have definitions, and have talked about directories, we come across the concept of a path. a path is a sequence of directories that provide one with a "path" in the graph that is a filesystem. however, there are some nuances. it is possible to have a path called a/b/../c/./. since .. and . are special entries in directories, this is a valid path that actually refers to a/c. most filesystem functions will allow uncompressed paths to be passed in.	system	212
the c library provides a function realpath to compress the path or get the absolute path. to simplify by hand, remember that .. means ‘parent folder’ and that . means ‘current folder’. below is an example that illustrates the simplification of the a/b/../c/. by using cd in a shell to navigate a filesystem.	a filesystem	294
the c library provides a function realpath to compress the path or get the absolute path. to simplify by hand, remember that .. means ‘parent folder’ and that . means ‘current folder’. below is an example that illustrates the simplification of the a/b/../c/. by using cd in a shell to navigate a filesystem.	a shell	274
the c library provides a function realpath to compress the path or get the absolute path. to simplify by hand, remember that .. means ‘parent folder’ and that . means ‘current folder’. below is an example that illustrates the simplification of the a/b/../c/. by using cd in a shell to navigate a filesystem.	system	300
12.2.7 metadata how can we distinguish between a regular file and a directory? for that matter, there are many other attributes that files also might contain. we distinguish a file type – different from the file extension i.e. png, svg, pdf – using fields inside the inode. how does the system know what type the file is?	system	287
12.2.7 metadata how can we distinguish between a regular file and a directory? for that matter, there are many other attributes that files also might contain. we distinguish a file type – different from the file extension i.e. png, svg, pdf – using fields inside the inode. how does the system know what type the file is?	type	181
struct stat s; stat("notes.txt", &s); printf("last accessed %s", ctime(&s.st_atime));	printf	38
file *file = fopen("notes.txt", "r"); int fd = fileno(file); /* just for fun - extract the file descriptor from a c file struct */ struct stat s; fstat(fd, & s); printf("last accessed %s", ctime(&s.st_atime));	printf	162
struct stat { dev_t st_dev; /* id of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* file type and mode */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user id of owner */ gid_t st_gid; /* group id of owner */ dev_t st_rdev; /* device id (if special file) */ off_t st_size; /* total size, in bytes */ blksize_t st_blksize; /* block size for filesystem i/o */ blkcnt_t st_blocks; /* number of 512b blocks allocated */ struct timespec st_atim; /* time of last access */ struct timespec st_mtim; /* time of last modification */ struct timespec st_ctim; /* time of last status change */ };	block	373
struct stat { dev_t st_dev; /* id of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* file type and mode */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user id of owner */ gid_t st_gid; /* group id of owner */ dev_t st_rdev; /* device id (if special file) */ off_t st_size; /* total size, in bytes */ blksize_t st_blksize; /* block size for filesystem i/o */ blkcnt_t st_blocks; /* number of 512b blocks allocated */ struct timespec st_atim; /* time of last access */ struct timespec st_mtim; /* time of last modification */ struct timespec st_ctim; /* time of last status change */ };	system	392
struct stat { dev_t st_dev; /* id of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* file type and mode */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user id of owner */ gid_t st_gid; /* group id of owner */ dev_t st_rdev; /* device id (if special file) */ off_t st_size; /* total size, in bytes */ blksize_t st_blksize; /* block size for filesystem i/o */ blkcnt_t st_blocks; /* number of 512b blocks allocated */ struct timespec st_atim; /* time of last access */ struct timespec st_mtim; /* time of last modification */ struct timespec st_ctim; /* time of last status change */ };	type	120
struct stat s; if (0 == stat(name, &s)) { printf("%s ", name); if (s_isdir( s.st_mode)) puts("is a directory"); if (s_isreg( s.st_mode)) puts("is a regular file"); } else { perror("stat failed - are you sure we can read this file’s metadata?"); }	printf	42
12.3 permissions and bits permissions are a key part of the way unix systems provide security in a filesystem. you may have noticed that the st_mode field in struct stat contains more than the file type. it also contains the mode, a description detailing what a user can and can’t do with a given file. there are usually three sets of permissions for any file.	a filesystem	97
12.3 permissions and bits permissions are a key part of the way unix systems provide security in a filesystem. you may have noticed that the st_mode field in struct stat contains more than the file type. it also contains the mode, a description detailing what a user can and can’t do with a given file. there are usually three sets of permissions for any file.	system	69
12.3 permissions and bits permissions are a key part of the way unix systems provide security in a filesystem. you may have noticed that the st_mode field in struct stat contains more than the file type. it also contains the mode, a description detailing what a user can and can’t do with a given file. there are usually three sets of permissions for any file.	type	198
table 12.3: permissions table octal code 755 644	code	36
there are several command line utilities for interacting with a file’s mode. mknod changes the type of the file.	type	95
12.3.1 user id / group id every user in a unix system has a user id. this is a unique number that can identify a user. similarly, users can be added to collections called groups, and every group also has a unique identifying number. groups have a variety of uses on unix systems. they can be assigned capabilities - a way of describing the level of control a user has over a system. for example, a group you may have run into is the sudoers group, a set of trusted users who are allowed to use the command sudo to temporarily gain higher privileges. we’ll talk more about how sudo works in this chapter. every file, upon creation, an owner, the creator of the file. this owner’s user id (uid) can be found inside the st_mode file of a struct stat with a call to stat. similarly, the group id (gid) is set as well.	a struct	733
12.3.1 user id / group id every user in a unix system has a user id. this is a unique number that can identify a user. similarly, users can be added to collections called groups, and every group also has a unique identifying number. groups have a variety of uses on unix systems. they can be assigned capabilities - a way of describing the level of control a user has over a system. for example, a group you may have run into is the sudoers group, a set of trusted users who are allowed to use the command sudo to temporarily gain higher privileges. we’ll talk more about how sudo works in this chapter. every file, upon creation, an owner, the creator of the file. this owner’s user id (uid) can be found inside the st_mode file of a struct stat with a call to stat. similarly, the group id (gid) is set as well.	system	47
every process can determine its uid and gid with getuid and getgid. when a process tries to open a file with a specific mode, it’s uid and gid are compared with the uid and gid of the file. if the uids match, then the process’s request to open the file will be compared with the bits on the user field of the file’s permissions. if the gids match, then the process’s request will be compared with the group field of the permissions. if none of the ids match, then the other field will apply.	a process	73
12.3.2 reading / changing file permissions before we discuss how to change permission bits, we should be able to read them. in c, the stat family of library calls can be used. to read permission bits from the command line, use ls -l. note, the permissions will output in the format ‘trwxrwxrwx’. the first character indicates the type of file type. possible values for the first character include but aren’t limited to.	type	330
1. (-) regular file 2. (d) directory 3. (c) character device file 4. (l) symbolic link 5. (p) named pipe (also called fifo) 6. (b) block device 7. (s) socket alternatively, use the program stat which presents all the information that one could retrieve from the stat library call.	block	131
to change the permission bits, there is a system call, int chmod(const char *path, mode_t mode);. to simplify our examples, we will be using the command line utility of the same name chmod short of “change mode”.	system	42
to change the permission bits, there is a system call, int chmod(const char *path, mode_t mode);. to simplify our examples, we will be using the command line utility of the same name chmod short of “change mode”.	a system call	40
there are two common ways to use chmod, with either an octal value or with a symbolic string.	string	86
the base-8 (‘octal’) digits describe the permissions for each role: the user who owns the file, the group and everyone else. the octal number is the sum of three values given to the three types of permission: read(4), write(2), execute(1) example: chmod 755 myfile 1. r + w + x = digit * user has 4+2+1, full permission 2. group has 4+0+1, read and execute permission 3. all users have 4+0+1, read and execute permission	type	188
as a code example, suppose a new file is created with open() and mode bits 666 (write and read bits for user, group and other):	code	5
sudo is usually a program that is owned by the root user - a user that has all capabilities. by using sudo, an otherwise unprivileged user can gain access to most parts of the system. this is useful for running programs that may require elevated privileges, such as using chown to change ownership of a file, or to use mount to mount or unmount filesystems (an action we will discuss later in this chapter). here are some examples:	system	176
when executing a process with the setuid bit, it is still possible to determine a user’s original uid with getuid.	a process	15
 getuid returns the real user id (zero if logged in as root)  geteuid returns the effective user id (zero if acting as root, e.g. due to the setuid flag set on a program) these functions can allow one to write a program that can only be run by a privileged user by checking geteuid or go a step further and ensure that the only user who can run the code is root by using getuid.	code	349
12.4 virtual filesystems and other filesystems posix systems, such as linux and mac os x (which is based on bsd) include several virtual filesystems that are mounted (available) as part of the file-system. files inside these virtual filesystems may be generated dynamically or stored in ram. linux provides 3 main virtual filesystems.	system	17
table 12.4: virtual filesystem list use case a list of physical and virtual devices (for example network card, cdrom, random number generator a list of resources used by each process and (by tradition) set of system information an organized list of internal kernel entities	resources	152
table 12.4: virtual filesystem list use case a list of physical and virtual devices (for example network card, cdrom, random number generator a list of resources used by each process and (by tradition) set of system information an organized list of internal kernel entities	system	24
12.4.1 managing files and filesystems given the multitude of operations that are available to you from the filesystem, let’s explore some tools and techniques that can be used to manage files and filesystems.	system	30
12.4.2 obtaining random data /dev/random is a file that contains a random number generator where the entropy is determined from environmental noise. random will block/wait until enough entropy is collected from the environment.	block	161
/dev/urandom is like random, but differs in the fact that it allows for repetition (lower entropy threshold), thus won’t block.	block	121
one can think of both of these as streams of characters from which a program can read as opposed to files with a start and end. to touch on a misconception, most of the time one should be using /dev/urandom. the only specific use case of /dev/random is when one needs cryptographically secure data on bootup and the system should block. otherwise, there are the following reasons.	the following	358
one can think of both of these as streams of characters from which a program can read as opposed to files with a start and end. to touch on a misconception, most of the time one should be using /dev/urandom. the only specific use case of /dev/random is when one needs cryptographically secure data on bootup and the system should block. otherwise, there are the following reasons.	block	330
one can think of both of these as streams of characters from which a program can read as opposed to files with a start and end. to touch on a misconception, most of the time one should be using /dev/urandom. the only specific use case of /dev/random is when one needs cryptographically secure data on bootup and the system should block. otherwise, there are the following reasons.	system	316
2. /dev/random may block at an inconvenient time. if one is programming a service for high scalability and relies on /dev/random, an attacker can reliably exhaust the entropy pool and cause the service to block.	block	19
4. some operating system don’t have a true /dev/random like macos.	system	18
12.4.3 copying files use the versatile dd command. for example, the following command copies 1 mib of data from the file /dev/urandom to the file /dev/null. the data is copied as 1024 blocks of block size 1024 bytes.	the following	64
12.4.3 copying files use the versatile dd command. for example, the following command copies 1 mib of data from the file /dev/urandom to the file /dev/null. the data is copied as 1024 blocks of block size 1024 bytes.	block	184
dd is also commonly used to make a copy of a disk or an entire filesystem to create images that can either be burned on to other disks or to distribute data to other users.	system	67
$ umask 077 # all future new files will mask out all r,w,x bits for group and other access $ touch file123 # create a file if it non-existant, and update its modified time $ stat file123 file: ‘file123’ size: 0 blocks: 0 io block: 65536 regular empty file device: 21h/33d inode: 226148 links: 1 access: (0600/-rw-------) uid: (395606/ angrave) gid: (61019/ ews) access: 2014-11-12 13:42:06.000000000 -0600 modify: 2014-11-12 13:42:06.001787000 -0600 change: 2014-11-12 13:42:06.001787000 -0600	block	211
12.4.5 managing filesystems to manage filesystems on your machine, use mount. using mount without any options generates a list (one filesystem per line) of mounted filesystems including networked, virtual and local (spinning disk / ssd-based) filesystems. here is a typical output of mount	system	20
$ mount /dev/mapper/cs241--server_sys-root on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0") /dev/sda1 on /boot type ext3 (rw) /dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw) /dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw) /dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind) /srv/software/mathematica-8.0 on /software/mathematica-8.0 type none (rw,bind) engr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)	system	219
$ mount /dev/mapper/cs241--server_sys-root on / type ext4 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) devpts on /dev/pts type devpts (rw,gid=5,mode=620) tmpfs on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0") /dev/sda1 on /boot type ext3 (rw) /dev/mapper/cs241--server_sys-srv on /srv type ext4 (rw) /dev/mapper/cs241--server_sys-tmp on /tmp type ext4 (rw) /dev/mapper/cs241--server_sys-var on /var type ext4 (rw)rw,bind) /srv/software/mathematica-8.0 on /software/mathematica-8.0 type none (rw,bind) engr-ews-homes.engr.illinois.edu:/fs1-homes/angrave/linux on /home/angrave type nfs (rw,soft,intr,tcp,noacl,acregmin=30,vers=3,sec=sys,sloppy,addr=128.174.252.102)	type	48
notice that each line includes the filesystem type source of the filesystem and mount point. to reduce this output, we can pipe it into grep and only see lines that match a regular expression.	system	39
notice that each line includes the filesystem type source of the filesystem and mount point. to reduce this output, we can pipe it into grep and only see lines that match a regular expression.	type	46
>mount | grep proc # only see lines that contain ’proc’ proc on /proc type proc (rw) none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)	type	70
filesystem mounting suppose you had downloaded a bootable linux disk image from the arch linux download page	system	4
before putting the filesystem on a cd, we can mount the file as a filesystem and explore its contents. note: mount requires root access, so let’s run it using sudo	a filesystem	64
before putting the filesystem on a cd, we can mount the file as a filesystem and explore its contents. note: mount requires root access, so let’s run it using sudo	system	23
arch/ will be drawn from the files and directories stored in the filesystem stored inside the archlinux-2014.11.01-dual.iso file. the loop option is required because we want to mount a regular file, not a block device such as a physical disk.	block	205
arch/ will be drawn from the files and directories stored in the filesystem stored inside the archlinux-2014.11.01-dual.iso file. the loop option is required because we want to mount a regular file, not a block device such as a physical disk.	system	69
arch/ will be drawn from the files and directories stored in the filesystem stored inside the archlinux-2014.11.01-dual.iso file. the loop option is required because we want to mount a regular file, not a block device such as a physical disk.	the loop	130
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.	block	45
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.	parameter	242
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.	system	113
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.	type	178
the loop option wraps the original file as a block device. in this example, we will find out below that the file system is provided under /dev/loop0. we can check the filesystem type and mount options by running the mount command without any parameters. we will pipe the output into grep so that we only see the relevant output line(s) that contain ‘arch’.	the loop	0
$ mount | grep arch /home/demo/archlinux-2014.11.01-dual.iso on /home/demo/arch type iso9660 (rw,loop=/dev/loop0)	type	80
the iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. cdroms).	the iso9660 filesystem	0
the iso9660 filesystem is a read-only filesystem originally designed for optical storage media (i.e. cdroms).	system	16
attempting to change the contents of the filesystem will fail	system	45
$ touch arch/nocando touch: cannot touch ‘/home/demo/arch/nocando’: read-only file system	system	83
12.5 memory mapped io while we traditionally think of reading and writing from a file as an operation that happens by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for ipc, and you can see more about mmap as a system call that enables shared memory in the ipc chapter. in this chapter, we’ll briefly explore mmap as a filesystem operation.	a filesystem	388
12.5 memory mapped io while we traditionally think of reading and writing from a file as an operation that happens by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for ipc, and you can see more about mmap as a system call that enables shared memory in the ipc chapter. in this chapter, we’ll briefly explore mmap as a filesystem operation.	mmap	208
12.5 memory mapped io while we traditionally think of reading and writing from a file as an operation that happens by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for ipc, and you can see more about mmap as a system call that enables shared memory in the ipc chapter. in this chapter, we’ll briefly explore mmap as a filesystem operation.	memory	5
12.5 memory mapped io while we traditionally think of reading and writing from a file as an operation that happens by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for ipc, and you can see more about mmap as a system call that enables shared memory in the ipc chapter. in this chapter, we’ll briefly explore mmap as a filesystem operation.	system	282
12.5 memory mapped io while we traditionally think of reading and writing from a file as an operation that happens by using the read and write calls, there is an alternative, mapping a file into memory using mmap. mmap can also be used for ipc, and you can see more about mmap as a system call that enables shared memory in the ipc chapter. in this chapter, we’ll briefly explore mmap as a filesystem operation.	a system call	280
mmap takes a file and maps its contents into memory. this allows a user to treat the entire file as a buffer in memory for easier semantics while programming, and to avoid having to read a file as discrete chunks explicitly.	mmap	0
mmap takes a file and maps its contents into memory. this allows a user to treat the entire file as a buffer in memory for easier semantics while programming, and to avoid having to read a file as discrete chunks explicitly.	memory	45
not all filesystems support using mmap for io. those that do have varying behavior. some will simply implement mmap as a wrapper around read and write. others will add additional optimizations by taking advantage of the kernel’s page cache. of course, such optimization can be used in the implementation of read and write as well, so often using mmap has identical performance.	mmap	34
not all filesystems support using mmap for io. those that do have varying behavior. some will simply implement mmap as a wrapper around read and write. others will add additional optimizations by taking advantage of the kernel’s page cache. of course, such optimization can be used in the implementation of read and write as well, so often using mmap has identical performance.	system	12
not all filesystems support using mmap for io. those that do have varying behavior. some will simply implement mmap as a wrapper around read and write. others will add additional optimizations by taking advantage of the kernel’s page cache. of course, such optimization can be used in the implementation of read and write as well, so often using mmap has identical performance.	optimizations	179
mmap is used to perform some operations such as loading libraries and processes into memory. if many programs only need read-access to the same file, then the same physical memory can be shared between multiple processes. this is used for common libraries like the c standard library.	mmap	0
mmap is used to perform some operations such as loading libraries and processes into memory. if many programs only need read-access to the same file, then the same physical memory can be shared between multiple processes. this is used for common libraries like the c standard library.	memory	85
the process to map a file into memory is as follows.	memory	31
1. mmap requires a file descriptor, so we need to open the file first 2. we seek to our desired size and write one byte to ensure that the file is sufficient length 3. when finished call munmap to unmap the file from memory.	mmap	3
1. mmap requires a file descriptor, so we need to open the file first 2. we seek to our desired size and write one byte to ensure that the file is sufficient length 3. when finished call munmap to unmap the file from memory.	memory	217
<stdio.h> <stdlib.h> <sys/types.h> <sys/stat.h> <sys/mman.h> <fcntl.h> <unistd.h> <errno.h> <string.h>	string	93
<stdio.h> <stdlib.h> <sys/types.h> <sys/stat.h> <sys/mman.h> <fcntl.h> <unistd.h> <errno.h> <string.h>	type	26
lseek(fd, size, seek_set); write(fd, "a", 1); void *addr = mmap(0, size, prot_read | prot_write, map_shared, fd, 0); printf("mapped at %p\n", addr); if (addr == (void*) -1 ) quit; int *array = addr; array[0] = 0x12345678; array[1] = 0xdeadc0de; munmap(addr,size); return 0; }	mmap	59
lseek(fd, size, seek_set); write(fd, "a", 1); void *addr = mmap(0, size, prot_read | prot_write, map_shared, fd, 0); printf("mapped at %p\n", addr); if (addr == (void*) -1 ) quit; int *array = addr; array[0] = 0x12345678; array[1] = 0xdeadc0de; munmap(addr,size); return 0; }	printf	117
the prot_read | prot_write options specify the virtual memory protection. the option prot_exec (not used here) can be set to allow cpu execution of instructions in memory.	memory	55
12.6 reliable single disk filesystems most filesystems cache significant amounts of disk data in physical memory. linux, in this respect, is extreme. all unused memory is used as a giant disk cache. the disk cache can have a significant impact on overall system performance because disk i/o is slow. this is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.	memory	106
12.6 reliable single disk filesystems most filesystems cache significant amounts of disk data in physical memory. linux, in this respect, is extreme. all unused memory is used as a giant disk cache. the disk cache can have a significant impact on overall system performance because disk i/o is slow. this is especially true for random access requests on spinning disks where the disk read-write latency is dominated by the seek time required to move the read-write disk head to the correct position.	system	30
for efficiency, the kernel caches recently used disk blocks. for writing, we have to choose a trade-off between performance and reliability. disk writes can also be cached (“write-back cache”) where modified disk blocks are stored in memory until evicted. alternatively, a ‘write-through cache’ policy can be employed where disk writes are sent immediately to the disk. the latter is safer as filesystem modifications are quickly stored to persistent media but slower than a write-back cache. if writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block. note, this is a simplified description because solid state drives (ssds) can be used as a secondary write-back cache.	memory	234
for efficiency, the kernel caches recently used disk blocks. for writing, we have to choose a trade-off between performance and reliability. disk writes can also be cached (“write-back cache”) where modified disk blocks are stored in memory until evicted. alternatively, a ‘write-through cache’ policy can be employed where disk writes are sent immediately to the disk. the latter is safer as filesystem modifications are quickly stored to persistent media but slower than a write-back cache. if writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block. note, this is a simplified description because solid state drives (ssds) can be used as a secondary write-back cache.	block	53
for efficiency, the kernel caches recently used disk blocks. for writing, we have to choose a trade-off between performance and reliability. disk writes can also be cached (“write-back cache”) where modified disk blocks are stored in memory until evicted. alternatively, a ‘write-through cache’ policy can be employed where disk writes are sent immediately to the disk. the latter is safer as filesystem modifications are quickly stored to persistent media but slower than a write-back cache. if writes are cached then they can be delayed and efficiently scheduled based on the physical position of each disk block. note, this is a simplified description because solid state drives (ssds) can be used as a secondary write-back cache.	system	397
both solid state disks (ssd) and spinning disks have improved performance when reading or writing sequential data. thus, operating systems can often use a read-ahead strategy to amortize the read-request costs and request several contiguous disk blocks per request. by issuing an i/o request for the next disk block before the user application requires the next disk block, the apparent disk i/o latency can be reduced.	block	246
both solid state disks (ssd) and spinning disks have improved performance when reading or writing sequential data. thus, operating systems can often use a read-ahead strategy to amortize the read-request costs and request several contiguous disk blocks per request. by issuing an i/o request for the next disk block before the user application requires the next disk block, the apparent disk i/o latency can be reduced.	system	131
12.6.1 raid - redundant array of inexpensive disks one way to protect against this is to store the data twice! this is the main principle of a “raid-1” disk array. by duplicating the writes to a disk with writes to another backup disk, there are exactly two copies of the data. if one disk fails, the other disk serves as the only copy until it can be re-cloned. reading data is faster since data can be requested from either disk, but writes are potentially twice as slow because now two write commands need to be issued for every disk block write. compared to using a single disk, the cost of storage per byte has doubled.	block	537
it is also common to combine these systems. if you have a lot of hard disks, consider raid-10. this is where you have two systems of raid-1, but the systems are hooked up in raid-0 to each other. this means you would get roughly the same speed from the slowdowns but now any one disk can fail and you can recover that disk. if two disks from opposing raid partitions fail, there is a chance that you can recover though we don’t could on it most of the time.	system	35
12.6.2 higher levels of raid raid-3 uses parity codes instead of mirroring the data. for each n-bits written, we will write one extra bit, the ‘parity bit’ that ensures the total number of 1s written is even. the parity bit is written to an additional disk. if any disk including the parity disk is lost, then its contents can still be computed using the contents of the other disks.	code	48
one disadvantage of raid-3 is that whenever a disk block is written, the parity block will always be written too. this means that there is effectively a bottleneck in a separate disk. in practice, this is more likely to cause a failure because one disk is being used 100% of the time and once that disk fails then the other disks are more prone to failure.	block	51
raid-5 is similar to raid-3 except that the check block (parity information) is assigned to different disks for different blocks. the check-block is ‘rotated’ through the disk array. raid-5 provides better read and write performance than raid-3 because there is no longer the bottleneck of the single parity disk. the one drawback is that you need more disks to have this setup, and there are more complicated algorithms that need to be used.	block	50
12.6.3 solutions simple redundancy (2 or 3 copies of each file) e.g., google gfs (2001). more efficient redundancy (analogous to raid 3++) e.g., google colossus filesystem (~2010): customizable replication including reed-solomon codes with 1.5x redundancy	code	229
12.6.3 solutions simple redundancy (2 or 3 copies of each file) e.g., google gfs (2001). more efficient redundancy (analogous to raid 3++) e.g., google colossus filesystem (~2010): customizable replication including reed-solomon codes with 1.5x redundancy	system	165
12.7 simple filesystem model software developers need to implement filesystems all the time. if that is surprising to you, we encourage you to take a look at hadoop, glusterfs, qumulo, etc. filesystems are hot areas of research as of 2018 because people have realized that the software models that we have devised don’t take full advantage of our current hardware.	system	16
12.7 simple filesystem model software developers need to implement filesystems all the time. if that is surprising to you, we encourage you to take a look at hadoop, glusterfs, qumulo, etc. filesystems are hot areas of research as of 2018 because people have realized that the software models that we have devised don’t take full advantage of our current hardware.	hadoop	158
additionally, the hardware that we use for storing information is getting better all the time. as such, you may end up designing a filesystem yourself someday. in this section, we will go over one of a fake filesystems and “walk through” some examples of how things work.	this section	163
additionally, the hardware that we use for storing information is getting better all the time. as such, you may end up designing a filesystem yourself someday. in this section, we will go over one of a fake filesystems and “walk through” some examples of how things work.	a filesystem	129
additionally, the hardware that we use for storing information is getting better all the time. as such, you may end up designing a filesystem yourself someday. in this section, we will go over one of a fake filesystems and “walk through” some examples of how things work.	system	135
additionally, the hardware that we use for storing information is getting better all the time. as such, you may end up designing a filesystem yourself someday. in this section, we will go over one of a fake filesystems and “walk through” some examples of how things work.	section	168
so, what does our hypothetical filesystem look like? we will base it off of the minixfs, a simple filesystem that happens to be the first filesystem that linux ran on. it is laid out sequentially on disk, and the first section is the superblock. the superblock stores important metadata about the entire filesystem. since we want to be able to read this block before we know anything else about the data on disk, this needs to be in a well-known location so the start of the disk is a good choice. after the superblock, we’ll keep a map of which inodes are being used. the nth bit is set if the nth inode – 0 being the inode root – is being used. similarly, we store a map recording which data blocks are used. finally, we have an array of inodes followed by the rest of the disk - implicitly partitioned into data blocks. one data block may be identical to the next from the perspective of the hardware components of the disk. thinking about the disk as an array of data blocks is simply something we do so that we have a way to describe where files live on disk.	block	239
so, what does our hypothetical filesystem look like? we will base it off of the minixfs, a simple filesystem that happens to be the first filesystem that linux ran on. it is laid out sequentially on disk, and the first section is the superblock. the superblock stores important metadata about the entire filesystem. since we want to be able to read this block before we know anything else about the data on disk, this needs to be in a well-known location so the start of the disk is a good choice. after the superblock, we’ll keep a map of which inodes are being used. the nth bit is set if the nth inode – 0 being the inode root – is being used. similarly, we store a map recording which data blocks are used. finally, we have an array of inodes followed by the rest of the disk - implicitly partitioned into data blocks. one data block may be identical to the next from the perspective of the hardware components of the disk. thinking about the disk as an array of data blocks is simply something we do so that we have a way to describe where files live on disk.	system	35
so, what does our hypothetical filesystem look like? we will base it off of the minixfs, a simple filesystem that happens to be the first filesystem that linux ran on. it is laid out sequentially on disk, and the first section is the superblock. the superblock stores important metadata about the entire filesystem. since we want to be able to read this block before we know anything else about the data on disk, this needs to be in a well-known location so the start of the disk is a good choice. after the superblock, we’ll keep a map of which inodes are being used. the nth bit is set if the nth inode – 0 being the inode root – is being used. similarly, we store a map recording which data blocks are used. finally, we have an array of inodes followed by the rest of the disk - implicitly partitioned into data blocks. one data block may be identical to the next from the perspective of the hardware components of the disk. thinking about the disk as an array of data blocks is simply something we do so that we have a way to describe where files live on disk.	section	219
below, we have an example of how an inode that describes a file may look. note that for the sake of simplicity, we have drawn arrows mapping data block numbers in the inode to their locations on disk. these aren’t pointers so much as indices into an array.	block	146
below, we have an example of how an inode that describes a file may look. note that for the sake of simplicity, we have drawn arrows mapping data block numbers in the inode to their locations on disk. these aren’t pointers so much as indices into an array.	pointer	214
figure 12.1: sample file filling up we will assume that a data block is 4 kib.	block	63
note that a file will fill up each of its data blocks completely before requesting an additional data block. we will refer to this property as the file being compact. the file presented above is interesting since it uses all of its direct blocks, one of the entries for its indirect block and partially uses another indirect block.	block	47
the following subsections will all refer to the file presented above.	the following	0
the following subsections will all refer to the file presented above.	section	17
12.7.1 file size vs space on disk our file’s size must be stored in the inode. the filesystem isn’t aware of the actual contents of what is in a file that data is considered the user’s and should only be manipulated by the user. however, we can compute upper and lower bounds on the filesize by only looking at how many blocks the file uses.	block	320
12.7.1 file size vs space on disk our file’s size must be stored in the inode. the filesystem isn’t aware of the actual contents of what is in a file that data is considered the user’s and should only be manipulated by the user. however, we can compute upper and lower bounds on the filesize by only looking at how many blocks the file uses.	system	87
there are two full direct blocks, which together store 2 ∗ sizeo f (dat a_block) = 2 ∗ 4k ib = 8k ib.	block	26
there are two used blocks referenced by the indirect block, which can store up to 8k ib as calculated above.	block	19
what about a lower bound? we know that we must use the two direct blocks, one block referenced by the indirect block and at least 1 byte of a second block referenced by the indirect block. with this information, we can work out the lower bound to be 2 ∗ 4k ib + 4k ib + 1 = 12k ib + 1b.	block	66
note that our calculations so far have been to determine how much data the user is storing on disk. what about the overhead of storing this data incurred while using this filesystem? you’ll notice that we use an indirect block to store the disk block numbers of blocks used beyond the two direct blocks. while doing our above calculations, we omitted this block. this would instead be counted as the overhead of the file, and thus the total overhead of storing this file on disk is sizeo f (ind i r ec t_bl ock) = 4k ib).	block	221
note that our calculations so far have been to determine how much data the user is storing on disk. what about the overhead of storing this data incurred while using this filesystem? you’ll notice that we use an indirect block to store the disk block numbers of blocks used beyond the two direct blocks. while doing our above calculations, we omitted this block. this would instead be counted as the overhead of the file, and thus the total overhead of storing this file on disk is sizeo f (ind i r ec t_bl ock) = 4k ib).	system	175
thinking about overhead, a related calculation could be to determine the max/min disk usage per file in this filesystem.	system	113
trivially a file of size 0 has no associated data blocks and takes up no space on disk (ignoring the space required for the inode since these are located in a fixed size array somewhere on disk). how about the disk usage of the smallest non-empty file? that is, consider a file of size 1b. note that when a user writes the first byte, a data block will be allocated. since each data block is 4k ib, we find that 4k ib is the minimum disk usage for a non-empty file. here, we observe that the file size will only be 1b, despite that 4k ib of the disk is used – there is a distinction between file size and disk usage because of overhead!	block	50
finding maximum is slightly more involved. as we saw earlier in this chapter, a filesystem with this structure can have 1024 data block numbers in one indirect block. this implies that the maximum filesize can be 2 ∗ 4k ib + 1024 ∗ 4k ib = 4m ib + 8k ib (after accounting for the direct blocks as well). however, on disk we also store the indirect block itself. this means that an additional 4k ib of overhead will be used to account for the indirect block, so the total disk usage will be 4m ib + 12k ib.	a filesystem	78
finding maximum is slightly more involved. as we saw earlier in this chapter, a filesystem with this structure can have 1024 data block numbers in one indirect block. this implies that the maximum filesize can be 2 ∗ 4k ib + 1024 ∗ 4k ib = 4m ib + 8k ib (after accounting for the direct blocks as well). however, on disk we also store the indirect block itself. this means that an additional 4k ib of overhead will be used to account for the indirect block, so the total disk usage will be 4m ib + 12k ib.	block	130
finding maximum is slightly more involved. as we saw earlier in this chapter, a filesystem with this structure can have 1024 data block numbers in one indirect block. this implies that the maximum filesize can be 2 ∗ 4k ib + 1024 ∗ 4k ib = 4m ib + 8k ib (after accounting for the direct blocks as well). however, on disk we also store the indirect block itself. this means that an additional 4k ib of overhead will be used to account for the indirect block, so the total disk usage will be 4m ib + 12k ib.	system	84
note that when only using direct blocks, completely filling up a direct block implies that our filesize and our disk usage are the same thing! while it would seem like we always want this ideal scenario, it puts a restrictive limit on the maximum filesize. attempting to remedy this by increasing the number of direct blocks seems promising, but note that this requires increasing the size of an inode and reducing the amount of space available to store user data – a tradeoff you will have to evaluate for yourself. alternatively always trying to split your data up into chunks that never use indirect blocks is may exhaust the limited pool of available inodes.	block	33
12.7.2 performing reads performing reads tend to be pretty easy in our filesystem because our files are compact. let’s say that we want to read the entirety of this particular file. what we’d start by doing is go to the inode’s direct struct and find the first direct data block number. in our case, it is #7. then we find the 7th data block from the start of all data	block	273
12.7.2 performing reads performing reads tend to be pretty easy in our filesystem because our files are compact. let’s say that we want to read the entirety of this particular file. what we’d start by doing is go to the inode’s direct struct and find the first direct data block number. in our case, it is #7. then we find the 7th data block from the start of all data	system	75
blocks. then we read all of those bytes. we do the same thing for all of the direct nodes. what do we do after?	block	0
we go to the indirect block and read the indirect block. we know that every 4 bytes of the indirect block is either a sentinel node (-1) or the number of another data block. in our particular example, the first four bytes evaluate to the integer 5, meaning that our data continues on the 5th data block from the beginning. we do the same for data block #4 and we stop after because we exceed the size of the inode now, let’s think about the edge cases. how would a program start the read starting at an arbitrary offset of n bytes given that block sizes are 4k ibs. how many indirect blocks should there be if the filesystem is correct?	block	22
we go to the indirect block and read the indirect block. we know that every 4 bytes of the indirect block is either a sentinel node (-1) or the number of another data block. in our particular example, the first four bytes evaluate to the integer 5, meaning that our data continues on the 5th data block from the beginning. we do the same for data block #4 and we stop after because we exceed the size of the inode now, let’s think about the edge cases. how would a program start the read starting at an arbitrary offset of n bytes given that block sizes are 4k ibs. how many indirect blocks should there be if the filesystem is correct?	system	618
12.7.3 performing writes writing to files performing writes fall into two categories, writes to files and writes to directories. first we’ll focus on files and assume that we are writing a byte to the 6th kib of our file. to perform a write on a file at a particular offset, first the filesystem must go to the data block would start at that offset. for this particular example we would have to go to the 2nd or indexed number 1 inode to perform our write. we would once again fetch this number from the inode, go to the root of the data blocks, go to the 5th data block and perform our write at the 2kib offset from this block because we skipped the first four kibibytes of the file in block 7. we perform our write and go on our merry way.	block	316
12.7.3 performing writes writing to files performing writes fall into two categories, writes to files and writes to directories. first we’ll focus on files and assume that we are writing a byte to the 6th kib of our file. to perform a write on a file at a particular offset, first the filesystem must go to the data block would start at that offset. for this particular example we would have to go to the 2nd or indexed number 1 inode to perform our write. we would once again fetch this number from the inode, go to the root of the data blocks, go to the 5th data block and perform our write at the 2kib offset from this block because we skipped the first four kibibytes of the file in block 7. we perform our write and go on our merry way.	system	289
 how would a program perform a write go across data block boundaries?	block	52
writing to directories performing a write to a directory implies that an inode needs to be added to a directory. if we pretend that the example above is a directory. we know that we will be adding at most one directory entry at a time. meaning that we have to have enough space for one directory entry in our data blocks. luckily the last data block that we have has enough free space. this means we need to find the number of the last data block as we did above, go to where the data ends, and write one directory entry. don’t forget to update the size of the directory so that the next creation doesn’t overwrite your file!	block	314
some more questions:  how would would a program perform a write when the last data block is already full?	block	83
 how about when all the direct blocks have been filled up and the inode doesn’t have an indirect block?	block	31
12.7.4 adding deletes if the inode is a file, then remove the directory entry in the parent directory by marking it as invalid (maybe making it point to inode -1) and skip it in your reads. a filesystem decreases the hard link count of the inode and if the count reaches zero, free the inode in the inode map and free all associated data blocks so they are reclaimed by the filesystem. in many operating systems, several fields in the inode get overwritten.	a filesystem	190
12.7.4 adding deletes if the inode is a file, then remove the directory entry in the parent directory by marking it as invalid (maybe making it point to inode -1) and skip it in your reads. a filesystem decreases the hard link count of the inode and if the count reaches zero, free the inode in the inode map and free all associated data blocks so they are reclaimed by the filesystem. in many operating systems, several fields in the inode get overwritten.	block	338
12.7.4 adding deletes if the inode is a file, then remove the directory entry in the parent directory by marking it as invalid (maybe making it point to inode -1) and skip it in your reads. a filesystem decreases the hard link count of the inode and if the count reaches zero, free the inode in the inode map and free all associated data blocks so they are reclaimed by the filesystem. in many operating systems, several fields in the inode get overwritten.	system	196
if the inode is a directory, the filesystem checks if it is empty. if not, then the kernel will most likely mark an error.	system	37
be sure to check out the appendix for modern and cutting edge filesystems.	system	66
12.8 topics  superblock  data block  inode  relative path  file metadata  hard and soft links  permission bits  mode bits  working with directories  virtual file system  reliable file systems  raid	block	18
12.8 topics  superblock  data block  inode  relative path  file metadata  hard and soft links  permission bits  mode bits  working with directories  virtual file system  reliable file systems  raid	system	162
12.9 questions  how big can files be on a file system with 15 direct blocks, 2 double, 3 triple indirect, 4kb blocks and 4byte entries? (assume enough infinite blocks)  what is a superblock? inode? data block?	block	69
12.9 questions  how big can files be on a file system with 15 direct blocks, 2 double, 3 triple indirect, 4kb blocks and 4byte entries? (assume enough infinite blocks)  what is a superblock? inode? data block?	system	47
for those of you with an architecture background, the interrupts used here aren’t the interrupts generated by the hardware. those interrupts are almost always handled by the kernel because they require higher levels of privileges. instead, we are talking about software interrupts that are generated by the kernel – though they can be in response to a hardware event like sigsegv.	background	38
this chapter will go over how to read information from a process that has either exited or been signaled. then, it will deep dive into what are signals, how does the kernel deal with a signal, and the various ways processes can handle signals both with and without threads.	thread	265
this chapter will go over how to read information from a process that has either exited or been signaled. then, it will deep dive into what are signals, how does the kernel deal with a signal, and the various ways processes can handle signals both with and without threads.	a process	55
first, a bit of terminology. a signal disposition is a per-process attribute that determines how a signal is handled after it is delivered. think of it as a table of signal-action pairs. the full discussion is in the man page.	disposition	38
the actions are 1. term, terminates the process 2. ign, ignore 3. core, generate a core dump 4. stop, stops a process 5. cont, continues a process	a process	108
3. the time between when a signal is generated and the kernel can apply the mask rules is called the pending state.	the mask	72
4. then the kernel then checks the process’ signal mask. if the mask says all the threads in a process are blocking the signal, then the signal is currently blocked and nothing happens until a thread unblocks it.	thread	82
4. then the kernel then checks the process’ signal mask. if the mask says all the threads in a process are blocking the signal, then the signal is currently blocked and nothing happens until a thread unblocks it.	the mask	60
4. then the kernel then checks the process’ signal mask. if the mask says all the threads in a process are blocking the signal, then the signal is currently blocked and nothing happens until a thread unblocks it.	a process	93
4. then the kernel then checks the process’ signal mask. if the mask says all the threads in a process are blocking the signal, then the signal is currently blocked and nothing happens until a thread unblocks it.	block	107
4. then the kernel then checks the process’ signal mask. if the mask says all the threads in a process are blocking the signal, then the signal is currently blocked and nothing happens until a thread unblocks it.	a thread	191
5. if a single thread can accept the signal, then the kernel executes the action in the disposition table. if the action is a default action, then no threads need to be paused.	thread	15
5. if a single thread can accept the signal, then the kernel executes the action in the disposition table. if the action is a default action, then no threads need to be paused.	disposition	88
6. otherwise, the kernel delivers the signal by stopping whatever a particular thread is doing currently, and jumps that thread to the signal handler. the signal is now in the delivered phase. more signals can be generated now, but they can’t be delivered until the signal handler is complete which is when the delivered phase is over.	thread	79
blocked	block	0
one of our favorite anecdotes is to never use kill -9 for a host of reasons. the following is an excerpt from useless use of kill -9 link to archive no no no. don’t use kill -9.	the following	77
name usual use sigint stop a process nicely sigquit stop a process harshly sigterm stop a process even more harshly sigstop suspends a process sigcont starts after a stop sigkill you want the process gone	a process	27
continues a process	a process	10
2. the system can send an event. for example, if a process accesses a page that it isn’t supposed to, the hardware generates an interrupt which gets intercepted by the kernel. the kernel finds the process that caused this and sends a software interrupt signal sigsegv. there are other kernel events like a child being created or a process needs to be resumed.	a page	68
2. the system can send an event. for example, if a process accesses a page that it isn’t supposed to, the hardware generates an interrupt which gets intercepted by the kernel. the kernel finds the process that caused this and sends a software interrupt signal sigsegv. there are other kernel events like a child being created or a process needs to be resumed.	a process	49
2. the system can send an event. for example, if a process accesses a page that it isn’t supposed to, the hardware generates an interrupt which gets intercepted by the kernel. the kernel finds the process that caused this and sends a software interrupt signal sigsegv. there are other kernel events like a child being created or a process needs to be resumed.	system	7
you or another process can temporarily pause a running process by sending it a sigstop signal. if it succeeds, it will freeze a process. the process will not be allocated any more cpu time. to allow a process to resume execution, send it the sigcont signal. for example, the following is a program that slowly prints a dot every second, up to 59 dots.	the following	271
you or another process can temporarily pause a running process by sending it a sigstop signal. if it succeeds, it will freeze a process. the process will not be allocated any more cpu time. to allow a process to resume execution, send it the sigcont signal. for example, the following is a program that slowly prints a dot every second, up to 59 dots.	a process	126
printf("my pid is %d\n", getpid() ); int i = 60; while(--i) { write(1, ".",1); sleep(1); } write(1, "done!",5); return 0; }	printf	0
we will first start the process in the background (notice the & at the end). then, send it a signal from the shell process by using the kill command.	background	39
as we saw above there is also a kill command available in the shell. another command killall works the exact same way but instead of looking up by pid, it tries to match the name of the process. ps is an important utility that can help you find the pid of a process.	a process	256
send sigkill (terminate the process) kill -sigkill 4409 kill -9 4409 use kill all instead to kill a process by executable name killall -l firefox	a process	98
13.3 handling signals there are strict limitations on the executable code inside a signal handler. most library and system calls are async-signal-unsafe, meaning they may not be used inside a signal handler because they are not re-entrant.	code	69
13.3 handling signals there are strict limitations on the executable code inside a signal handler. most library and system calls are async-signal-unsafe, meaning they may not be used inside a signal handler because they are not re-entrant.	system	116
re-entrant safety means that your function can be frozen at any point and executed again, can you guarantee that your function wouldn’t fail? let’s take the following	the following	153
int func(const char *str) { static char buffer[200]; strncpy(buffer, str, 199); # here is where we get paused printf("%s\n", buffer) }	printf	110
1. we execute (func("hello")) 2. the string gets copied over to the buffer completely (strcmp(buffer, "hello") == 0) 3. a signal is delivered and the function state freezes, we also stop accepting any new signals until after the handler (we do this for convenience) 4. we execute func("world") 5. now (strcmp(buffer, "world") == 0) and the buffer is printed out "world".	string	37
guaranteeing that your functions are signal handler safe can’t be solved by removing shared buffers. you must also think about multithreading and synchronization – what happens when i double lock a mutex? you also have to make sure that each function call is reentrant safe. suppose your original program was interrupted while executing the library code of malloc. the memory structures used by malloc will be inconsistent. calling printf, which uses malloc as part of the signal handler, is unsafe and will result in undefined behavior. a safe way to avoid this behavior is to set a variable and let the program resume operating. the design pattern also helps us in designing programs that can receive signals twice and operate correctly.	thread	132
guaranteeing that your functions are signal handler safe can’t be solved by removing shared buffers. you must also think about multithreading and synchronization – what happens when i double lock a mutex? you also have to make sure that each function call is reentrant safe. suppose your original program was interrupted while executing the library code of malloc. the memory structures used by malloc will be inconsistent. calling printf, which uses malloc as part of the signal handler, is unsafe and will result in undefined behavior. a safe way to avoid this behavior is to set a variable and let the program resume operating. the design pattern also helps us in designing programs that can receive signals twice and operate correctly.	a mutex	196
guaranteeing that your functions are signal handler safe can’t be solved by removing shared buffers. you must also think about multithreading and synchronization – what happens when i double lock a mutex? you also have to make sure that each function call is reentrant safe. suppose your original program was interrupted while executing the library code of malloc. the memory structures used by malloc will be inconsistent. calling printf, which uses malloc as part of the signal handler, is unsafe and will result in undefined behavior. a safe way to avoid this behavior is to set a variable and let the program resume operating. the design pattern also helps us in designing programs that can receive signals twice and operate correctly.	code	349
guaranteeing that your functions are signal handler safe can’t be solved by removing shared buffers. you must also think about multithreading and synchronization – what happens when i double lock a mutex? you also have to make sure that each function call is reentrant safe. suppose your original program was interrupted while executing the library code of malloc. the memory structures used by malloc will be inconsistent. calling printf, which uses malloc as part of the signal handler, is unsafe and will result in undefined behavior. a safe way to avoid this behavior is to set a variable and let the program resume operating. the design pattern also helps us in designing programs that can receive signals twice and operate correctly.	memory	369
guaranteeing that your functions are signal handler safe can’t be solved by removing shared buffers. you must also think about multithreading and synchronization – what happens when i double lock a mutex? you also have to make sure that each function call is reentrant safe. suppose your original program was interrupted while executing the library code of malloc. the memory structures used by malloc will be inconsistent. calling printf, which uses malloc as part of the signal handler, is unsafe and will result in undefined behavior. a safe way to avoid this behavior is to set a variable and let the program resume operating. the design pattern also helps us in designing programs that can receive signals twice and operate correctly.	printf	432
the above code might appear to be correct on paper. however, we need to provide a hint to the compiler and the cpu core that will execute the main() loop. we need to prevent compiler optimization. the expression pleasestop doesn’t get changed in the body of the loop, so some compilers will optimize it to true todo: citation needed. secondly, we need to ensure that the value of pleasestop is uncached using a cpu register and instead always read from and written to main memory. the sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptible operation. it is impossible to read a value that is composed of some new bit values and old bit values.	code	10
the above code might appear to be correct on paper. however, we need to provide a hint to the compiler and the cpu core that will execute the main() loop. we need to prevent compiler optimization. the expression pleasestop doesn’t get changed in the body of the loop, so some compilers will optimize it to true todo: citation needed. secondly, we need to ensure that the value of pleasestop is uncached using a cpu register and instead always read from and written to main memory. the sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptible operation. it is impossible to read a value that is composed of some new bit values and old bit values.	memory	473
the above code might appear to be correct on paper. however, we need to provide a hint to the compiler and the cpu core that will execute the main() loop. we need to prevent compiler optimization. the expression pleasestop doesn’t get changed in the body of the loop, so some compilers will optimize it to true todo: citation needed. secondly, we need to ensure that the value of pleasestop is uncached using a cpu register and instead always read from and written to main memory. the sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptible operation. it is impossible to read a value that is composed of some new bit values and old bit values.	type	498
the above code might appear to be correct on paper. however, we need to provide a hint to the compiler and the cpu core that will execute the main() loop. we need to prevent compiler optimization. the expression pleasestop doesn’t get changed in the body of the loop, so some compilers will optimize it to true todo: citation needed. secondly, we need to ensure that the value of pleasestop is uncached using a cpu register and instead always read from and written to main memory. the sig_atomic_t type implies that all the bits of the variable can be read or modified as an atomic operation - a single uninterruptible operation. it is impossible to read a value that is composed of some new bit values and old bit values.	the loop	258
by specifying pleasestop with the correct type volatile sig_atomic_t, we can write portable code where the main loop will be exited after the signal handler returns. the sig_atomic_t type can be as large as an int on most modern platforms but on embedded systems can be as small as a char and only able to represent (-127 to 127) values.	code	92
by specifying pleasestop with the correct type volatile sig_atomic_t, we can write portable code where the main loop will be exited after the signal handler returns. the sig_atomic_t type can be as large as an int on most modern platforms but on embedded systems can be as small as a char and only able to represent (-127 to 127) values.	system	255
by specifying pleasestop with the correct type volatile sig_atomic_t, we can write portable code where the main loop will be exited after the signal handler returns. the sig_atomic_t type can be as large as an int on most modern platforms but on embedded systems can be as small as a char and only able to represent (-127 to 127) values.	type	42
two examples of this pattern can be found in comp a terminal based 1hz 4bit computer [3]. two boolean flags are used. one to mark the delivery of sigint (ctrl-c), and gracefully shutdown the program, and the other to mark sigwinch signal to detect terminal resize and redraw the entire display.	a terminal	50
you can also choose a handle pending signals asynchronously or synchronously. to install a signal handler to asynchronously handle signals, use sigaction. to synchronously catch a pending signal use sigwait which blocks until a signal is delivered or signalfd which also blocks and provides a file descriptor that can be read() to retrieve pending signals.	block	213
13.3.1 sigaction you should use sigaction instead of signal because it has better defined semantics. signal on different operating system does different things which is bad. sigaction is more portable and is better defined for threads.	thread	227
13.3.1 sigaction you should use sigaction instead of signal because it has better defined semantics. signal on different operating system does different things which is bad. sigaction is more portable and is better defined for threads.	system	131
you can use system call sigaction to set the current handler and disposition for a signal or read the current signal handler for a particular signal.	system	12
you can use system call sigaction to set the current handler and disposition for a signal or read the current signal handler for a particular signal.	disposition	65
suppose you stumble upon legacy code that uses signal. the following snippet installs myhandler as the sigalrm handler.	the following	55
suppose you stumble upon legacy code that uses signal. the following snippet installs myhandler as the sigalrm handler.	code	32
suppose you stumble upon legacy code that uses signal. the following snippet installs myhandler as the sigalrm handler.	snippet	69
the equivalent sigaction code is:	code	25
however, we typically may also set the mask and the flags field. the mask is a temporary signal mask used during the signal handler execution. if the thread serving the signal is interrupted in the middle of a system call, the sa_restart flag will automatically restart some system calls that otherwise would have returned early with eintr error. the latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.	thread	150
however, we typically may also set the mask and the flags field. the mask is a temporary signal mask used during the signal handler execution. if the thread serving the signal is interrupted in the middle of a system call, the sa_restart flag will automatically restart some system calls that otherwise would have returned early with eintr error. the latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.	the mask	35
however, we typically may also set the mask and the flags field. the mask is a temporary signal mask used during the signal handler execution. if the thread serving the signal is interrupted in the middle of a system call, the sa_restart flag will automatically restart some system calls that otherwise would have returned early with eintr error. the latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.	code	392
however, we typically may also set the mask and the flags field. the mask is a temporary signal mask used during the signal handler execution. if the thread serving the signal is interrupted in the middle of a system call, the sa_restart flag will automatically restart some system calls that otherwise would have returned early with eintr error. the latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.	system	210
however, we typically may also set the mask and the flags field. the mask is a temporary signal mask used during the signal handler execution. if the thread serving the signal is interrupted in the middle of a system call, the sa_restart flag will automatically restart some system calls that otherwise would have returned early with eintr error. the latter means we can simplify the rest of code somewhat because a restart loop may no longer be required.	a system call	208
it is often better to have your code check for the error and restart itself due to the selective nature of the flag.	code	32
13.4 blocking signals to block signals use sigprocmask! with sigprocmask you can set the new mask, add new signals to be blocked to the process mask, and unblock currently blocked signals. you can also determine the existing mask (and use it for later) by passing in a non-null value for oldset.	block	5
 sig_block. the set of blocked signals is the union of the current set and the set argument.	block	5
 sig_unblock. the signals in set are removed from the current set of blocked signals. it is permissible to attempt to unblock a signal which is not blocked.	block	7
 sig_setmask. the set of blocked signals is set to the argument set.	block	25
the sigset type behaves as a set. it is a common error to forget to initialize the signal set before adding to the set.	type	11
correct code initializes the set to be all on or all off. for example,	code	8
if you block a signal with either sigprocmask or pthread_sigmask, then the handler registered with sigaction is not delivered unless explicitly sigwait’ed on todo: cite.	thread	50
if you block a signal with either sigprocmask or pthread_sigmask, then the handler registered with sigaction is not delivered unless explicitly sigwait’ed on todo: cite.	block	7
13.4.1 sigwait sigwait can be used to read one pending signal at a time. sigwait is used to synchronously wait for signals, rather than handle them in a callback. a typical use of sigwait in a multi-threaded program is shown below. notice that the thread signal mask is set first (and will be inherited by new threads). the mask prevents signals from being delivered so they will remain in a pending state until sigwait is called. also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is used as the set of signals that sigwait can catch and return.	thread	199
13.4.1 sigwait sigwait can be used to read one pending signal at a time. sigwait is used to synchronously wait for signals, rather than handle them in a callback. a typical use of sigwait in a multi-threaded program is shown below. notice that the thread signal mask is set first (and will be inherited by new threads). the mask prevents signals from being delivered so they will remain in a pending state until sigwait is called. also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is used as the set of signals that sigwait can catch and return.	the mask	320
13.4.1 sigwait sigwait can be used to read one pending signal at a time. sigwait is used to synchronously wait for signals, rather than handle them in a callback. a typical use of sigwait in a multi-threaded program is shown below. notice that the thread signal mask is set first (and will be inherited by new threads). the mask prevents signals from being delivered so they will remain in a pending state until sigwait is called. also notice the same set sigset_t variable is used by sigwait - except rather than setting the set of blocked signals it is used as the set of signals that sigwait can catch and return.	block	533
one advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more c library and system functions safely.	thread	50
one advantage of writing a custom signal handling thread (such as the example below) rather than a callback function is that you can now use many more c library and system functions safely.	system	165
based on sigmask code [2]	code	17
static sigset_t signal_mask; /* signals to block */ int main(int argc, char *argv[]) { pthread_t sig_thr_id; /* signal handler thread id */ sigemptyset (&signal_mask); sigaddset (&signal_mask, sigint); sigaddset (&signal_mask, sigterm); pthread_sigmask (sig_block, &signal_mask, null); /* new threads will inherit this thread’s mask */ pthread_create (&sig_thr_id, null, signal_thread, null); /* application code */ ...	thread	88
static sigset_t signal_mask; /* signals to block */ int main(int argc, char *argv[]) { pthread_t sig_thr_id; /* signal handler thread id */ sigemptyset (&signal_mask); sigaddset (&signal_mask, sigint); sigaddset (&signal_mask, sigterm); pthread_sigmask (sig_block, &signal_mask, null); /* new threads will inherit this thread’s mask */ pthread_create (&sig_thr_id, null, signal_thread, null); /* application code */ ...	code	408
static sigset_t signal_mask; /* signals to block */ int main(int argc, char *argv[]) { pthread_t sig_thr_id; /* signal handler thread id */ sigemptyset (&signal_mask); sigaddset (&signal_mask, sigint); sigaddset (&signal_mask, sigterm); pthread_sigmask (sig_block, &signal_mask, null); /* new threads will inherit this thread’s mask */ pthread_create (&sig_thr_id, null, signal_thread, null); /* application code */ ...	block	43
} void *signal_thread(void *arg) { int sig_caught; /* use the same mask as the set of signals that we’d like to know about! */ sigwait(&signal_mask, &sig_caught); switch (sig_caught) { case sigint: ...	thread	15
break; default: fprintf (stderr, "\nunexpected signal %d\n", sig_caught); break; } }	printf	17
13.5 signals in child processes and threads this is a recap of the processes chapter. after forking, the child process inherits a copy of the parent’s signal dispositions and a copy of the parent’s signal mask. if you have installed a sigint handler before forking, then the child process will also call the handler if a sigint is delivered to the child. if sigint is blocked in the parent, it will be blocked in the child as well. note that pending signals for the child are not inherited during forking.	thread	36
13.5 signals in child processes and threads this is a recap of the processes chapter. after forking, the child process inherits a copy of the parent’s signal dispositions and a copy of the parent’s signal mask. if you have installed a sigint handler before forking, then the child process will also call the handler if a sigint is delivered to the child. if sigint is blocked in the parent, it will be blocked in the child as well. note that pending signals for the child are not inherited during forking.	block	368
13.5 signals in child processes and threads this is a recap of the processes chapter. after forking, the child process inherits a copy of the parent’s signal dispositions and a copy of the parent’s signal mask. if you have installed a sigint handler before forking, then the child process will also call the handler if a sigint is delivered to the child. if sigint is blocked in the parent, it will be blocked in the child as well. note that pending signals for the child are not inherited during forking.	disposition	158
after exec though, only the signal mask and pending signals are carried over [1]. signal handlers are reset to their original action, because the original handler code may have disappeared along with the old process.	code	163
each thread has its own mask. a new thread inherits a copy of the calling thread’s mask. on initialization, the calling thread’s mask is the exact same as the processes mask. after a new thread is created though, the processes signal mask turns into a gray area. instead, the kernel likes to treat the process as a collection of threads, each of which can institute a signal mask and receive signals. to start setting your mask, you can use,	thread	5
blocking signals is similar in multi-threaded programs to single-threaded programs with the following translation.	the following	88
blocking signals is similar in multi-threaded programs to single-threaded programs with the following translation.	thread	37
blocking signals is similar in multi-threaded programs to single-threaded programs with the following translation.	block	0
1. use pthread_sigmask instead of sigprocmask 2. block a signal in all threads to prevent its asynchronous delivery the easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created.	thread	8
1. use pthread_sigmask instead of sigprocmask 2. block a signal in all threads to prevent its asynchronous delivery the easiest method to ensure a signal is blocked in all threads is to set the signal mask in the main thread before new threads are created.	block	49
just as we saw with sigprocmask, pthread_sigmask includes a ‘how’ parameter that defines how the signal set is to be used:	thread	34
just as we saw with sigprocmask, pthread_sigmask includes a ‘how’ parameter that defines how the signal set is to be used:	parameter	66
pthread_sigmask(sig_setmask, &set, null) - replace the thread’s mask with given signal set pthread_sigmask(sig_block, &set, null) - add the signal set to the thread’s mask pthread_sigmask(sig_unblock, &set, null) - remove the signal set from the thread’s mask	thread	1
pthread_sigmask(sig_setmask, &set, null) - replace the thread’s mask with given signal set pthread_sigmask(sig_block, &set, null) - add the signal set to the thread’s mask pthread_sigmask(sig_unblock, &set, null) - remove the signal set from the thread’s mask	block	111
a signal then can be delivered to any signal thread that is willing to accept that signal. if the two or more threads can receive the signal then which thread will be interrupted is arbitrary! a common practice is to have one thread that can receive all signals or if there is a certain signal that requires special logic, have multiple threads for multiple signals. even though programs from the outside can’t send signals to specific threads, you can do that internally with pthread_kill(pthread_t thread, int sig). in the example below, the newly created thread executing func will be interrupted by sigint	thread	45
as a word of warning pthread_kill(threadid, sigkill) will kill the entire process. though individual threads can set a signal mask, the signal disposition is per-process not per-thread. this means sigaction can be called from any thread because you will be setting a signal handler for all threads in the process.	thread	22
as a word of warning pthread_kill(threadid, sigkill) will kill the entire process. though individual threads can set a signal mask, the signal disposition is per-process not per-thread. this means sigaction can be called from any thread because you will be setting a signal handler for all threads in the process.	disposition	143
the linux man pages discuss signal system calls in section 2. there is also a longer article in section 7 (though not in osx/bsd):	system	35
the linux man pages discuss signal system calls in section 2. there is also a longer article in section 7 (though not in osx/bsd):	section	51
13.6 topics  signals  signal handler safety  signal disposition  signal states  pending signals when forking/exec  signal disposition when forking/exec  raising signals in c  raising signals in a multithreaded program	thread	201
13.6 topics  signals  signal handler safety  signal disposition  signal states  pending signals when forking/exec  signal disposition when forking/exec  raising signals in c  raising signals in a multithreaded program	disposition	52
 what is a process signal disposition? how does it differ from a mask?	a process	9
 what is a process signal disposition? how does it differ from a mask?	disposition	26
 what function changes the signal disposition in a single threaded program? how about a multithreaded program?	thread	58
 what function changes the signal disposition in a single threaded program? how about a multithreaded program?	disposition	34
 what happens to pending signals after a fork? exec? how about my signal mask? how about signal disposition?	disposition	96
 what is the process the kernel goes through from creation to delivery/block?	block	71
[2] pthreads i gmask. url.	thread	5
computer security is the protection of hardware and software from unauthorized access or modification. even if you don’t work directly in the computer security field, the concepts are important to learn because all systems will have attackers given enough time. even though this is introduced as a different chapter, it is important to note that most of these concepts and code examples have already been introduced at different points in the course. we won’t go in depth about all of the common ways of attack and defense nor will we go into how to perform all of these attacks in an arbitrary system. our goal is to introduce you to the field of making programs do what you want to do.	code	373
computer security is the protection of hardware and software from unauthorized access or modification. even if you don’t work directly in the computer security field, the concepts are important to learn because all systems will have attackers given enough time. even though this is introduced as a different chapter, it is important to note that most of these concepts and code examples have already been introduced at different points in the course. we won’t go in depth about all of the common ways of attack and defense nor will we go into how to perform all of these attacks in an arbitrary system. our goal is to introduce you to the field of making programs do what you want to do.	system	215
14.1 security terminology and ethics there is some terminology that needs to be explained to get someone who has little to no experience in computer security up to speed 1. an attacker is typically the user who is trying to break into the system. breaking into the system means performing an action that the developer of the system didn’t intend. it could also mean accessing a system you shouldn’t have access to.	system	239
2. a defender is typically the user who is preventing the attacker from breaking into the system. this may be the developer of the system.	system	90
3. there are different types of attackers. there are white hat hackers who attempt to hack a defender with their consent. this is commonly a form of pre-emptive testing – in case a not-so-friendly attack comes along.	type	23
danger will robinson before we let you go much further, it is important that we talk about ethics. before you skip over this section, know that your career quite literally can be terminated over an unethical decision that you might make. the computer fraud and security act is a broad, and arguably terrible law, that casts any 303	this section	120
danger will robinson before we let you go much further, it is important that we talk about ethics. before you skip over this section, know that your career quite literally can be terminated over an unethical decision that you might make. the computer fraud and security act is a broad, and arguably terrible law, that casts any 303	section	125
2. determine whetheryou need to “hack” the system. a hack is defined generally as trying to use a system unintendedly. first, you should determine if your use is intended or unintended or somewhere in the middle – get a decision for them. if you can’t get that, make a reasonable judgement as to what the intended use.	system	43
3. figure out a reasonable estimate of what the cost is to “hacking” the system. get that reasonable estimate checked out with a few engineers so they can highlight things that you may have missed. try to get someone to sign off on the plan.	system	73
14.1.1 cia triad there are three commonly accepted goals to help understand if a system is secure.	system	81
if any of these are broken, the security of a system (either a service or piece of information) has been compromised.	system	46
14.2 security in c programs 14.2.1 stack smashing consider the following code snippet	the following	59
14.2 security in c programs 14.2.1 stack smashing consider the following code snippet	code	73
14.2 security in c programs 14.2.1 stack smashing consider the following code snippet	snippet	78
void greeting(const char *name) { char buf[32]; strcpy(buf, name); printf("hello, %s!\n", buf); } int main(int argc, char *argv[]) { if (argc < 2){ return 1; } greeting(argv[1]); return 0; }	printf	67
there is no checking on the bounds of strcpy! this means that we could potentially pass in a large string and get the program to do something unintended, usually via replacing the return address of the function with the address of malicious code. most strings will cause the program to exit with a segmentation fault	code	241
there is no checking on the bounds of strcpy! this means that we could potentially pass in a large string and get the program to do something unintended, usually via replacing the return address of the function with the address of malicious code. most strings will cause the program to exit with a segmentation fault	address	187
there is no checking on the bounds of strcpy! this means that we could potentially pass in a large string and get the program to do something unintended, usually via replacing the return address of the function with the address of malicious code. most strings will cause the program to exit with a segmentation fault	string	99
if we manipulate the bytes in certain ways and the program was compiled with the correct flags, we can actually get access to a shell! consider if that file is owned by root, we put in some valid bytecode (binary instructions) as the string. what will happen is we’ll try to execute execve(’/bin/sh’, ’/bin/sh’, null , null) that is compiled to the bytecode of the operating system and pass it as part of our string. with some luck, we will get access to a root shell.	code	200
if we manipulate the bytes in certain ways and the program was compiled with the correct flags, we can actually get access to a shell! consider if that file is owned by root, we put in some valid bytecode (binary instructions) as the string. what will happen is we’ll try to execute execve(’/bin/sh’, ’/bin/sh’, null , null) that is compiled to the bytecode of the operating system and pass it as part of our string. with some luck, we will get access to a root shell.	a shell	126
if we manipulate the bytes in certain ways and the program was compiled with the correct flags, we can actually get access to a shell! consider if that file is owned by root, we put in some valid bytecode (binary instructions) as the string. what will happen is we’ll try to execute execve(’/bin/sh’, ’/bin/sh’, null , null) that is compiled to the bytecode of the operating system and pass it as part of our string. with some luck, we will get access to a root shell.	string	234
if we manipulate the bytes in certain ways and the program was compiled with the correct flags, we can actually get access to a shell! consider if that file is owned by root, we put in some valid bytecode (binary instructions) as the string. what will happen is we’ll try to execute execve(’/bin/sh’, ’/bin/sh’, null , null) that is compiled to the bytecode of the operating system and pass it as part of our string. with some luck, we will get access to a root shell.	system	375
the question arises, which parts of the triad does this break? try to answer that question yourself. so how would we go about fixing this? we could ingrain into most programmers at the c level to use strncpy or strlcpy on openbsd systems. turning on stack canaries as explained later will fix this issue as well.	openbsd	222
the question arises, which parts of the triad does this break? try to answer that question yourself. so how would we go about fixing this? we could ingrain into most programmers at the c level to use strncpy or strlcpy on openbsd systems. turning on stack canaries as explained later will fix this issue as well.	system	230
> cat main.c #include <stdio.h> int main() { char out[10]; char in[10]; fscanf(stdin, "%s", in); out[0] = ’a’; out[9] = ’\0’; printf("%s\n", out); return 0; } > gcc main.c -fno-stack-protector # need the special flag otherwise won’t work # stack protectors are explained later.	printf	126
what happens here should be clear if you recall the c memory model. out and in are both next to each other in memory. if you read in a string from standard input that overflows in, then you end up printing aoo. it gets a little more serious if the snippet starts out as	string	135
what happens here should be clear if you recall the c memory model. out and in are both next to each other in memory. if you read in a string from standard input that overflows in, then you end up printing aoo. it gets a little more serious if the snippet starts out as	memory	54
what happens here should be clear if you recall the c memory model. out and in are both next to each other in memory. if you read in a string from standard input that overflows in, then you end up printing aoo. it gets a little more serious if the snippet starts out as	snippet	248
14.2.3 out of order instructions & spectre out of order execution is an amazing development that has been recently adopted by many hardware vendors (think 1990s) todo: citation needed. processors now instead of executing a sequence of instructions (let’s say assigning a variable and then another variable) execute instructions before the current one is done [1, p. 45]. this is because modern processors spend a lot of time waiting for memory accesses and other i/o driven applications.	spectre	35
14.2.3 out of order instructions & spectre out of order execution is an amazing development that has been recently adopted by many hardware vendors (think 1990s) todo: citation needed. processors now instead of executing a sequence of instructions (let’s say assigning a variable and then another variable) execute instructions before the current one is done [1, p. 45]. this is because modern processors spend a lot of time waiting for memory accesses and other i/o driven applications.	memory	437
this means that a processor, while it is waiting for an operation to complete, will execute the next few operations.	a process	16
naturally, this allowed cpus to become more energy-efficient while executing more instructions in real-time and increased security risks from complex architectures. what system programmers are worried about is that operation with mutex locks among threads are out of order – meaning that a pure software implementation of a mutex will fail without copious memory barriers. therefore, the programmer has to acknowledge that updates may be missed among a series of threads, given that there is no barrier, on modern processors.	thread	248
naturally, this allowed cpus to become more energy-efficient while executing more instructions in real-time and increased security risks from complex architectures. what system programmers are worried about is that operation with mutex locks among threads are out of order – meaning that a pure software implementation of a mutex will fail without copious memory barriers. therefore, the programmer has to acknowledge that updates may be missed among a series of threads, given that there is no barrier, on modern processors.	a mutex	322
naturally, this allowed cpus to become more energy-efficient while executing more instructions in real-time and increased security risks from complex architectures. what system programmers are worried about is that operation with mutex locks among threads are out of order – meaning that a pure software implementation of a mutex will fail without copious memory barriers. therefore, the programmer has to acknowledge that updates may be missed among a series of threads, given that there is no barrier, on modern processors.	memory	356
naturally, this allowed cpus to become more energy-efficient while executing more instructions in real-time and increased security risks from complex architectures. what system programmers are worried about is that operation with mutex locks among threads are out of order – meaning that a pure software implementation of a mutex will fail without copious memory barriers. therefore, the programmer has to acknowledge that updates may be missed among a series of threads, given that there is no barrier, on modern processors.	system	170
one of the most prominent bugs concerning this is spectre [2]. spectre is a bug where instructions that otherwise wouldn’t be executed are speculatively executed due to out-of-order instruction execution. the following snippet is a high-level proof of concept.	the following	205
one of the most prominent bugs concerning this is spectre [2]. spectre is a bug where instructions that otherwise wouldn’t be executed are speculatively executed due to out-of-order instruction execution. the following snippet is a high-level proof of concept.	spectre	50
one of the most prominent bugs concerning this is spectre [2]. spectre is a bug where instructions that otherwise wouldn’t be executed are speculatively executed due to out-of-order instruction execution. the following snippet is a high-level proof of concept.	snippet	219
be in a register be in main memory --i, --j) {	memory	28
let’s analyze this code. the first loop allocates 9 elements through a valid malloc. the last element is 0xcafe, meaning a dereference should result in a segfault. for the first 9 iterations, the branch is taken and val is assigned to a valid value. the interesting part happens in the last iteration. the resulting behavior of the program is to skip the last iteration. therefore, val never gets assigned to the last value.	code	19
the processor thinks that the branch will be taken, since it has been taken in the last 9 iterations. as such, the processor will fetch those instructions. due to out-of-order instruction execution, while the value of i is being fetched from memory, we have to force it not to be in a register. then, the processor will try to dereference that address. this should result in a segfault. since the address was never logically reached by the program, the result is discarded.	address	344
the processor thinks that the branch will be taken, since it has been taken in the last 9 iterations. as such, the processor will fetch those instructions. due to out-of-order instruction execution, while the value of i is being fetched from memory, we have to force it not to be in a register. then, the processor will try to dereference that address. this should result in a segfault. since the address was never logically reached by the program, the result is discarded.	memory	242
now here is the trick. even though the value of the calculation would have resulted in a segfault, the bug doesn’t clear the cache that refers to the physical memory where 0xcafe is located. this is an inexact explanation,	memory	159
but essentially how it works. since it is still in the cache, if you again trick the processor to read form the cache using val then you will read a memory value that you wouldn’t be able to read normally. this could include important information such as passwords, payment information, etc.	memory	149
14.2.4 operating systems security 1. permissions. in posix systems, we have permissions everywhere. there are directories that you can and can’t access, files that you can and can’t access. each user account is given access to each file and directory through the read-write-execute (rwx) bits. the user gets matched with either the owner, the group, or ‘everyone else’, and their access to the file is limited using these bits. note that permissions work slightly differently on directories compared to files.	system	17
3. address space layout randomization (aslr). aslr causes the address spaces of important sections of a process, including the base address of the executable and the positions of the stack, heap and libraries, to start at randomized values, on every run. this is so that an attacker with a running executable has to randomly guess where sensitive information could be hidden. for example, an attacker may use this to easily perform a return-to-libc attack.	address	3
3. address space layout randomization (aslr). aslr causes the address spaces of important sections of a process, including the base address of the executable and the positions of the stack, heap and libraries, to start at randomized values, on every run. this is so that an attacker with a running executable has to randomly guess where sensitive information could be hidden. for example, an attacker may use this to easily perform a return-to-libc attack.	a process	102
3. address space layout randomization (aslr). aslr causes the address spaces of important sections of a process, including the base address of the executable and the positions of the stack, heap and libraries, to start at randomized values, on every run. this is so that an attacker with a running executable has to randomly guess where sensitive information could be hidden. for example, an attacker may use this to easily perform a return-to-libc attack.	section	90
5. write xor execute, also known as data execution prevention (dep). this is a protection that was covered in the ipc section that distinguishes code from data. a page can either be written to or executed but not both. this is to prevent buffer overflows where attackers write arbitrary code, often stored on the stack or heap, and execute with the user’s permissions.	code	145
5. write xor execute, also known as data execution prevention (dep). this is a protection that was covered in the ipc section that distinguishes code from data. a page can either be written to or executed but not both. this is to prevent buffer overflows where attackers write arbitrary code, often stored on the stack or heap, and execute with the user’s permissions.	a page	161
5. write xor execute, also known as data execution prevention (dep). this is a protection that was covered in the ipc section that distinguishes code from data. a page can either be written to or executed but not both. this is to prevent buffer overflows where attackers write arbitrary code, often stored on the stack or heap, and execute with the user’s permissions.	section	118
7. apparmor. apparmor is a suite of operating system tools at the userspace level to restrict applications to certain operations.	system	46
2. unveil. unveil is a system call that restricts the access of a current program to a few directories. those permissions apply to all forked programs as well. this means if you have a suspicious executable that you want to run whose description is “creates a new file and outputs random words” one could use this call to restrict access to a safe subdirectory and watch it receive the sigkill signal if it tries to access system files in the root directory, for example. this could be useful for your program as well. if you want to ensure that no user data is lost during an update (which is what happened with a steam system update), then the system could only reveal the program’s installation directory. if an attacker manages to find an exploit in the executable, it can only compromise the installation directory.	system	23
2. unveil. unveil is a system call that restricts the access of a current program to a few directories. those permissions apply to all forked programs as well. this means if you have a suspicious executable that you want to run whose description is “creates a new file and outputs random words” one could use this call to restrict access to a safe subdirectory and watch it receive the sigkill signal if it tries to access system files in the root directory, for example. this could be useful for your program as well. if you want to ensure that no user data is lost during an update (which is what happened with a steam system update), then the system could only reveal the program’s installation directory. if an attacker manages to find an exploit in the executable, it can only compromise the installation directory.	a system call	21
3. sudo. sudo is an openbsd project that runs everywhere! before to run commands as root, one would have to drop to a root shell. some times that would also mean giving users scary system capabilities. sudo gives you access to perform commands as root for one-offs without giving a long list of capabilities to all of your users.	openbsd	20
3. sudo. sudo is an openbsd project that runs everywhere! before to run commands as root, one would have to drop to a root shell. some times that would also mean giving users scary system capabilities. sudo gives you access to perform commands as root for one-offs without giving a long list of capabilities to all of your users.	system	181
one can imagine a single operating system per motherboard. virtualization in the software sense is providing “virtual” motherboard features like usb ports or monitors that another program (the bridge) communicates with the actual hardware to perform a task. a simple example is running a virtual machine on your host desktop!	system	35
one can spin up an entirely different operating system whose instructions are fed through another program and executed on the host system. there are many forms of virtualization that we use today. we will discuss two popular forms below. one form is virtual machines. these programs emulate all forms of motherboard peripherals to create a full machine. another form is containers. virtual machines are good but are often bulky and programs only need a certain level of protection. containers are virtual machines that don’t emulate all motherboard peripherals and instead share with the host operating system, adding in additional layers of security.	system	48
now, you can’t have proper virtualization without security. one of the reasons to have virtualization is to ensure that the virtualized environment doesn’t maliciously leak back into the host environment. we say maliciously because there are intended ways of communication that we want to keep in check. here are some simple examples of security provided through virtualization 1. chroot is a contrived way of creating a virtualization environment. chroot is short for change root. this changes where a program believes that (/) is mounted on the system. for example with chroot, one can make a hello world program believe /home/user/ is actually the root directory. this is useful because no other files are exposed. this is contrived because linux still needs additional tools (think the c standard library) to come from different directories such as /usr/lib which means those could still be vulnerable.	system	547
3. hardware virtualization technology. hardware vendors have become increasingly aware that physical protections are needed when emulating instructions. as such, there can be switches enabled by the user that allows the operating system to flip into a virtualization mode where instructions are run as normal but are monitored for malicious activity. this helps the performance and increases the security of virtualized environments.	system	230
14.3 cyber security cyber security is arguably the most popular area of security. more and more of our systems are hacked over the web, it is important to understand how we can protect against these attacks	system	103
there are no checks or federated databases in place. one just has to trust the dns server gave a reasonable response which is almost always the incorrect answer. apart from systems that have an approved white list or a “secret” connection protocol, there is little at the tcp level that one can do to stop.	system	173
4. syn-flood. before the first synchronization packet is acknowledged, there is no connection. that means a malicious attacker can write a bad tcp implementation that sends out a flood of syn packets to a hapless server. the syn flood is easily mitigated by using iptables or another netfilter module to drop all incoming connections from an ip address after a certain volume of traffic is reached for a certain period.	address	345
5. denial of service, distributed denial of service is the hardest form of attack to stop. companies today are still trying to find good ways to ease these attacks. this involves sending all sorts of network traffic forward to servers in the hopes that the traffic will clog them up and slow down the servers. in big systems, this can lead to cascading failures. if a system is engineered poorly, one server’s failure causes all the other servers to pick up more work which increases the probability that they will fail and so on and so forth.	system	317
3. an attacker gains root access on a linux system that you use to store private information. does this affect confidentiality, integrity, or availability of your information, or all three?	system	44
13. why shouldn’t stack memory be executable.	memory	24
15. meltdown and spectre is an example of what kind of security issue? which one(s) of the triad does it break?	spectre	17
bibliography [1] part guide. intel® 64 and ia-32 architectures software developers manual. volume 3b: system programming guide, part, 2, 2011.	system	102
[2] paul kocher, daniel genkin, daniel gruss, werner haas, mike hamburg, moritz lipp, stefan mangard, thomas prescher, michael schwarz, and yuval yarom. spectre attacks: exploiting speculative execution. arxiv preprint arxiv:1801.01203, 2018.	spectre	153
15.1 c 15.1.1 memory and strings 1. in the example below, which variables are guaranteed to print the value of zero?	string	25
15.1 c 15.1.1 memory and strings 1. in the example below, which variables are guaranteed to print the value of zero?	memory	14
int a; static int b; void func() { static int c; int d; printf("%d %d %d %d\n",a,b,c,d); }	printf	56
printf("%d %d %d %d\n",*ptr1,*ptr2,*ptr3,*ptr4); }	printf	0
3. explain the error in the following attempt to copy a string.	the following	24
3. explain the error in the following attempt to copy a string.	string	56
4. why does the following attempt to copy a string sometimes work and sometimes fail?	the following	12
4. why does the following attempt to copy a string sometimes work and sometimes fail?	string	44
5. explain the two errors in the following code that attempts to copy a string.	the following	29
5. explain the two errors in the following code that attempts to copy a string.	code	43
5. explain the two errors in the following code that attempts to copy a string.	string	72
6. which of the following is legal?	the following	12
7. complete the function pointer typedef to declare a pointer to a function that takes a void* argument and returns a void*. name your type ‘pthread_callback’	thread	142
7. complete the function pointer typedef to declare a pointer to a function that takes a void* argument and returns a void*. name your type ‘pthread_callback’	type	33
7. complete the function pointer typedef to declare a pointer to a function that takes a void* argument and returns a void*. name your type ‘pthread_callback’	pointer	25
typedef ______________________;	type	0
8. in addition to the function arguments what else is stored on a thread’s stack?	thread	66
8. in addition to the function arguments what else is stored on a thread’s stack?	a thread	64
9. implement a version of char* strcat(char*dest, const char*src) using only strcpy strlen and pointer arithmetic	pointer	95
11. identify the three bugs in the following implementation of strcpy.	the following	31
fprintf("you scored 100%");	printf	1
2. complete the following code to print to a file. print the name, a comma and the score to the file ‘result.txt’	the following	12
2. complete the following code to print to a file. print the name, a comma and the score to the file ‘result.txt’	code	26
3. how would you print the values of the variables a, mesg, val and ptr to a string? print a as an integer, mesg as c string, val as a double val and ptr as a hexadecimal pointer. you may assume the mesg points to a short c string(<50 characters). bonus: how would you make this code more robust or able to cope with?	code	279
3. how would you print the values of the variables a, mesg, val and ptr to a string? print a as an integer, mesg as c string, val as a double val and ptr as a hexadecimal pointer. you may assume the mesg points to a short c string(<50 characters). bonus: how would you make this code more robust or able to cope with?	string	77
3. how would you print the values of the variables a, mesg, val and ptr to a string? print a as an integer, mesg as c string, val as a double val and ptr as a hexadecimal pointer. you may assume the mesg points to a short c string(<50 characters). bonus: how would you make this code more robust or able to cope with?	pointer	171
char* tostring(int a, char*mesg, double val, void* ptr) { char* result = malloc( strlen(mesg) + 50); _____ return result; }	string	8
2. write a complete program that uses getline. ensure your program has no memory leaks.	memory	74
4. what mistake did the programmer make in the following code? is it possible to fix it i) using heap memory? ii) using global (static) memory?	the following	43
4. what mistake did the programmer make in the following code? is it possible to fix it i) using heap memory? ii) using global (static) memory?	code	57
4. what mistake did the programmer make in the following code? is it possible to fix it i) using heap memory? ii) using global (static) memory?	memory	102
4. what mistake did the programmer make in the following code? is it possible to fix it i) using heap memory? ii) using global (static) memory?	heap memory	97
static int id; char* next_ticket() { id ++; char result[20]; sprintf(result,"%d",id); return result; }	printf	62
15.2 processes 1. what is a process?	a process	26
2. what attributes are carried over from a process on fork? how about on a successful exec call?	a process	41
4. what is the wait system call used for?	system	20
7. how do we check the status of a process that has exited?	a process	33
15.3 memory 1. what are the calls in c to allocate memory?	memory	5
2. what must malloc memory be aligned to? why is it important?	memory	20
15.4 threading and synchronization 1. what is a thread? what do threads share?	thread	5
15.4 threading and synchronization 1. what is a thread? what do threads share?	a thread	46
2. how does one create a thread?	thread	25
2. how does one create a thread?	a thread	23
3. where are the stacks for a thread located in memory?	thread	30
3. where are the stacks for a thread located in memory?	memory	48
3. where are the stacks for a thread located in memory?	a thread	28
4. what is a mutex? what problem does it solve?	a mutex	11
6. write a thread safe linked list that supports insert front, back, pop front, and pop back. make sure it doesn’t busy wait!	thread	11
6. write a thread safe linked list that supports insert front, back, pop front, and pop back. make sure it doesn’t busy wait!	a thread	9
7. what is peterson’s solution to the critical section problem? how about dekker’s?	section	47
8. is the following code thread-safe? redesign the following code to be thread-safe. hint: a mutex is unnecessary if the message memory is unique to each call.	the following	6
8. is the following code thread-safe? redesign the following code to be thread-safe. hint: a mutex is unnecessary if the message memory is unique to each call.	thread	25
8. is the following code thread-safe? redesign the following code to be thread-safe. hint: a mutex is unnecessary if the message memory is unique to each call.	a mutex	91
8. is the following code thread-safe? redesign the following code to be thread-safe. hint: a mutex is unnecessary if the message memory is unique to each call.	code	20
8. is the following code thread-safe? redesign the following code to be thread-safe. hint: a mutex is unnecessary if the message memory is unique to each call.	memory	129
static char message[20]; pthread_mutex_t mutex = pthread_mutex_initializer; void *format(int v) { pthread_mutex_lock(&mutex); sprintf(message, ":%d:" ,v); pthread_mutex_unlock(&mutex); return message; }	thread	26
static char message[20]; pthread_mutex_t mutex = pthread_mutex_initializer; void *format(int v) { pthread_mutex_lock(&mutex); sprintf(message, ":%d:" ,v); pthread_mutex_unlock(&mutex); return message; }	printf	127
9. which one of the following may leave a process in running?	the following	16
9. which one of the following may leave a process in running?	a process	40
(a) returning from the pthread’s starting function in the last running thread.	thread	24
(b) the original thread returning from main.	thread	17
(c) any thread causing a segmentation fault.	thread	8
(d) any thread calling exit.	thread	8
(e) calling pthread_exit in the main thread with other threads still running.	thread	13
10. write a mathematical expression for the number of “w” characters that will be printed by the following program. assume a,b,c,d are small positive integers. your answer may use a ‘min’ function that returns its lowest valued argument.	the following	93
unsigned int a=...,b=...,c=...,d=...; void* func(void* ptr) { char m = * (char*)ptr; if(m == ’p’) sem_post(s); if(m == ’w’) sem_wait(s); putchar(m); return null; } int main(int argv, char** argc) { sem_init(s,0, a); while(b--) pthread_create(&tid, null, func, "w"); while(c--) pthread_create(&tid, null, func, "p"); while(d--) pthread_create(&tid, null, func, "w");	thread	228
pthread_exit(null); /*process will finish when all threads have exited */ }	thread	1
11. complete the following code. the following code is supposed to print alternating a and b. it represents two threads that take turns to execute. add condition variable calls to func so that the waiting thread need not to continually check the turn variable. q: is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?	the following	13
11. complete the following code. the following code is supposed to print alternating a and b. it represents two threads that take turns to execute. add condition variable calls to func so that the waiting thread need not to continually check the turn variable. q: is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?	thread	112
11. complete the following code. the following code is supposed to print alternating a and b. it represents two threads that take turns to execute. add condition variable calls to func so that the waiting thread need not to continually check the turn variable. q: is pthread_cond_broadcast necessary or is pthread_cond_signal sufficient?	code	27
12. identify the critical sections in the given code. add mutex locking to make the code thread safe. add condition variable calls so that total never becomes negative or above 1000. instead the call should block until it is safe to proceed. explain why pthread_cond_broadcast is necessary.	thread	89
12. identify the critical sections in the given code. add mutex locking to make the code thread safe. add condition variable calls so that total never becomes negative or above 1000. instead the call should block until it is safe to proceed. explain why pthread_cond_broadcast is necessary.	code	48
12. identify the critical sections in the given code. add mutex locking to make the code thread safe. add condition variable calls so that total never becomes negative or above 1000. instead the call should block until it is safe to proceed. explain why pthread_cond_broadcast is necessary.	block	207
12. identify the critical sections in the given code. add mutex locking to make the code thread safe. add condition variable calls so that total never becomes negative or above 1000. instead the call should block until it is safe to proceed. explain why pthread_cond_broadcast is necessary.	section	26
13. an thread unsafe data structure has size() enq and deq methods. use condition variable and mutex lock to complete the thread-safe, blocking versions.	a struct	24
13. an thread unsafe data structure has size() enq and deq methods. use condition variable and mutex lock to complete the thread-safe, blocking versions.	thread	7
13. an thread unsafe data structure has size() enq and deq methods. use condition variable and mutex lock to complete the thread-safe, blocking versions.	block	135
14. your startup offers path planning using the latest traffic information. your overpaid intern has created a thread unsafe data structure with two functions: shortest (which uses but does not modify the graph) and set_edge (which modifies the graph).	a struct	128
14. your startup offers path planning using the latest traffic information. your overpaid intern has created a thread unsafe data structure with two functions: shortest (which uses but does not modify the graph) and set_edge (which modifies the graph).	thread	111
14. your startup offers path planning using the latest traffic information. your overpaid intern has created a thread unsafe data structure with two functions: shortest (which uses but does not modify the graph) and set_edge (which modifies the graph).	a thread	109
for performance, multiple threads must be able to call shortest at the same time but the graph can only be modified by one thread when no threads other are executing inside shortest or set_edge.	thread	26
15. use mutex lock and condition variables to implement a reader-writer solution. an incomplete attempt is shown below. though this attempt is thread safe (thus sufficient for demo day!), it does not allow multiple threads to calculate shortest path at the same time and will not have sufficient throughput.	thread	143
path_t* shortest_safe(graph_t* graph, int i, int j) { pthread_mutex_lock(&m); path_t* path = shortest(graph, i, j); pthread_mutex_unlock(&m); return path; } void set_edge_safe(graph_t* graph, int i, int j, double dist) { pthread_mutex_lock(&m); set_edge(graph, i, j, dist); pthread_mutex_unlock(&m); }	thread	55
16. how many of the following statements are true for the reader-writer problem?	the following	16
(a) hold and wait (b) circular wait (c) no preemption (d) mutual exclusion 3. identify when dining philosophers code causes a deadlock (or not). for example, if you saw the following code snippet which coffman condition is not satisfied?	the following	169
(a) hold and wait (b) circular wait (c) no preemption (d) mutual exclusion 3. identify when dining philosophers code causes a deadlock (or not). for example, if you saw the following code snippet which coffman condition is not satisfied?	code	112
(a) hold and wait (b) circular wait (c) no preemption (d) mutual exclusion 3. identify when dining philosophers code causes a deadlock (or not). for example, if you saw the following code snippet which coffman condition is not satisfied?	snippet	188
pthread_mutex_lock( a ); if( pthread_mutex_trylock( b ) ) {/*failed*/ pthread_mutex_unlock( a ); ...	thread	1
4. how many processes are blocked?	block	26
 p1 acquires r1  p2 acquires r2  p1 acquires r3  p2 waits for r3  p3 acquires r5  p1 acquires r4  p3 waits for r1  p4 waits for r5  p5 waits for r1 5. what are the pros and cons for the following solutions to dining philosophers (a) arbitrator (b) dijkstra (c) stalling’s (d) trylock	the following	182
15.6 ipc 1. what are the following and what is their purpose?	the following	21
(a) translation lookaside buffer (b) physical address (c) memory management unit (d) the dirty bit 2. how do you determine how many bits are used in the page offset?	address	46
(a) translation lookaside buffer (b) physical address (c) memory management unit (d) the dirty bit 2. how do you determine how many bits are used in the page offset?	memory	58
3. 20 ms after a context switch the tlb contains all logical addresses used by your numerical code which performs main memory access 100% of the time. what is the overhead (slowdown) of a two-level page table compared to a single-level page table?	code	94
3. 20 ms after a context switch the tlb contains all logical addresses used by your numerical code which performs main memory access 100% of the time. what is the overhead (slowdown) of a two-level page table compared to a single-level page table?	address	61
3. 20 ms after a context switch the tlb contains all logical addresses used by your numerical code which performs main memory access 100% of the time. what is the overhead (slowdown) of a two-level page table compared to a single-level page table?	memory	119
5. fill in the blanks to make the following program print 123456789. if cat is given no arguments it simply prints its input until eof. bonus: explain why the close call below is necessary.	the following	30
6. use posix calls fork pipe dup2 and close to implement an autograding program. capture the standard output of a child process into a pipe. the child process should exec the program ./test with no additional arguments (other than the process name). in the parent process read from the pipe: exit the parent process as soon as the captured output contains the ! character. before exiting the parent process send sigkill to the child process. exit 0 if the output contained a !. otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. be sure to close the unused ends of the pipe in the parent and child process 7. this advanced challenge uses pipes to get an “ai player” to play itself until the game is complete. the program tic tac toe accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. a turn is specified using two characters. for example “a1” and “c3” are two opposite corner positions. the string b2a1a3 is a game of 3 turns/plys. a valid response is b2a1a3c1 (the c1 response blocks the diagonal b2 a3 threat). the output line may also include a suffix -i win -you win -invalid or -draw use pipes to control the input and output of each child process created. when the output contains a -, print the final output line (the entire game sequence and the result) and exit.	string	1024
6. use posix calls fork pipe dup2 and close to implement an autograding program. capture the standard output of a child process into a pipe. the child process should exec the program ./test with no additional arguments (other than the process name). in the parent process read from the pipe: exit the parent process as soon as the captured output contains the ! character. before exiting the parent process send sigkill to the child process. exit 0 if the output contained a !. otherwise if the child process exits causing the pipe write end to be closed, then exit with a value of 1. be sure to close the unused ends of the pipe in the parent and child process 7. this advanced challenge uses pipes to get an “ai player” to play itself until the game is complete. the program tic tac toe accepts a line of input - the sequence of turns made so far, prints the same sequence followed by another turn, and then exits. a turn is specified using two characters. for example “a1” and “c3” are two opposite corner positions. the string b2a1a3 is a game of 3 turns/plys. a valid response is b2a1a3c1 (the c1 response blocks the diagonal b2 a3 threat). the output line may also include a suffix -i win -you win -invalid or -draw use pipes to control the input and output of each child process created. when the output contains a -, print the final output line (the entire game sequence and the result) and exit.	block	1111
8. write a function that uses fseek and ftell to replace the middle character of a file with an ‘x’	ftell	40
9. what is an mmu? what are the drawbacks to using it versus a direct memory system?	memory	70
9. what is an mmu? what are the drawbacks to using it versus a direct memory system?	system	77
15.7 filesystems 1. what is the file api?	system	9
4. what are the two special file names in every directory 5. how do you resolve the following path a/../b/./c/../../c 6. what are the rwx groups?	the following	80
10. what is a virtual file system?	system	27
12. in an ext2 filesystem how many inodes are read from disk to access the first byte of the file /dir1/subdira/notes.txt ? assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.	memory	238
12. in an ext2 filesystem how many inodes are read from disk to access the first byte of the file /dir1/subdira/notes.txt ? assume the directory names and inode numbers in the root directory (but not the inodes themselves) are already in memory.	system	19
13. in an ext2 filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file /dir1/subdira/notes.txt ? assume the directory names and inode numbers in the root directory and all inodes are already in memory.	memory	256
13. in an ext2 filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file /dir1/subdira/notes.txt ? assume the directory names and inode numbers in the root directory and all inodes are already in memory.	block	61
13. in an ext2 filesystem what is the minimum number of disk blocks that must be read from disk to access the first byte of the file /dir1/subdira/notes.txt ? assume the directory names and inode numbers in the root directory and all inodes are already in memory.	system	19
14. in an ext2 filesystem with 32 bit addresses and 4kib disk blocks, an inode can store 10 direct disk block numbers. what is the minimum file size required to require a single indirection table? ii) a double direction table?	address	38
14. in an ext2 filesystem with 32 bit addresses and 4kib disk blocks, an inode can store 10 direct disk block numbers. what is the minimum file size required to require a single indirection table? ii) a double direction table?	block	62
14. in an ext2 filesystem with 32 bit addresses and 4kib disk blocks, an inode can store 10 direct disk block numbers. what is the minimum file size required to require a single indirection table? ii) a double direction table?	system	19
3. what is ip? what is an ip address?	address	29
10. why do we use non-blocking io for networking?	block	22
15. if a host address is 32 bits which ip scheme am i most likely using? 128 bits?	address	14
19. which one of the following is not a feature of tcp?	the following	17
25. which of the above calls can block, waiting for a new client to connect?	block	33
34. how does network address translation (nat) work?	address	21
if write returns -1 then immediately return -1 unless the errno is eintr - in which case repeat the last write attempt. you will need to use pointer arithmetic.	pointer	141
38. implement a multithreaded tcp server that listens on port 2000. each thread should read 128 bytes from the client file descriptor and echo it back to the client, before closing the connection and ending the thread.	thread	21
4. how does an operating system provide security? what are some examples from networking and filesystems?	system	25
4. write brief code that uses sigaction and a signalset to create a sigalrm handler.	code	15
5. what is the difference between a disposition, mask, and pending signal set?	disposition	36
16.1 the linux kernel throughout the course of cs 241, you become familiar with system calls - the userspace interface to interacting with the kernel. how does this kernel actually work? what is a kernel? in this section, we will explore these questions in more detail and shed some light on various black boxes that you have encountered in this course. we will mostly be focusing on the linux kernel in this chapter, so please assume that all examples pertain to the linux kernel unless otherwise specified.	this section	208
16.1 the linux kernel throughout the course of cs 241, you become familiar with system calls - the userspace interface to interacting with the kernel. how does this kernel actually work? what is a kernel? in this section, we will explore these questions in more detail and shed some light on various black boxes that you have encountered in this course. we will mostly be focusing on the linux kernel in this chapter, so please assume that all examples pertain to the linux kernel unless otherwise specified.	system	80
16.1 the linux kernel throughout the course of cs 241, you become familiar with system calls - the userspace interface to interacting with the kernel. how does this kernel actually work? what is a kernel? in this section, we will explore these questions in more detail and shed some light on various black boxes that you have encountered in this course. we will mostly be focusing on the linux kernel in this chapter, so please assume that all examples pertain to the linux kernel unless otherwise specified.	section	213
as it stands, most of you are probably familiar with the linux kernel, at least in terms of interacting with it via system calls. some of you may also have explored the windows kernel, which we won’t talk about too much in this chapter. or darwin, the unix-like kernel for macos (a derivative of bsd). those of you who might have done a bit more digging might have also encountered projects such a gnu hurd or zircon.	system	116
as it stands, most of you are probably familiar with the linux kernel, at least in terms of interacting with it via system calls. some of you may also have explored the windows kernel, which we won’t talk about too much in this chapter. or darwin, the unix-like kernel for macos (a derivative of bsd). those of you who might have done a bit more digging might have also encountered projects such a gnu hurd or zircon.	a system call	114
kernels can generally be classified into one of two categories, a monolithic kernel or a micro-kernel. a monolithic kernel is essentially a kernel and all of it’s associated services as a single program. a micro-kernel on the other hand is designed to have a main component which provides the bare-minimum functionality that a kernel needs. this involves setting up important device drivers, the root filesystem, paging or other functionality that is imperative for other higher-level features to be implemented. the higher-level features (such as a networking stack, other filesystems, and non-critical device drivers) are then implemented as separate programs that can interact with the kernel by some form of ipc, typically rpc. as a result of this design, micro-kernels have traditionally been slower than monolithic kernels due to the ipc overhead.	system	405
16.1.2 system calls demystified system calls use an instruction that can be run by a program operating in userspace that traps to the kernel (by use of a signal) to complete the call. this includes actions such as writing data to disk, interacting directly with hardware in general or operations related to gaining or relinquishing privileges (e.g. becoming the root user and gaining all capabilities).	system	7
in order to fulfill a user’s request, the kernel will rely on kernel calls. kernel calls are essentially the "public" functions of the kernel - functions implemented by other developers for use in other parts of the kernel. here is a snippet for a kernel call man page:	snippet	234
name kmalloc allocate memory synopsis void * kmalloc ( size_t size, gfp_t flags); arguments size_t size how many bytes of memory are required.	memory	22
gfp_t flags the type of memory to allocate.	memory	24
gfp_t flags the type of memory to allocate.	type	16
description kmalloc is the normal method of allocating memory for objects smaller than page size in the kernel.	memory	55
the flags argument may be one of: gfp_user - allocate memory on behalf of user. may sleep.	memory	54
you’ll note that some flags are marked as potentially causing sleeps. this tells us whetherwe can use those flags in special scenarios, like interrupt contexts, where speed is of the essence, and operations that may block or wait for another process may never complete.	block	216
16.2 containerization as we enter an era of unprecedented scale with around 20 billion devices connected to the internet in 2018, we need technologies that help us develop and maintain software capable of scaling upwards. additionally, as software increases in complexity, and designing secure software becomes harder, we find that we have new constraints imposed on us as we develop applications. as if that wasn’t enough, efforts to simplify software distribution and development, like package manager systems can often lead to headaches of their own, leading to broken packages, dependencies that are impossible to resolve and other such environmental nightmares that have become all to common today. while these seem like disjoint problems at first, all of these and more can be solved by throwing containerization at the problem.	system	504
16.2.1 what is a container?	a container	15
a container is almost like a virtual machine. in some senses, containers are to virtual machines as threads are to processes. a container is a lightweight environment that shares resources and a kernel with a host machine, while isolating itself from other containers or processes on the host. you may have encountered containers while working with technologies such as docker, perhaps the most well-known implementation of containers out there.	thread	100
a container is almost like a virtual machine. in some senses, containers are to virtual machines as threads are to processes. a container is a lightweight environment that shares resources and a kernel with a host machine, while isolating itself from other containers or processes on the host. you may have encountered containers while working with technologies such as docker, perhaps the most well-known implementation of containers out there.	resources	179
a container is almost like a virtual machine. in some senses, containers are to virtual machines as threads are to processes. a container is a lightweight environment that shares resources and a kernel with a host machine, while isolating itself from other containers or processes on the host. you may have encountered containers while working with technologies such as docker, perhaps the most well-known implementation of containers out there.	a container	0
16.2.2 linux namespaces 16.2.3 building a container from scratch 16.2.4 containers in the wild: software distribution is a snap	a container	40
17.1 shell a shell is actually how you are going to be interacting with the system. before user-friendly operating systems, when a computer started up all you had access to was a shell. this meant that all of your commands and editing had to be done this way. nowadays, our computers boot up in desktop mode, but one can still access a shell using a terminal.	a terminal	348
17.1 shell a shell is actually how you are going to be interacting with the system. before user-friendly operating systems, when a computer started up all you had access to was a shell. this meant that all of your commands and editing had to be done this way. nowadays, our computers boot up in desktop mode, but one can still access a shell using a terminal.	a shell	11
17.1 shell a shell is actually how you are going to be interacting with the system. before user-friendly operating systems, when a computer started up all you had access to was a shell. this meant that all of your commands and editing had to be done this way. nowadays, our computers boot up in desktop mode, but one can still access a shell using a terminal.	system	76
it is ready for your next command! you can type in a lot of unix utilities like ls, echo hello and the shell will execute them and give you the result. some of these are what are known as shell-builtins meaning that the code is in the shell program itself. some of these are compiled programs that you run. the shell only looks through a special variable called path which contains a list of colon separated paths to search for an executable with your name, here is an example path.	code	220
it is ready for your next command! you can type in a lot of unix utilities like ls, echo hello and the shell will execute them and give you the result. some of these are what are known as shell-builtins meaning that the code is in the shell program itself. some of these are compiled programs that you run. the shell only looks through a special variable called path which contains a list of colon separated paths to search for an executable with your name, here is an example path.	type	43
17.1.2 what’s a terminal?	a terminal	14
a terminal is an application that displays the output from the shell. you can have your default terminal, a quake based terminal, terminator, the options are endless!	a terminal	0
5. cd this is a shell builtin but it changes to a relative or absolute directory	a shell	14
6. man every system programmers favorite command tells you more about all your favorite functions!	system	13
struct packing structs may require something called padding (tutorial). we do not expect you to pack structs in this course, know that compilers perform it. this is because in the early days (and even now) loading an address in memory happens in 32-bit or 64-bit blocks. this also meant requested addresses had to be multiples of block sizes.	address	217
struct packing structs may require something called padding (tutorial). we do not expect you to pack structs in this course, know that compilers perform it. this is because in the early days (and even now) loading an address in memory happens in 32-bit or 64-bit blocks. this also meant requested addresses had to be multiples of block sizes.	memory	228
struct packing structs may require something called padding (tutorial). we do not expect you to pack structs in this course, know that compilers perform it. this is because in the early days (and even now) loading an address in memory happens in 32-bit or 64-bit blocks. this also meant requested addresses had to be multiples of block sizes.	block	263
figure 17.2: eight box struct, two boxes of slop this padding is common on a 64-bit system. other time, a processor supports unaligned access, leaving the compiler able to pack structs. what does this mean? we can have a variable start at a non-64-bit boundary. the processor will figure out the rest. to enable this, set an attribute.	a process	104
figure 17.2: eight box struct, two boxes of slop this padding is common on a 64-bit system. other time, a processor supports unaligned access, leaving the compiler able to pack structs. what does this mean? we can have a variable start at a non-64-bit boundary. the processor will figure out the rest. to enable this, set an attribute.	system	84
now our figure will look like the clean struct as in figure 17.1 but now, every time the processor needs to access data or encoding, two memory accesses are required. a possible alternative is to reorder the struct.	memory	137
17.2 stack smashing each thread uses a stack memory. the stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. stack memory includes non-static automatic (temporary) variables, parameter values, and the return address. if a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. the precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent.	thread	25
17.2 stack smashing each thread uses a stack memory. the stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. stack memory includes non-static automatic (temporary) variables, parameter values, and the return address. if a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. the precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent.	address	166
17.2 stack smashing each thread uses a stack memory. the stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. stack memory includes non-static automatic (temporary) variables, parameter values, and the return address. if a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. the precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent.	memory	45
17.2 stack smashing each thread uses a stack memory. the stack ‘grows downwards’ - if a function calls another function, then the stack is extended to smaller memory addresses. stack memory includes non-static automatic (temporary) variables, parameter values, and the return address. if a buffer is too small some data (e.g. input values from the user), then there is a real possibility that other stack variables and even the return address will be overwritten. the precise layout of the stack’s contents and order of the automatic variables is architecture and compiler dependent.	parameter	243
the example below demonstrates how the return address is stored on the stack. for a particular 32 bit	address	46
architecture live linux machine, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. the code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.	code	166
architecture live linux machine, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. the code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.	address	62
architecture live linux machine, we determine that the return address is stored at an address two pointers (8 bytes) above the address of the automatic variable. the code deliberately changes the stack value so that when the input function returns, rather than continuing on inside the main method, it jumps to the exploit function instead.	pointer	98
*((&p)+2) = breakout; } int main() { printf("main() code starts at %p\n",main); input(); while (1) { puts("hello"); sleep(1); } return 0; }	code	52
*((&p)+2) = breakout; } int main() { printf("main() code starts at %p\n",main); input(); while (1) { puts("hello"); sleep(1); } return 0; }	printf	37
17.3 compiling and linking this is a high-level overview from the time you compile your program to the time you run your program. we often know that compiling your program is easy. you run the program through an ide or a terminal, and it just works.	a terminal	219
int main() { printf("hello world!\n"); return 0; } $ gcc main.c -o main $ ./main hello world!	printf	13
1. preprocessing: the preprocessor expands all preprocesor directives.	preprocessing	3
3. assembly generation: the compiler then generates assembly code for all the functions after some optimizations if enabled.	code	61
3. assembly generation: the compiler then generates assembly code for all the functions after some optimizations if enabled.	optimizations	99
4. assembling: the assembler turns the assembly into 0s and 1s and creates an object file. this object file maps names to pieces of code.	code	132
5. static linking: the linker then takes a series of objects and static libraries and resolves references of variables and functions from one object file to another. the linker then finds the main method and makes that the entry point for the function. the linker also notices when a function is meant to be dynamically linked. the compiler also creates a section in the executable that tells the operating system that these functions need addresses right before running.	address	440
5. static linking: the linker then takes a series of objects and static libraries and resolves references of variables and functions from one object file to another. the linker then finds the main method and makes that the entry point for the function. the linker also notices when a function is meant to be dynamically linked. the compiler also creates a section in the executable that tells the operating system that these functions need addresses right before running.	system	407
5. static linking: the linker then takes a series of objects and static libraries and resolves references of variables and functions from one object file to another. the linker then finds the main method and makes that the entry point for the function. the linker also notices when a function is meant to be dynamically linked. the compiler also creates a section in the executable that tells the operating system that these functions need addresses right before running.	section	356
6. dynamic linking: as the program is getting ready to be executed, the operating system looks at what libraries that the program needs and links those functions to the dynamic library.	system	82
further classes will teach you about parsing and assembly – preprocessing is an extension of parsing. most classes won’t teach you about the two different types of linking though. static linking a library is similar to combining object files. to create a static library, a compiler combines different object files to create one executable.	preprocessing	60
further classes will teach you about parsing and assembly – preprocessing is an extension of parsing. most classes won’t teach you about the two different types of linking though. static linking a library is similar to combining object files. to create a static library, a compiler combines different object files to create one executable.	type	155
a static library is literally is an archive of object files. these libraries are useful when you want your executable to be secure, you know all the code that is being included into your executable, and portable, all the code is bundled with your executable meaning no additional installs.	code	149
the other type is a dynamic library. typically, dynamic libraries are installed user-wide or system-wide and are accessible by most programs. dynamic libraries’ functions are filled in right before they are run. there are a number of benefits to this.	system	93
the other type is a dynamic library. typically, dynamic libraries are installed user-wide or system-wide and are accessible by most programs. dynamic libraries’ functions are filled in right before they are run. there are a number of benefits to this.	type	10
 lower code footprint for common libraries like the c standard library  late binding means more generalized code and less reliance on specific behavior.	code	7
 all the code is no longer bundled into your program. this means that users have to install something else.	code	9
 there could be security flaws in the other code leading to security exploits in your program.	code	44
explanation of the fork-file problem to parse the posix documentation, we’ll have to go deep into the terminology. the sentence that sets the expectation is the following the result of function calls involving any one handle (the "active handle") is defined elsewhere in this volume of posix.1-2008, but if two or more handles are used, and any one of them is a stream, the application shall ensure that their actions are coordinated as described below. if this is not done, the result is undefined.	the following	157
explanation of the fork-file problem to parse the posix documentation, we’ll have to go deep into the terminology. the sentence that sets the expectation is the following the result of function calls involving any one handle (the "active handle") is defined elsewhere in this volume of posix.1-2008, but if two or more handles are used, and any one of them is a stream, the application shall ensure that their actions are coordinated as described below. if this is not done, the result is undefined.	the expectation	138
what this means is that if we don’t follow posix to the letter when using two file descriptors that refer to the same description across processes, we get undefined behavior. to be technical, the file descriptor must have a “position” meaning that it needs to have a beginning and an end like a file, not like an arbitrary stream of bytes. posix then goes on to introduce the idea of an active handle, where a handle may be a file descriptor or a file* pointer. file handles don’t have a flag called “active”. an active file descriptor is one that is currently being used for reading and writing and other operations (such as exit). the standard says that before a fork that the application or your code must execute a series of steps to prepare the state of the file. in simplified terms, the descriptor needs to be closed, flushed, or read to its entirety – the gory details are explained later.	code	699
what this means is that if we don’t follow posix to the letter when using two file descriptors that refer to the same description across processes, we get undefined behavior. to be technical, the file descriptor must have a “position” meaning that it needs to have a beginning and an end like a file, not like an arbitrary stream of bytes. posix then goes on to introduce the idea of an active handle, where a handle may be a file descriptor or a file* pointer. file handles don’t have a flag called “active”. an active file descriptor is one that is currently being used for reading and writing and other operations (such as exit). the standard says that before a fork that the application or your code must execute a series of steps to prepare the state of the file. in simplified terms, the descriptor needs to be closed, flushed, or read to its entirety – the gory details are explained later.	pointer	453
for a handle to become the active handle, the application shall ensure that the actions below are performed between the last use of the handle (the current active handle) and the first use of the second handle (the future active handle). the second handle then becomes the active handle. all activity by the application affecting the file offset on the first handle shall be suspended until it again becomes the active file handle. (if a stream function has as an underlying function one that affects the file offset, the stream function shall be considered to affect the file offset.) summarizing as if two file descriptors are actively being used, the behavior is undefined. the other note is that after a fork, the library code must prepare the file descriptor as if the other process were to make the file active at any time. the last bullet point concerns itself with how a process prepares a file descriptor in our case.	code	726
for a handle to become the active handle, the application shall ensure that the actions below are performed between the last use of the handle (the current active handle) and the first use of the second handle (the future active handle). the second handle then becomes the active handle. all activity by the application affecting the file offset on the first handle shall be suspended until it again becomes the active file handle. (if a stream function has as an underlying function one that affects the file offset, the stream function shall be considered to affect the file offset.) summarizing as if two file descriptors are actively being used, the behavior is undefined. the other note is that after a fork, the library code must prepare the file descriptor as if the other process were to make the file active at any time. the last bullet point concerns itself with how a process prepares a file descriptor in our case.	a process	877
if any previous active handle has been used by a function that explicitly changed the file offset, except as required above for the first handle, the application shall perform an lseek() or fseek() (as appropriate to the type of handle) to an appropriate location.	type	221
since the child calls fflush and the parent didn’t prepare, the operating system chooses to where the file gets reset. different file systems will do different things which are supported by the standard. the os may look at modification times and conclude that the file hasn’t changed so no resets are needed or may conclude that exit denotes a change and needs to rewind the file back to the beginning.	system	74
she maintains an amount of money p with her, at all times. for people to request money, they do the following: consider the state of the system (a = {a1 , a2 , ...}, l t = {l t,1 , l t,2 , ...}, p) at time t. a precondition is that we have p ≥ min(a), or we have enough money to suit at least one person. also, each person will work for a finite period and give back our money.	the following	96
she maintains an amount of money p with her, at all times. for people to request money, they do the following: consider the state of the system (a = {a1 , a2 , ...}, l t = {l t,1 , l t,2 , ...}, p) at time t. a precondition is that we have p ≥ min(a), or we have enough money to suit at least one person. also, each person will work for a finite period and give back our money.	system	137
since we can always make one additional move, the system can never deadlock. now, there is no guarantee that the system won’t livelock. if the process we hope to request something never does, no work will be done – but not due to deadlock. this analogy expands to higher orders of magnitude but requires that either a process can do its work entirely or there exists a process whose combination of resources can be satisfied, which makes the algorithm a little more tricky (an additional for loop) but nothing too bad. there are some notable downsides.	resources	398
since we can always make one additional move, the system can never deadlock. now, there is no guarantee that the system won’t livelock. if the process we hope to request something never does, no work will be done – but not due to deadlock. this analogy expands to higher orders of magnitude but requires that either a process can do its work entirely or there exists a process whose combination of resources can be satisfied, which makes the algorithm a little more tricky (an additional for loop) but nothing too bad. there are some notable downsides.	a process	316
since we can always make one additional move, the system can never deadlock. now, there is no guarantee that the system won’t livelock. if the process we hope to request something never does, no work will be done – but not due to deadlock. this analogy expands to higher orders of magnitude but requires that either a process can do its work entirely or there exists a process whose combination of resources can be satisfied, which makes the algorithm a little more tricky (an additional for loop) but nothing too bad. there are some notable downsides.	system	50
 the program first needs to know how much of each resource a process needs. a lot of times that is impossible or the process requests the wrong amount because the programmer didn’t foresee it.	a process	59
 the system could livelock.	system	5
 we know in most systems that resources vary, pipes and sockets for example. this could mean that the runtime of the algorithm could be slow for systems with millions of resources.	resources	30
 we know in most systems that resources vary, pipes and sockets for example. this could mean that the runtime of the algorithm could be slow for systems with millions of resources.	system	17
 also, this can’t keep track of the resources that come and go. a process may delete a resource as a side effect or create a resource. the algorithm assumes a static allocation and that each process performs a non-destructive operation.	resources	36
 also, this can’t keep track of the resources that come and go. a process may delete a resource as a side effect or create a resource. the algorithm assumes a static allocation and that each process performs a non-destructive operation.	a process	64
17.5 clean/dirty forks (chandy/misra solution) there are many more advanced solutions. one such solution is by chandy and misra [? ]. this is not a true solution to the dining philosophers problem because it has the requirement that philosophers can speak to each other. it is a solution that ensures fairness for some notion of fairness. in essence, it defines a series of rounds that a philosopher must eat in a given round before going to the next one.	the dining philosophers problem	165
as mentioned, there are frameworks like message passing interface that is somewhat based on the actor model and allows distributed systems in high-performance computing to work effectively, but your mileage may vary if you want to read further on the model, feel free to glance over the wikipedia page listed below. further reading on the actor model	a page	295
as mentioned, there are frameworks like message passing interface that is somewhat based on the actor model and allows distributed systems in high-performance computing to work effectively, but your mileage may vary if you want to read further on the model, feel free to glance over the wikipedia page listed below. further reading on the actor model	system	131
after preprocessing, the compiler sees this	preprocessing	6
using gcc your compiler would preprocess the source to the following.	the following	55
17.7.1 thread scheduling there are a few ways to split up the work. these are common to the openmp framework [? ].	thread	7
17.7.1 thread scheduling there are a few ways to split up the work. these are common to the openmp framework [? ].	openmp	92
 static scheduling breaks up the problems into fixed-size chunks (predetermined) and have each thread work on each of the chunks. this works well when each of the subproblems takes roughly the same time because there is no additional overhead. all you need to do is write a loop and give the map function to each sub-array.	thread	95
 dynamic scheduling as a new problem becomes available to have a thread serve it. this is useful when you don’t know how long the scheduling will take  guided scheduling this is a mix of the above with a mix of the benefits and tradeoffs. you start with static scheduling and move slowly to dynamic if needed  runtime scheduling you have absolutely no idea how long the problems are going to take. instead of deciding it yourself, let the program decide what to do!	thread	65
 dynamic scheduling as a new problem becomes available to have a thread serve it. this is useful when you don’t know how long the scheduling will take  guided scheduling this is a mix of the above with a mix of the benefits and tradeoffs. you start with static scheduling and move slowly to dynamic if needed  runtime scheduling you have absolutely no idea how long the problems are going to take. instead of deciding it yourself, let the program decide what to do!	a thread	63
no need to memorize any of the scheduling routines though. openmp is a standard that is an alternative to pthreads. for example, here is how to parallelize a for loop	thread	107
no need to memorize any of the scheduling routines though. openmp is a standard that is an alternative to pthreads. for example, here is how to parallelize a for loop	openmp	59
static scheduling will divide the problem into fixed-size chunks dynamic scheduling will give a job once the loop is over guided scheduling is dynamic with chunks runtime is a whole bag of worms.	the loop	105
17.8 threads.h we have a lot of threading libraries discussed in the extra section. we have the standard posix threads, openmp threads, we also have a new c11 threading library that is built into the standard. this library provides restricted functionality.	thread	5
17.8 threads.h we have a lot of threading libraries discussed in the extra section. we have the standard posix threads, openmp threads, we also have a new c11 threading library that is built into the standard. this library provides restricted functionality.	openmp	120
17.8 threads.h we have a lot of threading libraries discussed in the extra section. we have the standard posix threads, openmp threads, we also have a new c11 threading library that is built into the standard. this library provides restricted functionality.	section	75
why use restricted functionality? the key is in the name. since this is the c standard library, it has to be implemented in all operating systems that are compliant which are pretty much all of them. this means there is first-class portability when using threads.	thread	255
why use restricted functionality? the key is in the name. since this is the c standard library, it has to be implemented in all operating systems that are compliant which are pretty much all of them. this means there is first-class portability when using threads.	system	138
we won’t drone on about the functions. most of them are renaming of pthread functions anyway. if you ask why we don’t teach these, there are a few reasons 1. they are pretty new. even though the standard came out in roughly 2011, posix threads have been around forever. a lot of their quirks have been ironed out.	thread	69
2. you lose expressivity. this is a concept that we’ll talk about in later chapters, but when you make something portable, you lose some expressivity with the host hardware. that means that the threads.h library is pretty bare bones. it is hard to set cpu affinities. schedule threads together. efficiently look at the internals for performance reasons.	thread	194
3. a lot of legacy code is already written with posix threads in mind. other libraries like openmp, cuda, mpi will either use posix processes or posix threads with a begrudging port to windows.	thread	54
3. a lot of legacy code is already written with posix threads in mind. other libraries like openmp, cuda, mpi will either use posix processes or posix threads with a begrudging port to windows.	code	19
3. a lot of legacy code is already written with posix threads in mind. other libraries like openmp, cuda, mpi will either use posix processes or posix threads with a begrudging port to windows.	openmp	92
17.9 modern filesystems while the api for most filesystems have stayed the same on posix over the years, the actual filesystems themselves provide lots of important aspects.	system	16
 data integrity. file systems use journaling and sometimes checksums to ensure that the data written to is valid. journalling is a simple invention where the file system writes an operation in a journal. if the filesystem crashes before the operation is complete, it can resume the operation when booted up again using the partial journal.	system	22
 caching. linux does a good job of caching file system operations like finding inodes. this makes disk operations seem nearly instant. if you want to see a slow system, look at windows with fat/ntfs. disk operations need to be cached by the application, or it will burn through the cpu.	system	48
 speed. on spinning disk machines, data that is toward the end of a metallic platter will spin faster (angular velocity is farther from the center). programs used this to reduce time loading large files like movies in a video editing piece of software. ssds don’t have this problem because there is no spinning disk, but they will portion off a section of their space to be used as "swap space" for fiels.	section	345
 parallelism. filesystems with multiple heads (for physical hard disks) or multiple controllers (for ssds) can utilize parallelism by multiplexing the pcie slot with data, always serving some data to the application whenever possible.	system	18
 encryption. data can be encrypted with one or more keys. a good example of this is apple’s apfs file systems.	system	102
 redundancy. sometimes data can be replicated to blocks to ensure that the data is always available.	block	49
 efficient backups. many of us have data that we can’t store on the cloud for one reason or another. it is useful that when a filesystems is either being used as a backup medium or is the source to the backup that it is able to calculate what has changed efficiently, compress files, and sync between the external drive.	a filesystem	124
 efficient backups. many of us have data that we can’t store on the cloud for one reason or another. it is useful that when a filesystems is either being used as a backup medium or is the source to the backup that it is able to calculate what has changed efficiently, compress files, and sync between the external drive.	system	130
 integriy and bootability. file systems need to be resillient to bit flipping. most readers have their operating system installed on the same paritition as the file system that they used to do different operations. the file system needs to make sure a stray read or write doesn’t destroy the boot sector – meaning your computer can’t start up again.	system	32
 fragmentation. just like a memory allocator, allocating space for a file leads to both internal and external fragmentation. the same caching benefit occurs when disk blocks for a single file are located next to each other. file systems need to perform well under low, high, and possible fragmentation usage.	memory	28
 fragmentation. just like a memory allocator, allocating space for a file leads to both internal and external fragmentation. the same caching benefit occurs when disk blocks for a single file are located next to each other. file systems need to perform well under low, high, and possible fragmentation usage.	block	167
 fragmentation. just like a memory allocator, allocating space for a file leads to both internal and external fragmentation. the same caching benefit occurs when disk blocks for a single file are located next to each other. file systems need to perform well under low, high, and possible fragmentation usage.	system	229
 distributed. sometimes, the filesystem should be single machine fault tolerant. hadoop and other distributed file system allow you to do that.	system	33
 distributed. sometimes, the filesystem should be single machine fault tolerant. hadoop and other distributed file system allow you to do that.	hadoop	81
17.9.1 cutting edge file systems there are a few filesystem hardware nowadays that are truly cutting edge. the one we’d briefly like to touch on is amd’s storemi. we aren’t trying to sell amd chipsets, but the featureset of storemi warrants a mention.	system	25
17.9.1 cutting edge file systems there are a few filesystem hardware nowadays that are truly cutting edge. the one we’d briefly like to touch on is amd’s storemi. we aren’t trying to sell amd chipsets, but the featureset of storemi warrants a mention.	storemi	154
storemi is a hardware microcontroller that analyzes how the operating system accesses files and moves files/blocks around to speed up the load time. a common usage can be imagined as having a fast, but small capacity ssd and a slower, large capcity hdd. to make it seem like all the files are on an ssd, the storemi matches the pattern of file access. if you are starting up windows, windows will often access many files in the same order.	block	108
storemi is a hardware microcontroller that analyzes how the operating system accesses files and moves files/blocks around to speed up the load time. a common usage can be imagined as having a fast, but small capacity ssd and a slower, large capcity hdd. to make it seem like all the files are on an ssd, the storemi matches the pattern of file access. if you are starting up windows, windows will often access many files in the same order.	system	70
storemi is a hardware microcontroller that analyzes how the operating system accesses files and moves files/blocks around to speed up the load time. a common usage can be imagined as having a fast, but small capacity ssd and a slower, large capcity hdd. to make it seem like all the files are on an ssd, the storemi matches the pattern of file access. if you are starting up windows, windows will often access many files in the same order.	storemi	0
storemi takes note of that and when the microcontroller notices it is starting the boot, it will move files from the hdd drive to the ssd before they are requested by the operating system. by the time the operating system needs then, they are already on the ssd. storemi also does this with other applications as well. the technology still has a lot to be desired for, but it is an interesting intersection of data and pattern matching with filesystems.	system	181
storemi takes note of that and when the microcontroller notices it is starting the boot, it will move files from the hdd drive to the ssd before they are requested by the operating system. by the time the operating system needs then, they are already on the ssd. storemi also does this with other applications as well. the technology still has a lot to be desired for, but it is an interesting intersection of data and pattern matching with filesystems.	storemi	0
storemi takes note of that and when the microcontroller notices it is starting the boot, it will move files from the hdd drive to the ssd before they are requested by the operating system. by the time the operating system needs then, they are already on the ssd. storemi also does this with other applications as well. the technology still has a lot to be desired for, but it is an interesting intersection of data and pattern matching with filesystems.	section	399
 after a thread goes to sleep on a subset of cores, when it wakes up it can only be scheduled on the cores that it was sleeping on. if those cores are now busy, the thread will have to wait on them, wasting opportunities to use other idle cores.	thread	9
 after a thread goes to sleep on a subset of cores, when it wakes up it can only be scheduled on the cores that it was sleeping on. if those cores are now busy, the thread will have to wait on them, wasting opportunities to use other idle cores.	a thread	7
peterson’s algorithm is used to implement low-level linux kernel locks for the tegra mobile processor (a systemon-chip arm process and gpu core by nvidia) link to lock source in general now, cpus and c compilers can re-order cpu instructions or use cpu-core-specific local cache values that are stale if another core updates the shared variables. thus a simple pseudo-code to c implementation is too naive for most platforms. warning, here be dragons! consider this advanced and gnarly topic but (spoiler alert) a happy ending. consider the following code,	the following	537
peterson’s algorithm is used to implement low-level linux kernel locks for the tegra mobile processor (a systemon-chip arm process and gpu core by nvidia) link to lock source in general now, cpus and c compilers can re-order cpu instructions or use cpu-core-specific local cache values that are stale if another core updates the shared variables. thus a simple pseudo-code to c implementation is too naive for most platforms. warning, here be dragons! consider this advanced and gnarly topic but (spoiler alert) a happy ending. consider the following code,	code	368
peterson’s algorithm is used to implement low-level linux kernel locks for the tegra mobile processor (a systemon-chip arm process and gpu core by nvidia) link to lock source in general now, cpus and c compilers can re-order cpu instructions or use cpu-core-specific local cache values that are stale if another core updates the shared variables. thus a simple pseudo-code to c implementation is too naive for most platforms. warning, here be dragons! consider this advanced and gnarly topic but (spoiler alert) a happy ending. consider the following code,	system	105
an efficient compiler would infer that flag2 variable is never changed inside the loop, so that test can be optimized to while(true) using volatile goes some way to prevent compiler optimizations of this kind.	optimizations	182
an efficient compiler would infer that flag2 variable is never changed inside the loop, so that test can be optimized to while(true) using volatile goes some way to prevent compiler optimizations of this kind.	the loop	78
a related challenge is that cpu cores include a data cache to store recently read or modified main memory values. modified values may not be written back to main memory or re-read from memory immediately. thus data changes, such as the state of a flag and turn variable in the above example, may not be shared between two cpu codes.	code	326
a related challenge is that cpu cores include a data cache to store recently read or modified main memory values. modified values may not be written back to main memory or re-read from memory immediately. thus data changes, such as the state of a flag and turn variable in the above example, may not be shared between two cpu codes.	memory	99
but there is a happy ending. modern hardware addresses these issues using ‘memory fences’ also known as a memory barrier. this prevents instructions from getting ordered before or after the barrier. there is a performance loss, but it is needed for correct programs!	address	45
but there is a happy ending. modern hardware addresses these issues using ‘memory fences’ also known as a memory barrier. this prevents instructions from getting ordered before or after the barrier. there is a performance loss, but it is needed for correct programs!	memory	75
also, there are cpu instructions to ensure that main memory and the cpu’s cache is in a reasonable and coherent state. higher-level synchronization primitives, such as pthread_mutex_lock are will call these cpu instructions as part of their implementation. thus, in practice, surrounding critical sections with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.	thread	169
also, there are cpu instructions to ensure that main memory and the cpu’s cache is in a reasonable and coherent state. higher-level synchronization primitives, such as pthread_mutex_lock are will call these cpu instructions as part of their implementation. thus, in practice, surrounding critical sections with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.	a mutex	311
also, there are cpu instructions to ensure that main memory and the cpu’s cache is in a reasonable and coherent state. higher-level synchronization primitives, such as pthread_mutex_lock are will call these cpu instructions as part of their implementation. thus, in practice, surrounding critical sections with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.	memory	53
also, there are cpu instructions to ensure that main memory and the cpu’s cache is in a reasonable and coherent state. higher-level synchronization primitives, such as pthread_mutex_lock are will call these cpu instructions as part of their implementation. thus, in practice, surrounding critical sections with a mutex lock and unlock calls is sufficient to ignore these lower-level problems.	section	297
for further reading, we suggest the following web post that discusses implementing peterson’s algorithm on an x86 process and the linux documentation on memory barriers.	the following	32
for further reading, we suggest the following web post that discusses implementing peterson’s algorithm on an x86 process and the linux documentation on memory barriers.	memory	153
1. memory fences 2. memory barriers	memory	3
17.11 the curious case of spurious wakeups condition variables need a mutex for a few reasons. one is simply that a mutex is needed to synchronize the changes of the condition variable across threads. imagine a condition variable needing to provide its own internal synchronization to ensure its data structures work correctly. often, we use a mutex to synchronize other parts of our code, so why double the cost of using a condition variable. another example relates to high priority systems.	a struct	299
17.11 the curious case of spurious wakeups condition variables need a mutex for a few reasons. one is simply that a mutex is needed to synchronize the changes of the condition variable across threads. imagine a condition variable needing to provide its own internal synchronization to ensure its data structures work correctly. often, we use a mutex to synchronize other parts of our code, so why double the cost of using a condition variable. another example relates to high priority systems.	thread	192
17.11 the curious case of spurious wakeups condition variables need a mutex for a few reasons. one is simply that a mutex is needed to synchronize the changes of the condition variable across threads. imagine a condition variable needing to provide its own internal synchronization to ensure its data structures work correctly. often, we use a mutex to synchronize other parts of our code, so why double the cost of using a condition variable. another example relates to high priority systems.	a mutex	68
17.11 the curious case of spurious wakeups condition variables need a mutex for a few reasons. one is simply that a mutex is needed to synchronize the changes of the condition variable across threads. imagine a condition variable needing to provide its own internal synchronization to ensure its data structures work correctly. often, we use a mutex to synchronize other parts of our code, so why double the cost of using a condition variable. another example relates to high priority systems.	code	384
17.11 the curious case of spurious wakeups condition variables need a mutex for a few reasons. one is simply that a mutex is needed to synchronize the changes of the condition variable across threads. imagine a condition variable needing to provide its own internal synchronization to ensure its data structures work correctly. often, we use a mutex to synchronize other parts of our code, so why double the cost of using a condition variable. another example relates to high priority systems.	system	485
let’s examine a code snippet.	code	16
let’s examine a code snippet.	snippet	21
table 17.1: signaling without mutex thread 2	thread	36
thread 1 while(answer < 42)	thread	0
answer++ pthread_cond_signal(cv) pthread_cond_wait(cv)	thread	10
the problem here is that a programmer expects the signal to wake up the waiting thread. since instructions are allowed to be interleaved without a mutex, this causes an interleaving that is confusing to application designers.	thread	80
the problem here is that a programmer expects the signal to wake up the waiting thread. since instructions are allowed to be interleaved without a mutex, this causes an interleaving that is confusing to application designers.	a mutex	145
note that technically the api of the condition variable is satisfied. the wait call happens-after the call to signal, and signal is only required to release at most a single thread whose call to wait happened-before.	thread	174
another problem is the need to satisfy real-time scheduling concerns which we only outline here. in a timecritical application, the waiting thread with the highest priority should be allowed to continue first. to satisfy this requirement the mutex must also be locked before calling pthread_cond_signal or pthread_cond_broadcast.	thread	140
17.12 condition wait example the call pthread_cond_wait performs three actions: 1. unlock the mutex. the mutex must be locked.	thread	39
2. sleeps until pthread_cond_signal is called on the same condition variable.	thread	17
condition variables are always used with a mutex lock. before calling wait, the mutex lock must be locked and wait must be wrapped with a loop.	a mutex	41
this is a pretty naive example, but it shows that we can tell threads to wake up in a standardized manner. in the next section, we will use these to implement efficient blocking data structures.	a struct	181
this is a pretty naive example, but it shows that we can tell threads to wake up in a standardized manner. in the next section, we will use these to implement efficient blocking data structures.	thread	62
this is a pretty naive example, but it shows that we can tell threads to wake up in a standardized manner. in the next section, we will use these to implement efficient blocking data structures.	block	169
this is a pretty naive example, but it shows that we can tell threads to wake up in a standardized manner. in the next section, we will use these to implement efficient blocking data structures.	section	119
17.13 implementing cvs with mutexes alone implementing a condition variable using only a mutex isn’t trivial. here is a sketch of how we could do it.	a mutex	87
typedef struct cv_node_ { pthread_mutex_t *dynamic; int is_awoken; struct cv_node_ *next; } cv_node; typedef struct { cv_node_ *head } cond_t	thread	27
typedef struct cv_node_ { pthread_mutex_t *dynamic; int is_awoken; struct cv_node_ *next; } cv_node; typedef struct { cv_node_ *head } cond_t	type	0
so how does this work? instead of allocating space which could lead to deadlock. we keep the data structures or the linked list nodes on each thread’s stack. the linked list in the wait function is created while the thread has the mutex lock this is important because we may have a race condition on the insert and removal. a more robust implementation would have a mutex per condition variable.	a struct	96
so how does this work? instead of allocating space which could lead to deadlock. we keep the data structures or the linked list nodes on each thread’s stack. the linked list in the wait function is created while the thread has the mutex lock this is important because we may have a race condition on the insert and removal. a more robust implementation would have a mutex per condition variable.	thread	142
so how does this work? instead of allocating space which could lead to deadlock. we keep the data structures or the linked list nodes on each thread’s stack. the linked list in the wait function is created while the thread has the mutex lock this is important because we may have a race condition on the insert and removal. a more robust implementation would have a mutex per condition variable.	a mutex	364
so how does this work? instead of allocating space which could lead to deadlock. we keep the data structures or the linked list nodes on each thread’s stack. the linked list in the wait function is created while the thread has the mutex lock this is important because we may have a race condition on the insert and removal. a more robust implementation would have a mutex per condition variable.	the data structure	89
what is the note about (dynamic)? in the pthread man pages, wait creates a runtime binding to a mutex. this means that after the first call is called, a mutex is associated with a condition variable while there is still a thread waiting on that condition variable. each new thread coming in must have the same mutex, and it must be locked.	thread	42
what is the note about (dynamic)? in the pthread man pages, wait creates a runtime binding to a mutex. this means that after the first call is called, a mutex is associated with a condition variable while there is still a thread waiting on that condition variable. each new thread coming in must have the same mutex, and it must be locked.	a mutex	94
what is the note about (dynamic)? in the pthread man pages, wait creates a runtime binding to a mutex. this means that after the first call is called, a mutex is associated with a condition variable while there is still a thread waiting on that condition variable. each new thread coming in must have the same mutex, and it must be locked.	a thread	220
hence, the beginning and end of wait (everything besides the while loop) are mutually exclusive. after the last thread leaves, meaning when head is null, then the binding is lost.	thread	112
the signal and broadcast functions merely tell either one thread or all threads respectively that they should be woken up. it doesn’t modify the linked lists because there is no mutex to prevent corruption if two threads call signal or broadcast now an advanced point. do you see how a broadcast could cause a spurious wakeup in this case? consider this series of events.	thread	58
1. some number more than 2 threads start waiting 2. another thread calls broadcast.	thread	27
3. that thread calling broadcast is stopped before it wake any threads.	thread	8
4. another thread calls wait on the condition variable and adds itself to the queue.	thread	11
5. broadcast iterates through and frees all of the threads.	thread	51
there is no assurance as to when the broadcast was called and when threads were added in a high-performance mutex. the ways to prevent this behavior are to include lamport timestamps or require that broadcast be called with the mutex in question. that way something that happens-before the broadcast call doesn’t get signaled after.	thread	67
1. broadcast is called on a waiting queue of threads 2. first thread is freed, broadcast thread is frozen. since the mutex is unlocked, it locks and continues.	thread	45
in high-performance systems, we want to make sure that each thread that calls wait isn’t passed by another thread that calls wait. with the current api that we have, we can’t assure that. we’d have to ask users to pass in a mutex or use a global mutex. instead, we tell programmers to always signal or broadcast before unlocking.	thread	60
in high-performance systems, we want to make sure that each thread that calls wait isn’t passed by another thread that calls wait. with the current api that we have, we can’t assure that. we’d have to ask users to pass in a mutex or use a global mutex. instead, we tell programmers to always signal or broadcast before unlocking.	a mutex	222
in high-performance systems, we want to make sure that each thread that calls wait isn’t passed by another thread that calls wait. with the current api that we have, we can’t assure that. we’d have to ask users to pass in a mutex or use a global mutex. instead, we tell programmers to always signal or broadcast before unlocking.	system	20
17.14.1 sequentially consistent sequentially consistent is the simplest, least error-prone and most expensive model. this model says that any change that happens, all changes before it will be synchronized between all threads.	thread	218
thread 1 1.0 atomic_store(x, 1) 1.1 y = 10 1.2 atomic_store(x, 0);	thread	0
thread 2 2.1 if (atomic_load(x) == 0) 2.2 y != 10 && abort();	thread	0
will never quit. this is because either the store happens before the if statement in thread 2 and y == 1 or the store happens after and x does not equal 2.	thread	85
17.14.2 relaxed relaxed is a simple memory order providing for more optimizations. this means that only a particular operation needs to be atomic. one can have stale reads and writes, but after reading the new value, it won’t become old.	memory	36
17.14.2 relaxed relaxed is a simple memory order providing for more optimizations. this means that only a particular operation needs to be atomic. one can have stale reads and writes, but after reading the new value, it won’t become old.	optimizations	68
-thread 1atomic_store(x, 1); atomic_store(x, 0);	thread	1
but that means that previous loads and stores don’t need to affect other threads. in the previous example, the code can now fail.	thread	73
but that means that previous loads and stores don’t need to affect other threads. in the previous example, the code can now fail.	code	111
17.14.3 acquire/release the order of atomic variables don’t need to be consistent – meaning if atomic var y is assigned to 10 then atomic var x to be 0 those don’t need to propagate, and a threa could get stale reads. non-atomic variables have to get updated in all threads though.	thread	266
17.14.4 consume imagine the same as above except non-atomic variables don’t need to get updated in all threads. this model was introduced so that there can be an acquire/release/consume model without mixing in relaxed because consume is similar to relax.	thread	103
17.15 actor model and goroutines there are a lot of other methods of concurrency than described in this book. posix threads are the finest grained thread construct, allowing for tight control of the threads and the cpu. other languages have their abstractions.	thread	116
we’ll talk about a language go that is similar to c in terms of simplicity and design, go or golang to get the 5 minute introduction, feel free to read the learn x in y guide for go. here is how we create a "thread" in go.	thread	208
this actually creates what is known as a goroutine. a goroutine can be thought of as a lightweight thread.	thread	99
internally, it is a worker pool of threads that executes instructions of all the running goroutines. when a goroutine needs to be stopped, it is frozen and "context switched" to another thread. context switch is in quotes because this is done at the run time level versus real context switching which is done at the operating system level.	thread	35
internally, it is a worker pool of threads that executes instructions of all the running goroutines. when a goroutine needs to be stopped, it is frozen and "context switched" to another thread. context switch is in quotes because this is done at the run time level versus real context switching which is done at the operating system level.	system	326
the advantage to gofuncs is pretty self explanatory. there is no boilerplate code, or joining, or odd casting void *.	code	77
var counter = 0; var mut sync.mutex; var wg sync.waitgroup; func plus() { mut.lock() counter += 1 mut.unlock() wg.done() } func main() { num := 10 wg.add(num); for i := 0; i < num; i++ { go plus() } wg.wait() fmt.printf("%d\n", counter); }	printf	213
main actor that will be performing the main instruction set. the other actor will be the counter. the counter is responsible for adding numbers to an internal variable. we’ll send messages between the threads when we want to add and see the value.	thread	201
although there is a bit more boilerplate code, we don’t have mutexes anymore! if we wanted to scale this operation and do other things like increment by a number, or write to a file, we can have that particular actor take care of it. this differentiation of responsibilities is important to make sure your design scales well. there are even libraries that handle all of the boilerplate code as well.	code	41
17.16 scheduling conceptually this section could be useful for those that like to analyze these algorithms mathematically if your co-worker asked you what scheduling algorithm to use, you may not have the tools to analyze each algorithm. so, let’s think about scheduling algorithms at a high level and break them down by their times. we will be evaluating this in the context of a random process timing, meaning that each process takes a random but finite amount of time to finish.	this section	30
17.16 scheduling conceptually this section could be useful for those that like to analyze these algorithms mathematically if your co-worker asked you what scheduling algorithm to use, you may not have the tools to analyze each algorithm. so, let’s think about scheduling algorithms at a high level and break them down by their times. we will be evaluating this in the context of a random process timing, meaning that each process takes a random but finite amount of time to finish.	section	35
different use cases will be discussed after. let the maximum amount of time that a process run be equal to s. we will also assume that there are a finite number of processes running at any given time c. here are some concepts from queueing theory that you’ll need to know that will help simplify the theories.	a process	81
1. queueing theory involves a random variable controlling the interarrival time – or the time between two different processes arriving. we won’t name this random variable, but we will assume that (1) it has a mean of λ and (2) it is distributed as a poisson random variable. this means the probability of getting a process exp(−λ) t units after getting another process is λ t ∗ t! where t! can be approximated by the gamma function when dealing with real values.	a process	313
3. we won’t make many assumptions about how much time it takes to run each process except that it will take a finite amount of time – otherwise this gets almost impossible to evaluate. we will denote two variables var(s) that µ1 is the mean of the waiting time and that the coefficient of variation c is defined as c 2 = e[s]2 to help us control for processes that take a while to finish. an important note is that when c > 1 we say that the running times of the process are variadic. we will note below that this rockets up the wait and response times for fcfs quadratically.	fcfs	557
17.16.2 round robin or processor sharing it is hard to analyze round robin from a probabilistic sense because it is so state based. the next job that the scheduler schedules requires it to remember the previous jobs. queueing theory developers have made an assumption that the time quanta is roughly zero – ignoring context switching and the like. this leads way into processor sharing. many different tasks can get worked on at the same time but experience a slowdown. all of these proofs will be adapted from harchol-balter’s book [? ]. we highly recommend checking out the books if you are interested. the proofs are intuitive for people who don’t have a background in queueing theory.	background	658
1. before we jump to the answer let’s reason about this. with our new-found abstraction, we essentially have an fcfs queue where we are going to be working on each job a little slower than before. since we are always working on a job e[w ] = 0 under a non-strict analysis of processor sharing though, the number of time that the scheduler waits is best e[s] approximated by the number of times the scheduler need to wait. you’ll need q service periods where q	fcfs	112
is the quanta, and you’ll need about e[n ] ∗ q time in between those periods. leading to an average time of e[w ] = e[s] ∗ e[n ] the reason this proof is non-rigorous is that we can’t assume that there will always be e[n ] ∗ q time on average in between cycles because it depends on the state of the system. this means we need to factor in various variations in processing delay. we also can’t use little’s law in this case because there is no real steady state of the system. otherwise, we’d be able to prove some weird things.	system	300
4. that naturally leads to the comparison, what is better? the response time is roughly the same comparing the non-strict versions, the wait time is roughly the same, but notice that nothing about the variation of the jobs is put in. that’s because rr doesn’t have to deal with the convoy effect and any variances associated, otherwise fcfs is faster in a strict sense. it also takes more time for the jobs to finish, but the overall turnaround time is lower under high variance loads.	fcfs	336
all higher and similar priority processes to x. the last bit of notation is that we will assume that the probability of k p getting a process of priority i is pi and naturally pj = 1 j=0	a process	132
because the addition of ρ x can only increase the sum, decrease the denominator or increase the overall function. this means that if one is priority 0, then a process only need to wait for the other p0 processes which there should be ρc/(1 − ρ0 ) p0 processes arrived before to process in fcfs order. then the next priority has to wait for all the others and so on and so forth.	a process	157
because the addition of ρ x can only increase the sum, decrease the denominator or increase the overall function. this means that if one is priority 0, then a process only need to wait for the other p0 processes which there should be ρc/(1 − ρ0 ) p0 processes arrived before to process in fcfs order. then the next priority has to wait for all the others and so on and so forth.	fcfs	289
which we compare with fcfs’ model of 1 1−ρ in words – you can work this out with experimenting distributions – if the system has a lot of low priority processes who don’t contribute a lot to the average load, your average wait time becomes much lower.	system	118
which we compare with fcfs’ model of 1 1−ρ in words – you can work this out with experimenting distributions – if the system has a lot of low priority processes who don’t contribute a lot to the average load, your average wait time becomes much lower.	fcfs	22
which says that the scheduler needs to wait for all jobs with a higher priority and the same to go before a process can go. imagine a series of fcfs queues that a process needs to wait your turn. using little’s law for different colored jobs and the formula above we can simplify this	a process	106
which says that the scheduler needs to wait for all jobs with a higher priority and the same to go before a process can go. imagine a series of fcfs queues that a process needs to wait your turn. using little’s law for different colored jobs and the formula above we can simplify this	fcfs	144
equation, we see again if we have a lot of high priority jobs that don’t contribute a lot to the load then our entire sum goes down. we won’t make too many assumptions about the service time for a job because that would interfere with our analysis from fcfs where we left it as an expression.	fcfs	253
3. as for a comparison with fcfs in the average case, it usually does better assuming that we have a smooth probability distribution – i.e. the probability of getting any particular priority is zero. in all of our formulas, we still have some probability mass to put on lower priority processes, bringing the expectation down. this statement doesn’t hold for all smooth distributions but for most real-world smoothed distributions (which tend to be smooth) they do.	the expectation	305
3. as for a comparison with fcfs in the average case, it usually does better assuming that we have a smooth probability distribution – i.e. the probability of getting any particular priority is zero. in all of our formulas, we still have some probability mass to put on lower priority processes, bringing the expectation down. this statement doesn’t hold for all smooth distributions but for most real-world smoothed distributions (which tend to be smooth) they do.	fcfs	28
17.16.4 shortest job first this is a wonderful reduction to priority. instead of having discrete priorities, we’ll introduce a process that takes s t time to get serviced. t is the maximum amount of time a process can run for, our processes cannot run infinitely long. that means the following definitions hold, overriding the previous definitions in priority 1. let ρ(x) =	the following	280
17.16.4 shortest job first this is a wonderful reduction to priority. instead of having discrete priorities, we’ll introduce a process that takes s t time to get serviced. t is the maximum amount of time a process can run for, our processes cannot run infinitely long. that means the following definitions hold, overriding the previous definitions in priority 1. let ρ(x) =	a process	125
5. this means if you want low wait times on average compared to fcfs, your distribution needs to be right-skewed.	fcfs	64
17.16.5 preemptive priority we will describe priority and sjf’s preemptive version in the same section because it is essentially the same as we’ve shown above. we’ll use the same notation as before. we will also introduce an additional term ci which denotes the variation among a particular class ci =	section	95
if lower priorities jobs come in at a higher service time variance, that means our average response times could go down, unless they make up most of the jobs that come in. think of the extreme cases. if 99% of the jobs are high priority and the rest make up the other percent, then the other jobs will get frequently interrupted, but high priority jobs will make up most of the jobs, so the expectation is still low. the other extreme is if one percent of jobs are high priority and they come in a low variance. that means the chances the system getting a high priority jobs that will take a long time is low, thus making our response times lower on average. we only run into trouble if high priority jobs make up a non-negligible amount, and they have a high variance in service times. this brings down response times as well as wait times.	the expectation	387
if lower priorities jobs come in at a higher service time variance, that means our average response times could go down, unless they make up most of the jobs that come in. think of the extreme cases. if 99% of the jobs are high priority and the rest make up the other percent, then the other jobs will get frequently interrupted, but high priority jobs will make up most of the jobs, so the expectation is still low. the other extreme is if one percent of jobs are high priority and they come in a low variance. that means the chances the system getting a high priority jobs that will take a long time is low, thus making our response times lower on average. we only run into trouble if high priority jobs make up a non-negligible amount, and they have a high variance in service times. this brings down response times as well as wait times.	system	539
taking the expectation among all processes we get	the expectation	7
we incur the same cost on response time and then we have to suffer an additional cost based on what the probabilities are of lower priority jobs coming in and taking this job out. that is what we call the average interruption time. this follows the same laws as before. since we have a variadic, pyramid summation if we have a lot of jobs with small service times then the wait time goes down for both additive pieces. it can be analytically shown that this is better given certain probability distributions. for example, try with the uniform versus fcfs or the non preemptive version. what happens? as always the proof is left to the reader.	fcfs	550
17.16.6 preemptive shortest job first unfortunately, we can’t use the same trick as before because an infinitesimal point doesn’t have a controlled variance. imagine the comparisons though as the same as the previous section.	section	217
service type	type	8
source address destination address options	address	7
figure 17.3: ip datagram divisibility 1. the first octet is the version number, either 4 or 6 2. the next octet is how long the header is. although it may seem that the header is a constant size, you can include optional parameters to augment the path that is taken or other instructions.	parameter	221
figure 17.3: ip datagram divisibility 1. the first octet is the version number, either 4 or 6 2. the next octet is how long the header is. although it may seem that the header is a constant size, you can include optional parameters to augment the path that is taken or other instructions.	the header	124
3. the next two octets specify the total length of the datagram. this means this is the header, the data, the footer, and the padding. this is given in multiple of octets, meaning that a value of 20 means 20 octets.	the header	84
8. the next octet is the protocol number. although protocols between different layers of the oci model are supposed to be black boxes, this is included, so that hardware can peer into the underlying protocol efficiently. take for example ip over ip (yes you can do that!). your isp wraps ipv4 packets sent from your computer to the isp in another ip layer and sends the packet off to be delivered to the website. on the reverse trip, the packet is "unwrapped" and the original ip datagram is sent to your computer. this was done because we ran out of ip addresses, and this adds additional overhead but it is a necessary fix. other common protocols are tcp, udp, etc.	address	554
8. the next octet is the protocol number. although protocols between different layers of the oci model are supposed to be black boxes, this is included, so that hardware can peer into the underlying protocol efficiently. take for example ip over ip (yes you can do that!). your isp wraps ipv4 packets sent from your computer to the isp in another ip layer and sends the packet off to be delivered to the website. on the reverse trip, the packet is "unwrapped" and the original ip datagram is sent to your computer. this was done because we ran out of ip addresses, and this adds additional overhead but it is a necessary fix. other common protocols are tcp, udp, etc.	the protocol	21
10. the source address is what people generally refer to as the ip address. there is no verification of this, so one host can pretend to be any ip address possible 11. the destination address is where you want the packet to be sent to. destinations are crucial to the routing process.	address	15
14. after: your data! all data of higher-order protocols are put following the header.	the header	75
17.17.2 routing the internet protocol routing is an amazing intersection of theory and application. we can imagine the entire internet as a set of graphs. most peers are connected to what we call "peering points" – these are the wifi routers and ethernet ports that one finds at home, at work, and in public. these peering points are then connected to a wired network of routers, switches, and servers that all route themselves. at a high level there are two types of routing 1. internal routing protocols. internal protocols are routing designed for within an isp’s network. these protocols are meant to be fast and more trusting because all computers, switches, and routers are part of an isp. communication between two routers.	type	459
17.17.2 routing the internet protocol routing is an amazing intersection of theory and application. we can imagine the entire internet as a set of graphs. most peers are connected to what we call "peering points" – these are the wifi routers and ethernet ports that one finds at home, at work, and in public. these peering points are then connected to a wired network of routers, switches, and servers that all route themselves. at a high level there are two types of routing 1. internal routing protocols. internal protocols are routing designed for within an isp’s network. these protocols are meant to be fast and more trusting because all computers, switches, and routers are part of an isp. communication between two routers.	section	65
also, isps need to be nice to each other. theoretically, an isp can handle a smaller load by forwarding all packets to another isp. if everyone does that then, no packets get delivered at all which won’t make customers happy at all. these two protocols need to be fair so the result works if you want to read more about this, look at the wikipedia page for routing here routing.	a page	346
17.17.5 kqueue when it comes to event-driven io, the name of the game is to be fast. one extra system call is considered slow.	system	95
17.17.5 kqueue when it comes to event-driven io, the name of the game is to be fast. one extra system call is considered slow.	kqueue	8
17.17.5 kqueue when it comes to event-driven io, the name of the game is to be fast. one extra system call is considered slow.	a system call	93
openbsd and freebsd have an arguably better model of asynchronous io from the kqueue model. kqueue is a system call that is exclusive the bsds and macos. it allows you to modify file descriptor events and read file descriptors all in a single call under a unified interface. so what are the benefits?	openbsd	0
openbsd and freebsd have an arguably better model of asynchronous io from the kqueue model. kqueue is a system call that is exclusive the bsds and macos. it allows you to modify file descriptor events and read file descriptors all in a single call under a unified interface. so what are the benefits?	system	104
openbsd and freebsd have an arguably better model of asynchronous io from the kqueue model. kqueue is a system call that is exclusive the bsds and macos. it allows you to modify file descriptor events and read file descriptors all in a single call under a unified interface. so what are the benefits?	kqueue	78
openbsd and freebsd have an arguably better model of asynchronous io from the kqueue model. kqueue is a system call that is exclusive the bsds and macos. it allows you to modify file descriptor events and read file descriptors all in a single call under a unified interface. so what are the benefits?	a system call	102
1. no more differentiation between file descriptors and kernel objects. in the epoll section, we had to discuss this distinction otherwise you may wonder why closed file descriptors are getting returned on epoll. no problem here.	section	85
2. how often do you call epoll to read file descriptors, get a server socket, and need to add another file descriptor? in a high-performance server, this can easily happen 1000s of times a second. as such, having one system call to register and grab events saves the overhead of having a system call.	system	217
2. how often do you call epoll to read file descriptors, get a server socket, and need to add another file descriptor? in a high-performance server, this can easily happen 1000s of times a second. as such, having one system call to register and grab events saves the overhead of having a system call.	a system call	286
3. the unified system call for all types. kqueue is the truest sense of underlying descriptor agnostic. one can add files, sockets, pipes to it and get full or near full performance. you can add the same to epoll, but linux’s whole ecosystem with async file input-output has been messed up with aio, meaning that since there is no unified interface, you run into weird edge cases.	system	15
3. the unified system call for all types. kqueue is the truest sense of underlying descriptor agnostic. one can add files, sockets, pipes to it and get full or near full performance. you can add the same to epoll, but linux’s whole ecosystem with async file input-output has been messed up with aio, meaning that since there is no unified interface, you run into weird edge cases.	type	35
3. the unified system call for all types. kqueue is the truest sense of underlying descriptor agnostic. one can add files, sockets, pipes to it and get full or near full performance. you can add the same to epoll, but linux’s whole ecosystem with async file input-output has been messed up with aio, meaning that since there is no unified interface, you run into weird edge cases.	kqueue	42
name malloc, free, calloc, realloc - allocate and free dynamic memory synopsis #include <stdlib.h> void void void void void	memory	63
feature test macro requirements for glibc (see feature_test_macros(7)): reallocarray(): _gnu_source description the malloc() function allocates size bytes and returns a pointer to the allocated memory. the memory is not initialized.	memory	194
feature test macro requirements for glibc (see feature_test_macros(7)): reallocarray(): _gnu_source description the malloc() function allocates size bytes and returns a pointer to the allocated memory. the memory is not initialized.	pointer	169
if size is 0, then malloc() returns either null, or a unique pointer value that can later be successfully passed to free().	pointer	61
the free() function frees the memory space pointed to by ptr, which must have been returned by a previous call to malloc(),	memory	30
the calloc() function allocates memory for an array of nmemb elements of size bytes each and returns a pointer to the allocated memory. the memory is set to zero. if nmemb or size is 0, then calloc() returns either null, or a unique pointer value that can later be successfully passed to free().	memory	32
the calloc() function allocates memory for an array of nmemb elements of size bytes each and returns a pointer to the allocated memory. the memory is set to zero. if nmemb or size is 0, then calloc() returns either null, or a unique pointer value that can later be successfully passed to free().	pointer	103
the realloc() function changes the size of the memory block pointed to by ptr to size bytes. the contents will be unchanged in the range from the start of the region up to the minimum of the old and new sizes. if the new size is larger than the old size, the added memory will not be initialized. if ptr is null, then the call is equivalent to malloc(size), for all values of size; if size is equal to zero, and ptr is not null, then the call is equivalent to free(ptr). unless ptr is null, it must have been returned by an earlier call to malloc(), calloc(), or realloc(). if the area pointed to was moved, a free(ptr) is done.	memory	47
the realloc() function changes the size of the memory block pointed to by ptr to size bytes. the contents will be unchanged in the range from the start of the region up to the minimum of the old and new sizes. if the new size is larger than the old size, the added memory will not be initialized. if ptr is null, then the call is equivalent to malloc(size), for all values of size; if size is equal to zero, and ptr is not null, then the call is equivalent to free(ptr). unless ptr is null, it must have been returned by an earlier call to malloc(), calloc(), or realloc(). if the area pointed to was moved, a free(ptr) is done.	block	54
the reallocarray() function changes the size of the memory block pointed to by ptr to be large enough for an array of nmemb elements, each of which is size bytes. it is equivalent to the call realloc(ptr, nmemb * size); however, unlike that realloc() call, reallocarray() fails safely in the case where the multiplication would overflow.	memory	52
the reallocarray() function changes the size of the memory block pointed to by ptr to be large enough for an array of nmemb elements, each of which is size bytes. it is equivalent to the call realloc(ptr, nmemb * size); however, unlike that realloc() call, reallocarray() fails safely in the case where the multiplication would overflow.	block	59
if such an overflow occurs, reallocarray() returns null, sets errno to enomem, and leaves the original block of memory unchanged.	memory	112
if such an overflow occurs, reallocarray() returns null, sets errno to enomem, and leaves the original block of memory unchanged.	block	103
return value the malloc() and calloc() functions return a pointer to the allocated memory, which is suitably aligned for any built-in type. on error, these functions return null. null may also be returned by a successful call to malloc() with a size of zero, or by a successful call to calloc() with nmemb or size equal to zero.	memory	83
return value the malloc() and calloc() functions return a pointer to the allocated memory, which is suitably aligned for any built-in type. on error, these functions return null. null may also be returned by a successful call to malloc() with a size of zero, or by a successful call to calloc() with nmemb or size equal to zero.	type	134
return value the malloc() and calloc() functions return a pointer to the allocated memory, which is suitably aligned for any built-in type. on error, these functions return null. null may also be returned by a successful call to malloc() with a size of zero, or by a successful call to calloc() with nmemb or size equal to zero.	pointer	58
the realloc() function returns a pointer to the newly allocated memory, which is suitably aligned for any built-in type and may be different from ptr, or null if the request fails. if size was equal to 0, either null or a pointer suitable to be passed to free() is returned. if realloc() fails, the original block is left untouched; it is not freed or moved.	memory	64
the realloc() function returns a pointer to the newly allocated memory, which is suitably aligned for any built-in type and may be different from ptr, or null if the request fails. if size was equal to 0, either null or a pointer suitable to be passed to free() is returned. if realloc() fails, the original block is left untouched; it is not freed or moved.	block	308
the realloc() function returns a pointer to the newly allocated memory, which is suitably aligned for any built-in type and may be different from ptr, or null if the request fails. if size was equal to 0, either null or a pointer suitable to be passed to free() is returned. if realloc() fails, the original block is left untouched; it is not freed or moved.	type	115
the realloc() function returns a pointer to the newly allocated memory, which is suitably aligned for any built-in type and may be different from ptr, or null if the request fails. if size was equal to 0, either null or a pointer suitable to be passed to free() is returned. if realloc() fails, the original block is left untouched; it is not freed or moved.	pointer	33
on success, the reallocarray() function returns a pointer to the newly allocated memory. on failure, it returns null and the original block of memory is left untouched.	memory	81
on success, the reallocarray() function returns a pointer to the newly allocated memory. on failure, it returns null and the original block of memory is left untouched.	block	134
on success, the reallocarray() function returns a pointer to the newly allocated memory. on failure, it returns null and the original block of memory is left untouched.	pointer	50
the following error: enomem out of memory. possibly, the application hit the rlimit_as or rlimit_data limit described in getrlimit(2).	the following	0
the following error: enomem out of memory. possibly, the application hit the rlimit_as or rlimit_data limit described in getrlimit(2).	memory	35
attributes for an explanation of the terms used in this section, see attributes(7).	this section	51
attributes for an explanation of the terms used in this section, see attributes(7).	section	56
+---------------------+---------------+---------+ |interface | attribute | value | |-----------------------------------------------| |malloc(), free(), | thread safety | mt-safe | |calloc(), realloc() | | | +---------------------+---------------+---------+ conforming to malloc(), free(), calloc(), realloc(): posix.1-2001, posix.1-2008, c89, c99.	thread	154
reallocarray() is a nonstandard extension that first appeared in openbsd 5.6 and freebsd 11.0.	openbsd	65
notes by default, linux follows an optimistic memory allocation strategy. this means that when malloc() returns non-null there is no guarantee that the memory is available. in case it turns out that the system is out of memory, one or more processes will be killed by the oom killer. for more information, see the description of /proc/sys/vm/overcommit_memory and /proc/sys/vm/oom_adj in proc(5), and the linux kernel source file documentation/vm/overcommit-accounting.	memory	46
notes by default, linux follows an optimistic memory allocation strategy. this means that when malloc() returns non-null there is no guarantee that the memory is available. in case it turns out that the system is out of memory, one or more processes will be killed by the oom killer. for more information, see the description of /proc/sys/vm/overcommit_memory and /proc/sys/vm/oom_adj in proc(5), and the linux kernel source file documentation/vm/overcommit-accounting.	system	203
notes by default, linux follows an optimistic memory allocation strategy. this means that when malloc() returns non-null there is no guarantee that the memory is available. in case it turns out that the system is out of memory, one or more processes will be killed by the oom killer. for more information, see the description of /proc/sys/vm/overcommit_memory and /proc/sys/vm/oom_adj in proc(5), and the linux kernel source file documentation/vm/overcommit-accounting.	memory allocation	46
normally, malloc() allocates memory from the heap, and adjusts the size of the heap as required, using sbrk(2). when allocating blocks of memory larger than mmap_threshold bytes, the glibc malloc() implementation allocates the memory as a private anonymous mapping using mmap(2). mmap_threshold is 128 kb by default, but is adjustable using mallopt(3). prior to linux 4.7 allocations performed using mmap(2) were unaffected by the rlimit_data resource limit; since linux 4.7, this limit is also enforced for allocations performed using mmap(2).	the heap	41
normally, malloc() allocates memory from the heap, and adjusts the size of the heap as required, using sbrk(2). when allocating blocks of memory larger than mmap_threshold bytes, the glibc malloc() implementation allocates the memory as a private anonymous mapping using mmap(2). mmap_threshold is 128 kb by default, but is adjustable using mallopt(3). prior to linux 4.7 allocations performed using mmap(2) were unaffected by the rlimit_data resource limit; since linux 4.7, this limit is also enforced for allocations performed using mmap(2).	mmap	157
normally, malloc() allocates memory from the heap, and adjusts the size of the heap as required, using sbrk(2). when allocating blocks of memory larger than mmap_threshold bytes, the glibc malloc() implementation allocates the memory as a private anonymous mapping using mmap(2). mmap_threshold is 128 kb by default, but is adjustable using mallopt(3). prior to linux 4.7 allocations performed using mmap(2) were unaffected by the rlimit_data resource limit; since linux 4.7, this limit is also enforced for allocations performed using mmap(2).	memory	29
normally, malloc() allocates memory from the heap, and adjusts the size of the heap as required, using sbrk(2). when allocating blocks of memory larger than mmap_threshold bytes, the glibc malloc() implementation allocates the memory as a private anonymous mapping using mmap(2). mmap_threshold is 128 kb by default, but is adjustable using mallopt(3). prior to linux 4.7 allocations performed using mmap(2) were unaffected by the rlimit_data resource limit; since linux 4.7, this limit is also enforced for allocations performed using mmap(2).	block	128
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	a struct	115
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	thread	28
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	mmap	541
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	memory	94
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	system	517
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	memory allocation	310
to avoid corruption in multithreaded applications, mutexes are used internally to protect the memory-management data structures employed by these functions. in a multithreaded application in which threads simultaneously allocate and free memory, there could be contention for these mutexes. to scalably handle memory allocation in multithreaded applications, glibc creates additional memory allocation arenas if mutex contention is detected. each arena is a large region of memory that is internally allocated by the system (using brk(2) or mmap(2)), and managed with its own mutexes.	each arena	442
crashes in malloc(), calloc(), realloc(), or free() are almost always related to heap corruption, such as overflowing an allocated chunk or freeing the same pointer twice.	pointer	157
see also valgrind(1), brk(2), mmap(2), alloca(3), malloc_get_state(3), malloc_info(3), malloc_trim(3), malloc_usable_size(3), mallopt(3), mcheck(3), mtrace(3), posix_memalign(3)	mmap	30
see also valgrind(1), brk(2), mmap(2), alloca(3), malloc_get_state(3), malloc_info(3), malloc_trim(3), malloc_usable_size(3), mallopt(3), mcheck(3), mtrace(3), posix_memalign(3)	valgrind	9
17.19 system programming jokes 0x43 0x61 0x74 0xe0 0xf9 0xbf 0x5f 0xff 0x7f 0x00 warning: authors are not responsible for any neuro-apoptosis caused by these “jokes.” - groaners are allowed.	system	6
17.19.1 light bulb jokes q. how many system programmers does it take to change a lightbulb?	system	37
17.19.2 groaners why did the baby system programmer like their new colorful blankie? it was multithreaded.	thread	97
17.19.2 groaners why did the baby system programmer like their new colorful blankie? it was multithreaded.	system	34
why are your programs so fine and soft? i only use 400-thread-count or higher programs.	thread	55
17.19.3 system programmer (definition) a system programmer is. . .	system	8
someone who knows sleepsort is a bad idea but still dreams of an excuse to use it.	sleepsort	18
someone who never lets their code deadlock. . . but when it does, it causes more problems than everyone else combined.	code	29
someone who doesn’t trust their process to run correctly without testing with the same data, kernel, compiler, ram, filesystem size,file system format, disk brand, core count, cpu load, weather, magnetic flux, orientation, pixie dust, horoscope sign, wall color, wall gloss and reflectance, motherboard, vibration, illumination, backup battery, time of day, temperature, humidity, lunar position, sun-moon, co-position. . .	system	120
a system program . . .	system	2
evolves until it has the potential to create, connect and kill other programs and consume all possible cpu, memory, network, . . . resources on all possible devices but chooses not to. today.	resources	131
evolves until it has the potential to create, connect and kill other programs and consume all possible cpu, memory, network, . . . resources on all possible devices but chooses not to. today.	memory	108
this chapter is meant to serve as a big "why are we learning all of this". in all of your previous classes, you were learning what to do. how to program a data structure, how to code a for loop, how to prove something.	a struct	158
this chapter is meant to serve as a big "why are we learning all of this". in all of your previous classes, you were learning what to do. how to program a data structure, how to code a for loop, how to prove something.	code	178
this is the first class that is largely focused on what not to do. as a result, we draw experience from our past in real ways. sit back and scroll through this chapter as we tell you about the problems of past programmers. even if you are dealing with something much higher level like web-development, everything relates back to the system.	system	333
18.1 shell shock required: appendix/shell this was a back door into most shells. the bug allowed an attacker to exploit an environment variable to execute arbitrary code.	code	165
this meant that in any system that uses environment variables and doesn’t sanitize their input (hint no one sanitized environment variable input because they saw it as safe) you can execute whatever code you want on other’s machines including setting up a web server.	code	199
this meant that in any system that uses environment variables and doesn’t sanitize their input (hint no one sanitized environment variable input because they saw it as safe) you can execute whatever code you want on other’s machines including setting up a web server.	system	23
lessons learned: on production machines make sure that there is a minimal operating system (something like busybox with dietlibc) so that you can understand most of the code in the systems and their effectiveness.	code	169
lessons learned: on production machines make sure that there is a minimal operating system (something like busybox with dietlibc) so that you can understand most of the code in the systems and their effectiveness.	system	84
put in multiple layers of abstraction and checks to make sure that data isn’t leaked. for example the above is a problem insofar as getting information back to the attackers if it is allowed to communicate with them. this means that you can harden your machine ports by disallowing connections on all but a few ports. also, you can harden your system to never perform exec calls to perform tasks (i.e. perform an exec call to update a value) and instead do it in c or your favorite programming language of choice. although you don’t have flexibility, you have peace of mind what you allow users to do.	system	344
18.2 heartbleed required: intro to c to put it simply, there were no bounds on buffer checking. the ssl heartbeat is super simple. a server sends a string of a certain length, and the second server is supposed to send the string of the length back. the problem is someone can maliciously change the size of the request to larger than what they sent (i.e. send “cat” but request 500 bytes) and get crucial information like passwords from the server. there is a relevant xkcd on it.	string	148
lessons learned: check your buffers! know the difference between a buffer and a string.	string	80
18.3 dirty cow required: processes/virtual memory dirty cow a process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. dirty cow is a vulnerability where a bunch of threads attempts to access the same piece of memory at the same time hoping that one of the threads flips the nx bit and the writable bit. after that, an attacker can modify the page. this can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell.	thread	226
18.3 dirty cow required: processes/virtual memory dirty cow a process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. dirty cow is a vulnerability where a bunch of threads attempts to access the same piece of memory at the same time hoping that one of the threads flips the nx bit and the writable bit. after that, an attacker can modify the page. this can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell.	a process	60
18.3 dirty cow required: processes/virtual memory dirty cow a process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. dirty cow is a vulnerability where a bunch of threads attempts to access the same piece of memory at the same time hoping that one of the threads flips the nx bit and the writable bit. after that, an attacker can modify the page. this can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell.	memory	43
18.3 dirty cow required: processes/virtual memory dirty cow a process usually has access to a set of read-only mappings of memory that if they try to write to they get a segfault. dirty cow is a vulnerability where a bunch of threads attempts to access the same piece of memory at the same time hoping that one of the threads flips the nx bit and the writable bit. after that, an attacker can modify the page. this can be done to the effective user id bit and the process can pretend it was running as root and spawn a root shell, allowing access to the system from a normal shell.	system	554
18.4 meltdown there is an example of this in the background section	background	49
18.4 meltdown there is an example of this in the background section	section	60
18.5 spectre check in the security section.	spectre	5
18.5 spectre check in the security section.	section	35
18.6 mars pathfinder required sections: synchronization and a bit of scheduling pathfinder link the mars pathfinder was a mission that tried to collect climate data on mars. the finder uses a single bus to communicate with different parts. since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. the architecture was pretty simple. there was a thread that controlled data along the information bus, communications thread,	thread	461
18.6 mars pathfinder required sections: synchronization and a bit of scheduling pathfinder link the mars pathfinder was a mission that tried to collect climate data on mars. the finder uses a single bus to communicate with different parts. since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. the architecture was pretty simple. there was a thread that controlled data along the information bus, communications thread,	system	364
18.6 mars pathfinder required sections: synchronization and a bit of scheduling pathfinder link the mars pathfinder was a mission that tried to collect climate data on mars. the finder uses a single bus to communicate with different parts. since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. the architecture was pretty simple. there was a thread that controlled data along the information bus, communications thread,	a thread	459
18.6 mars pathfinder required sections: synchronization and a bit of scheduling pathfinder link the mars pathfinder was a mission that tried to collect climate data on mars. the finder uses a single bus to communicate with different parts. since this was 1997, the hardware itself didn’t have advanced features like efficient locking so it was up to the operating system developers to regulate that with mutexes. the architecture was pretty simple. there was a thread that controlled data along the information bus, communications thread,	section	30
and data collection thread in with high, regular, and low priorities with respect to scheduling. the other caveat is that if an interrupt happened at some interval and a task is running and a task is to be scheduled, the task that has the higher priority wins.	thread	20
the pattern that caused everything to start failing was the data collection thread starts writing to the bus, the information bus thread is waiting on the data. then out the communication thread comes in to preempt the other lower priority thread while the lower priority thread still held the mutex. this means when the regular priority thread tried to lock the bus, the rover would deadlock. after some time the system would reset but isn’t good to leave to chance.	thread	76
the pattern that caused everything to start failing was the data collection thread starts writing to the bus, the information bus thread is waiting on the data. then out the communication thread comes in to preempt the other lower priority thread while the lower priority thread still held the mutex. this means when the regular priority thread tried to lock the bus, the rover would deadlock. after some time the system would reset but isn’t good to leave to chance.	system	414
18.7 mars again required sections: malloc mars the short of it is that they ran out of memory. the long of it is that they ran out of memory, disk space, and swap space. the moral of the story? make sure to write code that can handle file failures and can handle files when they close and go out of memory, so the operating system can hot swap files to free up memory. also clean up files, assume that your temp directory is roughly a hundredth or a thousandth of the total size and use that.	code	213
18.7 mars again required sections: malloc mars the short of it is that they ran out of memory. the long of it is that they ran out of memory, disk space, and swap space. the moral of the story? make sure to write code that can handle file failures and can handle files when they close and go out of memory, so the operating system can hot swap files to free up memory. also clean up files, assume that your temp directory is roughly a hundredth or a thousandth of the total size and use that.	memory	87
18.7 mars again required sections: malloc mars the short of it is that they ran out of memory. the long of it is that they ran out of memory, disk space, and swap space. the moral of the story? make sure to write code that can handle file failures and can handle files when they close and go out of memory, so the operating system can hot swap files to free up memory. also clean up files, assume that your temp directory is roughly a hundredth or a thousandth of the total size and use that.	system	324
18.7 mars again required sections: malloc mars the short of it is that they ran out of memory. the long of it is that they ran out of memory, disk space, and swap space. the moral of the story? make sure to write code that can handle file failures and can handle files when they close and go out of memory, so the operating system can hot swap files to free up memory. also clean up files, assume that your temp directory is roughly a hundredth or a thousandth of the total size and use that.	section	25
18.8 year 2038 required sections: intro to c 2038 this is issue that hasn’t happened yet. unix timestamps are kept as the number of seconds from a particular day (jan 1st 1970). this is stored as a 32 bit signed integer. in march of 2038, this number will overflow. this isn’t a problem for most modern operating system who store 64 bit signed integers which is enough to keep us going until the end of time, but it is a problem for embedded devices that we can’t change the internal hardware to. stay tuned to see what happens.	system	313
18.8 year 2038 required sections: intro to c 2038 this is issue that hasn’t happened yet. unix timestamps are kept as the number of seconds from a particular day (jan 1st 1970). this is stored as a 32 bit signed integer. in march of 2038, this number will overflow. this isn’t a problem for most modern operating system who store 64 bit signed integers which is enough to keep us going until the end of time, but it is a problem for embedded devices that we can’t change the internal hardware to. stay tuned to see what happens.	section	24
18.9 northeast blackout of 2003 required sections: synchronization 2003 a race condition triggered a series of undefined events in a system that caused the blackout of most of the northeastern part of north america for quite some time. this bug also turned off or caused the backup system and logging systems to fail so people didn’t even know of the bug for an hour. the exact bits that were flipped are unknown, but patches have been put into place.	system	133
18.9 northeast blackout of 2003 required sections: synchronization 2003 a race condition triggered a series of undefined events in a system that caused the blackout of most of the northeastern part of north america for quite some time. this bug also turned off or caused the backup system and logging systems to fail so people didn’t even know of the bug for an hour. the exact bits that were flipped are unknown, but patches have been put into place.	section	41
lessons learned: modularize your code to localize failures (i.e. keeping race conditions different between processes). if you need to synchronize among processes make sure your failure detection system is not interlaced with your system.	code	33
lessons learned: modularize your code to localize failures (i.e. keeping race conditions different between processes). if you need to synchronize among processes make sure your failure detection system is not interlaced with your system.	system	195
18.10 apple ios unicode handling required sections: intro to c crashing your iphone with text wonder why we teach string parsing? because it is a hard thing to do even for professional software developers.	code	19
18.10 apple ios unicode handling required sections: intro to c crashing your iphone with text wonder why we teach string parsing? because it is a hard thing to do even for professional software developers.	string	114
18.10 apple ios unicode handling required sections: intro to c crashing your iphone with text wonder why we teach string parsing? because it is a hard thing to do even for professional software developers.	section	42
this bug allowed a lot of undefined behavior when trying to parse a series of unicode characters. apple probably knows why this happened, but our guess is that the parsing of the string happens somewhere inside the kernel and a segfault is reached. when you get a segfault in the kernel, your kernel panics, and the entire device reboots.	code	81
this bug allowed a lot of undefined behavior when trying to parse a series of unicode characters. apple probably knows why this happened, but our guess is that the parsing of the string happens somewhere inside the kernel and a segfault is reached. when you get a segfault in the kernel, your kernel panics, and the entire device reboots.	string	179
18.11 apple ssl verification required sections: intro to c apple bug due to a stray goto in apple’s code, a function always returned that an ssl certificate was valid. naturally, hackers were able to get away with some pretty crazy site names.	code	100
18.11 apple ssl verification required sections: intro to c apple bug due to a stray goto in apple’s code, a function always returned that an ssl certificate was valid. naturally, hackers were able to get away with some pretty crazy site names.	section	38
18.12 sony rootkit installation required sections: intro to c/processes root kit scandal picture this. it’s 2005, limewire came a few years prior, the internet was a growing pool of illegal activities – not to say that is fixed now. sony knew that it didn’t have the computing power to police all the interwebs or get around the various technologies people were using to get around copyright protection. so what did they do?	section	41
with 22 million music cds, they required users to install a rootkit on their operating system, so sony can monitor the device for unethical activities.	system	87
with 22 million music cds, they required users to install a rootkit on their operating system, so sony can monitor the device for unethical activities.	a rootkit	58
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.	this rootkit	89
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.	code	190
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.	system	131
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.	type	322
privacy concerns aside, and believe me there are a lot of them, the big problem was that this rootkit is a backdoor for everyone’s systems if programmed incorrectly. a rootkit is a piece of code usually installed kernel-side that keeps track of almost anything that a user does. what websites visited, what clicks or keys typed etc. if a hacker finds out about this and there is a way to access that api from the user space level, that means any program can find out important information about your device. needless to say, people were angry.	a rootkit	166
lessons learned: get an antivirus and/or apparmor and make sure that an application is only requesting permissions that make sense. if you are torn, try something like windows sandbox or keep a sacrificial vm around to see if installing it makes your computer horrible. don’t trust certificates trust code.	code	301
18.13 civilization and ghandi required sections: intro to c ghandi’s aggressiveness this is probably well known to gamers why someone as (in real life) non-violent as ghandi was aggressive in the video game civilization. in the original, the game kept aggressiveness as an unsigned integer. during the game, the integer could be decremented and then the problem ensued because ghandi was already at zero. this caused him to become the most aggressive character in the game.	section	39
18.14 the woes of shell scripting required sections: intro to c/appending steam there was a simple bug in steam that caused steam to remove all of your files in the form of something like this	section	43
what happens if $0 or the first parameter passed into a script doesn’t exist? you move to root, and you delete your entire computer.	parameter	32
lessons learned: do parameter checks, always always always set -e on a script and if you expect a command to fail, explicitly list it. you can also alias rm to mv and then delete the trash later.	parameter	20
18.15 appnexus double free required sections: intro to c/malloc double free appnexus uses an asynchronous garbage collector that reclaims different parts of the heap when it believes that objects are unused. the architecture is that an element is in the unavailable list and then it is taken out to a to-be-freed list. after a certain time if that element was unused, it is freed and added to the free list. this is fine until two thread try to delete the same object at once, adding to the list twice. after less time, one of the objects was deleted, the delete was announced to other computers.	the heap	157
18.15 appnexus double free required sections: intro to c/malloc double free appnexus uses an asynchronous garbage collector that reclaims different parts of the heap when it believes that objects are unused. the architecture is that an element is in the unavailable list and then it is taken out to a to-be-freed list. after a certain time if that element was unused, it is freed and added to the free list. this is fine until two thread try to delete the same object at once, adding to the list twice. after less time, one of the objects was deleted, the delete was announced to other computers.	thread	431
18.15 appnexus double free required sections: intro to c/malloc double free appnexus uses an asynchronous garbage collector that reclaims different parts of the heap when it believes that objects are unused. the architecture is that an element is in the unavailable list and then it is taken out to a to-be-freed list. after a certain time if that element was unused, it is freed and added to the free list. this is fine until two thread try to delete the same object at once, adding to the list twice. after less time, one of the objects was deleted, the delete was announced to other computers.	section	36
lessons learned: avoid making hacky software if you need to. modularize, and set memory limits, and monitor different parts of your code and optimize by hand. there is no general catch-all garbage collector that fits everyone. even highly tested ones like the jvm need some nudges if you want to get performance out of them.	code	132
lessons learned: avoid making hacky software if you need to. modularize, and set memory limits, and monitor different parts of your code and optimize by hand. there is no general catch-all garbage collector that fits everyone. even highly tested ones like the jvm need some nudges if you want to get performance out of them.	memory	81
18.16 att cascading failures - 1990 required sections: intro to c explanation the bug is explained well at the link above. we recommend reading to learn more. a series of network delays that caused some telephone switches across the country to think that other switches were operable when they weren’t. when the switches came back online, they realized they had a huge backlog of calls to route and began doing so. other routing failures and restarts only compounded the problem.	section	45
lessons learned: not using c would’ve actually helped here because of more rigorous fuzzing (though c++ in this day and age would be worse with its language constructs). the real moral of the story is networks are random and expect any jump at any point in your code. that means writing simulations and running them with random delays to figure out bugs before they happen.	code	262
