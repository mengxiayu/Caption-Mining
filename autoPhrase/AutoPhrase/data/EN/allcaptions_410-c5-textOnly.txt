This lecture is about the World
This lecture is about the Word
Association mining an analysis.
Association mining and analysis.
In this lecture we're going to talk
about how to mine associations of words
from text.
This is an example of knowledge about
the natural language that we can mine
natural language that we can mine
from text data.
Here's the outline.
Working the first talk about what is
We are gooing to first talk about what is
water Association and then.
word Association and then.
word Association and then
Explain why discovering such
explain why discovering such
relationships useful and finding a
relations is useful and finding a
relations is useful and finally we are
working to talk about some general
going to talk about some general
ideas about how to mine water
ideas about how to mine word
associations in general.
associations. In general
There are two water relations, and
there are two word relations, and
these are quite basic.
One is called a parody grammatical
One is called a paradigmatic
relation, the other is syntagmatic
relations.
A&B have paradigmatic relations if they
A&B have paradigmatic relation if they
can be substituted for each other.
That means.
That means
That means,
The two words that have paradigmatic
the two words that have paradigmatic
relation would be in the same semantic
class or syntactic class, and we can in
general replace one by the other
without affecting the understanding of
the sentence.
That means we would still have a valid
sentence, for example, cat and dog.
sentence. For example, cat and dog.
At these two words have
And these two words have
paradigmatically relation becausw.
paradigmatic relation because
They are in the same class.
they are in the same class.
they are in the same class
Of animal.
of animal.
An in general, if it replace cat with
And in general, if it replace cat with
And in general, if we replace cat with
dog in a sentence, the sentence would
still be a valid sentence that you can
make sense of.
Similarly, Monday and Tuesday have bear
Similarly, Monday and Tuesday have
diplomatic relation.
paradigmatic relation.
The second kind of religion is called
The second kind of relation is called
syntagmatic relation.
In this case, the two words that have
this relation can be combined with each
other.
So A&B have syntagmatic relation.
So A&B have syntagmatic relation
If they can be combined with each other
in a sentence.
That means these two words are
semantically related.
So for example, Cat and sit a related
So for example, Cat and sit are related
becausw a cat can sit somewhere.
because a cat can sit somewhere.
Similarly, car and drive are related.
Similarly, car and drive are related
Semantically, and they can be combined
semantically, and they can be combined
with each other to convey meaning.
However, in general we cannot replace
cat with sit in a sentence or car with
drive in a sentence to still get a
valid sentence.
Meaning that if we do that, the
sentence will become somewhat
meaningless.
So this is different from paradigmatic
relations and these two relations are
relation and these two relations are
in fact the soul fundamental, that they
in fact so fundamental, that they
can be generalized to capture basically
can be generalized to capture basic
relations between units in arbitrary
sequences.
And definitely they can be generalized
And definitely they can be generalized to
describe relations of any items in the
language.
So A&B don't have to be worse and they
So A&B don't have to be words and they
can be phrases example.
And they can even be more complex
phrases.
phrases
Then just a noun phrase.
than just a noun phrase.
If you think about the general problem
of the sequence mining, then we can
think about the units in the sequence
data, and then we think of paradigmatic
relation as relations that applied to
relation as relations that are applied to
units that tend to occur in similar
locations in a sentence or in.
locations in a sentence or in
In a sequence of their elements in
a sequence of their elements in
a sequence of data elements in
general.
So they occur in similar locations
relative to the neighbors.
relative to the neighbors
In the sequence.
in the sequence.
Syntac medical relation, on the other
Syntagmatic relation, on the other
Syntagmatic relation on the other
hand, is related to Co occurring
hand, is related to co-occurring
elements that tend to show up in the
same sequence.
So these two are complementary.
So these two are complementary
An basically relations of words, and
and basically relations of words, and
we're interested in discovering them
automatically from text data.
Discovering such world relations has
many applications.
First, such relations can be directly
useful for improving accuracy of
minutes and help tasks, and this is
many NLP tasks, and this is
because this is part of our knowledge
about the language.
So if you know these two words or
synonyms, for example, and then you can
help a lot of tasks.
An grandma learning can be also done by
And grammer learning can be also done by
And grammar learning can be also done by
using such techniques because.
using such techniques because
If we can learn paradigmatic relations,
then we form classes of words.
Syntactic classes.
Syntactic classes for example.
For example, an if we learn syntagmatic
And if we learn syntagmatic
relations, then we would be able to
know the rules for putting together a
larger expression based on component
expressions.
So we'll learn the structure and what
can go with what else.
Water relations can be also very useful
Word relations can be also very useful
for many applications in tax, retrieval
for many applications in text retrieval
and mining.
For example, in search in text
retrieval we can use word associations
to modify a query.
And this can be used to introduce
additional related words to a query to
make the query more effective.
It's often called query expansion.
Or you can use related words with
Or you can use related words to
suggested related queries to the user
suggest related queries to the user
to explore the information space.
Another application is to use water
Another application is to use word
associations to automatically construct
the topical map for browsing where we
the topic map for browsing where we
can have words as nodes.
can have words as nodes
An Association says Edge is a user
and associations as edge. is a user
and associations as edge. The user
could navigate from one world to
could navigate from one word to
another to find information in the
information space.
Finally, such water sessions can also
Finally, such word associations can also
be used for compare and summarize
be used to compare and summarize
opinions.
For example, we might be interested in
understanding positive and negative
opinions about iPhone 6.
In order to do that, we can look at
what words are most strongly associated
with a feature.
with a feature word
Water, like the battery in positive
like the battery in positive
versus negative reviews.
Such a syntagmatic relations would help
us show the detailed opinions about the
product.
So how can we discover such
associations automatically?
Now?
Now
Now,
Here are some intuitions about how to
here are some intuitions about how to
do that.
Let's first look at the parallel Matic
Let's first look at the paradigmatic
relation.
Here we essentially can take advantage
of similar Contacts.
of similar context.
So here you see some simple sentences
about cat and dog.
I can see at the general occur in
You can see they generally occur in
similar context.
And that, after all, is the definition
of paradigmatic relations.
of paradigmatic relation.
So on the right side you can see I
extracted explicitly the context of cat
and dog from this small sample of text
data.
So I have taken away cat and dog from
the corresponding sentences so that you
can see just the context.
Now of course we can have different
perspectives to look at the context.
For example, we can look at the what
words occur in the left part of this
context.
So we can call this left context what
So we can call this left context. What
would occur before we see cat, cat or
words occur before we see cat, cat or
dog.
So you can see in this case clearly dog
and cat have similar left context.
You generously he's cat or my cat, and
You generally say his cat or my cat, and
you say also my dog and his dog, so
you say also my dog and his dog. So
that makes them similar in the left
convex.
context.
Similarly, if you look at the words
that occur after cat and dog, which we
can call right context and they also
very similar in this case, of course
it's extreme case where you only see
eats an.
eats and
In general you will see many other
words.
Of course that can follow cat and not.
Of course that can follow cat and dog.
You can also even look at the general
context.
And that might improve the oil words in
And that might improve the all words in
the sentence or in sentence is around
the sentence or in sentences around
this world.
this word.
And even in the general convex you also
And even in the general context you also
see some similarity between the two
words.
So this is just suggesting that we can
discover paradigmatic relation by
looking at the similarity of context of
words.
So for example, if we think about the
following questions, how similar
following questions, how similar are
context of cat and context of dog?
In contrast, how similar are context of
cat and context of computer?
Now intuitively, with the image in the
Now intuitively, with imagine in the
Now intuitively, with imagine the
context of Catan, context of dog would
context of Cat, context of dog would
context of Cat and context of dog would
be more similar than the.
be more similar than the
As context of captan context of
context of cat and context of
computer, that means the first in the
first case, the similarity value would
be high.
Between the context of cat and dog,
whereas in the second the similarity
between contexts of cat and computer
would be low because they are.
would be low because they are
Not having apparently medical
not having paradigmatic
relationship.
And then imagine what words occur after
computer.
In general there will be very different
In general they will be very different
from what words occur after cat.
So this is the basic idea about
So this is the basic idea of
discovering paradigmatic relations.
discovering paradigmatic relation.
What about the syntagmatic relation?
Here we were going to exploit the
Here we we are going to explore the
correlated occurrences again based on
the definition of cinematic relation.
the definition of syntagmatic relation.
Here you see the same sample of text.
But here we are interested in knowing
what other words are correlated with
the verb eats.
And what would can go with eat?
And what words can go with eat?
And if you look at the right side of
the slide and you see I've taken away
the slide and you will see I've taken away
the two words around eats.
I've taken with word to its left and
I've taken away the word to its left and
also the world to its right.
also the world to its right,
In each sentence.
And then we can ask the question what
was tend to occur to the left of it.
words tend to occur to the left of it.
words tend to occur to the left of eat.
words tend to occur to the left of eat
Anne, what was tend to occur to the
and what word tend to occur to the
and what words tend to occur to the
right of each?
right of eat?
Now thinking about this question would
help us discover Syntagmatic relations.
Becausw syntagmatic relation
Because syntagmatic relation
essentially captures such correlations.
So the important question to ask her
So the important question to ask
for symptomatic a relation is whenever
for syntagmatic relation is whenever
eats occurs.
eats occurs,
What other words also tend to occur?
what other words also tend to occur?
So the question here has to do with
whether there are some other words that
tend to Co occur together with its
tend to Co occur together with eats,
tend to co-occur together with eats,
meaning that whenever you see it, you
meaning that whenever you see eat, you
tend to see the other words.
An if you don't see, it's probably you
And if you don't see, it's probably you
And if you don't see eat, probably you
don't see other words often either.
So this intuition can help us discover
select medical relations.
syntagmatic relations.
Again, consider example.
Now again, consider example.
Now again, consider example
Now again, consider example-
How helpful is the occurrence of eats
for predicting occurrence of meat?
Price of knowing whether it occurs in a
knowing whether eat occurs in a
knowing whether eats occurs in a
sentence with generally help us predict
sentence would generally help us predict
it.
the
Whether we'd also occurs indeed as if
Whether meat also occurs indeed as if
we will see eats occur in a sentence,
and that should increase the chance
that meet with also occur.
that meat will also occur.
In contrast, if you look at the
question in the bottom, how helpful is
occurrence of it's for predicting the
occurrence of eats for predicting the
occurrence of text?
Becaused eats and text are not really
Because eats and text are not really
related, so knowing whether it's a curd
related, so knowing whether eats occurred
in a sentence doesn't really help us
predict.
predict
Weather text also occurs in the
whether text also occurs in the
sentence.
So this is in contrast to the question
about eats an meet.
about eats and meat.
Is also how helps explain the intuition
This also helps explain the intuition
behind the methods for discovering
syntagmatic relation.
Mainly we need to capture the
correlation between the occurrences of
towards.
two words.
So to summarize, the general ideas for
discovering water associations or the
discovering word associations or the
discovering word associations are the
following.
For paradigmatically relation with
For paradigmatically relation we
represent each word by its contents,
represent each word by its context,
and then compute the context
similarity.
We can assume the words that have high
We can gonna assume the words that have high
We are gonna assume the words that have high
context similarity to have parity man.
context similarity to have paradigmatic
Medical relation
relation
relation.
For syntax medical relation, we will
For syntagmatic relation, we will
count how many times towards occur
count how many times two words occur
together in a context which can be a
sentence to paragraph or a document
sentence, paragraph or a document
even.
And we're going to compare their Co
occurrences with their individual
occurrences.
We're going to assume words with high
Co occurrences, but relatively low
co-occurrences, but relatively low
individual occurrences to have
syntagmatic relations because they tend
to occur together, and they don't
usually occur alone.
Note that the paralegal Matic relation
Note that the paradigmatic relation
and syntagmatic relation, or actually
and syntagmatic relation, are actually
closely related.
In that parametrically related words
In that paradigmatically related words
tend to have cinematic relation with
tend to have syntagmatic relation with
the same world.
the same word that
They tend to be associated with the
they tend to be associated with the
same world, and that suggests that we
same word, and that suggests that we
can also do join the discovery of the
two relations.
So these general ideas can be
implemented in many different ways, and
the course won't cover all of them, but
we will cover at least some of the
methods that effective for discovering
these relations.
This lecture is about the probabilistic
latent semantic analysis or P LSA.
In this lecture we're going to
introduce probabilistic latent semantic
analysis, often called PLA.
analysis, often called the PLSA.
Say this is the most basic topic model.
Also, one of the most useful topic
models.
Now, this kind of models can in general
be used to mine multiple topics from
text documents, and PSA is one of the
text documents, and PLSA is one of the
most basic topic models for doing this,
so let's first examine this problem in
a little more detail.
Here I show a sample article which is a
blog article about Hurricane Katrina.
An I showed some sample topics, for
example government response, flooding
of the city in new orlean's donation
and the background.
You can see in the article we use words
from all these distributions.
So we first for example.
See there's a criticism of government
response, and this is followed by the
discussion of flooding of the city and
donation, etc.
We also see background words or mixed
with them, so the goal of topic
analysis here is try to decode these
topics behind the text.
So segment of the topics to figure out
which words are from which distribution
and to figure out the first one of
these topics.
So how do we know there's a topic about
government response?
There is a public about the flooding of
the city.
So these are the tasks of topical
model.
If we can discover these topics can
color this words as you see here to
separate the different topics, then you
can do a lot of things such as
summarization or segmentation of the
topics, clustering of sentences, etc.
So the formal definition of the problem
of mining multiple topics from text is
shown here, and this is actually a
slide that you have seen in the earlier
lecture, so the input is the
collection, the number of topics and
vocabulary set.
And of course, the text data right?
And then the output is of two kinds.
One is the topic category
characterization Seedies HCI is a water
distribution and 2nd it's the topic
coverage for each document.
These are pie some ideas and they tell
us which document covers which topic to
what extent.
So we hope to generate these as output
because there are many useful
applications if we can do that.
So the idea of PSA is actually very
So the idea of PLSA is actually very
similar to the two component mixture
model that we have already introduced.
The only difference is that we're going
to have more than two topics.
Otherwise it's essentially the same.
So here I illustrate how we can
generate the text that I was multiple
topics.
And naturally, in all cases of
probabilistic modeling, would want to
figure out the likelihood function.
So we will also ask the question what's
the probability of observing a world W
from such a mixture model?
Now if you look at this picture and
compare this with the picture that you
have seen earlier, you will see the
only difference is that we have added
more topics here.
So before we have just one topic
besides the background topical, but now
we have more topics.
Specifically we have K topics.
Now all these are topics that we assume
that exist in the text data, so the
consequences that our switch for
choosing a topic now is multiway switch
before it's just a two way switch.
Going to think of as flipping a coin.
But now we have multiple is.
First we can flip a coin to decide
whether we will talk about the
background.
So it's the background.
Lambda sub B versus non background.
So this one minus Lambda B gives us the
probability of actually choosing a
topic.
And background help.
After we have made this decision, we
have to make another decision to choose
one of these K distributions.
So there's a key way.
Switch here, and this is characterized
by the pies, and there's someone.
So this is just the different designs
So this is just the different design
of switches, a little bit more
complicated, but once we decide which
distribution to use, the rest is the
same.
We're going to generate the world by
using one of these distributions,
assume here.
OK, so now let's look at this question
about the like hold.
So what's the probability of observing
a word from such a distribution?
What do you think?
Now we've seen this problem many Times
Now, and if you recall, it's generally
a sum over all the different
possibilities of generating the world.
So let's first look at the how the
world can be generated from the
background model.
The probability that the world is
generated from the background model is
Lambda multiplied by the probability of
the world from the background model,
right?
Two things must happen.
First, we have to have chosen the
background model.
And that's probability of Lambda sub B
and then the second we must have
actually obtained the world W from the
background, and that's probability of W
given sit out submit.
OK, so similarly we can figure out the
probability of observing the world from
another topic.
A topical sealer sabki.
A topical theta sub k.
Now notice that here's a product of
three terms, and that's the cause.
The choice of topic set us up.
The choice of topic theta sub k.
The choice of topic theta sub
K only happens if two things happen.
One is we decided not to talk about
background, so that's probability 1
minus Lambda subby.
minus Lambda sub b.
Second, we also have to actually
choose.
Set us up.
K among these K topics.
So that's probability of serious up
cake or pie.
And similarly, the probability of
generating the water from the second
topic and his first opera popular like
topic and his first topic popular like
what you're seeing here and then.
So in the end, the probability of
observing the world is just a sum of
all these cases.
And I have to stress again, this is a
very important formula to know becausw.
very important formula to know because.
This is really key to know to
understanding all the topic models and
for understanding all the topic models and
indeed a lot of mixture models, so make
sure that you really understand the
probability.
Of W is indeed the some of these terms.
So next, once we have the likelihood
function, we would be interested in
knowing the parameters right?
So to estimate the premise.
So to estimate the parameters.
But first let's put all these together
to have the complete likelihood
function for PSA.
function for PLSA.
Now the first line shows the
probability of a word as illustrated on
the previous slide and this is an
important formula as I said.
And so let's take a closer look at
this.
After that contains all the important
parameters.
So first we see Lambda sub be here.
So first we see Lambda sub b here.
This represents the percentage of
background words.
That would believe exist in the text
data and this can be unknown value that
we set empirically.
2nd we see the background language
second we see the background language
second, we see the background language
model and typically we also assume this
is known.
We can use a large collection of text
or use all the tests that we have
available to estimate the water
distribution.
Now next in the rest of this formula.
Excuse me, you see 2 interesting kinds
Excuse me, you see two interesting kinds
of parameters.
Those are the most important parameters
that we are asked, so one is pies and
these are the coverage of topic in the
document.
And the other is the word distributions
that characterize all the topics.
So the next line then is simply to plug
this in to calculate the probability of
document.
This is again of the familiar form
where you have some and you have
account of world in the document and
then log of a probability.
Now it's a little bit more complicated
than the two component because now we
have more components.
So the sum involves more terms and then
this line is just the Holder for the
this line is just the like holder for the
whole collection and it's very similar.
Just accounting for more documents in
the collection.
So what are the unknown primers?
I already said there are two kinds on
his coverage.
One is awarded distributions.
Again, it's a useful exercise for you
to figure out exactly how many premise
there on here.
How many unknown parameters are there?
Now trying to figure out that question
would help you understand the model in
more detail, and it would also allow
you to understand what would be the
output that we generate when we use PSA
output that we generate when we use PLSA
to analyze text data, and these are
precisely the unknown parameters.
So after we have obtained the
likelihood function shown here, the
next is to worry about parameter
estimation.
And we can do the usual thing.
Maximum likelihood estimator.
So again, it's a constrained
optimization problem like what we have
seen before, only that we have a
collection of text and we have more
parameters to estimate and we still
have two constraints, different
constraint, two kinds of constraints.
One is awarded distributions.
All the words must have probabilities
that sum to 141 distribution.
The other is the topic coverage
distribution.
Anna Document will have to cover
precisely these K topics, so the
probability of covering each topical
would have to sum to one.
So at this point it's basically where
they find applied math problem.
You just need to figure out the
solutions to optimization problem.
There's a function with many variables
and we need to just figure out the
values of these variables to make the
function which its maximum.
So I just showed you that empirically
the likelihood will converge, but
theoretically it can also be proved
that EM algorithm with converge to a
local maximum.
So here is just the illustration of
what happened an A detailed explanation
this.
Require more.
Knowledge about some of the
inequalities that we haven't really
covered yet.
So here what you see is on the X
dimension.
We have set up value.
This is the parameter that we left on
the Y axis.
We see the likelihood function.
So this curve is reaching or like
roller function, right?
So this one.
And this is the one that we hope to
maximize an we hope to find a set of
value at this point to maximize this.
But in the case of mixture model, we
cannot easily find the analytical
solution to the problem.
So we have to resolve a numerical
algorithm.
An EM algorithm is such an algorithm.
It's a Hill climb algorithm that would
mean you start with some random guess.
Let's say you start from here.
That's your starting point and then you
try to improve this by moving this to
another point where you can have a
higher like recorder.
So that's the idea of Hill climbing.
Any in the MRI was the way we achieve
this is to do two things.
First will fix a lower bound of
likelihood function, so this is the
lower bound you can see here.
An once we fit the lower bound we can
then maximise the lower bound and of
course the reason why this works is
because the lower bound is much easier
to optimize so we know our current gas
is here an by maximizing the lower
bound will move this point to the top
two here.
I.
And that we can then map to the
original like role function.
We find this point.
Be cause it's a lower bound, we are
guaranteed to improve this gas.
Right, because we improve our lower
bound and then the original lighter
Holder curve which is above this lower
bound will definitely be improved as
well.
I so we already know it's improving the
lower bound, so we definitely improve
this original like record function
which is above this lower bound.
So in our example, the current gas is
parameter value given by the current
generation and then the next guest is
the RE estimated parameter values.
From this illustration you can see the
next gas is always better than the
current gas unless it has reached the
maximum where it would be stuck there.
So the two would be equal.
So the E step is basically to compute
this lower bound.
And we don't direct it, just computed
this likely or function, but we
computed the latent variable values
and.
These are basically part of this lower
bound.
This helps determine the lower bound
the M step on the other hand, is to
maximize the lower bound.
It allows us to move parameters to a
new point.
And that's why EML is gone.
The little converge to a local maximum.
Now, as you can imagine, when we have
many local Maxima, we also have to
repeat the EML with multiple times in
order to figure out which one is the
actual global maximum.
And this actually in general is a
difficult problem in numerical
optimization.
So here for example, how do we start
from here?
Then we gradually just climb up to this
top, so that's not optimal, and we'd
like to climb up all the way to here.
So the only way to climb up to this
here.
This will start from somewhere here or
here.
Right so.
In the EM algorithm, we generally would
have to start from different points or
have some other way to determine a good
initial starting point.
To summarize, in this lecture we
introduce the EM algorithm.
This is a general algorithm for
computing.
Maximum regular is made of all kinds of
mixture models.
So not just for our simple mixture
model and so here climbing algorithm so
can only converge it or local maximum,
and it would depend on initial points.
The general idea is that we will have
two steps to improve the estimate of
parameters in the E step.
We roughly all augmenting our data by
predicting values of useful hidden
variables that we would use to simplify
the estimation.
In our case, this is the distribution
that has been used to generate the
world.
In the end step, then would exploit
such augmented data, which would make
it easier to estimate the distribution
to improve the estimate of parameters.
Here improve is guaranteed in terms of
the likelihood function.
Note that it's not necessary that we
will have a stable converged parameter
values, even though the likelihood
function is insured to increase.
There are some properties that have to
be satisfied in order for the
parameters also too.
Convert it to some stable value.
Now he thought data augmentation is
done probabilistically.
That means we're not going to just say
exactly what's the value of a hidden
variable, but we're going to have a
probability distribution over the
possible values of these hidden
variables, so this causes a split of
counts of events probabilistically and
in our case, will split the world
counts between the two distributions.
This lecture is about the syntagmatic
relation discovery.
An entropy.
In this lecture, we're going to
continue talking about word Association
mining.
mning.
mining.
In particular, we can talk about how to
discover syntactic medical relations.
discover syntagmatic relations.
And we're going to start with the
introduction of entropy, which is the
basis for designing some measures for
discovering such relations.
By definition, Syntagmatic relations
hold between words that have correlated
Co occurrences.
That means when we see one word occurs
in the context, we tend to see the
occurrence of the other world.
occurrence of the other word.
So take a more specific example here.
So take a more specific example, here
We can ask the question whenever eats
we can ask the question whenever eats
occurs, but other words also tend to
occur.
Not looking at the sentence is on the
Now looking at the sentence is on the
left.
We see some words that might occur
together with eats like a cat, dog or
fish.
fish
These bite and.
is right.
But if I take them out and if you look
at the right side where we only show
eats and some other words.
The question that is, can you predict
what other words occur?
To the left or to the right.
Right, so this would forces will think
Right, so this would force us to think
about what other words are associated
with heat.
with eats.
If they are associated with it, they
If they are associated with eats, they
tend to occur in the context of needs.
tend to occur in the context of eats.
So more specifically, our prediction
problem is to take any text segment,
which can be a sentence, paragraph or a
document, and then I asked the question
is a particular word present or absent
in this segment.
Right here we can ask the question
about the world W is present or absent
in this segment.
Now, what's interesting is that some
words are actually easier for it, in
other words.
If you take a look at the three words
shown here, meet the and Unicorn.
shown here, meet, the and Unicorn.
Which one do you think it is easier to
predict?
Now, if you think about it for a
moment, you might conclude that.
The is easier to predict 'cause it
The is easier to predict because it
tends to occur everywhere, so I can
just say with the in descendants.
just say with the in the semtence.
Unicorn is also relatively easy.
Becausw Unicorn is rare, is very rare.
Because Unicorn is rare, is very rare.
And I can bet that it doesn't occur in
this sentence.
But meat is somewhere in between in
terms of frequency, and it makes it
hard to predict because it's possible
that it occurs in the sentence or the
segment more accurately.
But it may also not occur in the
segment.
So now let's start this problem more
formally.
Alright, so the problem can be formally
defined as predicting the value of a
binary random variable.
Here we denoted by X sub WW denotes
Here we denoted by X sub w, w denotes
award.
a word.
So this random variable is associated
with precisely one word.
When the value of the variable is 1, it
means this world is present.
means this word is present.
When it's zero, it means the world is
When it's zero, it means the word is
absent, and naturally the probabilities
for one and zero should sum to 1.
Becausw Award is either present or
Because a word is either present or
absent in the segment.
There's no other choice.
So the intuition we discussed earlier
can be formally stated as follows.
The more random this random variable
is, the more difficult in the
is, the more difficult the
prediction would be.
Now the question is, how does one
quantitatively measure the randomness
of a random variable like except how in
of a random variable like X sub w, how in
general, can we quantify the randomness
of a variable?
And that's why we need a measure called
entropy.
And this is a measure introduced in
information theory to measure the
randomness of X.
There is also some connection with the
information here, but that's beyond the
scope of this course.
So for our purpose we just treat the
entropy function as a function defined.
entropy function as a function defined
The only random variable.
on a random variable.
In this case it's a binary random
variable, although the definition can
be easily generalized for a random
variable with multiple values.
Now the function form looks like this.
There's a sum over all the possible
values for this random variable inside
the sum.
the sum,
For each value we have a product of the
for each value we have a product of the
probability that the random variable
equals this value and log of this
probability.
And note that there is also an active
And note that there is also an negative
sign there.
Now, entropy in general is not negative
and that can be mathematically prove.
and that can be mathematically proved.
So if we expand this, some will see the
So if we expand this sum will see the
equation looks like a signal one I
equation looks like a second one I
explicitly plugged in the two values
zero and one.
And sometimes when we have 0 log of 0.
And sometimes when we have 0 log of 0,
We would generally find that as zero
we would generally find that as zero
because log of 0 is undefined.
So this is the entropy function and
this function will give a different
value for different distributions of
this random variable.
And this clear it clearly depends on
the probability that the random
variable taking a value of one or zero.
If we plotted his functioning.
If we plotted his function
Against the probability that the random
against the probability that the random
variable is equal to 1.
variable is equal to 1
And then the function looks like this.
and then the function looks like this.
I had the two ends.
At the two ends,
That means when the probability of X =
1 is very small or very large, then the
entropy function has a lower value when
it's .5 in the middle that it reaches
the maximum.
Now, if we plot the function against
the probability that the X.
the probability that the X
Is taking a value of 0 and it the
is taking a value of 0 and it the
is taking a value of 0 and the
function would show exactly the same
curve here.
And you can imagine why that.
And you can imagine why.
And you can imagine why and so
I thought that's cause.
that's because
The two probabilities are symmetric.
the two probabilities are symmetric.
the two probabilities are symmetric
And complete this magic.
and completely symmetric.
So an interesting question.
You could think about in general.
You could think about in general
Here is for what kind of X?
here is for what kind of X?
Does the entropy reached maximum or
minimum and we can in particular think
about some special cases.
For example, in one case we might have
a random variable that.
a random variable that
Always takes the value of what the
always takes the value of what the
always takes the value of one,
probability is.
the probability is.
the probability is
One is 1.
one.
one
Or there is a random variable that.
or there is a random variable that
Is equally likely taking a value of 1
or 0 zero.
or 0.
In this case, the probability that X =
1 is .5.
Now, which one has a higher entropy?
It's easier to look at the problem by
thinking of simple example.
Using coin tossing.
Using coin tossing,
So when we think about the random
so when we think about the random
experiment like a tossing a coin.
experiment like a tossing a coin
experiment like a tossing a coin,
It gives us a random variable that.
it gives us a random variable that.
it gives us a random variable that
Can represent the result.
can represent the result.
It can be head or tail, so we can
define a random variable X sub coin so
that it's one when the coin shows up as
head, it's zero when the coin shows up
as tell.
as tail.
So now we can compute the entropy of
this random variable, and this entropy
indicates how difficult it is to
predict the outcome of a coin for coin
tossing.
So we can think about the two cases.
One is a fair coin, it's completely
fair.
The coin shows up as head hotel equally
likely, so the two probabilities would
be.
be,
1/2 right so both will have both equal
to 1/2.
Another extreme case is completely
biased coin, where the coin always
shows up as head, so it's a completely
biased coin.
Now let's think about the entropies in
the two cases, and if you plug in these
values you can see the entropies.
values you can see the entropies,
Would be as follows for a fair coin we
would be as follows for a fair coin we
see the entropy reaches its maximum,
that's one.
For the completely biased coin we see
is 0 and that intuitively makes a lot
of sense becausw a fair coin is most
of sense because a fair coin is most
difficult to predict.
difficult to predict
Whereas a completely biased coin is
whereas a completely biased coin is
very easy to predict that we can always
say it's ahead because it is ahead all
say it's a head because it is a head all
the time so they can be shown on the
curve as follows.
So the fair coin corresponds to the
middle point, or it's very uncertain.
The complete advisor coin corresponds
The completely biased coin corresponds
to the end point.
We have a probability of 1.0 and the
entropy is 0.
So now let's see how we can use entropy
for water prediction.
for word prediction.
Now the problem, let's think about our
problem right, still predicted weather
problem right, still predicted whether
W is present or absolutely in this
segment.
Again, think about the three words.
Particularly, think about their
interface.
entropies.
Now we can assume high entropy words
are harder to predict.
And so we will now have quantitative
way to tell us which word is harder to
predict.
Now if you look at the three words,
meet the Unicorn again.
meat, the and Unicorn again.
An we clearly would expect the meat to
have a high entropy, then the OR
Unicorn.
In fact, if you look at the entropy of
the.
the,
It's close to 0 becauses, because
It's close to 0  because
It's close to 0,  because
everywhere.
it occurs everywhere.
So, it's like a completed biased coin.
So, it's like a completed biased coin,
Therefore the entropy is 0.
therefore the entropy is 0.
This lecture is about mixture model
estimation.
In this lecture, we're going to
continue discussing probabilistic topic
models.
In particular, we're going to talk
about how to estimate the parameters of
a mixture model.
So let's first look at our motivation
for using a mixture model and we hope
to factor out the background award from
to factor out the background words from
the topic world distribution.
the topic word distribution.
So the idea is to assume that the text
data actually contain two kinds of
words.
One kind is from the background here.
So the is away etc and the other kind
is from our topic word distribution
that we're interested in.
So in order to solve this problem of
factoring out background words, we can
set up our mixture model as follows.
We're going to assume that we already
know the parameters of all the.
know the parameters of all the
Values for all the parameters in the
values for all the parameters in the
mixture model except for the world
mixture model except for the word
distribution of theaters update, which
distribution of theta sub d, which
distribution of theta sub D, which
is our target.
is our target. So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in, but we are going to simplify other things.
is our target. So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in. But we are going to simplify other things. We are going to assume we have knowledge about others.
And this is a powerful way of
customizing a model for a particular
need.
Now you can imagine we could have
assumed that we also don't know the
background words, but in this case our
background word distribution, but in this case our
goal is reflect out precisely those
goal is factor out precisely those
high probability background words.
So we assume the background model is
already fixed.
And the problem here is how can we
adjust theater sub D in order to
adjust theta sub D in order to
maximize the probability of the
observed document here and we assume
all the other parameters or no.
all the other parameters are known.
Now, although we designed the model
heuristically to try to figure out this
heuristically to try to factor out this
background words.
Unclear whether if we use maximum
It's unclear whether if we use maximum
likelihood estimator we will actually
end up having order distribution where
the common words like the will be
indeed having smaller probabilities
than before.
So now.
In this case, it turns out that the
answer is yes and.
answer is yes, and
When we set up the problem is volume.
when we set up the problem is volume.
when we set up the probalistic model in
This way when we use maximum likelihood
this way when we use maximum likelihood
estimator we will end up having a word
distribution that where the common
words will be factored out via the use
of the background distribution.
So to understand why this is so, it's
useful to examine the behavior of a
mixture model.
So we're going to look at a very, very
simple case in order to understand some
interesting behaviors of a mixture
bottle the observed patterns here
model the observed patterns here
actually are generalizable to mixture
model in general, but it's much easier
to understand this behavior when we use
a very simple case like what we're
seeing here.
So specifically in this case.
Let's assume that the probability of
choosing each of the two models is
exactly the same, so we're going to
flip fair coin to decide which model to
use.
Furthermore, we are going to assume
there are precisely towards the end
there are precisely two words: the and
text.
Obviously this is a very naive
oversimplification of the actual text,
but again it is useful to.
but again it is useful to examine
Then the behavior in such a special
the behavior in such a special
case.
So we further assume that the
background model gives probability of
point line to the world.
point nine to the word
The end text .1.
the and text .1.
the and text  point one.
the and text point one.
Now, let's also assume that our data is
extremely simple.
The document has just the two words
text, Lambda.
text and the.
So now let's write down the likelihood
function in such a case first.
function in such a case. First,
What's the probability of text and
what's the probability of text and
what's the problem in the of the?
what's the probability of the?
I hope by this point and you will be
able to write it down.
So the probability of text is basically
some over 2 cases, where each case
the sum over 2 cases, where each case
corresponds to each of the water
corresponds to each of the word
distribution.
And accounts for the two ways of
And it accounts for the two ways of
generating text.
An inside each case we have the
probability of choosing the model which
is .5 multiplied by the probability of
observing text from that model.
Similarly, the would have a probability
of the same form, just with different
exact probabilities.
So naturally our likelihood function is
just the product of the two, so it's
very easy to see that.
Once you understand what's the
probability of each word, which is also
why it's so important to understand
what exactly the probability of
observing each water from such a
observing each word from such a
mixture model.
Now the interesting question now is,
how can we then optimize this?
how can we then optimize this
Like hold?
likelihood?
Well, you will notice that there were
only two variables.
They are precisely the two
probabilities of the two words text and
the given by theaters of D.
the given by theta sub D.
And this is the cause.
And this is because
We have assumed all the other
we have assumed all the other
parameters are known.
So now the question is a very simple
algebra question, right?
So we have a simple expression with two
variables and we hope to choose the
values of these two variables maximize
values of these two variables to maximize
this function.
And the exercise that we have seen some
simple algebra problems and notes that
simple algebra problems and note that
the two probabilities must sum to one.
So there's some constraint.
If there were no constraint, of course
we would set both probabilities to
their maximum value, which would be one
to maximize this.
But we can't do that 'cause text and
But we can't do that because text and
the must sum to one.
We can't give both a probability of 1.
So now the question is how should we
allocate the probability mass between
the two words?
What do you think now?
What do you think?
It would be useful to look at this
Now it would be useful to look at this
formula for moment and to see what
intuitively we do in order to set these
intuitively what we do in order to set these
probabilities to maximize the value of
this function.
OK, if we look into this further then
we'll see some interesting behavior of
the two component models that there
the two component models in that there
the two component models in that they
will be collaborating to maximize the
probability of the observed data which
is dictated by the maximum regular
is dictated by the maximum likelihood
estimator.
But there are also competing somewhere
But there are also competing in someway
an in particular there will be
an in particular they will be
competing on the words.
And they will tend to bet high
probabilities on different words to
avoid this competition in some sense.
Or to gain advantage in this
competition.
So again, booking at this objective
So again, looking at this objective
function and we have a constraint.
On the two probabilities.
Now.
If you look at the formula intuitively,
you might feel that you want to set the
probability of text to be somewhat
larger than the.
And this intuition can be well
supported by mathematical effect, which
supported by a mathematical fact, which
is when the sum of two variables is a
constant.
Then the product of them, which is
maximum when they are equal and this is
a factor that we know from algebra.
a fact that we know from algebra.
Now if we plug that in, will mean would
Now if we plug that in, it will mean would
Now if we plug that in, it will
mean that we have to make the two
probabilities equal.
And when we make them equal an, if we
consider the constraint that we can
easy to solve this problem and the
solution is the probability of text
would be point online and probability
would be point nine and probability
of the is .1.
of the is point one.
And as you can see, indeed the
probability of text is now much larger
than probability of the.
This is not the case when we have just
one distribution and this is clearly
because of the use of the background
model which assigns a very high
probability to the low probability to
probability to the and low probability to
text.
And if you look at the equation, you
will see obviously some interaction of
the two distributions here.
In particular, you will see in order to
make them equal and then the
probability assigned by senior sub D
probability assigned by theta sub D
must be higher for a word that has a
smaller probability given by the
background.
Hi Ann, this is obvious from examining
And, this is obvious from examining
this equation because the background
part is weak for text it's small.
So in order to compensate for that we
must make the probability of text given
by Cedars up D somewhat larger so that
by theta sub D somewhat larger so that
the two sides can be balanced.
So this is in fact a very general
behavior of this mixture model, and
that is if one distribution assigns a
high probability to one word, then
high probability to one word than
another, then the other distribution.
Would tend to do the opposite.
Basically it would discourage.
Basically it would discourage
Other distributions will do the same,
other distributions to do the same,
and this is to balance them out so that
we can account for all kinds of words.
And this also means that by using a
background model that is fixed to
assign high probabilities to background
words, we can indeed encourage the
unknown topic world distribution to
assign smaller probabilities for such
common words, instead put more
probability mass on the content words
that cannot be explained well by the
background model.
Meaning that they have a very small
probability from the background model,
like a text here.
So as we explained that different
So, as we explained different
So, as we explained, different
textual representation tends to enable
different analysis.
In particular, we can gradually add
more and more deeper analysis results
to represent text data, and that would
open up a more interesting
open up more interesting
representation.
representation
Opportunities and also analysis
opportunities and also analysis
capacities.
So this table summarizes what we have
just seen.
So the first column shows the text
Recognition II visualizes the
recognition, second visualizes the
recognition, the second visualizes the
generality of such representation,
meaning whether we can do this kind of
representation accurate before all the
text data, or only some of them, and so
text data, or only some of them, and third
the column shows the enabled analysis
column shows the enabled analysis
techniques.
And the final column shows some
examples of application that can be
achieved through this level
achieved through this level of
representation.
So let's take a look at them so as a
So let's take a look at them. So as a
string text can only be processed by
using stream processing algorithms, but
it's very robust, it's general.
And there was this some interesting
And there are still some interesting
applications that can be done at this
level for example, compressing of text
level. For example, compressing of text
level. For example, compression of text
doesn't necessarily need to know the
word boundaries.
Although knowing water boundaries might
Although knowing word boundaries might
actually also help.
Word based recommendation is very
Word based representation is very
important level of reputation.
important level of representation.
It's quite general and relatively
robust.
It can enable a lot of analysis
techniques such as water relation
techniques such as word relation
analysis, topic analysis and sentiment
analysis, and there are many
applications that can be enabled by
this kind of analysis.
For example, Thesaurus discovery has to
do with discovering related words and
topic and opinion related applications
overabundant, and there are for
are abundant, and there are for
example, and people might be interested
in knowing the major topics covered in
the collection of text.
And this can be the case.
In research literature, a scientist
want to know what are the most
important research topics today or
customer service people might want to
know?
know
What are the major complaints from
know what are the major complaints from
what are the major complaints from
their customers about by mining their
email messages?
email messages.
And business intelligence people might
be interested in understanding
consumers opinions about their products
and competitors products to figure out
the what are the winning features of
their products.
And in general there are many
applications that can be enabled by the
representation at this level.
Now moving down, we'll see.
Now moving down, we'll see
We can gradually add additional
we can gradually add additional
recommendations by adding syntactic
representation by adding syntactic
representations. By adding syntactic
structures we can enable.
structures we can enable,
Of course, syntactic graph analysis.
We can use graph mining algorithms to
analyze.
analyze
Syntactic graphs.
And some applications are related to
this kind of representation.
For example, scientistic ananse
For example, stylistic analysis
generally requires syntactical
representation.
Syntactical structure appendage.
"Syntactical structure
Syntactical structure representation.
We can also generate the structure
based feature features and those are
features that might help us classify
text objects into different categories.
By looking at the structures, sometimes
the classification can be more
accurate.
For example, if you want to classify
articles into.
articles into
Different category categories
different category categories
different categories
corresponding to different authors want
to figure out which which of the.
to figure out which of the
K authors has actually written this
article.
Then you generally need to look at the
syntactic structures.
When we add entities and relations,
then we can enable out of techniques
then we can enable lot of techniques
such as knowledge graph analysis or
information network analysis in general
and this analysis would enable
applications.
applications
About entities, for example, discovery
about entities, for example, discovery
of all the knowledge and opinions about
the real world energy entity.
You can also use this level
representation to integrate the
representation to integrate
everything about entity from scattered
sources.
Finally, when we add logic predicates
then we would enable logic inference of
then we would enable logic inference
course, and this can be very useful for
ofcourse, and this can be very useful for
integrative analysis of scattered
knowledge.
For example, we can also add ontology
on top of the extracted information
from text to make inferences.
A good example of application in this
enabled by this level of representation
is a intelligent knowledge assistant
for biologists.
And this is intended program that can
help biologists manage all the relevant
knowledge from literature about the
research problem, such as understanding
functions of genes.
And the computer can make inferences
about.
about
Some of the hypothesis that biologist
some of the hypothesis that biologist
some of the hypothesis that biologists
might be interesting, for example,
whether a gene has a certain function
and then the intelligent program can
read the literature to extract the
relevant effects.
relevant facts.
Doing by doing information extraction
and then using a logical system to
actually track.
actually track
That's the answers to researchers
that's the answers to researchers
questioning about what genes are
related to what functions.
So in order to support this level of
application, we need to go as far as
logical representation.
Now this course is covering techniques
mainly based on water based
mainly based on word based
representation.
These techniques are general and robust
and thus are more widely used in
various applications.
In fact, in virtually all the text
mining applications you need, this
mining applications you need this
level of representation and the
techniques that support analysis of
texting this level.
But obviously all these other levels
can be combined and should be combined
in order to support the sophisticated
in order to support sophisticated
applications.
So to summarize, here are the major
takeaway points.
Tax representation determines what kind
Text representation determines what kind
of mining algorithms can be applied.
And there are multiple ways to
represent text, strings, words,
represent text - strings, words,
syntactic structures and the relation
graphs, logical predicates, etc.
And these different repeated
"And these different
representations should in general be
combined in real applications to the
extent we can.
For example, if even if we cannot do
accurate.
accurate,
accurately,
This application of syntactic
this application of syntactic
structures we can stick at partial
structures extracted an if we can
structures extracted and if we can
recognize some entities and that would
be great.
So in general we want to do as much as
we can.
And when different levels are combined
together, we can enable richer
analysis.
More powerful analysis.
This course, however, focuses on water
This course, however, focuses on word
based representation.
Such techniques have also several
advantages.
First, their general robust, so they
First, they are general and robust, so they
are applicable to any natural language.
That's a big advantage over other
approaches that rely on more fragile
natural language processing techniques.
Secondly, it does not require much
manual effort or sometimes it does not
require any manual effort.
So that's again important benefit,
because that means you can apply
directly to any application.
3rd, these techniques are actually
Third, these techniques are actually
surprisingly powerful.
surprisingly powerful
An effective for many applications.
and effective for many applications.
Although not all, of course, as I just
explained.
Now they are very effective, partly
because the words are invented by
humans as basic units for
communications.
So they are actually quite sufficient
for representing all kinds of
semantics.
So that makes this kind of word based
representation also powerful?
representation also powerful.
I finally such a water based
And finally such a word based
representation and the techniques
enabled by such a recommendation can be
enabled by such a representation can be
combined with many other sophisticated
approaches.
So they're not competing with each
other.
This lecture is about the ordinal
logistic regression for sentiment
analysis.
So this is our problem set up for a
typical sentiment classification
problem, or more specifically, rating
prediction.
We have a opinionated text document D
We have an opinionated text document D
as input an we want to generate as
output already in the range of one
through K, so it's discrete rating and
thus this is a categorization problem.
We have K categories here.
Now we can use a regular text for
capitalization technique to solve this
categorization technique to solve this
problem, but such a solution would not
consider the order and dependency of
the categories.
Intuitively, the features that can
distinguish Category 2 from.
distinguish Category 2 from
One or rather rating 2 from 1 May be
One or rather rating 2 from 1 may be
1 or rather rating 2 from 1 may be
1, or rather rating 2 from 1 may be
1, or rather rating 2 from 1, may be
similar to those that can distinguish K
from K -- 1.
from K - 1.
For example, positive words generally
suggest a higher rating.
Now when we train categorisation
program by treating these categories as
independent, we would not capture this.
So what's the solution?
In general, we can add order to
classify and there are many different
approaches, and here we are going to
talk about one of them is called the
ordinal logistic regression.
Now let's first think about how we use
logistic regression for binary setting
categorization problem.
So suppose we just want to distinguish
it positive from negative and then it's
just a two category categorization
problem.
So the predictors are represented as X
and these are the features and there
are.
are
M features altogether, which feature
value is a real number, and this can be
representation of a text document.
And why has two values, binary response
And y has two values, binary response
variable 011 means X is positive, 0
variable {0,1}. 1 means X is positive, 0
means X is negative.
And then of course, this is a standard
two category categorization problem.
We can apply logistical regression.
You may recall that in logistic
regression we assume the log out of
regression we assume the log of
probability that Y is equal to 1 is
assumed to be a.
assumed to be a
Linear function of these features as
linear function of these features as
shown here.
So this would allow us to also write
the probability of y = 1 given X in
this equation that you are seeing on
the bottom, and so that's logistical
function and you can see it relates
this probability too.
this probability to
Probably that y = 1 to the feature
probaility that y = 1 to the feature
probability that y = 1 to the feature
values.
And of course, better eyes are
And of course, beta ys are
And of course, B_i is our
parameters here.
So this is just a direct application of
logistical regression for binary
categorization.
What if we have multiple categories,
multiple levels?
We actually use such a binary logistic
regression program to solve this multi
level rating prediction.
And the idea is we can introduce
multiple binary classifiers and each
case we ask the classifier to predict
whether the rating is J or above all
the ratings lower than J.
So when Y subject is equal to 1, it
So when Y_j is equal to 1, it
means rating is J or above.
When it's zero, that means the rating
is lower than G.
is lower than J.
So basically, if we want to predict
rating in the range of one through K,
we first have one classifier to
distinguish K versus others.
And that's our classifier one, and then
we're going to have another classifier
to distinguish K -- 1 from the rest.
to distinguish K - 1 from the rest.
That's classified too, and in the end
That's classifier two, and in the end
we need a classifier to distinguish
cool and well.
two and one
So altogether we'll have K -- 1
So altogether we'll have K - 1
classifiers.
Now if we do that of course, then we
can also solve this problem, and the
logistical regression program would be
also very straightforward as you have
just seen on the previous slide.
Only that here we have more parameters
because for each classify we need a
different set of parameters.
So now the logistic regression
classifiers indexed by J, which
corresponds to a reading level.
An I have also used offer subject to
And I have also used offer subject to
replace beta 0 Ann.
replace beta 0 And
replace beta 0. And
This is to make the notation more
this is to make the notation more
consistent with what we can show in the
audio ologist square crashing.
ordinal logistic regression.
So anyway, so here we now have
basically K -- 1 regular logistic
basically K - 1 regular logistic
regression classifiers.
Each has its own set of parameters.
So now with this approach we can now do
rating prediction as follows.
After we have trained these K -- 1
After we have trained these K - 1
logistic regression classifiers
logistic regression classifiers,
separately of course.
separately of course,
Then we can take a new instance and
then we can take a new instance and
then invoke.
then invoke
A classifier sequentially to make the
a classifier sequentially to make the
decision.
So first let's look at the classifier
that corresponds to level of rating K.
So this classifier will tell us whether
this object should have a rating of K
or above.
And if its probability according to
this logistical regression classifier
is larger than .5, we're going to say
yes, the ratings K.
yes, the rating is K.
Now, what if it's not as large as .5?
Well, that means the reading is Bill
Well, that means the reading is Below
Well, that means the reading is below
OK, right?
K, right?
So now we need to invoke the next class
file, which tells us whether it's above
K -- 1.
K - 1.
It's at least K -- 1 and if the
It's at least K - 1 and if the
probability is larger than .5 then will
say well.
say well,
say, well,
Then it's K -- 1.
then it's K -- 1.
then it's K - 1.
What if it says no?
Well, that means the rating will be
even bill OK minus one, and so we're
even below K minus one, and so we're
going to just keep invoking these
classifiers until we hit the end.
When we need to decide whether it's two
or one, so this will help us solve the
problem, right?
So we can have a classifier that would
actually give us a prediction of rating
in the range of one through.
in the range of one through
OK, unfortunately, such a strategy is
K, unfortunately, such a strategy is
not the optimal way of solving this
problem, and specifically there are two
problems with this approach.
So these equations are the same as you
have seen before.
Now the first problem is that there are
just too many parameters.
There are many parameters.
Now can you count how many premises we
Now can you count how many parameters we
have exactly here?
Now this may be interesting exercise.
To do so you might want to just pause
the video and try to figure out the
solution how many premises we have for
each classifier.
each classifier?
And how many classifiers do we have?
You can see the answer is that for each
classifier we have N + 1 parameters.
And we have K -- 1 classifieds
And we have K - 1 classifieds
And we have K - 1 classifiers
altogether, so the total number of
premises K -- 1 * M + 1.
premises K - 1 * M + 1.
premises (K - 1) * (M + 1).
That's alot alot of parameters.
So when the classifier has a lot of
parameters would in general need a lot
of data to actually help us training
data to help us decide the optimal
parameters of the this such a complex
model?
So that's not the idea.
The second problem is that these
problems these K -- 1 classifiers are
problems these K - 1 classifiers are
not really independent.
These problems are actually dependent.
In general, words that are positive
would make the rating higher and for
any of these classifiers, for all these
classifiers.
So we should be able to take advantage
of this factor.
Now the idea of ordinal logistic
regression is precisely that them.
regression is precisely that
A key idea is just the improvement over
the K -- 1 independent logistical
the K -1 independent logistical
regression classifiers, and that idea
is to tie these beta parameters and
that means we are going to assume the.
that means we are going to assume the
Beta parameters these are the
parameters that indicate the influence
of those weights.
And we're going to assume these better
values are the same for all the K -- 1
values are the same for all the K - 1
premise, and this just encodes our
intuition that positive words in
general would make a higher rating more
likely.
So this is intuitively appealing
assumption.
It's reasonable for our problem set up
when we have this order in these
categories.
Now, in effect, this would allow us to
Now, in fact, this would allow us to
have two positive benefit of one is
it's going to reduce the number of
parameters significantly.
And the other is to allow us to share
the training data, because all these
parameters are assumed to be equal.
So these training data for different
classifiers.
Can then be shared to help us set the
optimal value for beta.
So we have more data to help us choose
a good, better value.
a good beta value.
So what's the consequence?
The formula would look very similar to
what you have seen before, only that
now the beta parameter has just one
index that correspond to the feature
and no longer has the other index that
corresponds to the level of reading.
corresponds to the level of rating.
So that means we tide them together and
So that means we tie them together and
there's only one set of better values
there's only one set of beta values
for all the classifiers.
However, each classifiers there has a
distinct Alpha value, the Alpha
parameter, the except it's different
and this is of course needed to predict
the different levels of ratings.
So offer subject is different.
So apha subject is different.
It depends on J.
Different J has a different offer, but
Different J has a different alpha, but
the rest of the prime is the big guys
the rest of the parameters of beta
are the same.
So now you can also ask the question,
how many parameters do we have now?
Again, that's an interesting question
to think about.
So if you think about it for a moment
and you will see now the plan that we
have far fewer parameters.
Specifically, we have N + K -- 1
Specifically, we have N + K - 1
because we have M beta values and plus
K minus one of our values.
K minus one alpha values.
So that's just the basically that's
basically the main idea of ordinal
logistic regression.
So now let's see how we can use such a
method to assign ratings.
method to actually assign ratings.
It turns out that with this.
Idea of tying all the parameters.
Idea of tying all the parameters,
The beta values.
the beta values.
the beta values,
We also end up having a simpler way to
we also end up having a simpler way to
make decisions, and more specifically
now the criteria whether the predicted
probabilities above is or at least .5
above and now is equivalent to weather
above and now is equivalent to whether
the score of the object that is.
Larger than or equal to negative offer
Larger than or equal to negative of the
Larger than or equal to negative of
subject as shown here.
alpha_j as shown here.
Now the scoring function now is just
taking linear combination of all the
features weighted by better values.
features weighted by beta values.
So this means now we can simply make a
desicion of rating by looking at the
value of this scoring function and see
which bracket is falls into.
which bracket it falls into.
Now you can see the General Decision
rule is.
rule is
Thus when the score is in the
thus when the score is in the
particular range of our values.
particular range of our values,
Then we will assign the corresponding
then we will assign the corresponding
rating to that text object.
So in sum, in this approach we're going
to score the object.
By using the features and the parameter
values, better values.
values, beta values.
And this score will then be compared
with a set of training the other values
to see which range the score is in, and
then using the range we can then decide
which rating the object should be
getting becausw these ranges of our
getting because these ranges of our
values correspond to the different
levels of ratings, and that's from the
way we train these other values.
Each is tide to some level of reading.
Each is tie dto some level of reading.
Each is tied to some level of reading.
This vector is about a specific
[Intro] This vector is about a specific
This vector is about a specific
This lecture is about a specific
technique for contextual text mining
called contextual probabilistic latent
semantic analysis.
In this lecture, we're going to
continue discussing contextual text
mining.
And we're going to introduce contextual
probabilistic.
probabilistic
Then the semantical analysis.
latent semantic analysis
As exchanging of PSA for doing
As an extension of PLSA for doing
contextual text mining.
Recall that in contextual text mining
we hope to analyze topics in text.
In consideration of context so that we
can associate the topics with
appropriate context that we're
interested in.
So in this approach Contacts, you're
So in this approach contextual
probably circulating this semantic
probabilistic latent semantic
analysis or Cpl essay.
analysis or CPLSA
The main idea is to expressed the add
The main idea is to explicitly add
interesting context variables into a
generated model.
Recall that before when we generate the
text, we generally assume we will start
with some topics and then sample words
from some topics.
But here we are going to add context
variables so that the coverage of
topics and also the content of topics
will be tight little context.
Or in other words, we can do let the
context influence both coverage and
content of a topic.
The consequences that this would enable
us to discover contextualized topics
make the topics more interesting, more
meaningful, because we can then have
topics that can be interpreted as
specific to a particular context that
we're interested in.
For example, a particular time period.
As extension of PSA model, CPI say
As extension of PLSA model, CPLSA
mainly does the following changes.
Firstly it would model the conditional
likelihood of text given context.
That clearly suggests that the
generation of text would then depend on
context, and that allows us to bring
context into the generative model.
Secondly, it makes 2 specific
assumptions about the dependency of
topics on context.
One is to assume that depending on the
context depending on different time
periods or different locations, we
assume that there are different views
of the topic or different versions of
word distributions that characterize a
topic, and this assumption allows us to
discover different variations of the
same topic in different context.
The other is that we assume.
The topic coverage also depends on the
context.
And that means depending on the time or
location, we might cover topics
differently.
And then again this dependency would
then allow us to capture the
Association of topics with specific
association of topics with specific
Contacts.
context.
We can still use the EM algorithm to
solve the problem of parameter
estimation.
And in this case, the estimate premise
would naturally contain context
variables, and in particular a lot of
conditional probabilities of topics
given certain context.
And this would allow us to do
contextual text mining.
So this is the basic idea.
Now we don't have time to introduce
this model in detail, but there are
references here that you can look into
the no more detail here.
to know more detail here.
I just want to explain the high level
ideas in more detail, particularly
willing to explain the generation
process of text data that has context
associated in such a model.
So as you see here, we can assume there
are still multiple topics.
For example, some topics might
represent the themes like a government
response donation or the city of New
Orleans.
Now this example is in the context of
Hurricane Katrina and hit New Orleans.
Hurricane Katrina and that hit New Orleans.
Now, as you can see, we assume there
are different views associated with the
each of the topics.
And these are she wants, view one view
And these are shown as view one, view tow and view three
And these are shown as view one, view two and
2V3.
view three
Each view is a different version of
word distributions.
And these views are tide to some
context variables.
For example, type to the location taxes
For example, type to the location Texas
or the time July 2005 or the occupation
of the other being sociologist.
Now on the right side you we assume the
Now on the right side you see we assume the
Now on the right side you see now we assume the
document has contact information, so
the time is known to be July 2005,
locations, taxes, etc.
location is Texes, etc.
location is Texas, etc.
Now such context information is what we
hope to model as well.
So we're not going to just model the
text.
And so one idea here is to model the
variations of top topic content in
variations of topic content in
different context and this gives us
different views of the world
distributions.
Now on the bottom you will see the
theme coverage or topic coverage might
also vary according to these context.
Because in the.
In the case of allocation like taxes,
In the case of location like Texas,
people might want to cover the red
topics more at the new audience, as
visualized here.
But in a certain time period, maybe
particular topic donation will be
particular topic like donation will be
covered more so this variation is also
considered in CSA.
considered in CPLSA.
So to generate the such a document with
So to generate such a document with
context, we first also choose a view.
And this view of course now could be
from any of these contexts.
Let's say we have taken this view.
That depends on the time in the middle.
So now we have a specific version of
word distributions.
Now you can see some probabilities of
words for each topic.
Now, once we have chosen of you, now
Now, once we have chosen a view, now
the situation will be very similar to
what happened in standard PSA.
what happened in standard PLSA.
We assume we have got awarded
We assume we have got a word
distribution associated with each
topic, right?
And then next to Wells who choose a
And then next to we choose a
And then next to the view we choose a
coverage from the bottom.
So we're going to choose particular
coverage and that coverage.
Before is fixed in PSA and it's hard to
Before is fixed in PLSA and it's hard to
a particular document.
Each document has just one coverage
distribution.
Now here, because we consider context
so the distribution of topics or the
coverage of topics can vary depending
on the context that has influenced the
coverage.
So, for example, we might pick a
particular coverage, let's say in this
case.
We pick up.
We pick
We've picked the document specifically
coverage now with the coverage and
these were distributions, we can
these word distributions, we can
generate the document in exactly the
same way as in PSA.
same way as in PLSA.
So what it means we're going to use the
coverage to choose a topic to choose
one of these three topics.
Let's say we have picked up, let's say,
the yellow topic, then withdraw world
the yellow topic, then withdraw a word
from this particular topic on the top.
So we might get the warden.
So we might get the word
I get government.
like government.
And then next time we might choose a
different topic, an will get donate,
etc right until we generate all the
words and this is basically the same
process as in PSA.
process as in PLSA.
Now, so the main difference is when we
obtain the coverage and the word
distributions, we let the context
influence our choice.
So in other words, we have extra
switches that are tide to this context
switches that are tied to this context
that would control the choices of
different views of topics and choices
of coverage.
And naturally, the model will have more
parameters to estimate, but once we can
estimate those parameters that involve
the context, then we will be able to
understand the context.
understand the context of
Specific views of topics or context
specific views of topics or context
specific views of topics or context of
specific coverages of topics.
And this is precisely what we want in
contextual text mining.
So here are some sample results from
using such a model.
Not necessary exactly the same model,
but similar models.
So on this slide you see some sample
results of comparing news articles
about Iraq war and Afghanistan war.
Now we have about 30 articles.
On Iraq war and 26 articles on
Afghanistan war.
Now, in this case, the goal is to.
To review the common topics covered in
both sets, articles and the differences
or variations of the topic in of the
or variations of the topic in each of the
two collections.
So in this case, the context that is
explicitly specified by the topical
collection.
And we see the results here show that.
There is a common theme that's
corresponding to cluster around here in
this column.
That there is a common scene indicating
That there is a common theme indicating
that United Nations is involved in both
wars is a common topic covered in both
sets of articles, and that's indicated
by the high probability words show here
by the high probability words showmn here
by the high probability words shown here
United Nations.
Now if you the background, of course
this is not surprising and this is.
This topic is indeed very relevant,
both was.
to both wars.
If you look at the column further and
what's interesting is that the next two
cells of word distributions actually
tell US collection specific variations
of the topic of United Nations.
So it indicates that in Iraq war,
United Nations was more involved in
weapon inspections, whereas in
Afghanistan war it was more involved in
maybe 8 to Northern Alliance as a
maybe aid to Northern Alliance as a
different variation of the topic of
United Nations.
So this shows that by bringing the
context, in this case, different walls
context, in this case, different wards
context, in this case, different wars
are different collections of text.
We can have topic variations, tide to
We can have topic variations, tied to
these contexts to review the
differences of coverage of United
Nations in the two walls.
Nations in the two wars.
Similarly, if you look at the second
cluster.
Class 2 it has to do with the killing
Cluster 2 has to do with the killing
of people and against.
of people and again.
Not surprising if you the background
Not surprising if you know the background
about walls or the wars involved
about wars or the wars involved
killing of people.
But imagine if you are not familiar
with the tax collections or have a lot
with the text collections or have a lot
of text articles and such a technique
can review the common topics covered in
both sets of particles.
both sets of articles.
It can be used to review common topics
in multiple sets of articles as well.
If you look down, of course in that
column of class tool you see variations
column of cluster 2 you see variations
of killing of people and that
correspond to intervene in different
correspond to in different
correspond to in different different
contexts.
And here is another example of results.
Obtain the front block articles about
the Hurricane Katrina.
Now in this case, what you see here is
visualization of the.
Trains of topics overtime.
trends of topics overtime.
And the top one shows just the temporal
chains of two topics.
One is oil price and once.
One is oil price and one is.
About the flooding of the city.
about the flooding of the city.
New Orleans.
This these topics are obtained from
block articles about the Hurricane
Katrina.
And people talked about these topics.
And in addition to some other topics.
But the visualization shows that with
this technique that we can have
conditional distribution of time given
a topic.
So this allows us to plot this
conditional probability.
General curves like what you're seeing
here.
We see that initially the two curves
tracked each other very well.
tracked each other very well. But later
tracked each other very well. But later we see the topic of New Orleans
tracked each other very well. But later we see the topic of New Orleans was mentioned again but
tracked each other very well. But later we see the topic of New Orleans was mentioned again but oil price was not
Ann
anf
and
This turns out to be the time period
when another Hurricane Hurricane Rita
hit the region that apparently tricked
more discussion about the flooding of
the city.
The bottom curve shows the coverage of
this topic about the flooding of the
city by block articles in different
locations and also shows some shift of
coverage.
That might be related to peoples
migrating from the state of Louisiana
to Texas, for example.
So in this case we can see the time can
be used as context to reveal trends of
topics.
This is some additional result on
special patterns and this.
In this case it's about the topic of
government response.
And there was some criticism about the
slow response of government in the case
of Hurricane Katrina, and discussion
now is covered in different locations
and these visualizations show the
coverage in different weeks of the
event, and initially it's covered
mostly in the victim states in the
South, but then gradually it's
spreading to other.
Locations, but in week four, which is
shown on the bottom on the left, we see
a pattern that's very similar to the
very first week on the top left, and
that's why again he hurricane without
that's why again the hurricane without
that's why again the hurricane Rita
hidden region.
hit the region.
So such a technique would allow us to
use location as context to examine
variations of topics.
And of course, the model is completely
general, so you can apply this to any
other collections of text to reveal
spatial temporal patterns.
Is yet another application of this kind
of model where we look at the use of
the model for event impact analysis.
So here we are looking at the research
articles in information retrieval, IR,
particularly Sky our papers.
particularly SIGIR papers.
And the topic will focus on is about
And the topic we focus on is about
the retrieval models and you can see
the top world top words with high
the top word top words with high
probability is about this model on the
left.
And then we hope to examine the impact
of two events.
One is the start of track for text
One is the start of TREC for text
retrieval conference.
This is a major evaluation effort
sponsored by US government and was
launched in 1992 or around that time
and that is known to have made an
impact on the topics of research
information retrieval.
The other is the publication of a
Seminole paper by craft and pond, and
Seminal paper by craft and pond, and
Seminal paper by Croft and Ponte, and
this is about the language modeling
approach to information retrieval.
It's also known to have made a high
impact on information retrieval
research, so we hope to use this kind
of model, understand impact, and the
idea here is simply to use the time as
context an use these events to divide
the time periods into a period before
the event an another after this event,
and then we can compare the differences
of.
The topics, the coverage and
variations, etc.
So in this case the results show I've
seen that before track the study of
seen that before TREC the study of
retrieval models was mostly a vector
space model, Boolean model, etc.
But the after track.
But the after TREC.
Apparently the study of retrieval
models have involved a lot of other
words that seem to suggest some
different retrieval tasks though.
So for example email was used in the
enterprise search tasks and subtropical
retrieval, with another task introduced
later bye.
later by
Track.
TREC.
On the bottom we see the variations
that are correlated with the
publication of the language model
paper.
Before we have those classical
probabilistic model logic, model,
probabilistic model logic model,
Boolean model etc.
But after 99 eight that we see clear
But after 1998 that we see clear
dominance of language model as
probabilistic models and we see words
like a language model, estimation of
parameters etc.
So this technical here can use event as
So this technique here can use event as
context.
To understand the impact of event
again, the technique is general so you
can use this to analyze the impact of
any event.
Here are some suggested readings.
The first is.
The first is
Paper about simple extension of PSA to
paper about simple extension of PSA to
paper about simple extension of PLSA to
enable cross collection comparison.
It's to perform comparative text mining
to allow us to extract the common
topics shared by multiple collections
and their variations in each
collection.
The second one is the main paper about
the CPS and model with a discussion of
the CPLSA model with a discussion of
a lot of applications.
The third one has a lot of details
about this special temporal patterns
for Hurricane Katrina, example.
In this lecture, welcome to talk about
This lecture is  about topic mining and analysis.
different kind of mining task.
"We
"We
As you see on this road map, we have
"In this
"As you see on this roadmap, we have just
"As you see on this roadmap, we have just
just covered mining knowledge about the
v
"we have just
language, namely discovery of word
"about the language namely discovery of word
"about the language namely discovery of word
"about the language namely discovery of word
associations such as paradigmatic
associations such as paradigmatic relations
relations and syntagmatic relations.
Now, starting from this laptop, so
Now, starting from this lecture,
we're going to talk about mining
another kind of knowledge, which is
content mining and trying to discover
knowledge about.
The main topics.
In the text.
And we call that topic mining an
And we call that topic mining and
analysis.
In this lecture we're going to talk
about its motivation and the task
definition.
So first, let's look at the concept of
topic.
So topic is something that we all
understand, I think, but it's actually
not that easy to formally define it.
Roughly speaking, topic is the main
idea discussed in text data, and you
can think of this as a theme or subject
of discussion or conversation.
It can also have different
granularities.
For example, we can talk about the
topic of a sentence.
A topic of article topic of a
paragraph, or the topics of all the
research articles in the digital
library.
So different granularities of topics
obviously have different applications.
Indeed, there are many applications
that require discovery of topics in
text and then analyze them.
Here are some examples.
For example, we might be interested in
knowing what are Twitter users talking
about today?
Are they talking about NBS sports or
Are they talking about NBA sports or
talking about some international
events, etc.
Or we are interested in knowing about
the research topics.
For example, one might be interested in
knowing what are the current research
topics in data mining and how are they
different from.
different from
Those five years ago now, this involves
Those five years ago. Now this involves
those five years ago. Now this involves
discovery of topics in data mining,
literatures and also we want to
discover topics in today's literature
and those in the past.
And then we can make a comparison.
We might be also interested in knowing
what do people like about some product
iPhone 6 an what do they dislike?
like iPhone 6 an what do they dislike?
like iPhone 6 and what do they dislike?
And this involves discovering topics in
positive opinions about iPhone 6 and
also negative reviews about it.
Or perhaps we're interested in knowing
what are the major topics debated in
2012 presidential election?
And all these have to do is discovering
topics in texts and analyzing them, and
we're going to talk about a lot of
techniques for doing this.
In general, we can view topic as some
knowledge about the world.
So from text that we expected to
discover a number of topics and then
this topic is generally provided
this topic generally provide
description about the world and it
the description about the world and it
tells us something about the world,
about the product, about the person,
etc.
Now when we have some non text data
Now when we have some non-text data
then we can have more context for
analyzing the topics.
For example, we might know that I'm
For example, we might know the time
associated with the text data or
locations where the text it up will
locations where the text data will
locations where the text data will be
produced or the authors of text or the
sources of the text etc.
All such meta data or context variables
can be associated with the topics that
we discover.
And then we can use these context
variables to help us analyze patterns
of topics.
For example, looking at topics
overtime, we would be able to discover
whether there's a trending topic or
some topics might be fading away.
Similar looking at the topics in
different locations, we might know some
insights about peoples opinions in
insights about peoples' opinions in
insights about people's opinions in
different locations.
So that's why mining topics is very
important.
Now let's look at the tasks of top
Now let's look at the tasks of topic
mining analysis.
mining and analysis.
In general, it would involve first
discovering a lot of topics.
In this case K topics.
And then we also would like to which
And then we also would like to know which
topics are covered in which documents,
to what extent.
So for example in.
Document the one we might see that
Document one we might see that
topic one is covered, a lot, topic two
topic one is covered a lot, topic two
topic 1 is covered a lot, topic two
topic 1 is covered a lot, topic 2
and topic care are covered with a small
and topic three are covered with a small
and topic 3 are covered with a small
and topic k are covered with a small
portion.
An other topics perhaps are not
And other topics perhaps are not
covered.
Document 2 on the other hand, the
Document 2, on the other hand,
cover, the topic of two.
covered topic 2.
covered topic 2
Very well, but it did not cover topic
very well, but it did not cover topic
one at all and also covers topic key to
1 at all and also covers topic key to
1 at all and also covers topic K to
some extent.
It cetera, right?
etc., right?
So now you can see there are generally
two different tasks or subtasks.
The first is to discover K topics from
a collection of text data.
What are these K topics?
OK, major topics in the text data.
The second task is to figure out which
documents cover which topics to what
extent.
So more formally, we can define the
problem as follows.
First, we have as input a collection of
N text documents.
Here we can denote that extra
Here we can denote that text
connection.
Has C.
as C.
And denote a text article as the Eye an
And denote a text article as d i an
And denote a text article as d i and
And denote a text article as di and
we generally also need to have as input
we generally also need to have an input
we generally also need to have as input
the number of topics K.
But there may be techniques that can
automatically suggest a number of
topics, but in the techniques that we
will discuss which are also the most
useful techniques, we often need to
specify a number of topics.
Now the output would then be the key
Now the output would then be the K
topics that we would like to discover
denoted as theater sub one through
theater subkey.
theta sub k.
Also we want to generate the coverage
of topics in each document design by
of topics in each document di
of topics in each document d sub i
and this is denoted by π sub by J and
and this is denoted by π sub i j and
piles of idea is the probability of
"π sub i j
documented the sub by covering topic
document d sub i covering topic
theater subject.
theta sub j.
So obviously for each document we have
a set of such values indicate.
To what extent did the document covers
each topic.
An we can assume that these
probabilities sum to one, because a
document won't be able to cover other
topics outside the topics that we
discussed with discover.
discussed we discovered.
So now the question is how do we define
theater sub I?
theta sub i?
\theta sub i?
theta sub i?
How do we define the topic now?
This problem has not been completely
defined until we define what is exactly
theater.
theta.
So in the next few lectures, we're
So in the next a few lectures, we're
going to talk about different ways to
define theater.
define theta.
This lecture is about the evaluation of
taxable categorization.
So we've talked about many different
methods for taxi categorisation, but
how do you which method works better?
And for a particular application, how
do you this is the best way of solving
your problem.
To understand these will have to.
How to we have to know how to evaluate
categorisation results?
So first some general thoughts about
the evaluation in general for
evaluation of this kind of empirical
tasks such as categorisation, we use
methodology that was developed in 1960s
by information retrieval researchers
called Cranfield Evaluation
Methodology.
The basic idea is to help humans to
create test collection.
Where we already every document is
tagged with the desired categories, or
in the case of search for which query,
which documents should have been
retrieved and this is called ground
truth.
Now with this ground truth test
collection, we can then reduce the
collection to test many different
systems and compare different systems.
We can also turn off some component in
system to see what's going to happen.
Basically it provides.
A way to do controlled experiments to
compare different methods.
So this methodology has been virtually
used for all the tasks that involve
empirically defined problems.
So in our case, then we're going to
compare our systems categorization
results with the categorisation ground
truth created by humans.
And we're going to compare our systems
decisions on which documents should get
which category with what.
Categories have been assigned to those
documents by humans and we want to
quantify the similarity of these
decisions.
Or equivalently, to measure the
difference between the system output
and desired ideal output generated by
the humans?
So obviously the higher similarity is,
the better the results are.
The similarity can be measured in
different ways.
And that would lead to different
measures, and sometimes it's desirable
also to measure the similarity from
different perspectives just to have a
better understanding of the results in
detail.
For example, it might be also
interested in knowing which category
performs better, which category is easy
to categorize, etc.
In general, different categorization
mistakes, however, have different costs
for a specific application, so some
errors might be more serious than
others.
So ideally we would like to model such
differences.
But if you read many papers in texture
catalyzation, you will see that they
don't generally do that, and instead
they will use a simplified measure.
And that's the cause.
It's often OK not to consider such a
cost variation when we compare
different methods.
An we when we are interested in knowing
the relative difference of these
methods.
So it's OK to introduce some bias as
long as the bias is not correlated with
a particular method.
And then we should still expect the
more effective method to perform better
than a less effective one, even though
the measure is not perfect.
So the first measure that we will
introduce is called classification
accuracy, and this is basically to
measure the percentage of corrective
decisions.
So here you show that here you see that
there are K categories denoted by C1
through CK and there are N documents in
order by D1 through DN an for each pair
of a category on the document that we
can then look at the situation.
And see if the system has said yes to
despair.
Basically has assigned this category to
this document or no, so this is denoted
by Y or N.
That's the system to decision.
And similarly we can look at the humans
decision.
Also, if the human has assigned a
category to the document, there will be
a plus sign here.
That's just that just means a human
would think this assignment is correct
an if the incorrect, and then there's a
minus.
So we will see.
All combinations of these ends yes and
Nos with minus and plus.
So there are four combinations in total
and two of them are correct and when we
have Y plus or minus and then there are
also two kinds of errors.
So the measure of classification
accuracy is similar to count how many
of these decisions are correct and
normalize that by the total number of
decisions we have made.
So we know that the total number of
decisions is.
In multiplied by K.
And the number of characters decisions
obviously are basically of two kinds.
One is why pluses and the other is N
minus is and we just put together the
account.
Now this is a very convenient measure
that will give us a one number to
characterize performance of method and
the higher the better of course.
But the method I also had some
problems.
First it has treated all the decisions
equally so, but in reality there's some
decision errors are more serious than
others.
For example, it may be more important
to get the decisions right on some
documents than others, and or maybe
more important to get the divisions
right on some categories than others,
and this would call for some detailed
evaluation of this results to
understand.
The strengths and weaknesses of
different methods.
And to understand the performance of
these methods in detail.
In APA category or per document basis?
One example that shows clearly the
desicion errors are having different
causes, spam filtering that could be
retrieved as a two category
categorization problem.
Missing a legitimate email is all is 1
type of error.
But letting us ma'am to come into your
folder is another type of error.
The two types of errors are clearly
very different because it's very
important not to miss a legitimate
email.
It's OK to occasionally let us spam
email to come into your inbox, so the
error of the first missing a legitimate
email is very high cost.
It's very serious mistake.
And classification error classification
accuracy does not address this issue.
There's also another problem with
imbalanced tests at the Imagine there's
a skew.
The test set where most instances are
in category one.
And 98% of instances are in category
one only 2% are in category Two.
In such a case, we can have a very
simple baseline that actually performs
very, and the baseline would Simply put
all instances in the major category.
That would give us 98% accuracy.
In this case, it's going to be
appearing to be very effective, but in
reality this is obviously not a good
result.
And so, in general, when we use
classification accuracy as a measure,
we want to ensure that the classes are
balanced.
And we wonder about equal number of
instances.
For example, in each class the minority
categories or classes tend to be
overlooked in the evaluation of
classification accuracy.
How to address these problems?
We of course would like to also
evaluate the results in other ways and
in different ways.
As I said, it's beneficial to look at
the actual must multiple perspectives.
So for example, we can look at the
perspective from each document
perspective based on each document.
So the question here is, how could
other divisions on this document?
Now, as in the general cases of all
decisions, we can think about four
combinations of possibilities.
Depending on whether the system has
said yes, and depending on whether the
human has said it correctly or
incorrectly, or say yes or no, and so
the four combinations are first.
When both the human system said yes and
that's true positives when the system
says yes, it's actually positive.
So when the system says yes, it's a
positive.
But when the human confirmed that it is
indeed correct, that becomes true
positive.
When the system says yes, but human
says no, that's incorrect.
That's a false positive FP.
And when the system says no, but the
human says yes, then it's a false
negative.
We missed one assignment.
When does the system and human said no?
Then that's also corrected vision.
That's true negatives.
Alright, so then we can have some
meshes to just better characterize the
performance by using these phone
numbers and so 2 popular measures of
precision and recall.
And these are also proposed by
information retrieval researchers in
19, six days for evaluating searching
results.
But now they have become a standard
measure used everywhere.
So when the system says yes, we can ask
the question how many are correct?
What's the percentage of correct
decisions when the system says yes?
That's called precision.
It's a true positive divided by all the
cases when the system says yes all the
positives.
The other recall the other meshes
called Recall an this measures.
Whether the document that has called
all the categories it should have.
So in this case it's divided the true
positive by true positives and false
negatives.
So these are all the cases where this
human says the document should have
this category.
So this represents the old categories
that it should have got an.
So recall tells us whether the system
has actually indeed assigned all the
categories that it should have to this
document.
This gives us a detailed view of the
decision on each document.
Then we can aggregate them later.
And if you're interested in some
documents and this would tell us how
well we did that those documents a
subset of them might be more
interesting than others.
For example, and this allows us to
analyze errors in more detail as well.
We can separate the documents of
certain characteristic from others and
then look at the errors.
You might see a pattern here for this
kind of documents along documents it
doesn't do as well as.
For short documents.
And this gives you some insight for
improving the better.
Similarly, we can look at the popular
category valuation.
This.
In this case we're going to look at the
how good are the decision on a
particular category.
And as in the previous case, we can
define precision and recall and it will
just basically answer the questions
from a different perspective.
I saw when the system says yes, how
many are corrected that means looking
at this category to see if all the
documents that are assigned with this
category are indeed in this category.
An recall would tell us has the
category being actually assigned to all
the documents that should have this
category.
Is sometimes also useful to combine
precision and recall as one measure,
and this is often done by using if
mesh.
And this is just the harmonic mean of
precision and recall defined on this
slide.
Ann
It's also controlled by a parameter
beta two to indicate the weather
precision is more important, or recall
is more important when beta is set to
one, we have a measure called F1, and
in this case we just take a equal
weight on both precision and recall.
If one is very often used as a measure
for categorisation.
Now, as in all cases when we combine
results, you always should think about
the best way of combining them.
So in this case I don't know if you
have thought about it and we could have
combining them just with the arithmetic
mean, right?
So that would still give it the same
range of values.
But obviously there's a reason why we
didn't do that and why.
If one is more popular and it's
actually useful to think about
difference.
And if you think about that, you will
see that there is indeed some
difference and sum.
Undesirable property of this arithmetic
mean.
Basically, it would be obvious to you
if you think about a case when the
system says yes for all the category
and nothing appears.
And even tried to compute the precision
and recall in that case and see what
would happen.
I basically this kind of measure will
not the arithmetic mean is not going to
be as reasonable FF1, which tends to
prefer a tradeoff between precision and
recall.
So that the two values are about equal,
so we if there's an extreme case where
you have 041 value and one for the
other, than F1 will be low, but the
arithmetic mean would still be
reasonably high.
This lecture is about the text
categorization.
In this lecture we're going to talk
about the text categorization.
This is a very important technique for
a text, data mining and analytics.
It is relevant to discovery of various
different kinds of knowledge as shown
here.
First is related to top mining
First is related to topic mining
analysis.
And that's because it has to do with
analyzing text data based on some
predefined topics.
Secondly, it's also related to opinion
mining and sentiment analysis, which
has to do is discovering knowledge
has to do with discovering knowledge
about the observer that the human
sensor.
Because we can categorize the authors,
for example, based on the content of
the articles that they have written.
I we can in general categorize the
We can in general categorize the
observer based on the content.
That they produce.
Finally, it's also related to text
based prediction.
Because we can often use text
categorization techniques to predict
some variables in the real world that
are only remotely related to text data.
And so this is very important technique
And so this is a very important technique
for text data mining.
This is the overall plan for covering
the topic.
First wanted to talk about what is text
First we're going to talk about what is text
categorization and why we are
interested in doing that in this
lecture.
And then we're going to talk about how
to do text categorisation followed by
how to evaluate the categorisation
results so.
The problem of texture categorisation
is defined as follows.
Were given a set of predefined
We're given a set of predefined
categories.
Possibly forming a hierarchy so.
And often also a set of training
examples or training set of labeled
text objects.
Which means that text objects have
already been labeled with no
already been labeled with known
categories, and then the task is to
classify any tax object into one or
more of these predefined categories.
So the picture on the slide that shows
So the picture on the slide shows
what happens.
When we do text categorization, we have
a lot of text objects to be processed
by a categorisation system.
And the system we're in general assign
And the system will in general assign
categories to these documents as soon
categories to these documents as shown
on the right.
And the categorisation results.
And we often assume the availability of
training examples, and these are the
documents that are tagged with known
categories, and these examples are very
important for helping the system to
learn patterns in different categories,
and this would further help the system
then learn how to recognize.
The categories of NYU tax objects that
The categories of new tax objects that
it has not seen.
So here are some specific examples of
text categorization, Ann.
text categorization and
In fact, there are many examples.
Here are just a few.
So first text objects can vary, so we
can categorize a document.
Or a passage or sentence or collections
of text, as in the case of clustering
the units to be analyzed can vary a
lot, so this creates a lot of
possibilities.
Secondly, categories can also vary, and
we can generally distinguish two kinds
of categories.
One is internal categories.
These are categories that characterize
content of text object.
For example, topic categories.
Or sentiment the categories and they
Or sentiment categories and they
generally have to do with the content
of the tax objects directly.
of the tax objects direct
of the tax objects, direct
Characterization of the content.
The other kind is external categories
that can characterize the entity
associated with the text object.
For example, authors or entities
associated with the content that they
produce.
And so we can use their content,
determine which author has written
which part, for example, and that's
called other exhibition.
called author attribution.
Or we can have any other meaningful
categories associated with text data,
as long as.
There is a.
There are, there's a meaningful
connection between the entity and text
data.
For example, we might collect a lot of
reviews about the restaurant.
reviews about a restaurant.
Or a lot of reviews about the product.
And then these text that all can help
And then these text data can help
us infer properties of product or
restaurant.
a restaurant.
In that case, we can treat this as a
categorization problem.
We can categorize restaurants or
categorize products based on their
corresponding reviews.
So this is example of external
category.
Here are some specific examples of
applications.
News categorization is very common, has
been started.
been stuided.
A lot.
News agencies would like to assign
predefined categories to categorize
news generated every day.
And literature article categorizations
are not important task, for example, in
another important task, for example, in
biomedical domain.
biomedical domain,
Is this mesh annotations measure stand
Is this mesh annotations measure, stands
Is this mesh annotations , mesh stands
for medical subject heading?
for medical subject heading.
And this is ontology of terms
characterize content of literature
articles in detail.
Another example of application spam,
email detection or filtering right?
So we often have a spam filter to help
us distinguish spam from legitimate
emails, and this is clearly a binary
classification problem.
Send him in the categorisation of
Sentiment categorization of
product reviews or tweets is yet
another kind of applications where we
can categorize content into positive or
negative or positive and negative or
neutral.
I saw you can have the same sentiment
so you can have the same sentiment
categories assigned.
The two text content.
to text content.
Another application is automatically
email routing or sorting, so you might
want to automatically assault your
want to automatically sort your
emails into different folders, and
that's one application of text
categorization, or each folder is a
categorization, where each folder is a
category.
There is also another important kind of
applications of routing emails to the
right person to handle.
So in helpdesk email messages generally
routed to particular person to handle
routed to a particular person to handle
different people attempt to handle
different kinds of requests and in many
cases a person with manually assign the
cases a person will manually assign the
messages to the right people.
But you can imagine you can build
automatic text categorization system to
help routing request.
help routing  a request.
help routing a request.
And this is a classified the incoming
And this is a classify the incoming
And this is to classify the incoming
request in the one of the categories
request in to one of the categories
where each category actually cause
where each category actually
corresponds to a person to handle the
request.
And finally, author Attribution.
As I just mentioned, is yet another
application, and it's another example
of using text to actually infer
properties of some other entities.
And there are also many variants of the
problem formulation and so first we
have the simplest case, which is a
binary categorization where there are
only two categories and there are many
examples like that information
retrieval or search engine applications
would want to.
Distinguish it relevant documents from
non relevant documents for a particular
query.
Spam filter is interesting.
Distinguishing spams from non spam.
So also two categories.
Sometimes classification of opinions
can be in together with positive and
can be in two categories together with positive and
can be in two categories, positive and
negative.
A more general case would be kid
A more general case would be K-category
category categorisation and there are
categorization and there are
also many applications like that.
There could be more than two
categories, so topical categorisation
is often such example where you can
have multiple topics.
Email routing would be another example
when you may have multiple folders, or
if you rather the email to the right
if you route the email to the right
person to handle it, then there are
multiple people.
multiple people, to clasify
Hi so in all these cases there are more
so in all these cases there are more
than two kinds of categories.
And other variations.
And another variation
We have hierarchical categorization,
to have hierarchical categorization,
where categories are formal hierarchy,
where categories form hierarchy,
again, topic or hierarchy is very
again, topical hierarchy is very
common.
common. Yet another categorization is joint categorization. That's when you have multiple categorization tasks that are related. And then you hope to kind of join the categorization
common. Yet another variation is joint categorization. That's when you have multiple categorization tasks that are related. And then you hope to kind of do joint categorization. Try to leverage the dependents of these tasks to improve accuracy for each individual task.
Now among all these, binary
categorization is most fundamental and
partly also because it's simple and
partly it's cause it can actually be
used to perform all the other
categorization tasks.
For example, K category categorisation
task can be actually performed by using
binary categorization.
And basically we can look at each
category separately and then the binary
capitalization problem is whether
categorization problem is whether
object is in this category or not.
Meaning in other categories.
And the hierarchical category
categorisation can also be done by
progressively doing flat categorisation
at each level.
So we can first categorize all the
objects in tune.
It's a small number of high level
categories an inside each category.
We can further categorize into sub
categories etc.
So why is text categories important
while they show that you are several
well, I already showed you several
applications, but in general there are
several reasons.
One is taxable.
One is text
Categorization helps us enrich attacks
Categorization helps us enrich text
representation, and that's to achieve
more understanding of text data that's
always useful for text analysis.
So now with categorisation, text can be
represented in multiple levels, meaning
keyword bag of words representation as
keyword bag of words representation has
keyword bag of words representation as
often used for a lot of text text
often used for a lot of text
processing tasks.
But we can also add categories and they
provide a 2 levels of representation.
provide 2 levels of representation.
Semantic categories assigned that can
Semantic categories assigned can
also be directly or indirectly useful
for application.
So for example, sentiment categories
could be already very useful, or author
Attribution might be directly useful.
And.
Another example is when semantic
categories can facilitate aggregation
of tax content, and this is another
case of.
Applications of text categorisation.
For example, we if we want to know the
overall opinions about the product, we
could first categorize the opinions in
each individual review as positive or
negative, and then that would allow us
to easily aggregate all the sentiments
and it will tell us about 70% of the
views positive and 30%, not negative,
views positive and 30% are negative,
etc.
So without doing categorization it will
be much harder to aggregate such
opinions.
So it provides a concise way of coding
text in some sense based on our
vocabulary.
And sometimes you miss seeing some
applications, text or categorization is
called a text coding encoding with some
controller vocabulary.
The second kind of reasons is to use
text categorization to infer properties
of entities.
And text categorisation allows us to
infer the properties of such entities
that are associated with tax data.
that are associated with text data.
So this means we can use text for
So this means we can use text
capitalization to discover knowledge
categorization to discover knowledge
about the world in general, as long as
we can associate the entity with text
data, we can always use the text data
to help categorize the corresponding
entities.
So it's useful to think about the
information network that will connect
the other entities with text data.
The obvious entities that can be
directly connected or authors, but you
directly connected are authors, but you
can also imagine the authors
affiliations or the authors ages and
other things can be actually connected
to text data in directly.
to text data indirectly.
Once we can make the connection, then
we can make predictions about those
values.
So this is a general way to allow us to
use text mining tool.
Sorry, text categorization to discover
knowledge about the world.
Very useful, especially in big tax
Very useful, especially in big text
data.
Analytics, where we offer interested in
Analytics, where we are often interested in
using text data as extra sensor data
collected from humans to infer certain
desicion factors.
Often together with non texture later
Often together with non text data
specifically to text.
For example, we can also think of
examples of inferring properties of
entities.
For example discovery of non native
speakers of a language and this can be
done by categorizing the content of.
Speakers
Another example is Rue predicted the
Another example is to predict the
party affiliation of partition based on
party affiliation of a politician based on
the political speech at this is again
example of using text data to infer
some knowledge about real world.
In nature this all the problems are all
the same and that's as we defined and
it's a text categorization problem.
We can compute this maximum regular
estimated by using the EM algorithm.
So in the East step, we now have to
So in the E-step, we now have to
introduce more hidden variables because
we have more topics.
So our hidden variable Z now, which is
a topic indicator, can take more than
two values.
Specifically, will take a K plus one
values with B denoting the background
and one through K to denote or the K
and one through K to denote all the K
topics.
So now the E step as you can recall is
augmented data and by predicting the
values of the hidden variable.
So we're going to predictor for a word
So we're going to predict for word
whether the world has come from one of
whether the word has come from one of
these K plus one distributions.
these K+1 distributions.
This equation allows us to predict the
probability that the world in Document
probability that the word in Document
probability that the word W in Document
D is generated from top exceed our
"D is generated from theta
"D is generated from topic theta
subject.
sub-j
subject
sub j
And the bottom one is the predicted
probability that this world has been
probability that this word has been
generated from the background.
Note that we use Document D here to
index the world.
index the word.
Why becausw?
Why? Because
Whether a word is from a particular
topic, actually depends on the
document.
Can you see why it's through the piles?
Can you see why? Well, it's through the pis.
Can you see why? Well, it's through the pi.
The piles are tide to each document.
The pis are tied to each document.
Each document can have a potentially
different pies, right?
different pis, right?
The piles will then affect our
The pis will then affect our
prediction, so the pies are here, and
prediction, so the pis are here, and
this depends on the document.
And that might give a different guests
And that might give a different guess
of word for word in different
documents, and that's desirable.
In both cases we are using the baseball
In both cases we are using the base rule
In both cases we are using the bayes rule
as I explained, basically assessing the
likelihood of generating world in from
likelihood of generating word in from
each distribution is normalized.
each distribution and is normalized.
What about the end step?
What about the M-step?
We may recall.
Well, we may recall
the M step is to take advantage of the
infertile Z values to split the counts
inferred Z values to split the counts
and then collect the right counts to re
estimate parameters.
So in this case we can re estimate our
coverage probability and this is re
estimate the based on collecting all
estimated based on collecting all
the words in the document.
And that's why we have the count of the
world in document and some over all the
world in document and sum over all the
word in document and sum over all the
words.
And then we're going to look at the to
what extent this world belongs to the
what extent this word belongs to the
topic seed are subject, and this part
topic theta sub-j, and this part
topic's theta sub-j, and this part
is our guest from eastep.
is our guest from E-step.
is our guess from E-step.
This tells us how likely this would is
This tells us how likely this word is
actually from Cedar subject, and when
actually from theta sub-j, and when
we multiply them together we get the
discounted count that's allocated for
top exterior subject and we normalize
topic theta sub-j and we normalize
this over all the topics we get the
distribution over all the topics to
indicate the coverage.
And similarly, the bottom one is, do we
And similarly, the bottom one is to
estimate the probability of word for
re-estimate the probability of word for
topic now?
topic.
In this case we're using exactly the
same account.
same count.
You can see this is the same discounted
account, it tells us to what extent we
count, it tells us to what extent we
should allocate this world two topics,
should allocate this word to ,
should allocate this word to
either subject.
topic theta sub-j.
But the normalization is different
because in this case we are interested
in the world distribution.
in the word distribution.
So we simply normalize this over all
the words.
This is different.
In contrast, here we normalized among
all the topics.
It would be useful to take a comparison
between the two.
This gives us different distributions
and these tells us.
and these tells us
How to improve the parameters?
how to improve the parameters?
An as I just explained in both E step
And as I just explained in both E step
formulas, we have a maximum likelihood
estimator based on the allocated word
count.
"counts to
2 topics in our subject.
"topic theta sub-j.
Now this phenomena is actually general
phenomenon in all the EM algorithms in
the M step, you general with computers,
the M step, you generate
the M step, you generate computed
expected account of event based on the
expected count of event based on the
E step result and then you just collect
the relevant accounts for a particular
the relevant counts for a particular
parameter.
Anri estimated normalizing.
and re-estimate with normalizing.
Typically.
So in terms of computation of the EM
algorithm, we can.
Actually, just keep counting various
events and then normalize them.
And when we think in this way, we also
have a more concise way of presenting
the EM algorithm.
It actually helps us better understand
the formulas.
So I'm going to go over this in some
detail.
So as the algorithm, we first
initialize all the unknown parameters
randomly.
I also in our case we are interested in
In our case we are interested in
all those coverage parameters.
all those coverage parameters--
Pies and word distributions, cigars.
pis--and word distributions, cigars.
pis--and word distributions, thetas.
And we just randomly normalize them.
This is the initialization step, and
then we will repeat until like Holder
then we will repeat until likelihood
converges.
Now how do we know whether like or
Now how do we know whether likelihood
converges we're going to compute it
converges we're going to compute
Holder at each step and compare the
likelihood at each step and compare the
current likely with the previous, like
current likelihood with the previous likelihood
whole if it doesn't change much and
if it doesn't change much and
we're going to say stop right?
So in each step we can do easy step and
So in each step we can do E step and
M step in the East step we're going to
M step in the E step we're going to
augment the data by predicting the
hidden variable values.
In this case the hidden variable Z sub
DW indicates whether world in Dublin D
DW indicates whether world in W in D
DW indicates whether word in W in D
is from topical or background, an if
is from topic or background, an if
it's from a topic which topic?
So if you look at the E step formulas
essentially well, actually normalizing
essentially we're actually normalizing
these counts.
At all, sorry, these are probabilities
of observing the world from each
of observing the word from each
distribution, so you can see basically
the prediction of world from topical
the prediction of word from topical
the prediction of word from topic
theater subject is based on the
theTAsubject is based on the
theta sub-j is based on the
probability of selecting that theater
probability of selecting that theta
subject as a word to begin to generate
sub-j as a word to begin to generate
sub-j as a word distribution to begin to generate
the world multiplied by the probability
of observing the world from that
of observing the word from that
distribution.
An I said it's proportional to this
And I said it's proportional to this
because in completing the
implementation of EML with you can just
implementation of EM algorithm you can just
keep account counter for this quantity
keep count counter for this quantity
and then in the end you just normalize
it.
So the normalization here is over all
the topics and then you will get a
probability.
Now in the M step we do the same and we
are going to collect these.
Allocated accounts for each topic.
Allocated counts for each topic.
And we split words among the topics.
And then we're going to normalize them
in different ways to obtain the RE
in different ways to obtain the
estimate.
re-estimate.
So, for example, we can normalize among
all the topics to get re estimate of Pi
the coverage.
Or we can renormalize based on the.
For all the words and that would give
us a water distribution.
us a word distribution.
So it's useful to think of the
algorithm in this way, because when you
implement, you can just use.
Variables will keep track of these
Variables to keep track of these
quantities in each case.
And then you just normalize these
variables to make them a distribution.
Now I did not put the constraint for
this one and I intentionally leave this
as exercise for you and you can see
what's the normalizer for this one.
It's of a slightly different form, but
it's essentially the same as the one
that you have seen here.
Name let this one.
Namely this one.
So in general, in the implementation of
EML with you will see you accumulated
EM algorithm with you will see you accumulated
EM algorithm you will see you accumulated
account various comes and then you
counts various comes and then you
counts various counts and then you
normalize them.
So to summarize, we introduced the PSA
So to summarize, we introduced the PLSA
model, which is a mixture model with K
unigram language models representing K
topics.
And we also added a predetermined
background language model to help
discover discriminating topics.
Because this background language model
can help attract the common terms.
An.
And,
We show that with maximum Lycra is made
We show that with maximum likelihood estimator
we can discover topical knowledge from
text data.
In this case PSA allows us to discover
In this case PLSA allows us to discover
two things.
One is keyword distributions, each
One is k-word distributions, each
representing a topic and the other is
the proportion of each topic in each
document.
And such detailed characterization of
coverage of topics in documents can
enable a lot of further analysis.
For example, we can aggregate the
documents in the particular time period
to assess the coverage of a particular
topic in a time period that would allow
us to generate the temporal chains of
topics.
We can also aggregate topics covered in
documents associated with a particular
author, and then we can characterize
the topics written by this author, etc.
An in addition to this, we can also
And in addition to this, we can also
cluster terms and cost documents.
cluster terms and cast documents.
cluster terms and cluster documents.
In fact, each topic can be regarded as
a cluster, so we already have term
clusters.
And the higher probability was up can
And the higher probability words can
be regarded as in belonging to one
cluster.
Represented by the topic.
Similar documents can be clustered in
Similarly documents can be clustered in
the same way.
We can assign a document to the topic
cluster that's covered most in the
document.
So remember piles indicate to what
So remember pid indicate to what
So remember pis indicate to what
extent each topic is covered in the
document.
We can assign the document to the topic
cluster that has the highest pie.
cluster that has the highest pi.
And in general, there are many useful
applications of this technique.
In general, we can use the empirical
counts of events in the observed data
to estimate probabilities.
Anna commonly used technique is called
and a commonly used technique is called
a maximum likelihood estimate, where we
simply normalize the observed accounts.
So if we do that, we can see we can
compute these probabilities as follows
for estimating the probability that we
see a word occurring in segment, we
simply normalize the counts of segments
that contain this world.
that contain this word.
So let's first take a look at the data
here.
On the right side you see I listed some
hypothesizes that data these are
segments.
And in some segments you see both words
occur.
Their indicator as once for both
columns.
In some other cases, only one word
occurs, so only that column has one and
the other column has zero.
And of course in some other cases, none
of the words occur, so they are both
zeros.
Ann
And
For estimating these probabilities, we
simply need to collect the three
counts.
So the three counts of 1st, the count
of W.
1 and that's the total number of
segments that contain world W one.
It's just the ones in the column of W
one we can just count how many ones we
have seen there.
The second counter is for word 2 an we
The second counter is for word 2 and we
just count the ones in the second
column.
And these this would give us a total
number of segments that contain W2.
The third account is when both words
occurred, so this is time we're going
to count the segments where both
columns have once.
columns have ones.
And then so this would give us the
total number of segments where we have
seen both W and W2.
Once we have these counts, we can just
normalize.
This comes by end, which is the total
These counts by n, which is the total
number of segments and this will give
us the probabilities that we need to
compute mutual information.
Now there is a small problem.
When we have zero counts sometimes and
in this case we don't want a zero
probability because our data maybe a
small sample and in general we would
believe that it's potentially possible
for award to occur in any context.
So to address this problem we can use a
technique called smoothing and that's
basically to add some small constant to
discounts and then so that we don't get
a zero probability in any case.
Now, the best way to understand the
smoothing is imagine that we actually.
Observed more data than we actually
have.
We will pretend we observe some pseudo
segments that are illustrated on the
top on the right side of the slide and
these pseudo segments would contribute
additional counts of these words so
that no event will have zero
probability probability.
Now, in particular, we introduce the
four pseudo segments.
Each is weighted 1/4.
And these represent the four different
combinations of occurrences of these
words.
So now each event, each combination
will have at least one count or at
least non zero counter.
From these pseudo segment.
So in the actual segments that we
observed, it's OK if we haven't
observed all the combinations.
So more specifically, you can see the
point of five.
Here actually comes from the two ones
in the two pseudo segments, because
each is weighted 1/4, we added them up.
We get .5.
And similarly this .05 comes from one
single pseudo segment that indicates
the two words occur together.
And of course, in the denominator we
add the total number of pseudo segments
that we added.
In this case we added a 4th through the
segments.
Each is weighted 1/4, so the total the
sum is actually one.
So that's why in the denominator you
still want there.
So this basically concludes the
discussion of how to compute the mutual
information, how to use this for
syntagmatic relation discovery.
No.
So, to summarize, select the cinematic
relation can generally be discovered by
measuring correlations between
occurrences of two words.
We introduce the three concepts from
information theory, entropy, which
meshes uncertainly over random variable
X conditional entropy, which measures
the entropy of X.
Given we know why.
And mutual information of X&Y which
matches the entropy reduction of X.
Due to knowing why or entropy reduction
of why do too knowing eggs?
They are the same, so these three
concepts are actually very useful for
other applications as well.
That's why we spend some time to
explain this in detail, but in
particular there also very useful for
discovering syntagmatic relations.
In particular, mutual information is a
principled way for discovering such a
relation.
It allows us to have values computer on
different pairs of words that are
comfortable, and so we can rank these
pairs and discover the strongest
cinematical relationship from
collection of documents.
Now note that there is some relation
between syntactic medical relation
discovery and paradigmatically relation
discovery.
So we already discussed the possibility
of using PM 25 to achieve waiting for
of using BM 25 to achieve waiting for
terms in the context to potentially
also suggest the candidates that have
seen like medical relations with the
candidate word.
But here, once we use mutual
information to discover Syntagmatic
relations, we can also represent the
context with this mutual information as
weights.
So this would give us another way to
represent the context.
Of a word like a cat, and if we do the
same for all the words, then we can
cluster these words or computer
similarity between these words based on
their context similarity.
So this provides yet another way to do
term waiting for paradigmatic.
A relation discovery an.
So to summarize, this whole part about
word Association mining, we introduce
the two basic associations, called
Paradigmatic and Syntagmatic relations.
These are fairly general.
They can be applied to any items in any
language, so the units don't have to be
worse than they can be phrases or
entities.
Are we introduced multiple statistical
approaches for discovering them?
Then it showing that pure statistical
approaches are visible?
Available for discovering both kinds of
relations, and they can be combined to
perform.
Join the analysis as well.
These approaches can be applied to any
text with no helmet human effort.
And mostly becausw.
They are based on counting of words.
Yet they can actually discover
interesting relations of words.
We can also use different ways to
define context and segment and this
would lead to some interesting
variations of applications.
For example, the context can be very
narrow, like a few words around a word
or sentence or maybe paragraphs and
using different contexts, which allows
you to discover different flavors of
paradigmatic relations.
And similarly, counting Co occurrences
using, let's say mutual information to
discover syntagmatic relations, we also
have to define the segment and the
segment can be defined as an arrow,
text window or longer text article and
this would give us different kinds of
associations.
These discovery associations can
support them.
Any other applications in both
information retrieval and text data
mining.
So here are some recommended readings.
If you want to know more about the
topic, the 1st is a book with a chapter
on locations which is quite relevant to
the topic of these lectures.
The second is the article about the
using various statistical measures to
discover lexical atoms.
Those are phrases that are non
composition compositional or for
example hot dog is not really a dog
that's hot.
Blue chip is not a chip that's blue,
and the paper has a discussion about to
some techniques for discovering such
phrases.
The third one is new paper on unified
way to discover both paradigmatic a
relation and select medical relations
using random walks on world graphs.
So now let's talk about the problem
alittle bit more and specifically let's
a little bit more and specifically, let's
talk about the two different ways of
estimating parameters.
One is called maximum likelihood
estimate that I already just mentioned.
The other is Bayesians machine.
The other is Bayesian estimation.
So in maximum likelihood estimation, we
So in Maximum Likelihood estimation, we
So in Maximum likelihood estimation, we
define best as meaning the data like
define best as meaning the data likelihood
Holder has reached the maximum, so
has reached the maximum, so
formal it's given by this expression
formally it's given by this expression
here.
Where we define the estimate as Arg,
Where we define the estimate as Arg
Where we define the estimate as
Max.
arg max.
arg max
Of the probability of X given theater.
of the probability of X given theater.
of the probability of X given Theta.
Andso automax here just means it's
Andso arg max here just means it's
And so arg max here just means it's
actually function that would return.
actually a function that would return.
actually a function that would return
The argument that gives the function
the argument that gives the function
maximum value as the value, so the
value of AWG Max is not value of this
value of arg max is not value of this
value of arg max is not the value of this
function, but rather the argument that
has made the function reaches maximum.
has made the function reach maximum.
So in this case the value of argmax is
theater.
Theta.
That's the theater that makes the.
It's the theater that makes the
It's the Theta that makes the
Probability of X Cuban cigar reaches
probability of X given Theta reaches
probability of X given Theta reach its
maximum, so this is made.
maximum, so estimate.
maximum, so this estimate intuitively
The intuitive also makes sense, and
also makes sense, and
it's often very useful, and it seeks
the parameters that best explain the
data.
But it has a problem when the data is
too small, 'cause when the data points
too small, because when the data points
are too small.
are too small,
There are very few data points.
there are very few data points.
The sample is small.
The sample is small,
Then if we trust data entirely and try
then if we trust data entirely and try
to fit the data and then will be
to fit the data and then we will be
biased.
So in the case of text data, let's say
our observed 100 words did not contain
another word related to text mining,
then our maximum likelihood estimator
would give that world zero probability.
would give that word zero probability.
Cause give me the a non zero
Because giving a non zero
probability would take away, probably
probability would take away probability
not mess from some observed world which
mass from some observed world which
mass from some observed words which
obviously is not optimal in terms of
maximizing the likelihood of the
observed data.
But this zero probability for all the.
But this zero probability for all the
Something was may not be reasonable
unseen words may not be reasonable
sometimes, especially if we want the
distribution to characterize the topic
of text mining.
So one way to address this problem is
actually to use Bayesian estimation,
where we actually would look at the
where we actually would look at
both the data and all our prior
knowledge about the parameters.
We assume that we have some prior
belief about the parameters.
Now in this case of course, so we are.
Now in this case, of course, so we are.
Now in this case, of course, so we are
Not going to look at just the data, but
not going to look at just the data, but
also look at the prior so the prior
here is defined by P of Theta.
And this means we will impose some
preference on certain features of
preference on certain Thetas of
preference on certain Thetas over
others.
An by using Bayes rule that I have
And by using Bayes rule that I have
shown here.
shown here,
We can then combine the.
We can then combine the
we can then combine the
An old functioning with the prior.
likelihood function with the prior.
likelihood function with the prior
To give us this.
to give us this.
to give us this
Posterior.
posterior
Probability of the parameter.
probability of the parameter.
Now a four explanation of Bayes,
Now a full explanation of Bayes,
Now a full explanation of Bayes Rule,
Now a full explanation of Bayes Rule
Warren, some of these things related to
and some of these things related to
Bayesian reasoning, would be outside
Bayesian reasoning would be outside
the scope of this course, but I just
give a brief introduction because this
is a general knowledge that might be
useful for you, so the Bayes rule is
basically defined here.
And allows us to write down one
conditional probability of X given Y in
terms of the conditional probability of
Y given X.
And you can see the two probabilities
are two conditional probabilities are
different in the order of the two
variables, but often the rule is used
for making inferences of.
for making inferences of,
for making inferences of
Of a variable.
of a variable.
a variable.
So let's take a look at the.
So let's take a look at
So let's take a look at it
Again, we can assume that P of X
again, we can assume that P of X
encodes our prior belief about eggs.
encodes our prior belief about the X.
That means before we observe any other
data, that's our belief about X, what
we believe some X values have higher
probability than others.
And this probability of X given Y is a
conditional probability, and this is
our posterior belief about X, because
this is our belief about X values after
we have observed Y.
Given that we have observed why now?
Given that we have observed Y now?
Given that we have observed Y, now
What do we believe about X now, do we
what do we believe about X now, do we
believe?
believe
Some values have high probabilities
some values have high probabilities
than others.
than others?
The two probabilities of related
Now, the two probabilities of related
Now, the two probabilities are related
through this can be regarded as the
probability of.
probability of
The observed evidence why.
the observed evidence why.
the observed evidence Y
Here, given a particular acts.
here given a particular acts.
here given a particular X.
So you can think about X as our
hypothesis.
And we have some prior belief about
which hypothesis shoe and after we have
which hypothesis to choose and after we have
observed Y, we will update our belief
and this updating formula is based on.
and this updating formula is based on
The combination of our prior here and
the combination of our prior here and
the likelihood of observing is Y if X
the likelihood of observing this Y if X
is indeed true.
So much for Detour about baseball.
So much for a detour about Bayes Rule.
So in our case, what we're interested
in is inferring the theater values so
in is inferring the theta values so
we have a prior here.
That includes our prior knowledge about
the parameters.
And then we have the data likelihood
here that would tell us which parameter
value can explain the data well.
The posterior probability combines both
of them.
So it represents a compromise of the
two preferences.
And in such a case, we can maximize
this posterior probability to find a
theater that would maximize.
theta that would maximize.
theta that would maximize
This posterior probability.
this posterior probability.
And this is murder is called a maximum
And this is estimator is called a maximum
And this estimator is called a maximum
And this estimator is called the maximum
And this estimator is called the Maximum
of posteriori or map estimate.
a Posteriori or MAP estimate.
And this estimate is more is a more
And this estimate is a more
general estimate than the maximum
microwaves made.
likelihood estimate.
'cause what if we define our prior as a
Because what if we define our prior as a
Because once if we define our prior as a
noninformative prior meaning that it's
uniform over all the cell values?
uniform over all theta values?
uniform over all the theta values?
uniform over all the theta values,
No preference then.
no preference, then
no preference, then,
We basically would go back to the
we basically would go back to the
maximum likelihood estimator because in
such a case it's mainly going to be
determined by this likelihood value
here.
The same as here.
OK, but if we have some informative
prior some bias towards certain values,
prior, some bias towards certain values,
then map is made.
then MAP estimate
It can allow us to incorporate that,
can allow us to incorporate that,
but the problem here of course is how
to define the prior.
There's no free lunch, and if you want
to solve the problem with more
knowledge, we have to have that
knowledge and that knowledge ideally
should be reliable.
Otherwise your estimate may not
necessarily be more accurate than
maximum likelihood Smith.
maximum likelihood estimate.
Now let's look at the Bayesian
estimation in more detail.
OK, so I show the theater values as
OK, so I show the theta values as
just a one dimension value and that's a
just one dimension value and that's a
simplification of course.
So we're interested in which value of
data is optimal.
So now first we have the prior.
The prior tells us some data values are
The prior tells us some theta values are
more likely than others.
We believe for example.
We believe, for example,
These values are more likely than the
these values are more likely than the
values like here or here or other
places.
So this is our prior.
And then we have our data likelihood.
In this case, the data also tell us
In this case, the data also tells us
which values of Theta are more likely
which values of theta are more likely
and that just means those sealer values
and that just means those theta values
can best expand our data.
can best explain our data.
And then when we combine the two, we
get the posterior distribution and
that's just a compromise of the two.
It would say that is somewhere in
It would say that it is somewhere in
It would say that it's somewhere in
between, so we can now look at the some
between, so we can now look at some
interesting point estimates of Cedar.
interesting point estimates of theta.
Now this point represents the mode of
prior.
That means the most likely parameter
value according to our prior before we
observe any data.
This pointer is the maximum Riker is
This point is the maximum Riker is
This point is the maximum likelihood
made that represents the city that
estimate that represents the city that
estimate that represents the theta that
gives the data the maximum probability.
Now this point is interesting.
It's the posterior mode, it's the.
It's the most likely value of that are
It's the most likely value of theta
given by the posterior distribution,
and it represents a good compromise of
the prior mode and the maximum Acura is
the prior mode and the maximum likehood
made.
estimate.
In general, in Bayesian inference we
are interested in the distribution of
all these parameter values.
As you see, here is there's a
distribution over Theta values that you
can see here P of Cedar given X.
can see here P of theta given X.
So the problem of Bayesian inference is
to infer this posterior distribution
and also to infer other interesting
quantities that might depend on Theta.
So I showed F of Theta here as an
interesting variable that we want to
compute.
But in order to compute this value, we
need to know the value of Theta.
In Bayesian inference, we treats data
In Bayesian inference, we treat data
as uncertain variable.
So we think about all the possible
values of Theta.
Therefore we can estimate the value of
this function F as the expected value
of F according to the posterior
distribution of data.
distribution of data
Given the observed evidence X.
given the observed evidence X.
As a special case, we can assume F
Theta is just equal to 0.
of Theta is just equal to 0.
of Theta is just equal to Theta.
In this case we get the expected value
of the Cedar.
of Theta.
That's basically the posterior mean
that gives us also one point of set up
that gives us also one point of
that gives us also one point of Theta.
an.
Theta.
And
It's sometimes the same as posterior
And it's sometimes the same as posterior
it's sometimes the same as posterior
mode, but it's not always the same, so
it gives us another way to estimate the
parameters.
So this is a general illustration of
Bayesian estimation, Bayesian
Bayesian estimation and Bayesian
Bayesian estimation and Bayesian inference.
inference.
And later you will see this can be
useful for topic mining where we want
to inject some prior knowledge about
topics.
the topics.
So to summarize, we introduced the
language model.
language model
Which is basically probability
which is basically probability
distribution over text.
It's also called a generative model for
text data.
The simplest language model is unigram
language model.
It's basically a water distribution.
It's basically a word distribution.
We introduced the concept of likelihood
function, which is the probability of
function which is the probability of
data given some model.
And this function is very important.
Given a particular set of parameter
values, this function can tell us which
X which data point has a higher
X, which data point has a higher
likelihood higher probability.
likelihood, higher probability.
Given a data point, sorry, given a data
sample X, we can use this function to
determine which parameter values would
maximize the probability of the
observed data, and this is the maximum
library estimate.
likelihood estimate.
We also talked about the Bayesian
estimation or influence.
In this case we must define a prior on
the parameters piece of data, and then
the parameters P of Theta, and then
we're interested in computing the
posterior distribution of the
parameters and, which is poor
parameters which is
proportional to the prior and the
likelihood.
And this kind of.
And this kind of
A distribution would allow them to
distribution would allow them to
distribution would allow us, then, to
infer any derive values from Sarah.
infer any derived values from Theta.
This lecture is about the sentiment
classification.
If we assume that most of the elements
in the pinion representation our the
in the opinion representation our the
in the opinion representation are
in the opinion representation are already
known, then our only task maybe just
the sentiment classification as shown
in this case.
So suppose we know who is the opinion?
So suppose we know who is the opinion
Hold an what's opinion, target an also
holder and what's the opinion, target and also
holder and what's the opinion target and also
know the content and context of the
opinion.
Then we may need to decide the opinion
Then we mainly need to decide the opinion
sentiment of the review.
So this is a case of just using
sentiment classification for
understanding opinion.
Cinnamon a classification can be
Sentiment classification can be
defined the more specifically as
defined more specifically as
follows.
follows:
The input is opinionated attacks
The input is opinionated text
object.
The output is typical, a sentiment
The output is typically, a sentiment
label or sentiment attack, and that can
label or sentiment tag, and that can
be designed the in two ways.
be designed in two ways.
One is polarity analysis where we have
categories such as positive, negative
or neutral.
The other is emotion analysis.
That can go beyond polarity to
characterize the feeling of the opinion
Holder.
holder.
In the case of polarity analysis with
In the case of polarity analysis, we
sometimes also have numerical ratings,
as you often see in some reviews on the
web.
Five might denote the most positive and
one maybe at most negative.
one maybe at most negative,
For example, in general you have just
for example. In general you have just
discrete the categories to characterize
discrete categories to characterize
the sentiment.
In emotion analysis, of course, there
also different ways to design the
are also different ways to design the
categories.
The six most frequently used to
The six most frequently used
categories are happy, sad, fearful,
angry, surprised, and discussed it.
angry, surpised and disgusted.
So as you can see, the task is
essentially a classification task or
categorisation task.
As we've seen before, so it's a special
case of text for capitalization.
case of text categorization.
This also means any text categorization
method can be used to do sentiment
classification.
Now, of course, if you just do that,
the accuracy may not be good because
sentiment classification does require
some improvement over regular text
categorization technique or simple text
categorization technique.
In particular, it needs two kinds of
improvements.
One is to use more sophisticated
features that may be more appropriate
for sentiment tagging, as I will
discuss in more in a moment.
discuss more in a moment.
The other is to consider the order of
these categories.
An especially polarity analysis, very
clear that order here and so these
categories are not all that
independent.
There is order among them, and so it's
useful to consider the order.
For example, we could use ordinal
regression to do, and that's something
that will talk more about later.
So now let's talk about some features
that often very useful for text
categorization and text mining in
general, but some of them are
especially also needed for sentiment
analysis.
So let's start from the simplest one,
which is character in grams.
which is character n-grams.
You can just have a sequence of
characters as a unit.
characters as a unit,
And they can be mixed with different
and they can be mixed with different
ends, different lenses.
n(s) and different lengths.
n(s),  different lengths.
And this is a very general way, and the
And this is a very general way, and a very
robust way to represent the text data.
You could do that for any damage pretty
You could do that for any language pretty
much.
And this is also robust to spending
And this is also robust to spelling
errors or recognition errors, right?
So if you misspell the word by 1
So if you misspelled the word by 1
character and this representation
actually would allow you to match this
world when it occurs in the text
word when it occurs in the text
correctly.
So misspelled word and the correct form
can be matched becausw they contain
can be matched because they contain
some common engrams of characters.
some common n-grams of characters.
But of course such a representation
would not be as discriminatively as
would not be as discriminative as
words.
So next we have Warden Grams, a
So next we have word n-grams, a
sequence of words and again we can mix
them with different lenses.
them with different lengths.
Uni Grams are actually often very
effective for a lot of tax processing
effective for a lot of text processing
tasks and mostly becausw words are well
tasks and mostly because words are well
tasks and that's mostly because words are well
designed.
designed
The features by humans for
features by humans for
communication, and so they often good
enough for many tasks, but it's not
good or not sufficient for sentiment
analysis.
analysis
Clearly.
clearly.
For example, we might see a sentence
like it's not good or it's not as good
as something else.
So in such a case, if you just take a
good and that would suggest positive,
it's not good.
it's not good,
Right, so it's not accurate, but if you
so it's not accurate, but if you
take the bigram, not good together, and
then it's more accurate, so longer
engrams are generally more
n-grams are generally more
discriminative and their most
discriminative and they are more
specifically.
specific.
If you match it, Ann says a lot and
If you match it and it says a lot and
it's accurate.
It's unlikely, very ambiguous.
But it may cause overfitting because
with such very unique features the
machine learning program can easily
pick up such features from the training
set and to rely on such unique features
to distinguish categories.
An obviously that kind of classifier
wanna generalize to future data when
won't generalize to future data when
won't generalize well to future data when
such discriminating features will not
necessarily occur.
So that's a problem of overfitting.
That's not desirable.
We can also consider part of speech
tag.
tag
Grams if we can do part of speech
n-grams if we can do part of speech
n-grams, if we can do part of speech
tagging and for example, adjective,
noun could form a pair.
We can also mix N grams of words an N
We can also mix N grams of words and N
grams of part of speech tags.
For example, the world great might be
For example, the word great might be
followed by a nun and this could become
followed by a non and this could become
followed by a noun and this could become
a feature hybrid feature.
a feature, a hybrid feature.
That could be useful for sentiment
analysis.
So next weekend I also have world class
So next we can also have word classes
So next we can also have word classes,
is so these classes can be syntactic
so these classes can be syntactic
like a part of speech tags.
Or could be semantic and they might
represent concepts in the service or
represent concepts in the thesaurus or
ontology like word net.
Or they can be recognized the named
entities like people or place and these
categories can be used to enrich the
representation as additional features.
We can also learn word clusters
empirically, for example with talk
empirically, for example we talk
empirically, for example we talked
about mining, associations of words and
about mining associations of words and
so we can have cluster of
paradigmatically related words or
cinematically related words.
sementically related words.
And these clusters can be features to
supplement the world based
supplement the word based
representation.
Furthermore, we can also have frequent
pattern syntax and these could be
frequent word set.
The words that the former pattern do
The words that formed a pattern do
not necessarily occur together or next
to each other.
But we also have locations where the
words might occur more closely
together.
And such patterns provide a more
discriminative features than words,
obviously, and they may also generalize
better than just the regular engrams
better than just the regular n-grams
because they are frequent, so you can
expect them to occur also in test data
so they have a lot of advantages, but
they might still face the problem of
overfitting as the features become more
complex.
This is the problem in general, and the
same is true for Paws tree based
same is true for parse tree based
features where you can use a pause tree
features where you can use a parse tree
to derive features such as frequent
subtrees or paths, and those are even
more discriminating, but they also are
more likely to cause overheating.
more likely to cause overfitting.
And in General, Patton discovery
algorithms are very useful for feature
construction, because they allow us to
search email, largest space of possible
search in a larger space of possible
features that are more complex than
words that are sometimes useful.
So in general, natural language
processing is very important to derive
complex features.
They can enrich text representation.
So for example, this is a simple
sentence that I showed you long time
ago, an another lecture.
ago, and in another lecture.
So from these words we can only derive
simple world engrams representations or
simple world n-grams representations or
character in grams.
character n-grams.
But with an LP we can enrich the
But with NLP we can enrich the
reputation with a lot of other
representation with a lot of other
information such as part of speech
tags, parse trees or entities, or even
switch active.
speech act.
Now with such enriching information, of
Now with such enriched information, of
course, then we can generate a lot of
other features, more complex features,
like a mixed grams of word and part of
speech tags.
Or even a part of past tree.
Or even a part of parse tree.
So in general, feature design actually
affects categorization accuracy
significantly, and it's a very
important part of any machine learning
application in general.
application. In general
application. In general,
I think it would be most effective if
you can combine machine learning, error
analysis and domain knowledge in design
analysis and domain knowledge in designing
and features.
features.
So first you want to use domain
knowledge and understanding of the
knowledge and your understanding of the
problem that design see the features.
problem to design seed features.
And you can also define a basic feature
space with a lot of possible features
for the Machine learning program to
work on.
And machine learning can be applied to
select the most effective features or
construct the new features that feature
learning.
And these features can then be further
analyzed by humans through error
analysis.
An you can look at the categorization
And you can look at the categorization
errors and then further analyze what
features can help you recover from
those errors or what features cause
overfitting and cause those errors, and
so this can lead to feature validation
that would revise the feature set and
then you can iterate and we might
consider using a different feature
space.
So any LP enriches tax representation.
So NLP enriches text representation.
As I just said and becausw it enriches
As I just said and because it enriches
the feature space.
It allows much largest search space of
It allows much larger search space of
features.
And there are also many meaningful
features that can be very useful for a
lot of tasks.
But be careful not to use a lot of
complicated features because it can
cause overfitting or otherwise you have
to do the training carefully, not to
let.
let
Overfeeding happen?
overfeeding happen?
overfeeding happen.
overfitting happen.
So a main challenge in design features
So a main challenge in designing features
So a main challenge in designing features,
a common challenges to optimize the
a common challenge is to optimize the
tradeoff between exhaust activity and
tradeoff between exhaustive activity and
tradeoff between exhaustivity and
specificity.
And this trade off, it turns out to be
very difficult now.
very difficult.
Exhaustivity means we want that
Now, exhaustivity means we want that
Now, exhaustivity means we want the
features to actually have high coverage
of a lot of documents.
And so in that sense, you wanted
features to be frequent.
Specificity requires the feature to be
discriminative, so naturally infrequent
features tend to be more
discriminating, so this really caused
tradeoff between frequent versus
infrequent features, and that's why I
infrequent features, and that's why
feature design is generally are tan.
feature design is generally an art.
That's perhaps the most important part
in applying machine learning to any
problem in particular.
In our case, for text categorization,
or more specifically, sentiment
classification.
So this is indeed a general idea of the
expectation maximization EM algorithm.
expectation maximization, or EM algorithm.
So in all the EM algorithms, we
introduce a hidden variable to help us
solve the problem more easily.
In our case, the hidden variable is a
binary variable for each occurrence of
word.
And this binary variable would indicate
whether the world has been generated
from seed on Sunday or theater Super
from theta on Sunday or theater Super
Big.
B.
And here we show some possible values
of these variables for you.
of these variables.
For example for the it's from
Background, Z value is 1 and text on
the other hand is from the topic.
Then it's 040Z etc.
Then it's 0 for Z etc.
Now, of course we don't observe these
Now, of course we don't observe those Z
values.
We just imagine there are such a social
values of Z attached to all the world.
values of Z attached to all the words.
And that's why we call these hidden
variables.
Now the idea that we talked about
before for predicting the order
before for predicting the word
distribution that has been used with
the general the world is it'll predict
this.
The value of this hidden variable.
Ann
And
So.
The algorithm, the EM algorithm then
would work as follows.
First will initialize all the
parameters with random values.
In our case the parameters are mainly
the probability of award given by
the probability of a word given by
status update.
So this is the initialization stage.
It is initialized values would allow us
to use Bayes rule to take a guess of
the Z values.
these Z values.
So will guess these values we can say
for sure whether taxes from background
or not, but we can have our guests.
or not, but we can have our guesses.
This is given by this formula.
It's called eastep.
It's called E-step.
And so the algorithm would then try to
use the E Step 2 gas.
These values.
These Z values.
After that it would then invoke another
spec step called aim step.
spec step called M-step.
In this step we simply take advantage
of the inferred values and then just
group words that are in the same
distribution like this from background,
including this as well.
We can normalize the count to estimate
We can then normalize the count to estimate
the probabilities or to revise our
estimate of the parameters.
So let me also illustrate we can group
the words that are believed to have
come from Cedar sub D and as text
mining algorithm for example and
clustering.
And we had group them together.
To help us re estimate the parameters.
That were interested in so these will
help us re estimate these parameters.
But note that before we just set these
parameter values randomly, But with
this gas we will have a somewhat
this guess we will have a somewhat
improved estimate of this.
Of course, we don't know exactly
whether it's zero or one, so we're not
going to really do the split in
hardware, but rather we can do those
soft split and this is what happened
here.
So we're going to adjust the count.
By the probability that we believe this
would has been generated by using the
silence update.
theta sub d.
And you can see this.
Where does this come from?
Well, this has come from here right
from the E step.
So the EM algorithm with iteratively
improve our initial estimate of
parameters by using eastep first and
parameters by using E-step first and
then M step.
the E step is to augment the data with
additional information like Z.
And the M step is to take advantage of
the additional information to separate
the data to split the data accounts and
then collect the right data counts.
Re estimate our parameters.
re estimate our parameters.
And then once we have a new generation
of parameters, we're going to repeat
this.
We're going to use the eastep again to
We're going to use the E-step again to
improve our estimate of the hidden
variables, and then that would lead to
another generation of re estimate the
parameters.
For the word distribution that we're
interested in.
OK, so as I said, the bridge between
the two is really variable Z hidden
variable, which indicates how likely
this world is from the topic word
distributions Cedar subte.
distributions theta sub d.
So this slide has a lot of content and
you may need to pause the video to
digest it, but this basically captured
the essence of EM algorithm.
Start with initial values that are
often randomly set.
And then we invoke Eastep followed by M
And then we invoke E step followed by M
step to get an improved setting of
parameters, and then we repeat this.
So this is a Hill climbing algorithm
So this is a hill climbing algorithm
that would gradually improve the
estimate of parameters and as I will
explain later, there's some guarantee
for reaching a local maximum.
for reaching a local maximum
Of the likelihood function.
of the likelihood function.
So let's take a look at the computation
for specific case.
So these formulas are the M formulas
So these formulas are the EM formulas
that you see before, and you can also
see there are superscripts here N to
indicate the generation of parameters.
I go here.
For example, we have N + 1.
That means we have improved parameters
from here to.
Here we have improvement.
here we have improvement.
So in this setting we have assumed that
the two models have equal probabilities
and the background model is known.
So what are the relevant statistics?
Well, these are the word counts.
So assume we have just 4 words and
their counts are like this and this is
our background model that assigns high
probabilities to common words like the.
An in the first iteration you can
picture what would happen.
Well, we first we initialize all
values.
the values.
So here this probability that we're
interested in is normalized into an
uniform distribution over all the
words.
And then the E step would give us a
guess.
Of the distribution that has been used
to generate each word, we can see we
have different probabilities for
different words.
Why that's be cause these words have
different probabilities in the
background.
So even though the two distributions
are equally likely, and then our
initialization says uniform
distribution because of the difference
in the background world distribution,
we have different guest probabilities.
So these words are believed to be more
likely from the topic.
These, on the other hand, are less
likely probably from background.
So once we have the Z values, we know
in the end step these probabilities
in the E step these probabilities
would be used to adjust the counts.
So for must be multiplied by this point
So 4 must be multiplied by this point
3, three in order to get the allocated
three three in order to get the allocated
counts toward the topic.
And this is done by this
multiplication.
Note that if our gas says this is 100%.
Note that if our guess says this is 100%.
If this is 1.0.
If this is 1.0
Then we just get the full Council of
this world for this topic.
this word for this topic.
But in general, as I said, it's not
going to be 1.0, so we're going to just
get some percentage of the counts
toward this topic, and then we simply
normalize these counts.
To have a new generation of practice
mate so you can see, compare this with
the old one which is here.
So compare this with this one and will
see at the probability is different.
Not only that, we also see some words
that are believed to have come from the
topic.
We have high probability like this one
text.
And of course, this new generation of
parameters would allow us to further
adjust the infer the latent variable or
hidden variable values.
So we have a new generation of values
because of the E step based on the new
generation of parameters.
And this these new in further values of
these will give us then another
generation of the estimate of
probabilities of the words.
And so on so forth.
So this is what would actually happen
when we compute these probabilities
using the EM algorithm.
As you can see in the last rule where
And as you can see in the last rule where
we showed the log like code and the
likelihood is increasing as we do the
iteration.
And note that these log likelihood is
negative becausw the probability is
between zero and one when you take
logarithm, it becomes a negative value.
What's also interesting is do not last
column, and these are the in Ferd World
column, and these are the inferred word
Split, and these are the probabilities
split, and these are the probabilities
that award is believed to have come
that a word is believed to have come
from one distribution.
In this case the topic distribution,
and you might wonder whether this would
be also useful because our main goal is
to estimate these world distribution
to estimate these word distribution
right?
So this is our primary goal.
We hope to have a more discriminating
world distribution.
But the last column is also by product
and this actually can also be very
useful and you can think about that.
And one use is to.
For example, is made to what extent
this document has covered background
words.
And this when we add this up or take
the average will kind of know to what
extent it has covered background versus
content words that are not explained
well by the background.
This structure is about the
This lecture is about the
discriminative classifiers for text
categorization.
In this lecture, we're going to
continue talking about how to do text
categorization and cover discriminative
approaches.
This is a slide that you have seen from
the discussion of Naive Bayes
classifier, where we have shown that
although naive Bayes classifier tries
to model the generation of text data
from each categories, we can actually
use base rule and to eventually write
use bayes rule and to eventually write
use bayes rule and to eventually rewrite
the scoring function as you see on this
slide and this scoring function is
basically.
basically a
Away to the combination of a lot of
weighted combination of a lot of
world features where the feature values
word features where the feature values
are word count and the feature weights
are the log of probability ratios of
the world given by two distributions
the word given by two distributions
here.
Now this kind of scoring function can
be actually a general scoring function
where we can in general represent text
data as a feature vector.
Of course is the features don't have to
Of course the features don't have to
be all the words and their features can
be other signals that we want to use.
And we mentioned that this is
precisely.
precisely
Similar to logistic regression.
similar to logistic regression.
So in this likely we're going to
So in this lecture we're going to
introduce some discriminative
classifiers.
They try to model the conditional
distribution of labels given the data
directly rather than using Bayes rule
to compute that indirectly.
As we have seen in life based.
As we have seen in naive bayes.
So the general idea of logistical
regression is to model the dependency
of the binary response variable Y here.
of the binary response variable Y here,
On some predictors.
That are denoted as X.
So here we have also changed the
notation to ask.
notation to X
For future values you may recall in the
For feature values you may recall in the
previous slide we have used FI to
previous slide we have used Fi to
represent the future values.
represent the feature values.
An here we use the notation of X
Factor, which is more common when we.
vector, which is more common when we.
vector, which is more common when we
Introduce such machine learning
algorithms, so X is our input, it's a
vector.
And with M features.
At each video has a value exhibi here
And each feature has a value exhibi here
And each feature has a value X sub I here
and our goal is modeled the dependency
and our goal is model the dependency
of this binary response variable on all
these features.
So in our categorization problem we
have two categories nests in Hawaiian
have two categories, lets say theta 1 and
Stiller 2 an we can use the Y value to
theta 2, and we can use the Y value to
demote the two categories.
denote the two categories.
And why is 1 it means the category of
And when Y is1 it means the category of
And when Y is 1 it means the category of
the documents first class still around.
the documents first class
the documents first class theta 1
Now the goal here is to model the
conditional probability of Y given X
directly as opposed to model the
generation of X&Y as in the case of
Naive Bayes.
And another advantage of this kind of
approach is that it would allow many
other features than words could be used
other features than words to be used
in this vector.
Since we're not modeling the generation
of this factor and we can plug in any
of this vector and we can plug in any
signals that we want, so this is
potentially advantages for doing text
categorization.
So most specifically, in logistic
regression the assume the functional
regression the assumed functional
form of why depending on X is the
form of y depending on X is the
following, and this is very closed,
closely related to the log or log out
closely related to the log or log odds
that I introduced in the knife base or
that I introduced in the naive bayes or
log of probability ratio of the two
categories that you have seen on the
previous slide.
So that this is what I meant, right?
So in the case of Naive Bayes, we
compute this by using baseball and
compute this by using bayes rule and
eventually we have reached a formula
that look like this.
That looks like this.
But here we actually would assume
explicitly that we would model our.
explicitly that we would model our
Probability of Y given X.
As directly as a function of these
features.
So most specifically, we assume that
log of the ratio of probability of y =
1 and the probability of y = 0.
Is a function of X.
And so it's a function of X, and it's a
linear combination of these feature
values, controller by better values.
values, controller by beta values.
values, controlled by beta values.
An it since we know that probability of
And since we know that probability of
y = 0 is 1 minus probability of y = 1,
and this can be also written in this
way.
So this is a log out ratio.
So this is a log odds ratio.
Here.
And so in logistic regression, we
basically assume that the probability
of y = 1 K Max is dependent on this
of y = 1  given X is dependent on this
linear combination of all these
features.
So it's just one of the many possible
ways of assuming that the dependency,
But this particular form has been quite
useful, and it has also has some nice
properties.
So if we rewrite this question to
actually express the probability of Y
given X in terms of X by taking by
getting rid of the logarithm and we get
this functional form and this is called
a logistical function, it's a
transformation of X into Y.
As you see.
Right side here.
on the right side here.
I saw that the eggs will be mapped into
So that the Xs will be mapped into
a range of values from zero to 1.0.
You can see, and that's precisely what
we want.
Since we have a probability here.
And the function form looks like this.
So this is the basic idea of logistic
regression, and it's very useful
regression, and it's a very useful
classifier that can be used to do a lot
of classification tasks, including text
categorisation.
categorization.
So as in all cases of model, we would
be interested in estimating the
parameters an infecting all the machine
parameters and in fact in all the machine
learning programs.
Once you set up the model set of
objective function.
To model the classifier, then the next
step is to compute the parameter
values.
In general, we're going to adjust these
parameter values, optimize the
performance of classifier on the
training data.
So in our case, let's assume we have
training data.
The training data here, Zion, why I and
The training data here, X i and Y i and
each pair is basically feature vector
of X and known label for that XYZ,
of X and a known label for that X Y,
either one or zero.
So in our case we are interested in
maximizing this conditional likelihood.
The condition will likely hold here is
The condition likelihood here is
basically to model why given the
basically to model y given the
observed X.
So it's not like a.
It's not like a modeling X, but rather
we're going to model this.
Note that this is a conditional
probability of Y given X.
And this is also precisely what we want
for classification.
Now, so the likelihood function would
be just a product over all the training
cases.
And in each case, this is the model.
And in each case, this is the modeled
The probability of observing this
probability of observing this
particular training case.
So given a particular XI, how likely
will observe the corresponding why?
will we are going to observe the corresponding why?
we are going to observe the corresponding Y i
I of course, why I could be one or zero
of course, why I could be one or zero
of course, Y I could be one or zero
and in fact the function form here
would vary depending on whether Y sub I
is one or zero.
If it's one will be taking this form.
And that's basically the logistical
regression function.
But what about this if it's 0?
If it's zero then we have to use a
Well, if it's zero then we have to use a
different form and that's this one.
Now how do we get this one?
That's just one minus the probability
of y = 1, right?
An you can easy to see this now.
And you can easily see this now.
The key point here is that the function
form here depends on the observed.
Why I if it's one, it has a different
Y I if it's one, it has a different
form than when it's 0.
And if you think about when we want to
maximize this probability will
maximize this probability we will
basically going to want this
probability to be as high as possible
when the label is one.
That means the document is in popular
That means the document is in topic
one.
But if the document is not willing to
But if the document is not we are going to
maximize this value, and what's going
to happen is actually to make this
value as small as possible.
Because there's someone.
Because they sum to one.
When a maximize this one.
When I maximize this one.
It's equivalent to minimize this one.
So you can see basically the if we
maximize the conditional.
maximize the conditional likelihood
Like older, we're going to basically
we're going to basically
try to make the prediction on the
training data as accurate as possible.
So, as in other cases, when computes,
So, as in other cases, when compute
the maximum likelihood is made up.
the maximum likelihood estimator
Basically that's will find a beta
Basically lets go find a beta
value, a set of better values that will
value, a set of beta values that will
maximize this conditional likelihood.
And this again then gives us a standard
optimization problem.
In this case, it can be also solved in
many ways.
Newtons method is a popular way to
solve this problem.
There are other methods as well, but in
the end will we're going to get the set
of data values once we have the better
of beta values once we have the beta
values, then we have well defined the
values, then we have a well defined the
values, then we have a well defined
scoring function to help us classify
scoring function to help us classify a
document right?
So what's the function?
Well, it's this one.
If we have all the better values
If we have all the betavalues
already known, all we need a little
already known, all we need is to
computer.
compute
The exercise for that document.
The Xi's for that document.
And then plugging those values that
will give us a estimate.
The probability that the document is in
category one.
OK, so much for logistical regression.
Let's also introduce another
discriminative classifier called K
nearest neighbors.
Now in general, I should say there are
many such approaches.
An follow introduction to all of them
And thorough introduction to all of them
is clearly beyond the scope of this
course and you should take a machine
learning course or read more about
machine learning to know about them.
Here, just want to include the basic
introduction to some of the most
commonly used classifiers, since you
might use them often for text
categorization.
So the second classifier, colon key
So the second classifier, is called k
nearest neighbors.
In this approach, we're going to also
estimate the conditional probability of
label.
Given data, but even a very different
Given data, but in a very different
way.
So the idea is to keep all the training
examples and then once we see a text
object that we want to classify, we're
going to find the K examples in the
training set and that are most similar
to this text object.
Basically this is to find the neighbors
of this text object in the training
data set.
So once we found we found the
neighborhood and found the objects that
are close to the.
The object were interested in
The object we're interested in
classifying and say we have found the K
nearest neighbors.
That's why this method is called K
nearest neighbors.
And then we're going to assign the
category that's most common in these
neighbors.
Basically, we're going to allow these
neighbors who vote for the category of
neighbors to vote for the category of
the object that we're interested in
classifying.
Now that means if most of them have a
particular category, is the category
particular category, lets say category 1
while they're going to say this current
then we're going to say this current
object will have category one.
This approach, it counts, will be
This approach, can also be improved
improved by considering the distance of
by considering the distance of
a neighbor and the current object.
Basically, we can assume a close
neighbor will have more saying about
the category of this object, so we can
have we can give such a neighbor more
influence on the vote and we can take
away to some of their votes based on
weighted sum of their votes based on
the distances.
But the general idea is to look at the
neighborhood and then try to assess the
category based on the categories of the
neighbors.
Intuitively, this makes a lot of sense.
But mathematically, this can also be
regarded as a way to directly estimate
the conditional probability of label
given data that is P of Y given X.
Now I'm going to explain this intuition
in the moment, but before we proceed,
let me emphasize that we do need a
similarity function here in order for
this work.
I note that in nine base classifier we
I note that in naive base classifier we
did not need a similarity function.
An in logistical regression, we did not
talk about the similarity function
either.
But here we explicitly requires
But here we explicitly requires a
similarity function.
Now this is similar to function.
Now this is similarity function.
Now this similarity function.
Actually is a good opportunity for us
to inject any of our insights about
features.
Basically, effective features are those
that would make the objects that are in
the same category look more similar,
but distinguishing objects in different
categories.
So the design of this similarity
function is closely tide to the design
function is closely tied to the design
of the features in logistic regression.
Another classifiers, so let's
and other classifiers, so let's
illustrate how K and then works.
illustrate how K-NN works.
Suppose we have a lot of training
instances here.
And I've colored them differently and
to show just different categories.
Now suppose we have a new object in the
center that we want to classify.
So according to this approach will work
So according to this approach we're going to
in the final neighbors.
find neighbors.
find the neighbors.
And let's first think of a special case
of finding just one neighbor closest
of finding just one neighbor, the closest
neighbor.
Now in this case, if the, let's assume
the closest neighbor is the box filled
with diamonds an so then we're going to
with diamonds and so then we're going to
say.
Well, since this is in, this object is
in category of diamonds.
Let's say then we're going to say,
well, we're going to assign the same
Category 2 hour types object.
Category to our text object.
But let's also look at the another
possibility of finding a larger
neighborhood.
So let's think about the four
neighbors.
In this case, we're going to include a
lot of other solid field boxes in red
or pink.
So in this case now we will notice that
So in this case now we  are going to notice that
among the four neighbors there after
among the four neighbors there are actually
three neighbors in a different
category.
So if we take a vote, then we'll
conclude the object is actually over
conclude the object is actually of a
different category.
So this both illustrates how can
So this both illustrates how K
nearest neighbor works and also
illustrates some potential problems of
this classifier.
Basically the results might depend on
the cake and indeed case important
the K and indeed case important
the K and indeed K is and important
the K and indeed K is an important
parameter to optimize.
Now you can intuitively imagine if we
have a lot of neighbors around this
object and then we'll be OK because we
have a lot of neighbors for help us
decide categories.
But if we have only a few, then the
decision may not be reliable.
So on the one hand we want to find more
neighbors right?
And then we have more votes, but on the
other hand as we try to find the more
neighbors, we actually could risk on
getting neighbors that are not.
getting neighbors that are not
Really similar to this instance, they
really similar to this instance, they
might be actually far away as you try
to get more neighbors, so although you
get more labels to vote, but those
get more neighbors to vote, but those
neighbors aren't necessary so helpful
because they are not very similar to
the object.
So the parameter as there has to be set
empirically and typically you can
optimize such a parameter by using
cross validation.
Basically, you're going to separate
your training data into two parts and
then you're going to use one part to
actually help you choose.
The.
The
The printer K here or some other
The parameter K here or some other
parameters in other classifiers, and
then you're good to assume this number
then you're going to assume this number
that works well on your training set
would be actually the best for your
future data.
So as I mentioned that Cayanan can be
So as I mentioned that KNN can be
actually regarded as estimate of
conditional probability of Y given X,
and that's why we put this in the
category of discriminative approaches.
So the key assumption that we made it
So the key assumption that we made
in this approach is that the
distribution of the label given the
document or probability of a category
given document.
For example, probability of seat I
For example, probability of theta I
given document D is local smooth and
given document D is locally smoothed and
that just means we're going to assume
that this probability is the same for
all the documents in this region.
I are here.
R  here.
And suppose we draw a neighborhood and
we're going to assume in this
neighborhood, since the data instances
instances are very similar, we're going
are very similar, we're going
to assume that the conditional
distribution of the label, given the
data, would be roughly the same.
If D is not different, very different
than we're going to assume that the
probability of Cedar given D would be
probability of theta given D would be
also similar, and so that's a very key
assumption, and that that's.
That's actually important assumption
that would allow us to do a lot of
machine learning, but in reality,
whether this is true of course would
depend on how we define similarity,
because the neighborhood is largely
determined by our similarity function.
If our similarity function captures
objects that do follow similar
distributions, then this assumption is
OK.
But for our similarity function could
But if our similarity function could
not capture that.
Obviously the assumption would be a
problem, and then the classifier would
not be accurate.
Let's proceed with this assumption.
Then what we are saying is that in
order to estimate the probability of
order to estimate the probability of a
category given a document, we can try
to estimate the probability of the
category given that entire region.
Now this has the benefit of course, of
bringing additional data points to help
us estimate this probability.
And so this is precise idea of Canon.
And so this is precise idea of KNN.
Basically now we can use the known
categories of all the documents in this
region to estimate this probability.
An I have even given a formula here
And I have even given a formula here
where you can see we just count the
topics in this region and then
normalize that by the total number of
documents in the region.
So the numerator that you see here see
So the numerator that you see here c
of Theta INR is account of the
of Theta and R is a count of the
documents in region R with category
Theta I.
Since these are training documents, we
know they're categories.
We can simply count how many times we
have seen sports here, how many times
we have seen science etc.
And then.
And then
Denominator is just a total number of
denominator is just a total number of
documents.
documents
Training documents in this region, so
training documents in this region, so
this gives us a rough estimate of which
category is most popular in this
neighborhood, and we're going to assign
the popular category to our data
objective since it falls into this
region.
This lecture is about the latent
Dirichlet allocation or LDA.
In this lecture, we're going to
continue talking about that topic
continue talking about topic
models.
In particular.
In particular,
We can do talk about some extension of
we are going to talk about some extension of
we are going to talk about some extensions of
PSA, and one of them is LDA or latent
PLSA, and one of them is LDA or latent
Dirichlet allocation.
So the plan for this lecture is to
cover two things.
One is to extend the PSA with prior
One is to extend the PLSA with prior
knowledge that would allow us to have
in some sense a user controller PSA, so
in some sense a user controlled PSA, so
in some sense a user controlled PLSA, so
it doesn't blindly just listen to data
but also would listen to our needs.
The second is to extend the PSA as a
The second is to extend the PLSA as a
generative model fully generated model.
This has led to the development of
latent originate allocation or LDA.
latent dirichlet allocation or LDA.
Latent Dirichlet Allocation or LDA.
So first let's talk about the PR as it
So first let's talk about the PLSA as it
So first let's talk about the PLSA as s
So first let's talk about the PLSA as a
So first let's talk about the PLSA
was prior knowledge.
with prior knowledge.
In practice, when we apply PLC to
In practice, when we apply PLSA to
analyze text data, we might have
additional knowledge that we want to
inject to guide the analysis.
The standard PSA is going to blind and
The standard PLSA is going to blindly
listen to the data by using maximum
likelihood estimator.
Working just fit data as much as we can
We are going to just fit data as much as we can
and get some insight about data.
This is also very useful, but sometimes
a user might have some expectations
about which topics to analyze.
For example, we might expect to see in
For example, we might expect to see
retrieval models as a topic in
information retrieval.
We also may be interested in certain
aspects such as battery and memory when
looking at the opinions about the
laptop, because the user is
particularly interested in these
aspects.
Now, a user may also have knowledge
about the topic coverage.
And we may know which topic is
definitely not covered in which
document or is covered in the document.
For example, we might have seen those
tags topic attacks assigned to
tags topic tags assigned to
documents.
And those tax could be treated as
And those tag could be treated as
topics if we do that, then a document
that can only be generated using topics
corresponding to the tax already
corresponding to the tags already
assigned to the document.
If the document is not assigned A tag,
If the document is not assigned to a tag,
we're going to say there's no way for
using that topical to generate the
using that topic to generate the
document.
The document must be generated by using
the topics corresponding to the
assigned the tags.
assigned tags.
So the question is, how can we
incorporate the such knowledge into
incorporate such knowledge into
PSA?
PLSA?
It turns out that there's a.
A very elegant way of doing that, and
that's all incorporated such knowledge
as priors on the models.
And you may recall in Bayesian
inference we use prior together with
data to estimate parameters, and this
is precisely what will happen.
So in this case we can use maximum a
posteriori estimate, also called map
estimate, and the formula is given
here.
Basically is to maximize the posterior
distribution probability and this is a
combination of the likelihood of data
and the prior.
So what would happen is that we're
going to have estimated that listens to
going to have an estimate that listens to
the data and also listens to our prior
preferences.
We can use this prior, which is denoted
as P of Lambda to encode.
All kinds of preferences and
constraints.
So for example, we can use this to
encode the need of having precise 1
encode the need of having precisely 1
background the topic.
Now this can be included as a prior
Now this can be encoded as a prior
because we can say the prior for the
parameters is only a non zero if the
plan does contain one topic that's
equivalent to the background language
model.
In other words, in other cases if it's
not like that, we're going to say
supplier says it's impossible.
So the probability of that kind of
model setting would be 0 according to
our prior.
So now we can also, for example use the
prior to force particular choice of
topic to have a probability of a
certain number.
For example, we can force the document
D to choose topical one with
D to choose topic one with
probability of 1/2.
Or we can prevent a topic from being
used in generated document.
So we can say the third topic should
not be user generated.
Document D will set to the Pi value to
0 for that topic.
We can also use the prior to favor set
of parameters with topics that assign
high probabilities to some particular
words.
In this case, we're not going to say
it's impossible, but we're going to
just strong in favor certain kind of
just strongly favor certain kind of
distributions.
And you will see example later.
The map can be computed using a similar
EM algorithm as we have used for that
maximum likelihood estimator with just
some modification to smallest
parameters reflect the prior
preferences.
And in such a estimate, if we use a
special form of the prior called
conjugate prior, then the functional
form of the prior will be similar to
the data.
As a result, we can combine the two and
the consequences that you can basically
convert the influence of the prior into
the influence of having additional
pseudo data because the two functional
forms are the same and they can be
combined.
So the effect is as if we had more
data.
And this is convenient for computation.
It doesn't mean conjugate prior is the
best way to define prime.
best way to define the prior.
So now let's look at the specific
example.
Suppose the user is particularly
interested in battery life of a laptop,
and we're analyzing reviews.
So the prior says that the distribution
should contain one distribution that
would assign high probabilities, tool,
would assign high probabilities to
battery, and life.
So we could do say there's a
distribution that's entirely
concentrated on battery life and we all
Prius is that one of your distributions
priors is that one of your distributions
should be very similar to this.
Now if we use map estimate with the
Now if we use map estimator with the
conjugated prior, which is Dirichlet
prior Dirichlet distribution based on
this preference, then the only
difference in the EM algorithm is in
the M step.
When we re estimate word distributions,
we are going to add.
Additional counts to reflect our prior
right.
right?
So here you can see the pseudocounts
are defined the based on the
probability of words in our prior.
So battery obviously will have a high
pseudocounts similar life would have
also high pseudocounts or the other
words.
We have 0 pseudocounts because their
probability is zero in the prior and
when you see this is also controlled by
a parameter mu Ann.
a parameter mu and
We're going to add mu multiplied by the
probability of W given our prior
distribution to the connected accounts.
distribution to the connected counts.
When we re estimate the when we re
estimate the this world distribution
right?
So this is the only step that changed
and the changes happened here and
before we just collect the counts of
words that we believe have been
generated from this topic.
But now we force this distribution.
To give more probabilities to these
words by adding them to the
pseudocounts so to artificially in
effect, we artificially inflated their
probabilities and to make this
distribution we also need to add this
many pseudocounts to the denominator.
This is the total sum of all the
pseudocounts we have added for all the
words.
This would make this again a
distribution.
Now, this is a intuitively very
reasonable way of modifying the EM
algorithm an theoretically speaking,
algorithm and theoretically speaking,
this deal works, and it computes the
map estimator.
It's useful to think about two specific
extreme cases of MEW.
extreme cases of Mu.
Now cancel the picture.
Now can you picture.
Think about what would happen if we set
Muta Zero.
Mu to Zero.
Well, that's essentially to remove this
prior, so mu in some sense indicates
our strength on prior.
Now what would happen if we set Mewtwo
Now what would happen if we set Mu to
positive Infinity?
Well, that's to say this price is so
strong that we're not going to listen
to the data at all.
So in the end you can see in this case
we can do make one distribution fixed
to the prior.
You see why?
When view is Infinity, we basically let
When mu is Infinity, we basically let
this one dominate.
In fact, we can do set this one.
In fact, we are going to set this one.
Two precise this distribution, so in
to precise this distribution, so in
this case it is this distribution, and
that's why we said the background
language model is in fact a way to
improve supplier, because with false
enforce a prior, because with false
enforce a prior, because we force
one distribution to be exactly the same
as what we give that the background
as what we give, that's the background
distribution.
So in this case we can even force the
distribution to entirely focused on
battery life.
But of course this won't work well
'cause it cannot attract other words,
it would affect the accuracy of
counting.
Topics about the battery life so in
practice mule is set somewhere in
practice mu is set somewhere in
between, of course.
So this is one way to impose our prior.
We can also impose some other
constraints.
For example, we can set any parameters
for constant, including zero as needed.
for constraints, including zero as needed.
For example, we may want to set one of
the piles to 0.
the pis to 0.
And this would mean we don't allow that
topic to participate in generating that
document.
And this is only reasonable, of course,
when we have prior knowledge E.
when we have prior knowledge
That strongly suggests this.
that strongly suggests this.
This lecture is about the syntagmatic
relation discovery and conditional
entropy.
In this lecture, we're going to
continue the discussion of water
continue the discussion of word
Association mining an analysis.
association mining an analysis.
Working to talk about the conditional
We're going to talk about the conditional
entropy, which is useful for
discovering some mathematical
discovering syntagmatic
relations.
Earlier we talked about using entropy
to capture how easy it is to predict
the presence or absence of a word.
Now we address the different scenario
where we assume that we know something
about the text segment.
So now the question is, suppose we eats
So now the question is, suppose we know eats
a curd in the segment, how would that
occured in the segment, how would that
help us predict the presence or absence
of world?
of a word?
of a word like meat?
And I can meet, and in particular we
like meat, and in particular we
And in particular we
want to know whether the presence of
its has helped us predict the presence
eats has helped us predict the presence
of meat.
And if we frame this using entropy,
that would mean we are interested in
knowing whether knowing the presence of
eats could reduce uncertainty about the
meat or reduce the entropy of the
random variable corresponding to the
presence or absence of meat.
We can also ask the question, what if
we know of the absence of eats?
With that also help us predict the
Would that also help us predict the
presence or absence of meat.
So these questions can be addressed by
using.
Another concept, the color conditional
Another concept, called the conditional
entropy.
So to explain this concept, let's first
look at the scenario we had before
where we know nothing about the
where we know nothing about the segment
where we know nothing about the
segment.
So we have these probabilities
indicating whether a word like meat
occurs or does not occur in the
segment, and we have the entropy
function that looks like what you see
on the slide.
I suppose we know eats is present, so
now know the value of another random
variable that denotes eats.
Now that would change all these
probabilities to conditional
probabilities where we look at the
presence or absence of meter.
presence or absence of meat.
Given that we know it's a curd in the
Given that we know eats occured in the
context.
So as a result, if we replace these
probabilities with their corresponding
conditional probabilities in the
entropy function will get the
entropy function, we will get the
conditional entropy.
So this equation now here.
Would be.
The conditional entropy conditional on
The conditional entropy conditioned on
the presence of eats.
I.
Iike.
Right.
Right?
So you can see this is essentially the
same entropy function as you have seen
before, except that we all the
probabilities now have a condition.
And this then tells us the entropy of
meat after we have known each occurring
meat after we have known eats occurring
in the segment.
And of course, we can also define this
conditional entropy for the scenario
where we don't see each.
where we don't see eats.
So if we know it's did not occur in the
So if we know eats did not occur in the
segment, then this end conditional
segment, then this conditional
entropy would capture the uncertainty
of meat in that content in that
condition.
So now putting different scenarios
together, we have the complete
definition of conditional entropy as
follows.
Basically.
We're going to consider both scenarios
of the value of eats zero or one, and
this gives us the probability that eats
is equal to 01.
is equal to 0 or 1.
Basically, whether it is present or
Basically, whether eats is present or
absent, and this of course is the
entropy conditional entropy of meat in
that particular scenario.
So if you expand this entropy, then you
have the following equation.
Where you see the involvement of those
conditional probabilities.
Now in general, for any discrete random
variables X&Y we have.
The conditional entropy is no larger
than the entropy of the variable X, so
basically this is upper bound for the
conditional entropy.
That means by knowing more information
about the segment, we won't be able to
increase the uncertainty.
We can only reduce uncertainty, and
that intuitively makes sense because as
we know more information, it should
always help us.
Make the prediction and he cannot hurt
Make the prediction and it cannot hurt
the prediction in any case.
Now what's interesting here is also to
think about what's the minimum possible
value of this conditional entropy.
Now we know that the maximum value is
the entropy of X.
But what about the minimum?
So what do you think?
I hope you can reach the conclusion
that the minimum possible value would
be 0 and it will be interesting to
think about and what situation will
think about and in what situation will
achieve this.
So let's see how we can use conditional
entropy to capture syntagmatic
relations.
Now, of course this conditional entropy
gives us directly one way to measure
the Association of two words.
the association of two words.
Because it tells us to what extend we
Because it tells us to what extent we
can predict the one word given that we
know the presence or absence of another
word.
Now before we look at the intuition of
conditional entropy in capturing
syntagmatic relations, it's useful to
think of a very special case listed
here, that is, the conditional entropy
of.
of
The word given itself.
the word given itself.
So.
So,
Here we listed the this.
here we listed the this.
here we listed the this
And conditional entropy in the middle.
conditional entropy in the middle.
So it's here.
So what is the value of this?
Now.
This means we know whether meet occurs
This means we know whether meat occurs
in the sentence and we hope to predict
whether the meat occurs in the
sentence.
Not of course this is zero 'cause
Now of course this is zero 'cause
Now of course this is zero because
there's no uncertain there anymore.
there's no uncertain there anymore
Once we know whether the word
occurrence in the segment will already
occurs in the segment will already
occurs in the segment we will already
know the answer for the prediction.
So this is 0.
And that's also when this conditional
entropy reaches the minimum.
So now let's look at the some other
So now let's look at some other
cases.
So this is a case of.
Knowing the and trying to predict the
meat and this is the case of knowing
each in trying to predict the meat.
eats in trying to predict the meat.
eats and trying to predict the meat.
Which one do you think is smaller?
Note that a smaller entropy means
easier for prediction.
Which one do you think is higher?
Which one is smaller?
If you look at the uncertainty, then in
the first case that doesn't really tell
the first case the doesn't really tell
us much about the meat, so knowing the
occurrence of that doesn't really help
occurrence of the doesn't really help
us reduce the entropy that match, so it
stays as fairly close to the original
entropy of meat.
Whereas in the case of eats.
Whereas in the case of eats,
It is related to meet, so knowing
eats is related to meet, so knowing
presence of it or absence of it's what
presence of eats or absence of eats what
presence of eats or absence of eats would
helps us predict weather meet occurs so
help us predict weather meet occurs so
help us predict wether meat occurs so
it can help us reduce entropy.
it can help us reduce entropy
Of meat, so we should expect the
of meat, so we should expect the
signatum they made this one.
second term, namely, this one
To have a smaller entropy.
to have a smaller entropy.
And that means there is a stronger
Association between meat and eats.
association between meat and eats.
So will we not also know when this is
So we now also know when this is
So we now also know when this w is
the same as this meet then the entropy
the same as this meat then the entropy
conditional entropy would reach its
minimum which is 0?
And for what kind of words would be the
And for what kind of words would it
reach its maximum?
Well, that's when this W is not really
related to meet.
related to meat.
And the for example, it would be very
like the, for example, it would be very
close to the maximum, which is the
entropy of meat itself.
So this suggests that we can use
conditional entropy for mining
cinematica relations.
syntagmatic relations.
The algorithm would look as follows.
For each word one, we're going to
For each word w1, we're going to
For each word W1, we're going to
enumerate the overall other words W2,
and then we can compute the conditional
entropy.
entropy
Of W one given W2.
Of W1 given W2.
of W1 given W2.
And we thought all the candidate words
in ascending order of the conditional
entropy, because it will run the favor
entropy, because we want to favor
award that has a small entropy, meaning
a word that has a small entropy, meaning
that it helps us predict the target
word W one, and then we can take the
word W1, and then we can take the
top ranked the candidate words as words
that have potential select medical
that have potential syntagmatic
relations with WR.
relations with W1.
Note that we need to use a flash code
Note that we need to use a threshold
to find these words.
The threshold can be the number of top
candidates to take or absolute value
for the conditional entropy.
Now this would allow us to mine the
most strongly correlated words with a
particular word W 1 here.
particular word W1 here.
But it this algorithm does not help us
But this algorithm does not help us
mind the strongest that pay syntagmatic
mine the strongest that pay syntagmatic
mine the strongest K syntagmatic
relations from entire collection.
Be cause in order to do that, we have
Because in order to do that, we have
to ensure that these conditional
entropies are comperable across
entropies are  comparable across
different words.
In this case of discovering Syntagmatic
relations for a target word like W one,
relations for a target word like W1,
we only need to compare the conditional
entropies.
entropies
For W, one given different words.
For W1 given different words.
And in this case they all come parable
And in this case they all comparable
right?
So the conditional entropy of WR given
So the conditional entropy of W1 given
W2 and conditional entropy over WR
W2 and conditional entropy of W1
given W 3 or compatible.
given  W3 comparable.
given  W3 are comparable.
They all measure how hard it is to
predict that everyone.
predict W1.
But if we think about the two pairs
where we share W2 in the same condition
and we try to predict the WR&W 3, then
and we try to predict the W1&W3, then
the end conditional entropies are
the conditional entropies are
actually not comperable well.
actually not comperable.
And you can think about this question,
why so?
why?
Why are they not comfortable?
So Why are they not comfortable?
So Why are they not comparable?
Well, that was 'cause they have a
Well, that was because they have a
different upper bounds, right?
So those upper bounds are precisely the
entropy of WR.
entropy of W1
And the entropy of W.
and the entropy of W3.
3 and they have different upper bounds,
And they have different upper bounds,
so we cannot really compare them in
this way.
So how do we address this problem?
Later with discuss we can use mutual
Later we'll discuss we can use mutual
information to solve this problem.
Now let's look at the another behavior
of mixture model and in this case let's
look at their response to the data
frequencies.
OK, So what you're seeing now is
basically the likelihood function for
the two Word document, and we know in
the two word document, and we know in
this case the solution is to give text
a probability of .9 and the probability
a probability of 0.9 and the probability
of .1.
of 0.1.
Now it's interesting to think about a
scenario where we start adding more
words to the document.
So what would happen if we add mini
So what would happen if we add many
this to the document?
the's to the document?
Now this will change the game, right?
So how picture?
So how? Well, picture
What would the likelihood function look
what would the likelihood function look
like now?
It started with the likelihood function
for the two words.
As we add more words, we know that, but
As we add more words, we know that,
we have to just multiply the likelihood
function by additional terms to account
for the additional occurrences of the.
Since in this case all the additional
terms are there, we're going to just
terms are the, we're going to just
multiply by this time for the
multiply by this term for the
probability of the.
An if we have another occurrence of
the, we multiplied again by the same
the, we multiply again by the same
time and so on, so forth until we add
term and so on, so forth until we add
as many terms as the number of this
as many terms as the number of the's
that we added to the document D prime.
Now this obviously changes the
likelihood function, so what's
interesting is now to think about how
would that change our solution.
So what's the optimal solution now?
Intuitively, you would know the
original solution.
.9 versus .1 will no longer be optimal
0.9 and  0.1 will no longer be optimal
for this new function, right?
But the question is how should we
change it?
Or in general there's something one.
Well in general there's something one.
Well in general they sum to one.
So in order to change it, we must take
away some probability mass from one
away some probability mess from one
world.
word.
An added problem with the mass to the
An added the probability mass to the
other world.
other word.
The question is which word to have
The question is which word to have a
reduced the probability which word to
reduced the probability and which word to
have a larger probability?
And in particular, let's think about
the probability of the.
Should it be increased to be more than
.1 or should we decrease it to less
0.1 or should we decrease it to less
than .1?
than  0.1?
What do you think?
Now you might want to post a video
Now you might want to pause the video
moment to think more about this
a moment to think more about this
question, 'cause this has to do with
question, because this has to do with
understanding of important behavior of
a mixture model and indeed all the
maximum likelihood estimator.
Now if you look at the formula for a
moment then you will see.
It seems that now the objective
function is more influenced by the then
function is more influenced by the than
text before each contributed one turn.
So now, as you can imagine, it would
make sense to actually assign a smaller
probability for text and to make room
for a larger probability for the why,
for a larger probability for the. Why?
'cause the is repeated many times if we
Because the is repeated many times if we
increase it a little bit, it will have
more positive impact, whereas a slight
decrease of text.
We have relatively small impact because
it occurs just once.
Right, so this means there is another
behavior that we observe here that is
high frequency words general, how high
high frequency words generally how high
high frequency words generally will have high
probabilities from all the
distributions.
And this is no surprise at all, because
after all we are maximizing the
likelihood of the data.
So all the more word occurs, then it's
it makes more sense to give such a word
a high probability because the impact
would be.
would be
More on the likelihood function.
more on the likelihood function.
This is in fact a very general
phenomenon of all the maximum
likelihood estimator, but in this case
we can see as we see more occurrences
of term.
It also encourages the unknown
distribution center Sunday to assign
distribution theta sub D to assign
distribution theta sub d to assign
some of the higher probability to this
somewhat higher probability to this
world.
word.
Now it's also interesting to think
about the impact of probability of
Cedar sub B.
theta sub B.
The probability of choosing one of the
two component models.
Now, with being so far, assuming that
Now, we've being so far, assuming that
each model is equally likely and that
gives us .5, but you can again look at
gives us 0.5, but you can again look at
this like your function and try to
picture what would happen if we
increase the probability of choosing a
background model.
Now you will see those these terms for
Now you will see these terms for
the we have a different form where the
the will have a different form where the
probability of that would be even
probability of thewould be even
probability of the would be even
probability of 'the' would be even
larger 'cause the background that has a
larger because the background that has a
high probability for the world and the
high probability for the word  and the
high probability for the word and the
coefficient.
coefficient
Band of Point Line which is now .5
in front of point nine which is now .5
in front of point nine which is now 0.5
in front of 0.9 which is now 0.5
would be even larger when this is
would be even larger. When this is
larger the overall result would be
larger and that also makes the less
larger and that also makes them less
important for senior sub D to increase
important for thetasub D to increase
important for theta sub D to increase
the probability for the cause.
the probability for the because
It's already very large so the impact
it's already very large so the impact
here of increasing the probability of
the is somewhat regulated by this
coefficient .5.
coefficient 0.5.
If it's a larger on the background then
it becomes less important to increase
the value so.
This is the.
So.
This means the behavior here, which is
high frequency words, tend to get a
high frequency words tend to get a
high frequency words tend to get
higher probabilities are affected or
regularised some water by the
regularised somewhat by the
probability of choosing each component.
The more likely a component that is
being chosen, it's more important than
to have higher values for these
frequent words.
If you have a very small probability of
being chosen, than the incentive is
less.
So to summarize, we have just discussed
the mixture model and we discussed the
estimation problem of mixture model and
in particular we discussed some general
behavior of the estimate an that means
we can expect the our estimator to
capture these intuitions.
1st Every component component model
attempts to assign high probabilities
to high frequency words in the data.
And this is true collaboratively
And this is tocollaboratively
And this is to collaboratively
maximized likelihood.
maximize the likelihood.
maximize likelihood.
Second, different component models tend
to bet on about high probabilities on
to bet high probabilities on
different words, and this is avoided.
different words, and this is to avoid
Competition or waste of probability,
competition or waste of probability,
and this would allow them to
collaborate more efficiently to
maximize the likelihood.
3rd, the probability of choosing each
component.
component
Regulates the collaboration and
regulates the collaboration and
competition between the component
models.
It would allow some component models to
respond more to the change, for example
of frequency of data point in the data.
We also talk about the special case of
fixing one component or background or
fixing one component to a background or
fixing one component to a background word
distribution, and this distribution can
be estimated by using a collection of
documents.
Large collection of Indian English
A large collection of English
documents.
documents,
By using just one distribution and then
by using just one distribution and then
we'll just have normalized frequencies
of terms to give us the probabilities
of all these words.
Now when we use such a specialized
mixture model, we show that we can
effectively get rid of background words
in the other component.
And that would make it the discover the
And that would make it the discovered
And that would make the discovered
topic more discriminative.
This is also an example of imposing a
prior on the model parameters and the
prior.
prior
Here basically means one model must be
here basically means one model must be
exactly the same as the background
language model, and if you recall what
we talked about in Bayesian estimation
and this prior would allow us to favor
a model that's consistent with our
prior.
In fact, if it's not consistently,
In fact, if it's not consistent,
we're going to say the model is
impossible, so it has a zero prior
probability, and that effectively
excludes.
excludes
Such a scenario.
such a scenario.
This is also issue that will talk more
This is also an issue that we will talk more
later.
This lecture is about overview of
This lecture is about an overview of
statistical language models which cover
probabilistic topic models as special
cases.
In this lecture we're going to give a
In this lecture we're going to give an
overview of statistical language
models.
These models are general models that
cover probabilistic topic models as
special cases.
So first, what is the statistical
language model?
A statistical language model is
basically the probability distribution
over word sequences.
So, for example, we might have a
distribution that gives.
distribution that gives
Today is Wednesday a probability of .1.
Today is Wednesday a probability of 0.001
"""Today is Wednesday"" a probability of 0.001"
It might give.
It might give
Today Wednesday is which is a non
"""Today Wednesday is"" which is a non"
grammatical sentence very, very small
probability as shown here.
And similarly another sentence, the
"And similarly another sentence, ""the"
"And similarly another sentence, ""The"
eigenvalue is positive, might get a
"eigenvalue is positive"", might get a"
probability of pointer or what?
probability of 0.00001
So as you can see, such a distribution
clearly is context dependent.
It depends on the context of
discussion.
Some word sequences might have higher
probabilities than others.
But the same sequence of words might
have a different probability in a
different context.
And so this suggests that such a
distribution character characterized
distribution can actually characterized
distribution can actually characterize
topic.
Such a model account will be regarded
Such a model can also be regarded
as a probabilistic mechanism for
generating text.
An that just means we can view text
And that just means we can view text
data as data observed from such a
model.
For this reason, we also call such a
model generating model.
model generative model.
So now.
So now
Given a model, we can then sample
sequences of words.
So for example, based on the
distribution that I have shown here on
this slide, we might, let's say, sample
a sequence like today's Wednesday
a sequence like today is Wednesday
"a sequence like ""today is Wednesday"""
because it has a relatively high
probability, we might often get such a
sequence.
We might also get the eigenvalues
We might also get the eigenvalue is
"We might also get ""the eigenvalue is"
positive, sometimes with a smaller
"positive"", sometimes with a smaller"
probability.
Very, very occasional.
Very, very occasionally,
Might get today.
Might get today
"Might get ""today"
"we might get ""today"
Wednesday is because the probability is
"Wednesday is"" because the probability is"
so small.
So in general, in order to characterize
such a distribution, we must specify
probability values for all these
different sequences of words.
Obviously it's impossible to specify
that, because it's impossible to
enumerate all the possible sequences of
words.
So in practice we will have to simplify
the model in some way.
So the simplest language model is
called a unigram language model.
In such a case, we simply assume that
text is generated by generating each
word independently.
Now, in general, the words may not be
generated independently, but after we
make this assumption, we can
significantly simplify the language
model.
Basically now the probability of a
sequence of words W sub one through
sequence of words W _1 through
sequence of words W_1 through
sequence of words w_1 through
doubles up N would be just a product of
W_n would be just a product of
w_n would be just a product of
each.
The probability of each word.
So for such a model we have as many
parameters as the number of words in
our vocabulary.
So here we assume we have N words, so
we have N probabilities, one for each
word, and there's something one.
word, and they sum to one.
So now we can assume our text is a
sample drawn according to this world
sample drawn according to this word
distribution.
That just means we're gonna draw.
That just means we're gonna draw
Award each time and then eventually
a ward each time and then eventually
a word each time and then eventually
will get a text.
we'll get the text.
we'll get a text.
So for example now again.
We can try to sample words according to
a distribution.
We might get Wednesday off or today
We might get Wednesday often or today
often and some other words like
eigenvalue might have a small
probability, etc.
Now, with this we actually can also
compute the probability of every
sequence, even though our model only
specify the probabilities of words.
specifies the probabilities of words.
This is because of the independence
assumption.
So specifically we can compute the
probability of today is Wednesday.
"probability of ""today is Wednesday""."
Because it's just a product of the
probability of today, probability of is
and probably Wednesday.
For example, I showed some fake numbers
here and we might then multiply these
numbers together to get the probability
of today's Wednesday.
of today is Wednesday.
"of ""today is Wednesday""."
So as you can see, with N
probabilities, one for each word, we
actually can characterize the
probability distribution over all kinds
of sequences of words, and so this is a
very simple model.
Ignore the world order, so it may not
Ignore the word order, so it may not
be effective for some problems such as
speech recognition, where you may care
about the order of words.
But it turns out to be quite sufficient
for many tasks that involve.
for many tasks that involve
Topic analysis, and that's also what
topic analysis, and that's also what
we're interested in here.
So when we have a model, we generally
have two problems that we can think
about.
One is given a model.
How likely will observe certain kind of
How likely we'll observe certain kind of
data points.
That is, we're interested in the
sampling process.
The other is the estimation process and
that is to figure out the parameters of
the model given some observed data, and
we're going to talk about that in a
moment.
Let's first talk about the sampling.
So here I show 2 two examples of water
So here I show 2 two examples of word
So here I show two examples of word
distributions or unigram language
models.
The first one has higher probabilities
for words, a text mining Association,
for words,  text, mining, association,
etc.
Now this signals a topic about text
mining, because when we sample words
from such a distribution with 10 to see
from such a distribution we tend to see
words that often occur in text mining
context.
So in this case, if we ask the question
about what is the probability of
generating a particular document, then
we likely will see text that looks like
a text mining paper of course.
But
...
The taxes that we generated by drawing
The text that we generated by drawing
words from this distribution is
unlikely coherent, although the
probability of generating a text mining
paper publishing in the top conference
is non zero.
Assuming that no word has a zero
probability in the distribution and
that just means we can essentially
generate all kinds of text documents,
including very meaningful text
documents.
The second distribution show on the
bottom has different words that was
bottom has different words that with
higher property.
higher probability.
In addition, so food, nutrition and
Food, nutrition and
healthy to set etc.
healthy, diet etc.
So this clearly indicates a different
topic and in this case it's probably
about health.
So if we sample words from such
distribution, then the probability of
observing a text mining paper would be
very very small.
On the other hand, the probability of
observing attacks that looks like a
observing a text that looks like a
food nutrition paper would be high,
relatively higher.
So that just means given a particular
distribution, different attacks will
distribution, different text will
have different probabilities.
Now let's look at the estimation
problem.
Now, in this case, we're going to
assume that we have observed data.
We know exactly what the text data
looks like.
In this case, let's assume we have a
text mining paper.
In fact, it's abstract of the paper, so
the total number of words is 100, and
I've shown some counts of individual
words here.
If we ask the question, what is the
most likely language model that has
been used to generate this text data,
assuming that the text is observed from
some language model, what's our best
guess of this language model?
OK, so the problem now is just the
estimated probabilities of these words
as I've shown here.
So what do you think?
What would be your gas?
What would be your guess?
Would you guess?
Would you guess
Text that has a very very small
text that has a very very small
probability or relatively large
probability.
probability?
What about the query?
Your guess probably will be dependent
on how many times we have observed this
word in the text data, right?
And if you think about it for a moment,
an if you like many others, you would
and if you like many others, you would
have guessed that text has a
probability of 10 out of 100.
Because I've observed text 10 times in
the text that has a total of 100 words.
And similarly, mining has five out of
100.
And query as a relatively small
probability, just observa once.
probability, just observd once.
So it's one of the 100.
So it's one out of 100.
Right, so that, intuitively, is a
reasonable guess, but the question is
this.
Is this
Our best gas or best estimate of the
our best guess or best estimate of the
parameters?
Of course, in order to answer this
question we have to define what we mean
by fast.
by best.
In this case, it turns out that our
gases are indeed the best in some
guesses are indeed the best in some
sense, and this is called maximum
likelihood estimate.
And it's the best thing that it would
And it's the best in that it would
give our observed data the maximum
probability.
Meaning that if you change the estimate
somehow even slightly, then the
probability of the observed tax data
probability of the observed text data
will be somewhat smaller.
And this is called a maximum like Laura
And this is called a maximum likelihood
Smith.
extimation
estimate
estimate.
This lecture is about the similarity
based approaches to text for
clustering.
In this lecture, we're going to
continue the discussion of how to do a
text clustering.
In particular, we're going to cover a
different kind of approaches than
generative models.
And that is similarity based
approaches.
So the general idea of similarity based
clustering is to explicitly specify a
similarity function to measure the
similarity between 2:00 text objects.
Now this is in contrast with a
generative model where we implicitly
define the clustering bias.
By using a particular objective
function like a likelihood function.
The whole process is driven by
optimizing the likeable, but here we
explicitly provide a review of what we
think are similar, and this is often
very useful because then it allows us
to inject any particular view of
similarity into the clustering program.
So once we have a similarity function,
we can then aim at optimally
partitioning to partitioning the data
into clusters or into different groups.
Anne, try to maximize the intragroup
similarity and minimize the intergroup
similarity.
That is, to ensure the objects that are
put in the same group to be similar,
but the objects that are put into
different groups to be not similar, and
these are the general goals of
clustering.
And there's often a tradeoff between
achieving both goals.
Now, there are many different methods
for doing similarity based clustering.
In general, I think we can distinguish
two strategies at high level.
One is to progressively construct the
hierarchy of clusters.
And so this often leads to hierarchical
clustering an we can further
distinguishes two ways to construct the
hierarchy depending on whether we
started with the collection to divide
the collection or start with individual
objects and gradually group them
together.
So one is bottom up that can be called
agglomerative, where we gradually Group
A similar object into larger and larger
clusters until we group everything
together.
The other is top down or divisive.
In this case we gradually partitioning
the whole data set into smaller and
smaller clusters.
The other general strategy is to start
with the initial tentative clustering
and then iteratively improve it and
this often leads to a flat clustering.
One example is K means.
So as I just said, there are many
different clustering methods available
and.
A full coverage of all these custom
methods would be beyond the scope of
this course.
But here we can talk about the two
representative methods and.
In some detail.
One is hierarchical agglomerative
clustering or agency, the other is
Kenny's.
KMEANS
K-MEANS.
So first let's look at the
agglomerative hierarchical clustering.
In this case, we are giving a
similarity function calls to measure
similarity between two objects and then
we can gradually group similar objects
together in a bottom up profession to
form larger and larger groups, and they
also form a hierarchy and then we can
stop when some stopping criterions
that.
I could be either some number of
classes has been achieved, or the
threshold for similarity has been
reached.
There are different variations here and
there mainly differ in the ways to
computer group similarity based on the
individual object similarity.
So let's illustrate how can induce a
structure based on just similarity.
So start with all the text objects and
we can then measure the similarity
between them.
Of course based on the provider
similarity function and then we can see
which pair has the highest similarity
and then just group them together.
And then was going to see which pair
is.
The next one to group.
Maybe these two now have the highest
similarity.
And then we can gradually group them
together in every time we're going to
pick the highest similarity similarity
pairs to group.
This will give us a binary tree
eventually to group everything
together.
Now depending our applications, we can
use the whole hierarchy as structure
for browsing for example, or we can
choose the cut off at say come here to
get four clusters.
Or we can use the threshold to cut or
we can cut at this high level to get
just the two clusters.
So this is a general idea.
Now, if you think about how to
implement this algorithm, you will
realize that we have everything
specified except for how to compute the
group similarity.
We are only given the similarity
function or two objects, but as we
group groups together we also need to
assess the similarity between two
groups.
And there are also different ways to do
that, and there's the three popular
methods are single link complete link
an average link?
So given two groups and singling
algorithm is going to define the group
similarity as the similarity of the
closest repair of the two groups.
Complete Link defines the similarity of
two groups as the similarity of the
father sister pair.
Average link defines the similarity as
average of similarity of all the pairs
of the two groups.
So it's much easier to understand these
methods by illustrating them.
So here are two groups G1 and G2 with
some objects in each group, and we know
how to compute the similarity between
two objects.
But the question now is, how can we
compute the similarity between the two
groups?
And then we can in general basis on the
similarities of the objects in the two
groups.
So in terms of single link and we're
just looking at the closest pair.
So in this case these two pairs objects
would define the similarity of the two
groups.
As long as they are very close orders,
say the two groups are very.
Close so it's optimistic view of
similarity.
The complete link, on the other hand,
will be in some sense pessimistic and
by taking the similarity of the two
farthest appear as the similarity for
the two groups.
So we're going to make sure that if the
two groups are.
two groups are
Having a high similarity, then every
having a high similarity, then every
pair of the two the objects in the
tubes will have will be insured to have
group will be insured to have
high similarity.
Every link is in between, so it takes
average of all these pairs.
Now, these different ways of computing
group similarities will need to
different clustering algorithms, and
they will generally give different
results.
Now, so it's useful to take a look at
their differences and to make
comparison.
Our first single link.
Can be expected to generate the loose
clusters.
The reason is becausw.
As long as two objects are very similar
in the two groups, it would bring the
two groups together.
If you think about this is similar to
having parties with people, then it
just means two groups of two groups of
people would be putting together as
long as each group there is a person
that is well connected with the other
group.
So the two leaders of the two groups
can have a good relationship with each
other and then they will bring together
the two groups.
In this case, the cluster is rules
because there's no guarantee that other
members of the two groups are actually
very close to each other.
Sometimes they may be very far away.
Now in this case it's also based on
individual decision, so it could be
sensitive to outliers.
The complete linker is in the opposite
situation where we can expect the
clusters to be tight.
Anne, it's also based on individual
decision, so it can be sensitive to
outliers.
Again, to continue the analogy to
having a party of people then complete
the link would mean when two groups
come together they want to ensure that
even the.
Even the people that are unlikely to
talk to each other would be comfortable
with talking to each other, so ensure
the whole class to be coherent.
The average link, of course is in
between and group decision, so it's
going to be insensitive to outliers.
In practice, which one is the best?
Well, this will depend on the
application and sometimes you need a
loose classes and to aggressively on
cluster objects together.
Then maybe simple English good.
But other times you might need a tight
clusters, then completely completely
better, but in general you have to
empirically evaluate these methods for
your application to know which one is
better.
Now let's look at another example of
method for similarity based classroom
in this case.
Which is called K means clustering will
represent each text object as a term
vector and then assuming similarity
function defined onto objects.
Now we're going to start with some
tentative clustering result by just
selecting Kate randomly selected
vectors as centroids of K clusters and
treat them as sentence as they
represent each cluster.
So this is.
This gives us the initial tentative
classroom.
And then we're going to iteratively
improve it, and the process goes like
this.
And once we have these Central
Eastside, we're going to assign a
vector to the cluster hosts entry that
is closest to the current vector.
So basically we're going to measure the
distance between this vector and each
of the centroids, and see which one is
closest to this one, and then just put
this class this object into that
cluster.
Now this is to have tentative.
Assignment of objects into clusters and
we're going to partition or the objects
into K clusters based on our tentative
clustering centroids.
And then we're going to recovery,
compute the centroid based on the
allocated objects in each cluster.
And this is.
To adjust the centroid and then we had
repeated this process until the
similarity based on objective function.
In this case it's within cluster sum of
squares converges an theoretically we
can show that this process actually is
going to minimize the within cluster
sum of squares where define objective
function.
Given K clusters.
So it can be also shown this process
will converge to a local minimum.
I think about this process for a
moment.
It might remind you the EM algorithm
for mixture model.
Indeed, this algorithm is very similar
to the EM algorithm for the mixture
model for clustering.
So more specifically, we also
initialize these.
Predators in the EM algorithm, so the
random inner inner initialization is
similar.
And then in the EML with them, you may
recall that we're going to repeat
eastep and M step to improved our
primary destinations.
In this case, we're going to improve
the clustering result iteratively by
also doing 2 steps, and in fact the two
steps are very similar to EM algorithm.
In that when we allocate vector into
one of the clusters based on our
tentative clustering, it's very similar
to inferring the distribution that has
been used with generally the document
in the mixture model.
So it's essentially similar to eastep.
Also, what's the difference?
While the differences here, we don't
make a probabilistic on location as in
the case of the step.
But rather we make a choice.
We're going to make a call if this data
point is closest to cluster two that
were going to say you are in class too.
So there's no choice, and we're not
going to say you are 70% belonging to
class too.
And so we're not going to have a
probability, but we're going to just
put one object into precisely one
cluster.
In the E step, however, we do a
probabilistic location, so we split the
counts.
And we're not going to say exactly
which distribution has been used to
generate the data point.
Now the next we're going to adjust the
centroid, and this is very similar to M
step where we re estimate the
parameters.
That's when we'll have a better
estimate of the parameter.
So here we have a better clustering
result by adjusting the centroid.
And note that the central is adjusted
based on the average of the vectors in
the.
A cluster, so this is also similar to
the M step, where we do counts pull
together counter and normalize them, or
the difference of course is also
because of the difference in the
instep, and we're not going to consider
probabilities when we count the points
in this case, for K means we're going
to only count the objects allocated to
this cluster, and this is only a subset
of data points.
But in the EM algorithm, we in
principle consider all the data points.
Based on probabilistic allocations.
But in nature they are very similar and
that's why it's also maximizing where
defined objective function and it's
guaranteed to convert converted local
minimum.
So to summarize our discussion of
clustering methods, we first discussed
the model based approaches, mainly the
mixture model.
And here we use is implicitly
similarity function.
To define the clustering bias, there's
no explicit definer similarity
function.
The model defines clustering bias.
And the clustering structure is built
into a generated model.
That's why we can use potentially a
different model to recover different
instruction.
Complex generative models can be used
to discover complex clustering
structures.
We did not talk about it, but we can
easily design generated model to
generate a hierarchical clusters.
We can also use prior to further
customize clustering algorithm to for
example, control the topic of 1 cluster
or multiple clusters.
However, one disadvantage of this
approach is that there is no easy way
to direct or control the similarity
measure.
Sometimes we want to do that, but it's
very hard to inject the such a explicit
definition of similarity into such a
model.
We also talked about the similarity
based approaches.
These approaches are more flexible.
Directly specify similarity functions.
But one potential disadvantage is that
their object function is not always
very clear.
The K means algorithm has a clearly
defined the objective function, but
it's also very similar to a model based
approach.
The hierarchical clustering algorithm,
on the other hand, is.
It's harder to.
To specify the objective function so
it's not clear what exactly is being
optimized.
Both approaches can and generate the
term clusters and document clusters.
An term clusters can be in general
generated by representing each term
with some text content.
For example, take the context of each
term as a representation of each term
as we have done in paradigmatic
relation learning.
And then we can certainly cluster terms
based on actually their tax
representations.
Of course, term clusters can be
generated by using generative models as
well as we have seen.
This lecture is continued discussion of
evaluation of textual categorisation.
Earlier we have introduced measures
that can be used to compute the
precision and recall for each category.
precision and recall for each category
And each document now in this lecture
qnd each document. Now in this lecture
and each document. Now in this lecture
Wigan too.
We
We're going to do
we're going to do
Further, examine how to combine the
further  examine how to combine the
performance on these different
categories or different documents.
How do we aggregate them?
How do we take average you see on the
How do we take average? You see on the
title here?
title here,
I indicated it's called a macro average
and this is in contrast to micro
average that will talk more about that
later.
So.
Again, for each category, we can
compute the precision recall and F1 so
for example, for category C one.
We have precision.
We have precision,
We have precision
P1 recall are one and F value F1 and
P1 recall R1 and F value F1 and
similarly we can do that for Category 2
an all the other categories.
and all the other categories.
Once we compute that an we can
Once we compute that, then we can
aggregate them so for example, we can
aggregate them. So for example, we can
aggregate all the precision values for
all the categories to compute the
overall precision and this is often
very useful.
To summarize what we have seen in the
whole data set and the aggregation can
be done in many different ways.
Again, as I said, are in case one unit
Again, as I said, in case one unit
Again, as I said, in case you need to
Again, as I said, in case when you need to
to aggregate different values.
It's always good to think about what's
the best way of doing the aggregation.
For example, you can consider
arithmetic mean, which is very commonly
used.
Or you can use geometric mean which
would have different behavior depending
on the way you aggregate.
You might have got different
conclusions.
In terms of which method works better,
so it's important to consider these
differences and choosing the right one
or more suitable for your task.
or more suitable one for your task.
So the difference, for example between
arithmetic mean and geometric mean is
that the arithmetic mean would be
dominated by high values, whereas
geometric mean would be more affected
by low values, and so whether you want
to emphasize low values or high values
would be a question related to your
application.
And similar we can do that for recall
and F score, so that's how we can then
generate the overall precision, recall
and F score.
Now we can do the same for aggregation
over all the documents, right?
So it's exactly the same situation for
each document or computer precision.
Recall an F.
Recall and F.
And then after we have completed the
computational for all these documents
computations for all these documents
were going to aggregate them to
we were going to aggregate them to
generate the overall precision over
generate the overall precision, overall
recall an overall F score.
recall and overall F score.
These are again examining the results
from different angles and which one is
more useful would depend on your
application.
In general, it's beneficial to look at
the results from all these
perspectives, and especially if you
compare different methods in different
dimensions.
It might reveal which method is better,
in which measure or in what situations,
and this provides insight for
understanding the strength of a method
or weakness, and this provides further
insight for improving them.
So as I mentioned, there is also micro
averaging in contrast to the macro
average that we talked about earlier.
In this case, what we do is to pull
together all the decisions.
An then compute the precision and
recall.
So we can compute the overall precision
and recall by just counting how many
cases are in true positive, how many
cases in false positive, etc.
Basically computing the values to fill
in this contingency table and then we
can compute precision recall just once.
Now, in contrast, in macro averaging
we're going to do that for each
category 1st and then aggregate over
these categories.
Or we do that for each document and
then aggregate over all the documents.
But here we pulled them together.
Now this will be very similar to the
classification accuracy that we
introduced earlier, and one problem
here of course, is to treat all the
incidences, all the decisions equally.
instances, all the decisions equally.
Anne, this may not be desirable.
And, this may not be desirable.
But it may be a property for some
But it may be appropriate for some
applications, especially if we
associate, for example, the cost for
each combination.
Then we can actually compute, for
example, weighted classification
accuracy where you associate the
different cost or utility for each
specific decision.
So there could be variations of these
methods that would be more useful, but
in general macro average tends to be
more informative than micro averaging
just 'cause it might reflect the need
just because it might reflect the need
for understanding performance on each
category or performance oriented
category or performance on each
document which are needed in many
applications.
But the macro averaging an micro
But the macro averaging and micro
averaging, they're both very common and
you might see both reported in research
papers on texture categorisation.
papers on text categorisation.
Also, sometimes categorisation results
might actually be evaluated from
ranking perspective.
Now this is the cause.
Now this is because.
Categorisation results are sometimes or
often indeed passed to human for
various purposes.
For example, it might be passed to
humans for further editing.
For example, news articles can be
tentatively categorized by using the
system and then human editors would
then correct them.
An all the email messages might be
And all the email messages might be
routed to the right person for handling
in the help desk, and in such a case
the categorizations do help
prioritizing the task for a particular
customer service person.
So in this case, the results have to be
prioritized.
And if the system can give a score to
the categorisation, decision or
the categorisation decision or
confidence, then we can use the.
confidence, then we can use the
Scores to rank these decisions and then
scores to rank these decisions and then
evaluate the results as a ranked list,
just as in search engine evaluation,
where you rank the documents in
responsible query.
response to the query.
So for example, discovery of spam
emails can be evaluated.
emails can be evaluated,
Based on ranking emails for the spam
based on ranking emails for the spam
category and this is useful if you want
people to verify whether this is really
spam, right?
The person would then take the ranked
list to check one by one and then.
list to check one by one and then
Verify whether this is indeed a spam.
verify whether this is indeed a spam.
So to reflect the utility for humans in
such a task, it's better to evaluate
the ranking accuracy, and this is
basically similar to search again.
And in such a case, often the problem
can be better formulated as a ranking
problem instead of categorization
problem.
So for example, ranking documents in
the search engine can also be framed as
a binary categorization problem,
distinguishing relevant documents that
are useful to users from those that are
not useful.
But typically we frame this as a
ranking problem and we evaluated as a
ranked list.
That's be cause people tend to examine
the results sequentially, so ranking
evaluation more reflects the utility
from users perspective.
So, to summarize, categorization
evaluation, first evaluation is always
very important for all these tasks, so
get it right.
If you don't get it right, you might
get misleading results an you might be
misled to believe one method is better
than the other, which is in fact not
true.
So it's very important to get it right.
Measures must also reflect the intended
use of the results for particular
application.
For example, in spam filtering and news
categorization results are used in may
categorization results are used in maybe
be different ways.
different ways.
So then we would need to consider the
difference and design measures
appropriately.
We generally need to consider how will
the results be further processed by a
user and then think from a user's
perspective what quality is important.
But aspect of quality is important.
What aspect of quality is important.
Sometimes there are tradeoffs between
multiple aspects, like precision and
recall, and then, so we need to know
for this application is high recall
more important or high precision is
more important.
Ideally we associate the different cost
with each different decision error and
this of course has to be designed in
application specific away.
Some commonly used measures for
relative comparison of different
methods or the following classification
accuracy is very commonly used for
especially balanced.
especially balanced
Pasta set.
tester set.
Precision, recall, and F scores are
commonly reported to characterize the
performances in different angles, and
there are some also variations like per
document based on evaluation per
document based evaluation, per
category evaluation and then take
average of all of them in different
ways.
Micro versus macro averaging.
In general, you want to look at the
results from multiple perspectives and
for particular application in some
perspectives would be more important
than others, but for diagnosis,
analysis of categorization methods an
analysis of categorization methods and
it's generally useful to look at the
it's generally useful to look at
minute.
as
As many perspectives as possible to see
many perspectives as possible to see
subtle differences between methods or
to see where a method might be weak,
from which you can obtain insights for
improving a method.
Finally, sometimes ranking may be more
appropriate, so be careful.
Sometimes categorisation, task and
maybe better frame as a ranking task
and there are machine learning methods
for optimizing ranking measures as
well.
So here are two suggested readings are
one is some chapters of this book where
you can find more discussion about
evaluation measures.
The second is a paper about the
comparison of different approaches to
text categorization and it also has
excellent discussion of how to evaluate
the text localization.
the text categorisation.
This lecture is about how to use
generative probabilistic models for
text categorization.
There are in general are two kinds of
approaches to text categorization by
using machine learning.
One is generating problem risk models,
One is generative problem risk models,
One is generative probabilistic models,
the other is discriminative approaches.
In this selective, we're going to talk
In this lecture, we're going to talk
about the generative models.
In the next chapter, we're going to
In the next lecture, we're going to
talk about discriminative approaches.
So the problem of text categorization
is actually very similar to document
clustering in that we assume that each
document belongs to one category or one
cluster.
Main difference is that in clustering
we don't really know what other
we don't really know what are the
predefined categories or what are the
clusters.
In fact, that's the goal of text
clustering.
We want to find such clusters in the
data.
But in the case of categorization, will
But in the case of categorization, we are
given the categories.
So we kind of have predefined
categories and.
And then based on these categories and
then based on these categories and
training data, we would like to
allocate a document to one of these
categories, or sometimes multiple
categories.
But because of the similarity of the
two problems, we can actually adapt
document clustering models for text
categorization.
Or we can understand how we can use
generating models to do text
generative models to do text
categorization from the perspective of
clustering.
And so this is a slide that we've
talked about before about text
clustering, where we assume there are
multiple topics represented by word
distributions.
Each topic is 1 cluster.
So once we estimate such model, we
faced the problem of deciding which
cluster document should belong to and
cluster document d should belong to and
this question boils down to decide
which sitar I has been used to generate
which thtea i has been used to generate
which theta i has been used to generate
the.
D.
Suppose D has L words represent
represent as XI here.
represent as Xi here.
Now, how can you compute the
probability that particular topic word
distributions Eli has been used to
distributions theta i has been used to
generate this document?
In general, we use baseball to make
In general, we use bayes rule to make
this inference.
And you can see this.
And you can see this
Prior information here.
That we need to consider if a topic or
cluster has a higher prior then it's
more likely that the document has been
from this cluster, so we should favor
such a cluster.
The other is a likelihood part at this
The other is a likelihood part, that is this
part.
And this has to do with whether the
topic water distribution can explain
topic word distribution can explain
the content of this document well.
And we want to pick a topic that's high
by both values.
So more specifically, we just multiply
them together and then choose which
topic has highest product.
topic has the highest product.
So more rigorously, this is what we
would be doing, so we're going to
choose the topic of that with the
choose the topic that with the
choose the topic that will the
choose the topic that will
maximize this posterior probability of
the topic given the document.
Get posterior becausw this one P of
Theta is supplier that's our belief
Theta i is the prior, that's our belief
about which topic is more likely.
Before we observe any document.
But this conditional probability here?
But this conditional probability here
Is the posterior probability of the
topic after we have observed the
document of the.
document of d
document of d.
document  d.
document d.
And Bayes rule allows us to update this
probability based on the prior an.
probability based on the prior and
I have assumed the details.
I shown the details.
Below here you can see how the prior
here is related to the posterior on the
left hand side.
At this is related to how well this
And this is related to how well this
world distribution explains the
word distribution explains the
document here, and the two are related
in this way.
So to find the topic that has the
highest posterior probability here,
it's equivalent to maximize this
product as we have seen also multiple
times.
times
In this course.
in this course.
An we can then change the probability
of document in your product of the
probability of each word and that's
just because we've made the assumption
about the independence in generating
about the independence in generating each word
toward OK.
OK.
So this is just something that you have
seen in document clustering.
An we now can see clearly how we can
assign a documentary category based on
assign a documentary to a category based on
the.
the
Information about world distributions
Information about word distributions
information about word distributions
for these categories and the prior on
these categories.
So this idea can be directly adapted to
So this idea can be directly adapted to do
categorisation Ann.
categorization and
This is precisely what Naive Bayes
classifier is doing, so here it's
mostly the same information, except
that we're looking at the
categorization problem Now, so we
categorization problem now, so we
assume that if.
assume that if
Theater I represents category I
Theta I represents category I
Theta i represents category I
accurately that means the world
accurately that means the word
distribution characterizes the content
of documents in category I accurately.
of documents in category iii accurately.
of documents in category i accurately.
Then what we can do is precise like
Then what we can do is precisely
what we did for text clustering.
like what we did for text clustering.
Namely, we can do a signed document D
Namely, we are going to assign document D
to the category that has the highest
probability of generating this
document.
In other words, we're going to maximize
this posterior probability as well.
And this is related to the prior and
the likelihood an as you have seen on
the previous slide.
And so naturally, we can then decompose
this likelihood into a product.
As you see here now here I changed the
notation so that we will write down the
product as product over all the words
in the vocabulary an even if even
in the vocabulary and even if even
though the document doesn't contain all
the words an the product is there
the words and the product is there
accurately representing the product of
all the words in the document.
The cause of this account here.
because this account here.
because of count here.
because of this count here.
I went award doesn't occur in the
when the word doesn't occur in the
when a word doesn't occur in the
document.
The account would be 0, so this time
The count would be 0, so this time
The count would be 0, so this count
we're just disappear.
would just be zero.
would just disappear.
So effectively we're just have the
product over all the words in the
document.
So basically with naive Bayes
classifier, we're going to score each
category for a document by this
function.
Now you may not set here.
Now you may notice that here
It involves the product of a lot of
small probabilities an this can cause
small probabilities and this can cause
underflow problem.
So one way to solve the problem is to
take logarithm of this function, which
doesn't change the order of these
categories, but would help us preserve
precision and so this is often the.
This is often the function that we
actually use to score each category,
and then we're going to choose the
category that has highest.
category that has the highest
Score by this function.
score by this function.
So this is called a naive based
So this is called a Naiyes Bayes
classifier.
Now the keyboard base is understandable
Now the keyword Baayes is understandable
Now the keyword Bayes is understandable
because we are applying a Bayes rule
here.
When we go from the posterior
probability of the topic to a product
of the likelihood and the prior.
Now it's also called a naive cause.
Now it's also called a Naive because
We've made an assumption that every
word in the document is generated
independently, and this is indeed alive
independently, and this is indeed a Naive
independently, and this is indeed a naive
assumption, because in reality they are
not generated in dependently.
not generated independently.
Once you see some word than other words
Once you see some word and other words
were more likely occur.
will more likely occur.
For example, if you have seen a word a
For example, if you have seen a word like a
text, then that makes categorisation or
text, then it makes categorisation or
text, and then it makes categorization or
clustering more likely to appear.
clustering more likely to appear
And if you have not seen text.
But this assumption allows us to
simplify the problem, and it's actually
quite effective for many text
categorization tasks.
But you should know that this kind of
model doesn't have to make this
assumption.
We could, for example, assume the words
may be dependent on each other, so that
would make it a bigram language model
or trigram language model.
And of course you can even use a
mixture model to model what the
document looks like in each category.
So in nature there will be all using
So in nature they will be all using
Bayes rule to do classification, but
the actual generative model for
documents in each category.
Can vary, and here we just talk about a
very simple case.
Perhaps the simplest case.
So now the question is, how can we make
sure each state I actually represents
sure each state i actually represents
sure each theta i actually represents
category I accurate?
category i accurate?
Now, in clustering we learned this
category I or the word distributions
category i or the word distributions
for cattle I from the data.
for category i from the data.
But in our case what can we do to make
sure this theater I represents indeed
sure this theta i represents indeed
category I?
category i?
If you think about the question and
you're likely come up with the idea of
you're likely to come up with the idea of
using the training data right.
Indeed, in text categorization, we
typically assume that there are
training data available and those are
the documents that are known to have
been generated from which category.
In other words, these are the documents
with known categories assigned, and of
course human experts must do that.
And here you see that T1 represents the
And when you see that T1 represents the
And here you see that T1 represents the
set of documents that are known to have
been generated from category one, and
T2 represents the documents that are
known to have been generated from
category two, etc.
Now if you look at this picture, you
see that the model here is really a
simplified unigram language.
simplified unigram language model.
Model is no longer mixture model.
It is no longer mixture model.
Why becausw?
Why?
We already know which distribution has
Because already know which distribution has
been used to generate which documents.
There's no incident here.
There's no uncertainty here.
There's no mixing of different
categories here.
So the estimation problem of course
would be simplified, but in general you
can imagine what we want to do is to
estimate these probabilities that I
marked here and what are the
probabilities that we have to estimate
in order to do categorization where
there are two kinds.
So one is the prior.
The probability of CDI and this
The probability of theta i and this
indicates how popular each category is
or how likely would have observed the
or how likely we would have observed the
document in that category.
The other kind is the water
The other kind is word
distributions and we want to know what
words have high probabilities for each
category.
So the idea then is to just use the
observed training data to estimate
these two probabilities.
And in general we can do this
separately for different categories.
That's just becausw.
That's just because
These documents are known to be
these documents are known to be
generated from a specific category, so
once we know that it's in some sense
irrelevant what other characters we
irrelevant what other categories we
also dealing with.
are also dealing with.
So now this is statistical estimation
problem.
We have observed some data from some
model and we want to guess the
parameters of this model.
We want to take our best guess of the
parameters.
And this is the problem that you have
seen.
Also several times in this course.
Now, if you haven't thought about that
this problem, having the same naive
this problem, haven't seen  naive
Bayes classifier, it would be very
useful for you to pause the video for a
moment and to think about how to solve
this problem.
So let me state the problem again, so
let's just think about category One.
We know there is one word distribution
that has been used to generate
documents.
And we generated each word in the
document independently and we know that
we have observed set of N sub one
we have observed the set of N sub one
documents in the set of T1.
These documents have been all generated
from category one, namely have been all
generally using this same word
generated using this same word
distribution.
Now the question is what will be your
guess or estimate of the probability of
each word in this distribution an what
each word in this distribution and what
will be your guess of the prior
probability of this category?
Of course, this signal probability
Of course, this second probability
depends on how likely that you will see
documents in other categories.
Right, so think for a moment that how
do you use all these training data,
including all these documents that are
known to be in these categories.
known to be in these K categories.
To estimate all these parameters.
Now if you spend some time to think
about this and it would help you
understand the following a few slides.
understand the following few slides.
So to spend some time to make sure that
So do spend some time to make sure that
you can try to solve this problem or do
your best to solve the problem
yourself.
If you have thought about it and then
Now, if you have thought about it and then
you will realize the following tuition.
you will realize the following intuition.
First, what's the basis for estimating
the prior or the probability of each
category?
Well, this has to do with whether you
have observed a lot of documents from
that category.
Intuitively, if you have seen a lot of
documents in sports and very few in
medical science, then your guess is
that the probability of sports category
is larger or your prior.
is larger or your prior on
The category would be larger.
the category would be larger.
And what about the basis for estimating
the probability of world in each
the probability of word in each
category?
Well, the same and you'll be just
assuming that words that are observed
frequently in the documents that are
known to be generated from a category.
We likely have higher probability, and
will likely have higher probability, and
it's just the maximum regulates made
that's just the maximum regulates made
that's just the maximum likelihood estimator
indeed, and that's what we could do.
So to estimate the probability of each
category.
And to answer the question which
category is most popular, then we can
simply normalize the count of documents
in each category.
So here you see in sub I denotes the
So here you see n sub I denotes the
number of documents in each category.
And we simply just normalize this count
to make this a probability.
In other words, we make this
probability proportional to the size of
training data set in each category.
training dataset in each category.
That's the size of the set T sub I.
That's the size of the set T sub i.
Now, what about the word distribution?
We do the same again.
Well, we do the same again.
This time we can do this for each
category.
So let's say considering category I or
So let's say we are considering category I or
Theta.
Theta
Theta i
Theta
I.
So which word has higher probability?
Well, we simply count the world
Well, we simply count the word
occurrences in the documents that are
known to be generated from CDI.
known to be generated from theta i.
And then we put together all the
accounts of the same word in this set.
all the counts of the same word in this set.
And then we just normalize these counts
to make this distribution of all the
words make all the probabilities of all
these words something one.
these words sum to one.
So in this case you can see this is a
proportional to the count of the world
proportional to the count of the word
in the collection of training
documents.
T sub I and that's denoted by C of W&T
T sub I and that's denoted by C of w and T
sub I.
Now you may notice that we often write
down.
down
A probability estimate in the form of
a probability estimate in the form of
being proportional to certain number,
and this is often sufficient.
Becausw we have some constraints on
these distributions and so the
normalizer is dictated by the
constraint.
So in this case it will be useful for
you to think about what are the
constraints on these two kinds of
probabilities.
So once you figure out the answer to
this question and you will know how to
normalize, this counts and so this is a
good exercise too.
good exercise to
Work on if it's not obvious to you.
Work on it if it's not obvious to you.
work on it if it's not obvious to you.
There is another issue in Life Base
There is another issue in Naive Bayes
which is a smoothing.
In fact the smoothing is a general
problem in all the estimate of language
models an this has to do with what
models and this has to do with what
would happen if you have observed a
small amount of data.
So smoothing the important technique to
So smoothing is the important technique to
address data sparseness.
In our case the training data set can
be small and one data set is small.
When we use maximum likelihood
estimator we often face the problem of
zero probability.
That means if the event is not
observed.
Then the estimated probability would be
0 in this case if we have not seen a
word in the training documents for,
let's say, category I, then our
estimated would be 0 for the
estimate would be 0 for the
probability of this world in this
probability of this word in this
category.
And this is generally not accurate.
So we have to do smoothing to make sure
it's not zero probability.
The other reason for smoothing is that
this is a way to bring prior knowledge,
and this is also generally true for a
lot of situations of smoothing.
When the data set is small, we tend to
rely on some prior knowledge to.
rely on some prior knowledge to
To solve the problem.
to solve the problem.
So in this case I'll prior knowledge is
So in this case our prior knowledge is
So in this case our prior knowledge
says that the number words should have
says that no words should have
zero probability, so smoothing allows
us to inject the supplier to make sure
us to inject this prior to make sure
that no water has a zero probability.
that no word has a zero probability.
There is also a third reason, which is
sometimes not very obvious, but will
sometimes not very obvious, but we'll
explain that in a moment and that is to
help achieve discriminating waiting of
help achieve discriminative waiting of
help achieve discriminative weighting of
terms.
And this is also called IDF weighting
inverse document frequency.
inverse document frequency weighting
Waiting that you have seen in mining
that you have seen in mining
world relations.
word relations.
So how do we do smoothing?
Well in general we added pseudocounts
Well in general we added pseudo counts
to these events.
Will make sure that no event has zero
We'll make sure that no event has zero
count.
So one possible way of smoothing the
probability of category is to simply
add small nonnegative constant Delta to
the account.
the count.
We pretend that every category has
actually some extra number of documents
represented by Delta.
An in the denominator.
And in the denominator
We also add K multiplied by Delta
we also add K multiplied by Delta
'cause we want the probability to sum
because we want the probability to sum
to one.
So in total we've added Delta K Times
because we have K categories.
Therefore in the some we have to also
Therefore in the sum we have to also
add K multiplied by Delta as a total
pseudocounts that we added to the
pseudo counts that we added to the
pseudo counts that we add to the
estimate.
Now it's interesting to think about the
influence of that.
influence of delta.
Obvious Delta is a smoothing parameter
here, meaning that the larger data is
here, meaning that the larger delta is
and the most we will do smoothing and
and the more we will do smoothing and
that means we're more rely on pseudo
that means we'll more rely on pseudo
counts and we might indeed ignore the
actual counts if data is set to
actual counts if delta is set to
Infinity.
Imagine what would happen if there are
Imagine what would happen if delta
approaches positive Infinity?
Well, we're going to say every word has
inflated amount of sorry, not every
infinity amount of sorry, not every
word every category has.
Invented the amount of documents, and
inifinite amount of documents, and
infinite amount of documents, and
infinity amount of documents, and
then there's no distinction between
them, so it becomes just a uniform.
What is Delta is zero?
What if Delta is zero?
Well we just go back to the original
estimate based on the observed training
data to estimate the probability of
each category.
Now we can do the same for the word
distribution, but in this case we
sometimes we find it useful to use a
non uniform pseudo counts for the
non-uniform pseudo counts for the
words.
So here you wheels add pseudocounts to
So here you see we'll add pseudocounts to
each word and that's mu multiplied by
each word and that's miu multiplied by
each word and that's mu multiplied by
the probability of the world given by a
background language model.
Submit
Theta sub b
Now that background model in general
can be estimated by using a large
collection of test, or in this case we
collection of text, or in this case we
can use the whole set of all the
training data to estimate this
background language model.
But if we don't have to use this one,
we can use larger text data that are
available from somewhere else.
Now if we use such a background
language model to add pseudocounts, we
find that some words will receive more
pseudocounts.
So what are those words where those are
So what are those words? Well those are
the common words?
the common words.
Because they get higher probability by
the background language model through
the background language model so
the pseudocounts added for such words
would be higher rail words on the other
would be higher, rare words on the other
end will have smaller pseudocounts.
hand will have smaller pseudocounts.
Now, this addition of background model
would cause nonuniform smoothing of
this word.
this word
Distributions were going to bring the
distributions were going to bring the
distributions we aregoing to bring the
distributions we are going to bring the
probability of those common words, or
to a higher level because of the
background model.
Now this helps make the difference.
Now this helps make the difference
Of the probability of such words
of the probability of such words
smaller across categories?
smaller across categories.
Be cause every category has some help
Because every category has some help
from their background for words, the
from their background for words, like the,
from their background for words, like the, a
which have high probabilities.
Therefore it's no longer so important
that each category has documents that
contain such a lot of occurrences of
such word, or the estimate is more
influenced by the background model and
the consequences that when we do
categorization, such words tend not to
influence the decision that much as
words that have small probabilities.
From the background language model,
those words don't get some help from
the background language model, so the
difference would be primarily because
of the differences of the occurrences
in the training documents in different
categories.
You also see another smoothing
parameter mu here, which controls the
amount of smoothing, just like Adela
amount of smoothing, just like delta
does for the other probability.
An you can easy to understand why we
And you can easy to understand why we
And you can easily understand why we
add new to the denominator because that
add miu to the denominator because that
add mu to the denominator because that
represents the sum of all the pseudo
counts that we add for all the words.
So MU is also negative concern and it's
So MU is also non-negative constant and it's
So mu is also non-negative constant and it's
empirically set to control smoothing.
There are some interesting special
cases to think about as well.
First, let's think about the Wham you
First, let's think about when mu
approaches Infinity.
What would happen?
Or in this case, the estimate will
approach to the background language
model will tend to the background
language model, so we would bring every
word distribution to the same
background language model.
An that essentially removes the
And that essentially removes the
difference between these categories.
Obviously we don't want to do that.
The other special cases we think about
the background model an suppose we
actually set the two uniform
distribution and let's say one over the
size of the vocabulary.
So each word has the same probability.
Then this smoothing formula is going to
be very similar to the one on the top.
When we add Delta because we're going
to add a constant pseudo common to
to add a constant pseudo count to
every word.
So in general, in life based
So in general, in naiyes bayes
categorization we have to do such
smoothing and then look no.
smoothing and then
smoothing and
Once we have these probabilities, then
once we have these probabilities, then
we can compute the score for each
category for a document and then choose
the category with the highest score.
the category with the highest score
As we discussed earlier.
as we discussed earlier.
Now it's useful to further understand.
Now it's useful to further understand
Whether the naive Bayes scoring
whether the naive Bayes scoring
function actually makes sense, so to
understand that.
And also to understand why adding a
background model will actually achieve
background language model will actually achieve
the effect of idea of waiting and to
the effect of idea of IDF weighting and to
penalize common words.
Right, so it's suppose we have just two
categories and we're going to score
based on there.
based on their
Ratio of probability, so this is.
Ratio of probability, so this is
Ann
Let's say this is our scoring function
let's say this is our scoring function
for two categories.
So this is a score of a document for.
So this is a score of a document for
These two categories.
these two categories.
And we're going to score based on this
probability ratio.
So if the ratio is larger.
So if the ratio is larger
Is larger then it means it's more
then it means it's more
likely to be in category one, so the
larger the score is, the more likely
the document is in category One.
So use by use by using base roran we
So use by use by using bayes rule we
So by using bayes rule we
can write down this ratio as follows an
can write down this ratio as follows and
you have seen this before.
Now, we generally take logarithm of
this ratio and to avoid small
probabilities, and this would then give
us this formula in the second line.
And here we see something really
interesting, because this is our
scoring function for deciding between
the two categories.
And if you look at this function, will
And if you look at this function, we'll
see it has several parts.
The first part here is actually log of
prior probability ratio and so this is
the category bias.
I it doesn't really depend on the
So it doesn't really depend on the
document, it just says which category
is more likely and then would.
We would then favor this category
slightly.
So the second part has a son of all the
So the second part has a sum of all the
words.
Right, so at these are the words that
Right, so these are the words that
Right, so these are the words that are
observed in the document, but in
general we can consider all the words
in the vocabulary.
So here we're going to collect the
So here we're going to collect
evidence about which category is more
likely.
So inside the sum you can see there is
product of two things.
The first is account of the world.
The first is count of the word.
And this count of the world serves as a
And this count of the word serves as a
feature and to represent the document.
And this is what we can collect from
document.
The second part is the weight of this
feature.
Here it's the weight on each word at
Here it's the weight on each word and
this weight.
Tells us.
To what extent the observing this world
To what extent observing this world
To what extent observing this word
helps?
helps
Contributing our decision to put this
Contributing to our decision to put this
contributing to our decision to put this
document in Category One.
I remember the higher the scoring
function is more likely it's in
category one.
Now if you look at this ratio basically
or sorry this wait.
or sorry this weight
It's basically based on the ratio of
the probability of the world from of
the probability of the word from of
the two distributions.
Essentially what comparing the
Essentially when comparing the
Essentially we are comparing the
probability of the world from the two
probability of the word from the two
distributions and if it's higher
according to Theta one, then according
according to theta one, then according
to CR2 then this way it would be
to theta 2 then this way it would be
to theta 2 then this weight would be
positive and therefore it means when we
observe such a word.
Will say that it's more likely to be
We'll say that it's more likely to be
from category One, and the more we
observe such a world, the more likely
observe such a word, the more likely
the document will be classified as set
the document will be classified as theta
up one.
one.
If, on the other hand, the probability
of the world from theater one is
of the world from theta one is
of the word from theta one is
smaller than the probability of the
water from Cedar 2, then you can see
word from theta 2, then you can see
this wait is negative.
this weight is negative.
Therefore this is the neck negative
Therefore this is the negative
evidence for supporting category one.
That means the more we observe such a
world, the more likely the document is
word, the more likely the document is
actually from CR2.
actually from theta 2.
So this formula Now makes a lot of
So this formula now makes a lot of
sense, so we're going to aggregate all
the evidence from the document.
We take a sum over all the words we can
call this the features.
That we collect from the document that
would help us make the decision and
that each feature has a weight that
tells us how.
tells us how
There's this feature support category
does this feature support category
one or support that support the
category two, and this is estimated as
the log of probability ratio.
Here in naive Bayes.
And then finally we have this constant
of bias here, so that formula actually
is a formula that can be generalized to
accommodate a more features.
accommodate more features.
And that's why I've introduced to some
And that's why I've introduced some
other symbols here.
So introduce the beta zero to denote
the bias and FI to denote each feature,
the bias and Fi to denote each feature,
and then beta subdivide, denoted.
and then beta sub i, denoted.
and then beta sub i, to denote
Wait on which feature.
the weight on which feature.
Now if we do this generalization, what
we see is that in general we can
represent the document by feature
vector FFI here.
vector F, FI here.
Of course in this case FYI is the count
Of course in this case FI is the count
of a word, but in general we can put
any features that we think are relevant
for categorisation.
for categorization.
For example document lens or in font
For example document length or the font size
size or counts of other patterns in the
or counts of other patterns in the
document.
And then I was scoring function can be
And then ouurscoring function can be
And then ourmscoring function can be
And then our scoring function can be
defined as a sum of constant beta zero
and some of the feature weights over
and sum of the feature weights over
all the features.
So if HF sub I is official value then
So if HF sub I is the feature value then
So if HF sub I is a feature value then
we multiplied value by the
we multiply value by the
corresponding wait better some I and we
corresponding weight beta sub i and we
just take some and this is the
just take sum and this is the
just take sum and this is to
aggregate.
All evidence that we can collect from
all these features.
all these features. And of course there are parameter
all these features. And of course there are parameters here. So what are the parameters? Well
Anne, with a proper the setting of the
And with a proper the setting of the
These betas are the
These betas are the weights, and with proper settings of
These betas are the weights, and with appropriate settings of
weights then we can expect the such a
scoring function to work well to
classify documents.
I just like in the case of Naive Bayes
Just like in the case of Naive Bayes
we can clearly see naive Bayes
classifier is a special case of this
general classifier.
Actually, this general form is very
close to classifier called logistical
close to a classifier called logistical
regression, and this is actually one of
those conditional approaches or
these conditional approaches or
those conditional approaches or
discriminative approaches to
classification.
An within the talk.
And we are going to talk
More about such approaches later, but
more about such approaches later, but
here I want you to know that there's a
strong connection close connection
between the two kinds of approaches,
and this slide shows how naive Bayes
classifier can be connected to a
logistical regression.
logistic regression.
And you can also see that in
discriminative classifiers that tend to
use a more general form on the bottom,
we can accommodate more features.
we can accommodate more features
To solve the problem.
to solve the problem.
This lecture is about probabilistic
This lecture is about the probabilistic
topic models for topic mining an
topic models for topic mining and
analysis.
In this lecture we could continue
In this lecture we're going to continue
talking about the top mining analysis.
talking about the top mining and analysis.
We're going to introduce probabilistic
topic models.
So this is a slide that you have seen
earlier where we discussed the problems
with using a term as a topic.
So to solve these problems intuitively
we need to use more words to describe
the topic and this would address the
problem of lack of expressive power.
When we have more words that we can use
to describe the topic of weakened,
to describe the topic of a topic, we can
to describe the topic, we can
escriba complicated topics to address
describe complicated topics to address
describe complicated topics, to address
the second problem, we need to
introduce weights on words.
introduce weights of words.
This would allow you to distinguish
subtle differences in topics and to
introduce semantically related words in
the fuzzy manner.
Finally, to solve the problem of word
ambiguity, we need to split an
ambiguous world so that we can
ambiguous word so that we can
disambiguate its topic.
It turns out that the all these can be
It turns out that all these can be
done by using a probabilistic topic
model, and that's why we're going to
spend a lot of lectures to talk about
this topic.
So the basic idea here is an improved
So the basic idea here is improved
representation of topic as a word
distribution.
So what you see now is the old
representation, where we represent each
topic with just one word or one term or
one phrase.
But now we're going to use a word
distribution to describe the topic.
So here you see that for sports, we're
going to use a word distribution over
theoretical speaking.
theoretical speaking
All the words in our vocabulary.
all the words in our vocabulary.
So for example, the high probability
words here are sports game, basketball,
words here are sports, game, basketball,
football, play store, etc.
football, play star, etc.
football, play, star, etc.
These are sports related terms and of
These are sports-related terms and of
course it would also give a non zero
probability to some other word like
probability to some other words like
travel which might be related to
"""travel"" which might be related to"
sports.
But in general not so much related to
the topic.
In general, we can imagine a non zero
probability for all the words and some
words that are not relevant would have
very very small probabilities and these
probabilities which sum to one.
probabilities will sum to one.
So that it forms a distribution of all
the words.
Now intuitively, this distribution
represents a topic in that if we sample
words from the distribution, we tend to
see words that already do sports.
You can also see it as a very special
case if the probability mass is
concentrated in tier of just one word.
concentrated entire of just one word.
Let's sports, and this basically
degenerates to the simple
representation of topical with just one
representation of topic with just one
word.
But as a distribution, this topic
representation can in general involve
many words to describe the topic and
can model subtle differences in
semantics of the topic.
Similarly, we can model travel and
science with their respective
distributions.
So in the distribution for travel we
see top words like Attraction, Trip,
"see top words like ""attraction, trip,"
flight, hotel etc.
"flight, hotel etc."""
Whereas in science with the Scientist
"Whereas in science, we see ""scientist,"
Spaceship Telescope or Genomics and new
"spaceship, telescope or genomics"" and new"
science related terms, now, that
science-related terms, now, that
doesn't mean sports really determines
doesn't mean sports-related terms
where necessary have zero probabilities
necessary have zero probabilities
for science in general, we can imagine
all these words.
We have non zero probabilities, it's
just that for a particular topic of
some words we have very very small
probabilities.
Now you can also see there are some
words that are shared by these topics.
Well, when I say share, that just means
Well, when I say shared, that just means
even with some probability threshold
you can still see one word to occur in
multiple topics.
In this case I marked them in black so
you can see travel.
"you can see ""travel"","
For example a curd in all the three
For example occured in all the three
for example, occured in all the three
topics here, but with different
probabilities.
It has the highest probability for the
travel topic .5.
travel topic 0.05
travel topic 0.05.
But with much smaller probabilities for
sports and science, which makes sense.
And similarly you can see Star also
"And similarly you can see ""star"" also"
occurred in sports and science with
reasonably high probabilities, because
they might be actually related to the
two topics.
So with this recommendation is
So with this representation is
So with this representation it
addresses the three problems that
mentioned earlier.
First, it now uses multiple words that
describe topic, so it allows us to
describe fairly complicated topics.
Second, it assigns weights to terms, so
now we can model several differences of
semantics and bringing related words
semantics and you can bring in related words
together to model topic third.
together to model topic. Third,
'cause we have probabilities for the
because we have probabilities for the
same world in different topics.
same word in different topics.
We can disambiguate the sense of world
We can disambiguate the sense of word
in the text to decode its underlying
topic, so we address all these three
problems with this new way of
representing a topic.
So now, of course, our problem
definition has been refined just
slightly.
The slide is very similar to what you
have seen before, except that we have
added refinement for.
added refinement for
What the topic is.
what the topic is.
So now each topic is world
So now each topic is word
distribution.
And for each word distribution, we know
that all the probabilities should
someone over all the words in the
somed one over all the words in the
sum to one over all the words in the
vocabulary.
So you see a constraint here and we
still have another constraint on the
topic coverage, namely pies.
topic coverage, namely pis.
So all the piles of IGS must sum to one
So all the pis of IGS must sum to one
So all the pis of ij's must sum to one
for the same document.
So how do we solve this problem?
Well, let's look at this problem as a
computation problem now.
So we clearly specify the input and
output an illustrated here on this
output as illustrated here on this
side.
The input, of course is our tax data
The input, of course is our text data
sees the collection, but we also
C is the collection, but we also
generally assume we know the number of
topics K or we hypothesize a number and
then try to mine K topics, even though
we don't know the exact topics that
exist in the collection and these
vocabulary set.
As a set of words that determines what
units would be treated as.
units would be treated as
The basic units for analysis.
the basic units for analysis.
In most cases, when use words as the
In most cases, we use words as the
basis.
For analysis, and that means each word
is a unit.
Now the output would consist of's first
Now the output would consist of as first
set of topics represented by Theta
a set of topics represented by Theta
eyes.
i's
Each CI is a world distribution.
Each theta_i is a word distribution.
Ann
And
We also want to note coverage of topics
We also want to know the coverage of topics
in each document so that that's the
same pie IGS that we have seen before.
same pi_ij's that we have seen before.
So given a set of text data, we would
like to compute all these distributions
an all these coverages, as you have
and all these coverages, as you have
and all these coverages as you have
seen on this slide.
Now of course, there may be many
different ways of solving this problem.
Indeed, you can write a heuristic
program to solve this problem, But here
program to solve this problem, but here
willing to introduce a general way of
we're going to introduce a general way of
solving this problem called a
solving this problem called
generative model, and this is in fact
very general idea, and it's a
principled way of using statistical
principle way of using statistical
modeling to solve text mining problems,
and here I dim the picture that you
have seen before in order to show the
generation process.
So the idea of this approach is
actually to 1st design A model for our
actually to 1st design a model for our
data.
So we design A probabilistic model to
So we design a probabilistic model to
model how the data are generated.
Of course this is based on our
assumption.
The actual data on necessary generating
The actual data aren't necessary generating
The actual data aren't necessary generated
this way, so that would give us a
probability distribution of the data
that you are seeing on this slide given
a particular model and parameters that
are denoted by Lambda.
So this capital M law actually consists
So this capital lambda actually consists
of all the parameters that were
of all the parameters that we're
interested in his parameters.
interested in. And these parameters
In general, we control the behavior of
in general, we control the behavior of
in general, will control the behavior of
the probabilistic model, meaning that
if you set these parameters for
different values.
different values,
And it will give some data points
it will give some data points
higher probabilities than others.
Now in this case, of course, for our
tax mining problem, or more precisely
topic mining problem, we have the
following plans.
following parameters.
First we have seen eyes.
First we have theta_i's
First, we have theta_i's
Each is word distribution and then we
Each is a word distribution and then we
have a set of pies for each document.
have a set of pi's for each document.
And since we have end documents so we
And since we have N documents so we
have N sets of pies.
have N sets of pis.
And each set all the pile Pi values
And each set of the pi values
will sum to one.
So this is to say that we first pretend
we already have these word
distributions and coverage numbers, and
then we're going to see how we can
generate data by using such
distributions.
So how do we model the data in this
way?
And we assume that data are actually
samples drawn from such a model that
depends on these parameters.
Now one interesting question here is to
think about how many parameters are
there in total.
Now obviously we can already see N * K
parameters for pies.
parameters for pi's.
We also see KC dies, but each seed I is
We also see K theta_i's, but each theta_i is
actually a set of probability values.
Right, it's a distribution over words.
Right? it's a distribution over words.
Right? It's a distribution over words.
So I leave this as exercise for you to
figure out exactly how many parameters
there are here.
Now, once we set up with a model, then
we can fit the model to our data,
meaning that we can estimate the
parameters or in further parameters
parameters or infer the parameters
based on the data.
In other words, we would like to adjust
these parameter values until we give
our data set the maximum probability.
I just say that depending on the
parameter values, some data points will
have higher probabilities than others.
What we're interested in here is what
parameter values will give our data set
the highest probability.
So I also illustrates the problem with
So I also illustrate the problem with
the picture that you see here on the X
the picture that you see here. On the X
axis.
axis,
I just illustrate the Lambda the
I just illustrate the Lambda, the
parameters as one dimensional variable.
It's oversimplification obviously, but
in surface is to show the idea and the
it's suffice is to show the idea and the
it suffices is to show the idea and the
Y axis shows the probability of the
data.
data
Observed this probability obviously
observed this probability obviously
observe. This probability obviously
depends on the setting of Lambda, so
that's why it varies as you change the
value of Lambda.
What we're interested in here is
refined the Lambda star that would
find the Lambda star that would
to find the Lambda star that would
maximize the probability of the
observed data.
So this would be then our estimate of
the parameters and these parameters.
the parameters and these parameters
Note that are precisely what we hope to
note that are precisely what we hope to
discover from text data, so would treat
these parameters as actually the
outcome or the output of the data
mining algorithm.
So this is a general idea of using a
generative model for text mining.
First we design A model with some
First we design a model with some
First, we design a model with some
parameters that we are interested in,
and then we model the data.
We adjust the parameters to fit the
data as well as we can.
After we have fitted data then we will
recover some parameter values will get
this specific parameter values and
those would be the output of the
algorithm and we treat those as.
algorithm and we treat those as
Actually the discovered knowledge from
actually the discovered knowledge from
test data.
text data.
By varying the model, of course we can
discover different knowledge.
So to summarize, we introduced a new
way of representing topic, namely
way of representing a topic, namely
represented as world distribution, and
represented as word distribution, and
this has advantage of using multiple
words to describe a complicated topic.
It also allows us to assign weights on
words so we can model Suttles
words so we can model subtle
variations of semantics.
We talked about the task of topic
mining analysis when we define a topic
mining and analysis when we define a topic
as a distribution, so the input is a
collection of tax articles.
collection of text articles.
The number of topics and vocabulary set
in the output is a set of topics.
and the output is a set of topics.
Each is world distribution.
Each is word distribution.
And also the coverage of all the topics
in each document and these are formally
represented by size and piyes an we
represented by theta_i's and pi_i's an we
represented by theta_i's and pi_i's and we
have two constraints here.
have two constraints here
For these parameters, the first is the
for these parameters. The first is the
constraints on the world distributions.
constraints on the word distributions.
constraint on the word distributions.
In each world distribution of the
In each world distribution, the
probabilities on all the words must sum
to one over all the words in the
vocabulary.
The second constraint is on the topic
coverage in each document.
A document is not allowed to cover.
A document is not allowed to cover
Topical outside the set of topics that
A topical outside the set of topics that
a topic outside the set of topics that
we are discovering.
So the coverage of each of these K
topics would sum to one for a document.
We also introduce the general idea of
using a generated model for text mining
using a generative model for text mining
and the idea here is to 1st design A
and the idea here is to first design a
model to model the generation of data.
We simply assume that there are
We simply assume that they are
generating this way and inside the
generated this way and inside the
model will embed some parameters that
model we embed some parameters that
model, we embed some parameters that
were interested in denoted by Lambda.
And then we can infer the most likely
parameter values than the start giving
parameter values lambda* giving
parameter values lambda star giving
parameter values lambda star given
a particular data set, and we can then
take the Lambda star as knowledge
discovered from the text for our
problem, and we can adjust the design
of the model and parameters with this
car discover various kinds of knowledge
discover various kinds of knowledge
from text.
As you will see later.
As you will see later
In the other lectures.
in the other lectures.
This lecture is the first one about the
text clustering.
This is very important that technique
for doing topic mining an analysis.
In particular, in this lecture organ to
start with some basic questions about
the classroom.
the clustering: What is text clustering and why text clustering?
the clustering: What is text clustering and why we are interested in text clustering?
In the following lectures, we're going
to talk about how to do text clustering
and how to evaluate the clustering
results.
So what is text classroom?
"So what is text
Clustering actually is a very general
technique for data mining.
As you might have learned in some other
causes.
courses.
The idea is to discover natural
structures in the data.
In other words, we want to group
similar objects together.
In our case, these objects are of
course texture objects.
For example, they can be documents,
turns, passages, sentences or websites.
And then I'll go little group similar
And then our goal is to group similar
texture objects together.
So let's see a example here.
You don't really see textile objects,
You don't really see text objects,
but I just use some shapes to denote
objects that can be grouped together.
Now, if I ask you what are some natural
structures or natural groups while you,
structures or natural groups well you,
if you look at it, you might agree that
we can group these objects based on
shapes or their locations on this 2
dimensional space.
So we got the three clusters in this
case.
And then may not be so much
disagreement about these three
clusters, but it really depends on the
perspective to look at the objects.
Maybe some of you have also seen it in
a different way, so we might get
different clusters.
Annual see another example about this
And you will see another example about this
ambiguity more clearly, but the main
point here is the problem is actually
not so well defined.
And the problem lies in how to define
similarity.
What do you mean by similar objects?
Now this problem.
Has to be clearly defined in order to
have well defined clustering problem.
And the problem is in general that any
two objects can be seen that depending
two objects can be similar that depending
on how you look at them.
So for example.
Let's look at the two words like car
and horse.
I saw all the two words similar well.
So are the two words similar
It depends on how you look at it.
If you look at the physical.
Physical properties of Caran horse.
Physical properties of car and horse.
They are very different.
But if you look at the them
functionally, a car in the horse can
both be transportation tool, so in that
sense they may be similar.
So as you can see, it really depends on
our perspective to look at the objects
and so in order to make the clustering
problem well defined, a user must
define the perspective.
For assessing similarity.
And we call this perspective the
clustering bias.
And when you define a clustering
problem, it's important to specify your
perspective for similarity or for
defining the similarity that would be
used to group similar objects 'cause
otherwise.
Similarity is not well defined.
An one can have different ways to group
objects.
So let's look at a concrete example.
Here you are seeing some objects or
some shapes that are very similar to
what you have seen on the 1st slide.
But if I ask you to group these objects
again, you might.
Might.
Feel there's more uncertainty here than
on the previous slide.
For example.
You might think, well, we can still
group by ships, so that would give us
group by shapes, so that would give us
cluster that looks like this.
However, you might also feel that.
Maybe the objects can be grouped based
on the sizes, so that would give us a
different way to cluster the data.
If we look at the size and look at the
similarity in size.
So as you can see clearly here,
depending on the perspective will get
different clustering results, so that
also clearly tells us that in order to
evaluate the clustering result we must
use perspective.
Without perspective, it's very hard to
define what is the best clustering
result.
So there are many examples of text
clustering.
Set up.
And so, for example, we can cluster
documents in the heart Texas
documents in the whole text
collection.
So in this case documents are the units
to be clustered.
We may be able to cluster terms in this
case.
Terms are objects.
Anna Cluster of terms can be used to
And Cluster of terms can be used to
define the concept or theme or topic.
In fact, the topic models that you have
seen some previous lectures.
Can give you cluster of terms in some
sense.
If you take the terms with high
probabilities from world distribution.
Another example is to just a cluster
any texts segments, for example
passages, sentences or any segments
that you can extract the from a large
attacks objects.
text objects.
For example, we might extract all the
text segments about the topic, let's
say by using a topic model.
Now, once we've got those text objects,
then we can cluster.
The segments that we've got to discover
interesting clusters that might also
represent the subtopics.
So this is a case of combining text
clustering with some other techniques,
and in general you will see a lot of
text mining algorithms can be actually
combined in a flexible way to achieve.
The goal of doing more sophisticated
mining and analysis of text data.
We can also cluster fairly large tax
We can also cluster fairly large text
law gets, and by that I just mean text
objects may contain a lot of documents.
So for example we might cluster
websites.
Each website is actually composed of
multiple documents.
Similarly, we can also cluster articles
written by the same author, for
example.
So we can treat all the articles
published by author as one unit for
Clustering.
In this way, we might group authors
together based on whether they are
published papers or similar.
Furthermore, tax clusters can also be
Furthermore, text clusters can also be
further clustered.
Regenerate the hierarchy that that's
'cause we can in general, cluster any
text object at different levels.
So more generally, why is text
clustering interesting?
Well, it's been 'cause it's a very
Well, it's brcause it's a very
useful technique for text mining,
particularly exploratory text analysis.
An so a typical scenario is that you
And so a typical scenario is that you
are getting a lot of text data.
Let's say all the email messages from
customers in some time period, or all
the literature, articles, etc.
And then you hope to get the sense
about what are the overall content of
the collection.
So, for example, you might be
interested in getting.
A sense about the major topics or what
are some typical or representative
document in the collection?
Anne clustering help us achieve this
And clustering help us achieve this
goal.
We sometimes also want to link similar
text objects together and these.
These objects might be duplicated
content for example, and in that case
such a technique can help us remove
redundancy, removing duplicated
documents.
Sometimes they are about the same topic
and by linking them together we can
have more complete coverage of the
topic.
We may also use tax the clustering to
We may also use text the clustering to
create a structure on the text data,
and sometimes we can create a hierarchy
of structures and this is very useful
for browsing.
We may also use text clustering to
induce additional features to represent
text data when we cluster documents
together, we can treat each cluster as
a feature and then we can say when a
document is in this cluster and then
the feature value would be one and if a
document is not in this cluster, then
the future value is zero and this helps
provide additional discrimination that
might be used for texture
classification as we will discuss
later.
So there are in general many
applications of tax class ring any.
applications of text clustering any.
I just saw it with two very specific
ones.
One is to cluster search without for
One is to cluster search results for
example an.
example and
You can imagine a search engine can
cluster the search results so that user
can see overall structure of those.
Results returned for a query.
And when the query is ambiguous, this
is particularly useful.
Becausw clusters likely represent
different senses of ambiguous world.
different senses of ambiguous word.
Another application is to understand
the major complaints from customers
based on their emails, right?
So in this case we can cluster email
messages and then find the major
clusters.
From there.
We can understand what other major
complaints about.
This lecture is a continued discussion
of.
Discriminative classifiers for tax for
Discriminative classifiers for text
categorization.
So in this lecture will introduce yet
another discriminative classifier
called a support vector machine or VM,
which is a very popular classification
method, and there has been also shown
to be effective for text
categorization.
So to introduce this classifier, let's
also think about the simple case of two
categories and we have two public
categories, season one and Season 2
here.
An we want to classify documents into
these two categories and we're going to
represent again a document by a feature
vector X here.
Now the idea of this classifier is do
design.
Also a linear separator.
Here that you see and it's very similar
to what you have seen or just for
aggression.
logistic regression.
And we're going to also say that if the
sign of this function value is
positive, then we're going to say the
object is in Category 1.
Otherwise, we're going to say it's in
Category 2, so that makes 0 value.
The decision boundary between two
categories.
So in general in high dimensional space
such a zero point corresponds to a
hyperplane.
I show you a simple case of two
dimensional space with just X1 and X2.
In this case this corresponds to a line
that you can see here.
So this is.
Align defined by just three parameters
A line defined by just three parameters
here better 0 better one better too.
"here beta0
Now this line.
Is the heading in this direction, so it
shows that as we increase X, one X2
shows that as we increase X1, X2
will also increase.
So know that beta one and better to
So know that beta1 and beta2
have different signs or one is negative
and there is positive.
I so let's just assume that beta one is
negative and bitter two is positive.
negative and beta two is positive.
Now it's interesting to examine then
the data instances on the two sides of
this line, so here that there are
incidences are visualized as circles
for one class and diamonds for the
other class.
Now one question is to take a point
like this one and to ask the question
what's the value of this expression or
this classifier for this data point.
So what do you think?
Basically working to evaluate its value
by using this dysfunction.
by using this function.
And as we said, if this value is
positive woman to say this is in
positive we're gonna say this is in
category one, and if it's negative it's
going to be in category Two.
Intuitively, this line separates these
two categories, so we expect the points
on one side would be positive and
points on the other side would be
negative.
Or the question is under the assumption
that I just mentioned, let's examine a
particular point like this one.
So what do you think is the sign of
this expression?
To examine the sign, we can simply look
at this expression.
Here we can compare this with, let's
say, value on the line.
Let's say compare this with this point.
They have identical X one, but then one
has a higher value for its too.
Now let's look at the sign of the
coefficient for X2, where we know this
is a positive.
So what that means is that the F value
for this point should be higher than
the F value for this point on the line.
That means this will be positive,
right?
So we know in general for all the
points on this side, the.
Functions about it would be positive.
And you can also verify all the points
on this side would be negative, and so
this is how this kind of linear
classifier or linear separator can then
separate the points in the two
categories.
So now the natural question is, which
linear separate is the best?
Now I've again she want lying here that
can separate the two classes.
And this line, of course, is determined
by the vector beta, the coefficients,
different coefficient will give us a
different line.
So we could imagine there are other
lines that can do the same job.
So gamma, for example, could give us
another line that can also separate
these instances.
And of course there are also lines that
won't separate them, and those are bad
lines.
But the question is when we have
multiple lines that can separate the
both clauses, which line is the best?
In fact, you can imagine there are many
different ways of choosing the line.
So the logistical regression classifier
that you have seen earlier actually
uses some criteria to determine where
this line should be, and it's a linear
separate as well and uses a conditional
likelihood on the training data to
determine which line is the best.
But in this VM, we're going to look at
another criteria for determining which
lines best and this time the criteria
is more tide to the classification
error.
As you will see.
So the basic idea is to choose the
separator.
To maximize the margin.
So what is the margin?
Well, I choose.
So I've shown some daughter lines here
to indicate the boundaries of those
data points in.
In each class and the margin is simply
the distance between the line, the
separator and the closest points from
each class.
So you can see the margin of this side
is as I've shown here.
And you can also define the margin on
the other side.
And in order for the separate to
maximizing the margin, it has to be
kind of in the middle of the two
boundaries, and you don't want this
separator to be very close to one side.
And then that inducing intuitively
makes a lot of sense.
So this is the basic idea of ecfmg.
We're going to choose a linear
separator to maximize the margin.
Now on this slide I've also changed the
notation so that I'm not going to use
beta.
Didn't know the parameters and, but
instead I'm going to use W, although W
was used to denote the words before.
So don't be confused here.
W here is actually wait set of weights.
And.
So I'm also using locates be to denote
beta zero, the bias constant.
And there are instances do represented
as X.
And I also use the vector form of
multiplication here.
So we see transpose of W vector
multiplied by the feature vector.
So P is a biased constant and W is a
set of weights and with one wait for
each feature we have M features and so
have aim weights and are represented as
a vector.
An similarly the data instance.
Here the text object is represented by
also a feature vector of the same
number of elements.
XI is future value.
For example word count.
I can you can verify when we multiply
these two vectors together, take the
dot product that we get the same form
of the NIA separate as you have seen
before.
It's just a different way of
representing this.
Now I use this way so that it's more
consistent with what notations people
usually use when they talk about SVM.
This way you can.
Better connected the slides with some
other readings you might do.
OK.
So.
When we maximize the margins of
separate, it just means with the
boundary of.
The separate is only determined by a
few data points, and these are the data
points that we call support vectors.
So here are illustrated to support
vectors for one class and two for the
other class.
At this, porters define the margin
basically.
And you can imagine once we know which
are support vectors, then this center
separate line will be determined by
them so.
The other data points actually don't
really matter that much.
And you can see if they you change
other data points, it won't really
affect the margin, so the separate with
the stay the same mainly affected by
the support vector machines.
Sorry it's mainly affected by the
support vectors and that's why it is
called a support vector machine.
OK, so.
The next question is of course, how can
we set it up to optimize the line?
How can we actually find the line?
Or the separator.
Now this is equivalent to finding
values for W&B because they would
determine where exactly the separator
is.
So in the simplest case, the linear
osfm is just a simple optimization
problem.
So again we let's recall that our
classifier is such a linear separator
where we have weights for all the
features and the main goal is to learn
these weights W&B.
And the classifier will say X is in
category one if it's positive.
Otherwise it's going to say it's in the
other category.
So this is our assumption or setup.
So in the linear is UVM, we're going to
then seek these parameter values to
optimize the margins and then the
training error.
The training laid out would be
basically like a in other classifiers
we have a set of training points where
we know the X vector and then we also
the corresponding label, why I?
An here we define why I as two values,
but these two values are not 01 as you
have seen before, but rather negative
one and positive one and their
corresponding to these two categories
as I've shown here.
Now you might wonder why we don't
define them as zero and one, but
instead of having negative 11 and this
is purely for mathematical convenience,
as you will see in a moment.
So the goal of optimization first is to
make sure the labeling on training data
is all correct.
So that just means if Yi, the known
label, for instance XI is one we would
like this classify value to be large.
And here we just choose threshold one
here.
But if you use another threshold, you
can see you can easily affect that
constant into the parameter values B&W
to make the right hand side.
Just one.
Now, if, on the other hand, why I is
negative one that means it's in a
different class then we want this
classifier to give us a very small
value.
In fact a negative value.
And we want this value to be less than
or equal to negative one.
These are the two different instances,
different kinds of cases and how can we
combine them together now.
This is where it's convenient when we
have chosen why I as negative one for
the other category cause it turns out
that we can easily combine the two into
one constraint.
Why I multiplied by the classifier
value must be larger than or equal to
1?
An obviously when?
Why is just one you see.
This is the same as the constraint on
the left hand side.
But when Yi is negative one you also
see a new.
This is equivalent to the other
inequality, so this one actually
captures both constraints in a unified
way, and that's a convenient way of
capturing these constraints.
What's our second goal?
That's true.
Maximizing margin, right?
So we want to ensure the separate can
do well on the training data, but then,
among all the cases where we can
separate the data, we also would like
to choose the separate that has the
largest margin.
Now the margin can be shown to be
related to the magnitude of the
weights.
The sum of squares of all those
weights.
So this to have a small value for this
expression.
It means all the eyes must be small.
So we've just assume that we have a
constraint for the getting the data on
the training set to be classified
correctly.
Now we also have the objective that's
Tide to maximization of margin and this
is simply to maximize sorry to minimize
W transpose multiplied by W and we
often denote this by file W.
So now you can see this is basically
optimization problem, right?
We have some variables to optimize and
these are the weights and B and we have
some constraints.
These are linear constraints and the
objective function is a quadratic
function of the weights.
So this is a quadratic program with
linear constraints and there are
standard algorithms that are available
for solving this problem.
And once we solve, the problem, will
obtain the weights W&B and then this
would give us a well defined the
classifier, so we can then use this
classifier to classify any new texture
objects.
Now the previous formulation did not
allow any error in the classification,
but sometimes the data may not be
linearly separable.
That means they may not look as nice as
you have seen on the previous slide
where align can separate all of them.
And what would happen if we.
Allow some errors.
The principle can stay right, so we
want to minimize the training error,
but try to also maximize the margin.
But in this case we have a soft margin
because the data points may not be a
completely separate bowl.
So it turns out that we can easily
modify it as VM to accommodate this.
So what you see here is very similar to
what you have seen before, but we have
introduced the extra variables.
Cassie I an we in fact will have one
for each data instance and this is
going to model the error that will
allow for each instance.
But the optimization problem will be
very similar.
So specifically, you will see we have
added something to the optimization
problem.
First we have added some.
Some error to the constraint so that
now we allow.
Allow the classifier to make some
mistakes here, so this KCI is allowed
error if we set KCI to 0, then we go
back to the original constraint.
We want every instance we classified
accurately, but if we allow this to be.
Zero, then we allow some errors here.
In fact, the one CI is very large.
The error can be very, very large, so
naturally we don't want this to happen.
So we want to then also minimize this
CI.
So Cassie, I needs to be minimized in
order to control the error.
And so as a result in the objective
function we also add more to the
original 1, which is only an by
basically ensuring that we're going to
not only minimize the weights, but also
minimize the errors as you see here, we
simply take a sum over all the
instances.
Each one has a CI to model the error
allowed for that instance an when we
combine them together, we basically
want to minimize the errors on.
All of them.
Now you see there's a parameter.
See here and that's a constant to
control the tradeoff between minimizing
the errors and maximizing the region of
the margin if C is set to zero, you can
see we go back to the original object
function where we only maximize margin.
And we don't really optimize the
training errors and then see I can be
set to a very large value to make the
constraints easy to satisfy.
That's not very good of course, so see
should be set to a non 0 value and a
positive value.
But when she is settled very, very
large value would see the objective
function will be dominated mostly by
the training errors and so the
optimization of margin will then play a
secondary role.
So if that happens, what would happen?
What would happen is then we will try
to do our best to minimize the training
errors.
But then we're not going to take care
of the margin and that affects the
generalization capacity of the
classifier for future data.
So it's also not good.
So apparently this parameter C has to
be actually set.
Carefully, and this is just like in the
case of nearest neighbor way you need
to optimize the number of neighbors.
Here you need to optimize the C and
this is the general also achievable by
doing cross validation.
Basically you look at the empirical
data to see what values should be set
to in order to optimize the
performance.
Now with this modification in the
problem, is there a quadratic program
with linear constraints, so the
optimization algorithm can be actually
applied to solve this different version
of the program?
Again, once we have obtained the
weights and the bias, then we can have
classified.
That's ready for classifying new
objects.
So that's the basic idea of Sven.
So to summarize, the text
categorisation methods we have
introduced many methods and some are
generative models, some more
discriminative methods, and these tend
to perform similarly when optimized, so
there's still no clear winner, although
each one has its pros and cons, and the
performance might also very different
data sets for different problems.
Ann
One reason is also becausw.
The feature representation is very
critical an so that these methods all
require effective feature
representation and to design effective
feature set that we need domain
knowledge and humans definitely play
important role here.
Although there are new machine learning
methods like representation learning
that can help with learning features.
An another common scene is that they
might be.
Be performing similarly on the data set
but with different mistakes and so
their performance might be similar, but
then the mistakes that make might be
different, so that means it's useful to
compare different methods for
particular problem and then maybe
combine multiple methods 'cause this
can improve the robustness and they
want to make the same mistakes so.
And symbol approaches that would
combine different methods and tend to
be more robust and can be useful in
practice.
Most techniques that we introduce the
use supervised machine learning and
which is a very general method.
So that means these methods can be
actually applied to any text
categorization problem as long as we
have humans to help annotate some
training data set and design features,
then supervised machine learning an all
these classifiers can be easily applied
to those.
Problems to solve the categorization
problem.
To allow us to characterize content of
text concisely with categories or the
predictor, some properties of real
world variables that are associated
with text data.
The computers of course here are trying
to optimize the combinations of the
features provided by human an.
As I say that there are many different
ways of combining them and they also
optimize different objects and
functions.
But in order to achieve good
performance, they all require effective
features and also plenty of training
data.
So as a general rule, and if you can
improve the feature representation an
and then provide more training data,
then you can generate do better.
So performance is often much more
affected by the effectiveness of
features and then by the choice of
specific classifiers.
So feature design tends to be more
important than the choice of specific
classifier.
So how do we design effective features?
Well, unfortunately this is very
application specific, so there's no
really much general thing to say here.
But
We can.
And do some analysis of the
categorization problem and try to
understand the what kind of features
might help us distinguish categories,
and in general we can use a lot of
domain knowledge to help us design
features.
An another way to figure out effective
features is to do error analysis on the
categorisation results.
You could, for example, look at the
which category tends to be confused
with each other categories and you can
use a confusion matrix to examine the
errors systematically across
categories, and then you can look into
specific instances to see why the
mistake has been made and what features
can prevent the.
This can allow you to obtain.
Insights for design new features.
So error analysis very important in
general, and that's where you can get
the insights about your specific
problem.
And then finally we can leverage some
machine learning techniques.
So for example, feature selection is a
technique that we haven't really talked
about, but it's very important and it
has to do with trying to select the
most useful features before you
actually trainer for classifier, and
sometimes training a classifier would
also help you identify which features
have high values.
And there are also other ways to ensure
the sparsity of the model.
Meaning to recognize the weights.
So for example, the SVM actually tries
to minimize the weights on features,
but you can further for some features
to falsely use only a small number of
features.
There are also techniques for dimension
reduction, and that's to reduce the
high dimensional feature space into a
lower dimensional space.
Typical biclustering of features in
various ways, so metrics factorization
has been used to do such a job, and
this and some of the techniques are
after very similar to the topic models
that we discussed, so topic models.
LDA can actually help us reduce the
dimension of features.
Imagine the words are original feature
representation, but the representation
can be mapped to the topic space
representation.
Let's say we have K topics, so a
document cannot be represented as a
vector of justice K values
corresponding to the topics.
So we can let each topic define one
dimension.
So we have K dimensional space instead
of the original high dimensional space
corresponding to words.
And this is.
Often another way to learn factor
features, especially, we could also use
the categories to supervise learning of
such low dimensional structures.
An so the original word features can be
also combined with such such latent
dimension features or low dimensional
space features to provide a
multiresolution representation, which
is often very useful.
Deep learning is a new technique that
has been developed in machine learning.
It's particularly useful for learning
representations, so different learning
refers to deep neural network.
It's another kind of classifier where
you can have intermediate features
embedded in the model so that it's
highly non linear classifier.
An some reason advance has allowed us
to train such a complex network
effectively.
Ann is the technique has been shown to
be quite effective for speech
recognition, computer vision and
recently it has been applied through
text as well.
It has shown some promise and one
important advantage of this approach in
relationship with the feature design is
that they can learn intermediate
representations or compound features
automatically, and this is very
valuable for learning effective
representation for text localization.
Although in Texas domain cause words
are excellent representation of text
content because these are.
Humans invention for communication and
they are generous sufficient for
representing content for many tasks.
If there's a need for some new
representation, people would have
invented a new words and new World.
So because of this reason, the value of
deep learning for text processing tends
to be lower than for computer vision
and speech recognition, where there
aren't corresponding wedding design.
The words.
As features.
But deep learning is still very
promising for learning effective
features, especially for complicated
tasks like a sentiment analysis, and
has been shown to be effective because
it can provide replenishing that goes
beyond bag of words.
Regarding the training examples, it's
generally hard to get a lot of training
examples because it involves human
labor.
But there are also some ways to help
with this, so one is to assume some low
quality training examples can also be
used so those can be called a pseudo
training examples.
For example, if you take a reviews from
the Internet, they might have overall
ratings.
So to train a sentiment categorizer
meaning we want to distinguish positive
from negative opinions and categorize
reviews into these two categories then.
We could assume five star reviews are
all positive training examples.
OnStar negative but of course sometimes
in five star reviews.
We also mention negative opinions so
that rain example is not all of that
high quality, but they can still be
useful.
Another idea is really exploit unable
data and there are techniques called a
semi supervised machine learning
techniques that can allow you to
combine label data with unlabeled data.
So in our case actually it's easy to
see the mixture model can be used for
both text clustering and
categorisation, so even imagine if you
have a lot of unable text data for
categorization then you can actually do
clustering on these text data to learn
categories.
And then try to somehow align these
categories with the categories defined
by the training data where we already
know which documents are in which
category.
So you can in fact use the EM algorithm
to actually combine both.
That would allow you essentially to
also pick up a useful words in the
unlabeled data.
You can think of this in another way.
Basically, we can use, let's say a
naive Bayes classifier to classify all
the unlabeled text documents.
And then we're going to assume the high
confidence classification results, or
actually reliable.
Then you certainly have more training
data.
The cause from the unlabeled data we
some are labeled as category ones and
more labeled as category two.
Although the label is not completely
reliable.
But then they can still be useful.
So let's assume they are actually
training label examples and then we
combine them with the true training
examples.
To improve categorization method and so
this idea is very powerful and when the
enable data and training data are very
different and we might need to use
other advanced machine learning
techniques called domain adaptation or
transfer learning, this is when we can
borrow some training examples from a
related problem that may be different
or from a categorisation task that.
That involves data that follow very
different distributions from what we
are working on.
But basically when the two domains are
very different than we need to be
careful not to overfit the training
domain, but yet we can still want to
use some signals from the related
training data.
So for example, training categorisation
on news might not give you an
immediately effective classifier for
classifying topics in tweets, but you
can still learn something from news to
help categorizing tweets, so there are
machine learning techniques that can
help you.
Do that effectively.
Here's a suggestion reading an where
you can find more details about some of
the methods that we have covered.
This lecture is about the generative
probabilistic models for text
classroom.
clustering.
In this lecture we can do continue
discussing text, classroom, and we're
discussing text clustering, and we're
going to introduce generating
going to introduce generative
probabilistic models as a way to do tax
probabilistic models as a way to do text
of classroom.
clustering
So this is the overall plan for
covering tax classroom in the previous
covering text clustering in the previous
lecture we have talked about what is
text clustering and white text
text clustering and why text
classroom is interesting.
clustering is interesting.
In this lecture we're going to talk
about how to do text clustering now in
about how to do text clustering, in
general, as you see on this slide,
there are two kinds of approaches.
One is generating probabilistic models,
which is the topic of this lecture, and
later will also discuss similarity
based approaches.
So to talk about generating models for
So to talk about generative models for
text clustering, it would be useful to
revisit the topic mining problem using
topic models.
Because the two problems are very
similar, so this is a slide that you
have seen earlier in the intellectual
have seen earlier in the lecture
on topic model.
Here we show that we have input of text
collection C and number of topics K and
vocabulary V, and we hope to generate
as output 2 two things.
as output two things.
One is a set of topics denoted by Theta
eyes.
i's.
Each is a water distribution and the
Each is a word distribution and the
other is a pie ideas and these are the
other is a pi ij's and these are the
probabilities that each document covers
each topic.
So this is a public coverage and it's
So this is a topic coverage and it's
also visualized here on this slide you
can see that this is what we can get by
using a topic model.
Now a main difference between this and
text clustering problem is that here a
document is assumed to possibly cover
multiple topics, and indeed in general
document will be covering more than one
topic with non zero probabilities.
In text clustering, however, we only
allow a document to cover one topic.
If we assume one topic is a cluster.
So.
That means if we change the problem
That means if we change the topic
definition just slightly by assuming
that each document account may be
that each document can only be
generated by using precise one topic.
generated by using precisely one topic.
Then we'll have a definition of the
clustering problem.
As you hear.
As shown here.
So here the output is changed so that
we no longer have the detailed coverage
distributions pious, but instead will
distributions pi ij's, but instead will
have cluster assignment decisions.
An CI and CI is decision for the
document.
document i.
I an see somebody is going to take a
And see sub i is going to take a
And c sub i is going to take a
value from one through K to indicate
one of the K clusters.
And basically tells us document the eye
And basically tells us document Di
is in which cluster.
As illustrated here, we no longer have
multiple talking topics covered in each
multiple topics covered in each
document is precisely one topic,
although which topic is still
uncertain.
There is also a connection with the.
Problem of mining.
One topic that we discussed earlier.
So here again it's a slide that you
have seen before.
And here we hope to estimate a topic
model award distribution based on
model or word distribution based on
precisely one document, and that's when
we assume that this document covers
precisely one topic.
But we can also consider some
variations of the problem.
For example, we can consider there are
N documents, each covers different
topic.
So that's N documents and topics.
Of course, in this case these documents
are independent and these topics also
independent.
But we can further allow these
documents share topics and why.
documents share topics and then.
We can also assume that we are going to
assume there are fewer topics.
The number of documents.
So this document must share some
topics.
And if we have N documents for
sharecare topics, then will again have
share k topics, then will again have
precisely the document clustering
problem.
So because of these connections,
naturally we can think about how to use
a probabilistic generating model to
solve the problem of text clustering.
So the question now is what generating
model can be used to do clustering.
As in all cases of designing a
generative model, we hope the
generative model would adopt the output
that we hope to generate, or the
structure that we hope to model.
So in this case it's a clustering
structure.
The topics and each document that
covers one topic, and we hope to embed
such such preferences in a generating
such such preferences in a generative
model.
But if you think about the main
difference between this problem and the
topic model that we talked about
earlier and then you will see a main
requirement is how can we force every
document to be generated from precisely
one topic instead of K topics?
As in the topic model.
So let's revisit the topic model again
in more detail.
So this is a detailed view of two
component mixture model and when we
have K components it looks similar.
So here we see that when we generate a
document.
We generated each word independent.
We generated each word independently.
And we generated each word with.
And we generated each word
First make a choice between these
distributions with decided to use one
of them with probability.
So P of Cedar one is the probability of
So P of theta one is the probability of
choosing the distribution on the top.
Now we first make this decision
regarding which distribution should be
used to generate the world, and then
we're going to use this distribution to
sample world.
sample word.
Now.
Note that in such a generative model.
Notice that in such a generative model.
The decision on which distribution to
use for each word is independent, so
that means, for example, the here could
"that means, for example, ""the"" here could"
have been generated from the second
distribution.
There are two, whereas text is more
Theta two, whereas text is more
likely generated from the first one on
the top.
That means the words in the document
could have been generated in general
from multiple distributions.
Now this is not what we want, as we say
Now this is not what we want to see
for text clustering.
For document clustering where we hope
this document will be generated from
precisely one topic.
So now that means we need to modify the
model, but how well, let's first think
about why this model cannot be used for
clustering, and I just say the reason
is becausw.
is because.
It has allowed multiple topics to
contribute the words to the document.
And that causes confusion because we're
not going to know which cluster this
document is from an it's more
importantly, it's violating our
assumption about the partitioning of
documents in the clusters.
If we really have one topic to
correspond to one cluster of documents,
then we would have a document to be
generated from precisely one topic.
That means all the words in the
document must have been generated from
precise around distribution, and this
precisely one distribution, and this
is not true for such a topic model that
we're seeing here, and that's why this
cannot be used for clustering because
it did not ensure that only one
distribution has been used to generate.
All the words in one document.
So if you realize this problem, then we
can naturally design alternative
mixture model for doing clustering.
So this is what you're seeing here and
we again would have to make a decision
regarding which is distributing views
regarding which is distributing to use
to generate document, because the
document that could potentially be
generated from any of the K word
distributions that we have.
But this time, once we have made the
decision to choose one of the topics,
we're going to stay with this
distribution to generate the all the
words in the document.
And that means once we have made the
choice of the distribution for in
generating the first word.
We're going to stay with this year's
We're going to stay with this
decision in generating all the other
words in the document.
So in other words, we only make the
choice once.
Fall.
for all.
Basically we make the digit ones for
Basically we make the decision once for
this document and stay with this to
generate all the world.
generate all the words.
Similarly, if I had chosen the second
distribution, set us up to here, you
distribution, theta sub two here, you
can see will stay with this one and
can see we will stay with this one and
then generate the entire document D.
Now, if you compare this picture with
the previous one, you will see the
desicion of.
Of using a particular distribution is
made of just once for this document.
In the case of document password.
In the case of document clustering.
But in the case of topic model we have
to make as many decisions as the number
of words in the document because for
each word we can make a potential
different decision and that's the key
difference between the two models.
But this is obviously also a mixture
model, so we can just group them
together as one box to show that this
is.
Model that will give us a probability
of a document.
Now inside this model there's also
this, which of choosing a different
distribution and we don't observe that,
so that's a mixture model.
And of course, the main problem in
document clustering is to infer.
Which distribution has been used to
generator a document and that would
allow us to recover the cluster
identity over document?
identity over document
So it would be useful to think about
the difference from the topic model, as
I have also mentioned multiple times.
There are many.
Two differences.
One is the choice of.
Using a particular distribution is made
just once for document clustering
model, whereas in the topic model it's
made multiple times.
Four different words.
The second is that word distribution
here is going to be used to generate
all the words for a document.
But in the case of topic modeling, one
distribution doesn't have to join with
distribution doesn't have to generate with
all the words in a document.
Multiple distribution could have been
used to generate the words in the
document.
It's also think about the special case
when one of the one the probability of
when one of the one of the probability of
choosing a particular distribution is
equal to 1.
Now that just means we have no
uncertain it out.
uncertainty.
uncertainty now.
We just stick with one particular
distribution.
Now in that case, clearly we will see
this is no longer mixture model 'cause
there's no certainty here and we're
going to just use precise one of the
distributions for generating a
document, and we're going back to the
case of estimating one word
distribution based on one document.
So that's the connection that we
discussed earlier.
But now you can see the more clearly.
But now you can see more clearly.
So as more cases of using a generative
model to solve a problem, we first look
at data and then think about how to
at theta and then think about how to
design the model.
But once we design model, the next step
is to write down the likelihood
function.
And after that we can do look at the
And after that we can do is to look at the
how to estimate the parameters.
I saw in this case what's the
so in this case what's the
likelihood function or it's going to be
very similar to what we have seen
before in topic models, but it will be
also different.
If you still recall what the likelihood
function looks like in PSA, then you
function looks like in PLSA, then you
realize that in general the probability
of observing a data point from mixture
of observing a theta point from mixture
of observing a data point from mixture
model is going to be a sum over all the
possibilities of generating the data.
I in this case, so it's going to be
some over these K topics because
everyone can be used to generate the
document and then inside the sun you
document and then inside the sum you
can still recall what the formula looks
like an it's going to be.
A product of two probabilities and one
is the probability of choosing a
distribution.
The other is the probability of
observing a particular data point from
that distribution.
So if you are map, this formula is kind
of formula to our problem.
Here you will see the probability of
observing a document D is basically a
sum, in this case over two different
distributions.
Because we have a very simplified
situation of just two clusters.
And so in this case you can see it's a
sum of two cases.
In each case it's indeed the
probability of choosing the.
Choosing the world distribution.
Is acetyl one or two right?
Is theta one or theta two right?
And then it's this probability is
multiplied by the probability of
observing this document from this
particular distribution.
An if you further expand this
And if you further expand this
probability of observing the whole
document, we see that it's product of
observing each word X survive an.
observing each word X sub i.
Here we made the assumption that each
word is generated independently, so the
probability of the whole document is
just a product of the probability of
each word in the document.
So this form should be very similar to
the topic model, but it's also useful
to think about the difference and for
that purpose I am also copying the
probability of.
Popping model is 2 components here.
topic model is two components here.
topic model with two components here.
So here you can see at the formula
looks very similar or in many ways they
are similar.
But there's also some difference.
And in particular, the differences on
the top you see for the mixture model,
document clustering, we first take a
product and then take a sum.
And that's corresponding to our
assumption of 1st make a choice of
choosing one this vision and then stay
choosing one distribution and then stay
with this religion with all the words.
with this distribution with all the words.
with this distribution to generate all the words.
And that's why we had the product
inside the sun.
inside the sum.
The sum corresponds to the choice.
I.
right.
Now in the topic model, we see that the
Sun is actually inside the product and
sum is actually inside the product and
that's be cause we generated each word
independently.
And that's why we have the product
outside.
But when we generate each each word, we
have to make a decision regarding which
distribution we use.
So we have some there for each word.
So we have sum there for each word.
But in general, ideas are all mixture
models and we can estimate these models
models that we can estimate these models
by using the EM algorithm as we will
discuss more later.
This lecture is about a mixture of
unigram language models.
In this lecture we will continue
discussing probabilistic topic models.
In particular, we're going to introduce
a mixture of unigram language models.
This is a slide that you have seen
earlier where we talked about how to
get rid of the background words that
will have on top of estimated language
we have on top of estimated language
model for one document.
So if you want to solve the problem.
I will be useful to think about why we
It will be useful to think about why we
end up having this problem.
Well, this is obviously cause these
Well, this is obviously because these
words are very frequent in our data and
we are using a maximum likelihood
estimate and then the estimator
obviously would have to assign high
probabilities for these words in order
to maximize the likelihood.
So in order to get rid of them, that
would mean we have to do something
different here.
In particular, we have to say that this
distribution doesn't have to explain
all the words in the text data, or
we're going to say these common words
should not be explained by this
distribution.
So one natural way to solve the problem
is to think about using another
distribution to account for just these
common words.
This way the two distributions can be
mixed together to generate the text
data and will let the other model which
we called background topic model to
generate the common words.
This way our target is the topic set up
This way our target is the topic theta
here would be only the content words
here would be only generating the content words
that characterize the content of the
document.
So how does this work?
It's just a small modification of the
previous set up where we have just one
distribution.
Since we now have two distributions, we
have to decide which distribution to
use when we generate the world, but
use when we generate the word, but
each word will still be sample from one
each word will still be sampled from one
of the two distributions, right?
So text data is still generating the
same way.
Namely, we're going to generate a one
word at each time.
An eventually we generated a lot of
words.
When we generate the world, however,
When we generate the word, however,
we're going to 1st decide which of the
two distributions to use, and this is
controlled by.
controlled by
Another probability probability of
another probability. Probability of
another probability, probability of
another probability: probability of
theater sub D and probability of
theta sub D and probability of
theater suffer B here.
theta sub B here.
So this is the probability of selecting
the topic or distribution.
the topic word distribution.
This is the probability of selecting
the background world distribution
the background word distribution
denoted by Theta sub B.
Now in this case I just give example
where we can set both 2.5.
where we can set both to .5.
So if you can do basically flip a coin
affair coin to decide which one to use.
a fair coin to decide which one to use.
But in general these probabilities
don't have to be equal, so you might
bias toward using one topic more than
bias towards using one topic more than
the other.
So now the process of generating a
world would be the first to flip a coin
word would be the first to flip a coin
based on these probabilities of
choosing each model and if.
Let's say the coin shows up as head,
which means we're going to use the
topical water distribution.
topic world distribution.
topic word distribution.
Then we're going to use this word
distribution to generator award.
distribution to generator a word.
distribution to generate a word.
Otherwise we might be going through
this path.
And we're going to use the background
or distribution to generate the world.
or distribution to generate the word.
word distribution to generate the word.
So in such a case we have model that
So in such a case we have a model that
has some uncertainty associated with
the use of a word distribution.
But we can still think of this as a
model for generating text data and such
a model is called a mixture model.
So now let's see.
In this case, what's the probability of
observing the world?
observing the word w?
Now I hear I showed some words like the
Now here I showed some words like the
"Now here I showed some words like ""the"""
and text, so as in all cases, once we
"and ""text"", so as in all cases, once we"
set up the model, we're interested in
computing the like hold function.
computing the likelihood function.
The basic question is, so what's the
probability of observing a specific
word here?
Now we know that the world can be
Now we know that the word can be
observed from each of the two
distributions, so we have to consider 2
cases.
Therefore it's a sum over these two
cases.
The first case is to use the topic
water distribution to generate the
word distribution to generate the
world, and in such a case, then the
word, and in such a case, then the
probability would be the probability of
Cedar sub D, which is the probability
Theta sub D, which is the probability
of choosing the model multiplied by the
probability of actually observing the
world from that model.
word from that model.
Both events must happen in order to
observe the.
"observe ""the""."
We first must have chosen the topic of
the and then we also have to actually.
and then we also have to actually
Have sampled award from the
"have sampled the word ""the"" from the"
distribution and similarly the second
part accounts for a different way of
generating the world from the
generating the word from the
background.
Now obviously the probability of text
the same is all similar, right?
So we also consider two ways of
generating text, and each case is a
product of the probability of choosing
a particular word distribution
multiplied by the probability of
observing the world from that
observing the word from that
distribution.
Now later you will see this is actually
general form, so you might want to make
sure that you have really understood
this expression here.
And you should convince yourself that
this is indeed the probability of
observing text.
So to summarize, what we observe here,
the probability of a world from a
the probability of a word from a
mixture model is in general a sum over
all different ways of generating the
world.
word.
And in each case it's a product of the
probability of selecting that component
model.
Multiplied by the probability of
multiplied by the probability of
actually observing the data point from
that component model, and this is
something quite a general and you will
something quite general and you will
see this occurring often later.
So the basic idea of a mixture model is
just retreated these two distributions
just to treated these two distributions
together as one model.
So I use the box to bring all these
components together.
So if you view this whole box as one
model, it's just like any other
generative model.
It would just give us the probability
of a word.
But the way that determines this
probability is quite different from
when we have just one distribution.
And this is basically a more
complicated mixture model.
Sorry, more complicated model than just
one distribution, and it's called a
mixture model.
So as I just said, we can treat this as
just generated model and it's often
just a generative model and it's often
useful to think of just the likelihood
function.
The illustration that you have seen
before, which is dinner now is just the
before, which is dimmer now is just the
illustration of this generation model.
So mathematically, this model.
This is nothing but to just define the
following generated model where the
following generative model where the
probability of world is assumed to be a
probability of word is assumed to be a
sum over 2 cases of generating the
world.
word.
The form you're seeing now is more
general form than.
What you have seen in the calculation.
what you have seen in the calculation earlier.
I just used a simple W to denote any
I just used a simple w to denote any
word, but you can still see.
This is basically first son.
This is basically the first sum.
Like
And this sum is due to the fact that
the world can be generating multiple
the word can be generating multiple
ways.
Two ways in this case.
At insider, some each term is a product
At inside sum, each term is a product
again of two terms.
And the two terms are first the
And the two terms are ,first, the
probability of selecting a component
like the loss of the second, the
like Theta sub D of the second, the
like Theta sub D. Second, the
probability of actually observing the
world from this component model.
word from this component model.
And so this is very general description
And so this is a very general description
of in fact all the mixture models.
of, in fact, all the mixture models.
And I just want to make sure that you
understand this, because this is really
the basis for understanding all kinds
of topic models.
So now once we set up the model and we
can write down the likelihood function
as we see here, the next question is
how can we estimate the parameter or
what to do with the parameters given
the data?
Well, in general we can use some
observed text data to estimate the
model parameters and this is mission
model parameters and this mission
would allow us to discover.
would allow us to discover
The interesting knowledge about the
the interesting knowledge about the
text, so in this case, what do we
discover?
Well, these are represented by our
parameters, and we have two kinds of
parameters.
One is the two word distributions.
Those are two topics and the other is
the coverage of each topic in each.
The coverage of each topic and this is
determined by probability of Cedars of
determined by probability of Theta sub D
determined by probability of Theta sub D.
determined by probability of Theta
Dian.
sub D.
Probability of Theta submit.
and probability of Theta sub B.
Note that they sum to one.
Now what's interesting is also to think
about the special cases, like when we
set one of them to one.
What would happen with the other would
What would happen? Well, the other would
be 0, right?
And if you look at the likelihood
function.
It will then degenerate to the special
case of just want this page right so
case of just one distribution right so
you can easily verify that by assuming
one of these two is 1.0 and the other
is 0.
So in this sense, the mixture model is
more general than the previous model
where we have just one distribution and
it can cover that as a special case.
So to summarize, and we talked about
the mixture of two unigram language
models.
And the data will, considering here is
And the data we consider here is
just still 1 document.
And the model is a mixture model with
two components to unigram language
two components: two unigram language
models.
Specifically sealer sub D which is
Specifically, Theta sub D which is
intended to denote the topic of
document and theaters of B which is
document D and Theta sub B which is
representing a background topic.
representing a background topic
That we can set to attract the common
that we can set to attract the common
words.
Because common words will be assigned
Because common words would be assigned
the high probabilities in this model.
high probabilities in this model.
So the parameters can be collectively
called a Lambda, which I show here
again, and you can again.
again, and you can again
Think about the question about how many
think about the question about how many
parameters are we talking about
exactly.
This is usually good exercise to do
because it allows you to see the model
index and to have a complete
understanding of what's going on in
this model and we have mixing weights
of course also.
So what is the likelihood function look
like?
It looks very similar to what we had
before, so for the document first it's
before, so for the document, first, it's
a product over all the words in the
a product of all the words in the
document exactly the same as before.
The only difference is that inside here
now it's a sum instead of justice one,
now it's a sum instead of just one,
so you might recall before we just had
this one.
But now we had this some because of the
But now we had this sum because of the
mixture model, an becausw of the mixed
mixture model and because of the mixed
model.
model
We also have to introduce the
probability of choosing that particular
component distribution.
And so this is just another way of
writing it again by using a product
over all the unique words in our
vocabulary, instead of having a product
of all the positions in the document
and this form where we look at the
and this form where we look at
different unique words is a convenient
form for computing the maximum
likelihood estimator later.
An the maximum likelihood is made that
An the maximum likelihood estimator
And the maximum likelihood estimator
is, as usual, just find the parameters.
is, as usual, just to find the parameters.
is, as usual, just to find the parameters
That would maximize this like hold
That would maximize this likelihood
that would maximize this likelihood
function and the constraints.
function and the constraints
We all, of course two kinds.
of course two kinds.
,of course, two kinds.
here, of course, two kinds.
here, of course, are two kinds.
One is the word probabilities in each
topic must sum to one, the other is the
choice of each topic must sum to one.
This lecture is about the expectation
maximization algorithm, also called the
EM algorithm.
In this lecture, we're going to
continue the discussion of
probabilistic topic models.
In particular, we're going to introduce
the EM algorithm, which is a family of
useful algorithms for computing the
maximum regular estimate of mixture
maximum likelihood estimate of mixture
models.
So this is now familiar scenario of
using a two component mixture model to
try to factor out the background words
from one topic award distribution here.
from one topic words distribution here.
from one topic word distribution here.
So we are interested in.
So we are interested in
Computing this estimate.
computing this estimate.
And we're going to try to adjust these
probability values to maximize the
probability of the observed document,
and know that we assume that all the
other parameters are known.
So the only thing unknown is these
So the only thing unknown is this
world probabilities given by Cedars
word probabilities given by theta sub d
word probabilities are given by theta sub d
update.
And in this lecture were going to look
into how to compute this maximum like
into how to compute this maximum likelihood
overestimated.
estimated.
estimator.
Now let's start with the idea of
separating the words in the text data
into two groups.
One group would be explained by the
background model, the other group would
be explained by the Unknown topic order
be explained by the Unknown topic word
space.
distribution
After all, this is the basic idea of
mixture model.
But suppose we which word is from which
But suppose we actually know which word is from which
distribution, so that would mean, for
example these words.
example these words:
The is an we are known to be from this
the is and we are known to be from this
background or distribution.
background word distribution.
On the other hand, the other words,
text, mining, clustering, etc are known
to be from the topic or distribution.
to be from the topic word distribution.
If you can see the color, then these
are showing blue.
are shown in blue.
These blue words are then assumed to be
from the topic award distribution.
from the topic word distribution.
If we already know how to separate
these words, then the problem of
estimating the world distribution would
be extremely simple, right?
If you think about this for a moment,
you realize that we can simply take all
you realize that well we can simply take all
these words that are known to be from
this world distribution status of the
this word distribution theta sub d
and normalize them.
So indeed this problem will be very
easy to solve.
If we had known which words are from
which distribution precisely.
And this is in fact.
And this is in fact
Making this model no longer mixture
model becausw we can already observe
model because we can already observe
which distribution has been used to
generate which part of the data, so we
actually go back to the Single world
actually go back to the Single word
distribution problem, and in this case
let's call these words.
let's call these words
That are known to be from C to DA
that are known to be from theta d
pseudo document.
pseudo document
The prime and then all we need to do is
d prime and then all we need to do is
just normalize these word counts for
each word W sub I.
each word w sub I.
And that's fairly straightforward, and
it's just dictated by the maximum micro
it's just dictated by the maximum likelihood
race made up now.
estimate now.
This idea, however, it doesn't work,
This idea, however, doesn't work,
because we in practice don't really
know which word is from which
distribution.
But this gives us the idea of perhaps
we can guess which word is from which.
It is rich.
distribution.
Specifically, given all the parameters.
Specifically, given all the parameters
Specifically, given all the parameters can we infer the distribution
Specifically, given all the parameters can we infer the distribution the word is from
So let's assume that we tentative
So let's assume that we actually know tentative
probabilities for these words in
theater sub D.
theta sub D.
So now all the parameters are known for
this mixture model.
And now let's consider would like a
And now let's consider word like a
And now let's consider word like
text.
So the question is, do you think text
is more likely have been having been
generated from Cedar subbed or from
generated from theta sub d or from
philosopher B?
"theta sub b
theta sub b?
theta sub B?
So in other words, we want to infer
which distribution has been used to
generate this text.
Now, this inference process is a
typical Bayesian inference situation
where we have some prior about.
where we have some prior about
These two distributions so can you see
what is our prior here?
The player here is the probability of
Well the prior here is the probability of
each distribution, right?
So the prior is given by these two
probabilities.
In this case the prior is.
In this case the prior is
Saying that each model is equally
likely, but we can imagine perhaps a
different price possible.
different prior possible.
So this is called prior to cause this
So this is called prior because this
is our guess of which distribution has
been used to generate the world before
we even observe the world.
we even observe the word.
So that's why we call it apply.
So that's why we call it prior.
I see if we don't observe the world or
if we don't observe the world or
if we don't observe the word or
we don't know what word has been
observed, our best guess is just say
well.
They are equally likely.
Alright, so it's just a flipping a
coin.
Now in Bayesian inference, we typically
then would update our belief after we
have observed evidence.
So what is evidence here?
While the evidence here is the word
text.
Now that we are interested in the world
Now that we are interested in the word
text, so test can be regarded as
text, so text can be regarded as
evidence.
And in the if we use Bayes rule to
combine the prior and the data
likelihood, what we will end up with is
to combine the prior with the like hold
to combine the prior with the likelihood
that you see here, which is basically
the probability of the world text from
the probability of the word text from
each distribution and we see that in
both cases text is possible that even
in the background it is still possible.
It just has a very small probability.
So intuitively, what would be your
guess?
So in this case?
So in this case
Now, if you're like many others, you
will guess text is probably from Cedars
will guess text is probably from theta sub d
up D small likely from philosophy,
is more likely from philosophy,
"is more likely from
is more likely from  theta sub d,
while and you will probably see that
why? And you will probably see that
it's be cause.
it's because.
Text has a much higher probability
here.
By the seasons of deep, then by the
"By the
By the theta sub d, then by the
By the theta sub d, than by the
background model, which has a very
small probability.
And by this we are going to say text is
more likely from serious update.
"more likely from
more likely from  theta sub d.
more likely from theta sub d.
So you see our guests of which
So you see our guess of which
distribution has been used to generate
the text would depend on how high the
probability of the data the text is in
each world distribution.
each word distribution.
Working do tend to gas the distribution
Working do tend to guess the distribution
We are going do tend to guess the distribution
that gives the world higher probability
that gives the word higher probability
and this is likely to maximize the
likelihood right so?
likelihood right so.
We're going to choose world that has a
We're going to choose word that has a
higher likelihood.
So in other words, we're going to
compare these two probabilities.
Of the word given by each
distributions.
But our.
But our
Guess must also be affected by the
guess must also be affected by the
prior, so we also need to compare these
two players WHI becausw imagine if we
two priors why? because imagine if we
adjust these probabilities, we're going
to say the probability of choosing a
background model is almost 100%.
Now if we have that kind of strong
prior, then that would affect your
guess.
You might think well, wait a moment,
maybe text could have been from the
background as well, although the
probability is very small here.
The prior is very high.
So in the end we have to combine the
two and the base formula provides
two and the bayse formula provides
two and the bayes formula provides
provides us a solid, unprincipled way
provides us a solid and principled way
of making these kind of gas to quantify
of making these kind of guess to quantify
that.
So more specifically, let's think about
the probability that this world text
the probability that this word text
has been generated.
In fact from sealer sub D, The in order
In fact from theta sub D, The in order
for texture to be generated from Cedars
"for texture to be generated from
for texture to be generated from theta sub d
of D2, things must happen first.
two things must happen first.
The theater somebody must have been
"The
The theta sub d must have been
selected, so we have the selection
probability here, and Secondly, we also
have to actually have observed attacks
have to actually have observed text
from the distribution.
So when we multiply the two together,
we get the probability that text has in
fact been generated from philosophy.
"fact been generated from
"fact been generated from theta sub d
"fact has been generated from theta sub d
Similarly, for the background model an.
The probability of generating texture
The probability of generating text
is another product of similar form.
We also introduced latent variable Z
We also introduced a latent variable Z
here to denote whether.
here to denote whether
All the world is from the background or
the word is from the background or
the topic.
When Z is zero, it means it's from the
topic.
topic
Status update when it's one, it means
"theta sub d
theta sub d when it's one, it means
it's from the background philosophy.
"it's from the background
it's from the background theta sub b.
So now we have the probability that
text is generated from each.
Then we simply we can simply normalize
them to have estimate of the
probability that the word text is from
Cedar sub Dior from V.
"theta sub d
theta sub d or from  theta sub b.
theta sub d or from theta sub b.
And equivalently, the probability that
Z is equal to 0 given that the observed
evidence is text.
So this is.
So this is
Application of Bayes rule.
But this step is very crucial for
understanding the EM algorithm.
Because if we can do this, then we
would be able to 1st initialize the
parameter values.
parameter values
Somewhat randomly, and then we're going
to take a guess of these values and or
to take a guess of these z values and or
to take a guess of these Z values and or
which distribution has been used to
generate which word and the prime
generate which word and the initialized parameter
values would allow us to have a
complete specification of the mixture
model, which further allows us to apply
Bayes rule to infer which distribution
is more likely to generate a.
is more likely to generate
Each word and this prediction
essentially helped us to separate words
from the two distributions, although we
can't separate them for sure, but we
can separate them probabilistically.
can separate them probabilistically
As you hear.
as shown hear.
as shown here.
This lecture is about the contextual
text mining.
Contextual text mining is related to
multiple kinds of knowledge that we
mined from test data.
mine from text data.
As I'm showing here, is related to
topic mining becausw can make topics
topic mining because can make topics
associated with context, like a time or
location, and similarly it can make
opinion mining more contextualized,
making opinions connected to context.
And it's related to text based
It's related to text based
prediction because it allows us to
combine non tests that are with taxes
combine non text data with text data
out to derive sophisticated predictors
to derive sophisticated predictors
for the prediction problem.
So more specifically, why are we
interested in contextual text mining?
Well that's first.
Well that's, first,
Becausw text often has rich context
Because text often has rich context
because text often has rich context
information and this can include
directive context such as meta data.
direct context such as meta data.
Anne also interacted context, so the
And also interacted context, so the
And also indirect context, so the
director context can include the matter
direct context can include the matter
direct context can include the meta-data
that are such as time, location,
such as time, location,
authors, and source of the text data.
And they almost always available to us.
An interactive text context refers to
An indirect text context refers to
Indirect text context refers to
additional data related to the meta
data.
So, for example, from authors, we can
further obtain additional context, such
as social network of the author or the
authors age and such information is not
author's age and such information is not
author's age. And such information is not
author's age. And such information is not,
in general directly related to the text
,in general, directly related to the text
in general, directly related to the text
yet through the answers we can connect
yet through the authors we can connect
them.
There could be also other text data
from the same source as this one, so
the other context data can be connected
with this text.
with this text,
As well, so in general, any related
as well. So in general, any related
data can be regarded as context, so
there could be remotely related to
there could be remotely related to context.
Contacts.
context.
And so what's the use of?
And so what's the use of,
Why is text Contacts useful?
why is text Contacts useful?
why is text context useful?
Well, Contacts that can be used to.
Well, context can be used to
Partition texted are in many
partition text are in many
partition text data in many
interesting ways.
It can almost allows positioning text
It can almost allows partition text
data in arbitrary ways as we need, and
data in arbitrary ways as we need. And
this is very important because this
allows us to do interesting comparative
analysis.
It also in general provides meaning to
the discovery topics if we can
the discovery topics if we gonna
associate the text with context.
So his illustration of how context.
So here's illustration of how context.
So here's illustration of how context
Can be regarded as interesting ways of
can be regarded as interesting ways of
partitioning of text data, so here I
partitioning of text data. So here I
just show some research papers
published in different years.
Different values, different conference
on different venues, different conference
names here listed on the bottom, the
names here listed on the bottom, like
Siegal ACL, etc.
SIGIR, ACL, etc.
Such text data can be partitioning,
Now, such text data can be partitioning,
Now, such text data can be partitioning in
meaning, trusting ways because we have
many interesting ways because we have
context.
So the context here just includes time
and the conference venues.
And but perhaps we can include some
other variables as well.
But let's see how we can partition the
But let's see how we can partition data
interesting ways.
in interesting ways.
First, we can treat each paper as a
separate unit.
So in this case a paper ID and the each
So in this case a paper ID and each
So in this case, a paper ID and each
paper is has its own context, it's
paper has its own context, it's
independent.
Ann
And.
But we can also treat all the papers
written in 1998 as one group, and this
is only possible because of the
availability of time and we can
partition data in this way.
This would allow us to compare topics,
for example in different years.
Similarly, we can partition the data
based on the values.
based on the venues.
We can get all the CIA papers and
We can get all the SIGIR papers and
compare those papers with the rest or
compare cigar papers with KD papers
compare SIGIR papers with KDD papers
with ACL papers.
We can also partitioning the data to
We can also partition the data to
obtain the papers written by authors in
the US, and that of course uses
additional context.
Of the office and this would allow us
of the authors and this would allow us
to then compare such a subset with
another set of papers written by others
another set of papers written by authors
in other countries.
Or we can obtain a set of papers about
the text mining, and this can be
compared with papers about another
compared with papers about another topic.
topic.
And note that these partitions can be
And note that these partitioning can be
also inspected with each other to
also intersect with each other to
generate even more complicated
partitions.
And so in general, this enables
discovery of knowledge associated with
different context as needed.
And in particular, we can compare
different contexts, and this often
gives us a lot of useful knowledge.
For example, comparing topics overtime,
we can see trends of topics and
comparing topics in different context
can also reveal differences about the
two Contacts.
two contexts.
So there are many interesting questions
that require contextual text mining
here, at least some very specific ones.
here, I list some very specific ones.
For example, what topics have been
gaining increasing attention recently
in data mining research?
Now to answer this question, obviously
we need to analyze text in the context
of time.
So time is a context in this case.
Is there any difference in the
responses of people in different
regions to the event to an event?
regions to the event, to any event?
So this is a very broad analysis
question, in this case, of course,
location is the context.
What are the common research interests
of two researchers?
In this case, authors can be the
context.
Is there any difference in the research
topics published by others in the USA
topics published by authors in the USA
and those outside now?
and those outside?
In this case, the context would include
Now, in this case, the context would include
the authors and their affiliation and
location.
So this goes beyond just the author
himself or herself.
We need to look at the additional
information connected to the other.
information connected to the author.
Is there any difference in the opinions
about the topic expressed on one social
network and another?
In this case, the social network of
others and the topic can be the
authors and the topic can be the
context.
Are there topics in news data that are
correlated with sudden changes in stock
prices?
In this case, we can use a time series
such as a stock prices as Contacts.
such as stock prices as context.
What issues mattered in the 2012
presidential campaign or presidential
election?
Now in this case, time series again as
Now in this case, time series again as context.
context.
df
So as you can see, the list can go on
So, as you can see, the list can go on
and all basically con textual text
and on, basically contextual text
mining can have many applications.
In this lecture, we continue discussing
paradigmatic relation discovery.
Earlier, we introduced a method called
expected overlap of words in context.
In this method, we represent each
context via word vector that represents
context by a word vector that represents
the probability of world.
the probability of word
In the context an we measure the
in the context and we measure the
similarity by using the DOT product.
Which can be interpreted as the
probability that to randomly pick the
words from the two contexts or
words from the two contexts
identical, we also discuss the two
are identical, we also discuss the two
problems of this method.
The first is that it favors matching
one frequent term very well over
matching more distinctive terms.
matching more distinct terms.
It put too much emphasis on matching
one tone very well.
one term very well.
The second is that it treats every word
equally.
Even a common word like the would
Even a common word like 'the' would
contribute equally as content word like
eats.
'eats'.
So now we are going to talk about how
to solve these problems.
Most specifically, we're going to
introduce some retrieval heuristics
used in text retrieval, and these
heuristics can effectively solve these
problems, as these problems also occur
in text retrieval when we match a query
vector with document vector.
So to address the first problem, we can
use a sub linear transformation of term
frequency.
That is, we don't have to use the raw
frequency count of term to represent
the context.
We can transform it into some form that
wouldn't emphasize so much on the raw
frequency to address the second
frequency. To address the second
problem, we can put more weight on rare
terms.
That is, we can reward matching a
railworld and this heuristic is called
rare word and this heuristic is called
IDF term weighting in text retrieval.
Idea stands for inverse document
IDF stands for inverse document
frequency.
So now we're going to talk about the
two heuristics in more detail.
First, let's talk about the TF
transformation.
That is, to convert the raw count of
word in the document into some weight
that reflects our belief about how
important this word in the document.
And so that will be denoted by TF of
W&D as shown in the Y access.
W&D as shown in the Y axis.
Now in general there are many ways to
map that, and let's first look at the
simple way of mapping and then.
simple way of mapping.
In this case, we're going to say any
In this case, we're going to say, any
non zero counts will be mapped to one.
And then zero count will be mapped to
0.
So with this mapping, all the
frequencies will be mapped to only two
values, zero or one, and the mapping
function is shown here as a flat line
here.
Right now this is not even because it
Now this is naive because it
ignored the frequency of words.
However, this actually has the
advantage of emphasizing matching all
the words in the context, so it does
not allow a frequent word to dominate
the matching.
Now the approach that we have taken
earlier in the expected overlap account
approach is a linear transformation.
We basically take Y as the same as X.
So we use the raw count as
representation.
And that created the problem that we
just talked about.
Namely it answers too much on just
matching one frequent term matching one
matching one frequent term. Matching one
frequent home can contribute a lot.
frequent term can contribute a lot.
So we can have a lot of other
interesting transformations in between
the two extremes.
And they generally form a sub linear
transformation.
So for example, one possibility is to
take logarithm of the raw count, and
this will give us curve that looks like
this, right?
That you're seeing here.
In this case, you can see the high
frequency counts.
frequency counts,
The high count are penalized a little
The high counts are penalized a little
the high counts are penalized a little
bit right?
So the curve is a sub linear curve, an
it brings down the weight of.
it brings down the weight of
Really those really high counts.
really those really high counts.
And this is what we want, because it
prevents that kind of terms from
dominating the scoring function.
Now there is also another interesting
transformation called a PM 25
transformation called a BM25
transformation which has been shown to
be very effective for retrieval and in
this transformation we have a form
that.
Looks like this.
I saw it's K + 1 * X / X + K where K is
I saw it's (K + 1) * X /( X + K) where K is
a parameter.
X is the count, the raw count of world.
X is the count, the raw count of word.
Louder transformation is very
Now the transformation is very
interesting in that it can actually
kind of go from one extreme to the
other extreme by varying K.
And it also is interesting that it has
upper bound.
upper bound
K plus one in this case.
K +1 in this case.
So this puts a very strict constraint
on high frequency terms, because their
weight would never exceed K plus one.
weight would never exceed K+1.
As we vary K.
As we vary K,
If we can simulate the two extremes.
if we can simulate the two extremes.
So one case is set to zero.
We roughly have the 01 vector.
Whereas when we set the key to a very
large value, it would behave more like
the linear transformation.
So this transformation function is by
far the most effective transformation
function for text retrieval, and it
also makes sense for our problem set
up.
So we just talk about how to solve the
problem of over emphasizing a
frequently frequent term.
Now let's look at the second problem,
and that is how we can penalize popular
terms.
Matching the is not surprising because
Matching 'the' is not surprising because
the occurs everywhere, but matching
'the' occurs everywhere, but matching
eats with account alot.
'eats' with account alot.
'eats' will account a lot.
So how can we address that problem now?
So how can we address that problem.
So how can we address that problem?
In this case we can use the idea of
In this case we can use the IDF
In this case we can use the
waiting pop that's commonly used in
weighting...that's commonly used in
IDF weighting pop that's commonly used in
IDF weighting that's commonly used in
retrieval idea for stands for inverse
retrieval.  IDHfor stands for inverse
retrieval.  IDF stands for inverse
retrieval. IDF stands for inverse
document frequency.
That document frequency means the count
Document frequency means the count
of the total number of documents that
contain a particular world.
contain a particular word.
contain a particular word.
So here we show that the idea of
So here we show that the IDM of
So here we show that the IDF of
So here we show that the IDF
So here we show that the IDF
measure is defined as a logarithm
function of the number of documents
that match your turn.
that match the term.
that match the term,
Or document frequency.
or document frequency.
So K is the number of documents
containing word or document frequency.
containing word or document frequency
Anne M.
And M
And M
and M
Here is the total number of documents
here is the total number of documents
in the collection.
The idea of function is giving a higher
The IDF function is giving a higher
value for a lower K, meaning that it
rewards rail time.
rewards a rare term.
And the maximum value is log of M + 1.
That's when the world occurs just once
That's when the word occurs just once
in the context.
So that's a very rare item.
So that's a very rare term.
So that's a very rare term,
The rare is that I'm in the whole
The rarest term in the whole
the rarest term in the whole
collection.
The lowest value you can see here is
when K reaches its maximum, which would
be M.
I saw that would be a very low value.
That would be a very low value.
That would be a very low value
Close to 0 in fact.
close to 0 in fact.
Right so this.
This of course measure is used in
search where we naturally have a
collection.
In our case, what will be our
collection?
We can also use the context that we can
collect for all the words as our
collection and that is to say, a word
that's popular in the collection in
general.
general
Would also have a low IDF.
would also have a low IDF.
Because depending on the data set, we
can.
can
Construct the context vectors in
construct the context vectors in
different ways, but in the end, if a
term is very frequently in the original
data set, then it would still be
frequently in the collected context
documents.
So how can we add these heuristics to
improve our?
improve our.....
Our similarity function.
Here's one way, and there are many
other ways that are possible.
But this is a reasonable way where we
can adapt the VM 25 retrieval model for
can adapt the BM25 retrieval model for
paradigmatically relation mining.
paradigmatic relation mining.
So here.
So here
We define in this case we define the
we define in this case we define the
document vector.
As containing elements.
As containing elements
Representing normalized BM 25 values.
representing normalized BM 25 values.
So in this normalization function we
see we take some over some of all the
see we take sum over some of all the
words and then we.
words and then we
Normalize the weight of each word by
normalize the weight of each word by
the some of the ways of order.
the sum of the weights of
All the words.
all the words.
This is to again ensure all the
exercise with some to one in this
x(i) will sum to one in this
vector.
So this would be very similar to what
we had before in that this vector is
actually something similar to award
actually something similar to word
distribution or the exercise with sum
to one.
Now the weight of BM25 for each word is
defined here.
And if you compare this with our old
definition where we just have a
normalized count.
On this one, right?
So we only have this one and the
document lens or the total count of
document length or the total count of
words in that context document.
And that's what we had before.
But now with the BM 25 transformation,
we introduced something else.
First, of course, this extra occurrence
of this count is just to achieve the
sub linear normalization.
But we also see we introduce the
parameter K here.
And this parameter is generally non
active number, although zero is also
negetive number, although zero is also
possible.
Like this controls the upper bound and
This controls the upper bound and
the kinds of kind of also controls.
the kinds controls can choose
To what extent is simulates the linear
transformation.
And so this is 1 parameter.
But we also see there is another
parameter here B and this will be
parameter here b and this will be
within zero and one.
And this is a parameter to control lens
And this is a parameter to control Lance
And this is a parameter to control  length
normalization.
Ann, and in this case the normalizing
And, and in this case the normalizing
formula has average document length
here.
And this is the computed by taking the
average of the lenses of all the
average of the lengths of all the
documents in the collection.
In this case, all the lenses of all the
In this case, all the lengths of all the
context of documents that we are
context documents that we are
considering.
So this average documents will be a
constant for any given collection, so
it actually is only affecting the
effect of the parameter B here.
Because this is a constant.
But I kept it here because it's
constant that's useful in retrieval,
where it would give us a stabilized
interpretation of parameter be.
interpretation of parameter b.
But for our purpose this will be a
constant, so it would only be.
constant, so it would only be
Affecting the lens formalization.
Affecting the lenngth formalization.
Affecting the length formalization.
affecting the length formalization.
affecting the length formalization
So together with parameter B.
together with parameter B.
Now with this definition, then we have
a new way to define our document
vectors and we can compute the vector
D2 in the same way.
The difference is that the high
frequency terms will now have a
somewhat lower weights and this would
help control the influence of these
high frequency terms.
Now the idea of can be added here in
Now the IDF can be added here in
the scoring function.
That means we'll introduce await for
That means we'll introduce weight for
matching each term, so you may recall.
matching each term. So you may recall
This sum indicates all the possible
this sum indicates all the possible
words that can be overlap between the
words that can be a overlap between the
two contexts.
And the XI&YI probabilities of.
And the Xi and Yi probabilities of
Of picking the word from both Contacts,
Of picking the word from both contexts,
picking the word from both contexts,
therefore it indicates how likely will
see a match on this world.
see a match on this word.
Now idea would give us the importance
Now IDF would give us the importance
of matching this world.
of matching this word.
A common word will be worth less than
rare word, so we emphasize more on
matching rail words now.
matching rare words now.
So with this modification, then the new
function will likely address those two
problems.
Now interesting Lee.
Now interestingly
We can also use this approach to
we can also use this approach to
discover syntagmatic relations.
In general, when we represent a
convector to represent the sorry to
term vector to represent the sorry to
repent context with the convector, we
represent context with the term vector, we
would likely see some terms have
highways and other terms have lower
higher weights and other terms have lower
weights depending on how we assign
weights to these terms, we might be
able to use these weights to discover
the words that are strongly associated
with the candidate award in the
with the candidate word in the
context.
So let's take a look at the convector
So let's take a look at the term vector
in more detail here.
And we have each.
And we have each
XI, defined as a normalized weight of
Xi, defined as a normalized weight of
BM 25.
Now this weight alone only reflects how
frequently the word occurs in the
context.
But we can't just say and if we
But we can't just say any
couldn't turn in the context that would
frequent term in the context that would
be correlated with the candidate world.
be correlated with the candidate word.
Cause many common words like there will
Because many common words like 'the' there will
Because many common words like 'the' will
occur frequently in all the context.
But if we apply idea of waiting as you
But if we apply IDF weighting as you
see here, we can then.
see here, we can then
We wait these terms based on ideas.
We wait these terms based on IDF
we weight these terms based on IDF
That means the words that are common,
like the will get penalized.
like 'the' will get penalized.
So now the highest weighted terms will
not be those common terms because they
have lower ideas.
have lower IDFs.
Instead, those terms would be the terms
that are frequent in the context, but
not frequently in the collection.
So those are clearly the words that
tend to occur in the context of the
candidate word, for example, cat.
So for this reason, the highly weighted
terms in this idea of weighted vector.
terms in this IDF weighted vector.
terms in this IDF weighted vector
Can also be assumed to be candidate for
can also be assumed to be candidate for
Syntagmatic relations.
Now of course, this is only a byproduct
Now of course, this is only a bi-product
of our approach for discovering
paradigmatic relations.
And in the next lecture, we're going to
talk more about how to discover
Syntagmatic a relations.
Syntagmatic relations.
But it clearly shows the relation
between discovering the two relations.
An indeed they can be discussed,
And indeed they can be discussed,
discovered in a joint manner by
leveraging such associations.
So to summarize.
So to summarize,
The main idea for discovering parallel
The main idea for discovering
the main idea for discovering
medical relations is to collect the
paradigmatic relations is to collect the
context of a candidate world to form a
context of a candidate word to form a
pseudo document, and this is typically
represented as a bag of words.
And then compute the similarity of the
corresponding context documents of two
candidate words.
An then we can take the highly similar
word pairs and treat them as having
priority medical relations.
paradigmatic relations.
These are the words that share similar
context.
And there are many different ways to
implement this general idea and we just
talk about some of the approaches.
And more specifically, we talked about
using tax retrieval models to help us
using text retrieval models to help us
design effective similarity function to
compute the paralegal medical
compute the paradigmatic
relations.
More specifically, we have used the PM.
More specifically, we have used the
25 An idea of waiting to.
BM25 An idea of waiting to.
BM25 An IDF weighting to.
BM25 and IDF weighting to
To discover paradigmatic relation and
discover paradigmatic relation and
these approaches also represent the
state of the art in text retrieval
techniques.
Finally, Syntagmatic relations can also
be discovered as a byproduct.
be discovered as a bi-product
When we discover Parodical medical
When we discover paradigmatic
when we discover paradigmatic
relations.
This lecture is about the syntagmatic
relation discovery and mutual
information.
In this lecture, we're going to
continue discussing select medical
continue discussing syntagmatic
relation discovery.
In particular, we're going to talk
about another concept, information
about another concept, the information
theory, called mutual information.
theory, called Mutual information.
theory, called mutual information.
And how it can be used to discover
cinematical relations?
syntagmatic relations?
Before we talked about a problem of
conditional entropy, and that is the
conditional entropy computed on
different pairs of words is not really
comfortable, so that makes it hard to
comparable, so that makes it hard to
discover strong mathematical relations
discover strong syntagmatic relations
globally from corpus.
So now we're going to introduce mutual
information, which is another concept
in information theory that allows us
to, in some sense, normalize the
conditional entropy to make.
Eight more comfortable across different
a more comparable across different
pairs.
In particular, mutual information
In particular, mutual information,
denoted by I of X&Y measures.
denoted by I of X&Y, measures
denoted by I(X;Y), measures
The introduction of X obtained from
the entropy reduction of X obtained from
knowing why.
knowing Y.
More specifically the question we're
interested in here, is how much
reduction in the entropy of X can we
obtain by knowing why.
obtain by knowing Y.
So mathematically, it can be defined as
the difference between the original
entropy of X and the conditional
entropy of X given Y.
And you might see here you can see
here.
It can also be defined as a reduction
of entropy of why, because of knowing
of entropy of Y, because of knowing
X.
Normally the two conditional entropies
edge of X given Y&H of Y given X are
H(X|Y) and H(Y|X) are
not equal, but interesting.
not equal, but interestingly
not equal.
Lee.
But interestingly,
The reduction of entropy.
the reduction of entropy.
the reduction of entropy
By knowing one of them is actually
by knowing one of them is actually
equal, so this quantity is called a
equal, so this quantity is called
mutual information denoted by I hear
mutual information denoted by I here
and this function has some interesting
properties.
First, it's also non negative.
This is easy to understand becausw the
original entropy is always not going to
be lower than the.
be lower than the
Possibly reduce the conditional
possibly reduce the conditional
possibly reduced conditional
entropy.
In other words, the conditional entropy
would never exceed original entropy.
would never exceed the original entropy.
Knowing some information can always
help us potentially, but won't hurt us
in predicting eggs.
in predicting X.
The signal property is that it's
The second property is that it's
symmetric while conditional entropy is
not symmetrical.
Mutual information is.
At the sort of property is that?
The third property is that?
The third property is that
It reaches its minimum zero if and only
it reaches its minimum zero if and only
if the two random variables are
completely independent.
That means knowing one of them doesn't
tell us anything about the other.
And this last property can be verified
by simply looking at the equation
above.
And it reaches 0 if and only if the
conditional entropy of X comma Y is
conditional entropy of X given Y is
exactly the same as original entropy of
X.
So that means knowing why did not help
at all, and that's when X&Y are
completely independent.
Now when we fix X to rank different
wise using conditional entropy would
Ys using conditional entropy would
give the same order as ranking based on
mutual information cause in the
mutual information, because in the
function here H of X is fixed because X
is fixed.
So ranking based on mutual information
is exactly the same as ranking based on
the conditional entropy of X given Y.
But the mutual information allows us to
compare different pairs of X&Y, so
that's why mutual information is more
general and in general more useful.
So let's examine them intuition of
using which information for syntagmatic
using mutual information for syntagmatic
relation mining.
Now the question we ask for syntactic
relation mining is whenever eats
occurs, what other words also tend to
occur?
So this question can be framed as a
mutual information question, that is,
which was have higher mutual
information with ease.
information with eats.
So we're going to compute the mutual
information between each an other
information between eats and other
words.
And if we do that, and it's basically a
based on the same intuition as in
conditional entropy, we will see that
words that are strongly associated with
each will tend to have high bridge
each will tend to have high mutual
information, whereas words that are not
related.
We have lower mutual information, so
this I give some example here.
The mutual information between eats
animite, which is the same as between
and meat, which is the same as between
and meata, which is the same as between
and meats, which is the same as between
meats and eat cause major information
meats and eats cause major information
is symmetric is expected to be higher
than.
than
The beauty information between East and
The mutual information between East and
The mutual information between eats and
the.
Because knowing that doesn't really
Because knowing the doesn't really
help us predict it's similarly knowing.
help us predict eats. Similarly knowing
It doesn't help us predicting the as
eats doesn't help us predicting the as
well.
An you also can easily see that the
And you also can easily see that the
mutual information between award and
mutual information between  a word and
itself is the largest which is equal to
the muting.
the mutual info.
The entropy of this world.
The entropy of this word.
I saw Becausw in this case the
So because in this case the
reduction is maximum.
reduction is maximum
The cause knowing one would allow the
because knowing one would allow the
because knowing one would allow us to
predicted the other completely so the
predict the other completely so the
conditional entropy is zero.
Therefore the mutual information
reaches its maximum.
It's going to be.
It's going to be
Larger than or equal to the machine
larger than or equal to the machine
larger than or equal to the mutual information
forming between it's an another word.
between it's an another word.
between eats an another word.
between eats and another word.
In other words, picking any other
world, an computer measure information
word, and computing measure information
word, and computing mutual information
between eats and that world.
between eats and that word.
between eats and that word,
You won't get any visual information
you won't get any visual information
you won't get any mutual information
larger than the mutual information
between eats an itself.
between eats and itself.
So now let's think about how to compute
the mutual information.
Now, in order to do that, we often.
Use a different form of mutual
use a different form of mutual
information, and we can mathematically
right the mutual information into the
write the mutual information into the
form shown on this slide, where we
essentially see a formula that computes
what's called KL, divergences or
what's called KL-divergences or
callback labeler divergance.
This is another term in information
theory that measures the divergance
between two distributions.
Now if you look at the formula, it's
also some over many combinations of
also sum over many combinations of
different values of the two random
variables, but inside the sum may name
variables, but inside the sum mainly
we're doing a comparison between 2
joint distributions.
The numerator has the joint actual
observed.
Join the distribution of the two random
variables.
The bottom part of the denominator can
be interpreted as the expected joint
distribution of the two random
variables.
If there were independent.
Because when two random variables are
independent, they joined distribution
is equal to the product of the two
probabilities.
So this comparison would tell us
whether the two variables are indeed
independent if there indeed
independent, then we would expect that
the two are the same.
But if the numerator is different from
the denominator, that would mean the
two variables are not independent, and
that helps measure the Association.
that helps measure the association.
The sum is simply to take into
consideration of all the combinations
of the values of these two random
variables.
In our case, each random variable can
choose one of the two values 01, so we
choose one of the two values 0 or 1, so we
have four combinations here.
So if we look at this form of which
So if we look at this form of mutual
information it shows that the mutual
information measures the diversions of
the actual joint distribution from the
expected distribution under the
independence assumption.
The larger this divergences, the higher
The larger this divergence is, the higher
the mutual information would be.
So now let's further look at the what
are exactly the probabilities involved
in this formula of mutual information.
And here I listed all the probabilities
involved and it's easy for you to
verify that basically we have first 2
probabilities corresponding to the
presence or absence of each word.
So for one, we have two probabilities
So for W1, we have two probabilities
shown here.
They should sum to 1 cause a word can
They should sum to 1 because a word can
either be present or absent in the
segment.
And similarly for the second word, we
also have two probabilities
representing presence or absence of
this world, and there's something one
this word, and there's something one
this word, and this sums to one
as well.
And then finally we have a lot of joint
probabilities that represented the
scenarios of Co occurrences of the two
scenarios of Co-occurrences of the two
words.
And they are shown here.
Right, so in this sum to 1 cause.
Right, so this sums to 1 cause.
Right, so this sums to 1 because
The two words can only have these four
the two words can only have these four
possible scenarios.
Either they both occur.
So in that case both variables will
have a value of one or one of them
occurs.
There are two scenarios.
In these two cases, one of the random
variables will be equal to 1 and the
other would be 0.
And finally we have the scenario when
none of them occurs.
So this is when the two variables
taking a value of 0.
And there's something well, so these
And they're summing up to 1, so these
are the probabilities involved in the
calculation of mutual information.
No, yeah.
here.
Once we know how to calculate these
probabilities, we can easily calculate
the mutual information.
It's also interesting.
It's also interesting to
Note that there are after some
note that there are after some
note that there are some
relations or constraints among these
probabilities, and we already saw two
of them, so the in the previous side
of them, so the in the previous slide
that you have seen that the.
that you have seen that the
Marginal probabilities of these words
marginal probabilities of these words
sum to one, and we also have seen this
constraint.
constraint
That says the two words can only have
that says the two words can only have
these four different scenarios of Co
occurrences, but we also have some
additional constraints listed in the
bottom.
And so, for example, this one means.
And so, for example, this one means
If we add up the probabilities that we
if we add up the probabilities that we
observe, the two words occur together
observe the two words occur together
and the probabilities when the world
and the probabilities when the word
the first word occurs and the signal
the first word occurs and the second
word doesn't occur, we get exactly the
probability that the first word is
observed.
In other words, and when the world is
In other words, and when the word is
observed when the first word is
observed and there are only two
scenarios depending on weather signal,
scenarios depending on weather second
word is also observed.
So this probability captures the first
scenario when the signal word actually
is also observed.
And this captures the signal scenario.
And this captures the second scenario.
And this captures the second scenario
When the Second World is not observed,
when the seond word is not observed,
so we only see the first word.
And it's easy to see the other
equations also follow the same
reasoning.
Now these equations allow us to compute
some probabilities based on other
probabilities.
And this can simplify the computation.
So more specifically, an if we know the
So more specifically, and if we know the
probability that award is present, and
probability that a word is present, and
in this case right?
So if we know this.
And if we know the presence of the
probability of presence of the Second
probability of presence of the second
World, then we can easily compute their
word, then we can easily compute their
absence probability, right?
It's very easy to use this question
It's very easy to use this equation
with that.
to do that.
An so we this will take care of the
computation of these probabilities of
presence or absence of each word.
Now let's look at their joint
distribution, right?
Let's assume that we also have
available the probability that they
available probability that they
occur together.
Now it's easy to see that we can
actually compute the all the rest of
these probabilities based on these.
Specifically, for example, using this
equation, we can compute the
probability that the first word
occurred in the Second World did not,
occurred and the second word did not,
because we know these probabilities in
the boxes.
And similarly, using this equation we
can compute the probability that we
observe only the second word.
And then finally we.
This probability can be calculated by
using this equation, because now this
is known and this is also known and
this is already know right?
this is already known right?
So this can be easier to calculate.
Right, so now this can be calculated.
So this slide shows that we only need
to know how to compute these three
probabilities that are shown in the
boxes, namely the presence of each word
and the Co occurrence of both worlds in
and the Co occurrence of both words in
a segment.
This lecture is about the text based
This lecture is about text based
This lecture is about text based prediction.
prediction.
"
In this lecture we're going to start
talking about mining a different kind
of knowledge as you have, you can see
of knowledge as you, you can see
of knowledge as you, you can see here on this slide.
here on this slide.
Namely, we're going to use text data to
infer values of some other variables in
the real world.
That may not be directly related to the
text, or only remotely related test
text, or only remotely related to text
text, or only remotely related to text data
text, or only remotely related to text data,
text, or only remotely related to text data.
data, so this is very different from
So this is very different from
content analysis or topic mining where
we directly characterize the content of
we directly characterize the content of text.
text.
"
It's also different from opinion mining
or sentiment analysis, which still have
to do with characterizing mostly the
content only.
content only
That we focus more on the subjective
that we focus more on the subjective
that we focus more on the subjective content
content which reflects what we know
which reflects what we know
about the opinion Holder.
about the opinion holder.
But this only provides limited view of
what we can predict in this lecture
what we can predict. In this lecture
ended the following actions we're going
and the following actions we're going
and the following lecture we're going
and the following lecture, we're going
and the following lectures, we're going
to talk more about how we can predict
the more information about the world.
more information about the world.
the more information about the world.
more information about the world.
How can we get sophisticated patterns
of text together with other kinds of
of text together with other kinds of data?
data?
"
It would be useful to first take a look
at the big picture of prediction in
data mining in general an I call this
data mining in general and I call this
data mining in general and I call this data mining loop.
data mining loop.
"
So the picture that you're seeing right
So the picture that you're seeing right now
now is that there are multiple sensors,
is that there are multiple sensors,
including humans, senses to report what
including human sensors to report what
we have seen in the real world in the
we have seen in the real world in the form of data.
form of data.
"
And of course the data in the form of
And of course the data are in the form of
non test data and text data.
non text data and text data.
And our goal is to see if we can
predict the sum values of important the
predict some values of important the
predict some values of important
real world variables that matter to us.
For example, someone's health condition
or the weather, or etc.
So these variables would be important
because we might want to act on that.
We might want to make decisions based
on that.
So how him get from the data to these
So how can we get from the data to these
predicted values?
Well, in general will first have to do
Well, in general we first have to do
data mining.
data mining and
An analysis of the data.
analysis of the data.
Because we in general should treat all
the data that we collected.
In such a prediction problem set up, we
in such a prediction problem set up, we
in such a prediction problem set up,
are very much interested in joining the
are very much interested in join the
we are very much interested in join the
we are very much interested in joint
mining of non text and text data.
We should mine all the data together.
And then through the analysis, we
generally can generate the multiple
predictors of this interesting,
predictors of this interesting
valuable to us, and we call these
variables to us, and we call these
variable to us, and we call these
features.
And these features can then be put into
a predictive model to actually predict
the value of any interesting variable.
So this then allows us to change the
world and so this basically is the
general process for making a position
general process for making a prediction
based on data including tax there.
based on data including text data.
based on data, including text data.
Now it's important to emphasize that
human plays very important role in this
human actually plays very important role in this
process.
Especially because of the involvement
of text data and so human first would
of text data. And so human first would
be involved in the mining of the data.
It will control the generation of these
features.
And also help us understand the text it
And also help us understand the text data
up because it tasted are created to be
because text data are created to be
consumed by humans.
Humans are the best in consuming or
interpreting text data.
But went there, of course a lot of text
But when there are, of course a lot of text
But when there are, of course a lot of text data
later than machines have to help, and
than machines have to help, and
that's why we need to do text data
that's why we need to do text data mining.
mining.
"
Sometimes machines can see patterns in
a lot of data that humans may not see,
but in general human would play an
important role in analyzing text data
in all applications.
Next human also must be involved in
predictive model building and adjusting
or testing.
So in particular we will have a lot of
domain knowledge about the problem of
prediction that we can build into this
predictive model, and then next.
predictive model, and then next,
Of course, when we have predicted
of course, when we have predicted
values for the variables, than humans
values for the variables, then humans
would be involved in taking actions to
change the world or make decisions
based on these predictive values.
And finally, it's interesting that
human could be also involved in
controlling the sensors.
Ann, this is so that we can adjust the
And, this is so that we can adjust the
sensors to collect the most useful data
for prediction.
So that's why I called this data mining
loop becausw as we perturb the sensors
loop because as we perturb the sensors
to collect the new data and more useful
data than we will obtain more data for
data then we will obtain more data for
prediction.
This data generally will help us
improve the prediction accuracy and in
this loop are humans will recognize
what additional data needs to be
collected and machines would of course
help humans identify what data should
be collected next.
In general, we want to collect data
that are most useful for learning.
And this there was actually a subarea
And this there is actually a subarea
machine learning called active learning
in machine learning called active learning
that has to do with this.
How do you identify data points?
That would be most helpful for machine
learning programs if you can label
them.
them, right.
So in general, you can see there's a
loop here from data acquisition to data
analysis or data mining to prediction
or values, and to take actions to
of values, and to take actions to
change the world and then observe what
happens.
And then you can then decide what
additional data.
Have to be collected by adjusting the
census or from the prediction errors.
snsor or from the prediction errors.
snsor. Or from the prediction errors
sensor. Or from the prediction errors
You can also know what additional data
you can also know what additional data
we need to acquire in order to improve
the accuracy of prediction and this big
the accuracy of prediction. And this big
picture is actually very general and
it's reflecting a lot of important
applications of big data.
So it's useful to keep that in mind
while we're looking at some text mining
while we're looking at some text mining techniques.
techniques.
"
So from text mining perspective and
we're interested in text based
prediction, of course sometimes text
alone can make predictions and this is
alone can make predictions. And this is
most useful for prediction about the
most useful for prediction about
human behavior or human preferences or
opinions.
But in general text it will be put
But in general text data will be put
together with non text data.
So the interesting questions here would
be first how can we design effective
predictors?
And how do we generate such effective
predictors from text?
Hey, this question has been addressed
... This question has been addressed
This question has been addressed
it to some extent in some previous
to some extent in some previous
lectures where we talked about what
kind of features we can design for test
kind of features we can design for text
data.
It has also been addressed to some
extent by talking about the other
knowledge that we can mine from text.
So for example, topic mining can be
very useful to generate the patterns or
topic based indicators or predictors
that can be further fed into a
predictive model.
So topics can be intermediate
representation of text.
That would allow us to design high
level features or predictors that are
useful for prediction of some other
variable.
It maybe, although it's generated from
original text data, it provides a much
better representation of the problem
and it serves as more effective
predictors.
And similarly, sentiment analysis can
lead to such predictors as well.
So those other than a mining or text
So those are the than a mining or text
So those are the data mining or text
mining algorithms can be used to
generate the predictors.
The other question is how can we join
remind text an non text data together?
mine text and non text data together?
Now this is a question that we have not
addressed yet so in this lecture and
addressed yet. So in this lecture and
then the following lectures were going
the following lectures were going
the following lectures we're going
to address this problem because this is
where we can generate the much more
enriched features for prediction and
allows us to review a lot of
interesting knowledge about the world.
These patterns that are generated from
text and non text data themselves.
text and non text data themselves
Can sometimes already be useful for
can sometimes already be useful for
prediction, but when they are put
together with many other predictors
they can really help improving the
accuracy of prediction.
Basically you can see text based
prediction character serve as a unified
framework to combine many text mining
and analysis techniques, including
topic mining and content.
topic mining and content,
Any content, mining techniques or
any content mining techniques or
sentiment analysis.
The goal here is mainly to involve
The goal here is mainly to infer
values of real world variables.
But in order to achieve the goal, we
can do some other preparations and
these are sub tasks.
So one sub task could be mine mine the
So one sub task could be mine, mine the
content of text data like topic mining.
And the other could be the mind
And the other could be to mine
knowledge about the observer so
sentiment and answer opinion, answers
sentiment analysis or opinion analysis
sentiment analysis or opinion analysis.
and both can help provide predictors
And both can help provide predictors
for the prediction problem.
And of course we can also add non text
data directly to the predictive model,
but then not text that also helps
but then not text data also helps
but then non text data also helps
provide context for text analysis and
provide context for text analysis
that further improves the topic of
that further improves the topic
mining and the opinion analysis and
mining and the opinion analysis. And
mining and the opinion analysis.
such improvement often leads to more
And such improvement often leads to more
effective predictors for our problems
it would enlarge the space of patterns
of opinions or topics that we can mine
from text.
As well as discuss more later, so the
As we'll discuss more later, so the
join the analysis of text and non text
join analysis of text and non text
join analysis of text and non text data
data can be actually understood from 2
can be actually understood from 2
perspectives.
In one perspective, we can see none.
In one perspective, we can see non
In one perspective, we can see
Text data can help text mining.
text data can help text mining.
non text data can help text mining.
Be cause not text it out can provide a
Be cause non text data can provide a
Because non text data can provide a
Because non text data can provide
context for mining tax data.
context for mining text data.
a context for mining text data.
Provide a way to partition tested in
Provide a way to partition text data in
different ways and this leads to a
different ways, and this leads to a
number of techniques for contextual
text mining.
And let's remind text in the context
And that's to mine text in the context
defined by non text data.
And you can see this reference here.
And you can see this reference here
For a large body of work in this
for a large body of work in this
direction, and we're going to highlight
some of them in the next lectures.
Now the other perspective is text.
Now the other perspective is text
Now the other perspective is text data
Data can help, but not text their
data can help, but not text their
data can help non text data
can help non text data
mining as well.
And this is because texted him help
And this is because text data can help
interpret patterns discovered from lab
interpret patterns discovered from non
test data.
text data.
This save you discover some frequent
This help you discover some frequent
This helps discover some frequent
patterns from non text there.
patterns from non text data.
Now we can use the text data that are
associated with instances where the
pattern occurs as well as tax data that
pattern occurs as well as text data that
are associated with instances where the
pattern doesn't occur.
And this gives us two sets of text data
and then we can see what's the
difference and this difference impacts
difference and this difference in text
data is interpretable because text
data is interpretable because text content
content.
content
"
Is easy to digest and that difference
is easy to digest and that difference
might suggest some meaning for this
pattern that we've found it from non
pattern that we've found from non
text data, so that helps interpret such
patterns.
And this technique is called pattern
annotation.
An you can see this reference listed
And, you can see this reference listed
here for more detail.
So here are the reference that I just
mentioned.
The first is reference for pattern
The first is reference for pattern annotation.
annotation.
"
The second are is childhood maze
The second is a Qiaozhu Mei
dissertation on contextual text
reminding it had contains a large body
mining it had contains a large body
mining. it had contains a large body
mining. It had contains a large body
mining. It contains a large body
of work on contextual text mining
of work on contextual text mining techniques.
techniques.
"
This lecture is about the methods for
text categorization.
So in this lecture were going to
discuss how to do text look at
discuss how to do text
validation.
categorization.
1st.
There are many methods for tax
There are many methods for text
localization.
categorization
In such a method, the idea is to
determine the category based on some
rules that we design carefully to
reflect the domain knowledge about the
categorization problem.
So, for example, if you want to do
topical categorisation for news
articles, you can say if the news
article mentions word like gay Man
article mentions word like game and
Sports three times that we're going to
say it's about sports.
Things like that and this would allow
us to deterministically decide which
category A document should be put into.
LA such a strategy would work well if
Now such a strategy would work well if
the following conditions hold.
First, the categories must be very well
defined, and this allows the person to
clearly decide the category based on
some clear rules.
Secondly, the categories.
Secondly, the categories
Have to be easy to distinguish based on
have to be easy to distinguish based on
surface features in text, so that means
superficial features like keywords or
punctuation's or whatever.
punctuations or whatever.
You can easily identifying text data.
You can easily identify text data.
For example, if there is some special
vocabulary that is known to only occur
in a particular category, and that
would be most effective because we can
easily use such a vocabulary or pattern
of such a vocabulary to recognize this
category.
Now we also should have sufficient
knowledge.
For designing this walls and so if
For designing these walls and so if
For designing these rules and so if
that's the case, then such a method can
be effective, and so it does have a
provisions in some domains and
sometimes.
However in general there are several
problems with this approach.
First, of course it's labor intensive.
It requires a lot of manual work.
Obviously we can't do this for all
kinds of categorization problems.
We have to do it.
We have to do it
From scratch it for a different
From scratch for a different
problem, becauses different rules would
be needed so it doesn't scale up as
well.
Our Secondly, it cannot handle
Secondly, it cannot handle
uncertainties in rules.
Often the rules and 100% reliable take
Often the rules aren't 100% reliable take
for example, and looking at the
occurrences of words in text and try to
decide the topic.
It's actually very hard to have 1%
correct the rule.
So for example, you can say if it has
games, sports, basketball, then for
sure it's about sports.
But one can also imagine some tax
But one can also imagine some text
articles that mention these keywords.
But that may not be exactly about the
sports, or only marginally touching
sports.
The main topic could be another topic,
different topic then sports.
So that's one disadvantage of this
approach, and then finally the rules
may be inconsistent and this would need
to concern about robustness more
specifically, and sometimes the results
of capitalization may be different
of categorization may be different
depending on which rule to be applied.
So in that case then your face
So in that case then you will face
uncertainty and you will also have to
decide the order of applying the rules
or combination of results that are
contradictory.
So all these.
Problems with this approach, and it
turns out that the both problems can be
solved or alleviated by using machine
learning.
So these machine learning methods are
more automatic, but I still put
automatic in quotation marks cause
they're not really completely automatic
because it still require many work.
because it still require manual work.
More specifically, we have to use human
experts to help in two ways.
First, the human experts must annotate
datasets with category labels, will
tell the computer which documents
should not receive which categories.
And this is called a training data.
And then Secondly the human experts
also need to provide a set of features
to represent each text object that can
potentially provide accrue about
potentially provide a clue about the
category.
So we need to provide some basic
features for the computers to look
into.
And in the case of text, natural choice
would be the words.
So using each word as a feature is a
very common choice to start with.
But of course there are other surface
But of course there are other sophisticated
to get the features like a phrases or
features like a phrases or
features like phrases or
even policy which tags or even
even policy feature tags or even
syntactic structures.
So once human experts can provide this,
then we can use machine learning to
learn soft rules for categorization
from the training data.
So soft rules just means we're going to
still decide which category should be
assigned to the document.
But it's not going to be used using a
rule that is deterministic, so we might
use something similar to saying that if
it matches game sports many times, it's
likely to be a sports.
But we're not going to say exactly for
sure, but instead we're going to use
probabilities or weights so that we can
combine multiple evidences, so the
learning process basically is going to
figure out which features are most
useful for separating different
categories.
And it's going to also figure out how
to optimally combine features to
minimize errors of categorisation on
the training data, so the training data
as you can see very important.
It's the basis for learning.
And then the A classifier can be
And then the train classifier can be
applied to a new texture object to
applied to a new text object to
predict the most likely category, and
that's the simulate the prediction of
that's to simulate the prediction of
what human would assign to this text
what a human would assign to this text
object.
If the human would to make a judgement.
So when we use machine learning for
taxable catalyzation, we can also talk
text categorization, we can also talk
about the problem in the general
setting of supervised learning.
So the setup is.
To learn a classifier to map a value of
X into a map of Y.
So here X is all that extra objects.
So here X is all the text objects.
And why is all the categories a set of
And Y is all the categories a set of
categories, so the classifier would
take any value in X as input and we
generate the value in Y as output, and
we hope the output Y would be the right
category 4X, and here correct of course
category for X, and here correct of course
is judged based on the training data,
so that's the general goal, like in all
the machine learning problems or
supervised learning problems where you
are given some examples of.
are given some examples of
Input an output for function and then
Input and output for function and then
the computer is going to figure out how
the function behaves like based on
these examples and then try to be able
to compute the values for future access
that we have not seen.
So in general, are all methods would
So in general, all methods would
rely on discriminating features of text
objects to distinguish different
categories, so that's why these
features are very important and they
have to be provided by humans.
And they will also combine multiple
features in awaiting matter with
features in a weighted matter with
weights to be optimized to minimize the
errors on the training data.
So ultimately, the learning processes
optimization problem and the objective
function is often tide to the errors on
the training data.
Different methods tend to vary in their
ways of measuring the errors on the
training data.
They might optimize a different object
function, which is often also called a
loss function or cost function.
They also tend to vary in their ways of
combining the features, so linear
combination for example is simple is
often used.
But they're not as powerful as non
linear combination, but nonlinear
models might be more complex for
training.
So there are tradeoffs as well, but
that would lead to different variations
of.
Many variations of these learning
methods.
So in general, we can distribute the
So in general, we can distinguish the
two kinds of classifiers and high level
two kinds of classifiers at a high level
one is going to generative classifiers.
The other is called discriminative
classifiers.
The generative classifiers try to learn
what the data looks like in each
category.
So it attempts to model the join the
distribution of the data and the label
X&Y.
Anne, this can then be factored out to
And, this can then be factored out to
a product of Y.
The distribution of labels and join the
probability of sorry the conditional
probability of X given Y so it's wide.
probability of X given Y so it's Y.
So we first model distribution of
labels and then we model how the data
is generated given a particular label
here.
And once we can estimate these models,
then we can compute the this
then we can compute this
conditional probability of label given
data based on.
The probability of that given label.
The probability of data given label.
And the label distribution here by
using the base war.
using the base rule.
Now this is the most important thing
'cause this conditional probability of
the label can then be used directly to
decide which label is most likely.
So in such approaches, the objective
function is actually hold, so we model
function is actually likelihood, so we model
the data are generated, so only thus it
how the data are generated, so only thus it
only indirectly captures the training
errors.
But if we can model the data in each
category accurately, then we can also
classify accurately.
One example is naive Bayes classifier.
In this case.
The other kind of approaches are called
discriminative classifiers.
These classifiers try to learn what
features separate categories, so they
directed to tackle the problem of
directly tackle the problem of
categorisation or separation of
classes.
So sorry for the problem.
So these discriminative classifiers
attempted to model the.
Conditional.
Probability of the label given the data
point directly.
So the objective function tends to
directly measure the errors of
categorisation on the training data.
Some examples include the logistical
regression support vector machines and
the K nearest neighbors.
We will cover some of these classifiers
in detail in the next few lectures.
So looking at the text reminding
So looking at the text mining
problem more closely, we see that the
problem is similar to general data
mining, except that we'll be focusing
more on text data.
And we're going to have text mining
algorithms to help us to turn text data
into actionable knowledge that we can
use in real world.
use in (the) real world.
Especially for decisionmaking or for
Especially for decision making or for
completing whatever tasks that require
text data to support, but cause in
text data to support, but because in
text data to support, now because in
general in many real world problems of
their mining, we also tend to have
data mining, we also tend to have
other kinds of data that are non
textual.
So a more general picture would be to
include non text data as well.
And for this reason, we might be
concerned with joint mining of text and
non text data and so in this course
we're going to focus more on text
mining.
But we can also touch how to join the
analysis of both tax data and non text
analysis of both text data and non text
analysis of both text data and non-text
data with this problem definition we
data. With this problem definition we
can now look at the landscape of the
topics in text mining analytics.
Now this slide shows the process of
generating text later in more detail.
generating text data in more detail.
Most specifically, human sensor or
human observer would look at the world
from some perspective.
Different people would be looking at
the world from different angles and
they will pay attention to different
things.
The same person at different time might
The same person at a different time might
also pay attention to different aspects
of the observed world.
And so the human sensor would perceive
the world from some perspective.
And that human.
And that human...
The sensor would then form a view of
the world and that can be called the
observed.
observed
The world.
world.
Of course this would be different from
the real world because of the
perspective that the person has taken.
This can often be biased also.
Now the observable world can be.
Now the observable world can be
Represented As for example entity
represented ss for example entity
represented as for example entity
relation graphs or more in a more
general way, using knowledge
representation language.
But in general, this is basically what
a person has in mind about the world,
and we don't really know what exactly
it looks like, of course.
But then the human would express what
the person has observed using a natural
language such as English, and the
result is text data.
Of course, the person could have used a
different language to express what he
or she has observed.
In that case, we might have texted are
In that case, we might have text data
of mixed languages for different
languages.
So the main goal of text mining is
actually to revert this process of
generating test data.
And we hope to be able to uncover some
aspect in this process.
And so specifically we can think about
the mining, for example, knowledge
about the language.
And that means by looking at the text
And that means by looking at text
data in English, we may be able to
discover something about English.
discover something about English...
Some usage of English.
Some usage of English...
Have some patterns of English.
some patterns of English.
Some patterns of English.
So this is 1 type of mining problems
where the result is some knowledge
about language which may be useful.
about language which may be useful
In various ways.
in various ways.
If you look at the picture, we can also
then my knowledge about the Observer
then mine knowledge about the Observer
then mine knowledge about the Observed
"then mine knowledge about the ""Observed"
Award.
World.
"World""."
As so, this has much to do with mining
the content of text data.
And we're going to look at the what the
we're going to look at the what the
We're going to look at the what the
text it all about and then try to get
text data are about and then try to get
the essence of it.
Or extracting high quality information
about a particular aspect of the world
that we're interested in.
For example, everything that has been
said about a particular person or
particular entity, and this can be
regarded as mining content to describe
the observed world in the users minor
the observed world in the users mind
the observed world in the user's mind
the observed world in the user's mind,
in the person's mind.
If you look further than you can also.
If you look further than you can also
If you look further then you can also
Imagine we can mine knowledge about
imagine we can mine knowledge about
this, observers himself or herself.
this observer himself or herself.
So this has also to do with using text
data to infer the some properties of
data to infer some properties of
this person.
And these properties could include the
mood of the person or sentiment of the
person.
And note that we distinguish the
observed the world from the person
becausw text that I can't describe what
because text that I can't describe what
because text data can't describe what
because text data can describe what
the person has observed in the
the person has observed in an
objective way, but the description can
be also subject with sentiment, and so
in general you can imagine the text
data would contain some factual
descriptions of the world plus some
subjective comments, so that's why it's
also possible to do text mining to my
also possible to do text mining to mine
knowledge.
knowledge
About the observer.
about the observer.
Finally, if you look at the picture to
the left side of this picture, then you
can see we can certainly also say
something about the real world, right?
So indeed we can do text mining to
infer other real world variables, and
this is often called predictive
analytics.
And we want to predict the value of
certain interesting variables.
So this picture basically covered
multiple types of knowledge that we can
mine from texture in general.
mine from text in general.
When we infer other real world
variables, we could also use some of
the results from mining textile as
the results from mining text data as
intermediate results to help the
prediction.
For example, after we mind the content
For example, after we mine the content
of text data, we might generate some
summary of content, and that summary
could be that used to help us predict
could be then used to help us predict
the variables of.
the variables of
The real world.
the real world.
Now of course, this is still generated
from the original tax data, but I want
from the original text data, but I want
to emphasize here that often the
processing of text data to generate
there's some features that can help
some features that can help
with the prediction is very important.
with the prediction, is very important.
And that's why here we show that the
results of some other mining tasks,
including mining the content of text
data and mining knowledge above the
observer can all be very helpful for
prediction.
In fact, when we have a long text data,
In fact, when we have a non text data,
In fact, when we have a non-text data,
we could also use the non text data to
we could also use the non-text data to
help prediction.
And of course, it depends on the
problem.
In general, non tested it all can be
In general, non text data it all can be
In general, non text data it can be
In general, non text data can be
In general, non-text data can be
very important for such prediction
tasks.
For example, if you want to predict the
stocks.
Stock prices or changes of stock prices
based on discussion in the news
articles or in social media, then this
is an example of using text data to
predict some other real world
variables.
Now in this case, obviously the
historical stock price data would be
very important for this prediction, and
so that's example of non text data that
so that's example of non-text data that
would be.
would be
Very useful for the prediction and we
very useful for the prediction and we
can combine both kinds of data to make
the prediction.
Now long text data out can be also
Now non text data can be also
Now non-text data can be also
useful for analyzing text by supplying
context.
When we look at the text, data alone
When we look at the text data alone
will be mostly looking at the content
and opinions expressed in text.
But text that are generally have also
But text data generally have also
context associated.
For example, at the time, the location
For example, the time, the location
For example, the time, the location,
of that associated with the test data
of that associated with the text data
and these are useful context
information.
And the Contacts that can provide
And the context can provide
interesting angles for analyzing text
data.
For example, we might partition tests
For example, we might partition text
that are in two different time periods
data in two different time periods
data into different time periods
because of the availability of time.
Now we can analyze text data in each
time period and then make a comparison.
Similarly, we can partition tested all
Similarly, we can partition text data
based on locations or any matter that's
based on locations or any meta data that's
based on locations or any metadata that's
associated.
associated
To form interesting comparison
to form interesting comparison
scenarios.
So in this sense are not text data can
So in this sense non text data can
So in this sense, non-text data can
actually provide interesting angles or
perspectives for text analysis, and we
perspectives for text analysis, and
can help us make context sensitive.
can help us make context sensitive
Analysis of content or the language
analysis of content or the language
usage or the.
usage or the
Opinions about the observer or the
opinions about the observer or the
authors of text data.
We could analyze the sentiment.
We could analyze the sentiment
In different context, so this is fairly
in different context, so this is fairly
general landscape of the topics in text
mining and analytics.
In this course we're going to
selectively cover some of those topics.
We are today hope to cover most of
We actually hope to cover most of
these general topics.
First, we can do cover natural language
First, we are going to cover natural language
processing very briefly becausw.
processing very briefly because
This has to do with understanding text
this has to do with understanding text
data, and this determines how we can
represent text after text mining.
represent text for text mining.
Second, we're going to talk about how
to mine word associations from text
data and what associations is a form of
data and word associations is a form of
useful lexical knowledge about a
language.
Third, we're going to talk about the
topic mining.
topic mining
An analysis, and this is only one way
and analysis, and this is only one way
to analyze content of text, but it's a
very useful way of analyzing content.
It's also one of the most useful
techniques in text mining.
And then we're going to talk about the.
And then we're going to talk about
Opinion mining and sentiment analysis.
opinion mining and sentiment analysis.
So this can be regarded as one example
of mining knowledge about the observer.
And finally, we can do cover a tax
And finally, we are going to cover a text
based prediction problems where we try
to predict some real world variable
based on tax data.
based on text data.
So this slide also serves as a road map
for this course.
And will use this as outline for the
topics that will cover in the rest of
this course.
This lecture is about the latent aspect
rating analysis or opinion mining and
sentiment analysis.
In this lecture, we're going to
continue discussing opinion mining and
sentiment analysis.
In particular, we're going to
introduce.
Late in the aspect of rating analysis,
which allows us to perform detailed
analysis of reviews with overall
ratings.
First, motivation.
Here are two reviews that you often see
on the Internet about the Hotel Ann.
on the Internet about the Hotel and
You see some overall ratings.
In this case, both reviewers have given
five stars.
And of course there are also reviews
that are in text.
Now, if you just look at these reviews,
it's not very clear whether a hotel is
good for its location or for its
service, and it's also unclear why are.
If you are like this hotel.
So what we want to do is to decompose
this overall rating.
Into ratings on different aspects such
as value, rooms, location and service.
So if we can decompose overrating two
ratings on these different aspects.
Then we can obtain more detailed
understanding of the reviewers opinions
about the hotel.
And this would also allow us to rank
hotels along different dimensions, such
as valuable rooms, but in general such
detailed understanding would reveal
more information about the users,
preferences, reviews, preferences and
also we can understand better how
reviewers view this hotel from
different perspectives.
Now, not only do we want to.
Infer this aspect ratings.
We also want to infer the aspect of
weights, so some reviewers may care
more about values as opposed to
service, and that would be a case like
what's shown on the left for the weight
distribution where you can see a lot of
weight is placed on value.
But others might care more about
service and therefore they might place
more weight on service then value.
Now, the reason why this is also
important that is be cause do you think
about a five star on value?
It might still be very expensive if the
reviewer cares a lot about service,
right?
For this kind of service, this price is
good, so the reviewer might give it a
five star.
But if reviewer really cares about the
value of the hotel, then the five star
most likely would mean really cheaper
prices.
So in order to interpret the ratings on
different aspects accurately, we also
need to know these aspect weights.
When they are combined together, we can
have a more detailed understanding of
the opinion.
So the task here is to get these
reviews and their overall ratings as
input and then generate the both the
aspect ratings, decomposed aspect
ratings and the aspect of weights as
output.
And this is a problem called latent
aspect rating analysis.
So the task in general is given a set
of review articles about the topic with
overall ratings.
An we hope to generate the three
things.
One is the major aspects comment on in
the reviews.
The second is the ratings on each
aspect, such as value and room or
service.
And 3rd is the relative weights placed
on different aspects by the reviewers,
and this task has a lot of
applications.
If we can do this and we would enable a
lot of applications, I just listed some
here and later.
I will show you some results.
And for example, we can do opinion
based and the ranking.
We can generate a aspect level opinion
summary.
We can also analyze reviewers
preferences, compare them or compare
their preferences on different hotels.
And we can do personalized
recommendation of products.
So of course the question is how can we
solve this problem?
Now, as in other cases of these
advanced topics, we won't have time to
really cover the technique in detail,
but I'm going to give a press basic
introduction to the technique developed
for this problem.
So first we're going to talk about how
to solve the problem in two stages.
Later, we're going to also mention that
we can do this in the unified model.
Now take this review with the overall
reading as input.
What we want to do is first we're going
to segment the aspects.
So we're going to figure out what words
are talking about location in what was
are talking about location in what words
I talking about, the room conditioning,
are talking about, the room conditioning,
etc.
So with this we would be able to obtain
aspect segments.
In particular, we're going to obtain
the counts of all the words in each
segment, and this is denoted by C
supply of WND.
This can be done by using seed words
like location and room.
Or price to retrieve the relevant the
segments and then from those segments
we can further mine correlated words.
With these seed words and that would
allow us to segment the text into
segments.
Discussing different aspects, but of
course later as we would see, we can
also use topic models to do the
segmentation, But anyway, that's the
first stage where we would obtain the
counts of words in each segment.
In the sickness stage, which is called
In the segmentation stage, which is called
latent rating regression, we're going
to use these words and their
frequencies in different aspects to
predict the overall rating, and this
prediction happens in two stages.
In the first stage, we're going to use
the sentiment weights of these words in
each aspect to predict the aspect
rating.
So, for example, if in the discussion
of location using a word like amazing
mentioned many times and it has a high
weight.
For example, here is 3.9.
Then it would increase the aspect
rating for location.
But another word, like a far, which is
a negative weight if it's mentioned
many times and it will decrease the
rating.
So the aspect rating is assumed to be a
weighted combination of these word
frequencies where the weights are the
sentiment weights on the words.
Now of course these sentiment weights
might be different for different
aspects.
So we have for each aspect a set of
sentiment weights.
As shown here, and that's denoted by
better sub I and W.
beta sub I and W.
In the second stage, or in a second
step, we're going to assume that the
overall rating is simply weighted
combination of these aspect ratings.
So we're going to assume we have aspect
weights in order by of our supply of D.
weights in order by of R sub of D.
And this would be used to take a
weighted average of the aspect ratings,
which are denoted by our supply of the.
And we can assume the overall rating is
simply a weighted average of this
aspect ratings.
So this setup allows us to predict the
overall rating based on the observed
word frequencies.
So on the left side you will see all
these observa information, the arts,
these observed information, the arts,
the and the count.
But on the right side you see all the
information that we're interested in is
actually latent.
So we hope to discover them.
Now this is a typical case of
generating model where we would embed
the interesting variables in the
generating model.
And then we're going to set up a
generation probability for the overall
rating given the observed words.
And then of course, then we can adjust
these parameter values including
baiters, arzan, Alpha eyes.
beta, r, alpha i.
betas, rs, alpha i.
In order to maximize the probability of
the data in this case, the conditional
probability of the observed rating
given the document.
And so we have seen such cases before
in, for example, PSA, where we predict
in, for example, PLSA, where we predict
the text data.
But here we predicting the rating and
the parameters of course are also very
different.
But if you can see if we can uncover
these parameters, that would be nice
because also buy of D is precisely the
because also r of D is precisely the
because also R of D is precisely the
aspect ratings that we want to get, and
these are decomposer ratings on
different aspects of our sub ID is
precisely the aspect weights that we
hope to get.
As a by product that will also get the
As a bi product that will also get the
beta vector and these are the aspects
of specifica sentiment, weights of
words, so more formally.
They thought we are modeling.
Here is a set of review documents with
overall ratings.
And each review documents denoted by AT
and overall rating is Dino Dilipbhai, R
and overall rating is denoted by R
Subte and these pre segmented into K as
sub D and these pre segmented into K as
their segments and we're going to use C
sub I of WND to denote the count of
sub I of R and to denote the count of
sub W and D and to denote the count of
world W in aspect segment I.
Of course it's zero if the world
doesn't occur in the segment.
Now the model is going to predict the
rating based on the.
So we are interested in the conditional
probability of our sub T given D.
probability of R sub T given D.
And this model is set up as follows.
So all of this is assumed to follow a
normal distribution with a mean that
denotes actually await the average of
the aspect ratings.
Our supply of D as shown here is normal
R sub of D as shown here is normal
distribution has a variance of or
square.
Now of course, this is just what our
assumption in the actual reading is not
necessary generating this way.
But as always when we make this
assumption, we have a formal way to
model the problem, and that allows us
to computer interesting quantities.
to compute interesting quantities.
In this case, the aspect ratings and
aspect of weights.
Now the aspect rating as you see on the
second line is assumed to be weighted
sum of these weights where the weight
is just sentiment wait.
So.
As I said, the overall rating is
assumed to be a weighted average of
aspect ratings.
Now this offer.
Now this alpha
Values of a supply of the ordinary
Values of a alpha sub of D
together by our vector that depends on
D is the document specific weights and
we can assume this factor itself is
drawn from another multivariate
Gaussian distribution with mean denoted
by a mule vector and covariance matrix
Sigma, yeah.
Now, so this means when we generate our
overall rating, we're going to first
draw.
A set of our values from this
A set of other values from this
multivariate Gaussian prior
distribution and once we get these
awful values were going to use, then
alpha values were going to use, then
the weighted average of aspect ratings
as the mean here to use the normal
distribution.
And to generate the overall rating.
Now the aspect rating as I just said is
the sum of the sentiment weights of
words in their spectrum.
Note that here the sentiment weights
are specifically to aspects, so beta is
indexed by EI.
indexed by I.
And As for aspect.
And that gives us way to model
different segment of award.
This is neither because of the same
word might have positive sentiment for
once back, but negative sentiment for
another aspect.
It's also useful to then see.
What premise we have here, but I just
said that the better sub IW gives us a
said that the beta sub I W gives us a
aspect specific sentiment of W.
So obviously that's one of the
important parameters, but in general we
can see we have these parameters.
The beta values that Delta and then the
mu and Sigma.
So next question is, how can we
estimate these parameters and so we
collectively denote all the parameters
by Lambda here.
Now we can, as usual, use.
Now we can, as usual, use
The maximum likelihood is made and this
will give us the settings of this
premise that with the maximizer
observed.
Observer ratings condition on their
respective reviews.
An of course, this would then give us
And of course, this would then give us
all the useful variables that will
interest in computing.
So now more specifically, we can now
once we estimate the parameters, we can
easily compute the abstract rating for
aspect I or supply of D and that's
aspect I or sub I of D and that's
simply to take all the words that
occurred in the segment I and then take
their accounts and then multiply that
by the sentiment weight of each word
and take a son.
and take a sum.
So of course this counter would be 04
words that are not occurring in the
aspect I, and that's why we can take
some over all the words in the
vocabulary.
Now, what about the aspect weights?
Oversupply of D?
Alpha sub I of D?
It's not part of our parameter, right?
So we have to use Bayesian inference to
compute it.
And in this case we can use the maximum
a posteriori.
2 computer this awful value.
2 computer this alpha value.
Basically we're going to maximize the
product of the prior of our according
to our assumed market valued Gaussian
distribution and the likelihood in this
case likely is the probability of
generating this observed overall rating
given this particular Alpha value and
some other parameters.
As you see here.
So for more details about this model,
you can read this paper cited here.
This lecture is about the topic mining
analysis.
and analysis.
We are going to talk about using a term
as topic.
This is a slide that you have seen in
the earlier lecture where we defined
the task of top mining an analysis.
the task of top mining and analysis.
We also raised the question how do we
exactly define the topic theater?
exactly define the topic theta?
So in this lecture were going to offer
So in this lecture we are going to offer
one way to define it, and that's our
initial idea.
Our idea here is defining a topic
Our idea here is to define a topic
simply as a term.
A term can be a word or a phrase.
An in general, we can use these terms
And in general, we can use these terms
to describe topics, so our first
thought is just to define a topic as
one term.
For example, we might have terms like
sports.
sports,
Travel or science as you see here.
travel or science as you see here.
Now if we define a topic in this way,
we can analyze the coverage of such
topics in each document.
Here, for example, we might want to
discover toward extend document one
discover to what extent document 1
covers sports and we found that 30% of
the content of documents about sports.
the content of document 1 is about sports.
And 12% is about the travel etc.
We might also discover Document 2 does
not cover sports at all, so the
coverage is zero, etc.
So now of course, as we discussed.
In the task definition for topic mining
and analysis, we have two tasks, one to
and analysis, we have two tasks, one is to
discover the topics and the 2nd is to
analyze the coverage.
So let's first think about how we can
discover topics if we represent each
topic by a term.
So that means we need to mine K topical
terms from a collection.
Now there are of course many different
ways of doing that an.
ways of doing that and.
We're going to talk about a natural way
of the net, which is also likely
of doing that, which is also likely
effective.
So first we're going to pause the text
So first we're going to parse the text
data in the collection to obtain
candidate terms.
Here, candy returns can be words or
Here, candidate terms can be words or
phrases.
Let's say the simplest solution is to
just take each word as a term.
These words then become candidate
topics.
Then we're going to design your scoring
Then we're going to design a scoring
function to measure how good each term
is as a topic.
So how can we design such a function?
Well, there are many things that we can
consider.
For example, we can use pure statistics
to design such as scoring function.
Intuitively, we would like to favor
representative terms, meaning terms
that can represent a lot of content in
the collection.
So that would mean we want to favor a
frequent term.
However, if we simply use the frequency
to design the scoring function, then
the highest score terms would be
the highest scored terms would be
general terms or functional terms, the
"general terms or functional terms, like ""the"", ""a"""
etc.
Those terms are very frequently
Those terms are very frequent
English.
in English.
So we also want to avoid having such
words on the top, so we want to
penalize such words, but in general
would like the favor terms that are
fairly frequently but not so frequent.
So a particular approach you could be
So a particular approach could be
based on, TF IDF weighting from
based on, TF-IDF weighting from
retrieval.
Anti F stands for term frequency idea
And TF stands for term frequency idea
And TF stands for term frequency
for stands for inverse document
IDF stands for inverse document
frequency and we talked about some of
these ideas in the lectures about the
discovery of what associations.
discovery of word associations.
So these are statistical methods,
meaning that the function is defined
mostly based on statistics.
So the scoring function would be very
general.
It can be applied to any language and
it helps.
any text.
But when we apply such approach to a
But when we apply such an approach to a
particular problem, we might also be
able to leverage some domain specific
heuristics.
For example, in news we might favor
title words.
Actually, in general, we might want to
favor title words becauses the authors
tend to use the title to describe the
topic of article.
topic of an article.
If we're dealing with tweets, we could
also favor hashtags which.
also favor hashtags which are
Invented to denote topics so naturally
invented to denote topics so naturally
invented to denote topics. So naturally
hashtags can be good candidates for
representing topics.
Anyway, after we have this designer
Anyway, after we have designed the
scoring function, then we can discover
the key topical terms by simply picking
the K topical terms by simply picking
K terms with the highest scores.
Now of course we might encounter a
situation where the highest scorer
situation where the highest scored
terms are all very similar.
They are semantically similar or
closely related or even synonyms.
Right, so that's not desirable, so we
So that's not desirable, so we
also want to have coverage over all the
content in the collection.
So we would like to remove redundancy
an one way to do that is to do a greedy
and one way to do that is to do a greedy
algorithm, which is sometimes called
maximal marginal relevance ranking.
Basically, the idea is to go down the
list based on our scoring function an
gradually take terms to collect the key
gradually take terms to collect the K
topical terms.
The first term of course will be picked
when we pick the next term.
We're going to look at the what terms
have already been picked and try to
avoid picking a term that's too
avoid picking a term that's too similar.
similar.
So why we are considering the rank
So why we are considering the
So while we are considering the
ranking of term in the list?
ranking of term in the list,
We also consider in the redundancy of
we're also consider in the redundancy of
the candidate term with respect to the
terms that we already picked.
And with some thresholding then we can
With some thresholding then we can
get balance of redundancy, removal and
get balance of redundancy removal and
also high score over term.
OK so after this then we will get K
topical terms and those can be regarded
as the topics that we discovered
as the topics that we
discovered from the collection.
Next let's think about how we can
compute the topic coverage price up by
"compute the topic coverage
Jay.
i j
So looking at this picture, we have
small travel and signs and these topics
sports, travel and science and these topics
and how suppose you are given a
and now suppose you are given a
document?
document
How should we figure out the coverage
of each topic in the document?
One approach can be to simply count
occurrences of these terms.
So for example, sports might have
occurred four times in this document,
and travel occurred twice, etc, and
then we can just normalize.
This counts as our estimate of the
these counts as our estimate of the
coverage probability for each topic.
So in general the formula would be to
collect the accounts of all the terms
collect the counts of all the terms
that represented the topics and then
simply normalize them so that.
The coverage of each topic in the
document would add to one.
This forms a distribution over the
topics for the document to characterize
coverage of different topics in the
document.
Now, as always when we think about the
idea for solving problem, we have to
ask the question, how good is this one?
Or is this the best way of solving the
problem?
So now let's examine this approach.
In general, we have to do some
empirical evaluation by using actual
datasets and to see how it works.
datasets and to see how well it works.
In this case, let's take a look at the
In this case, let's take a look at a
simple example.
Here we have text document that it's
Here we have the text document that is
about NBA basketball game.
about the NBA basketball game.
So in terms of the content, it's about
sports.
the sports.
But if we simply count these words that
represent our topics, and will find
represent our topics, and we will find
that the world sports actually did not
that the word sports actually did not
occur in the article, even though the
content is about the sports.
So the count of sports is zero.
That means the coverage of sports will
be S.
be
Today is made as zero.
"estimated
Now of course.
The term science also did not occur in
the document, and it's estimated is
the document, and it's estimated
also zero.
That's OK, but sports certainly is not
OK.
'cause we know the content is about
sports.
So this estimate has problem.
What's worse, term travel actually
occurred in the document, so when we
estimate the coverage of the topic
travel, we have gotten an O account, so
travel, we have gotten a non-zero count, so
it's estimated coverage would be non 0.
it's estimated coverage would be non zero.
So this obviously is also not
desirable.
So this simple example illustrates some
problems of this approach first.
problems of this approach. First,
When we count what words belong to the
when we count what words belong to the
topic, we also need to consider related
words.
We can't simply just count the topic.
World sports.
word sports.
In this case, it did not occur at all,
but there are many related words like a
but there are many related words like
basketball game, etc.
basketball, game, etc.
So we need to count related words.
Also.
The second problem is that award like
The second problem is that a word like
star can be actually ambiguous.
sport can be actually ambiguous.
star can be actually ambiguous.
So here it probably means a basketball
school start.
star
But we can imagine it might also mean a
star on the Sky.
So in that case the song might actually
So in that case the star might actually
suggest perhaps a topic of science.
suggest perhaps a topic of science. So we need to deal with that as well.
suggest perhaps a topic of science. So we need to deal with that as well. Finally, the main restriction of this approach is that
suggest perhaps a topic of science. So we need to deal with that as well. Finally, the main restriction of this approach is that we have only one term to describe this topic
For example, a very specialized topic
So we need to deal with that as well. Finally, For example, a very specialized topic
So we need to deal with that as well. Finally, the main restriction of this approach is
So we need to deal with that as well. Finally, the main restriction of this approach is that we only have one term to describe this topic. So
So we need to deal with that as well. Finally, the main restriction of this approach is that we only have one term to describe this topic. So it cannot really describe complicated topics.
Finally, the main restriction of this approach is that we only have one term to describe this topic. So it cannot really describe complicated topics.
So it cannot really describe complicated topics.
So it cannot really describe complicated topics.
in sports would be hard to describe by
For example a very specialized topic would be hard to describe by
using just a word or one phrase, we
need to use more words, so this example
illustrates some general problems with
this approach of treating a term as
topic.
First, it lacks expressive power,
meaning that it can only represent the
symbol general topics.
But it cannot represent the complicated
topics that might require more words to
describe.
2nd.
Second,
It's incomplete in vocabulary coverage,
it's incomplete in vocabulary coverage,
meaning that the topic itself is only
represented as long term.
represented as one term.
It does not suggest what other terms
are related to the topic, even if we're
talking about the sports, there are
many terms that are related, so it does
not allow us to easily count related
terms toward contributing to coverage
of this topic.
Finally, there's this problem of word
sense disambiguation, a topical term or
sense ambiguation, a topical term or
related term can be ambiguous.
For example, basketball star versus
star in the Sky.
So in the next structure we're going to
So in the next lecture we're going to
talk about how to solve the problem
with probabilistic model of the topic.
with probabilistic modeling of the topic.
This lecture is about the parody
This lecture is about the paradigmatic
grammatical relation discovery.
relation discovery.
In this lecture we're going to talk
about how to discover a particular kind
of word Association is called parallel
of word Association called paradigmatic
medical relations.
relations.
By definition, 2 words are parodically
By definition, 2 words are
paradigmatically related if they share
similar Contacts.
similar contexts.
Namely, they occur in similar positions
in text.
So naturally, our idea for discovering
such relation is to look at the context
of each word and then try to compute
the similarity of those contexts.
So here's an example of context of
World Cat.
word Cat.
Here I have taken the world cat out of
Here I have taken the word cat out of
the context.
And you can see we are seeing some
remaining words in the sentences that
contain cat.
Now we can do the same thing for
another word like a dog.
So in general we would like to capture
such a context and then try to assess
the similarity of the context of cat
and the context of award like dog.
and the context of a word like dog.
So now the question is, how can we
formally represent the context and then
define the similarity function?
So first we note that the context
actually contains a lot of words.
So they can be regarded as a pseudo
document.
I imagine the document.
An imaginary document.
But there are also different ways of
looking at the context.
For example, we can look at the word
that occurs before the world cat.
that occurs before the word cat.
We can call.
We can call this context left one
We can call this context left1
context.
So in this case you will see words
like, am I, his or big, a, the, etc.
like my, his or big, a, the, etc.
These are the words that can occur to
the left of the world cat.
So we say my cat, his cat big cat.
A cat etc.
a cat etc.
Similarly, we can also collect the
words that occur right after the world
words that occur right after the word
cat.
We can call this context right one.
We can call this context right1.
And here we see words eats 8, is, has,
And here we see words eats, ate, is, has,
etc.
Or more generally, we can look at the
all the words in the window of text
around the world cat here.
around the word cat here.
around the word cat. Here
Let's say we can take a window of eight
let's say we can take a window of eight
words around the world cat.
We call this context.
We call this context
Window it.
Window8
Now of course, you can see all the
words from left or from right, and so
we have a bag of words in general to
represent the context.
Now, such a word based representation
would actually give us interesting way
to define the perspective of measuring
the similarity.
'cause if you look at the justice
"
similarity of left one, then we'll see
similarity of left1, then we'll see
words that share just the words in the
left context and we kind of ignore the
other words that also in the general
other words that are also in the general
context.
So that gives US1 perspective to
So that gives us one perspective to
measure the similarity.
And similarly, if we only use the right
And similarly, if we only use the right1
one, context will capture the
context will capture the
similarity from another perspective.
Using both left one an right one, of
Using both left1 and right1,
course would allow us to capture the
ofcourse would allow us to capture the
similarity with even more strict
criteria.
So in general, context may contain
adjacent words like eat sanmi that you
adjacent words like eats and my that you
see here or knowledge isn't words like
see here or non-adjacent words like
Saturday, Tuesday or some other words
in the context.
And this flexibility also allows us to
measure the similarity similarity in
some other different ways.
Sometimes this is useful as we might
want to capture similarity based on
general content that would give us
loosely related paradigmatic relations,
whereas if you use only the words
immediately to the left and to the
right of the world, then you likely
will capture words that are very much
related by.
related by
Their syntactical categories, or an
their syntactical categories, or an
their syntactical categories and
semantics.
So the general idea of discovering
paradigmatic relations is to compute
the similarity of context of towards.
the similarity of context of two words.
So here for example, we can measure the
similarity of cat and dog based on the
similarity of their Contacts.
similarity of their contexts.
In general, we can combine all kinds of
views of the context and so the
similarity function is in general
combination of similarities on
different contexts.
And of course we can also assign
weights to these different similarities
to allow us to focus more on particular
kind of context, and this would be.
kind of context, and this would be
Natural application specific, but again
naturally application specific, but again
here.
here
That main idea for discovering
that main idea for discovering
paradigmatically related words is to
compute the similarity of their
context.
So next, let's see how we exactly
compute these similarity functions.
Now to answer this question is useful
Now to answer this question it's useful
to think of bag of words representation
as vectors in the vector space model.
Now those of you who have been familiar
with information retrieval, attacks
with information retrieval or text
retrieval techniques would realize that
vector space model has been used
frequently for modeling documents and
queries for search.
But here we also find it convenient to
model the context of award for
model the context of a word for
paradigmatically relation discovery.
So the idea of this approach to view
So the idea of this approach is to view
each word in our vocabulary as defining
one dimension in high dimensional space
so we have N words in total in the
vocabulary.
Then we have N dimensions as
illustrated here.
Anne on the bottom you can see
And on the bottom you can see
frequency vector representing a
context.
And here we see when eats a curd five
And here we see when eats occured five
times in this context.
times in this context,
8 occurred, three times etc.
are occurred three times etc.
ate occurred three times etc.
So this vector can then be placed in
this vector space model.
So in general, we can represent a
pseudo document or context of cat as
one vector.
D1.
d1.
An another word dog might give us a
different context, so D2.
different context, so d2.
And then we can measure the similarity
of these two vectors.
So by viewing context in the vector
space model, we convert the problem of
paradigmatic relations discovery into
the problem of computing the vectors
and their similarity.
So the two questions that we have to
address is our first.
address is first
How to compute each vector, that is,
how to compute each vector, that is,
how to compute the XIORYI.
how to compute the xi or yi.
how to compute the xi or yi?
And the other question is, how do you
compute the similarity?
Now in general there are many
approaches that can be used to solve
the problem, and most of them are
developed for information retrieval.
Ann
And
They have been shown to work well for
they have been shown to work well for
matching a query vector and a document
vector, but we can adapt the many of
the ideas to compute the similarity of
context documents for our purpose here.
So let's first look at the one possible
approach.
approach,
Where we try to measure the similarity
where we try to measure the similarity
of context based on the expected
overlap of words and we call this YOWC.
overlap of words and we call this EOWC.
So the idea here is represent.
So the idea here is represent
A context by award vector where each
a context by award vector where each
word has a weight that is equal to the
probability that a randomly pick the
probability that a randomly picked
world from this document vector is this
word from this document vector is this
world.
word.
So in other words.
XI is defined as the normalized count
xi is defined as the normalized count
of world Wii in the context.
of word wi in the context.
And this can be interpreted as a
probability that you would actually
pick this world from the one if you
pick this word from d1 if you
randomly pick the world.
randomly pick the word.
Now of course these XYZ with someone
Now of course these xi's will sum to 1
because they are normalized
frequencies.
Ann
And
This means the vector is actually
this means the vector is actually
probability distribution over words.
So.
So,
The vector D2 can be also computed in
the vector d2 can be also computed in
the same way.
And this would give us then two
probability distributions representing
two Contacts.
two contexts.
So that addresses the problem how to
compute the vectors?
Next, let's see how we can define
similarity in this approach.
Well, here we simply define the
similarity as adult product of two
similarity as a dot product of two
vectors and this is defined as the sum
of the products of all the
corresponding elements of the two
vectors.
Now it's interesting to see that this
similarity function actually has a nice
interpretation.
And there is this dot product are in
And there is this dot product
fact gives us the probability that to
infact gives us the probability that to
infact gives us the probability that two
infact gives us the probability that to
infact gives us the probability that two
randomly pick the words from the two
randomly picked words from the two
contexts.
contexts
Are identical that means if we try to
are identical that means if we try to
pick a word from one context and try to
pick another word from another context,
we can then ask the question, are they
identical?
If the two contexts are very similar,
then we should expect the we frequently
then we should expect that we frequently
will see the two words picked from the
two Contacts, or identical.
two Contacts are identical.
two contexts are identical.
If they are very different than the
If they are very different then the
chance of seeing identical words being
picked from the two contexts would be
small.
So this intuitively makes sense for
measuring similarity of Contacts.
measuring similarity of contexts.
Now you might want to also take a look
at the exact formulas and see why this
can be interpreted as the probability
that to randomly pick the words are
that two randomly picked words are
identical.
I so if you just stay at the formula.
So if you just stay at the formula.
So if you just stay at the formula
To check what's inside this some then
to check what's inside this some then
to check what's inside this sum then
you will see.
you will see,
Basically in each case it gives us the
basically in each case it gives us the
probability that we'll see overlap on a
particular word, WI Anwer XI gives us
particular word, wi and where xi gives us
the probability that will pick this
particular world from day one and Y
particular word from d1 and Y
particular word from d1 and
sub.
yi
I gives us the probability of picking
gives us the probability of picking
this word from D2 and when we pick the
this word from d2 and when we pick the
same word from the two Contacts and we
same word from the two contexts and we
same word from the two contexts then we
have identical.
have identical
Take.
pick.
Alright, so that's one possible
approach.
You WC expected overlap of words in
EOWC expected overlap of words in
context.
Now, as always, we would like to assess
whether this approach it would work.
whether this approach it would work well.
Now, of course, ultimately we have to
test it, approach it with real data and
test the approach it with real data and
test the approach with real data and
see if it gives us really semantically
related words really give us a
paradigmatic relations.
But analytically, we can also analyze
this formula little bit.
So first, as I said, it does make sense
right?
'cause this formula will give a higher
because this formula will give a higher
score if there is more overlap between
the two contexts.
So that's exactly what we want.
But if you analyze the formula more
carefully, then you also see there
might be some potential problems.
And specifically there are two
potential problems first.
potential problems. First
potential problems. First it
Might favor matching one frequently
might favor matching one frequently
might favor matching one frequent
term very well over matching more
distinct terms, and that is because in
the dot product, if one element has a
high value and this element is shared
by both context and it contributes a
lot to the overall sum.
And it might indeed make the score
higher than in another case where the
two vectors actually have a lot of
overlap in different terms, but each
term has a relatively low frequency.
So this may not be desirable.
Of course, this might be desirable in
some other cases, but in our case we
shoot intuitively prefer a case where
should intuitively prefer a case where
we match more different terms in the
context so that we have more confidence
in saying that the two words indeed
occur in similar context.
If you only rely on one term and that's
a little bit.
a little bit
Questionable.
questionable.
It may not be robust.
The second problem is that it treats
every word equally, so.
every word equally, so
If you match on the water like the and
if you match on the water like the and
if you match a word like the, and
match was, it would be the same as
matching on the world like eats.
matching on the word like eats.
But intuitively we know matching the
isn't really surprising because the
occurs everywhere, so matching the is
not as such.
not as such
A strong evidence as matching award
a strong evidence as matching award
a strong evidence as matching a word
like eats.
like eats
Which doesn't occur frequently.
which doesn't occur frequently.
So this is another problem of this
approach.
In the next, actually, we're going to
In the next lecture, we're going to
talk about how to address these
problems.
So now let's talk about the extension
of P.
of PLSA
RSA to derive LDA and to motivate that
to derive LDA and to motivate that
we need to talk about some deficiency
of PSA.
of PLSA.
First, it's not really generating model
because we can compute the probability
because we can't compute the probability
because we can compute the probability
because we cannot compute the probability
of a new document.
You can see why, and that's because the
pies are needed to generate the
document, but the piles are tide to the
document, but the pis are tied to the
document that we have in the training
data.
So we cannot compute the pies for
So we cannot compute the pis for
future document.
And there was some heuristic.
A work around though.
And Secondly, it has many parameters
and I've asked you to compute how many
primers exactly there are in PSA and
primers exactly there are in PLSA and
parameters exactly there are in PLSA and
you will see there are many problems.
"you will see there are many
That means the model is very complex
and that also means there are many
local Maxima and it's prone to
"local
overfitting and that means it's very
hard to also find a good local maximum.
And that really represents global
maximum.
And in terms of explaining future data,
we might find that it would overfit the
training that up the cause of the
training data because of the
complexity of the model.
The model is so flexible to fit the
precisely what the training data looks
like, and then it doesn't allow us to
generalize the model for using other
data.
This, however, is not necessary problem
for text mining because here we are
often only interested in fitting the
training documents that we have.
We are not always interested in
modeling future data, but in other
cases or if we care about generality,
would worry about this over fitting.
we would worry about this over fitting.
So LDA is proposed to improve that and
it basically to make poesia generative
it basically to make PLSA a generative
model by imposing a Dirichlet prior on
the model parameters duration.
the model parameters. Dirichlet
There is just a special distribution
is just a special distribution
that we can use to specify prior.
So in this sense, LDA is just a
Bayesian version of PSA and the
Bayesian version of PLSA and the
parameters are now much more
recognized.
regularized.
You will see there are many fewer
parameters.
And you can achieve the same goal as
PSA for text mining.
PLSA for text mining.
It means it can compute the topic
coverage and topic word distributions
as in PSA.
as in PLSA.
However, there is no free lunch while
However, there is no free launch while
the parameters for pier is in our much
the parameters for PLSA  is much
fuel, there were fewer parameters and
fewer, there were fewer parameters and
in order to compute the top coverage
in order to compute the topoc coverage
in order to compute the topic coverage
and world distributions, we again face
and word distributions, we again face
the problem of influence of these
variables because they're not the
parameters of the model.
So the inference part.
Again, face the local Maxima problem.
So essentially they are doing something
very similar, but theoretically LDA is
more elegant way of looking at the
topic modeling problem.
So let's see how we can generalize PSA
So let's see how we can generalize PLSA
to LDA or extend the PSA to have LDA
to LDA or extend the PLSA to have LDA
now a full treatment of LDA is beyond
the scope of this course and we just
don't have time to go in depth in
talking about that.
But here I just want to give you a
brief idea about what's exchanging and
brief idea about what's the extension and
what it enables.
So this is a picture of LDA.
Now I remove the background model just
for simplicity.
Now in this model, all these parameters
are free to change and we do not impose
any prior, so these were distribution
any prior, so these word distribution
any prior, so these word distributions
is now represented as theater I
is now represented as theta i
are now represented as theta i
vectors.
So these word distributions.
So here and the other set of parameters
are pies and we present as a vector
are pi's and we present as a vector
are pis and we present as a vector
also.
And this is for convenience will
And this is for convenience to
introduce LDA and we have one vector
for each document.
And in this case instead, are we have
And in this case each theta we have
And in this case in theta we have
one vector for each topic.
Now that the difference between LD and
Now that the difference between LDA and
PSA is that in LDA we're going to not
PLSA is that in LDA we're going to not
allow them to free the chain.
allow them to free the change.
Instead, we're going to force them to
be drawn from another distribution.
So most specifically they will be drawn
So more specifically they will be drawn
from 2 Dirichlet distributions
respectively at the original.
respectively.
The distribution is a distribution over
"The
vectors, so it gives us a probability
for a particular choice of a vector.
Take for example pies, right?
Take for example pi's, right?
Take for example pis, right?
So this Dirichlet distribution tell us
which vector of pies is more likely,
which vector of pis is more likely,
and this distribution itself is
controlled by another vector of
parameters of ours.
parameters of alpha's.
Depending on others, we can
"Depending on
"Depending on
characterize the distribution in
different ways and with false certain
different ways and with force certain
choices of pies.
choices of pi's.
To be more likely than others.
For example, you might favor a choice
of relatively uniform distribution of
all the topics, or you might favor
generating skilled coverage of topics,
generating skewed coverage of topics,
and this is controlled by Alpha.
And similar here.
The topic word distributions are drawn
from another direction distribution
from another Dirichlet distribution
with bait out parameters and note that
with beta parameters and note that
here Alpha has K parameters
corresponding to our influence on the
corresponding to our inference on the
key values of pies for a document,
k values of pies for a document,
k values of pi's for a document,
k values of pis for a document,
whereas here beta has N values
corresponding to controlling the end
corresponding to controlling the N
words in our vocabulary.
Now, once we impose these price than
the generation process will be
different an we all start withdrawing
different an we all start with drawing
pies from this Dirichlet distribution
pi's from this Dirichlet distribution
and this pile will tell us these
and this pi will tell us these
probabilities.
And then we're going to use the pie to
And then we're going to use the pi to
further choose which topic to use, and
this is of course very similar to the
PSA model.
PLSA model.
A similar here we're not going to have
these distributions free.
Instead we can do draw one from the
Dirichlet distribution, and then from
this, then we're going to further
sample world and the rest is very
sample a word and the rest is very
similar to the RSA.
similar to the PLSA.
The likelihood function now is more
complicated for LDA, but there's a
close connection between the likelihood
function of LDA and PSA, so I'm going
function of LDA and PLSA, so I'm going
to illustrate the difference here.
So in the top you see PSA.
So in the top you see PLSA.
Likelihood function that you have
already seen before it's copied from
previous slide only that I dropped the
background for simplicity.
So in the area formulas you see very
So in the LDA formulas you see very
similar things.
First you see the first equation is
essentially the same and this is the
probability of generating a word from
multiple word distributions.
Anne, this formula is a sum of all the
And this formula is a sum of all the
possibilities of generating the world
possibilities of generating the word
inside the sun is a product of the
inside the sum is a product of the
probability of choosing a topic
multiplied by the probability of
observing the world from that topic.
So this is a very important formula as
I have stressed but multiple times and
this is actually the core assumption in
all the topic models and you might see
other topic models that are extensions
of LDLPSA and they all rely on this.
of LDA or PLSA and they all rely on this.
So it's very important to understand
this.
And this gives us the probability of
getting a word from a mixture model.
Now next in the probability of a
document we see there is a PR is a
document we see there is a PLSA
component in the LDA formula.
But the LDA formula would add some
integral here, and that's to explain to
account for the fact that the piles are
account for the fact that the pis are
not fixed, so they are drawn from
Dirichlet distribution.
And that's shown here.
That's why we have to take the integral
to consider all the possible pies that
to consider all the possible pi's that
we could possibly draw from this
duration of distribution.
"Dirichlet
And similarly, in the likelihood for
the whole collection, we also see
further components added.
Another integral here.
Right, so basically in the LDA we just
added these integrals to account for
the uncertainties and we added of
course the divisions distributions to
"course the
govern the choice of these parameters,
pies and theaters.
pi's and theta's.
So this is a likelihood function for
LDA.
Now let's next let's talk about
parameter is making an inference is now
the parameters can be now estimated
using exactly the same approaching
using exactly the same approach
maximum likelihood estimator for LDA.
Now you might think about how many
parameters are there in LDA versus PSA.
parameters are there in LDA versus PLSA.
You will see there are fewer parameters
in LDA because in this case the only
parameters are alphas and betas.
So we can use the maximum micro
So we can use the maximum likelihood
estimated to compute that.
Of course it's more complicated 'cause
Of course it's more complicated because
the form of likelihood functions more
complicated.
But what's also important is not set.
Now.
These premise that we are interested
These parameters that we are interested
in, namely the topics and the coverage,
are no longer parameters in LDA.
In this case we have to use Bayesian
inference or posterior inference to
compute them based on the parameters
Alpha and beta.
Unfortunately, this computation is
intractable, so we generally have to
resort to approximate.
Influence.
And there are many methods are
available for and then.
So you will see them when you use
different toolkits for LDA, or you read
the papers about that these different
extensions of LDA.
Now here we of course can't give in
depth introduction to, but just know
that they are computed based on
Bayesian inference with.
By using the parameters of ours and
By using the parameters of alphas and
leaders.
beta.
But algorithmically, actually in the
end, in some algorithm at least, it's
very similar to PSA an, especially when
very similar to PLSA an, especially when
we use algorithm called collapsed Gibbs
sampling.
Then the algorithm looks very similar
to the EM algorithm.
So in the end they're doing something
very similar.
So to summarize, our discussion of
probabilistic topic models and these
models provide a general principle way
models provide a general principal way
of mining and analyzing topics in texts
with many applications.
The best basis test setup is to take
tax data as input, and we're going to
output the key topics.
Each topic is characterized by a water
Each topic is characterized by a word
distribution, and we're going to also
output proportions of these topics
covered in each document.
And PSA is the basic topic model, and
And PLSA is the basic topic model, and
in fact the most basic Top Model.
in fact the most basic topic Model.
And this is also often adequate for
most applications.
That's why we spend a lot of time to
explain PSA in detail.
explain PLSA in detail.
Now LDA improves over PSA by imposing
Now LDA improves over PLSA by imposing
priors.
This has led to theoretically more
opinion models.
appealing models.
However, in practice, LDA and appears
However, in practice, LDA and PLSA
intended to give similar performance,
so in practice, PSA, an LDA, would work
so in practice, PLSA, an LDA, would work
equally well for most tasks.
Here are some suggested readings if you
want to know more about the topic.
First is a nice review of probabilistic
topic models.
The 2nd paper has a discussion about
how to automatically label a topic
model.
Now I've shown some distributions and
they intuitively suggest the topic, but
what exactly is the topic?
Can we use phrases to label the topic
to make it more easy to understand?
And this paper is about the techniques
for doing that.
The third one is empirical comparison
of LDA and PSA for various tasks.
of LDA and PLSA for various tasks.
The conclusion is that they tend to
perform similarly.
This lecture is about evaluation of
This lecture is about evaluation of
text cluster.
text cluster.
So far we have talked about multiple
So far we have talked about multiple
ways of doing tax classroom but.
ways of doing tax classroom but.
ways of doing text clustering but
How do we know which method works the
How do we know which method works the
how do we know which method works the
best?
best?
So this has to do with evaluation.
So this has to do with evaluation.
Now let's talk about evaluation.
Now let's talk about evaluation.
Now to talk about evaluation,
One must go to go back to the classroom
One must go to go back to the classroom
one must go to go back to the classroom
one must go to go back to the clustering
bias that we introduced at the
bias that we introduced at the
beginning.
beginning.
Be cause two objects can be similar
Be cause two objects can be similar
Because two objects can be similar
depending on how you look at them.
depending on how you look at them.
depending on how you look at them,
We must clearly specify the perspective
We must clearly specify the perspective
we must clearly specify the perspective
of similarity.
of similarity.
Without that, the problem of clustering
Without that, the problem of clustering
is not well defined.
is not well defined.
So this perspective is also very
So this perspective is also very
important for evaluation.
important for evaluation.
If you look at this slide and you can
If you look at this slide and you can
see we have two different ways to
see we have two different ways to
cluster these shapes.
cluster these shapes.
An if you ask a question, which one is
An if you ask a question, which one is
the best or which one is better you
the best or which one is better you
the best or which one is better you actually see
there's no way to answer this question
there's no way to answer this question
without knowing whether we'd like to
without knowing whether we'd like to
cluster based on shapes or cluster
cluster based on shapes or cluster
based on sizes.
based on sizes.
And that's precisely why the
And that's precisely why the
perspective or clustering biases.
perspective or clustering biases.
perspective or clustering bias
Crucial for evaluation.
Crucial for evaluation.
is crucial for evaluation.
In general, we can evaluate text
In general, we can evaluate text
clusters in two ways.
clusters in two ways.
One is directly evaluation and the
One is directly evaluation and the
One is direct evaluation and the
other is indirectly valuation, so
other is indirectly valuation, so
other is indirect valuation, so
other is indirect evaluation, so
other is indirect evaluation. So
indirectly valuation.
indirectly valuation.
in directl valuation,
We want to answer the following
We want to answer the following
we want to answer the following
question.
question.
question;
question:
How close are those system generated
How close are those system generated
How close are the system generated
clusters to the ideal clusters that are
clusters to the ideal clusters that are
generated by humans?
generated by humans?
So the closeness here can be assessed
So the closeness here can be assessed
assessment from multiple perspectives
assessment from multiple perspectives
assessed from multiple perspectives
and that would help us characterize the
and that would help us characterize the
quality of clustering results in
quality of clustering results in
multiple angles.
multiple angles.
And this is sometimes desirable.
And this is sometimes desirable.
Now.
Now.
But we also want to quantify the
But we also want to quantify the
We also want to quantify the
closeness because this would allow us
closeness because this would allow us
to easily compare different methods
to easily compare different methods
based on their performance figures.
based on their performance figures.
And finally, you can see in this case
And finally, you can see in this case
we essentially inject the clustering
we essentially inject the clustering
bias by using humans.
bias by using humans.
Basically, humans would bring needed or
Basically, humans would bring needed or
Basically, humans would bring the need or
Basically, humans would bring the needed or
desired clustering bias.
desired clustering bias.
desire clustering bias.
desired clustering bias.
Now how do we do that exactly?
Now how do we do that exactly?
The general procedure would look like
The general procedure would look like
this.
this.
Given the test set which consists of a
Given the test set which consists of a
lot of text objects, we can have humans
lot of text objects, we can have humans
who create the ideal clustering result.
who create the ideal clustering result.
That is, we're going to ask humans to
That is, we're going to ask humans to
partition the objects to create the
partition the objects to create the
gold standard.
gold standard.
And they will use their judgments based
And they will use their judgments based
on the need of a particular application
on the need of a particular application
to generate the what they think are the
to generate the what they think are the
to generate what they think are the
best clustering results.
best clustering results.
And this would be then used to compare
And this would be then used to compare
with the system generated clusters from
with the system generated clusters from
the same test set.
the same test set.
An ideally we wanted the system results
An ideally we wanted the system results
And ideally we wanted the system results
And ideally we want the system results
to be the same as a human January
to be the same as a human January
to be the same as human generated
results, but in general they are not
results, but in general they are not
going to be the same, so we would like
going to be the same, so we would like
to then qualify the similarity between
to then qualify the similarity between
the system generated clusters and the
the system generated clusters and the
gold standard clusters, and this
gold standard clusters, and this
similarity can be also measured from
similarity can be also measured from
multiple perspectives and this will
multiple perspectives and this will
give us various measures to
give us various measures to
quantitatively evaluate a cluster
quantitatively evaluate a cluster
clustering result and some of the
clustering result and some of the
commonly used measures include the
commonly used measures include the
commonly used measures include
purity.
purity.
purity,
Which meshes whether a cluster has
Which meshes whether a cluster has
which measures whether a cluster has
similar objects from the same cluster
similar objects from the same cluster
in the gold standard and normalized
in the gold standard and normalized
mutual information is commonly used
mutual information is commonly used
mutual information is a commonly used
measure which basically matches based
measure which basically matches based
measure which basically measures based
on the identity of the cluster of
on the identity of the cluster of
on the identity or the cluster of
on the identity of or the cluster of
object in the system, then results.
object in the system, then results.
object in the system-generated results.
How well can you predict the cluster of
How well can you predict the cluster of
the object in the gold standard or vice
the object in the gold standard or vice
versa.
versa.
Mutual information captures the
Mutual information captures the
correlation between these cluster
correlation between these cluster
labels and normalized mutual
labels and normalized mutual
information is often used for
information is often used for
quantifying the similarity for this
quantifying the similarity for this
evaluation purpose.
evaluation purpose.
If measure is another possible measure.
If measure is another possible measure.
F measure is another possible measure.
Now get a thorough discussion of the
Now get a thorough discussion of the
Now again a thorough discussion of the
Now again, a thorough discussion of the
Now again, a thorough discussion of this
evaluation of these evaluations.
evaluation of these evaluations.
evaluation, of these evaluations,
Issues would be beyond the scope of
Issues would be beyond the scope of
would be beyond the scope of
this course.
this course.
I've suggested some reading in the end
I've suggested some reading in the end
that you can take a look to know more
that you can take a look to know more
about that.
about that.
So here I just want to discuss some
So here I just want to discuss some
high level ideas that would allow you
high level ideas that would allow you
to think about how to do evaluation,
to think about how to do evaluation,
to think about how to do evaluation in
your applications.
your applications.
The 2nd way to evaluate text clusters
The 2nd way to evaluate text clusters
is to do indirectly evaluation.
is to do indirectly evaluation.
is to do indirect evaluation.
So in this case the question to answer
So in this case the question to answer
is how useful are the clustering
is how useful are the clustering
results for the intended applications?
results for the intended applications?
Now this of course is application
Now this of course is application
specific question, so usefulness is.
specific question, so usefulness is.
specific question, so usefulness is
Is going to depend on specific
Is going to depend on specific
is going to depend on specific
applications.
applications.
In this case, the clustering biases
In this case, the clustering biases
In this case, the clustering bias
In this case, the clustering bias is
imposed by the intended application as
imposed by the intended application as
well.
well.
So what counts as the best clustering
So what counts as the best clustering
result would be dependent on
result would be dependent on
application.
application.
the application.
Procedure wise we also would create the
Procedure wise we also would create the
test set with tax objects for the
test set with tax objects for the
test set with text objects for the
intended application to quantify the
intended application to quantify the
performance of the system.
performance of the system.
How in this case what we care about is
How in this case what we care about is
In this case what we care about is
the contribution of clustering to some
the contribution of clustering to some
application.
application.
So we often have a baseline system
So we often have a baseline system
compare with.
compare with.
to compare with.
This could be the current system for
This could be the current system for
doing something and then you hope to
doing something and then you hope to
add a clustering to improve it or the
add a clustering to improve it or the
add clustering to improve it or the
baseline system could be using a
baseline system could be using a
different clustering method and you
different clustering method and you
then what you are trying to experiment
then what you are trying to experiment
with and you hope to have better idea
with and you hope to have better idea
with and you hope to have a better idea
for clustering.
for clustering.
So in case you have a baseline system
So in case you have a baseline system
work with and then you can add a
work with and then you can add a
to work with and then you can add a
clustering algorithm to the baseline
clustering algorithm to the baseline
system to produce a clustering system.
system to produce a clustering system.
And then we're going to compare the
And then we're going to compare the
performance of your clustering system
performance of your clustering system
and the baseline system in terms of the
and the baseline system in terms of the
performance measure for that particular
performance measure for that particular
application.
application.
So in this case we call it in directly
So in this case we call it in directly
So in this case we call it indirect
valuation of clusters because there's
valuation of clusters because there's
evaluation of clusters because there's
no explicit assessment of the quality
no explicit assessment of the quality
of clusters, but rather is to assess
of clusters, but rather is to assess
of clusters, but rather its to assess
the contribution of clusters to a
the contribution of clusters to a
particular application.
particular application.
So to summarize text clustering, it's a
So to summarize text clustering, it's a
very useful unsupervised general text
very useful unsupervised general text
mining techniques as particularly
mining techniques as particularly
mining technique as particularly
useful for obtaining an overall picture
useful for obtaining an overall picture
of the text content.
of the text content.
This is often needed to explore text
This is often needed to explore text
data.
data.
And this is often the first step when I
And this is often the first step when I
And this is often the first step when you
deal with a lot of text data.
deal with a lot of text data.
The second application or second kind
The second application or second kind
of applications is to discover
of applications is to discover
of application is to discover
interesting clustering structures in
interesting clustering structures in
text data, and these structures can be
text data, and these structures can be
very meaningful.
very meaningful.
There are many approaches.
There are many approaches.
There are many approaches
That can be used for text clustering
That can be used for text clustering
that can be used for text clustering
and we discussed them.
and we discussed them.
and we discussed them:
Model based approaches and similarity
Model based approaches and similarity
based approaches.
based approaches.
In general, strong clusters tend to
In general, strong clusters tend to
show up no matter what method is used.
show up no matter what method is used.
Also the.
Also the.
Also the
Effectiveness of a method highly
Effectiveness of a method highly
effectiveness of a method highly
depends on whether the desired
depends on whether the desired
clustering biases captured
clustering biases captured
clustering bias is captured
appropriately, and this can be done
appropriately, and this can be done
either through using the right
either through using the right
generating model, the model design,
generating model, the model design,
generative model, the model design,
appropriate for counseling, or the
appropriate for counseling, or the
appropriate for clustering, or the
right similarity function to explicitly
right similarity function to explicitly
define bias.
define bias.
Deciding the optimal number of clusters
Deciding the optimal number of clusters
is very difficult problem for all the
is very difficult problem for all the
classroom methods, and that's because
classroom methods, and that's because
it's unsupervised algorithm and there's
it's unsupervised algorithm and there's
no training data to guide us to select
no training data to guide us to select
the best number of.
the best number of.
the best number of
Clusters
Clusters
clusters
clusters.
Now sometimes you may see some methods
Now sometimes you may see some methods
that can automatically determine the
that can automatically determine the
number of classes.
number of classes.
number of clusters.
But in general, that has some implied
But in general, that has some implied
application of clustering bias there,
application of clustering bias there,
and that's just not the specified
and that's just not the specified
and that's just not specified.
without clearly defining a clustering
without clearly defining a clustering
Without clearly defining a clustering
bias, it's just impossible to say the
bias, it's just impossible to say the
optimal number of cluster is what?
optimal number of cluster is what?
So this is.
So this is.
So this is
Important to keep in mind.
Important to keep in mind.
important to keep in mind.
And I should also say sometimes we can
And I should also say sometimes we can
use application to determine the number
use application to determine the number
of clusters.
of clusters.
For example, if your clustering search
For example, if your clustering search
For example, if you are clustering search
results, then obviously you don't want
results, then obviously you don't want
to generate 100 clusters, right?
to generate 100 clusters, right?
So the number can be dictated by the
So the number can be dictated by the
interface design.
interface design.
In other situations, we might be able
In other situations, we might be able
to use the.
to use the.
to use the
The fitness today to assess whether
The fitness today to assess whether
fitness of data to assess whether
we've got a good number of clusters.
we've got a good number of clusters.
we've got a good number of clusters
To explain our data well and to do
To explain our data well and to do
to explain our data well and to do
that, you can vary the number of
that, you can vary the number of
clusters and watch how well you can fit
clusters and watch how well you can fit
the data.
the data.
If it's in general, when you add more
If it's in general, when you add more
components to mixture model, you should
components to mixture model, you should
fit the data better, because you can
fit the data better, because you can
always set the probability of using the
always set the probability of using the
new component at 0, so you can't in
new component at 0, so you can't in
general fit the data worse than.
general fit the data worse than.
general fit the data worse than
Before, but as the question is, as you
Before, but as the question is, as you
before, but as the question is, as you
add more components which will be able
add more components which will be able
add more components would you be able
to significantly improve the fitness of
to significantly improve the fitness of
the data and that can be used to
the data and that can be used to
determine the right number of clusters.
determine the right number of clusters.
And finally, evaluation of classroom
And finally, evaluation of classroom
And finally, evaluation of clustering
results and can be done both directly
results and can be done both directly
and indirectly.
and indirectly.
And we also would like to do both in
And we also would like to do both in
order to get good sense about how we're
order to get good sense about how we're
order to get good sense about how our
method works.
method works.
So here's some suggested reading, and
So here's some suggested reading, and
this is particularly useful to better
this is particularly useful to better
understand the how the measures are
understand the how the measures are
calculated and classroom in general.
calculated and classroom in general.
calculated and clustering in general.
This lecture is about the text
representation.
In this lecture we're going to discuss
tax representation.
text representation.
And discuss how natural language
processing can allow us to represent
text in many different ways.
Let's take a look at this example
sentence again.
We can represent this sentence in many
different ways.
1st.
We can always represented such a
We can always represent such a
sentence as a string of characters.
This is true for all the languages when
we store them in the computer.
When we store a natural language
sentence as a string of characters.
sentence as a string of characters,
We have perhaps the most general way of
we have perhaps the most general way of
representing text, since we always use
representing text, since we can always use
this approach to represent any text
data.
But unfortunately, using such a
representation would not help us do
semantic analysis, which is often
needed for many applications of text
mining.
The reason is because we're not even
recognizing words.
So as a stream we're going to keep all
So as a string we're going to keep all
the spaces and these ASCII symbols.
We can perhaps count how what's the
We can perhaps count how... what's the
most frequent character in English
text, or the correlation between those
characters, but we can't really analyze
semantics.
Yet this is the most general way of
representing text, because we can use
this to represent any natural language
text.
If we try to do a little bit more
natural language processing by doing
word segmentation.
Then we can obtain a representation of
the same tax, but in the form of a
the same text, but in the form of a
sequence of words.
So here we see that we can identify
words like a dog is chasing etc.
words like: a dog is chasing etc.
words like: a, dog, is, chasing, etc.
Now with this level of representation,
we certainly can do a lot of things,
and this is mainly because words are
the basic units of human communication
in natural language, so they are very
powerful.
By identifying words we can, for
example, easily count what are the most
frequent words in this document or in
the whole collection, etc.
At these words can be used to form
And these words can be used to form
topics.
topics...
topics.
When we combine related words together
and some words are positive, somewhat
"and some words are positive, some words
and some words are positive, some words are
negative, so we can also do sentiment
analysis.
Representing text data as a sequence of
So representing text data as a sequence of
words opens up a lot of interesting
analysis possibilities.
However, this level of representation
is slightly less general than string of
characters, because in some languages
such as Chinese.
such as Chinese,
It's not actually not that easy to
it's not actually not that easy to
it's actually not that easy to
identify all the word boundaries,
because in such a language you see text
as a sequence of characters with no
space in between.
So you have to rely on some special
techniques to identify words.
In such a language, of course, then we
might make mistakes in segmenting
words.
So the sequence of words representation
is not as robust as string of
characters.
But in English it's very easy to obtain
this level of representation, so we can
do that all the time.
Now if we go further to do natural
language processing, we can add a part
of speech tags.
Now, once we do that, we can count for
example, the most frequently nouns or
example, the most frequent nouns or
what kind of nouns are associated with
what kind of verbs, etc.
So this opens up a little bit more
interesting opportunities for further
analysis.
Note that I use the plus sign here,
becausw by representing text as a
because by representing text as a
sequence of part of speech tags.
We don't necessarily replace the
original world sequence recommendation.
original word sequence recommendation.
original word sequence representation
Instead, we added this as an additional
Instead, we add this as an additional
way of representing text data so that
way of representing text data, so that
now the data is represented as both or
now the data is represented as both a
sequence of words, and the sequence of
sequence of words, and a sequence of
part of speech tags.
This enriches representation of text
This enriches the representation of text
data and thus.
"data and thus, also,
"data and thus, also,
data and thus, also,
data and thus, also,
Also enables a more interesting
enables a more interesting
analysis.
If we go further then will be pausing
If we go further then will be parsing
If we go further then we'll be parsing
the sentence or obtaining to obtain a
the sentence to obtain a
syntactic structure.
Now this of course further open up a
Now this of course further opens up a
"Now this of course further opens up
"Now this of course further opens up
Now this of course further opens up
Now this of course further open up
more interesting analysis of, for
example, the writing styles.
example, the writing styles,
Or correcting grammar mistakes.
or correcting grammar mistakes.
If we could further go further for
If we could go further for
semantic analysis, then we might be
able to recognize dog as animal and we
also can recognize a boy as a person
also can recognize boy as a person
and playground as a location.
And we can further analyze their
relations.
relations,
For example, dog is chasing the boy and
for example, dog is chasing the boy and
the boy is on the playground.
Now this is the Adam or entities and
Now this is to add more entities and
relations through entity, relation
relations through entity-relation
regulation.
recognition
recognition.
At this level, then we can do even more
interesting things.
For example, now we can counter easily
For example, now we can count easily
the most frequent person that's
mentioned in this whole collection of
news articles, or whenever you mention
this person, you also tend to see
mention of another person, etc.
So this is very useful representation
an it's also related to the Knowledge
Graph that some of you may have heard
of.
That Google is doing as a more semantic
way of representing tax data.
way of representing text data.
However, it's also less robust than.
However, it's also less robust than
Sequence of words or even syntactic
sequence of words or even syntactic
analysis, cause it's not always easy to
analysis, because it's not always easy to
identify all the entities with the
right types, and we might make
mistakes, and relations are even harder
to find and we might make mistakes.
So this makes this level of
representation less robust, yet it's
very useful.
Now if we move further to logical
position then we can have predicates an
representation then we can have predicates an
representation then we can have predicates and
even inference rules.
"even inference rules. And with inference rules we can
even inference rules. And with inference rules we can infer
"even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it is even less robust and we can make mistakes,
even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it is even less robust and we can make mistakes,
even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes,
And we can't do that all the time for
and we can't do that all the time for
all kinds of sentences.
And finally, speech acts with added yet
another level of repetition of the
another level of representation... Of the
another level of representation of the
intent of saying this sentence.
So in this case it might be a request.
So knowing that would allow us to
analyze more, even more interesting
things about the observer order.
things about this observed order.
things about this observer order.
things about the observer order.
Also this sentence, what's the
Author of this sentence, what's the
intention of saying that?
What scenarios, what kind of actions
will be made?
So this is?
So this is...
Another level of analysis that would be
very interesting.
So this picture shows that if we move
down, we generally see more
sophisticated natural language
processing techniques to be used.
And unfortunately, such techniques
would require more human effort.
And they are less accurate.
That means there are mistakes.
So if we analyze text dinner at the
So if we analyze text data at the
levels that are represented, deeper
analysis of language, then we have to
tolerate the errors.
So that also means it's still necessary
to combine such deep analysis with
shadow analysis based on, for example
shallow analysis based on, for example
sequence of words on the right side you
sequence of words... On the right side you
sequence of words. On the right side you
see the arrow points down.
see the arrow points down,
To indicate that as we go down without
to indicate that as we go down without
"to indicate that as we go down with our
"to indicate that as we go down with our
to indicate that as we go down with our
representation of text is closer to
representation of text, is closer to
representation of text, it's closer to
knowledge representation in our mind,
an need for solving a lot of problems.
and need for solving a lot of problems.
Now, this is desirable becausw as we
Now, this is desirable because as we
can represent text at the level of
knowledge, we can easily extract the
knowledge.
That's the purpose of text mining.
So there is a trade off here between
doing deeper analysis that might have
errors, but would give us direct
knowledge that can be extracted from
text and doing shadow analysis, which
text and doing shallow analysis, which
is more robust.
But wouldn't they actually give us the.
But wouldn't they actually give us the
But wouldn't actually give us the
Necessary deeper representation of
necessary deeper representation of
knowledge I should also say that text
knowledge... I should also say that text
knowledge. I should also say that text
data are generated by humans.
data are generated by humans
An are meant to be consumed by humans.
An are meant to be consumed by humans
and are meant to be consumed by humans
and are meant to be consumed by humans,
So as a result in a text data analysis
so as a result in a text data analysis
text mining humans play a very
text mining, humans play a very
important role.
They are always in the loop.
Meaning that we should optimize the
collaboration of humans and computers.
So in that sense, it's OK that
computers may not be able to have
completely accurate representation of
text data and patterns that are
extracted from text later can be
extracted from text data can be
interpreted by humans, and humans can
guide the computers to do more accurate
analysis by annotating more data by
providing features to guide the machine
learning programs to make them work
more effectively.
This lecture is a continued discussion
of generative probabilistic models for
text classroom.
text clustering.
In this lecture we're going to finish
the discussion of generative
probabilistic models for text
clustering.
So this is a slide that you have seen
before and here we show how we define
the mixture model for text clustering
an what the likelihood function looks
like an.
like and
We can also compute the maximum, like
we can also compute the maximum liklihood
your estimate to estimate the
estimate to estimate the
parameters in this lecture, we're going
parameters. In this lecture, we're going
to talk more about how exactly we're
going to compute the maximum likelihood
estimator.
Now, as in most cases, the EM algorithm
can be used to solve this problem for
mixture models.
So here's the detail of this EM
algorithm for document clustering.
Now, if you have understood how EML
with works for topic models, PSA and I
works for topic models, PSA and I
works for topic models, PLSA and I
think here it will be very similar and
you just need to adapt a little bit to
the this new mixture model.
this new mixture model.
So as you may recall, EM algorithm
starts with initialization of all the
parameters.
So this is the same as what happened
before for a topic models.
before for topic models.
And then we're going to repeat until
they're likely hold converges.
they're likely converges.
their likely converges.
their likelihood converges.
An inch step will do eastep an M step
An in each step will do E step and M step
An in each step will do E step and M step.
in M step.
We're going to infer which distribution
we're going to infer which distribution
has been used to generate each
document.
And so we have to introduce a hidden
variable ZD for each document and this
value variable could take a value from
the range of one through K representing
K different distributions.
And more specifically, basically we're
going to apply Bayes rule to infer, or
which distribution is more likely to
have generated this document or
computing the posterior probability of
the distribution.
Given the document.
An we know it's proportional to the
probability of selecting this
distribution P of Cedar I and the
distribution P of Theta I and the
probability of generating this whole
document from that distribution, which
is a product of all the probabilities
of words for this document, as you see
here.
Now, as in all cases, it's useful to
kind of remember the normalizer or the
OR the constraint on this probability.
So in this case we know the constraint
on this probability in the E step is
that all the probabilities of Z equals
I must sum to one 'cause the document
must have been generated from precise
or one of these K topics.
So the probability of the generator
from each of them should sum to one.
And if this constraint then you can
easy to compute this distribution as
long as.
What it is proportional to, right?
what it is proportional to, right?
So once you compute this product that
you see here, then you simply normalize
this these probabilities to make them
something what over all the topics.
some to 1 what over all the topics.
So that's easy step after eastep over
So that's E step after E Step we would know
which distribution is more likely to
have generated this document ID, which
have generated this document D, which
is unlikely.
And then in the M step, we're going to
relist, made all the parameters based
on the, infer the Z values, or in
further knowledge about which district
has been used to generate which
document.
So the re estimation involves two kinds
So there estimation involves two kinds
of parameters.
One is P of Theta and this is the
probability of selecting a particular
distribution.
Before we observe anything, we don't
have any knowledge about which cluster
is more likely, but after we have
observed these documents, then we can
collect the evidence.
To infer which cluster is more likely,
and so this is proportional to the sum
of the probability of Z sub DJ is equal
to I.
And so this gives us all the evidence
about using topic.
I said I to generate a document and we
put them together and again we
normalize them into probabilities.
And then so this is 4 P of Theta sub I.
And then so this is for P of Theta sub I.
Now the other kind of parameters are
the probabilities of words in each
distribution, each cluster, and this is
very similar to the case of PSA.
very similar to the case of PLSA.
And here we just pulled the counts of
words that are in documents that are
inverted to have been generated from a
particular topic.
particular topic Theta I here
Theta I hear and this would allow us to
and this would allow us to
then estimate how many words have
actually been generated from Cedar I.
actually been generated from Theta I.
And then we normalize again.
These counts into probabilities so that
the probabilities on all the words with
the probabilities on all the words
some to one.
Note that it's very important to
understand these constraints as they
are precisely the normalizers in all
these formulas, and it's also important
to know that distribution is over what?
For example, the probability of Theta
is overall the key topics and that's
why these capabilities with something
why these K probabilities sum to 1.
well, whereas the probability of a word
given Theta is a probability
distribution over all the words.
So there are many probabilities and
they have to send one.
So now let's take a look like this.
Take a look at the simple example of
two clusters.
I have two clusters.
I've shown some initializer values for
the two distributions.
And let's assume we randomly
initialized to probabilities of
selecting each cluster as .5.
So equally likely.
And then let's consider one document
that you have seen here.
There are two words, sorry, two
occurrences of text and two occurrences
of mining.
So there are four words together.
Medical and health did not occur in
this document, so this first thing
about the hidden variables.
Now for each document we must use a
hidden variable and before in PSA we
hidden variable and before in PLSA we
used 1 hidden variable for each word.
Because that's the output from what
mixture model.
So in our case the output from a
mixture model or the observation from
mixture model is a document, another
mixture model is a document not a
word.
So now we have 1 hidden variable
attached to the document.
That hidden variable must tell us which
distribution has been used to generate
the document, so it's going to take two
values, one and two to indicate the two
topics.
So now how do we infer which
distribution has been used to generate
the D?
It's to use Bayes rule so it looks like
this in order for the first topic is
setup, want to generate the document.
We two things must happen.
Two things must happen.
First theater subway must have been
selected, so it's given by P of 01
second.
It must have also been generating the
four words in the document, namely two
occurrences of text and two occurrences
of mining.
That's why you see the numerator has
the product of the probability of
selecting theater one and the
selecting Theta one and the
probability of generating the document
from Silva.
from Theta 1.
So the denominator is just the sum of
two possibilities of generating this
document, and you can plug in the
numerical values to verify.
Indeed in this case the document is
more likely to be generated from much
more likely to be generated from Theta 1,  much
more likely than from zero 2.
more likely than from than Theta 2.
So once we have this problem that we
can easily compute the probability of Z
= 2 given this document, how we're
going to use the constraint?
Right now it's going to be 1 -- 100 /
Right now it's going to be 1 - 100 /
Right now it's going to be 1 - 100 /101
1,000,000 one.
So now it's important to note that in
such a computation there is a potential
problem of underflow, and that is
because if you look at the numerator,
the original numerator and denominator
it involves the computation of a
product of many small probabilities.
Imagine if a document has many words
and it's going to be a very small value
here, as it can cause the problem of
underflow.
So to solve the problem.
We can use a normalized.
So here you see that we take average of
all these two solutions to compute
another average district called Theater
Bar.
And this does the average distribution
will be comperable to each of these
distributions.
In terms of the quantities, the
magnitude.
So we can then divide the numerator and
the denominator both by this
normalizer.
So basically this normalizes the
probability of generating this document
by using this average voter
by using this average word
distribution.
So you can see the normalizer here.
And since we have used exact the same
normalizer for the numerator and
denominator, the whole value of this
expression is not changed.
But by doing this normalization you can
see we can make the numerators and
denominators more manageable in that
the overall value is not going to be
very small for each, and thus we can
avoid underflow problem.
In some other times we sometimes also
use logarithm of the product to convert
this into a sum of log of
probabilities.
This can help preserve precision as
well, but in this case we cannot use
logarithms to solve the problem because
there's some in the denominator, But
there's sum in the denominator, But
this kind of normalizes can be
effective for solving this problem, so
it's a technique that's sometimes
useful in other situations as well.
Now let's look at the M step.
So from the E step we can see our
estimate of which distribution is more
likely to have generated a document,
and you can see the wise more likely
and you can see D1 is more likely
from the first topic.
Where is D2 is more like from the
second topic, etc.
Now let's think about what we need to
compute in the M step.
Basically we need to re estimate all
the parameters.
Let's first look at the P of set up one
Let's first look at the P of Theta 1
and PLC .2.
and P of Theta 2.
How do we estimate that?
Intuitively, you can just pull together
the Z probability Z probabilities from
eastep, right?
E Steps, right?
So if all these documents say they're
more likely from silouan, then we
intuitively would give a high
probability to see that one right?
So in this case, so we can just take
the average of these probabilities that
you see here, and we obtain the .6401
you see here, and we obtain the .6 for Theta 1
so still aligns more likely.
so Theta 1 is more likely
And then C .2.
Theta 2.
than Theta 2.
So you can see the probability of 02
So you can see the probability of Theta 2
would be naturally .4.
What about these world probabilities?
What we do the same?
And intuition is the same, so we're
going to see in order to estimate the
probabilities of words in theater one,
probabilities of words in Theta one,
we're going to look at which documents
have been generated from Scylla and
we're going to pull together the words
in those documents and normalize them.
So this is basically what I just said.
Most specifically, we're going to for
example.
Use all the counts of text in these
documents to estimate the probability
of tax given still awhile, but we're
not to use their raw counts or total
account.
Instead, we can do that.
Discount them by the probabilities that
each document is likely be generated
from silouan.
from Theta 1.
So this gives us some fractional
counts, and then these Council would be
then normalized in order to get the
probability.
Now how do we normalize them?
These probabilities of these words must
sum to one.
So to summarize, our discussion of
generating models for clustering.
We showed that a slight variation of
Top Model can be used for classroom
Top Model can be used for clustering
documents and this also shows the power
of generating models in general by
changing the generation assumption and
changing the model slightly we can
achieve different goals and we can
capture different patterns in text
data.
So in this case, each class is
represented by unigram language model
or distribution, and that's similar to
or word distribution, and that's similar to
topic model.
So here you can see the word
distribution actually generates a term
cluster as a byproduct.
A document that is generated by first
choosing a unigram language model and
then generating all the words in the
document that using this single
language model and this is very
different from again topping model
different from again topic model
where we can generate the words in the
document by using multiple unigram
language models.
And then the estimated model premise
And then the estimated model pamateter
will give both a topic capitalization
of each cluster and the probabilistic
assignment of each document into a
cluster.
And this probabilistic assignment that
sometimes is useful for some
applications.
But if we want to achieve a harder
clusters may need to partition
clusters mainly to partition
documents into destroying the clusters.
documents into disjoint clusters.
Then we can just force the document
into the cluster corresponding to the
water distribution.
That's most likely to have generated
the document.
We've also should have the EM algorithm
We've also shown that the EM algorithm
can be used with computer.
The maximum lag or is made up an.
In this case we need to use a special
normalization technique to avoid
underflow.
This doctor is a continued discussion
This lecture is a continued discussion
of probabilistic topic models.
In this lecture, we're going to
continue discussing probabilistic
models, working to talk about a very
models, we are going to talk about a very
simple case where we are interested in
just mining one topic from one
document.
So in this simple setup.
So in this simple setup
We are interested in analyzing one
we are interested in analyzing one
document and trying to.
document and trying to
Discover just one topic.
discover just one topic.
So this is the simplest case of topic
modeling.
The input now no longer has K, which is
the number of topics becausw we know
the number of topics because we know
there is only one topic.
And the collection has only one
document also.
In the output will also no longer have
In the output we will also no longer have
In the output we also no longer have
coverage because we assumed that the
document covers this topic 100%.
So the main goal is just to discover
the world of probabilities for this
the word probabilities for this
single topic, as shown here.
As always, when we think about using a
generated model to solve such a problem
generative model to solve such a problem
generative model to solve such a problem,
with, start with thinking about what
we'll start with thinking about what
kind of data we're going to model or
from what perspective we're going to
model the data or data representation.
And then we're going to design A
"And then we're going to design a
"And then we're going to design a
And then we're going to design a
specific model for the generation of
the data from our perspective.
Where our perspective just means we
want to take a particular angle of
looking at the data so that the model
would have the right parameters for
discovering the knowledge that we want,
and then will be thinking about the
and then we'll be thinking about the
likelihood function or write down the
library function to capture more
formally how likely a data point will
be obtained from this model.
And the likelihood function will have
some parameters in the function and
then we are usually interested in
estimating those parameters, for
example by maximizing the likelihood
which would lead to maximum likelihood
estimator and these estimated
parameters would then become the output
of the mining algorithm.
Which means will take the estimated
Which means we'll take the estimated
parameters as a knowledge that we
discover from the text.
So let's look at these steps for this
very simple case.
Later, we'll look at this these this
"Later, we'll look at this
"Later, we'll look at this
Later, we'll look at this
procedure for some more complicated
cases.
So our data in this case is just the
document which is a sequence of words.
Each word here is denoted by X sub I.
Our model is a unigram language model
Our model is a unigram language model,
award distribution that we hope to
a word distribution that we hope to
denote a topic and that's our goal.
So we will have as many parameters as
many words in our vocabulary.
many words in our vocabulary,
In this case M.
in this case M.
And for convenience we're going to use
Theater South I to denote the
theta sub I to denote the
probability of world W some I.
probability of word W sub I.
And obviously these theaters of ice
And obviously these thetas of i's
would sum to one.
Now, what is the likelihood function
Now, what does the likelihood function
look like?
This is just the probability of
generating this whole document that
generating this whole document
given such a model, because we assume
given such a model. Because we assume
the independence in generating each
word.
word,
So the probability of the world the
So the probability of the word the
so the probability of the word the
document would be just a product of the
probability of each word.
And since some were might have repeated
And since some word might have repeated
occurrences, so we can also rewrite
this product in a different form.
So in this line we have rewriting the
So in this line we have rewritten the
formula into a product over all the
unique words in the vocabulary.
unique words in the vocabulary,
W sub one through the WS of M.
W sub one through the W's of M.
W sub one through the W sub M.
Now this is different from the previous
line where the product is over
different positions of words in the
document.
Now when we do this transformation.
Now when we do this transformation,
We then would need to introduce account
we then would need to introduce account
"we then would need to introduce a count
"we then would need to introduce a count
we then would need to introduce a count
function here.
This denotes the count of world one in
This denotes the count of word one in
document.
And similarly, this is the count of
words of M in locking becausw.
words of M in the document. becausw.
words of M in the document. Because
These words might have repeated
these words might have repeated
occurrences.
You can also see if award did not occur
You can also see if a word did not occur
in the document, it would have a zero
counter and therefore that
count and therefore that
corresponding term will disappear.
So this is a very useful form of
writing down the microphone function
writing down the likelihood function
that we will often use later.
So I want you to pay attention to this.
Just get familiar with this.
Just get familiar with this notation.
A notation it's just to change the
It's just to change the
product over all the different words in
the vocabulary.
So in the end, of course will use Cedar
So in the end, of course will use theta
So in the end, of course we'll use theta
supply to express this, like hold
sub I to express this, like hold
sub I to express this likelihood
function and it would look like this.
Next, we're going to find the theater
Next, we're going to find the theta
values.
values,
Or probabilities of these words that
or probabilities of these words that
would maximize this likelihood
function.
So now let's take a look at the maximum
likelihood estimate problem more
closely.
This line is copied from the previous
slide.
It's just our likelihood function.
So our goal is to maximize this like a
"So our goal is to maximize this likelihood
"So our goal is to maximize this likelihood
So our goal is to maximize this likelihood
function.
We will find it often easy to.
We will find it often easy to
Maximize the log likelihood instead of
maximize the log likelihood instead of
the original, like whole and this is
the original likelihood and this is
purely for mathematical convenience,
because after the logarithm
transformation.
transformation,
Our function will becomes a sum instead
our function will becomes a sum instead
our function will become a sum instead
of product.
of a product.
And we also have constraints.
And we also have constraints
Over these probabilities.
over these probabilities.
The sum makes it easier to take
derivative, which is often needed for
finding the optimal solution of this
function.
So please take a look at the this some
So please take a look at this sum
again here and this is a form of
function that you often see later also.
"function that you often see later also in
"function that you often see later also in
function that you often see later also in
More general topic models.
more general topic models.
So it's a sum over all the words in the
vocabulary and inside the sun there is
vocabulary and inside the sum there is
account of world in the document.
a count of world in the document.
a count of words in the document.
And this is multiplied by the logarithm
of a probability.
of the probability.
So let's see how we can solve this
problem.
Now at this point the problem is purely
a mathematical problem, cause we're
a mathematical problem, because we're
going to just to find the optimal
solution of a constrained maximization
problem.
The objective function is the
likelihood function, and the constraint
is that all these probabilities must
sum to one.
So one way to solve the problem is to
use LaGrange multiplier approach.
use Lagrange multiplier approach.
Now this content is beyond the scope of
this course.
But since Lagrangian multiplier is very
But since Lagrange multiplier is very
useful approach, I also would like to
just give a brief introduction to this.
just give a brief introduction to this
For those of you who are interested.
for those of you who are interested.
So in this approach we will construct a
larger image function here.
LaGrange function here.
Lagrange function here.
And this function would combine our
objective function with another term
that encodes our constraints.
And we introduce LaGrange multiplier
And we introduce Lagrange multiplier
here, Lambda.
So it's additional parameter.
Now the idea of this approach is just
Now the idea of this approach is to just
return the constrained optimization
turn the constrained optimization
into in some sense unconstrained
into, in some sense, unconstrained
optimizing problem.
So now we're just interested in
optimizing this LaGrange function.
optimizing this Lagrange function.
As you may recall from calculus, an
optimal point would be achieved when
the derivative is set to 0.
This is a necessary condition.
It's not sufficient though, so.
If we do that, you will see the partial
derivative with respect to see die here
derivative with respect to theta i here
is equal to this.
And this part comes from the.
And this part comes from the
Derivative of the logarithm function.
derivative of the logarithm function.
And this Lambda is simply taken from
here.
And when we set it to zero, we can
easily see sealer supplies related to
easily see theta sub i is related to
Lambda in this way.
Since we know all the cities must sum
Since we know all the theta I's must sum
to one, we can plug this into this
constraint here, and this will allow us
to solve for Lambda.
And this is.
And this is
Just negative sum of all the counts and
just negative sum of all the counts and
this further allows us to then solve
optimization problem.
Eventually to find the optimal setting
for Cedar Sub I.
for Theta Sub I.
And if you look at this formula, it
turns out that it's actually very
intuitive 'cause this is just the
intuitive because this is just the
normalized account of these words by
normalized count of these words by
the document length, which is also a
sum of all the counts of words in the
document.
So after all these mass, after all, we
So after all this math, after all, we
have just obtained something that's
very intuitive, Ann.
"very intuitive, and
"very intuitive, and
very intuitive, and
This will be just our intuition where
this will be just our intuition where
we want to maximize the.
we want to maximize the
Theater by assigning as much
Theta by assigning as much
theta by assigning as much
probability mass as possible to all the
observed words here.
And you might also notice that this is
the general result of maximum
likelihood estimator.
In general, the estimate would be to
normalize count and it's just.
normalize count and it's just
Sometimes the counts have to be done in
sometimes the counts have to be done in
a particular way, as you will see
a particular way, as you will also see
later.
So this is basically anything or
So this is basically an analytical
solution to our optimization problem.
In general, though, when the likelihood
function is very complicated, we're not
going to be able to solve the
optimization problem by having a closed
form formula.
Instead, we have to use some numerical
algorithms, and we're going to see such
cases later also.
So if you imagine what would we get if
we use such a maximum macular estimator
we use such a maximum likelihood estimator
to estimate one topic for a single
document D here, let's imagine this
document is a text mining paper.
Now what you might see is something
that looks like this.
On the top you will see the high
probability words tend to be those very
common words, often functional words in
English, and this will be followed by
some content words that really
characterized.
"characterized the topic
"characterized the topic
characterized the topic
Optical out a text mining etc and then
well out a text mining etc and then
well like text, mining etc and then
in the end you also see various more
probabilities of words.
probabilities of words
That are not really related to the
that are not really related to the
topic, but they might be externally
mentioning that document.
mentioned in the document.
As a topic representation, you will see
this is not ideal, right?
The because of the high probability
words are functional words.
words are functional words
They are not really characterizing the
they are not really characterizing the
topic.
So one question is how can we get rid
of such common words?
This is a topic of the next, and after
"This is a topic of the next lecture.
"This is a topic of the next lecture.
"Now this is a topic of the next lecture.
we're going to talk about how to use
We're going to talk about how to use
probably some models to somehow get rid
probabilistic models to somehow get rid
of these common words.
This lecture is a continued discussion
of latent aspect rating analysis.
Earlier we talked about how to solve
the problem of Laura in two stages when
the problem of Lara in two stages when
we first do segmentation of different
aspects and then we use a little
regression model to learn the aspect
ratings and letting the weights.
Now, it's also possible to develop a
unified generating model for solving
unified generative model for solving
this problem, and that is we not only
modeling, we not only model the
generation of overrating based on text,
we also model the generation of text
and so a natural solution would be to
use topic model.
So given an entity, we can assume there
are aspects that are described by word
distributions.
Topics and then we can use a topic
model to model the generation of the
review text.
Our assumed the words in the review
text are drawn from these
distributions.
In the same way as we assumed for a
generative model like PSA.
And then we can then plug in the little
And then we can then plug in the latent
regression model to use the text to
further predict the.
further predict the
Overall rating and that means we first
predict the aspect rating and then
combine them with aspect weights to
predict the overall rating.
So this would give us a unified
generative model where we model both
the generation of text and the overall
rating conditional text.
rating condition on text.
So we don't have time to discuss this
model in detail, as in many other cases
in this part of the course where we
discuss the cutting edge topics.
But there is a reference site here
where you can find more details.
So now I'm going to show you some
simple results that you can get by
using this kind of generating models.
using this kind of generative models.
First it's about rating decomposition.
So here what you see are the decomposed
ratings for three hotels that have the
same overall rating.
So if you just look at the overall
rating you don't.
You can't really tell much difference
between these hotels, but by
decomposing these ratings into aspect
ratings we can see some hotels have
higher ratings for some.
Dimensions like value, but others might
score better in other dimensions like
location and so this can reveal
detailed opinions at the aspect level.
Here, the ground truth is shown in the
Now here, the ground truth is shown in the
plans, so this also allows you to see
whether the prediction is accurate.
It's not always accurate, but it's
mostly still reflecting some of the
trends.
The 2nd result to compare different
The 2nd result is to compare different
reviewers on the same hotel so the
table shows the decompose ratings for
two reviewers about same hotel again
their high level overall ratings are
the same.
So if you just look at the overall
ratings, you don't really get that much
information about the difference
between the two reviews.
But after you decompose the ratings you
can see clearly they have high scores
on different dimensions.
So this shows that the model can reveal
differences in.
Opinions of different reviewers and
such a detailed understanding can help
us understand better about reviews and
also better about their feedback on the
hotel.
This is something very interesting
because this is in some sense some
byproduct in our problem formulation.
We did not really have to do this, but
the design of the generated model has
the design of the generative model has
this component and these are sentiment
waits for words in different aspects.
And you can see the highly weighted
words versus the negatively load
words versus the negatively lower
weighted words here for each of the
four dimensions.
Value, rooms, location and cleanliness.
I added the top words, cleared it,
makes sense, and the bottom words also
makes sense.
So this shows that with this apology,
we can also learn sentiment information
directly from the data.
Now this kind of laxing is very useful
becausw in general award like long,
becausw in general a word like long,
let's say, may have different the
sentiment polarities for different
context.
So if I say the battery life of this
laptop is long, then that's positive.
But if I say the rebooting time for the
laptop is long, that's bad, right?
So even for reviews about the same
product laptop, the word loan.
product laptop, the word long
Is ambiguous, it could mean positive or
could be negative, but this kind of
lexicon that we can learn by using this
kind of generating models can show
kind of generative models can show
whether a word is positive for a
particular aspect, so this is clearly
very useful, and in fact such a lexicon
can be directly used to tag other
reviews about hotels or tag comments
about the hotels in social media like
tweets.
Ann, what's also interesting that since
And, what's also interesting that since
this is an almost computer and
supervised, assuming that the reviews
with overall ratings are available, and
then this can allow us to learn from
potentially a large amount of data on
the Internet to reach sentiment
lexicon.
And here are some results to validate
the preference weights.
Remember, the model can infer whether a
reviewer cares more about service or
the price.
Now, how do we know whether the
inferred weights are correct and this
poses a very difficult challenge for
evaluation.
Now here we show some interesting way
of evaluating without what you here are
of evaluating result. what you here are
the prices of hotels in different
cities, and these are the prices of
hotels that are favored by different
groups of reviews.
The top ten other reviewers with the
highest in Ferd value to other aspect
highest inferred value to other aspect
ratio.
So for example, value versus location
value versus room etc.
But the top ten other reviewers that
But the top ten are the reviewers that
have the highest ratios by this
measure.
And that means these reviewers tend to
put a lot of weight on value as
compared with other dimensions.
That means they really emphasize on
value.
The bottom then, on the other hand, are
The bottom ten, on the other hand, are
the reviews that have the lowest ratio.
What does that mean?
Well, that means these reviewers have
put higher weights on other aspects
than value, so those are people that
care about the another dimension and
they didn't care so much about the
value in some sense by this, less
compared with the top ten group.
Now these ratios are computer based on
the in ferd weights from the model.
the inferred weights from the model.
So now you can see the average prices
of hotels are favored by toptenreviews
are indeed and much cheaper than those
that are favored by the bottom 10.
And this provides some.
And this provides some
Indirect way of validating the infer
wait.
It just means the weights are not
random and they are actually meaningful
here and in comparison with the average
price in these three cities, you can
actually the top ten tends to have
below average price, whereas the bottom
time where they care a lot about other
things like service or room condition
tend to have hotels that have higher
prices than average.
So with these results we can build a
lot of interesting applications.
For example, direct application would
be the generator rated aspect, the
summary.
An because of the decomposition, we can
now generate the summaries for each
aspect.
The positive sentence is negative
sentences about each aspect.
It's more informative than original
review that has just overall rating and
review test.
He also mother results about the
Here are also mother results about the
aspects discovered from reviews with
ratings.
low ratings.
These are MP3 three reviews an these
results show that the model can
discover some interesting aspects
commented on low overall ratings versus
those high overall ratings, and they
care more about the different aspects.
Or they comment more on different
aspects.
So that can help us discover, for
example, consumers trained in
appreciating different features of
product.
For example, one might have discovered
the trend that people tend to screens
the trend that people tend to like large screens
of cell phones or lightweight of laptop
etc.
And such knowledge can be useful for
manufacturers to design their next
generation of products.
Here are some interesting results on
analyzing users rating behavior.
So what you see is average weights on
different dimensions by different
groups of reviewers.
Anne on the left side you see the
And on the left side you see the
weights of reviews that the expensive
weights of reviews like the expensive
hotels they give.
The whole expensive hotels five stars
and you can see their average weights
tend to be more focused on service and
that suggests that people might be
expensive hotels because of good
service.
And that's not surprising as also
another way to validate the inferred
weights.
But if you look at the right side where
look at the column of five stars, these
are the reviewers that the cheaper
are the reviewers that like the cheaper
hotels and they give cheaper hotels,
five stars as we expected, and they put
more weight on value and that's why
they like the cheap hotels.
they like the cheaper hotels.
But if you look at the when they didn't
like expensive hotels or cheaper hotels
and you seal it tended to have more
weights on the condition of the room
cleanliness.
So this shows that by using this model
we can infer some information that's
very hard to obtain, even if you read
all the reviews.
Even if you read all the reviews, it's
very hard to infer such preferences or
such emphasis.
So this is a case where text mining
algorithms can go beyond what humans
can do to review interesting patterns
in the data, and this of course can be
very useful.
You can compare different hotels,
compare the opinions from different
consumer groups in different locations,
and of course the model is general.
It can be applied to any reviews with
overall ratings, so this is very useful
technique that can support a lot of
text mining applications.
Finally, there is also some result on
applying this model for personalized
ranking or recommendation of entities.
So because we can infer the reviewers
weights on different dimensions, we can
allow a user to actually say what do
you care about.
So, for example, if a query here that
shows 90% of the way it should be on
value and 10% on others.
So that just means I don't care about
other aspects, I just care about
getting a cheap hotel.
My emphasis is on the value dimension.
Now what we can do is such a query is
that we can use reviewers that we
believe have a similar preference to
recommend the hotels for you.
How can we know that we can infer the
weights of those viewers on different
weights of those reviewers on different
aspects?
We can find the reviewers whose weights
or more precise or host Inn in Ferd
or more precise whose inferred
weights or similar to yours and then
use those reviewers to recommend the
use those reviews to recommend the
hotels for you.
And this is what we call a personalized
or rather query specific
recommendation.
The non personalized recommendation
results are shown on the top.
An you can see the top results
generally have much higher price than
the Law Group, and that's because when
the low Group, and that's because when
reviewers cared more about the value as
dictated by this query and they tend to
really have favor low price hotels.
So this is yet another application of
this technique.
And shows that by doing text mining we
can understand the users better.
And once we can end users better, we
can serve these users better.
So to summarize our discussion of
opinion mining in general, this is a
very important topic and with a lot of
applications.
Ann as a task sentiment analysis can be
And as a task sentiment analysis can be
usually done by using just text
categorization, but standard techniques
tend not to be enough and so we need to
have enriched feature representation.
And we also need to consider the order
of those categories and we talk about
the ordinal regression.
For solving this problem.
We have also shown that generating
We have also shown that generative
models are powerful for mining latent
user preferences, in particular in the
generating model for letting the rating
regression, we embed some interesting
preference information and sentiment
weights of words in the model.
As a result, we can learn those useful
information when fitting the model to
the data.
Most approaches have been proposed and
evaluated for product reviews, and that
was the cause in such a context of the
opinion Holder an opinion target or
opinion holder an opinion target or
clear and they are easy to analyze and
there of course also have a lot of
practical applications, but opinion
mining from news and social media is
also important, but that's more
difficulty than analyzing review data,
difficult than analyzing review data,
mainly because the opinion holders
antenna target Oregon.
and opinion target are all.
and opinion targets are all.
Implicit and so that calls for natural
implicit and so that calls for natural
language processing techniques to
uncover them accurately.
So here are some suggested readings,
the first 2.
Our small books that are excellent
are small books that are excellent
reviews of this topic where you can
find a lot of discussion about the
other variations of the problem and
techniques proposal for solving the
problem.
The next two papers are about the
generating models for letting the
generative models for letting the
aspect rating analysis.
The first one is about solving the
problem using two stages and the second
one is about the unified model where
topic model is integrated with the
regression model.
To solve the problem using a unified
model.
This lecture is about opinion mining
and sentiment analysis covering its
motivation.
In this lecture were gonna start
In this lecture we are going to start
talking about mining a different kind
of knowledge, namely knowledge about
the observer or humans that have
generated text data.
In particular, we're going to talk
about the opinion mining and sentiment
analysis.
As we discussed earlier, text data can
be regarded as the data generated from
humans as subjective sensors.
In contrast, we have other devices such
as video recorder that can report
what's happening in the real world
objectively to generate the view data,
objectively to generate the video data,
for example.
Now a mean difference between text data
Now the main difference between text data
and other data like video data is that
it has rich and rich opinions an the
it has rich and rich opinions and the
content tends to be subjective because
it's generated from humans.
Now this is actually unique advantage
of text.
of text data.
of text data,
It all as compared with other data
as compared with other data
because of the office is a great
because it offers us a great
opportunity to understand the
observers.
We can mine text it out to understand
We can mine text out to understand
We can mine the text data to understand
that opinions understand the people's
the opinions understand the people's
the opinions, understand the people's
preferences, how people think about
something.
So this lecture ended.
So this lecture and
The following actions will be made
the following lectures will be made
the following lectures will be mainly
about how we can mine an analyze
about how we can mine and analyze
opinions buried in a lot of text data.
So let's start with the concept of
opinion that it's not that easy to
formally define opinion, but mostly we
would define opinion as a subjective
statement describing what a person
believes or thinks about something.
Now I highlighted it quite a few words
Now I highlighted a quite a few words
here, and let's be cause it's was
here, and that's because it's was
here, and that's because it was
thinking a little more about these
words an that would help us better
words and that would help us better
understand what scene, opinion and this
understand what is in the opinion and this
understand what's in the opinion and this
further helps us to define opinion more
formally, which is always needed to
computationally solve the problem of
opinion mining.
So let's first look at the keyboard
So let's first look at the keyword
subjective here.
Now this is in contrast with object if
Now this is in contrast with objective
statement or factual statement.
Those statements can be proven right or
Those statements can be proved right or
wrong.
And this is a key differentiating
factor from opinion, which tends to be
not easy to prove wrong or right
because it reflects what a person
thinks about something.
So in contrast, object statement can
So in contrast, objective statement can
usually be proved wrong or correct.
For example, you might say this
computer has.
computer has a
Screen Anna battery.
screen and a battery.
Now that's something you can check.
It's either having a battery or not.
But in contrast, if you think about the
sentence such as this laptop has the
best battery.
Or this laptop has a nice screen and
Or this laptop has a nice screen, now
these statements are more subjective
and it's very hard to prove whether
it's wrong or correct.
So opinion is subjective statement.
Anne next, let's look at the keyword
And next, let's look at the keyword
person here and that indicates this
opinion Holder 'cause when we talk
about opinion, it's about the opinion
held by someone and then we notice that
there is something here.
So that's the target of the opinion.
The opinions expressed on this
The opinions is expressed on this
something.
And now, of course, believes all things
And now, of course, believes or thinks
implies that opinion would depend on
implies that the opinion would depend on
the culture or background and context
in general, because of person might
think differently in the different
context.
People from different background may
also think in different ways.
So this analysis shows that there are
multiple elements that we need to
include in order to characterize
opinion.
an opinion.
So what's the basic opinion
representation like it should include
representation like, well,  it should include
at least three measurements, right?
First it has to specify what's the
opinion Holder.
So whose opinions this second must also
So whose opinions this, second must also
So whose opinion is this, second must also
specify the target.
What's this opinion about?
And 3rd, of course we want opinion
content and So what exactly is the
opinion?
If you can identify this, we get a
basic understanding of opinion and can
basic understanding of an opinion and can
already be useful.
Sometimes if you want to understand
further, we want to enrich the opinion
representation.
And that means we also want to
understand, for example, the context of
the opinion and what situation was
opinion expressed.
For example, in what time was it
expressed?
We also would like to deeply understand
opinion sentiment an this is to
opinion sentiment and this is to
understand.
understand,
What the opinion tells us about the
what the opinion tells us about the
opinion holders feeling, for example,
opinion holder's feeling, for example,
is this opinion positive or negative?
Or perhaps the opinion Holder is was
Or perhaps the opinion holder is was
Or perhaps the opinion holder was
happy or sad.
And so such understanding obviously
goes beyond just extracting the opinion
content and needs some analysis.
So let's take a simple example of a
product radio.
product review.
In this case, this actually explicitly
In this case, this actually explicit
opinion Holder and explicitly target,
opinion Holder and explicit target,
so it's all.
so it's
It's obviously what's opinion Holder,
and that's just a reviewer, and it's
also often very clear what's the
opinion targeted, and that's the
opinion target, and that's the
product being reviewed.
For example iPhone 6.
When the review was posted, usually you
can extract this such information
can extract such information
easily.
Now the content of course is the review
text that's in general also easy to
obtain.
So you can see product reviews or
So you can see product reviews are
fairly easy to analyze in terms of
obtaining a basic opinion
representation.
But of course, if you want to get more
information, we might want to know the
context.
For example, the review was written in
2015.
Or we want to know that the sentiment
of this review is positive, and so this
additional understanding of course adds
value to mining the opinions.
Now you can see in this case the task
is relatively easy, and that's the
is relatively easy, and that's
cause.
because.
The opinion Holder and opinion target
that have already been identified.
Now let's take a look at the sentence
in the news.
In this case, we have impressed Holder
In this case, we have implicit holder
and impressive target.
and implicit target.
And the task is in general harder so we
can identify.
can identify
Opinion hold up here and that's
opinion holder here and that's
governor of Connecticut.
We can also identify the target.
So one target is Hurricane Sandy.
But there is also another target.
But there is also another target,
Imagine that which is a hurricane of
which is a hurricane of
1938.
So what's the opinion?
Well, this negative sentiment here
that's indicated by words like a bad
and worst.
And we can also then.
And we can also then
Identify the context.
identify the context.
New England in this case.
Unlike in the product review, all these
Now unlike in the product review, all these
elements must be extracted by using
natural language processing techniques.
So the task is much harder and we need
a deeper natural language processing.
And these examples also.
And these examples also,
Suggest that a lot of work can be
suggest that a lot of work can be
easily done for product reviews, and
that's indeed what has happened.
Analyzing sentiment in news is still
quite difficult.
It's more difficult than the analysis
of opinions in product reviews.
Now there are also some other
interesting variations.
In fact, the Hill we're going to
In fact, here we're going to
examine the variations of opinions more
systematically.
First, listing think about the opinion
First, lets think about the opinion
Holder.
Now the Holder could be an individual
or could be a group of people and
sometimes opinion was from a committee
or from a whole country of people.
Opinion Target Council very lot.
Opinion Target Council vary a lot.
Opinion Target can sometime vary a lot.
Opinion Target can also vary a lot.
It can be about 1 entity particular
It can be about 1 entity, a particular
person particular product, that
person, a particular product, that
person, a particular product, a
particular policy, etc.
But it could be about a group of
products.
Could be about the product from a
company in general.
Could also be very specific about one
attribute.
attribute,
Ashfield of Entity for example.
attribute of Entity for example.
It's just about the battery of iPhone.
It could be about someone elses opinion
It could be about someone else opinion
and one person might.
and one person might
The comment on another persons opinion
comment on another persons opinion
etc.
So you can see there is a lot of
variation here that will cause the
problem to vary a lot.
Law opinion content, of course, can
Now opinion content, of course, can
also vary about on the surface.
also vary a lot on the surface.
You can identify one sentence opinion
or one phrase opinion, but you can also
have longer text to express their
have longer text to express the
opinion like a whole article.
And Furthermore, we identify the
And Furthermore, we can identify the
variation in the sentiment or emotion
dimension.
That's about the feeling of the opinion
Holder.
So we can distinguish positive versus
negative or neutral or happy versus
sad, etc.
Finally, the opinion context can also
vary.
We can have simple context, like
different time or different locations,
but there could be also complex text
such as some background topic being
discussed.
So when opinion expressed in the
particular discourse context, it has to
be interpreted in different ways than
when it's expressed in another context,
so the context can be very rich to
improve the entire discourse.
improve the entire discourse
Context of opinion from computational
context of opinion. From computational
perspective, we're most interested in
what opinions?
what opinions
Can be extracted from text data, so it
can be extracted from text data, so it
turns out that we can also
differentiate distinguish different
kinds of opinions in text it or from
kinds of opinions in text data from
computation perspective.
First, the Observer might make a
comment about the opinion target in the
observed world.
So in this case we have the authors
So in this case we have the author's
opinion.
For example, I don't like this phone at
all, and that's opinion of this author.
In contrast, the text might also.
In contrast, the text might also
Report opinions about others so the
report opinions about others so the
person could also make observation
about another persons opinion an report
about another persons opinion and report
about another person's opinion and report
this opinion.
So for example, I believe he loves the
painting and that opinion is really
about the is really expressed by
about, is really expressed by
another person.
Here, so it doesn't mean this author
loves that painting.
So clearly the two kinds of opinions
need to be analyzed in different ways
and sometimes in product reviews you
can see, although mostly the opinions
are from this reviewer.
Sometimes a reviewer might mention
opinions of his friend or her friend,
right?
An another complication is that there
And another complication is that there
may be in direct opinions or in Ferd
may be indirect opinions or infered
may be indirect opinions or inferred
opinions that can be obtained by making
inferences on what's expressed in the
text that might not necessarily look
like opinion.
For example, one statement might be
this phone ran out of battery in just
one hour.
Now this is in a way, a factual
statement, 'cause you know it's either
true or false, right?
You can even verify that.
But from this statement one can also
infer some negative opinions about the
quality of the battery of this form or
quality of the battery of this phone or
the feeling of the opinion Holder about
the feeling of the opinion holder about
the battery.
In the opinion, Holder clearly wish the
battery to last longer.
So these are interesting variations
that we need to pay attention to when
we extract opinions.
Also, for this reason about the
interact opinions.
indirect opinions.
It's often also very useful to extract
it or whatever the person had said
about the product, and sometimes
factual sentences like this.
factual sentences like this
Also very useful.
are also very useful.
So from practical viewpoint, sometimes
we don't necessarily extract the
subjective sentences.
Instead, would you just get all the
sentences that are about opinions that
are useful for understanding the person
or understanding the product being
or understanding the product we
or understanding the product that we are
commanded?
comenting on.
commenting on.
So the task of opinion mining can be
defined as taking taxes, eyes input to
defined as taking text data as input to
generate a set of opinion
representations.
Each representation we should identify
In each representation we should identify
opinion Holder, target content and
context.
Ideally we can also infer opinion
sentiment from the content and context
to better understand.
to better understand
The opinion.
the opinion.
Now often some elements of the
replenishing already know.
representation are already know.
representation are already known.
I just gave a good example.
I just gave a good example,
In the case of product reviews where
in the case of product reviews where
the opinion Holder an opinion target
the opinion Holder and opinion target
are often explicitly identified, and
that's not why this turns out that we
that's not why this turns out to be one of the
wanted the simplest opinion mining
simplest opinion mining
tasks.
Now it's interesting to think about
other tasks that might be also simple,
because those are the cases where you
can easily build applications by using
opinion mining techniques.
So now that we have talked about what
is opinion mining and we have defined
the task, let's also just talk a little
bit about the why opinion mining is
very important and why it's very
useful.
So here I identify three major reasons,
3 broad reasons.
The first is it can help decision
support.
I can help us optimize our decisions.
We often look at the other peoples
opinions and look at the reader reviews
in order to make a decision like
buying, buying a product, or using the
service.
We also
Would be interested in others opinions.
When we decide whom to vote, for
example.
And policymakers may also want to know
peoples opinions when designing a new
policy.
So that's one general kind of
applications.
And it's very proud, of course.
And it's very broad, of course.
The second application is to understand
people, and this is also very
important.
For example, that help understand
For example, that can help understand
For example, it can help understand
peoples preferences.
So and this could help us better serve
people.
For example, we can optimize the
product search engine Optimizer
product search engine, optimize
recommended system if we know what
recommender system if we know what
people are interested in, what people
think about products.
It can also help her with advertising,
of course, and we can have targeted
advertising if we know what kind of
people tend to know to like what kind
of product.
Now the third kind of applications can
be called voluntary survey.
Now this is most little supported
Now this is mostly to support
research that used to be done by doing
surveys doing manual service question
surveys, doing manual surveys. question
surveys, doing manual surveys, questioning
answering.
People need to fill in forms to the
People need to fill in forms to
answer some questions.
This is directly related to humans as
Now this is directly related to humans as
sensors, and we can usually aggregate
opinions from a lot of humans to kind
of assess the general opinion.
Now this is would be very useful for
business intelligence, where product
manufacturers want to know.
manufacturers want to know,
Where their products have advantages
where their products have advantages
over others.
What are the winning features of their
product or winning features of
competitive products?
Market research has to do with
understanding consumers opinions and
this is clearly very useful directed
this is clearly very useful, directed
for that.
They don't driven social science
Data Driven social science
research can benefit from this cause
research can benefit from this because
they can do text mining to understand
the peoples opinions.
the people's opinions.
And if we can aggregate a lot of
opinions from social media from a lot
of public information, then you can
actually do some study of some
questions.
For example, we can study the behavior
of people.
of people
On social media or in on social
on social media or in on social
on social media or in social
networks, and these can be regarded as
voluntary survey.
voluntary survey,
But down by those people.
but done by those people.
At in general, we can gain a lot of
And in general, we can gain a lot of
advantage in any prediction task
because we can leverage the text that
because we can leverage the text data
are as extra data about any problem and
as extra data about any problem and
so we can use text based prediction
techniques to help you make prediction
or improve the accuracy of prediction.
This factor is a continued discussion
This lecture is a continued discussion
of generating proper risk models for
of generating probabilistic models for
of generative probabilistic models for
tax of classroom.
text clustering.
In this lecture, we're going to
continue talking about the tax capture
text clustering, particularly
generating proper risk models.
generating probabilistic models.
"generative
So this is a slide that you have seen
earlier where we have written down the
likelihood function for a document.
With two distributions in two component
mixture model for document clustering.
Now in this sector, we're going to
Now in this lecture, we're going to
generalize this to include the K
clusters.
Now if you look at the formula and
think about the question how to
generalize it, you will realize that
all we need is to add more terms like
what you have seen here.
So you can just add more theaters and
So you can just add more thetas and
the probabilities of theaters and the
the probabilities of thetas and the
probabilities of generating D from
those Cedars.
those thetas.
So this is precisely what we're going
to use.
This is general presentation of the
mixture model for document clustering.
So as more cases we follow these steps
using a generated model.
First think about our data, right?
So in this case our data is a
collection of documents and documents
collection of documents N documents
denoted by the subway I.
denoted by the sub I.
And then we talk about the model.
Think about the model.
In this case, we design A mixture of K
In this case, we design a mixture of K
unigram language models.
It's a little bit different from the
topic model.
But we have similar parameters.
We have a set of theorized denote the
We have a set of theta i denote the
We have a set of theta i's denote the
word distributions corresponding to the
K unigram language models.
We have P of each seed I as the
We have P of each theta I as the
probability of each of the K
probability of selecting each of the K
distributions regenerated document.
distributions to generate the document.
Now loads that, although our goal is to
Now note that, although our goal is to
find the clusters and we actually have
used a more general notion of a
probability of each cluster.
And this, as you see later, would allow
us to assign a document to the.
Cluster that has the highest report
Cluster that has the highest probability
viniti of being able to generate the
of being able to generate the
document.
So as a result, we can also recover
some other interesting.
Properties.
As you will see later.
So the model basically would make the
following assumption about the
generation of the document.
We first choose a theory I according to
We first choose a theta I according to
probability of zero I and then generate
probability of theta I and then generate
all the words in the document using
this distribution.
Note that it's important that we use
this distributed generator.
All the words in the document.
This is very different from topic
model, so the likelihood function would
be like what you are seeing here.
So the.
You can take a look at the formula
here.
We have used the different.
Notation here in the second line of
this.
Of this equation.
But you can see now the.
Location has been changed to use unique
notation has been changed to use unique
word in the vocabulary in the product
instead of particular position in the
document.
So from X subject to W is a change of
So from X sub J to W is a change of
notation, and this change allows us to
show the estimation formulas more
easily and you have seen this change
also in the topic model presentation,
but it's basically still just a product
of the probabilities of all the words.
I and so with the lack of functioning.
Now we can talk about how to do
parameter estimation.
Here we can simply use the maximum
likelihood estimator, so that's just a
standard way of doing things, so all
should be familiar to you now, it's
just a different model.
So after we have estimated parameters,
So after we have estimate the parameters,
how can we then allocate clusters to
the documents?
Let's take a look at this situation
more closely, so we just repeated the
parameters here.
For this mixture model.
Now, if you think about what we can get
by is made such a model, we can
by estimate such a model, we can
actually get more information than what
we need for doing clustering, right?
So see the.
So see theta.
So theta.
I, for example, represents the content
of class lie.
of class I.
This is actually a byproduct.
It helps summarize what the cluster is
about to look at the top terms in this
cluster or in this world distribution.
cluster or in this word distribution.
And they will tell us what the cluster
is about.
An P of siedi can be interpreted as.
An P of theta i can be interpreted as.
Indicating the size of cluster because
it tells us how likely cluster would be
used to generate the document.
The more likely a cluster is used to
generate the document, we can assume
the larger the cluster sizes.
the larger the cluster size is.
Note that unlike in PSA and this
Note that unlike in PLSA and this
probability of the die is not dependent
probability of theta I is not dependent
on D.
Now.
You may recall that the top choice in
You may recall that the topic choice in
each document actually depends on D.
That means each document can have a
potentially different choice of topics,
but here we have a generic choice
probability for all the documents.
But of course, given a particular
document that we still have to infer
which topic is more likely.
To generate the document so in that
sense, we can still have a document
independent.
dependent probability of clusters.
So that's two computer.
So lets look at a key problem
So lets look at a key problem assigning
So lets look at a key problem of assigning clusters
So lets look at a key problem of assigning clusters for documents
So lets look at a key problem of assigning document to clusters or assigning clusters to documents
See sub D here and this will take one
Lets to compute the C sub D here and this will take one
of the values in the range of one 2K to
of the values in the range of one to k to
indicate which cluster should be
assigned to D.
Let's first you might think about a way
to use likelihood online, and that is
to use likelihood only, and that is
to assign D to the cluster
corresponding to the topic CDI.
corresponding to the topic Theta I.
That most likely has been used to
generate the.
generate D.
So that means we're going to choose one
of those distributions that gives the
of those distributions that gives D
highest probability.
In other words, we see which
distribution has a content that matches
our the best.
our D best.
Intuitively, that makes sense.
However, this approach does not
consider the size of clusters, which is
also available to us.
And so a better way is to use the light
And so a better way is to use the likelihood
hold together with the prior.
together with the prior.
In this case the price is P of Theta I.
In this case the prior is P of Theta I.
And together, that is, we're going to
use the base formula to compute the
posterior probability of Theta given D.
And if we choose seat are based on this
And if we choose theta based on this
posterior probability and we would have
the following formula that you see
here.
On the bottom of this slide, and in
this case, we're going to choose the
theater that has a large P of Theta I.
theta that has a large P of Theta I.
That means a large cluster and also a
high probability of generating D.
So we're going to favor cluster that's
So we're going to favor a cluster that's
large and also consistent with the
document.
And that intuitively makes sense
because the chance of a document being
a large cluster is generally higher
than in a small cluster.
So this means once we can estimate the
parameters of the model, then we can
easy to solve the problem of document
easily solve the problem of document
clustering.
So next we have to discuss how to
actually compute the estimate of the
model.
