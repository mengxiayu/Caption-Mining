[
  {
    "text": "text",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're go",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. espe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "lem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially for decision making or for c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially for decision making or for completing whatever tasks that require text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "text data into actionable knowledge that we can use in (the) real world. especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. so",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "equire text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. so a more general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of text and non",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " real world problems of data mining, we also tend to have other kinds of data that are non textual. so a more general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "are non textual. so a more general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "al. so a more general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text da",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ta as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the lands",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ning of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "on text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows the process of gener",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows the process of generating text data in more detail. most specifically, human sensor or human observer would lo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows the process of generating text data in more detail. most specifically, human sensor or human observer would look at the world from some perspective. different people would be looki",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "y it looks like, of course. but then the human would express what the person has observed using a natural language such as english, and the result is text data. of course, the person could have used a different language to express what he or she has observed. in that case, we might have text data of mix",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "e result is text data. of course, the person could have used a different language to express what he or she has observed. in that case, we might have text data of mixed languages for different languages. so the main goal of text mining is actually to revert this process of generating test data. and we h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "anguage to express what he or she has observed. in that case, we might have text data of mixed languages for different languages. so the main goal of text mining is actually to revert this process of generating test data. and we hope to be able to uncover some aspect in this process. and so specifically",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "me aspect in this process. and so specifically we can think about the mining, for example, knowledge about the language. and that means by looking at text data in english, we may be able to discover something about english... some usage of english... some patterns of english. so this is 1 type of mining",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": ". if you look at the picture, we can also \"then mine knowledge about the \"\"observed\" \"world\"\".\" as so, this has much to do with mining the content of text data. we're going to look at the what the text data are about and then try to get the essence of it. or extracting high quality information about a p",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " mine knowledge about the \"\"observed\" \"world\"\".\" as so, this has much to do with mining the content of text data. we're going to look at the what the text data are about and then try to get the essence of it. or extracting high quality information about a particular aspect of the world that we're intere",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "s mind. if you look further then you can also imagine we can mine knowledge about this observer himself or herself. so this has also to do with using text data to infer some properties of this person. and these properties could include the mood of the person or sentiment of the person. and note that we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "operties could include the mood of the person or sentiment of the person. and note that we distinguish the observed the world from the person because text data can describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "cribe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine k",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. finally, if you look at the picture to the left side of this picture, then you can see we can certainly ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "t the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? so indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. and we want to predict the value of certain interesting va",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "nd we want to predict the value of certain interesting variables. so this picture basically covered multiple types of knowledge that we can mine from text in general. when we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "types of knowledge that we can mine from text in general. when we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. for example, after we mine the content of text data, we might generate some summary of content, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "we could also use some of the results from mining text data as intermediate results to help the prediction. for example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world. now of cours",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ent, and that summary could be then used to help us predict the variables of the real world. now of course, this is still generated from the original text data, but i want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very import",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "bles of the real world. now of course, this is still generated from the original text data, but i want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important. and that's why here we show that the results of some other minin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "help with the prediction, is very important. and that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction. in fact, when we have a non-text data, we could also use the non",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction. in fact, when we have a non-text data, we could also use the non-text data to help prediction. and of course, it depends on the problem. in general, non-text data can be very importa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "data and mining knowledge above the observer can all be very helpful for prediction. in fact, when we have a non-text data, we could also use the non-text data to help prediction. and of course, it depends on the problem. in general, non-text data can be very important for such prediction tasks. for exa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "fact, when we have a non-text data, we could also use the non-text data to help prediction. and of course, it depends on the problem. in general, non-text data can be very important for such prediction tasks. for example, if you want to predict the stocks. stock prices or changes of stock prices based o",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ict the stocks. stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables. now in this case, obviously the historical stock price data would be very important for this predict",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "orld variables. now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction. now non-text data can be also useful ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction. now non-text data can be also useful for analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ld be very useful for the prediction and we can combine both kinds of data to make the prediction. now non-text data can be also useful for analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data general",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " the prediction and we can combine both kinds of data to make the prediction. now non-text data can be also useful for analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data generally have also context ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " combine both kinds of data to make the prediction. now non-text data can be also useful for analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data generally have also context associated. for example, t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "o useful for analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data generally have also context associated. for example, the time, the location, of that associated with the text data and these are usefu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "or analyzing text by supplying context. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data generally have also context associated. for example, the time, the location, of that associated with the text data and these are useful context ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ntext. when we look at the text data alone will be mostly looking at the content and opinions expressed in text. but text data generally have also context associated. for example, the time, the location, of that associated with the text data and these are useful context information. and the context can ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "nd opinions expressed in text. but text data generally have also context associated. for example, the time, the location, of that associated with the text data and these are useful context information. and the context can provide interesting angles for analyzing text data. for example, we might partitio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " text data generally have also context associated. for example, the time, the location, of that associated with the text data and these are useful context information. and the context can provide interesting angles for analyzing text data. for example, we might partition text data into different time pe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "o context associated. for example, the time, the location, of that associated with the text data and these are useful context information. and the context can provide interesting angles for analyzing text data. for example, we might partition text data into different time periods because of the availabi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ocation, of that associated with the text data and these are useful context information. and the context can provide interesting angles for analyzing text data. for example, we might partition text data into different time periods because of the availability of time. now we can analyze text data in each",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ata and these are useful context information. and the context can provide interesting angles for analyzing text data. for example, we might partition text data into different time periods because of the availability of time. now we can analyze text data in each time period and then make a comparison. si",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "or analyzing text data. for example, we might partition text data into different time periods because of the availability of time. now we can analyze text data in each time period and then make a comparison. similarly, we can partition text data based on locations or any metadata that's associated to fo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "periods because of the availability of time. now we can analyze text data in each time period and then make a comparison. similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. so in this sense, non-text data can actually prov",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "arly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. so in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the lan",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "at's associated to form interesting comparison scenarios. so in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "parison scenarios. so in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. we could analyze the sentiment in",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. we could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. in this co",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "lysis of content or the language usage or the opinions about the observer or the authors of text data. we could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. in this course we're going to selectively cover some of those topics",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "t the observer or the authors of text data. we could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. in this course we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "o cover most of these general topics. first, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical kno",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge ab",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "anding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ul lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "on mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data. so this slide also serves as a road map for this cours",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "dge about the observer. and finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data. so this slide also serves as a road map for this course. and will use this as outline for the topics that will cover in the rest of this course",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "this lecture is about the text representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent tex",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "this lecture is about the text representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent text in many different ways. let's take a look at this example ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ext representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent text in many different ways. let's take a look at this example sentence again. we can represent this sentence in many different ways. 1st. we can always r",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "store them in the computer. when we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. but unfortunately, using such a representation would not help us do semantic analy",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. but unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mini",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " any text data. but unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. the reason is because we're not even recognizing words. so as a string we're going to keep all the spaces and these ascii symbols. we can per",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " so as a string we're going to keep all the spaces and these ascii symbols. we can perhaps count how... what's the most frequent character in english text, or the correlation between those characters, but we can't really analyze semantics. yet this is the most general way of representing text, because w",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "in english text, or the correlation between those characters, but we can't really analyze semantics. yet this is the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmenta",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "t we can't really analyze semantics. yet this is the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "guage text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but in the form of a sequence of words. so here we see that we can identify words like: a, dog, is, chasing, etc. now with this level of representat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "s. when we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. so representing text data as a sequence of words opens up a lot of interesting analysis possibilities. however, this level of representation is slightly less general than",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "cters, because in some languages such as chinese, it's actually not that easy to identify all the word boundaries, because in such a language you see text as a sequence of characters with no space in between. so you have to rely on some special techniques to identify words. in such a language, of course",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": ", etc. so this opens up a little bit more interesting opportunities for further analysis. note that i use the plus sign here, because by representing text as a sequence of part of speech tags. we don't necessarily replace the original word sequence representation instead, we add this as an additional wa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "part of speech tags. we don't necessarily replace the original word sequence representation instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. this enriches the representation of tex",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ext data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. this enriches the representation of text data and thus, also, \u00a0enables a more interesting analysis. if we go further then we'll be parsing the sentence to obtain a syntactic structure. now t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "sentation an it's also related to the knowledge graph that some of you may have heard of. that google is doing as a more semantic way of representing text data. however, it's also less robust than sequence of words or even syntactic analysis, because it's not always easy to identify all the entities wit",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ogical representation then we can have predicates and even inference rules. and with inference rules we can infer interesting, derived facts from the text. so that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes, and we can't do that all th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "used. and unfortunately, such techniques would require more human effort. and they are less accurate. that means there are mistakes. so if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors. so that also means it's still necessary to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "s based on, for example sequence of words. on the right side you see the arrow points down, to indicate that as we go down with our representation of text, it's closer to knowledge representation in our mind, and need for solving a lot of problems. now, this is desirable because as we can represent text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "text, it's closer to knowledge representation in our mind, and need for solving a lot of problems. now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. that's the purpose of text mining. so there is a trade off here between doing deeper ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "oblems. now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. that's the purpose of text mining. so there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ing. so there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust. but wouldn't actually give us the necessary deeper representation of knowledge. i should also say t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " doing shallow analysis, which is more robust. but wouldn't actually give us the necessary deeper representation of knowledge. i should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "per representation of knowledge. i should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. they are always in the loop. meaning that we should optimize the collaboration of human",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "of knowledge. i should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. they are always in the loop. meaning that we should optimize the collaboration of humans and computers. so",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ize the collaboration of humans and computers. so in that sense, it's ok that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "so in that sense, it's ok that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to g",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "so, as we explained, different textual representation tends to enable different analysis. in particular, we can gradually add more and more deeper analysis results to represent text dat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "ent textual representation tends to enable different analysis. in particular, we can gradually add more and more deeper analysis results to represent text data, and that would open up more interesting representation opportunities and also analysis capacities. so this table summarizes what we have just s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "interesting representation opportunities and also analysis capacities. so this table summarizes what we have just seen. so the first column shows the text recognition, the second visualizes the generality of such representation, meaning whether we can do this kind of representation accurate before all t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "cognition, the second visualizes the generality of such representation, meaning whether we can do this kind of representation accurate before all the text data, or only some of them, and third column shows the enabled analysis techniques. and the final column shows some examples of application that can ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "nal column shows some examples of application that can be achieved through this level of representation. so let's take a look at them. so as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general. and there are still some interesting applications th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "ms, but it's very robust, it's general. and there are still some interesting applications that can be done at this level. for example, compression of text doesn't necessarily need to know the word boundaries. although knowing word boundaries might actually also help. word based representation is very im",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text. and this can be the case. in research literature, a scientist want to know what are the most important research topics today or customer service peo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "n. syntactical structure representation. we can also generate the structure based feature features and those are features that might help us classify text objects into different categories. by looking at the structures, sometimes the classification can be more accurate. for example, if you want to class",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "is can be very useful for integrative analysis of scattered knowledge. for example, we can also add ontology on top of the extracted information from text to make inferences. a good example of application in this enabled by this level of representation is a intelligent knowledge assistant for biologists",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "rd based representation. these techniques are general and robust and thus are more widely used in various applications. in fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level. but obviously all these oth",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "pplications. in fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level. but obviously all these other levels can be combined and should be combined in order to support sophisticated applications. so to summ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "er levels can be combined and should be combined in order to support sophisticated applications. so to summarize, here are the major takeaway points. text representation determines what kind of mining algorithms can be applied. and there are multiple ways to represent text - strings, words, syntactic st",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "are the major takeaway points. text representation determines what kind of mining algorithms can be applied. and there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc. \"and these different representations should in general be co",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "this lecture is about the word association mining and analysis. in this lecture we're going to talk about how to mine associations of words from text. this is an example of knowledge about natural language that we can mine from text data. here's the outline. we are gooing to first talk about what i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "e we're going to talk about how to mine associations of words from text. this is an example of knowledge about natural language that we can mine from text data. here's the outline. we are gooing to first talk about what is word association and then explain why discovering such relations is useful and fi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "up in the same sequence. so these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data. discovering such world relations has many applications. first, such relations can be directly useful for improving accuracy of many nlp tasks, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n component expressions. so we'll learn the structure and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce addi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query mor",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "w, here are some intuitions about how to do that. let's first look at the paradigmatic relation. here we essentially can take advantage of similar context. so here you see some simple sentences about cat and dog. you can see they generally occur in similar context. and that, after all, is the definition",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "tially can take advantage of similar context. so here you see some simple sentences about cat and dog. you can see they generally occur in similar context. and that, after all, is the definition of paradigmatic relation. so on the right side you can see i extracted explicitly the context of cat and dog ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r in similar context. and that, after all, is the definition of paradigmatic relation. so on the right side you can see i extracted explicitly the context of cat and dog from this small sample of text data. so i have taken away cat and dog from the corresponding sentences so that you can see just the co",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "the definition of paradigmatic relation. so on the right side you can see i extracted explicitly the context of cat and dog from this small sample of text data. so i have taken away cat and dog from the corresponding sentences so that you can see just the context. now of course we can have different per",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "of cat and dog from this small sample of text data. so i have taken away cat and dog from the corresponding sentences so that you can see just the context. now of course we can have different perspectives to look at the context. for example, we can look at the what words occur in the left part of this c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "cat and dog from the corresponding sentences so that you can see just the context. now of course we can have different perspectives to look at the context. for example, we can look at the what words occur in the left part of this context. so we can call this left context. what words occur before we see ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "now of course we can have different perspectives to look at the context. for example, we can look at the what words occur in the left part of this context. so we can call this left context. what words occur before we see cat, cat or dog. so you can see in this case clearly dog and cat have similar left ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "t perspectives to look at the context. for example, we can look at the what words occur in the left part of this context. so we can call this left context. what words occur before we see cat, cat or dog. so you can see in this case clearly dog and cat have similar left context. you generally say his cat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o we can call this left context. what words occur before we see cat, cat or dog. so you can see in this case clearly dog and cat have similar left context. you generally say his cat or my cat, and you say also my dog and his dog. so that makes them similar in the left context. similarly, if you look at ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "nd cat have similar left context. you generally say his cat or my cat, and you say also my dog and his dog. so that makes them similar in the left context. similarly, if you look at the words that occur after cat and dog, which we can call right context and they also very similar in this case, of course",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "his dog. so that makes them similar in the left context. similarly, if you look at the words that occur after cat and dog, which we can call right context and they also very similar in this case, of course it's extreme case where you only see eats and in general you will see many other words. of course ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "where you only see eats and in general you will see many other words. of course that can follow cat and dog. you can also even look at the general context. and that might improve the all words in the sentence or in sentences around this word. and even in the general context you also see some similarity ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o even look at the general context. and that might improve the all words in the sentence or in sentences around this word. and even in the general context you also see some similarity between the two words. so this is just suggesting that we can discover paradigmatic relation by looking at the similarit",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o see some similarity between the two words. so this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. so for example, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "paradigmatic relation by looking at the similarity of context of words. so for example, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "on by looking at the similarity of context of words. so for example, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog woul",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " words. so for example, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ple, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and context of compute",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ilar are context of cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similari",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " cat and context of dog? in contrast, how similar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "lar are context of cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high. between the context of cat and dog, where",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "cat and context of computer? now intuitively, with imagine the context of cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high. between the context of cat and dog, whereas in the second th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "e similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high. between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " the first in the first case, the similarity value would be high. between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship. and then imagine what words occur after computer. in general",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "on? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of text. but here we are interested in knowing what other words are correlated with the verb eats. and what words can go with eat? and if you look at the rig",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "that meat will also occur. in contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text? because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ur. in contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text? because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence. so th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "currence of text? because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence. so this is in contrast to the question about eats and meat. this also helps explain the intuition behind the methods for",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o to summarize, the general ideas for discovering word associations are the following. for paradigmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagma",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "eas for discovering word associations are the following. for paradigmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count ho",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "digmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "t have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. and we're going to compare their co occurrences with their individual occurrences. we're going",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ver a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, our idea for discovering such relation is to look at the context of each word and th",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "aradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ey share similar contexts. namely, they occur in similar positions in text. so naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. so here's an example of context of word cat. here i have taken the word cat ou",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " text. so naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. so here's an example of context of word cat. here i have taken the word cat out of the context. and you can see we are seeing some remaining words ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. so here's an example of context of word cat. here i have taken the word cat out of the context. and you can see we are seeing some remaining words in the sentences that contain cat.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "d and then try to compute the similarity of those contexts. so here's an example of context of word cat. here i have taken the word cat out of the context. and you can see we are seeing some remaining words in the sentences that contain cat. now we can do the same thing for another word like a dog. so i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "g words in the sentences that contain cat. now we can do the same thing for another word like a dog. so in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " do the same thing for another word like a dog. so in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent the context and then define the similarity function?",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "another word like a dog. so in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent the context and then define the similarity function? so first we note that ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "n try to assess the similarity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent the context and then define the similarity function? so first we note that the context actually contains a lot of words. so they can be regarded as a pseudo docu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ord like dog. so now the question is, how can we formally represent the context and then define the similarity function? so first we note that the context actually contains a lot of words. so they can be regarded as a pseudo document. an imaginary document. but there are also different ways of looking a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " contains a lot of words. so they can be regarded as a pseudo document. an imaginary document. but there are also different ways of looking at the context. for example, we can look at the word that occurs before the word cat. we can call. we can call this context left1 context. so in this case you will ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "also different ways of looking at the context. for example, we can look at the word that occurs before the word cat. we can call. we can call this context left1 context. so in this case you will see words like my, his or big, a, the, etc. these are the words that can occur to the left of the world cat. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " ways of looking at the context. for example, we can look at the word that occurs before the word cat. we can call. we can call this context left1 context. so in this case you will see words like my, his or big, a, the, etc. these are the words that can occur to the left of the world cat. so we say my c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " cat. so we say my cat, his cat big cat. a cat etc. similarly, we can also collect the words that occur right after the word cat. we can call this context right1. and here we see words eats, ate, is, has, etc. or more generally, we can look at the all the words in the window of text around the word cat.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " we can call this context right1. and here we see words eats, ate, is, has, etc. or more generally, we can look at the all the words in the window of text around the word cat. here let's say we can take a window of eight words around the world cat. we call this context window8 now of course, you can see",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "the all the words in the window of text around the word cat. here let's say we can take a window of eight words around the world cat. we call this context window8 now of course, you can see all the words from left or from right, and so we have a bag of words in general to represent the context. now, suc",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "l this context window8 now of course, you can see all the words from left or from right, and so we have a bag of words in general to represent the context. now, such a word based representation would actually give us interesting way to define the perspective of measuring the similarity. \"\u00a0 similarity of",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ting way to define the perspective of measuring the similarity. \"\u00a0 similarity of left1, then we'll see words that share just the words in the left context and we kind of ignore the other words that are also in the general context. so that gives us one perspective to measure the similarity. and similarly",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ty of left1, then we'll see words that share just the words in the left context and we kind of ignore the other words that are also in the general context. so that gives us one perspective to measure the similarity. and similarly, if we only use the right1 context will capture the similarity from anothe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "r words that are also in the general context. so that gives us one perspective to measure the similarity. and similarly, if we only use the right1 context will capture the similarity from another perspective. using both left1 and right1, ofcourse would allow us to capture the similarity with even more s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "another perspective. using both left1 and right1, ofcourse would allow us to capture the similarity with even more strict criteria. so in general, context may contain adjacent words like eats and my that you see here or non-adjacent words like saturday, tuesday or some other words in the context. and th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ral, context may contain adjacent words like eats and my that you see here or non-adjacent words like saturday, tuesday or some other words in the context. and this flexibility also allows us to measure the similarity similarity in some other different ways. sometimes this is useful as we might want to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "h related by their syntactical categories and semantics. so the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "compute the similarity of context of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "mple, we can measure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. and of course we can also assign weights to these dif",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "in general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. and of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "es on different contexts. and of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the sim",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. so next, let's see how we exactly compute these similarity functions. now to answer this question it's useful to think of bag of words representatio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "l to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. but here we also f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "e that vector space model has been used frequently for modeling documents and queries for search. but here we also find it convenient to model the context of a word for paradigmatically relation discovery. so the idea of this approach is to view each word in our vocabulary as defining one dimension in h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "e n words in total in the vocabulary. then we have n dimensions as illustrated here. and on the bottom you can see frequency vector representing a context. and here we see when eats occured five times in this context, ate occurred three times etc. so this vector can then be placed in this vector space m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "s as illustrated here. and on the bottom you can see frequency vector representing a context. and here we see when eats occured five times in this context, ate occurred three times etc. so this vector can then be placed in this vector space model. so in general, we can represent a pseudo document or con",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": ", ate occurred three times etc. so this vector can then be placed in this vector space model. so in general, we can represent a pseudo document or context of cat as one vector. d1. an another word dog might give us a different context, so d2. and then we can measure the similarity of these two vectors. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " space model. so in general, we can represent a pseudo document or context of cat as one vector. d1. an another word dog might give us a different context, so d2. and then we can measure the similarity of these two vectors. so by viewing context in the vector space model, we convert the problem of parad",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ctor. d1. an another word dog might give us a different context, so d2. and then we can measure the similarity of these two vectors. so by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ave been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. so let's first look at the one possible approach, where we try to measure the similarity of context based on the expe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " similarity of context documents for our purpose here. so let's first look at the one possible approach, where we try to measure the similarity of context based on the expected overlap of words and we call this eowc. so the idea here is represent a context by award vector where each word has a weight th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "h, where we try to measure the similarity of context based on the expected overlap of words and we call this eowc. so the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word. so ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ity that a randomly picked word from this document vector is this word. so in other words. xi is defined as the normalized count of word wi in the context. and this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word. now of course these xi's ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ion over words. so, the vector d2 can be also computed in the same way. and this would give us then two probability distributions representing two contexts. so that addresses the problem how to compute the vectors? next, let's see how we can define similarity in this approach. well, here we simply defin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ion actually has a nice interpretation. and there is this dot product\u00a0 infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "t\u00a0 infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? if the two contexts are very similar, then we sh",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? if the two contexts are very similar, then we should expect that we frequently will see the two wo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ry to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? if the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical. if they are very diffe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ion, are they identical? if the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical. if they are very different then the chance of seeing identical words being picked from the two contexts would be small. so this intui",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "o words picked from the two contexts are identical. if they are very different then the chance of seeing identical words being picked from the two contexts would be small. so this intuitively makes sense for measuring similarity of contexts. now you might want to also take a look at the exact formulas a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "he chance of seeing identical words being picked from the two contexts would be small. so this intuitively makes sense for measuring similarity of contexts. now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "t will pick this particular word from d1 and\u00a0 yi gives us the probability of picking this word from d2 and when we pick the same word from the two contexts then we have identical\u00a0 pick. alright, so that's one possible approach. eowc expected overlap of words in context. now, as always, we would like to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "pick the same word from the two contexts then we have identical\u00a0 pick. alright, so that's one possible approach. eowc expected overlap of words in context. now, as always, we would like to assess whether this approach it would work well. now, of course, ultimately we have to test the approach with real ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " little bit. so first, as i said, it does make sense right? because this formula will give a higher score if there is more overlap between the two contexts. so that's exactly what we want. but if you analyze the formula more carefully, then you also see there might be some potential problems. and specif",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "well over matching more distinct terms, and that is because in the dot product, if one element has a high value and this element is shared by both context and it contributes a lot to the overall sum. and it might indeed make the score higher than in another case where the two vectors actually have a lot",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ourse, this might be desirable in some other cases, but in our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context. if you only rely on one term and that's a little bit qu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "fer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context. if you only rely on one term and that's a little bit questionable. it may not be robust. the second problem is that it treats every word equally, so",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "in this lecture, we continue discussing paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by u",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ng paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the dot product. which can be interpre",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ed expected overlap of words in context. in this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the dot product. which can be interpreted as the probability that to randomly pick the words from the two cont",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "t and we measure the similarity by using the dot product. which can be interpreted as the probability that to randomly pick the words from the two contexts\u00a0 are identical, we also discuss the two problems of this method. the first is that it favors matching one frequent term very well over matching more",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " 'eats'. so now we are going to talk about how to solve these problems. most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector wit",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ntroduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. so to address the first problem, we can use a sub linear transformation of term frequenc",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " problem, we can use a sub linear transformation of term frequency. that is, we don't have to use the raw frequency count of term to represent the context. we can transform it into some form that wouldn't emphasize so much on the raw frequency. to address the second problem, we can put more weight on ra",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " second problem, we can put more weight on rare terms. that is, we can reward matching a rare word and this heuristic is called idf term weighting in text retrieval. idf stands for inverse document frequency. so now we're going to talk about the two heuristics in more detail. first, let's talk about the",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "now this is naive because it ignored the frequency of words. however, this actually has the advantage of emphasizing matching all the words in the context, so it does not allow a frequent word to dominate the matching. now the approach that we have taken earlier in the expected overlap account approach ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " value, it would behave more like the linear transformation. so this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. so we just talk about how to solve the problem of over emphasizing a frequently frequent t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "gher value for a lower k, meaning that it rewards a rare term. and the maximum value is log of m + 1. that's when the word occurs just once in the context. so that's a very rare term, the rarest term in the whole collection. the lowest value you can see here is when k reaches its maximum, which would be",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "this. this of course measure is used in search where we naturally have a collection. in our case, what will be our collection? we can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "t is to say, a word that's popular in the collection in general would also have a low idf. because depending on the data set, we can construct the context vectors in different ways, but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ctors in different ways, but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected context documents. so how can we add these heuristics to improve our..... our similarity function. here's one way, and there are many other ways that are pos",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "on where we just have a normalized count. on this one, right? so we only have this one and the document length or the total count of words in that context document. and that's what we had before. but now with the bm 25 transformation, we introduced something else. first, of course, this extra occurrence",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "re. and this is the computed by taking the average of the lengths of all the documents in the collection. in this case, all the lengths of all the context documents that we are considering. so this average documents will be a constant for any given collection, so it actually is only affecting the effect",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "s we'll introduce weight for matching each term. so you may recall this sum indicates all the possible words that can be a overlap between the two contexts. and the xi and yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word. now idf wo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "sum indicates all the possible words that can be a overlap between the two contexts. and the xi and yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word. now idf would give us the importance of matching this word. a common word will be ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "y we can also use this approach to discover syntagmatic relations. in general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "gn weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context. so let's take a look at the term vector in more detail here. and we have each xi, defined as a normalized weight of bm 25. now this weight alone onl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "tail here. and we have each xi, defined as a normalized weight of bm 25. now this weight alone only reflects how frequently the word occurs in the context. but we can't just say any frequent term in the context that would be correlated with the candidate word. because many common words like 'the' will o",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "weight of bm 25. now this weight alone only reflects how frequently the word occurs in the context. but we can't just say any frequent term in the context that would be correlated with the candidate word. because many common words like 'the' will occur frequently in all the context. but if we apply idf ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "equent term in the context that would be correlated with the candidate word. because many common words like 'the' will occur frequently in all the context. but if we apply idf weighting as you see here, we can then we weight these terms based on idf that means the words that are common, like 'the' will ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ghest weighted terms will not be those common terms because they have lower idfs. instead, those terms would be the terms that are frequent in the context, but not frequently in the collection. so those are clearly the words that tend to occur in the context of the candidate word, for example, cat. so f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ould be the terms that are frequent in the context, but not frequently in the collection. so those are clearly the words that tend to occur in the context of the candidate word, for example, cat. so for this reason, the highly weighted terms in this idf weighted vector can also be assumed to be candidat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "overed in a joint manner by leveraging such associations. so to summarize, the main idea for discovering\u00a0 paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. and then compute the similarity of the correspond",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "andidate word to form a pseudo document, and this is typically represented as a bag of words. and then compute the similarity of the corresponding context documents of two candidate words. an then we can take the highly similar word pairs and treat them as having paradigmatic relations. these are the wo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "e words. an then we can take the highly similar word pairs and treat them as having paradigmatic relations. these are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked abo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "re are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "ations. by definition, syntagmatic relations hold between words that have correlated co occurrences. that means when we see one word occurs in the context, we tend to see the occurrence of the other word. so take a more specific example, here we can ask the question whenever eats occurs, but other words",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "ight, so this would force us to think about what other words are associated with eats. if they are associated with eats, they tend to occur in the context of eats. so more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then i asked",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "d with eats. if they are associated with eats, they tend to occur in the context of eats. so more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then i asked the question is a particular word present or absent in this segment. righ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "e how easy it is to predict the presence or absence of a word. now we address the different scenario where we assume that we know something about the text segment. so now the question is, suppose we know eats occured in the segment, how would that help us predict the presence or absence of a word like m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ange all these probabilities to conditional probabilities where we look at the presence or absence of meat. given that we know eats occured in the context. so as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the condit",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " a zero probability because our data maybe a small sample and in general we would believe that it's potentially possible for award to occur in any context. so to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we ",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "tion discovery and paradigmatically relation discovery. so we already discussed the possibility of using bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "e medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ord like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity. so this provides yet another way to do term waiting for paradigmatic. a relation discovery an. so to summarize, this whole part about wor",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "lable for discovering both kinds of relations, and they can be combined to perform. join the analysis as well. these approaches can be applied to any text with no helmet human effort. and mostly becausw. they are based on counting of words. yet they can actually discover interesting relations of words. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ausw. they are based on counting of words. yet they can actually discover interesting relations of words. we can also use different ways to define context and segment and this would lead to some interesting variations of applications. for example, the context can be very narrow, like a few words around ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " we can also use different ways to define context and segment and this would lead to some interesting variations of applications. for example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover differe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "s of applications. for example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual informatio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. these discovery associations can support them. any other applic",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "l information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. these discovery associations can support them. any other applications in both informa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ould give us different kinds of associations. these discovery associations can support them. any other applications in both information retrieval and text data mining. so here are some recommended readings. if you want to know more about the topic, the 1st is a book with a chapter on locations which is ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": ", we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. the main topics. in the text. and we call that topic mining and analysis. in this lecture we're going to talk about its motivation and the task definition. so first, let's look a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "mething that we all understand, i think, but it's actually not that easy to formally define it. roughly speaking, topic is the main idea discussed in text data, and you can think of this as a theme or subject of discussion or conversation. it can also have different granularities. for example, we can ta",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": ". so different granularities of topics obviously have different applications. indeed, there are many applications that require discovery of topics in text and then analyze them. here are some examples. for example, we might be interested in knowing what are twitter users talking about today? are they ta",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "erhaps we're interested in knowing what are the major topics debated in 2012 presidential election? and all these have to do is discovering topics in texts and analyzing them, and we're going to talk about a lot of techniques for doing this. in general, we can view topic as some knowledge about the worl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "zing them, and we're going to talk about a lot of techniques for doing this. in general, we can view topic as some knowledge about the world. so from text that we expected to discover a number of topics and then this topic generally provide the description about the world and it tells us something about",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "rovide the description about the world and it tells us something about the world, about the product, about the person, etc. now when we have some non-text data then we can have more context for analyzing the topics. for example, we might know the time associated with the text data or locations where the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "rld and it tells us something about the world, about the product, about the person, etc. now when we have some non-text data then we can have more context for analyzing the topics. for example, we might know the time associated with the text data or locations where the text data will be produced or the ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": ". now when we have some non-text data then we can have more context for analyzing the topics. for example, we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. all such meta data or context variables",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "data then we can have more context for analyzing the topics. for example, we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. all such meta data or context variables can be associated with the topic",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "ing the topics. for example, we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. all such meta data or context variables can be associated with the topics that we discover. and then we can use these",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": ", we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. all such meta data or context variables can be associated with the topics that we discover. and then we can use these context variables to help ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": " with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. all such meta data or context variables can be associated with the topics that we discover. and then we can use these context variables to help us analyze patterns of topics. for ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "the sources of the text etc. all such meta data or context variables can be associated with the topics that we discover. and then we can use these context variables to help us analyze patterns of topics. for example, looking at topics overtime, we would be able to discover whether there's a trending top",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "e extent. etc., right? so now you can see there are generally two different tasks or subtasks. the first is to discover k topics from a collection of text data. what are these k topics? ok, major topics in the text data. the second task is to figure out which documents cover which topics to what extent.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "y two different tasks or subtasks. the first is to discover k topics from a collection of text data. what are these k topics? ok, major topics in the text data. the second task is to figure out which documents cover which topics to what extent. so more formally, we can define the problem as follows. fir",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": " which documents cover which topics to what extent. so more formally, we can define the problem as follows. first, we have as input a collection of n text documents. here we can denote that text connection. as c. and denote a text article as di and we generally also need to have as input the number of t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "hat extent. so more formally, we can define the problem as follows. first, we have as input a collection of n text documents. here we can denote that text connection. as c. and denote a text article as di and we generally also need to have as input the number of topics k. but there may be techniques tha",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": " define the problem as follows. first, we have as input a collection of n text documents. here we can denote that text connection. as c. and denote a text article as di and we generally also need to have as input the number of topics k. but there may be techniques that can automatically suggest a number",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "rent ways of doing that and. we're going to talk about a natural way of doing that, which is also likely effective. so first we're going to parse the text data in the collection to obtain candidate terms. here, candidate terms can be words or phrases. let's say the simplest solution is to just take each",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ing that the function is defined mostly based on statistics. so the scoring function would be very general. it can be applied to any language and any text. but when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. for example, in news ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ome empirical evaluation by using actual datasets and to see how well it works. in this case, let's take a look at a simple example. here we have the text document that is about the nba basketball game. so in terms of the content, it's about the sports. but if we simply count these words that represent ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "rds together to model topic. third, because we have probabilities for the same word in different topics. we can disambiguate the sense of word in the text to decode its underlying topic, so we address all these three problems with this new way of representing a topic. so now, of course, our problem defi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "t this problem as a computation problem now. so we clearly specify the input and output as illustrated here on this side. the input, of course is our text data c is the collection, but we also generally assume we know the number of topics k or we hypothesize a number and then try to mine k topics, even ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ibution. and we also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before. so given a set of text data, we would like to compute all these distributions and all these coverages as you have seen on this slide. now of course, there may be many diffe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the picture that you have seen before in order to show the generation process. so the idea of this approach is actual",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "of the observed data. so this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a genera",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit the data",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "s will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. by varying the model, of course we can discover different knowledge. so to summarize, we introduced a new way of representing a topic, namely r",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "iations of semantics. we talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. the number of topics and vocabulary set and the output is a set of topics. each is word distribution. and also the coverage of all the topi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "vering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they are generated this way and inside the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "infer the most likely parameter values lambda star given a particular data set, and we can then take the lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text. as you will see la",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text. as you will see later in the other lectures.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "imilarly another sentence, \"\"the\" \"eigenvalue is positive\"\", might get a\" probability of 0.00001 so as you can see, such a distribution clearly is context dependent. it depends on the context of discussion. some word sequences might have higher probabilities than others. but the same sequence of words m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "genvalue is positive\"\", might get a\" probability of 0.00001 so as you can see, such a distribution clearly is context dependent. it depends on the context of discussion. some word sequences might have higher probabilities than others. but the same sequence of words might have a different probability in ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " some word sequences might have higher probabilities than others. but the same sequence of words might have a different probability in a different context. and so this suggests that such a distribution can actually characterize topic. such a model can also be regarded as a probabilistic mechanism for ge",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this suggests that such a distribution can actually characterize topic. such a model can also be regarded as a probabilistic mechanism for generating text. and that just means we can view text data as data observed from such a model. for this reason, we also call such a model generative model. so now gi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " can actually characterize topic. such a model can also be regarded as a probabilistic mechanism for generating text. and that just means we can view text data as data observed from such a model. for this reason, we also call such a model generative model. so now given a model, we can then sample sequen",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but after we make this assumption,",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ds in our vocabulary. so here we assume we have n words, so we have n probabilities, one for each word, and they sum to one. so now we can assume our text is a sample drawn according to this word distribution. that just means we're gonna draw a word each time and then eventually we'll get a text. so for",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ume our text is a sample drawn according to this word distribution. that just means we're gonna draw a word each time and then eventually we'll get a text. so for example now again. we can try to sample words according to a distribution. we might get wednesday often or today often and some other words l",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words tha",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "butions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we as",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "tion, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see te",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks l",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "xt mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "n this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of gene",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "bout what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "aper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. assuming that no word has a zero probability in the distribution and that just means we ca",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "conference is non zero. assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents. the second distribution show on the bottom has different words that with higher probability. foo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ord has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents. the second distribution show on the bottom has different words that with higher probability. food, nutrition and healthy, diet etc. so thi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ates a different topic and in this case it's probably about health. so if we sample words from such distribution, then the probability of observing a text mining paper would be very very small. on the other hand, the probability of observing a text that looks like a food nutrition paper would be high, r",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " such distribution, then the probability of observing a text mining paper would be very very small. on the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher. so that just means given a particular distribution, different text will have",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "serving a text that looks like a food nutrition paper would be high, relatively higher. so that just means given a particular distribution, different text will have different probabilities. now let's look at the estimation problem. now, in this case, we're going to assume that we have observed data. we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "robabilities. now let's look at the estimation problem. now, in this case, we're going to assume that we have observed data. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": ", in this case, we're going to assume that we have observed data. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we as",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "d i've shown some counts of individual words here. if we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the e",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ndividual words here. if we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated probabilities of the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "he problem now is just the estimated probabilities of these words as i've shown here. so what do you think? what would be your guess? would you guess text that has a very very small probability or relatively large probability? what about the query? your guess probably will be dependent on how many times",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "lity or relatively large probability? what about the query? your guess probably will be dependent on how many times we have observed this word in the text data, right? and if you think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " we have observed this word in the text data, right? and if you think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100. because i've observed text 10 times in the text that has a total of 100 words. and similarly, mining has five out",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100. because i've observed text 10 times in the text that has a total of 100 words. and similarly, mining has five out of 100. and query as a relatively small probability, just obse",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100. because i've observed text 10 times in the text that has a total of 100 words. and similarly, mining has five out of 100. and query as a relatively small probability, just observd once. so it's one",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "d give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "e are very few data points. the sample is small, then if we trust data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that wor",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probabilit",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "t this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. so one way to address this problem is actually to use bayesian estimation, where we actually would look at both the data and all our prior kn",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "t to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we int",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function whi",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "meters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge that we discover from the text. so let's look at these steps for this very simple case. later, we'll look at this procedure for some more complicated cases. so our data in this cas",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the high probability words tend to be those very comm",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e very common words, often functional words in english, and this will be followed by some content words that really characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be external",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " would mean we have to do something different here. in particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution. so one natural way to solve the problem is to think abou",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o think about using another distribution to account for just these common words. this way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. this way our target is the topic theta here wo",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " have to decide which distribution to use when we generate the word, but each word will still be sampled from one of the two distributions, right? so text data is still generating the same way. namely, we're going to generate a one word at each time. an eventually we generated a lot of words. when we ge",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "se we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating text data and such a model is called a mixture model. so now let's see. in this case, what's the probability of observing the word w? \"now here i showed s",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "d a mixture model. so now let's see. in this case, what's the probability of observing the word w? \"now here i showed some words like \"\"the\"\"\" \"and \"\"text\"\", so as in all cases, once we\" set up the model, we're interested in computing the likelihood function. the basic question is, so what's the probabi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " distribution and similarly the second part accounts for a different way of generating the word from the background. now obviously the probability of text the same is all similar, right? so we also consider two ways of generating text, and each case is a product of the probability of choosing a particul",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "rating the word from the background. now obviously the probability of text the same is all similar, right? so we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " to make sure that you have really understood this expression here. and you should convince yourself that this is indeed the probability of observing text. so to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e, the next question is how can we estimate the parameter or what to do with the parameters given the data? well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "l we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover? well, these are represented by our parameters, and we have two kinds of parameters. one is the two word distri",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "tivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "y the same, so we're going to flip fair coin to decide which model to use. furthermore, we are going to assume there are precisely two words: the and text. obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. so we f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "use. furthermore, we are going to assume there are precisely two words: the and text. obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. so we further assume that the background model gives probability of point nin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ul to examine the behavior in such a special case. so we further assume that the background model gives probability of point nine to the word the and text point one. now, let's also assume that our data is extremely simple. the document has just the two words text and the. so now let's write down the li",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ility of point nine to the word the and text point one. now, let's also assume that our data is extremely simple. the document has just the two words text and the. so now let's write down the likelihood function in such a case. first, what's the probability of text and what's the probability of the? i h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "e. the document has just the two words text and the. so now let's write down the likelihood function in such a case. first, what's the probability of text and what's the probability of the? i hope by this point and you will be able to write it down. so the probability of text is basically the sum over 2",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": ", what's the probability of text and what's the probability of the? i hope by this point and you will be able to write it down. so the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. and it accounts for the two ways of generating text.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. and it accounts for the two ways of generating text. an inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model. simil",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "two ways of generating text. an inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model. similarly, the would have a probability of the same form, just with different exact probabilities. so naturally our likelihood funct",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "we then optimize this likelihood? well, you will notice that there were only two variables. they are precisely the two probabilities of the two words text and the given by theta sub d. and this is because we have assumed all the other parameters are known. so now the question is a very simple algebra qu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "re no constraint, of course we would set both probabilities to their maximum value, which would be one to maximize this. but we can't do that because text and the must sum to one. we can't give both a probability of 1. so now the question is how should we allocate the probability mass between the two wo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "d we have a constraint. on the two probabilities. now. if you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger than the. and this intuition can be well supported by a mathematical fact, which is when the sum of two variables is a constant",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "es equal. and when we make them equal an, if we consider the constraint that we can easy to solve this problem and the solution is the probability of text would be point nine and probability of the is point one. and as you can see, indeed the probability of text is now much larger than probability of th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "em and the solution is the probability of text would be point nine and probability of the is point one. and as you can see, indeed the probability of text is now much larger than probability of the. this is not the case when we have just one distribution and this is clearly because of the use of the bac",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "t one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. and if you look at the equation, you will see obviously some interaction of the two distributions here. in particular, you will see in order to make",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ord that has a smaller probability given by the background. and, this is obvious from examining this equation because the background part is weak for text it's small. so in order to compensate for that we must make the probability of text given by theta sub d somewhat larger so that the two sides can be",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "m examining this equation because the background part is weak for text it's small. so in order to compensate for that we must make the probability of text given by theta sub d somewhat larger so that the two sides can be balanced. so this is in fact a very general behavior of this mixture model, and tha",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ontent words that cannot be explained well by the background model. meaning that they have a very small probability from the background model, like a text here.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ncies. ok, so what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1. now it's interesting to think about a scenario where we start adding more words to the document. so ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "enced by the than text before each contributed one turn. so now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the. why? because the is repeated many times if we increase it a little bit, it will have more positive",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "lity for the. why? because the is repeated many times if we increase it a little bit, it will have more positive impact, whereas a slight decrease of text. we have relatively small impact because it occurs just once. right, so this means there is another behavior that we observe here that is high freque",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " this lecture were going to look into how to compute this maximum likelihood\u00a0 estimator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "o that would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "tentative probabilities for these words in theta sub d. so now all the parameters are known for this mixture model. and now let's consider word like\u00a0 text. so the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub b? so in other words, we wan",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " in theta sub d. so now all the parameters are known for this mixture model. and now let's consider word like\u00a0 text. so the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub b? so in other words, we want to infer which distribution has been ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "een having been generated from theta sub d or from theta sub b? so in other words, we want to infer which distribution has been used to generate this text. now, this inference process is a typical bayesian inference situation where we have some prior about these two distributions so can you see what is ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "n inference, we typically then would update our belief after we have observed evidence. so what is evidence here? while the evidence here is the word text. now that we are interested in the word text, so text can be regarded as evidence. and in the if we use bayes rule to combine the prior and the data ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ur belief after we have observed evidence. so what is evidence here? while the evidence here is the word text. now that we are interested in the word text, so text can be regarded as evidence. and in the if we use bayes rule to combine the prior and the data likelihood, what we will end up with is to co",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " after we have observed evidence. so what is evidence here? while the evidence here is the word text. now that we are interested in the word text, so text can be regarded as evidence. and in the if we use bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ta likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basically the probability of the word text from each distribution and we see that in both cases text is possible that even in the background it is still possible. it just has a very small prob",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "prior with the likelihood that you see here, which is basically the probability of the word text from each distribution and we see that in both cases text is possible that even in the background it is still possible. it just has a very small probability. so intuitively, what would be your guess? so in t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ible. it just has a very small probability. so intuitively, what would be your guess? so in this case now, if you're like many others, you will guess text is probably from theta sub d is more likely from\u00a0 theta sub d, why? and you will probably see that it's because. text has a much higher probability h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ike many others, you will guess text is probably from theta sub d is more likely from\u00a0 theta sub d, why? and you will probably see that it's because. text has a much higher probability here. by the theta sub d, than by the background model, which has a very small probability. and by this we are going to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "a much higher probability here. by the theta sub d, than by the background model, which has a very small probability. and by this we are going to say text is more likely from theta sub d. so you see our guess of which distribution has been used to generate the text would depend on how high the probabili",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ility. and by this we are going to say text is more likely from theta sub d. so you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution. we are going do tend to guess the distribution that gives",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "m theta sub d. so you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution. we are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "kground model is almost 100%. now if we have that kind of strong prior, then that would affect your guess. you might think well, wait a moment, maybe text could have been from the background as well, although the probability is very small here. the prior is very high. so in the end we have to combine th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " us a solid and principled way of making these kind of guess to quantify that. so more specifically, let's think about the probability that this word text has been generated. in fact from theta sub d, the in order for texture to be generated from theta sub d two things must happen first. the theta sub d",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "tify that. so more specifically, let's think about the probability that this word text has been generated. in fact from theta sub d, the in order for texture to be generated from theta sub d two things must happen first. the theta sub d must have been selected, so we have the selection probability here,",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "appen first. the theta sub d must have been selected, so we have the selection probability here, and secondly, we also have to actually have observed text from the distribution. so when we multiply the two together, we get the probability that text has in \"fact has been generated from theta sub d\u00a0 simil",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e, and secondly, we also have to actually have observed text from the distribution. so when we multiply the two together, we get the probability that text has in \"fact has been generated from theta sub d\u00a0 similarly, for the background model an. the probability of generating text is another product of si",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "get the probability that text has in \"fact has been generated from theta sub d\u00a0 similarly, for the background model an. the probability of generating text is another product of similar form. we also introduced a latent variable z here to denote whether the word is from the background or the topic. when ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "z is zero, it means it's from the topic theta sub d when it's one, it means it's from the background theta sub b. so now we have the probability that text is generated from each. then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or fro",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " have the probability that text is generated from each. then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub b. and equivalently, the probability that z is equal to 0 given that the observed evidence is text. so this is ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "hat the word text is from theta sub d or from theta sub b. and equivalently, the probability that z is equal to 0 given that the observed evidence is text. so this is application of bayes rule. but this step is very crucial for understanding the em algorithm. because if we can do this, then we would be ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ta on sunday or theater super b. and here we show some possible values of these variables. for example for the it's from background, z value is 1 and text on the other hand is from the topic. then it's 0 for z etc. now, of course we don't observe those z values. we just imagine there are such a social v",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " or to revise our estimate of the parameters. so let me also illustrate we can group the words that are believed to have come from cedar sub d and as text mining algorithm for example and clustering. and we had group them together. to help us re estimate the parameters. that were interested in so these ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "probability is different. not only that, we also see some words that are believed to have come from the topic. we have high probability like this one text. and of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values. so we hav",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "s the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. here i show a ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " city and donation, etc. we also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. so segment of the topics to figure out which words are from which distribution and to figure out the first one of these topics. so how do we know th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " as summarization or segmentation of the topics, clustering of sentences, etc. so the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocab",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "tually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocabulary set. and of course, the text data right? and then the output is of two kinds. one is the topic category characterization seedies hci is a water distribution and 2nd it's the topi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e only difference is that we're going to have more than two topics. otherwise it's essentially the same. so here i illustrate how we can generate the text that i was multiple topics. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "besides the background topical, but now we have more topics. specifically we have k topics. now all these are topics that we assume that exist in the text data, so the consequences that our switch for choosing a topic now is multiway switch before it's just a two way switch. going to think of as flippin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " all the important parameters. so first we see lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ue that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water distribution. now next in the rest of this formula. excuse me, you see two interest",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use plsa to analyze text data, and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parame",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. one is awarded distributio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ckground language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of e",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "evelopment of latent dirichlet allocation or lda. so first let's talk about the plsa with prior knowledge. in practice, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by us",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " the training data looks like, and then it doesn't allow us to generalize the model for using other data. this, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. we are not always interested in modeling future data",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "of plsa and the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "lar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to take tax data as input, and we're going to output the key topics. each topic is characterize",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "this lecture is the first one about the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic q",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "t technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to ev",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is te",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you migh",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "xt clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have learned in some other courses. the idea is to discover natural st",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ea is to discover natural structures in the data. in other words, we want to group similar objects together. in our case, these objects are of course texture objects. for example, they can be documents, turns, passages, sentences or websites. and then our goal is to group similar texture objects togethe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ects are of course texture objects. for example, they can be documents, turns, passages, sentences or websites. and then our goal is to group similar texture objects together. so let's see a example here. you don't really see text objects, but i just use some shapes to denote objects that can be grouped",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s, passages, sentences or websites. and then our goal is to group similar texture objects together. so let's see a example here. you don't really see text objects, but i just use some shapes to denote objects that can be grouped together. now, if i ask you what are some natural structures or natural gro",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster documents in the whole text collection. so in this case documents are the units to be cluster",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster documents in the whole text collection. so in this case documents are the units to be clustered. we may be able to cluster terms in this case. terms are objects. and cluster of ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "e you cluster of terms in some sense. if you take the terms with high probabilities from world distribution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text object",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "t might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we ca",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ee a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "tually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cluster websites. each website is actually com",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "hieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cluster websites. each website is actually composed of multiple documents. similarly,",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "y author as one unit for clustering. in this way, we might group authors together based on whether they are published papers or similar. furthermore, text clusters can also be further clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "pers or similar. furthermore, text clusters can also be further clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, p",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ther clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical sce",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "uster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say all the email message",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "els. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say all the email messages from customers in some time period, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say all the email messages from customers in some time period, or all the literature, articles, etc. and then you hope to get the sense a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " what are some typical or representative document in the collection? and clustering help us achieve this goal. we sometimes also want to link similar text objects together and these. these objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, r",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ated documents. sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic. we may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. we",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " topic and by linking them together we can have more complete coverage of the topic. we may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. we may also use text clustering to induce additiona",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "g to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. we may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then w",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "can create a hierarchy of structures and this is very useful for browsing. we may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "e one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later. so there are in general many applications of text clustering any. i just saw it with two very specific on",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "rovide additional discrimination that might be used for texture classification as we will discuss later. so there are in general many applications of text clustering any. i just saw it with two very specific ones. one is to cluster search results for example and you can imagine a search engine can clust",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to d",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "lustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "c models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on thi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two k",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ious lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "abilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "se the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have input of text collection c and number of topics k and vocabulary v, and we hope to generate as output two things. one is a set of topics denoted by theta i's. each",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than on",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ent is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. in text clustering, however, we only allow a document to cover one topic. if we assume one topic is a cluster. so. that means if we change the topic definiti",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "clustering problem. so because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the g",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e for each word is independent, so \"that means, for example, \"\"the\"\" here could\" have been generated from the second distribution. theta two, whereas text is more likely generated from the first one on the top. that means the words in the document could have been generated in general from multiple distr",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e top. that means the words in the document could have been generated in general from multiple distributions. now this is not what we want to see for text clustering. for document clustering where we hope this document will be generated from precisely one topic. so now that means we need to modify the m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "n of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "enerative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lect",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "g each cluster as .5. so equally likely. and then let's consider one document that you have seen here. there are two words, sorry, two occurrences of text and two occurrences of mining. so there are four words together. medical and health did not occur in this document, so this first thing about the hid",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "must have been selected, so it's given by p of 01 second. it must have also been generating the four words in the document, namely two occurrences of text and two occurrences of mining. that's why you see the numerator has the product of the probability of selecting theta one and the probability of gene",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ds in those documents and normalize them. so this is basically what i just said. most specifically, we're going to for example. use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account. instead, we can do ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "general by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this lecture is about the similarity based approaches to text for clustering. in this lecture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a differe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this lecture is about the similarity based approaches to text for clustering. in this lecture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function lik",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " similarity based on the individual object similarity. so let's illustrate how can induce a structure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "tter. now let's look at another example of method for similarity based classroom in this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "approaches can and generate the term clusters and document clusters. an term clusters can be in general generated by representing each term with some text content. for example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. and then we c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "usters and document clusters. an term clusters can be in general generated by representing each term with some text content. for example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. and then we can certainly cluster terms based on act",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "this lecture is about evaluation of text cluster. so far we have talked about multiple ways of doing text clustering but how do we know which method works the best? so this has to do with ev",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "this lecture is about evaluation of text cluster. so far we have talked about multiple ways of doing text clustering but how do we know which method works the best? so this has to do with evaluation. now to talk about evaluation, one must go to go back to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " shapes or cluster based on sizes. and that's precisely why the perspective or clustering bias is crucial for evaluation. in general, we can evaluate text clusters in two ways. one is direct evaluation and the other is indirect evaluation. so in directl valuation, we want to answer the following questio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " or desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. that is, we're going to ask humans to partition the objects to create the gold st",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "e i just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications. the 2nd way to evaluate text clusters is to do indirect evaluation. so in this case the question to answer is how useful are the clustering results for the intended applications?",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "on as well. so what counts as the best clustering result would be dependent on the application. procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. in this case what we care about is the contribution of clustering to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ther its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "mmarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is often the first step when you deal with a lot of text data. the second application or",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "supervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is often the first step when you deal with a lot of text data. the second application or second kind of application is to discover int",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ining an overall picture of the text content. this is often needed to explore text data. and this is often the first step when you deal with a lot of text data. the second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " step when you deal with a lot of text data. the second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful. there are many approaches that can be used for text clustering and we discussed them: model based ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " discover interesting clustering structures in text data, and these structures can be very meaningful. there are many approaches that can be used for text clustering and we discussed them: model based approaches and similarity based approaches. in general, strong clusters tend to show up no matter what ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and ana",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of k",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analy",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "very of various different kinds of knowledge as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledg",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "the articles that they have written. we can in general categorize the observer based on the content. that they produce. finally, it's also related to text based prediction. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "eneral categorize the observer based on the content. that they produce. finally, it's also related to text based prediction. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important te",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "sed prediction. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk abou",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ion techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested in doing that in this lecture. and then we're going to talk about how to do text categorisation followed by ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "going to talk about what is text categorization and why we are interested in doing that in this lecture. and then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. the problem of texture categorisation is defined as follows. we're given a ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s lecture. and then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. the problem of texture categorisation is defined as follows. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of trainin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ws. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categor",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "en the task is to classify any tax object into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these d",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ect into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ld further help the system then learn how to recognize. the categories of new tax objects that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clustering the units to be anal",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can vary a lot, so this creates a lot of possibilities. secondly, categories can also vary, an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "an also vary, and we can generally distinguish two kinds of categories. one is internal categories. these are categories that characterize content of text object. for example, topic categories. or sentiment categories and they generally have to do with the content of the tax objects, direct characteriza",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "f the tax objects, direct characterization of the content. the other kind is external categories that can characterize the entity associated with the text object. for example, authors or entities associated with the content that they produce. and so we can use their content, determine which author has w",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "hich author has written which part, for example, and that's called author attribution. or we can have any other meaningful categories associated with text data, as long as. there is a. there are, there's a meaningful connection between the entity and text data. for example, we might collect a lot of rev",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " any other meaningful categories associated with text data, as long as. there is a. there are, there's a meaningful connection between the entity and text data. for example, we might collect a lot of reviews about a restaurant. or a lot of reviews about the product. and then these text data can help us ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "en the entity and text data. for example, we might collect a lot of reviews about a restaurant. or a lot of reviews about the product. and then these text data can help us infer properties of product or a restaurant. in that case, we can treat this as a categorization problem. we can categorize restaura",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e we can categorize content into positive or negative or positive and negative or neutral. so you can have the same sentiment categories assigned. to text content. another application is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category. there is also another important kind of applications of routing emails to the right person to handle",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "erent kinds of requests and in many cases a person will manually assign the messages to the right people. but you can imagine you can build automatic text categorization system to help routing a request. and this is to classify the incoming request in to one of the categories where each category actuall",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "o a person to handle the request. and finally, author attribution. as i just mentioned, is yet another application, and it's another example of using text to actually infer properties of some other entities. and there are also many variants of the problem formulation and so first we have the simplest ca",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e objects in tune. it's a small number of high level categories an inside each category. we can further categorize into sub categories etc. so why is text categories important well, i already showed you several applications, but in general there are several reasons. one is text categorization helps us e",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "categories etc. so why is text categories important well, i already showed you several applications, but in general there are several reasons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ories important well, i already showed you several applications, but in general there are several reasons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with categorisation, text can be",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "but in general there are several reasons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with categorisation, text can be represented in multiple levels, meaning keyword bag of words rep",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "asons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks. but we can also ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "lysis. so now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks. but we can also add categories and they provide 2 levels of representation. semantic categories assigned can also be directly or in",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ctly useful. and. another example is when semantic categories can facilitate aggregation of tax content, and this is another case of. applications of text categorisation. for example, we if we want to know the overall opinions about the product, we could first categorize the opinions in each individual ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " and 30% are negative, etc. so without doing categorization it will be much harder to aggregate such opinions. so it provides a concise way of coding text in some sense based on our vocabulary. and sometimes you miss seeing some applications, text or categorization is called a text coding encoding with ",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "te such opinions. so it provides a concise way of coding text in some sense based on our vocabulary. and sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "concise way of coding text in some sense based on our vocabulary. and sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer properties of entities. and text c",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "eing some applications, text or categorization is called a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with t",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "t categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text da",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "text data. so this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. so it's useful to think about the information network that will ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "se text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. so it's useful to think about the information network that will connect the other entities with t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "t data to help categorize the corresponding entities. so it's useful to think about the information network that will connect the other entities with text data. the obvious entities that can be directly connected are authors, but you can also imagine the authors affiliations or the authors ages and othe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e directly connected are authors, but you can also imagine the authors affiliations or the authors ages and other things can be actually connected to text data indirectly. once we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "o text data indirectly. once we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are ofte",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "nce we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using tex",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. often",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ext categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. often together with non text data specifically to text. for exampl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "here we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. often together with non text data specifically to text. for example, we can also think of examples of inferring properties of entities. for example discovery of non native speake",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ed in using text data as extra sensor data collected from humans to infer certain desicion factors. often together with non text data specifically to text. for example, we can also think of examples of inferring properties of entities. for example discovery of non native speakers of a language and this ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "tent of. speakers another example is to predict the party affiliation of a politician based on the political speech at this is again example of using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorizati",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ple of using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorization problem.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "person to clearly decide the category based on some clear rules. secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. you can easily identify text data. for example, if there is some speci",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. you can easily identify text data. for example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "\u00a0secondly, it cannot handle uncertainties in rules. often the rules aren't 100% reliable take for example, and looking at the occurrences of words in text and try to decide the topic. it's actually very hard to have 1% correct the rule. so for example, you can say if it has games, sports, basketball, th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " 1% correct the rule. so for example, you can say if it has games, sports, basketball, then for sure it's about sports. but one can also imagine some text articles that mention these keywords. but that may not be exactly about the sports, or only marginally touching sports. the main topic could be anoth",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ve which categories. and this is called a training data. and then secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. so we need to provide some basic features for the computers to look into. and in the c",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " that can potentially provide a clue about the category. so we need to provide some basic features for the computers to look into. and in the case of text, natural choice would be the words. so using each word as a feature is a very common choice to start with. but of course there are other sophisticate",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human woul",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "r can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the ge",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to map a val",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "oblem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all the text objects. and y is all the categories a set of categories, so the classifier would take any value in x as input and we generate the value in y as outp",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "hen try to be able to compute the values for future access that we have not seen. so in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. and they will al",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic mo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're goin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. main diff",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ". main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. in fact, that's the goal of text clustering. we want to find such clusters in the data. but in the case of categorization, we are given the categories. so we kind of have predefined ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "at means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely like what we did for text clustering. namely, we are going to assign document d to the category that has the highest probability of generating this document. in other words, w",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lity they are not generated independently. once you see some word and other words will more likely occur. for example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear and if you have not seen text. but this assumption allows us to simplify the pr",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ccur. for example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear and if you have not seen text. but this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. but you should know tha",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ore likely to appear and if you have not seen text. but this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. but you should know that this kind of model doesn't have to make this assumption. we could, for example, assume the words may b",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "represents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from whi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "y of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ou have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring fu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " word given by two distributions here. now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. of course the features don't have to be all the words and their features can be other signals that we want to use. and we m",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ctor. since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very close",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": ". so this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. on",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. so the second classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability o",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "he conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interested ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ll, since this is in, this object is in category of diamonds. let's say then we're going to say, well, we're going to assign the same category to our text object. but let's also look at the another possibility of finding a larger neighborhood. so let's think about the four neighbors. in this case, we're",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this lecture is a continued discussion of. discriminative classifiers for text categorization. so in this lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popula",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ve classifier called a support vector machine or vm, which is a very popular classification method, and there has been also shown to be effective for text categorization. so to introduce this classifier, let's also think about the simple case of two categories and we have two public categories, season o",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "d with one wait for each feature we have m features and so have aim weights and are represented as a vector. an similarly the data instance. here the text object is represented by also a feature vector of the same number of elements. xi is future value. for example word count. i can you can verify when ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "roblem, will obtain the weights w&b and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects. now the previous formulation did not allow any error in the classification, but sometimes the data may not be linearly separable. that me",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "e weights and the bias, then we can have classified. that's ready for classifying new objects. so that's the basic idea of sven. so to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform si",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "earning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. the computers of course here ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. the computers of course here are trying to optimize the combinations of the features provided by human an. as i say that there are many differe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "fectively. ann is the technique has been shown to be quite effective for speech recognition, computer vision and recently it has been applied through text as well. it has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn interm",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "t they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization. although in texas domain cause words are excellent representation of text content because these are. humans invention for communication",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "is is very valuable for learning effective representation for text localization. although in texas domain cause words are excellent representation of text content because these are. humans invention for communication and they are generous sufficient for representing content for many tasks. if there's a ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "s a need for some new representation, people would have invented a new words and new world. so because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design. the words. as features",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "hniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. and then try to somehow align these categories wi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. and then try to somehow align these categories with the categories defined by the training data where we already know which",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " in the unlabeled data. you can think of this in another way. basically, we can use, let's say a naive bayes classifier to classify all the unlabeled text documents. and then we're going to assume the high confidence classification results, or actually reliable. then you certainly have more training dat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "fic application, so some errors might be more serious than others. so ideally we would like to model such differences. but if you read many papers in texture catalyzation, you will see that they don't generally do that, and instead they will use a simplified measure. and that's the cause. it's often ok ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "this lecture is continued discussion of evaluation of textual categorisation. earlier we have introduced measures that can be used to compute the precision and recall for each category and each document. now ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "ded in many applications. but the macro averaging and micro averaging, they're both very common and you might see both reported in research papers on text categorisation. also, sometimes categorisation results might actually be evaluated from ranking perspective. now this is because. categorisation resu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "ters of this book where you can find more discussion about evaluation measures. the second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "the second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "s lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the d",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. in contrast, we have other devices such as video recorder that can repo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ideo recorder that can report what's happening in the real world objectively to generate the video data, for example. now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "hat it has rich and rich opinions and the content tends to be subjective because it's generated from humans. now this is actually unique advantage of text data, as compared with other data because it offers us a great opportunity to understand the observers. we can mine the text data to understand the o",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ally unique advantage of text data, as compared with other data because it offers us a great opportunity to understand the observers. we can mine the text data to understand the opinions, understand the people's preferences, how people think about something. so this lecture and the following lectures wi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "people think about something. so this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data. so let's start with the concept of opinion that it's not that easy to formally define opinion, but mostly we would define opinion as a subjecti",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ions is expressed on this something. and now, of course, believes or thinks implies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context. people from different background may also think in different ways. so",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "lies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context. people from different background may also think in different ways. so this analysis shows that there are multiple elements that we need to include i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "times if you want to understand further, we want to enrich the opinion representation. and that means we also want to understand, for example, the context of the opinion and what situation was opinion expressed. for example, in what time was it expressed? we also would like to deeply understand opinion ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " reviewed. for example iphone 6. when the review was posted, usually you can extract such information easily. now the content of course is the review text that's in general also easy to obtain. so you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "asy to analyze in terms of obtaining a basic opinion representation. but of course, if you want to get more information, we might want to know the context. for example, the review was written in 2015. or we want to know that the sentiment of this review is positive, and so this additional understanding ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " 1938. so what's the opinion? well, this negative sentiment here that's indicated by words like a bad and worst. and we can also then identify the context. new england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ion content, of course, can also vary a lot on the surface. you can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article. and furthermore, we can identify the variation in the sentiment or emotion dimension. that's about the f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " about the feeling of the opinion holder. so we can distinguish positive versus negative or neutral or happy versus sad, etc. finally, the opinion context can also vary. we can have simple context, like different time or different locations, but there could be also complex text such as some background t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "so we can distinguish positive versus negative or neutral or happy versus sad, etc. finally, the opinion context can also vary. we can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. so when opinion expr",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": ". finally, the opinion context can also vary. we can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. so when opinion expressed in the particular discourse context, it has to be interpreted in different w",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "cations, but there could be also complex text such as some background topic being discussed. so when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discou",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "sed. so when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. from computational perspective, we're most interested in what op",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "nion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. from computational perspective, we're most interested in what opinions can be ex",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "s to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. from computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "rich to improve the entire discourse context of opinion. from computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. first, the ob",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "nterested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. first, the observer might make a comment about the opinion target in the observed world. so in this case we have t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "world. so in this case we have the author's opinion. for example, i don't like this phone at all, and that's opinion of this author. in contrast, the text might also report opinions about others so the person could also make observation about another person's opinion and report this opinion. so for exam",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion. for example, one statement might be this phone ran out of battery in just one hour. now this is in a wa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "re useful for understanding the person or understanding the product that we are commenting on. so the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, target content and context. ide",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "king text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, target content and context. ideally we can also infer opinion sentiment from the content and context to better understand the opinion. now often some elements of the representa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ach representation we should identify opinion holder, target content and context. ideally we can also infer opinion sentiment from the content and context to better understand the opinion. now often some elements of the representation are already known. i just gave a good example, in the case of product",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " consumers opinions and this is clearly very useful, directed for that. data driven social science research can benefit from this because they can do text mining to understand the people's opinions. and if we can aggregate a lot of opinions from social media from a lot of public information, then you ca",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "rded as voluntary survey, but done by those people. and in general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of predic",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "eneral, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of prediction.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ent classification as shown in this case. so suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. then we mainly need to decide the opinion sentiment of the review. so this is a case of just using sentiment classification for under",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ng sentiment classification for understanding opinion. sentiment classification can be defined more specifically as follows: the input is opinionated text object. the output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. one is polarity analysis where we have cat",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "d disgusted. so as you can see, the task is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "cation. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "hat, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated features that may be more appropriate f",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " use ordinal regression to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-gra",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ters as a unit, and they can be mixed with different n(s),\u00a0 different lengths. and this is a very general way, and a very robust way to represent the text data. you could do that for any language pretty much. and this is also robust to spelling errors or recognition errors, right? so if you misspelled t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "errors, right? so if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly. so misspelled word and the correct form can be matched because they contain some common n-grams of characters. but of course such a repres",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " we have word n-grams, a sequence of words and again we can mix them with different lengths. uni grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "omplex than words that are sometimes useful. so in general, natural language processing is very important to derive complex features. they can enrich text representation. so for example, this is a simple sentence that i showed you long time ago, and in another lecture. so from these words we can only de",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " feature validation that would revise the feature set and then you can iterate and we might consider using a different feature space. so nlp enriches text representation. as i just said and because it enriches the feature space. it allows much larger search space of features. and there are also many mea",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "eature design is generally an art. that's perhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "alysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we have an opinionated text document d as input an we want to generate as output already in the range of one through k, so it's discrete rating and thus this is a categorization",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "in the range of one through k, so it's discrete rating and thus this is a categorization problem. we have k categories here. now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories. intuitively, t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "sented as x and these are the features and there are m features altogether, which feature value is a real number, and this can be representation of a text document. and y has two values, binary response variable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "n see the general decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object. so in sum, in this approach we're going to score the object. by using the features and the parameter values, beta values. and this score will",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "bout the hotel and you see some overall ratings. in this case, both reviewers have given five stars. and of course there are also reviews that are in text. now, if you just look at these reviews, it's not very clear whether a hotel is good for its location or for its service, and it's also unclear why a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "relevant the segments and then from those segments we can further mine correlated words. with these seed words and that would allow us to segment the text into segments. discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, but anyway, th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " conditional probability of the observed rating given the document. and so we have seen such cases before in, for example, plsa, where we predict the text data. but here we predicting the rating and the parameters of course are also very different. but if you can see if we can uncover these parameters, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "evelop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a generative model like psa. and then",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ribed by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a generative model like psa. and then we can then plug in the latent regression",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ese distributions. in the same way as we assumed for a generative model like psa. and then we can then plug in the latent regression model to use the text to further predict the overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the over",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ombine them with aspect weights to predict the overall rating. so this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. so we don't have time to discuss this model in detail, as in many other cases in this part of the course wh",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "t the overall rating. so this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. so we don't have time to discuss this model in detail, as in many other cases in this part of the course where we discuss the cutting edge topics. b",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": ". now this kind of laxing is very useful becausw in general a word like long, let's say, may have different the sentiment polarities for different context. so if i say the battery life of this laptop is long, then that's positive. but if i say the rebooting time for the laptop is long, that's bad, right",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ven if you read all the reviews. even if you read all the reviews, it's very hard to infer such preferences or such emphasis. so this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. you can compare",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "and of course the model is general. it can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. finally, there is also some result on applying this model for personalized ranking or recommendation of entities. so because we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ated by this query and they tend to really have favor low price hotels. so this is yet another application of this technique. and shows that by doing text mining we can understand the users better. and once we can end users better, we can serve these users better. so to summarize our discussion of opini",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. and we also need to consider th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "rmation when fitting the model to the data. most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opin",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "this lecture is about text based prediction. \" in this lecture we're going to start talking about mining a different kind of knowledge as you, you can see\u00a0here on this slide. h",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ng to start talking about mining a different kind of knowledge as you, you can see\u00a0here on this slide. here on this slide. namely, we're going to use text data to infer values of some other variables in the real world. that may not be directly related to the text, or only remotely related to text data. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "n this slide. namely, we're going to use text data to infer values of some other variables in the real world. that may not be directly related to the text, or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "to use text data to infer values of some other variables in the real world. that may not be directly related to the text, or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus mo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " the following lectures, we're going to talk more about how we can predict more information about the world. how can we get sophisticated patterns of text together with other kinds of\u00a0data? \" it would be useful to first take a look at the big picture of prediction in data mining in general and i call th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e sensors, including human sensors to report what we have seen in the real world in the\u00a0form of data. \" and of course the data are in the form of non text data and text data. and our goal is to see if we can predict some values of important real world variables that matter to us. for example, someone's ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "luding human sensors to report what we have seen in the real world in the\u00a0form of data. \" and of course the data are in the form of non text data and text data. and our goal is to see if we can predict some values of important real world variables that matter to us. for example, someone's health conditi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "se we in general should treat all the data that we collected. in such a prediction problem set up, we are very much interested in joint mining of non text and text data. we should mine all the data together. and then through the analysis, we generally can generate the multiple predictors of this interes",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "general should treat all the data that we collected. in such a prediction problem set up, we are very much interested in joint mining of non text and text data. we should mine all the data together. and then through the analysis, we generally can generate the multiple predictors of this interesting vari",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ng variable. so this then allows us to change the world and so this basically is the general process for making a prediction based on data, including text data. now it's important to emphasize that human actually plays very important role in this process. especially because of the involvement of text da",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ng text data. now it's important to emphasize that human actually plays very important role in this process. especially because of the involvement of text data. and so human first would be involved in the mining of the data. it will control the generation of these features. and also help us understand t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ta. and so human first would be involved in the mining of the data. it will control the generation of these features. and also help us understand the text data because text data are created to be consumed by humans. humans are the best in consuming or interpreting text data. but when there are, of cours",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "irst would be involved in the mining of the data. it will control the generation of these features. and also help us understand the text data because text data are created to be consumed by humans. humans are the best in consuming or interpreting text data. but when there are, of course a lot of text da",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "s. and also help us understand the text data because text data are created to be consumed by humans. humans are the best in consuming or interpreting text data. but when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. \" sometimes machine",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "se text data are created to be consumed by humans. humans are the best in consuming or interpreting text data. but when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. \" sometimes machines can see patterns in a lot of data that humans ma",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "est in consuming or interpreting text data. but when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. \" sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in ana",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ining. \" sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text data in all applications. next human also must be involved in predictive model building and adjusting or testing. so in particular we will have a lot",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ctually very general and it's reflecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictio",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for pr",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ta. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for prediction about human behavior or human preferenc",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "le we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for prediction about human behavior or human preferences or opinions. but in general text data wi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "metimes text alone can make predictions. and this is most useful for prediction about human behavior or human preferences or opinions. but in general text data will be put together with non text data. so the interesting questions here would be first how can we design effective predictors? and how do we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " and this is most useful for prediction about human behavior or human preferences or opinions. but in general text data will be put together with non text data. so the interesting questions here would be first how can we design effective predictors? and how do we generate such effective predictors from ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " data. so the interesting questions here would be first how can we design effective predictors? and how do we generate such effective predictors from text? this question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rs from text? this question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. it has also been addressed to some extent by talking about the other knowledge that we can mine from text. so for example, topic mining can be ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "at kind of features we can design for text data. it has also been addressed to some extent by talking about the other knowledge that we can mine from text. so for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a pred",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "the patterns or topic based indicators or predictors that can be further fed into a predictive model. so topics can be intermediate representation of text. that would allow us to design high level features or predictors that are useful for prediction of some other variable. it maybe, although it's gener",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "us to design high level features or predictors that are useful for prediction of some other variable. it maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "em and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a q",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rs as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question that we have not addressed yet. so in this lecture and the following lectures we're going to addre",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "o those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question that we have not addressed yet. so in this lecture and the following lectures we're going to address this probl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " more enriched features for prediction and allows us to review a lot of interesting knowledge about the world. these patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can reall",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "d features for prediction and allows us to review a lot of interesting knowledge about the world. these patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improv",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " really help improving the accuracy of prediction. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "bles. but in order to achieve the goal, we can do some other preparations and these are sub tasks. so one sub task could be mine, mine the content of text data like topic mining. and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. and both can help prov",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "bserver so sentiment analysis or opinion analysis. and both can help provide predictors for the prediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "th can help provide predictors for the prediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. and such improvement often leads t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "or the prediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. and such improvement often leads to more effective predictors for ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. and such improvement often leads to more effective predictors for our probl",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rovement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. as we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "roblems it would enlarge the space of patterns of opinions or topics that we can mine from text. as we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "uld enlarge the space of patterns of opinions or topics that we can mine from text. as we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can pro",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rstood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "spective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non te",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "a can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ovide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this directi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight some of them i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "xt data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures. now the other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps disco",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e body of work in this direction, and we're going to highlight some of them in the next lectures. now the other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patte",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ing to highlight some of them in the next lectures. now the other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non text data. now we can use the tex",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non text data. now we can use the text data that are associated with instances where the patter",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non text data. now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instance",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ext data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non text data. now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't oc",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "scover some frequent patterns from non text data. now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur. and this gives us two sets of text data and then we can see what's the diffe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur. and this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content \" is easy to digest and that di",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "h instances where the pattern doesn't occur. and this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content \" is easy to digest and that difference might suggest some meaning for this pattern that we've found from ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "'t occur. and this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content \" is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interp",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "a is interpretable because text content \" is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns. and this technique is called pattern annotation. and, you can see this reference listed here for more de",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "rst is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "eference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can mak",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "o multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text ba",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to d",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "opics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the predic",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "e opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text min",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "g more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that'",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also i",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "phisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can includ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data such as",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": ", why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ning? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data. and they almost always available to us",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "t, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data. and they almost always available to us. indirect text context",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "text such as meta data. and also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data. and they almost always available to us. indirect text context refers to additional data related to the meta data. so, for example, from authors",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "rect context can include the meta-data such as time, location, authors, and source of the text data. and they almost always available to us. indirect text context refers to additional data related to the meta data. so, for example, from authors, we can further obtain additional context, such as social n",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "text can include the meta-data such as time, location, authors, and source of the text data. and they almost always available to us. indirect text context refers to additional data related to the meta data. so, for example, from authors, we can further obtain additional context, such as social network o",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "e to us. indirect text context refers to additional data related to the meta data. so, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age. and such information is not, in general, directly related to the text yet through the auth",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "obtain additional context, such as social network of the author or the author's age. and such information is not, in general, directly related to the text yet through the authors we can connect them. there could be also other text data from the same source as this one, so the other context data can be c",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "r's age. and such information is not, in general, directly related to the text yet through the authors we can connect them. there could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. so in general, any related data can be reg",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "elated to the text yet through the authors we can connect them. there could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. so in general, any related data can be regarded as context, so there could be remotely related to cont",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "thors we can connect them. there could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. so in general, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use o",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "he same source as this one, so the other context data can be connected with this text, as well. so in general, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "t data can be connected with this text, as well. so in general, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can alm",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "n be connected with this text, as well. so in general, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allow",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " so in general, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allows partition text data in arbitrary ways",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "eneral, any related data can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allows partition text data in arbitrary ways as we n",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ta can be regarded as context, so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allows partition text data in arbitrary ways as we need. and this is very ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " so there could be remotely related to context. context. and so what's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allows partition text data in arbitrary ways as we need. and this is very important because this allows ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "t's the use of, why is text context useful? well, context can be used to partition text data in many interesting ways. it can almost allows partition text data in arbitrary ways as we need. and this is very important because this allows us to do interesting comparative analysis. it also in general provi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " because this allows us to do interesting comparative analysis. it also in general provides meaning to the discovery topics if we gonna associate the text with context. so here's illustration of how context can be regarded as interesting ways of partitioning of text data. so here i just show some resear",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " allows us to do interesting comparative analysis. it also in general provides meaning to the discovery topics if we gonna associate the text with context. so here's illustration of how context can be regarded as interesting ways of partitioning of text data. so here i just show some research papers pub",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "e analysis. it also in general provides meaning to the discovery topics if we gonna associate the text with context. so here's illustration of how context can be regarded as interesting ways of partitioning of text data. so here i just show some research papers published in different years. on different",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "very topics if we gonna associate the text with context. so here's illustration of how context can be regarded as interesting ways of partitioning of text data. so here i just show some research papers published in different years. on different venues, different conference names here listed on the botto",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "earch papers published in different years. on different venues, different conference names here listed on the bottom, like sigir, acl, etc. now, such text data can be partitioning in many interesting ways because we have context. so the context here just includes time and the conference venues. and but ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "conference names here listed on the bottom, like sigir, acl, etc. now, such text data can be partitioning in many interesting ways because we have context. so the context here just includes time and the conference venues. and but perhaps we can include some other variables as well. but let's see how we ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " here listed on the bottom, like sigir, acl, etc. now, such text data can be partitioning in many interesting ways because we have context. so the context here just includes time and the conference venues. and but perhaps we can include some other variables as well. but let's see how we can partition da",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " can partition data in interesting ways. first, we can treat each paper as a separate unit. so in this case, a paper id and each paper has its own context, it's independent. and. but we can also treat all the papers written in 1998 as one group, and this is only possible because of the availability of t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "h kdd papers with acl papers. we can also partition the data to obtain the papers written by authors in the us, and that of course uses additional context. of the authors and this would allow us to then compare such a subset with another set of papers written by authors in other countries. or we can obt",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "d allow us to then compare such a subset with another set of papers written by authors in other countries. or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. topic. and note that these partitioning can be also intersect with each other to ge",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ect with each other to generate even more complicated partitions. and so in general, this enables discovery of knowledge associated with different context as needed. and in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. for example, comparing topics ove",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "s. and so in general, this enables discovery of knowledge associated with different context as needed. and in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. for example, comparing topics overtime, we can see trends of topics and comparing topics in diff",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "is often gives us a lot of useful knowledge. for example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some ver",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "xample, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have bee",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "aring topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ample, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text in the context of time. so time is a context in this case. is there any difference in the responses of people in different regions to the event, to a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text in the context of time. so time is a context in this case. is there any difference in the responses of people in different regions to the event, to any event? so th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ng attention recently in data mining research? now to answer this question, obviously we need to analyze text in the context of time. so time is a context in this case. is there any difference in the responses of people in different regions to the event, to any event? so this is a very broad analysis qu",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ses of people in different regions to the event, to any event? so this is a very broad analysis question, in this case, of course, location is the context. what are the common research interests of two researchers? in this case, authors can be the context. is there any difference in the research topics ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ion, in this case, of course, location is the context. what are the common research interests of two researchers? in this case, authors can be the context. is there any difference in the research topics published by authors in the usa and those outside? now, in this case, the context would include the a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "thors can be the context. is there any difference in the research topics published by authors in the usa and those outside? now, in this case, the context would include the authors and their affiliation and location. so this goes beyond just the author himself or herself. we need to look at the addition",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " in the opinions about the topic expressed on one social network and another? in this case, the social network of authors and the topic can be the context. are there topics in news data that are correlated with sudden changes in stock prices? in this case, we can use a time series such as stock prices a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " there topics in news data that are correlated with sudden changes in stock prices? in this case, we can use a time series such as stock prices as context. what issues mattered in the 2012 presidential campaign or presidential election? now in this case, time series again as context. df so, as you can s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "s stock prices as context. what issues mattered in the 2012 presidential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ial campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "aign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. ",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we'r",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce c",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mini",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. reca",
        "label": "Intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "l probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "iscussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the top",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "g contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ng to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interes",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 p",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "n extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilistic latent semantic ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilistic latent semantic analysis or cplsa the main idea is to explicitly add interest",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "cs in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilistic latent semantic analysis or cplsa the main idea is to explicitly add interesting context variables into a generated model. recall t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e're interested in. so in this approach contextual\u00a0 probabilistic latent semantic analysis or cplsa the main idea is to explicitly add interesting context variables into a generated model. recall that before when we generate the text, we generally assume we will start with some topics and then sample wo",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ic analysis or cplsa the main idea is to explicitly add interesting context variables into a generated model. recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics. but here we are going to add context variables so that th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "when we generate the text, we generally assume we will start with some topics and then sample words from some topics. but here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context. or in other words, we can do let the context in",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "rom some topics. but here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context. or in other words, we can do let the context influence both coverage and content of a topic. the consequences that this would enable us to discover ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "context variables so that the coverage of topics and also the content of topics will be tight little context. or in other words, we can do let the context influence both coverage and content of a topic. the consequences that this would enable us to discover contextualized topics make the topics more int",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "r in other words, we can do let the context influence both coverage and content of a topic. the consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in. for example, a particular time period. as extension of plsa model, cplsa mainly does the following changes. firstly it woul",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": ", a particular time period. as extension of plsa model, cplsa mainly does the following changes. firstly it would model the conditional likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generativ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " time period. as extension of plsa model, cplsa mainly does the following changes. firstly it would model the conditional likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secon",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "nly does the following changes. firstly it would model the conditional likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the depe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "s. firstly it would model the conditional likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "onal likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. one is to assume that depending on th",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ontext, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. one is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. one is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distr",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ons of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that we assume. the topic coverage also depends on the context. and that means depending on the time or location, we might cover topics",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ws us to discover different variations of the same topic in different context. the other is that we assume. the topic coverage also depends on the context. and that means depending on the time or location, we might cover topics differently. and then again this dependency would then allow us to capture t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "location, we might cover topics differently. and then again this dependency would then allow us to capture the association of topics with specific context. we can still use the em algorithm to solve the problem of parameter estimation. and in this case, the estimate premise would naturally contain conte",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "xt. we can still use the em algorithm to solve the problem of parameter estimation. and in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "lly contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " into to know more detail here. i just want to explain the high level ideas in more detail, particularly willing to explain the generation process of text data that has context associated in such a model. so as you see here, we can assume there are still multiple topics. for example, some topics might r",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ail here. i just want to explain the high level ideas in more detail, particularly willing to explain the generation process of text data that has context associated in such a model. so as you see here, we can assume there are still multiple topics. for example, some topics might represent the themes li",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "cs. for example, some topics might represent the themes like a government response donation or the city of new orleans. now this example is in the context of hurricane katrina and that hit new orleans. now, as you can see, we assume there are different views associated with the each of the topics. and t",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " and these are shown as view one, view two and view three each view is a different version of word distributions. and these views are tide to some context variables. for example, type to the location texas or the time july 2005 or the occupation of the other being sociologist. now on the right side you ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " the right side you see now we assume the document has contact information, so the time is known to be july 2005, location is texas, etc. now such context information is what we hope to model as well. so we're not going to just model the text. and so one idea here is to model the variations of topic con",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "is known to be july 2005, location is texas, etc. now such context information is what we hope to model as well. so we're not going to just model the text. and so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "we hope to model as well. so we're not going to just model the text. and so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary acc",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. because in the. in the case of location like texas, people might want to cover the red topics more at the new audience, as visualized here. but in a",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "riod, maybe particular topic like donation will be covered more so this variation is also considered in cplsa. so to generate such a document with context, we first also choose a view. and this view of course now could be from any of these contexts. let's say we have taken this view. that depends on the",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "sidered in cplsa. so to generate such a document with context, we first also choose a view. and this view of course now could be from any of these contexts. let's say we have taken this view. that depends on the time in the middle. so now we have a specific version of word distributions. now you can see",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e. before is fixed in plsa and it's hard to a particular document. each document has just one coverage distribution. now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. so, for example, we might",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "one coverage distribution. now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. so, for example, we might pick a particular coverage, let's say in this case. we pick we've picked the document spe",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d this is basically the same process as in plsa. now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. so in other words, we have extra switches that are tied to this context that would control the choices of different views of to",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " the coverage and the word distributions, we let the context influence our choice. so in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage. and naturally, the model will have more parameters to estimate, ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "nd choices of coverage. and naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are som",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a mode",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ll be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide y",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "le to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see s",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e common topics covered in both sets, articles and the differences or variations of the topic in each of the two collections. so in this case, the context that is explicitly specified by the topical collection. and we see the results here show that. there is a common theme that's corresponding to cluste",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "it was more involved in maybe aid to northern alliance as a different variation of the topic of united nations. so this shows that by bringing the context, in this case, different wars are different collections of text. we can have topic variations, tied to these contexts to review the differences of co",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "erent variation of the topic of united nations. so this shows that by bringing the context, in this case, different wars are different collections of text. we can have topic variations, tied to these contexts to review the differences of coverage of united nations in the two wars. similarly, if you look",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "is shows that by bringing the context, in this case, different wars are different collections of text. we can have topic variations, tied to these contexts to review the differences of coverage of united nations in the two wars. similarly, if you look at the second cluster. cluster 2 has to do with the ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " and again. not surprising if you know the background about wars or the wars involved killing of people. but imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles. it can be used to revi",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "know the background about wars or the wars involved killing of people. but imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles. it can be used to review common topics in multiple sets ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ell. if you look down, of course in that column of cluster 2 you see variations of killing of people and that correspond to in different different contexts. and here is another example of results. obtain the front block articles about the hurricane katrina. now in this case, what you see here is visuali",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": ". that might be related to peoples migrating from the state of louisiana to texas, for example. so in this case we can see the time can be used as context to reveal trends of topics. this is some additional result on special patterns and this. in this case it's about the topic of government response. an",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "the very first week on the top left, and that's why again the hurricane rita hit the region. so such a technique would allow us to use location as context to examine variations of topics. and of course, the model is completely general, so you can apply this to any other collections of text to reveal spa",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ocation as context to examine variations of topics. and of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns. is yet another application of this kind of model where we look at the use of the model for event impact analysis",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d top words with high probability is about this model on the left. and then we hope to examine the impact of two events. one is the start of trec for text retrieval conference. this is a major evaluation effort sponsored by us government and was launched in 1992 or around that time and that is known to ",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divide the time periods into a period before the event an another after this event, and then we can compare the differences of",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "nguage model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of event again, the technique is general so you can use this to analyze the impact of any event. here are some suggested re",
        "label": "Use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ere are some suggested readings. the first is paper about simple extension of plsa to enable cross collection comparison. it's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. the second one is the main pa",
        "label": "Use"
      }
    ]
  },
  {
    "text": "word",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "mixture",
    "contexts": []
  },
  {
    "text": "likelihood",
    "contexts": []
  },
  {
    "text": "mutual",
    "contexts": []
  },
  {
    "text": "training",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "language",
    "contexts": []
  },
  {
    "text": "conditional",
    "contexts": []
  },
  {
    "text": "maximum",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "word",
    "contexts": []
  },
  {
    "text": "em",
    "contexts": []
  },
  {
    "text": "topic",
    "contexts": []
  },
  {
    "text": "syntagmatic",
    "contexts": []
  },
  {
    "text": "topic",
    "contexts": []
  },
  {
    "text": "generative",
    "contexts": []
  },
  {
    "text": "background",
    "contexts": []
  },
  {
    "text": "likelihood",
    "contexts": []
  },
  {
    "text": "similarity",
    "contexts": []
  },
  {
    "text": "e",
    "contexts": []
  },
  {
    "text": "scoring",
    "contexts": []
  },
  {
    "text": "parameter",
    "contexts": []
  },
  {
    "text": "random",
    "contexts": []
  },
  {
    "text": "topic",
    "contexts": []
  },
  {
    "text": "non",
    "contexts": []
  },
  {
    "text": "paradigmatic",
    "contexts": []
  },
  {
    "text": "sentiment",
    "contexts": []
  },
  {
    "text": "real",
    "contexts": []
  },
  {
    "text": "logistic",
    "contexts": []
  },
  {
    "text": "data",
    "contexts": []
  },
  {
    "text": "syntagmatic",
    "contexts": []
  },
  {
    "text": "objective",
    "contexts": []
  },
  {
    "text": "bayes",
    "contexts": []
  },
  {
    "text": "opinion",
    "contexts": []
  },
  {
    "text": "naive",
    "contexts": []
  },
  {
    "text": "overall",
    "contexts": []
  },
  {
    "text": "natural",
    "contexts": []
  },
  {
    "text": "k",
    "contexts": []
  },
  {
    "text": "background",
    "contexts": []
  },
  {
    "text": "common",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "categorization",
    "contexts": []
  },
  {
    "text": "opinion",
    "contexts": []
  },
  {
    "text": "high",
    "contexts": []
  },
  {
    "text": "conditional",
    "contexts": []
  },
  {
    "text": "unigram",
    "contexts": []
  },
  {
    "text": "related",
    "contexts": []
  },
  {
    "text": "topic",
    "contexts": []
  },
  {
    "text": "high",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "contextual",
    "contexts": []
  },
  {
    "text": "data",
    "contexts": []
  },
  {
    "text": "paradigmatic",
    "contexts": []
  },
  {
    "text": "hidden",
    "contexts": []
  },
  {
    "text": "total",
    "contexts": []
  },
  {
    "text": "zero",
    "contexts": []
  },
  {
    "text": "document",
    "contexts": []
  },
  {
    "text": "aspect",
    "contexts": []
  },
  {
    "text": "logistical",
    "contexts": []
  },
  {
    "text": "language",
    "contexts": []
  },
  {
    "text": "likelihood",
    "contexts": []
  },
  {
    "text": "posterior",
    "contexts": []
  },
  {
    "text": "machine",
    "contexts": []
  },
  {
    "text": "clustering",
    "contexts": []
  },
  {
    "text": "overall",
    "contexts": []
  },
  {
    "text": "background",
    "contexts": []
  },
  {
    "text": "topic",
    "contexts": []
  },
  {
    "text": "optimization",
    "contexts": []
  },
  {
    "text": "general",
    "contexts": []
  },
  {
    "text": "relation",
    "contexts": []
  },
  {
    "text": "probabilistic",
    "contexts": []
  },
  {
    "text": "special",
    "contexts": []
  },
  {
    "text": "information",
    "contexts": []
  },
  {
    "text": "probabilistic",
    "contexts": []
  },
  {
    "text": "data",
    "contexts": []
  },
  {
    "text": "generative",
    "contexts": []
  },
  {
    "text": "text",
    "contexts": []
  },
  {
    "text": "right",
    "contexts": []
  },
  {
    "text": "basic",
    "contexts": []
  },
  {
    "text": "multiple",
    "contexts": []
  },
  {
    "text": "previous",
    "contexts": []
  },
  {
    "text": "language",
    "contexts": []
  },
  {
    "text": "beta",
    "contexts": []
  },
  {
    "text": "training",
    "contexts": []
  },
  {
    "text": "higher",
    "contexts": []
  },
  {
    "text": "makes",
    "contexts": []
  },
  {
    "text": "bayesian",
    "contexts": []
  },
  {
    "text": "aspect",
    "contexts": []
  },
  {
    "text": "another",
    "contexts": []
  },
  {
    "text": "prior",
    "contexts": []
  },
  {
    "text": "word",
    "contexts": []
  },
  {
    "text": "observed",
    "contexts": []
  },
  {
    "text": "world",
    "contexts": []
  },
  {
    "text": "small",
    "contexts": []
  },
  {
    "text": "sentiment",
    "contexts": []
  },
  {
    "text": "high",
    "contexts": []
  },
  {
    "text": "classification",
    "contexts": []
  },
  {
    "text": "use",
    "contexts": []
  },
  {
    "text": "maximum",
    "contexts": []
  },
  {
    "text": "non",
    "contexts": []
  },
  {
    "text": "background",
    "contexts": []
  },
  {
    "text": "contextual",
    "contexts": []
  },
  {
    "text": "maximum",
    "contexts": []
  }
]