{
    "nodes": [
        {
            "id": "CS410",
            "group": 0
        },
        {
            "id": "conditional entropy",
            "group": 2
        },
        {
            "id": "410_11",
            "group": 1,
            "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we",
            "label": "Use",
            "lecture": "Lecture 11 — Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
            "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0"
        },
        {
            "id": "410_12",
            "group": 1,
            "context": "tion. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable,",
            "label": "Use",
            "lecture": "Lecture 12 — Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
            "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860"
        },
        {
            "id": "410_13",
            "group": 1,
            "context": "he three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches th",
            "label": "Use",
            "lecture": "Lecture 13 — Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
            "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305"
        },
        {
            "id": "maximum likelihood",
            "group": 2
        },
        {
            "id": "410_17",
            "group": 1,
            "context": ". in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meanin",
            "label": "Intro",
            "lecture": "Lecture 17 — Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
            "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8"
        },
        {
            "id": "410_18",
            "group": 1,
            "context": "e and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood ",
            "label": "Intro",
            "lecture": "Lecture 18 — Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
            "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566"
        },
        {
            "id": "410_19",
            "group": 1,
            "context": "rested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. whic",
            "label": "Use",
            "lecture": "Lecture 19 — Probabilistic Topic Models  Mining One Topic | UIUC",
            "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a"
        },
        {
            "id": "410_20",
            "group": 1,
            "context": "oblem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words i",
            "label": "Use",
            "lecture": "Lecture 20 — Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
            "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916"
        },
        {
            "id": "410_21",
            "group": 1,
            "context": " the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be",
            "label": "Use",
            "lecture": "Lecture 21 — Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
            "transcription": "27d06808-2624-4922-a079-04dccb301dde"
        },
        {
            "id": "410_22",
            "group": 1,
            "context": "cause this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the obj",
            "label": "Use",
            "lecture": "Lecture 22 — Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
            "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf"
        },
        {
            "id": "410_23",
            "group": 1,
            "context": "we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model",
            "label": "Use",
            "lecture": "Lecture 23 — Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
            "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee"
        },
        {
            "id": "410_26",
            "group": 1,
            "context": "unction shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only th",
            "label": "Use",
            "lecture": "Lecture 26 — Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
            "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f"
        },
        {
            "id": "410_27",
            "group": 1,
            "context": " tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually",
            "label": "Use",
            "lecture": "Lecture 27 — Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
            "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116"
        },
        {
            "id": "410_28",
            "group": 1,
            "context": " to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is",
            "label": "Use",
            "lecture": "Lecture 28 — Latent Dirichlet Allocation LDA - Part 1 | UIUC",
            "transcription": "5190e288-54f7-4021-9083-8e8ceac11345"
        },
        {
            "id": "410_29",
            "group": 1,
            "context": "r is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you ",
            "label": "Use",
            "lecture": "Lecture 29 — Latent Dirichlet Allocation LDA - Part 2 | UIUC",
            "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012"
        },
        {
            "id": "410_32",
            "group": 1,
            "context": "ck of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it'",
            "label": "Use",
            "lecture": "Lecture 32 — Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
            "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d"
        },
        {
            "id": "410_33",
            "group": 1,
            "context": " parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture mo",
            "label": "Use",
            "lecture": "Lecture 33 — Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
            "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9"
        },
        {
            "id": "410_38",
            "group": 1,
            "context": "are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and",
            "label": "Use",
            "lecture": "Lecture 38 — Text Categorization Generative Probabilistic Models | UIUC",
            "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302"
        },
        {
            "id": "410_39",
            "group": 1,
            "context": "he prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditi",
            "label": "Use",
            "lecture": "Lecture 39 — Text Categorization  Discriminative Classifier - Part 1 | UIUC",
            "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6"
        },
        {
            "id": "410_46",
            "group": 1,
            "context": "ters and so we collectively denote all the parameters by lambda here. now we can, as usual, use the maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed. observ",
            "label": "Use",
            "lecture": "Lecture 46 — Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
            "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3"
        },
        {
            "id": "word distribution",
            "group": 2
        },
        {
            "id": "410_9",
            "group": 1,
            "context": "is would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one. now the weight of bm25 for each word is defined here. and if you c",
            "label": "Use",
            "lecture": "Lecture 9 — Paradigmatic Relation Discovery - Part 2 | UIUC",
            "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939"
        },
        {
            "id": "410_16",
            "group": 1,
            "context": " lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the old representation, where we represent each topic with just one word or",
            "label": "Use",
            "lecture": "Lecture 16 — Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
            "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb"
        },
        {
            "id": "410_24",
            "group": 1,
            "context": "t's why we call these hidden variables. now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. the value of this hidden varia",
            "label": "Use",
            "lecture": "Lecture 24 — Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
            "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5"
        },
        {
            "id": "410_31",
            "group": 1,
            "context": "nd we hope to generate as output two things. one is a set of topics denoted by theta i's. each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. so",
            "label": "Use",
            "lecture": "Lecture 31 — Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
            "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568"
        },
        {
            "id": "410_47",
            "group": 1,
            "context": "ld be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed ",
            "label": "Use",
            "lecture": "Lecture 47 — Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
            "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402"
        },
        {
            "id": "410_50",
            "group": 1,
            "context": "different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the s",
            "label": "Use",
            "lecture": "Lecture 50 — Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
            "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616"
        }
    ],
    "links": [
        {
            "source": "410_11",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_12",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_13",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_11",
            "target": "conditional entropy",
            "value": 1
        },
        {
            "source": "410_12",
            "target": "conditional entropy",
            "value": 1
        },
        {
            "source": "410_13",
            "target": "conditional entropy",
            "value": 1
        },
        {
            "source": "410_13",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_17",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_18",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_19",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_20",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_21",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_22",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_23",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_26",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_27",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_28",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_29",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_32",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_33",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_38",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_39",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_46",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_13",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_17",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_18",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_19",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_20",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_21",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_22",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_23",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_26",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_27",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_28",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_29",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_32",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_33",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_38",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_39",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_46",
            "target": "maximum likelihood",
            "value": 1
        },
        {
            "source": "410_9",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_16",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_17",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_18",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_19",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_20",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_21",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_22",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_23",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_24",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_26",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_27",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_28",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_29",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_31",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_32",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_33",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_38",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_47",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_50",
            "target": "CS410",
            "value": 1
        },
        {
            "source": "410_9",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_16",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_17",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_18",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_19",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_20",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_21",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_22",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_23",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_24",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_26",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_27",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_28",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_29",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_31",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_32",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_33",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_38",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_47",
            "target": "word distribution",
            "value": 1
        },
        {
            "source": "410_50",
            "target": "word distribution",
            "value": 1
        }
    ]
}