{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 25.0/25.0 [00:00<00:00, 9.55kB/s]\n",
      "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 761kB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 5.72MB/s]\n",
      "Downloading: 100%|██████████| 1.74k/1.74k [00:00<00:00, 841kB/s]\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForSeq2SeqLM, AutoTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb#ch0000000?line=2'>3</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb#ch0000000?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb#ch0000000?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yirenliu/Programming/NSF_NETTL/Caption-Mining/QuestionGeneration/models/T5/test_qg.ipynb#ch0000000?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_question\u001b[39m(answer, context, max_length\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:498\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    499\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1743\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1741\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1743\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1744\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1745\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1746\u001b[0m     init_configuration,\n\u001b[1;32m   1747\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1748\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1749\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1750\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1871\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1871\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1872\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1873\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1874\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1875\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1876\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:128\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[1;32m    123\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are provided to T5Tokenizer. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIn this case the additional_special_tokens must include the extra_ids tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[0;32m--> 128\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    129\u001b[0m     vocab_file,\n\u001b[1;32m    130\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    131\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    132\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    133\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    134\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[1;32m    135\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RETTL/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:117\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mrm8488/t5-base-finetuned-question-generation-ap\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "  return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"this section is a short review of system architecture topics that you’ll need for system programming.\"\n",
    "answer = \"this section\"\n",
    "\n",
    "get_question(answer, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RETTL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c16594d52f1bbdc28772c674e21bc33b9170541f681f2409e5dbd92f2b3c8f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
