410	01c0f454-a1bb-450d-babe-6201b372d9e9	90
#s1	So I showed you how we rewrite the query likelihood retrieval function into a form that looks like the formula of this slide.
#c1	I;you;we;the query likelihood retrieval function;a form;the formula;this slide
#s2	After we make the assumption about the smoothing the language model.
#c2	we;the assumption;the language model
#s3	Based on the collection language model.
#c3	the collection language model
#s4	If you look at the this rewriting it actually would give us two benefits.
#c4	you;it;us;two benefits
#s5	The first benefit is it helps us better understand this ranking function.
#c5	The first benefit;it;us;this ranking function
#s6	In particular, we're going to show that from this formula we can see smoothing with the collection language model will give us something like a TF IDF weighting and length normalization.
#c6	we;this formula;we;the collection language model;us;something;a TF IDF weighting and length normalization
#s7	The second benefit is that it also allows us to compute the query likelihood more efficiently.
#c7	The second benefit;it;us;the query likelihood
#s8	In particular, we see that the main part of the formula is a sum over the matched query terms.
#c8	we;the main part;the formula;a sum;the matched query terms
#s9	So this is much better than if we take a sum over all the words.
#c9	we;a sum;all the words
#s10	After we smooth the document language model, we send you to have non zero probabilities for all the words.
#c10	we;the document language model;we;you;non zero probabilities;all the words
#s11	So this new form of the formula is much easier to score or to compute.
#c11	this new form;the formula
#s12	It's also interesting to note that the last term here is actually independent of the document, since our goal is to rank the documents for the same query, we can ignore this term for ranking.
#c12	It;the last term;the document;our goal;the documents;the same query;we;this term;ranking
#s13	Because it's going to be the same for all the documents.
#c13	it;all the documents
#s14	Ignoring it wouldn't affect the order of the documents.
#c14	it;the order;the documents
#s15	Inside the sum, We also see that each matched query term would contribute.
#c15	the sum;We;each matched query term
#s16	weight
#c16	weight
#s17	And this weight actually is very interesting.
#c17	this weight
#s18	because it looks like a TF IDF weighting.
#c18	it;a TF IDF weighting
#s19	First, we can already see it has a frequency of the word in the query, just like in the vector space model.
#c19	we;it;a frequency;the word;the query;the vector space model
#s20	When we take a dot product we see the word frequency in the query to show up in such a sum.
#c20	we;a dot product;we;the word frequency;the query;such a sum
#s21	And so naturally, this pot would correspond to the vector element from the document vector, and here indeed we can see it actually encodes a weight that has similar factor to TF IDF weighting.
#c21	this pot;the vector element;the document vector;we;it;a weight;similar factor;TF IDF weighting
#s22	I'll let you examine it.
#c22	I;you;it
#s23	Can you see it?
#c23	you;it
#s24	Can you see which part is capturing TF?
#c24	you;which part;TF
#s25	and which part is capturing IDF weighting?
#c25	which part;IDF weighting
#s26	So if you want you can pause the video to think more about it.
#c26	you;you;the video;it
#s27	So, have you noticed that this p  of seen is related to the term frequency?
#c27	you;this p;the term frequency
#s28	In the sense that if a word occurs very frequently in the document, then the estimated probability here would tend to be larger.
#c28	the sense;a word;the document;the estimated probability
#s29	So this means this term is really doing something like TF weighting.
#c29	this term;something;TF weighting
#s30	Have you also notice that?
#c30	you
#s31	This term in the denominator.
#c31	This term;the denominator
#s32	Is actually achieving the effect of IDF?
#c32	the effect;IDF
#s33	Why?
#s34	Because this is the popularity of the term in the collection.
#c34	the popularity;the term;the collection
#s35	But it's in the denominator, so if.
#c35	it;the denominator
#s36	"The probability in the collection is   larger, then the weight is actually smaller, and this means a popular term.
#c36	The probability;the collection;the weight;a popular term
#s37	We actually have a smaller weight and this is precisely what IDF weighting is doing.
#c37	We;a smaller weight;precisely what;IDF weighting
#s38	Only that we now have a different form of TF and IDF. "
#c38	we;a different form;TF;IDF
#s39	Remember, IDF document frequency.
#s40	But here we have something different.
#c40	we;something
#s41	But intuitively it achieves a similar fact.
#c41	it;a similar fact
#s42	Interestingly, we also have something related to the length normalization.
#c42	we;something;the length normalization
#s43	Again, can you see which factor is related to the document length?
#c43	you;which factor;the document length
#s44	In this formula.
#c44	this formula
#s45	I just say that this term is related to IDF weighting.
#c45	I;this term;IDF weighting
#s46	This.
#s47	This collection probability, but it turns out that this term here is actually related to the document length.
#c47	This collection probability;it;this term;the document length
#s48	Normalization in particular alpha sub
#c48	Normalization;particular alpha sub
#s49	d might be related to document.
#c49	document
#s50	length, so it encodes how much probability mass we want to give to unseen words.
#c50	it;how much probability mass;we;unseen words
#s51	How much smoothing do we want to do ?
#c51	How much smoothing;we
#s52	Intuitively, if a document is long then we need to do less smoothing because we can assume that data is large enough.
#c52	a document;we;we;data
#s53	We probably have observed all the words that the author could have written, but the document is short.
#c53	We;all the words;the author;the document
#s54	Then alpha sub d could be expected to be to be large.
#c54	d
#s55	We need to do more smoothing.
#c55	We;more smoothing
#s56	It's like that there are words that have not been written yet by the other.
#c56	It;words
#s57	So this term appears to penalize long documenting in that the alpha sub d would tend to be longer than larger than.
#c57	this term;long documenting;d
#s58	for a long document.
#c58	a long document
#s59	But note that the alpha sub d also occurs here.
#c59	the alpha
#s60	And so this may not actually be necessary.
#s61	Penalizing long documents effect is not so clear here.
#c61	long documents effect
#s62	But as we will see later when we consider some specific smoothing methods, it turns out that they do penalize long documents just like in TF "IDF weighting  normalization formulas in the vector space model.
#c62	we;we;some specific smoothing methods;it;they;long documents;TF "IDF weighting  normalization formulas;the vector space model
#s63	So that's a very interesting observation, because it means we don't even have to think about the specific way of doing smoothing.
#c63	a very interesting observation;it;we;the specific way;smoothing
#s64	We just need to assume that if we smooth with this collection language model, then we would have a formula.
#c64	We;we;this collection language model;we;a formula
#s65	That looks like a TF IDF weighting and documents length normalization.
#c65	a TF IDF weighting;documents
#s66	What's also interesting is that we have very fixed form of the ranking function.
#c66	What;we;very fixed form;the ranking function
#s67	And see we have not heuristically put a logarithm here.
#c67	we;a logarithm
#s68	In fact, you can think about why we will have a logarithm here.
#c68	fact;you;we;a logarithm
#s69	If you look at the assumptions that we have made, it will be clear it's because we have.
#c69	you;the assumptions;we;it;it;we
#s70	used logarithm of query likelihood for scoring.
#c70	logarithm;query likelihood;scoring
#s71	And we turned the product into a sum of logarithm of probability and that's why we have this logarithm.
#c71	we;the product;a sum;logarithm;probability;we;this logarithm
#s72	Note that if we only want to heuristically implement the TF weight and IDF weighting, we don't necessarily have to have a logarithm here.
#c72	we;the TF weight;IDF weighting;we;a logarithm
#s73	Imagine if we drop this logarithm, we would still have TF and IDF weighting.
#c73	we;we;TF and IDF weighting
#s74	But what's nice with probabilistic modeling is that we are automatically given a logarithm function here.
#c74	what;probabilistic modeling;we;a logarithm function
#s75	And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case, if you try to drop this logarithm, the more of probably one work as well as if you keep logarithm.
#c75	a fixed form;the formula;we;this case;you;probably one work;you;logarithm
#s76	So a nice property of probabilistic modeling is that by following some assumptions and probability rules we will get a formula automatically.
#c76	a nice property;probabilistic modeling;some assumptions;probability rules;we;a formula
#s77	And the formula would have a particular form, like in this case.
#c77	the formula;a particular form;this case
#s78	And if we heuristically design the formula, we may not necessary end up  having such a specific form.
#c78	we;the formula;we;such a specific form
#s79	So to summarize, we talked about the need for smoothing document language model, otherwise would give zero probability for unseen words in the document.
#c79	we;the need;document language model;zero probability;unseen words;the document
#s80	And that's not good for scoring a query with such an unseen world.
#c80	a query;such an unseen world
#s81	And it's also necessary in general to improve the accuracy of estimating the model represents the topic of this document.
#c81	it;the accuracy;the model;the topic;this document
#s82	The general idea of smoothing in retrieval is to use the collection language Model to Give us some clue about the which unseen word should have a higher probability.
#c82	The general idea;retrieval;the collection language Model;us;some clue;the which unseen word;a higher probability
#s83	That is, the probability of an unseen word is assumed to be proportional to its probability in the collection.
#c83	the probability;an unseen word;its probability;the collection
#s84	With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has the effect of TF IDF weighting and document length normalization.
#c84	this assumption;we;we;a general ranking formula;query likelihood;the effect;TF IDF weighting;document length normalization
#s85	We also see that through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on match query terms, just like it in the vector space model.
#c85	We;some rewriting;the scoring;such a ranking function;sum;weights;match query terms;it;the vector space model
#s86	But the actual ranking function is given us automatically by the probability rules and the assumptions that we have made and unlike in the vector space model where we have to heuristically.
#c86	the actual ranking function;us;the probability rules;the assumptions;we;the vector space model;we
#s87	Think about the form of the function.
#c87	the form;the function
#s88	However, we still need to address the question how exactly we should smooth the document language model.
#c88	we;the question;we;the document language model
#s89	How exactly we should use the reference language model based on the collection to adjust the probability of the maximum likelihood estimate?
#c89	we;the reference language model;the collection;the probability;the maximum likelihood estimate
#s90	And this is the topic of the next lecture.
#c90	the topic;the next lecture
410	09a64f72-3fa7-4dc9-8385-498e17d0bd8b	95
#s1	This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model.
#c1	This lecture;the specific smoothing methods;language models;probabilistic retrieval model
#s2	In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method, and we're going to talk about the specifics smoothing methods used for such a retrieval function.
#c2	this lecture;we;the discussion;language models;information retrieval;particularly the query likelihood retrieval method;we;the specifics smoothing methods;such a retrieval function
#s3	So this is a slide from a previous lecture where we show that with the query likelihood ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following.
#c3	a slide;a previous lecture;we;the query likelihood;the collection language model;we;a retrieval function;the following
#s4	So this is the retrieval function based on these assumptions that we have discussed, you can see it's a sum over all the matched The query terms here.
#c4	the retrieval function;these assumptions;we;you;it;a sum;all the matched The query terms
#s5	And inside the sum is the count of the term in the query and some weight.
#c5	the sum;the count;the term;the query;some weight
#s6	For the term in the document.
#c6	the term;the document
#s7	And we have TF IDF weight here
#c7	we;TF IDF weight
#s8	and then we have another constant here in the end
#c8	we;another constant;the end
#s9	So clearly, if we want to implement this function using a program language, we will still need to figure out a few variables.
#c9	we;this function;a program language;we;a few variables
#s10	In particular, we're going to need to know how to estimate the probability of a word Exactly and.
#c10	we;the probability;a word
#s11	How do we set alpha?
#c11	we;alpha
#s12	So in order to answer these questions, we have to think about this very specifically, smoothing methods and that is the main topic of this lecture.
#c12	order;these questions;we;methods;the main topic;this lecture
#s13	We are gonna talk about the two smoothing  methods.
#c13	We;  methods
#s14	The first is the simple linear interpolation with a fixed coefficient.
#c14	the simple linear interpolation;a fixed coefficient
#s15	And this is also called a Jelinek-Mercer smoothing.
#c15	a Jelinek-Mercer smoothing
#s16	So the idea is actually very simple.
#c16	the idea
#s17	This picture shows how we estimate.
#c17	This picture;we
#s18	Document the language model by using maximum likelihood estimate that gives us word counts normalized by the total number of words in the text.
#c18	the language model;maximum likelihood estimate;us;word counts;the total number;words;the text
#s19	The idea of using this method.
#c19	The idea;this method
#s20	Is to maximize the probability of the observed text as a result, if a word like network.
#c20	the probability;the observed text;a result;network
#s21	Is not observed in the text, it's going to get zero probability as shown here.
#c21	the text;it;zero probability
#s22	So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what non zero probability should be assigned to such a word.
#c22	the idea;smoothing;collection language model;this word;a zero probability;us;what;non zero probability;such a word
#s23	So we can note that Network has a non zero probability here.
#c23	we;Network;a non zero probability
#s24	So in this approach in what we do is we do a linear interpolation between the maximum likelihood estimate here and the collection language model and this is controlled by the smoothing parameter Lambda.
#c24	this approach;what;we;we;a linear interpolation;the maximum likelihood estimate;the collection language model;the smoothing parameter;Lambda
#s25	Which is.
#s26	Between zero and one.
#s27	So this is a smoothing parameter.
#c27	a smoothing parameter
#s28	The larger lambda is, the more smoothing  we will have.
#c28	The larger lambda;we
#s29	So by mixing them together we achieve the goal of assigning non zero probabilities to a word network.
#c29	them;we;the goal;non zero probabilities;a word network
#s30	So let's see how it works for some of the words here.
#c30	's;it;the words
#s31	For example, if we compute the smooth probability for text.
#c31	example;we;the smooth probability;text
#s32	Now the maximum likelihood estimate gives us 10 / 100 and that's going to be here.
#c32	the maximum likelihood estimate;us
#s33	But the collection probability is this, so we just combine them together with this simple formula.
#c33	the collection probability;we;them;this simple formula
#s34	We can also see.
#c34	We
#s35	The word network which used to have zero probability now is getting a non zero probability.
#c35	The word network;zero probability;a non zero probability
#s36	Of this value, and that's because the count is going to be 0 for network.
#c36	this value;the count;network
#s37	Here, but this part is non zero, and that's basically how this method works.
#c37	this part;this method
#s38	If you think about this and you can easily see now the  Alpha sub D in this smoothing method is basically Lambda.
#c38	you;you;the  Alpha sub D;this smoothing method;Lambda
#s39	Because that's remember the coefficient in front of the probability of the word given by the collection language model here, right?
#c39	the coefficient;front;the probability;the word;the collection language model
#s40	OK, so this is the first smoothing method.
#c40	the first smoothing method
#s41	A second one is similar, but it has a dynamic coefficient for linear interpolation.
#c41	A second one;it;a dynamic coefficient;linear interpolation
#s42	It is often called the Dirichlet Prior or Bayesian smoothing.
#c42	It;the Dirichlet;Bayesian
#s43	So again, here we face the problem of zero probability for an unseen word like network.
#c43	we;the problem;zero probability;an unseen word;network
#s44	Again, we will use the collection language model, but in this case we're going to combine them in somewhat different ways.
#c44	we;the collection language model;this case;we;them;somewhat different ways
#s45	The formula first can be seen as an interpolation of the.
#c45	The formula;an interpolation
#s46	Maximum likelihood estimate and the collection language model as before as in the JM smoothing method.
#c46	Maximum likelihood estimate;the collection language model;the JM smoothing method
#s47	Only at the coefficient now is not the Lambda a fixed number, but that dynamic coefficient in this form where mu is a parameter.
#c47	the coefficient;the Lambda;a fixed number;that dynamic coefficient;this form;mu;a parameter
#s48	It's a non-negative value.
#c48	It;a non-negative value
#s49	And you can see what if we set the mu to a constant.
#c49	you;we;the mu;a constant
#s50	The effect is that a long document would actually get a smaller coefficient here.
#c50	The effect;a long document;a smaller coefficient
#s51	Because a long document that will have longer length, therefore, the coefficient is actually smaller.
#c51	a long document;longer length;the coefficient
#s52	And so a long document would have less smoothy as we would expect.
#c52	a long document;we
#s53	So this seems to make more sense than fixed coefficient smoothing.
#c53	more sense;fixed coefficient smoothing
#s54	Of course this part.
#c54	Of course this part
#s55	Would be of this form so that the two coefficients would sum to one.
#c55	this form;the two coefficients
#s56	Now this is one way to understand that this smoothing Basically it means it's a dynamic coefficient interpolation  
#c56	one way;this smoothing;it;it;a dynamic coefficient interpolation
#s57	There is another way to understand this formula.
#c57	another way;this formula
#s58	Which is even easier to remember, and that's this side.
#c58	this side
#s59	So it's easy to see.
#c59	it
#s60	We can rewrite the smoothing method in this form.
#c60	We;the smoothing method;this form
#s61	Now in this form we can easily to see what changes we have made to the maximum likelihood estimate which would be this part right?
#c61	this form;we;what changes;we;the maximum likelihood estimate;this part
#s62	So normalized count by the document length So in this form we can see what we did is we add this to the count of every word.
#c62	count;the document length;this form;we;what;we;we;the count;every word
#s63	So what does this mean?
#c63	what
#s64	This is basically.
#s65	Something related to the probability of the word in the collection, and we multiply that by the parameter mu.
#c65	Something;the probability;the word;the collection;we;the parameter mu
#s66	And when we combine this with the count here, essentially we are adding pseudo counts.
#c66	we;the count;we;pseudo counts
#s67	To the observed text.
#c67	the observed text
#s68	We pretend.
#c68	We
#s69	Every word has got this many pseudo count.
#c69	Every word;this many pseudo count
#s70	So the total counts would be the sum of these pseudo counts and the actual count.
#c70	the total counts;the sum;these pseudo counts;the actual count
#s71	Of the word in the document.
#c71	the word;the document
#s72	As a result, in total we would have added this many pseudo counts why?
#c72	a result;total;we
#s73	Because if you take a sum of this.
#c73	you;a sum
#s74	"This one, over all the words that we see,  So this probability would still sum to one.
#c74	This one;all the words;we;this probability
#s75	  
#s76	So in this case we can either see the method.
#c76	this case;we;the method
#s77	Is essentially to Add these.
#s78	As pseudo count to this data pretend we actually augment the data by including some pseudo data defined by the collection language model.
#c78	pseudo count;this data;we;the data;some pseudo data;the collection language model
#s79	As a result, we have more counts.
#c79	a result;we;more counts
#s80	I still.
#c80	I
#s81	The total counts for a word would be like this and as a result even if a word has zero count here let's says we have zero account here and it would still have non zero count because of this part.
#c81	The total counts;a word;a result;a word;zero count;'s;we;zero account;it;non zero count;this part
#s82	So this is how this method works.
#c82	this method
#s83	Let's also take a look at some specific example here.
#c83	's;a look;some specific example
#s84	Right, so for text again, we will have 10 as an original count that we actually observe, but we also add some pseudo count.
#c84	text;we;an original count;we;we;some pseudo count
#s85	And so the probability of the text would be of this form naturally.
#c85	the probability;the text;this form
#s86	The probability of network would be just this part.
#c86	The probability;network;just this part
#s87	And so here you can also see what's alpha
#c87	you;what
#s88	sub d here.
#s89	Can you see it if you want to think about, you can pause the video.
#c89	you;it;you;you;the video
#s90	Have you notice that this part is basically alpha sub D?
#c90	you;this part;sub D
#s91	So we can see this case.
#c91	we;this case
#s92	Alpha sub D does depend on the document.
#c92	Alpha sub D;the document
#s93	Because.
#s94	This length depends on the document, whereas in the linear interpolation the JM smoothing method.
#c94	This length;the document;the linear interpolation;the JM
#s95	This is a constant.
410	0be0ce0e-3101-4deb-90c8-e34762c20d57	21
#s1	this lecture is about how to do faster search by using inverted index in this latter we're going to continue the discussion of the system implementation in particular we're going to talk about how to support the faster search by using mercury index
#c1	this lecture;faster search;inverted index;we;the discussion;the system implementation;we;the faster search;mercury index
#s2	so let's think about watt a general scoring function might look like now of course the vector space model is a special case of this
#c2	's;watt;a general scoring function;course;the vector space model;a special case
#s3	but we can imagine many other retrieval functions of the same form so the form of this function is as follows we see this scoring function of document D and query Q is defined as first a function of FA that's adjustment the function that would the consider two factors that are assume here at the end if somebody of the NF subq of Q these are adjustment factors of document and query so they are at the level of a document and query
#c3	we;many other retrieval functions;the same form;the form;this function;we;this scoring function;document D;query Q;a function;FA;adjustment;two factors;the end;somebody;the NF subq;Q;adjustment factors;document;query;they;the level;a document;query
#s4	so and then inside of this function we also see there's another function called edge so this is the main part of the scoring function an these as i just said all the scoring factors at the level of the whole document and query for example document or lands and this aggregate function would then combine all these now inside this edge function there are functions that with the computer the weights of the contribution of a magical query term T
#c4	this function;we;another function;edge;the main part;the scoring function;i;all the scoring factors;the level;the whole document;query;example document;lands;this aggregate function;this edge function;functions;the computer;the contribution;a magical query term
#s5	so this is G the function G gives us the weight of a match to query term T
#c5	G;the function;G;us;the weight;a match;query term;T
#s6	I in document D and this H function would then aggregate all these weights so it will for example take a sum but of all the match the query terms but it can also be a product or it could be another way of aggregating them and then finally this adjustment function would then consider the document level or query level factors to further adjusted score for example of documents model addition so this general form would cover many state of large real functions let's look at how we can score such for documents with such a function using inverted index
#c6	I;document D;this H function;all these weights;it;example;a sum;all the match;the query terms;it;a product;it;another way;them;this adjustment function;the document level or query level factors;example;documents model addition;this general form;many state;large real functions;'s;we;documents;such a function;inverted index
#s7	so here's a general algorithm that works as follows first these query level and document level factors can be precomputed in the indexing time of course for the query we have to compute it at the query in time but for document for example document length can be precomputed and then women hang a score accumulator for each document D to computer edge edges aggregation function over order magically returns
#c7	a general algorithm;these query level and document level factors;the indexing time;course;the query;we;it;the query;time;document;example document length;women;a score accumulator;each document D;edge;aggregation function;order
#s8	so how do we do that well for each query term we're going to fetch the inverter list from the inverted index this would give us all the documents that match this query term an that includes D one F one anthro D N F N so each pair is document ID and the frequency of the term in the document then for each entry the subject an F subject a particular match of the term in this particular document the subject we're going to compute the function G that would give us something like a TF IDF weights of this come so will compute the weighted contribution of matching this query term in this document and then we're going to updated the score accumulator for this document and this would allow us to add this tour accumulator that would incrementally compute function edge so this is basically a general way to allow us to computer all functions of this form by using inverted index note that we don't have to touch any document that didn't match any query account this is why it's a fast we only need to process the document that that matched at least one query term in the end then we're going to adjust to the score to compute this function F subway and then we can sort so let's take a look at the specific example in this case let's assume the scoring function is a very simple one it just takes the sum of TF the road here the count of a term in the document now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include the other weights like the transformation of TF or document length normalization or IDF weighting so let's take a look at specific example where the queries information security
#c8	we;each query term;we;the inverter list;the inverted index;us;all the documents;this query term;D one F one anthro D N F N;each pair;document ID;the frequency;the term;the document;each entry;the subject;a particular match;the term;this particular document;the subject;we;the function;us;something;a TF IDF weights;the weighted contribution;this query term;this document;we;the score accumulator;this document;us;this tour accumulator;function edge;a general way;us;all functions;this form;inverted index note;we;any document;any query account;it;we;the document;at least one query term;the end;we;the score;this function;F subway;we;'s;a look;the specific example;this case;'s;the scoring function;a very simple one;it;the sum;TF;the road;the count;a term;the document;this simplification;the algorithm;it;the computation;the other weights;the transformation;TF or document length normalization;IDF weighting;'s;a look;specific example
#s9	and i show some entries of the inverted index on the right side information recording four documents and their frequencies also their security error code in three documents so let's see how the algorithm works so first we iterate over all the query terms and we fetch the first query them what is that that information so and imagine we have all these score accumulate yrs to slow scored up install the scores for these documents we can imagine there will be allocated but then they will only be allocated as needed so before we do any waiting of terms we don't even need a score communicators that concept if we have these the score accumulated eventually are located so let's fetch the entries from the inventory list for information for the first one so these score accumulate is obviously would be initialized zeros
#c9	i;some entries;the inverted index;the right side information;four documents;their frequencies;also their security error code;three documents;'s;the algorithm;we;all the query terms;we;the first query;them;what;that information;we;all these score;yrs;the scores;these documents;we;they;we;any waiting;terms;we;a score communicators;that concept;we;the score;'s;the entries;the inventory list;information;these score accumulate
#s10	so the first avenger is D one and three three is occurrences of information in this document since our scoring function assume that the school is just a sum of these raw counts we just need to add a three to the score accumulator to account for the increase of score do two matching this term information a document that you are and then we go to the next entry that's D two and four and then we added four to the score accumulator of the two of course at this point that we were located the score accumulator as needed and so at this point we allocated D one and D two and the next wednesday three and we add one we allocate another score committed the parties and everyone do it and then find the default gets a five
#c10	the first avenger;D;occurrences;information;this document;our scoring function;the school;just a sum;these raw counts;we;the score accumulator;the increase;score;information;a document;you;we;the next entry;we;the score accumulator;course;this point;we;the score accumulator;this point;we;D;D;we;we;another score;the parties;everyone;it;the default
#s11	be cause the information the term information occured in five times in this document
#c11	the information;the term information;five times;this document
#s12	OK so this completes the processing of all the edges in the inverted index for information that process the all the contributions of matching information in these four documents so now our algorithm will go to the next query time that security so we're going to fetch order inverted index entries for security so in this case there are three entries and we're going to go through each of them the first is D two and three and that means security recorded three times in D two and what do we do well we do exactly the same as what we did for information
#c12	the processing;all the edges;the inverted index;information;the all the contributions;matching information;these four documents;our algorithm;the next query time;we;order inverted index entries;security;this case;three entries;we;them;D;security;D;what;we;we;what;we;information
#s13	so this time we're going to change the score accumulated D two since it's already allocated and what we do is to add a three to the existing value which is four so we now get a seven ford E two D two score is increased with the cause of the match to both the information and security go to the next entry that's D four and one so we would updated the score forty four
#c13	we;the score;it;what;we;the existing value;we;a seven ford E;two D two score;the cause;the match;both the information;security;the next entry;D;we;the score
#s14	and again we add one to default so default now goes from five to six finally we process the five and a three since we have not yet allocated a score accumulated forty five at this point with an allocated one forty five and we're going to add a three to it so those scores on the last roll are the final scores for these documents if our scoring function is just a simple sum of TF values now what if we actually would like to do length normalization well we can do the normalization at this point for each document so to summarize this so you can see we first are processed information turn query term information we process all the edges in the inverted index for this term then with process security it's was think about what should be the order of processing here when we consider query terms it might make difference especially if we don't want to keep all the score accumulated 's let's say we only want to keep the most promising score accumulate yrs what do you think it would be a good order to go through would you go would you process a common term first all with you processor rail home first the answer is we should go to who should process the rail term first auratum with metra fewer documents and then the score contribution would be higher because the idea of value would be higher
#c14	we;default;we;we;a score;this point;we;it;those scores;the last roll;the final scores;these documents;our scoring function;just a simple sum;TF values;we;length normalization;we;the normalization;this point;each document;you;we;information;query term information;we;all the edges;the inverted index;this term;process security;it;what;the order;processing;we;query terms;it;difference;we;all the score;'s;we;what;you;it;a good order;you;you;a common term;you;the answer;we;who;the rail term;metra fewer documents;the score contribution;the idea;value
#s15	and then it allows us to attach the most promising documents first so it helps pruning some non promising ones if we don't need a so many documents to be returned to the user so those are all heuristics for further improving accuracy here you can also see how we can incorporate the IDF weighting so they can be incorporated at when we process each query term when we fetch the inverted index we confess to the document frequency and then we can compute the idea or maybe perhaps the IDF value has already been precomputed when we index the documents at that time we already computed the IDF value that we can just attach it so all these can be down at this time so that would mean when we process all the entries for information these these words will be adjusted by the same idea which is the idea for information
#c15	it;us;the most promising documents;it;some non promising ones;we;a so many documents;the user;heuristics;further improving accuracy;you;we;the IDF weighting;they;we;each query term;we;the inverted index;we;the document frequency;we;the idea;the IDF value;we;the documents;that time;we;the IDF value;we;it;this time;we;all the entries;information;these these words;the same idea;the idea;information
#s16	so this is the basic idea of using inverted index for fast research and works well for all kinds of formulas that are of the general form and this general general form covers actually most state of the art and retrieval functions so there are some tricks to further improve the efficiency some general mac technique techniques include cashing this
#c16	the basic idea;inverted index;fast research;all kinds;formulas;the general form;this general general form;most state;the art and retrieval functions;some tricks;the efficiency;some general mac technique techniques
#s17	is we just the store some results of popular queries so that next time when you see the same query you simply return the store results similarly you can also small the list of inverted index in the memory for popular term and if the query terms popular likely you will soon need to fetch the inverted index for the same term again so keeping the in the memory with help and these are general techniques for improving efficiency we can also keep only the most promising accumulate yrs because a user generators in the one to examine so many documents we only need to return high quality subset of documents that likely are ranked on the top in for that purpose we can then prune the accumulator is we don't have to store all the documentaries at some point we just keep the highest value accumulate yrs another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and the scale up through the web scale really do special to have a special techniques to do parallel processing and to distribute the storage of files on multiple machines so here as here's a list of some text retrieval tool kits it's not a complete list you can find more information at this URL on the bottom
#c17	we;just the store;some results;popular queries;you;the same query;you;the store results;you;the list;inverted index;the memory;popular term;the query;you;the inverted index;the same term;the memory;help;general techniques;efficiency;we;only the most promising accumulate yrs;the one;so many documents;we;high quality subset;documents;the top;that purpose;we;the accumulator;we;all the documentaries;some point;we;the highest value;yrs;another technique;parallel processing;such a large data;the web data;the scale;the web scale;special;a special techniques;parallel processing;the storage;files;multiple machines;a list;some text retrieval tool kits;it;a complete list;you;more information;this URL;the bottom
#s18	yeah i listed before here lucy 's one of the most popular tool kit that can support a lot of applications and it has very nice for the for applications you can use the builder search any application very quickly the downside is that it's not that easy to extend it an the algorithms implemented there also not the most advanced the algorithms lima or injury is another tool kit that that that does not have such a nice support of application as we'll see
#c18	i;the most popular tool kit;a lot;applications;it;applications;you;the builder search;any application;the downside;it;it;an the algorithms;not the most advanced the algorithms lima;injury;another tool kit;such a nice support;application;we
#s19	but it has many advanced search algorithms
#c19	it;many advanced search algorithms
#s20	and it's also easy to extend terrier is yet another tool kit that also has good support for application capability and some amounts of algorithms
#c20	it;another tool kit;good support;application capability;some amounts;algorithms
#s21	so that's maybe in between lima or losing or maybe rather combining the strains of both so that's also useful tool kit metal is tool kit that we use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms and so top of the models are implement their and there are a number of text analysis algorithms implemented in the toolkit 's whereas basically search algorithms so to summarize all the discussion about the system implementation here are the major takeaway points an inverted index is the primary there are structural for supporting a search engine that's the key to enable fast response to a user 's query and the basic idea is a process preprocess data as much as we can and we want to do compression a proper it so that we can save disk space and can speed up IO and processing of inverter in there some general we talk about how to construct the inverted index when the data can fit into the memory and then we talk about fast search using that index basically it's exploited the inverted index accumulator scores for documents matching operator and we exploit the zipf 's law to avoid attaching many documents that don't match any query term and this algorithm can actually support a wide range of ranking algorithms so these basic techniques have have great potential for further scanning up using distributed file system parallel processing and the caching here are two additional readings that you can take a look and if you have time and you're interested in learning more about this the first one is the classical textbook efficiency of inverted index and the compression techniques and how to in general build efficient search engine in terms of the space overhead and speed the second one is a newer xbox that has a nice discussion of implementing and evaluating search engines
#c21	lima;the strains;useful tool kit metal;tool kit;we;the programming assignment;a new tool kit;a combination;both text retrieval algorithms;text mining algorithms;so top;the models;their;a number;text analysis algorithms;the toolkit;algorithms;all the discussion;the system implementation;the major takeaway;an inverted index;the primary;a search engine;the key;fast response;a user 's query;the basic idea;a process preprocess data;we;we;compression;a proper it;we;disk space;IO;processing;inverter;some general;we;the inverted index;the data;the memory;we;fast search;that index;it;the inverted index accumulator scores;documents;matching operator;we;the zipf 's law;many documents;any query term;this algorithm;a wide range;ranking algorithms;these basic techniques;great potential;distributed file system parallel processing;the caching;two additional readings;you;a look;you;time;you;the classical textbook efficiency;inverted index;the compression techniques;efficient search engine;terms;the space;the second one;a newer xbox;a nice discussion;search engines
410	0c5d9020-626f-4f0e-87c7-04e310bf2c48	16
#s1	this lecture is about the document length normalization in the vector space model in this latter we're going to continue the discussion of the vectors based model in particular we're going to discuss the issue of talking the length normalization so far in the lectures about the vector space model we have used the various signals from the document to assess the matching of the document with the query in particular we have to consider the throne frequency to count of a term in the document we have also considered ITS global statistics such as IDF inverse document frequency but we have not considered a document lance
#c1	this lecture;the document length normalization;the vector space model;we;the discussion;the vectors;model;we;the issue;the length normalization;the lectures;the vector space model;we;the various signals;the document;the matching;the document;the query;we;the throne frequency;a term;the document;we;ITS global statistics;IDF inverse document frequency;we;a document lance
#s2	so
#s3	yeah i show two example documents T four is much shorter with only one hundred words these six on the other hand has a five thousand words if you look at the matching of these query words we see that in the six there are more matchings of the query words but one might reason that the six they have matched these query words in scattered manner
#c3	i;two example documents;T;only one hundred words;the other hand;a five thousand words;you;the matching;these query words;we;more matchings;the query words;one;they;these query words;scattered manner
#s4	so maybe the topic of the six is not really about the topic of the query so the discussion of campaigning at the beginning of the document may have nothing to do with the managing of presidential at the end in general if you think about long documents they would have a higher chance to match any query in fact if you generate a long document that randomly by it simply sampling words from the distribution of words then eventually you probably will match any query so in this sense we should penalize non documents because they just naturally have better chances for match any query and this is the idea of document lancer imitation we also need to be careful in avoiding to over penalize non documents on the one hand and want to penalize a long document but on the other hand we also don't want to over penalize them now the reason is be cause a document that may belong becaus of different reasons in one case the document may be long because it uses more words so for example think about four thirds article of a research paper it would use more words then the corresponding abstract so this is a case where we probably should penalize the matching of long document such as a four paper when we compare the matching of words in such a long document that with matching of the words in a short abstract then long papers generally have a higher chance of matching queer awards therefore we should penalize them however there is another case when the document is long and that is when the document is simply has more content now consider another case of a long document where we simply concatenate a lot of abstracts of different papers in such a case obviously we don't want to over penalize such a long document indeed we probably don't want to paralyze such a document becaus it's long so that's why we need to be careful about using the right degree of generalization a method that has been working well based on research results is called a pivotal length normalization and in this case the idea is to use the average document lens as a pivot as a reference point that means we'll assume that for the average lens documents the score is about right so the normalizer would be one but if the document is longer than the average document lens then there will be some parallelization whereas if it's a shorter then there's even some reward so this is illustrated using this slide on the axis X axis you can see the lens of document on the Y axis we show the normalizer in this case the people that immense normalization formula for the normalizer is is seem to be interpolation of one and the normalized the document lens controller by a parameter be here so you can see here voice first divide the length of the document by the average documents this not only gives us some sense about how this document is compared with the average documents but also gives us benefit of not worrying about the unit of length we can measure the lance by words or by characters anyway this normalizer has an interesting property first we see that if we set the parameter B two zero then the value would be one so there's no length normalization at also be in this sense controls the length normalization whereas if we set me to a non zero value then the normalizer would look like this so the value would be higher for documents that are longer than the average document lines whereas the value of the normalizer would be short away be smaller for shorter documents so in this sense we see there is a parallelization for long documents and there is a reward for short documents the degree of parallelization is controlled by BB cause if we set B to a larger value than the normal as a would look like this there's even more panelization for long documents the more reward for the short documents by adjusting P which varies from zero to one we can control the degree of length normalization so if we plug in this lens normalization factor into the vertical space model ranking functions that we have already examined then we will end up having the following formulas and these are in fact the state of that vector space model formulas so let's let's take a look at each of them the first one is called a pivotal lance normalization back to space model an a reference in the end that has detail about the derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed the idea of component that should be very familiar how to you there is also a query term frequency component here and then in the middle there is the normalizer TF and in this case we see we used a double logarithm as we discussed it before and this is to achieve a sub linear transformation but we also put poking the lens normalizer in the bottom so this would cause canalization for long document because the larger the denominators denominators then smaller than the FAA these and this is of course controller by the parameter be here and you can see again if these two zero then there there is no length normalization
#c4	the topic;the topic;the query;the discussion;campaigning;the beginning;the document;nothing;the managing;the end;you;long documents;they;a higher chance;any query;fact;you;a long document;it;words;the distribution;words;you;any query;this sense;we;non documents;they;better chances;match;any query;the idea;document lancer imitation;we;non documents;the one hand;a long document;the other hand;we;them;the reason;a document;becaus;different reasons;one case;the document;it;more words;example;about four thirds article;a research paper;it;more words;the corresponding abstract;a case;we;the matching;long document;a four paper;we;the matching;words;such a long document;matching;the words;a short abstract;then long papers;a higher chance;awards;we;them;another case;the document;the document;more content;another case;a long document;we;a lot;abstracts;different papers;such a case;we;such a long document;we;such a document becaus;it;we;the right degree;generalization;a method;research results;a pivotal length normalization;this case;the idea;the average document lens;a pivot;a reference point;we;the average lens;the score;the normalizer;the document;the average document lens;some parallelization;it;even some reward;this slide;the axis X axis;you;the lens;document;the Y axis;we;the normalizer;this case;the people;immense normalization formula;the normalizer;interpolation;the document lens controller;a parameter;you;voice;the length;the document;the average documents;us;some sense;this document;the average documents;us;benefit;the unit;length;we;the lance;words;characters;this normalizer;an interesting property;we;we;the parameter B;the value;no length normalization;this sense;the length normalization;we;me;a non zero value;the normalizer;the value;documents;the average document lines;the value;the normalizer;shorter documents;this sense;we;a parallelization;long documents;a reward;short documents;the degree;parallelization;BB cause;we;B;a larger value;even more panelization;long documents;the more reward;the short documents;P;we;the degree;length normalization;we;this lens normalization factor;the vertical space model ranking functions;we;we;the following formulas;fact;the state;that vector space model formulas;'s;'s;a look;them;a pivotal lance normalization;space model;an a reference;the end;detail;the derivation;this model;we;it;the TF IDF weighting model;we;the idea;component;you;a query term frequency component;the middle;the normalizer TF;this case;we;we;a double logarithm;we;it;a sub linear transformation;we;the lens normalizer;the bottom;canalization;long document;the FAA;course;controller;the parameter;you;no length normalization
#s5	OK so this is one of the most effective about this base model formulas the next one called a PM twenty five or copy is also similar in that it also has a idea of component here and a query TF component here but in the middle the normalization is a little bit different as we explained there is this all copy TF transformation here and that the sub linear transformation with the upper bound in this case we have put the length normalization factor here we are adjusting K
#c5	this base model;a PM;copy;it;a idea;component;the middle;the normalization;we;TF transformation;this case;we;the length normalization factor;we;K
#s6	but it achieves similar factor just becaus we put a normalizer in the denominator therefore again if a document is longer than that i'm way that with this model so you can see after we have gone through all the analysis that we talked about and we have in the end reached basically the state of the art which is all functions so so far we have talked about the main eh how to place the document vector in the vector space and this has played an important role in determining the effectiveness of the retrieval function but there are also other dimensions where we did not really examine in detail for example can we further improve the instantiation of the dimension of the vector space model now we've just assumed the bag of words representation so each dimension is the war but obviously we can consider many other choices for example stem rewards those are the words that are having transformed into the same root form so that the computation and computing will all become the same and they can be matched we can do stop water removal this is remove some very common words that don't carry any content and i get the off we can use the phrase is to define dimensions we can even use latent semantic analysis will find some clusters of words that represent a lady in the concept as one by an engine we can also use smaller units like a character engrams those are sequences of N characters for dimensions however in practice people have found that the bag of words representation with the phrase is is there the most effective one
#c6	it;similar factor;we;a normalizer;the denominator;a document;i;way;this model;you;we;all the analysis;we;we;the end;basically the state;the art;all functions;we;the document vector;the vector space;an important role;the effectiveness;the retrieval function;other dimensions;we;detail;example;we;the instantiation;the dimension;the vector space model;we;the bag;words;each dimension;the war;we;many other choices;example;stem rewards;the words;the same root form;the computation;computing;they;we;water removal;some very common words;any content;i;the off;we;the phrase;dimensions;we;latent semantic analysis;some clusters;words;a lady;the concept;an engine;we;smaller units;a character engrams;sequences;N characters;dimensions;practice;people;the bag;words;the phrase
#s7	and it's also efficient so this is still so far the most popular dimension instantiation method
#c7	it;the most popular dimension instantiation method
#s8	and it's used in all the major search engines i should also mention that sometimes we need to do languages specifically and domain specific that organization and this is actually very important as we might have variations of terms that might prevent us from matching them with each other even though they mean the same thing in some of them which is like chinese there is also the challenge in segmenting text to obtain word boundaries be cause it's just a sequence of characters award might correspond to one character or two characters or even three characters
#c8	it;all the major search engines;i;we;languages;that organization;we;variations;terms;us;them;they;the same thing;them;chinese;the challenge;segmenting text;word boundaries;it;just a sequence;characters award;one character;two characters;even three characters
#s9	so it's easier in english when we have a space with separate the words but in some other languages we may need to do some natural language processing to figure out where are the boundaries for words there is also a possibility to improve the similarity function and so far we have used the dot product but one can imagine there are other measures for example we can measure the cosine of the angle between two vectors or we can use euclidean distance measure and these are all possible but dot product the sims is still the best and one reason is be cause it's very general in fact it's sufficient in general if you consider the possibilities of doing waiting in different ways so for example cosine measure can be regarded as the thought product of two normalized vectors that means we first normalize each vector
#c9	it;english;we;a space;separate the words;some other languages;we;some natural language processing;the boundaries;words;a possibility;the similarity function;we;the dot product;one;other measures;example;we;the cosine;the angle;two vectors;we;euclidean distance measure;dot product;the sims;the best and one reason;it;fact;it;you;the possibilities;different ways;example;cosine measure;the thought product;two normalized vectors;we;each vector
#s10	and then we take the dot product that would be equivalent to the cosine measure i just mentioned that the PM twenty five seems to be one of the most effective formulas but there has been also further development in improving PM twenty five although none of these works have changed the VM qualified fundamentally so in one line work people have derived the PM twenty five F here have stands for field and this is to use VM twenty five and four documents with the structures so for example you might consider title field the abstract or body of the research article or even anchor text on the web pages those are the text fields that describer links to other pages and these can all be combined with a proper weights on different fields to help improve scoring for document when we use VM twenty five for such a document and the obvious choices will apply BM twenty five for each field and then combine the scores basically the idea of BM twenty five F is to first combine the frequency counts of terms in all the fields and then apply BM twenty five now this has advantage of avoiding over counting the first occurrence of the time remember in the sub linear transformation of TF the first occurrence is very important than contributes a large weight and if we do that for all the fields then the same term might have gained a lot of advantage in every field
#c10	we;the dot product;the cosine measure;i;the PM;the most effective formulas;further development;PM;none;these works;the VM;one line work people;the PM twenty five F;field;VM twenty five and four documents;the structures;example;you;title field;the research article;the web pages;the text fields;links;other pages;a proper weights;different fields;scoring;document;we;VM;such a document;BM;each field;the scores;basically the idea;BM twenty five F;the frequency counts;terms;all the fields;BM;advantage;the first occurrence;the time;the sub linear transformation;TF;the first occurrence;a large weight;we;all the fields;the same term;a lot;advantage;every field
#s11	but when we combine these word frequencies together we just do the transformation one time at that time then the extra occurrences will not be counted as a fresh first recurrences and this method has been working very well for scoring structure the documents the other line of exchanging is called a BM twenty five plus in this line riches have addressed to the problem of overturn azatian of non documents by BM twenty five so to address this problem the fix is after a quite simple we're going to add a small constant to the TF normalization formula
#c11	we;these word frequencies;we;the transformation;that time;the extra occurrences;a fresh first recurrences;this method;scoring structure;the other line;a BM;this line riches;the problem;non documents;BM;this problem;the fix;we;a small constant;the TF normalization formula
#s12	but what's interesting is that we can analytically prove that by doing such a small modification we will fix the problem of over canonization of long documents by the original BM twenty five so the new formula called BM twenty five plus is empirically anna nicholas soon to be petted VM twenty five so to summarize all what we have said about the vector space model here are the made you take away points first in such a model we use the similarity notion relevance assuming that the relevance of a document with respect to a query is basically proportional to the similarity between the query and document so naturally that implies that the query an document must be represented in the same way and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general and we generally need to use a lot of heuristics to design a ranking function we use some examples will show the need for several heuristics including TF wedding and transformation an IDF weighting and document and storm addition these major heuristics are the most important heuristics to ensure such a general ranking function to work for all kinds of text and finally BM twenty five and pivoted normalization seems to be the most effective formula 's out of them act space model
#c12	what;we;such a small modification;we;the problem;canonization;long documents;the original BM;the new formula;BM;anna nicholas;VM;what;we;the vector space model;you;points;such a model;we;the similarity notion relevance;the relevance;a document;respect;a query;the similarity;the query;document;the query;an document;the same way;this case;we;them;vectors;high dimensional vector space;the dimensions;words;concepts;terms;we;a lot;heuristics;a ranking function;we;some examples;the need;several heuristics;TF wedding;an IDF weighting;document;storm;these major heuristics;the most important heuristics;such a general ranking function;all kinds;text;the most effective formula;them;space model
#s13	now i have to say that i've put VM twenty five in the category of vector space model
#c13	i;i;VM;the category;vector space model
#s14	but in fact the VM twenty five has been derived using probabilistic modeling so the reason why i've put it in the vector space model is first the ranking function actually has a nice interpretation in the vectors
#c14	fact;probabilistic modeling;the reason;i;it;the vector space model;the ranking function;a nice interpretation;the vectors
#s15	based model we can easy to see it looks very much like a map of the space model with a special waiting function the second reason is because the original VM twenty five has a somewhat different form of IDF and that form of IDF actually doesn't really work so well as the standard IDF that you have seen here so as a effective retrieval function VM twenty five should probably use heuristic modification of the IDF to make it even more look like a baptist based model there are some additional readings the first is a paper about the pivotal length normalization it's excellent example of using empirical data analysis to suggest the need of all men summarization and then further derived length normalization formula the second there is the original paper where BM twenty five was proposed the third paper has a thorough discussion of BM twenty five
#c15	based model;we;it;a map;the space model;a special waiting function;the second reason;the original VM;a somewhat different form;IDF;that form;IDF;the standard IDF;you;a effective retrieval function;heuristic modification;the IDF;it;a baptist based model;some additional readings;a paper;the pivotal length normalization;it;excellent example;empirical data analysis;the need;all men summarization;length normalization formula;the original paper;BM;the third paper;a thorough discussion;BM
#s16	and it's exchanging 's particularly BM twenty five F and finally the last paper has a discussion of improving PM twenty five to correct the over penetration of long documents
#c16	it;particularly BM twenty five F;finally the last paper;a discussion;the over penetration;long documents
410	0d005f7e-0c3f-465b-b7fe-08f45355e3de	108
#s1	This lecture is about the Word Association mining and analysis.
#c1	This lecture;the Word Association mining;analysis
#s2	In this lecture we're going to talk about how to mine associations of words from text.
#c2	this lecture;we;mine associations;words;text
#s3	This is an example of knowledge about natural language that we can mine from text data.
#c3	an example;knowledge;natural language;we;text data
#s4	Here's the outline.
#c4	the outline
#s5	We are gooing to first talk about what is word Association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations.
#c5	We;what;word Association;such relations;we;some general ideas;word associations
#s6	In general there are two word relations, and these are quite basic.
#c6	two word relations
#s7	One is called a paradigmatic relation, the other is syntagmatic relations.
#c7	a paradigmatic relation;syntagmatic relations
#s8	A&B have paradigmatic relation if they can be substituted for each other.
#c8	A&B;paradigmatic relation;they
#s9	That means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the other without affecting the understanding of the sentence.
#c9	the two words;paradigmatic relation;the same semantic class or syntactic class;we;the understanding;the sentence
#s10	That means we would still have a valid sentence.
#c10	we;a valid sentence
#s11	For example, cat and dog.
#c11	example;cat;dog
#s12	And these two words have paradigmatic relation because they are in the same class of animal.
#c12	these two words;paradigmatic relation;they;the same class;animal
#s13	And in general, if we replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of.
#c13	we;cat;dog;a sentence;the sentence;a valid sentence;you;sense
#s14	Similarly, Monday and Tuesday have paradigmatic relation.
#c14	Monday;Tuesday;paradigmatic relation
#s15	The second kind of relation is called syntagmatic relation.
#c15	The second kind;relation;syntagmatic relation
#s16	In this case, the two words that have this relation can be combined with each other.
#c16	this case;the two words;this relation
#s17	So A&B have syntagmatic relation If they can be combined with each other in a sentence.
#c17	A&B;syntagmatic relation;they;a sentence
#s18	That means these two words are semantically related.
#c18	these two words
#s19	So for example, Cat and sit are related because a cat can sit somewhere.
#c19	example;Cat;a cat
#s20	Similarly, car and drive are related semantically, and they can be combined with each other to convey meaning.
#c20	car;drive;they;meaning
#s21	However, in general we cannot replace cat with sit in a sentence or car with drive in a sentence to still get a valid sentence.
#c21	we;cat;sit;a sentence;car;drive;a sentence;a valid sentence
#s22	Meaning that if we do that, the sentence will become somewhat meaningless.
#c22	we;the sentence
#s23	So this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences.
#c23	paradigmatic relation;these two relations;fact;they;basic relations;units;arbitrary sequences
#s24	And definitely they can be generalized to describe relations of any items in the language.
#c24	they;relations;any items;the language
#s25	So A&B don't have to be words and they can be phrases example.
#c25	A&B;words;they;phrases;example
#s26	And they can even be more complex phrases than just a noun phrase.
#c26	they;more complex phrases;just a noun phrase
#s27	If you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general.
#c27	you;the general problem;the sequence mining;we;the units;the sequence data;we;paradigmatic relation;relations;units;similar locations;a sentence;a sequence;data elements
#s28	So they occur in similar locations relative to the neighbors in the sequence.
#c28	they;similar locations;the neighbors;the sequence
#s29	Syntagmatic relation on the other hand, is related to co-occurring elements that tend to show up in the same sequence.
#c29	Syntagmatic relation;the other hand;co-occurring elements;the same sequence
#s30	So these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data.
#c30	basically relations;words;we;them;text data
#s31	Discovering such world relations has many applications.
#c31	such world relations;many applications
#s32	First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about the language.
#c32	such relations;accuracy;many NLP tasks;part;our knowledge;the language
#s33	So if you know these two words or synonyms, for example, and then you can help a lot of tasks.
#c33	you;these two words;synonyms;example;you;a lot;tasks
#s34	And grammar learning can be also done by using such techniques because If we can learn paradigmatic relations, then we form classes of words.
#c34	grammar learning;such techniques;we;paradigmatic relations;we;classes;words
#s35	Syntactic classes for example.
#c35	Syntactic classes;example
#s36	And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions.
#c36	we;syntagmatic relations;we;the rules;a larger expression;component expressions
#s37	So we'll learn the structure and what can go with what else.
#c37	we;the structure;what;what
#s38	Word relations can be also very useful for many applications in text retrieval and mining.
#c38	Word relations;many applications;text retrieval;mining
#s39	For example, in search in text retrieval we can use word associations to modify a query.
#c39	example;search;text retrieval;we;word associations;a query
#s40	And this can be used to introduce additional related words to a query to make the query more effective.
#c40	additional related words;a query;the query
#s41	It's often called query expansion.
#c41	It;query expansion
#s42	Or you can use related words to suggest related queries to the user to explore the information space.
#c42	you;related words;related queries;the user;the information space
#s43	Another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge.
#c43	Another application;word associations;the topic map;we;words;nodes;associations;edge
#s44	The user could navigate from one word to another to find information in the information space.
#c44	The user;one word;information;the information space
#s45	Finally, such word associations can also be used to compare and summarize opinions.
#c45	such word associations;opinions
#s46	For example, we might be interested in understanding positive and negative opinions about iPhone 6.
#c46	example;we;positive and negative opinions;iPhone
#s47	In order to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews.
#c47	order;we;what words;a feature word;the battery;negative reviews
#s48	Such a syntagmatic relations would help us show the detailed opinions about the product.
#c48	Such a syntagmatic relations;us;the detailed opinions;the product
#s49	So how can we discover such associations automatically?
#c49	we;such associations
#s50	Now, here are some intuitions about how to do that.
#c50	some intuitions
#s51	Let's first look at the paradigmatic relation.
#c51	's;the paradigmatic relation
#s52	Here we essentially can take advantage of similar context.
#c52	we;advantage;similar context
#s53	So here you see some simple sentences about cat and dog.
#c53	you;some simple sentences;cat;dog
#s54	You can see they generally occur in similar context.
#c54	You;they;similar context
#s55	And that, after all, is the definition of paradigmatic relation.
#c55	the definition;paradigmatic relation
#s56	So on the right side you can see I extracted explicitly the context of cat and dog from this small sample of text data.
#c56	the right side;you;I;the context;cat;dog;this small sample;text data
#s57	So I have taken away cat and dog from the corresponding sentences so that you can see just the context.
#c57	I;cat;dog;the corresponding sentences;you;just the context
#s58	Now of course we can have different perspectives to look at the context.
#c58	course;we;different perspectives;the context
#s59	For example, we can look at the what words occur in the left part of this context.
#c59	example;we;what words;the left part;this context
#s60	So we can call this left context.
#c60	we;this left context
#s61	What words occur before we see cat, cat or dog.
#c61	What words;we;cat;cat;dog
#s62	So you can see in this case clearly dog and cat have similar left context.
#c62	you;this case;dog;cat;similar left context
#s63	You generally say his cat or my cat, and you say also my dog and his dog.
#c63	You;his cat;my cat;you;my dog;his dog
#s64	So that makes them similar in the left context.
#c64	them;the left context
#s65	Similarly, if you look at the words that occur after cat and dog, which we can call right context and they also very similar in this case, of course it's extreme case where you only see eats and In general you will see many other words.
#c65	you;the words;cat;dog;we;right context;they;this case;course;it;extreme case;you;eats;you;many other words
#s66	Of course that can follow cat and dog.
#c66	cat;dog
#s67	You can also even look at the general context.
#c67	You;the general context
#s68	And that might improve the all words in the sentence or in sentences around this word.
#c68	the all words;the sentence;sentences;this word
#s69	And even in the general context you also see some similarity between the two words.
#c69	the general context;you;some similarity;the two words
#s70	So this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words.
#c70	we;paradigmatic relation;the similarity;context;words
#s71	So for example, if we think about the following questions, how similar are context of cat and context of dog?
#c71	example;we;the following questions;context;cat;context;dog
#s72	In contrast, how similar are context of cat and context of computer?
#c72	contrast;context;cat;context;computer
#s73	Now intuitively, with imagine the context of Cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high.
#c73	the context;Cat;context;dog;the context;cat;context;computer;the first case;the similarity value
#s74	Between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship.
#c74	the context;cat;dog;the similarity;contexts;cat;computer;they;paradigmatic relationship
#s75	And then imagine what words occur after computer.
#c75	what words;computer
#s76	In general they will be very different from what words occur after cat.
#c76	they;what;words;cat
#s77	So this is the basic idea of discovering paradigmatic relation.
#c77	the basic idea;paradigmatic relation
#s78	What about the syntagmatic relation?
#c78	the syntagmatic relation
#s79	Here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation.
#c79	we;we;the correlated occurrences;the definition;syntagmatic relation
#s80	Here you see the same sample of text.
#c80	you;the same sample;text
#s81	But here we are interested in knowing what other words are correlated with the verb eats.
#c81	we;what other words;the verb
#s82	And what words can go with eat?
#c82	what words;eat
#s83	And if you look at the right side of the slide and you will see I've taken away the two words around eats.
#c83	you;the right side;the slide;you;I;the two words;eats
#s84	I've taken away the word to its left and also the world to its right, In each sentence.
#c84	I;the word;also the world;its right;each sentence
#s85	And then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat?
#c85	we;the question;words;the left;eat;what words;the right;eat
#s86	Now thinking about this question would help us discover Syntagmatic relations.
#c86	this question;us;Syntagmatic relations
#s87	Because syntagmatic relation essentially captures such correlations.
#c87	syntagmatic relation;such correlations
#s88	So the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur?
#c88	the important question;syntagmatic relation;eats;what other words
#s89	So the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever you see eat, you tend to see the other words.
#c89	the question;some other words;eats;you;you;the other words
#s90	And if you don't see eat, probably you don't see other words often either.
#c90	you;you;other words
#s91	So this intuition can help us discover syntagmatic relations.
#c91	this intuition;us;syntagmatic relations
#s92	Now again, consider example-
#c92	example-
#s93	How helpful is the occurrence of eats for predicting occurrence of meat?
#c93	the occurrence;eats;occurrence;meat
#s94	knowing whether eats occurs in a sentence would generally help us predict the Whether meat also occurs indeed as if we will see eats occur in a sentence, and that should increase the chance that meat will also occur.
#c94	eats;a sentence;us;meat;we;eats;a sentence;the chance;meat
#s95	In contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text?
#c95	contrast;you;the question;the bottom;occurrence;eats;the occurrence;text
#s96	Because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence.
#c96	eats;text;eats;a sentence;us;text;the sentence
#s97	So this is in contrast to the question about eats and meat.
#c97	contrast;the question;eats;meat
#s98	This also helps explain the intuition behind the methods for discovering syntagmatic relation.
#c98	the intuition;the methods;syntagmatic relation
#s99	Mainly we need to capture the correlation between the occurrences of two words.
#c99	we;the correlation;the occurrences;two words
#s100	So to summarize, the general ideas for discovering word associations are the following.
#c100	the general ideas;word associations;the following
#s101	For paradigmatically relation we represent each word by its context, and then compute the context similarity.
#c101	paradigmatically relation;we;each word;its context;the context similarity
#s102	We are gonna assume the words that have high context similarity to have paradigmatic relation.
#c102	We;the words;high context similarity;paradigmatic relation
#s103	For syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even.
#c103	syntagmatic relation;we;how many times two words;a context;a sentence;paragraph;a document
#s104	And we're going to compare their Co occurrences with their individual occurrences.
#c104	we;their Co occurrences;their individual occurrences
#s105	We're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone.
#c105	We;words;high co;-;occurrences;relatively low individual occurrences;syntagmatic relations;they;they
#s106	Note that the paradigmatic relation and syntagmatic relation, are actually closely related.
#c106	the paradigmatic relation;syntagmatic relation
#s107	In that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations.
#c107	that paradigmatically related words;syntagmatic relation;the same word;they;the same word;we;the discovery;the two relations
#s108	So these general ideas can be implemented in many different ways, and the course won't cover all of them, but we will cover at least some of the methods that effective for discovering these relations.
#c108	these general ideas;many different ways;the course;them;we;the methods;these relations
410	0f263731-f4fe-490a-aec2-188e637bbd19	153
#s1	This lecture is about how we can evaluate a ranked list.
#c1	This lecture;we;a ranked list
#s2	In this lecture we will continue the discussion of evaluation.
#c2	this lecture;we;the discussion;evaluation
#s3	In particular, we're going to look at how we can evaluate the ranked list of results.
#c3	we;we;the ranked list;results
#s4	In the previous lecture we talked about.
#c4	the previous lecture;we
#s5	Precision and Recall.
#c5	Precision;Recall
#s6	These are the two basic measures for quantitatively measuring the performance of search result.
#c6	the two basic measures;the performance;search result
#s7	But as we talked about Ranking before, we framed the tax retrieval problem as a ranking problem.
#c7	we;Ranking;we;the tax retrieval problem;a ranking problem
#s8	So, we also need to evaluate the quality of a ranked list.
#c8	we;the quality;a ranked list
#s9	How can we use precision and recall to evaluate a ranked list?
#c9	we;precision;a ranked list
#s10	Naturally, we have to look at the precision and recall at different cut offs because in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing, right?
#c10	we;the precision;different cut offs;the end;the approximation;relevant documents;a ranked list;the user
#s11	If we assume the user sequentially browses the list of results, the user would stop at some point and that point will determine the set, and then that's the most important cut off that will have to consider when we compute the precision recall without knowing where exactly the user would stop, then we have to consider all the positions where the user could stop.
#c11	we;the user;the list;results;the user;some point;that point;the set;the most important cut;we;the user;we;all the positions;the user
#s12	So let's look at these positions.
#c12	's;these positions
#s13	Look at this slide and then let's look at the what if the user stops at the first document?
#c13	this slide;'s;the user;the first document
#s14	What's the precision and recall at this point?
#c14	What;the precision;this point
#s15	What do you think?
#c15	What;you
#s16	It's easy to see, that this document is relevant, so the Precision is one out of one.
#c16	It;this document;the Precision
#s17	We have got one document and that's relevant.
#c17	We;one document
#s18	What about the recall ?
#c18	the recall
#s19	Note that we assume that there are 10 relevant documents for this query in the collection, so it's one out of 10.
#c19	we;10 relevant documents;this query;the collection;it
#s20	What if the user stops at the second position?
#c20	What;the user;the second position
#s21	Top 2.
#s22	The precision is the same - 100% two out of two in the record 2 out of 10.
#c22	The precision;the same - 100%;the record
#s23	What if the user stops at third position?
#c23	What;the user;third position
#s24	Well, this is interesting because in this case we have not got any additional relevant document.
#c24	this case;we;any additional relevant document
#s25	So the recall doesn't change.
#c25	the recall
#s26	But the precision is lower because we've now got a random number.
#c26	the precision;we;a random number
#s27	So what exactly the Precision?
#c27	So what exactly the Precision
#s28	It's two out of three, right? "
#c28	It
#s29	And recall is the same - 2 out of 10.
#c29	recall
#s30	So when would we see another point where the recall would be different?
#c30	we;another point;the recall
#s31	Now if you look down the list it won't happen until we have seen another relevant document.
#c31	you;the list;it;we;another relevant document
#s32	In this case D5.
#c32	this case D5
#s33	At that point, the recall is increased to three out of 10.
#c33	that point;the recall
#s34	And the precision is a three out of five.
#c34	the precision
#s35	So you can see if we keep doing this we can also get to D8
#c35	you;we;we
#s36	and then we will have a Precision of four out of eight.
#c36	we;a Precision
#s37	Because there are eight documents, and four of them are relevant and the recall is a four out of 10.
#c37	eight documents;them;the recall
#s38	Now when can we get a recall of five out of 10?
#c38	we;a recall
#s39	Well, in this list we don't have it.
#c39	this list;we;it
#s40	So we have to go down on the list.
#c40	we;the list
#s41	We don't know where it is.
#c41	We;it
#s42	But as a convenience, we often assume that the precision is 0.
#c42	a convenience;we;the precision
#s43	The precision is zero at all the other levels of Recall that are beyond the search results.
#c43	The precision;all the other levels;Recall;the search results
#s44	So of course this is a pessimistic assumption.
#c44	course;a pessimistic assumption
#s45	The actual precision would be higher, but we may make this assumption in order to have an easy way to compute another measure called average precision that we will discuss later.
#c45	The actual precision;we;this assumption;order;an easy way;another measure;average precision;we
#s46	Now I should also say now here you see, we make these assumptions that are clearly not accurate.
#c46	I;you;we;these assumptions
#s47	But this is usually OK for the purpose of comparing to text retrieval methods, and this is for the relative comparison, so it's OK if the actual measure or actually actual number deviates a little bit from the true number as soon as the deviation is not biased toward any particular retrieval method and we are OK, we can still accurately tell which method works better, and this is an important point to keep in mind.
#c47	the purpose;retrieval methods;the relative comparison;it;the actual measure;the true number;the deviation;any particular retrieval method;we;we;which method;an important point;mind
#s48	When you compare different algorithms, the keys to avoid any bias toward each method, and as long as you can avoid that, it's OK if you do transformation of these measures in anyway, so you can preserve the order.
#c48	you;different algorithms;any bias;each method;you;it;you;transformation;these measures;you;the order
#s49	OK, so we just talked about that we can get a lot of precision recall numbers at different positions, so now you can imagine we can plot a curve and this just shows on the X axis we show recalls.
#c49	we;we;a lot;precision recall numbers;different positions;you;we;a curve;the X axis;we
#s50	And on the Y axis we show the precision.
#c50	the Y axis;we;the precision
#s51	So the precision levels are marked as 0.1, 0.2, 0.3 and 1.0 .
#c51	the precision levels
#s52	So this is the different levels of recall.
#c52	the different levels;recall
#s53	And the Y axis also has different amounts that for precision.
#c53	the Y axis;different amounts;precision
#s54	So we plotted these precision recall numbers that we have got as points on this picture.
#c54	we;these precision recall numbers;we;points;this picture
#s55	Now we can further link these points to form a curve as you see, we assumed that all the other precision that start high level records to be 0 and that's why they are down here, right, So they are zero in this.
#c55	we;these points;a curve;you;we;all the other precision;high level records;they;they
#s56	The actual curve probably will be something like this, but as we just discussed, it doesn't matter that much for comparing two methods.
#c56	The actual curve;something;we;it;two methods
#s57	'cause this would be an underestimate for all the methods.
#c57	an underestimate;all the methods
#s58	OK, so now that we have this precision recall curve, how can we compare 2 ranked lists right?
#c58	we;this precision recall curve;we;2 ranked lists
#s59	So that means we have to compare two PR curves.
#c59	we;two PR curves
#s60	And here I show 2 cases where the "system A is shown in red, System B is showing blue with crosses.
#c60	I;2 cases;the "system A;red;System B;crosses
#s61	Alright, so which one is better?
#c61	one
#s62	I hope you can see here System A is clearly better.
#c62	I;you;System A
#s63	Why?
#s64	Because for the same level of recall, it's the same level of recall here, and you can see the precision point by system is better than system B, so there's no question.
#c64	the same level;recall;it;the same level;recall;you;the precision point;system;system B;no question
#s65	Indeed, you can imagine what does the curve look like for ideal search system.
#c65	you;what;the curve;ideal search system
#s66	It has to have perfect precision at all the recall points, so it has to be this line.
#c66	It;perfect precision;all the recall points;it;this line
#s67	That would be the ideal system.
#c67	the ideal system
#s68	In general.
#s69	The higher the curve is, the better, right
#c69	the curve
#s70	The problem is that we might see a case like this actually happens often like the two curves across each other.
#c70	The problem;we;a case;the two curves
#s71	Now in this case, which one is better?
#c71	this case;one
#s72	What do you think?
#c72	What;you
#s73	Now this is a real problem that you actually might face.
#c73	a real problem;you
#s74	Suppose you build a search engine and you have old algorithm that's shown here in blue or system B and you have come up with a new idea and you test it and the results are shown in red curve
#c74	you;a search engine;you;old algorithm;system B;you;a new idea;you;it;the results;red curve
#s75	A.
#c75	A.
#s76	Now your question is - is your new method better than the older method?
#c76	your question;your new method;the older method
#s77	Or more practically, do you have to replace the algorithm that you are already using your in your search engine with another new algorithm?
#c77	you;the algorithm;you;your;your search engine;another new algorithm
#s78	So should we use system method A to replace method B?
#c78	we;system method;A;method B
#s79	This is going to be a real decision that you have to make.
#c79	a real decision;you
#s80	If you make the replacement, the search engine would behave like system made here, whereas if you don't do that, It will be like a system
#c80	you;the replacement;the search engine;system;you;It;a system
#s81	B.
#c81	B.
#s82	So what do you do?
#c82	what;you
#s83	Now, if you want to spend more time to think about this, pause the video and it's after a very useful to think about that.
#c83	you;more time;the video;it
#s84	As I said, it's a real decision that you have to make if you are building your own search engine, or if you're working for a company that cares about search.
#c84	I;it;a real decision;you;you;your own search engine;you;a company;search
#s85	Now if you have thought about this for a moment, you might realize that, well in this case it's hard to say there was.
#c85	you;a moment;you;this case;it
#s86	Some users might like system A, some users might like system
#c86	Some users;A;some users;system
#s87	B. What's the difference here?
#c87	What;the difference
#s88	The difference is just that in the low level of recall in this region, system B is better, there's higher precision, but in high recall reading system A is better.
#c88	The difference;the low level;recall;this region;system B;higher precision;high recall reading system;A
#s89	Now, so that also means it depends on whether the user cares about the high recall or low recall, but high Precision.
#c89	it;the user;the high recall;low recall;high Precision
#s90	And imagine if someone is just going to check out what's happening today and you want to find some random in the news.
#c90	someone;what;you;some random;the news
#s91	Which one is better?
#c91	Which one
#s92	What do you think?
#c92	What;you
#s93	In this case, clearly system B is better because the user is unlikely examining a lot of results.
#c93	this case;system B;the user;a lot;results
#s94	The user doesn't care about high recall.
#c94	The user;high recall
#s95	On the other hand, if you think about a case where a user is doing, it's a literature survey, you're starting a problem.
#c95	the other hand;you;a case;a user;it;a literature survey;you;a problem
#s96	You want to find whether your idea has been started before.
#c96	You;your idea
#s97	In that case, you emphasize high recall, so you want to see as many relevant documents as possible.
#c97	that case;you;high recall;you;as many relevant documents
#s98	Therefore, you might favor system A.
#c98	you;system A.
#s99	So that means which one is better actually depends on users, and more precisely user's task.
#c99	one;users;more precisely user's task
#s100	So this means you may not necessarily be able to come up with one number that would accurately depict the performance.
#c100	you;one number;the performance
#s101	You have to look at the overall picture yet as I said, when you have a practical decision to make whether you replace the algorithm with another, then you may have to actually come up with a single number to quantify each method.
#c101	You;the overall picture;I;you;a practical decision;you;the algorithm;you;a single number;each method
#s102	Or when we compare many different methods in research, ideally we have one number to compare them with, so that we can easily make a lot of comparisons.
#c102	we;many different methods;research;we;one number;them;we;a lot;comparisons
#s103	So for all these reasons it's desirable to have one single number to measure that.
#c103	all these reasons;it;one single number
#s104	So how do we do that?
#c104	we
#s105	And that needs a number to summarize a range.
#c105	a number;a range
#s106	So here again, it's the precision recall curve, right.
#c106	it;the precision recall curve
#s107	And one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve.
#c107	one way;this whole ranked list;this whole curve;the area;the curve
#s108	Right, so this is one way to measure that, there are other ways to measure that, but it just turns out that this particular way of measuring it has been very popular and has been used since a long time ago for text retrieval evaluation.
#c108	one way;other ways;it;this particular way;it;text retrieval evaluation
#s109	And this is basically computed in this way, and it's called Average Precision.
#c109	this way;it;Average Precision
#s110	Basically, we're going to take a look at every different recall point.
#c110	we;a look;every different recall point
#s111	And then look after the precision.
#c111	the precision
#s112	So we this is one precision and this is another with different recall.
#c112	we;one precision;different recall
#s113	Now this we don't count this one because the recall level is the same.
#c113	we;this one;the recall level
#s114	An we can do then look at this number and that's the precision at a different recall level, etc.
#c114	we;this number;the precision;a different recall level
#s115	So we have all these, add it up.
#c115	we;it
#s116	These are the provisions that had the different points corresponding to retrieving the first relevant document.
#c116	the provisions;the different points;the first relevant document
#s117	The 2nd
#s118	and then the third, the fourth, etc.
#s119	Now we missed the mini random documents.
#c119	we;the mini random documents
#s120	So in all those cases we just assumed they have zero precisions.
#c120	all those cases;we;they;zero precisions
#s121	And then finally we take the average.
#c121	we;the average
#s122	So we divided by 10 and which is a total number of relevant documents in the collection.
#c122	we;a total number;relevant documents;the collection
#s123	Note that here we are not dividing this sum by 4, which is a number of retrieved relevant documents.
#c123	we;this sum;a number;retrieved relevant documents
#s124	Now imagine if I divide by 4, what would happen?
#c124	I;what
#s125	Now think about this for a moment.
#c125	a moment
#s126	It's a common mistake that people sometimes overlook.
#c126	It;a common mistake;people
#s127	So if we you divide this by 4, it's actually not very good.
#c127	we;you;it
#s128	In fact, you are favoring a system that would retrieve very few rather than documents, as in that case the denominator would be very small, so this would be not a good measure.
#c128	fact;you;a system;that case;the denominator;a good measure
#s129	So note that this dinominator is 10.
#c129	this dinominator
#s130	The total number of relevant documents.
#c130	The total number;relevant documents
#s131	And this will basically compute the area underneath the curve.
#c131	the area;the curve
#s132	And this is the standard method used for evaluating a ranked list.
#c132	the standard method;a ranked list
#s133	Note that it actually combines recall and precision, but first we have precision numbers here.
#c133	it;recall;precision;we;precision numbers
#s134	But second, that we also consider recall because if you miss the many, there will be many zeros here.
#c134	we;you;many zeros
#s135	So it combines precision and recall, and furthermore you can see this measure is sensitive to a small change of a position of a relevant document.
#c135	it;precision;you;this measure;a small change;a position;a relevant document
#s136	Let's say if I move this relevant document up a little bit, now it would increase this average precision, whereas if I move any relevant document down, let's say I move this random document down, then it would decrease the average precision.
#c136	's;I;this relevant document;a little bit;it;this average precision;I;any relevant document;'s;I;this random document;it;the average precision
#s137	So this is very good because it's a very sensitive to the ranking of every relevant document.
#c137	it;the ranking;every relevant document
#s138	It can tell small differences between 2 ranked lists and that's what we want.
#c138	It;small differences;2 ranked lists;what;we
#s139	Sometimes one algorithm only works slightly better than another, and we want to see this difference.
#c139	we;this difference
#s140	In contrast, if we look at the precision at the 10 documents.
#c140	contrast;we;the precision;the 10 documents
#s141	If you look at this whole set.
#c141	you;this whole set
#s142	What's the precision?
#c142	What;the precision
#s143	What do you think?
#c143	What;you
#s144	Well, it's easy to see.
#c144	it
#s145	That's four out of 10, right?
#s146	So that precision is very meaningful because it tells us what user would see.
#c146	that precision;it;us;what;user
#s147	So that's pretty useful, right?
#s148	So it's a meaningful measure from a user's perspective.
#c148	it;a meaningful measure;a user's perspective
#s149	But if we use this measure to compare systems, it wouldn't be good because it wouldn't be sensitive to where these four relevant documents are ranked.
#c149	we;this measure;systems;it;it;these four relevant documents
#s150	if I move them around the precision at 10 is still the same.
#c150	I;them;the precision
#s151	Right, so this is not a good measure for comparing different algorithms.
#c151	a good measure;different algorithms
#s152	In contrast, the average precision is a much better measure.
#c152	contrast;the average precision;a much better measure
#s153	It can tell the difference of different difference in ranked lists in subtle ways.
#c153	It;the difference;different difference;ranked lists;subtle ways
410	0f2bbdfe-4bb9-4dba-a897-3a64efd8aeae	10
#s1	this lecture is about the feedback in catcher retrieval so in this lecture we're going to continue the discussion of text retrieval methods in particular we're going to talk about the feedback in pets are retrieval this is a diagram that shows the retrieval process we can see the user with typing the query and then the query would be sent to a retrieval engine or search engine and the engine will return results these results will be shown to the user now after the user has seen these results the user can actually make judgments so for example the user had to say well this is good and this document is not very useful this is a good again etc now this is called a relevance judgment or relevance feedback becaus we've got some feedback information from the user based on the judgments this can be very useful to the system remember what exactly is interesting to the user so the feedback module would then take this as input and also use the document collection through try to improve ranking typically would involve updating the query so the system can now rank the results more accurately for the user
#c1	this lecture;the feedback;catcher retrieval;this lecture;we;the discussion;text retrieval methods;we;the feedback;pets;retrieval;a diagram;the retrieval process;we;the user;the query;the query;a retrieval engine;search engine;the engine;results;these results;the user;the user;these results;the user;judgments;example;the user;this document;a relevance judgment;relevance feedback becaus;we;some feedback information;the user;the judgments;the system;what;the user;the feedback module;input;the document collection;the query;the system;the results;the user
#s2	so this is all relevance feedback the feedback is based on relevance judgments made by the users now these judgments are reliable for the users generally don't want to make extra fault unless they have to so the downsides that it involves some extra effort by the user there is another form of feet about called a pseudo relevance feedback or blind feedback also called automatically back in this case you can see once the user has gotten without all in fact that we don't have to involve users so you can see there's no user involved here
#c2	all relevance feedback;the feedback;relevance judgments;the users;these judgments;the users;extra fault;they;the downsides;it;some extra effort;the user;another form;feet;a pseudo relevance feedback;blind feedback;this case;you;the user;fact;we;users;you;no user
#s3	and we simply assume that the top ranked documents to be relevant let's say we had assumed top ten as well
#c3	we;the top;documents;'s;we
#s4	and then we would then use these assume the documents to learn and to improve the query now you might wonder how could this help if we simply assume the top ranked documents will be random well you can imagine these top rank the documents actually similar to relevant documents even if they are non random they look like relevant documents
#c4	we;the documents;the query;you;we;the top ranked documents;you;these top rank;the documents;relevant documents;they;they;relevant documents
#s5	so it's possible to learn some related terms to the query from this set in fact that you may recall that we talked about using language model to analyze water association to learn related words to the world computer right
#c5	it;some related terms;the query;this set;fact;you;we;language model;water association;related words;the world computer
#s6	and then what we did is your first we use computer to retrieve all the documents that contain computer so imagine now the query here is computer and then the results will be those documents that contain computer and what we can do then is to take the top end results they can match computer very well and we're going to count the terms in this set and then welcome to use the background language model to choose the terms that are frequently in this set but not frequently in the whole collection
#c6	what;we;we;computer;all the documents;computer;the query;computer;the results;those documents;computer;what;we;the top end results;they;computer;we;the terms;this set;the background language model;the terms;this set;the whole collection
#s7	so if we make a contrast between these two what we can find is that we will learn some religious terms to the water computer as we have seen before and these will related was can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software so this was the effective for improving the search result
#c7	we;a contrast;what;we;we;some religious terms;the water computer;we;the original query;the query;us;documents;computer;other words;a program;software;the search result
#s8	but of course pseudorandom asleep back is completely unreliable we have to arbitrary set a kind of so there's also something in between called implicit feedback in this case what we do is we do involve users but we don't have to ask users to make judgments instead we're going to observe how the user interacts with search results so in this case we are going to look at the pixels so the user clicked on this one and the user view with this one and the user skip this one and the user view of this one again now this also is a claw about whether a document is useful to the user and we can even assume that we're going to use only the slip it here in this document that text that's actually seen by the user is there of the actual document them of this entry by the link there let's stay in web search maybe broken
#c8	course;we;a kind;something;called implicit feedback;this case;what;we;we;users;we;users;judgments;we;the user;search results;this case;we;the pixels;the user;this one;the user;this one;the user view;this one;a claw;a document;the user;we;we;only the slip;it;this document;that text;the user;the actual document;them;this entry;the link;'s;web search
#s9	but now it doesn't matter if the user tried to fetch this document becaus of the display the text we consume these display the text is probably relevant is interesting to use our code so we can learn from such information
#c9	it;the user;this document becaus;the display;the text;we;these display;the text;our code;we;such information
#s10	and this is called implicit feedback and we can again use the information to update the query this is a very important technique using more than search engines you think about the global and being and they can collect a lot of user activities why they're serving us so they would observe what documents we click on what documents will scape and this information is very valuable and they can use this to improve the search engine so to summarize we talked about the three kinds of feedback here relevance feedback where the user makes explicit judgments it takes some user effort but the judgment information is reliable we talked about the pseudo feedback where we simply assume top ranked documents to be red and we don't have to involve the user therefore we could do that actually before we return the results to the user and the third is in crystal feedback where we use click throws well we don't we involve users but the user doesn't have to make explicit the effort to make judgment
#c10	implicit feedback;we;the information;the query;a very important technique;search engines;you;they;a lot;user activities;they;us;they;what documents;we;what documents;this information;they;the search engine;we;the three kinds;feedback;relevance feedback;the user;explicit judgments;it;some user effort;the judgment information;we;the pseudo feedback;we;documents;we;the user;we;we;the results;the user;crystal feedback;we;click;we;we;users;the user;the effort;judgment
410	11ef1861-7f1b-410f-81ba-f8ea7ccbb172	9
#s1	in this lecture we're going to talk about the text access in the previous lecture we talked about natural language.. of content analysis we explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner as a result bag of words representation remains very popular in applications like a search engines in this lecture we're going to talk about some high level strategies to help users get access to the text data this is also important step to convert roar big text data into small relevant data that are actually needed in a specific application so the main question would rest here is how kind of text information system help users get access through the relevant the text data we're going to cover two complementary strategies push versus pull
#c1	this lecture;we;the text access;the previous lecture;we;natural language;content analysis;we;the state;the art natural language processing techniques;a lot;unrestricted text data;a robust manner;a result bag;words;representation;applications;a search engines;this lecture;we;some high level strategies;users;access;the text data;important step;roar big text data;small relevant data;a specific application;the main question;how kind;text information system;users;access;the text data;we;two complementary strategies
#s2	and then we're going to talk about two ways to implement the poor mode querying versus browsing so first push versus pull these are two different ways to connect users with the right information at the right time the difference is which takes the initiative which part it takes the initiative in the poor mode the users would take the initiative to start the information access process and in this case a user typically would use a search engine to fulfill the goal for example the user may type in the query and then browser results two find the relevant information so this is usually a property for satisfying users eric information need an airplane information needed is temporary information need for example you want to buy a product so you suddenly have a need to read reviews about related products but after you have collected information have purchase your product you general no longer need such information
#c2	we;two ways;the poor mode;so first push;two different ways;users;the right information;the right time;the difference;the initiative;which part;it;the initiative;the poor mode;the users;the initiative;the information access process;this case;a user;a search engine;the goal;example;the user;the query;the relevant information;a property;users;eric information;an airplane information;temporary information need;example;you;a product;you;a need;reviews;related products;you;information;your product;you;such information
#s3	so it's a temporary information need in such a case it's very hard for system to predict your need an it's more property for the users to take the initiative
#c3	it;a temporary information;such a case;it;system;your need;it;more property;the users;the initiative
#s4	and that's why the search engines are very useful today be cause many people have many airport information needs all the time so as we're speaking google properties processing many queries from this and those are all or mostly out of information needs so this is a poor mode in contrast in the push mode in the system will take the initiative to push the information to the user or to recommend any information to the user so in this case this is usually supported by a recommender system now this would be a property if the user has a stable information mean for example you may have a research interest in some topic and that interest tends to stay for awhile
#c4	the search engines;many people;many airport information;all the time;we;google properties;many queries;information;a poor mode;contrast;the push mode;the system;the initiative;the information;the user;any information;the user;this case;a recommender system;a property;the user;a stable information;example;you;a research interest;some topic;interest
#s5	so it's relatively stable your hobby is another example of a stable information need in such a case the system can interact with you and can learn your interest and then can monitor the information stream if the system has seen any rather than the items to interest the system could then take the initiative direct meant information to you so for example a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you this mode of information access maybe also a property when the system has good knowledge about the users need and this happens in the search context so for example when you search for information on the web a search engine might infer you might be also interesting some related information and they would recommend information to you so that should remind you for example advertisement placed on search page so this is about the two high level strategies or two modes of texter abscess now let's look at the poor mode in more detail in the poor mold we have further distinguishing two ways to help users querying versus browsing in querying a user would just enter a query typical the keyword query and search engine system would return relevant documents to users and this works well when the user knows what exactly are the key words to be used so if you know exactly what you're looking for you tend to know the right keywords and then querying would work very well and we do that all the time but we also know that sometimes it doesn't work so well when you don't know the right keywords using the query or you want to browse information in some topic area in this case browsing would be more useful so in this case in the case of browsing the users would simply navigate into the relevant information by following the paths supported by the structures documents so the system would maintain some kind of structures and then the user could follow these structures to navigate
#c5	it;your hobby;another example;a stable information;such a case;the system;you;your interest;the information stream;the system;the items;the system;the initiative;direct meant information;you;example;a news filter;news recommender system;the news stream;interesting news;you;the news articles;you;this mode;information access;a property;the system;good knowledge;the users;the search context;example;you;information;the web;a search engine;you;some related information;they;information;you;you;example advertisement;search page;the two high level strategies;two modes;texter;'s;the poor mode;more detail;the poor mold;we;two ways;users;a user;a query;the keyword query and search engine system;relevant documents;users;the user;what;the key words;you;exactly what;you;you;the right keywords;we;we;it;you;the right keywords;the query;you;information;some topic area;this case;this case;the case;the users;the relevant information;the paths;the structures;documents;the system;some kind;structures;the user;these structures
#s6	so this is really it works well when the user wants to explore information space or the user doesn't know what are the keywords to use in the query or simply becaus the user finds it inconvenient to type in the query
#c6	it;the user;information space;the user;what;the keywords;the query;the user;it;the query
#s7	so even if the user knows what query to typing if the user is using a cell phone to search for information then it's still hard to enter the query in such a case again browsing tends to be more convenient the relationship between browsing and the query is vessel in the store by making analogy to the site singing
#c7	the user;what query;the user;a cell phone;information;it;the query;such a case;the relationship;the query;vessel;the store;analogy;the site singing
#s8	imagine if you are touring the city now if you know the exact address of attraction then taking a text there is perhaps the fastest way you can go directly to the site but if you don't know the exact address you may need to work around or you can take a text to a nearby place and then work around it turns out that we do exactly the same in the information space if you know exactly what you're looking for then you can use the right keywords in your query to finally implementing directory that's usually the fastest way to do final information but what if you don't know the exact keywords to use well your query probably won't work so well you amend on some related pages
#c8	you;the city;you;the exact address;attraction;a text;the fastest way;you;the site;you;the exact address;you;you;a text;a nearby place;it;we;the information space;you;exactly what;you;you;the right keywords;your query;directory;the fastest way;final information;you;the exact keywords;your query;you;some related pages
#s9	and then you need to also work around in the information space meaning by following the links or by browsing you can then finally getting to the right of the page if you want to learn about soplica again you would likely do a lot of browsing so just like that you are looking around in some area and you want to see some interesting attractions in related in the same region so this analogy also tells us that today we have very good support for query but we don't really have would support for browsing and this is becaus in order to browse effectively we need a map to guide us just like if you need a map of chicago the two of the city of chicago you need atop the map to tour the information space so how to construct the such a topical map is in fact that a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications so to summarize this lecture we've talked about the two high level strategies in for text to access push and pull push tends to be supported by recommended system and pull things will be supported by the search engine of course in the sophisticated in current in the information system we should combine the two in the poor more that we can further disclosure querying and browsing again we generally want to combine the two ways to help users so that you can support both querying and browsing if you want to know more about the relationship between pull and push you can read this article this gives excellent discussion of the relationship between information filtering and information retrieval here information filtering is similar to information recommendation or the push mode of information access
#c9	you;the information space;the links;you;the right;the page;you;soplica;you;a lot;you;some area;you;some interesting attractions;the same region;this analogy;us;we;very good support;query;we;becaus;order;we;a map;us;you;a map;chicago;the city;chicago;you;the map;the information space;the such a topical map;fact;a very interesting research question;us;more interesting browsing experience;the web;other applications;this lecture;we;the two high level strategies;text;push;push;recommended system;things;the search engine;course;current;the information system;we;we;we;the two ways;users;you;you;the relationship;pull;you;this article;excellent discussion;the relationship;information filtering and information retrieval here information filtering;information recommendation;the push mode;information access
410	12ec877b-f300-4841-b334-95cc89e3672a	6
#s1	there are many more the monster learning algorithms then the regression based approaches and they generally attempt to direct their optimizer retrieval machine like a map or entity note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure by maximizing the prediction of one or zero we don't necessarily optimize the ranking of those documents one can imagine that while prediction may not be too bad and it's they both are around the point five
#c1	many more the monster learning algorithms;the regression based approaches;they;their optimizer retrieval machine;a map;entity;note;the optimization objective function;we;the previous slide;retrieval measure;the prediction;we;the ranking;those documents;one;prediction;it;they;the point
#s2	so it's kind of in the middle of zero and one for the two documents
#c2	it;the middle;the two documents
#s3	but the ranking can be wrong so we might have got a larger value for D two
#c3	the ranking;we;a larger value;D
#s4	and then he won so that won't be good from retrieval perspective even though by likelihood function is not bad in contrast we might have another case where we predicted values or around the point nine let's say and by the objective functioning the arrow the larger
#c4	he;retrieval perspective;likelihood function;contrast;we;another case;we;values;the point;'s;the arrow
#s5	but if you can get the order of the two documents correct that's actually a better result so these new more advanced approaches will try to correct that problem of course then the challenges that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end no more about these are proteins now these learning to rank approaches are actually general so they can also be applied to many other ranking problems not just the retrieval problem so here i list assignment for example recommender systems computation or by advertise E or summarization and there are many others that you can probably encounter in your applications to summarize this lecture we have talked about using machine learning to combine multiple features to improve ranking results actually the use of machine learning information retrieval has started since many decades ago so for example of the rock hill feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback but the most reason the use of machine learning has been driven by some changes in the environment of applications of retrieval systems first it's mostly driven by the availability of a lot of training data in the form of click throughs such data want available before so the data can provide a lot of useful knowledge about relevance and machine learning methods can be applied to leverage this
#c5	you;the order;the two documents;a better result;these new more advanced approaches;that problem;course;the challenges;the optimization problem;researchers;many solutions;the problem;you;more reference;the end;proteins;rank approaches;they;many other ranking problems;i;assignment;example recommender systems computation;advertise E;summarization;many others;you;your applications;this lecture;we;multiple features;ranking results;the use;machine learning information retrieval;example;we;machine learning approach;relevance feedback;the most reason;the use;machine learning;some changes;the environment;applications;retrieval systems;it;the availability;a lot;training data;the form;click;throughs;such data;the data;a lot;useful knowledge;relevance;machine learning;methods;leverage
#s6	secondly it's also given by the meter for combining many features and this is not only just be cause there are more features available on the web that can be naturally used to improve scoring it's also be cause by combining them we can improve the robustness of ranking so this is desired for combatting this lamps modern search engines are all used some kind of machine learning techniques combined many features to optimize ranking at this major feature of these commercial engines such as google or bing the topic of learning the rank is still active research topic in the community and so you can expect to see new results being developer in the next few years perhaps here are some additional readings that can give you more information about how learning to rank works and also some advanced methods
#c6	it;the meter;many features;more features;the web;scoring;it;them;we;the robustness;this lamps;modern search engines;some kind;machine learning techniques;many features;ranking;this major feature;these commercial engines;google;the topic;the rank;active research topic;the community;you;new results;developer;the next few years;some additional readings;you;more information;works;also some advanced methods
410	1311ce11-68b0-42bf-82a4-17bc5d316e6f	153
#s1	This lecture is about the inverted index construction.
#c1	This lecture;the inverted index construction
#s2	In this lecture we will continue the discussion of system implementation.
#c2	this lecture;we;the discussion;system implementation
#s3	In particular, we're going to discuss how to construct the inverted index.
#c3	we;the inverted index
#s4	The construction of the inverted index is actually very easy if the data set is very small, it's very easy to construct dictionary and then store the postings in a file.
#c4	The construction;the inverted index;the data;it;dictionary;the postings;a file
#s5	The problem is that when our data is not able to fit to the memory then we have to use some special methods to deal with it.
#c5	The problem;our data;the memory;we;some special methods;it
#s6	Unfortunately, in most retrieval applications, that data set would be large and they generally cannot be loaded into memory at once.
#c6	most retrieval applications;data set;they;memory
#s7	There are many approaches to solve the problem and sorting based method is quite common.
#c7	many approaches;the problem;sorting based method
#s8	It works in four steps as shown here.
#c8	It;four steps
#s9	First, you collect the local term ID, document ID, and frequency tuples.
#c9	you;the local term ID;document ID;frequency tuples
#s10	Basically you will not count the terms in a small set of documents.
#c10	you;the terms;a small set;documents
#s11	And then once you collect those counts, you can sort those counts based on terms so that you build a local partial inverted index, and these are called rounds.
#c11	you;those counts;you;those counts;terms;you;a local partial inverted index;rounds
#s12	And then you write them into a temporary file on the disk, and then you merge in step three would do pair-wise merging of these runs until you eventually merge all the runs to generate a single inverted index.
#c12	you;them;a temporary file;the disk;you;step;pair-wise merging;these runs;you;all the runs;a single inverted index
#s13	So this is an illustration of this method.
#c13	an illustration;this method
#s14	On the left, you see some documents.
#c14	the left;you;some documents
#s15	And on the right, we have shown Term Lexicon and DocumentID Lexicon.
#c15	the right;we;Term Lexicon;DocumentID Lexicon
#s16	These lexicons are to map stream-based representations of document IDs or terms into integer representations or to map back from integers to the stream representation.
#c16	These lexicons;stream-based representations;document IDs;terms;integer representations;integers;the stream representation
#s17	The reason why we are interested in using integers to represent these IDs is because integers are often easier to handle.
#c17	The reason;we;integers;these IDs;integers
#s18	For example, integers can be used as index for array.
#c18	example;integers;index;array
#s19	They are also easy to compress.
#c19	They
#s20	So this is one reason why we tend to map these strings into integers.
#c20	one reason;we;these strings;integers
#s21	So that well, so that we don't have to carry these strings around.
#c21	we;these strings
#s22	So how does this approach work?
#c22	this approach
#s23	Well, it's very simple.
#c23	it
#s24	We're going to scan these documents sequentially, and then parse the document, and count the frequencies of terms.
#c24	We;these documents;the document;the frequencies;terms
#s25	In this stage, we generally sort the frequencies by document IDs because we process each document sequentially.
#c25	this stage;we;the frequencies;document IDs;we;each document
#s26	So we first encounter all the terms in the first document.
#c26	we;all the terms;the first document
#s27	Therefore, the document IDs are all 1s in this case.
#c27	the document IDs;1s;this case
#s28	And this will be followed by document IDs 2s.
#c28	document IDs
#s29	They are naturally sorted in this order just because we process the data in the sequential order at some point we will run out of memory and then we would have to write them into the disk.
#c29	They;this order;we;the data;the sequential order;some point;we;memory;we;them;the disk
#s30	But before we do that, we can sort them just use whatever memory we have.
#c30	we;we;them;whatever memory;we
#s31	We can sort them and then this time we're going to sort based on term IDs.
#c31	We;them;we;term IDs
#s32	Notice that here we're using the term IDs  as a key to sort.
#c32	we;the term IDs;a key
#s33	So all the entries that share the same term would be grouped together.
#c33	all the entries;the same term
#s34	In this case, we can see  IDs of documents that match the term one will be grouped together.
#c34	this case;we;  IDs;documents;the term;one
#s35	And we're going to write this into the disk as a temporary file, and that would allow us to use the memory to process the next batch of documents.
#c35	we;the disk;a temporary file;us;the memory;the next batch;documents
#s36	And we're going to do that for all the documents.
#c36	we;all the documents
#s37	So we are going to write a lot of temporary files into the disk.
#c37	we;a lot;temporary files;the disk
#s38	And then the next stage is merge sort.
#c38	the next stage;merge sort
#s39	We are going to merge them and sort them.
#c39	We;them;them
#s40	Eventually we will get a single inverted index with their entries are sorted based on term IDs.
#c40	we;a single inverted index;their entries;term IDs
#s41	And on the top we can see these are the old entries for the documents that match term ID1.
#c41	the top;we;the old entries;the documents;term ID1
#s42	So this is basically how we can do the construction of inverted index even though the data cannot be  loaded into the memory.
#c42	we;the construction;inverted index;the data;the memory
#s43	Now we mentioned earlier that because of postings are very large, it's desirable to compress them.
#c43	we;postings;it;them
#s44	So let's now talk a little bit about how we compress inverted index.
#c44	's;we;inverted index
#s45	The idea of compression in general is to leverage skewed distributions of values, and we generally have to use variable length encoding instead of the fixed length encoding as we use by default in program languages like C++.
#c45	The idea;compression;skewed distributions;values;we;variable length encoding;the fixed length encoding;we;default;program languages;C++
#s46	So how can we leverage the skewed distributions of values to compress these values?
#c46	we;the skewed distributions;values;these values
#s47	In general, we would use fewer bits to encode those frequently awards at the cost of using longer bits to encode those rare values.
#c47	we;fewer bits;those frequently awards;the cost;longer bits;those rare values
#s48	So in our case, let's think about how we can compress the TF (term frequency).
#c48	our case;'s;we;the TF (term frequency
#s49	Now if you can picture what the inverted index would look like, and you see in postings, there are a lot of term frequencies.
#c49	you;what;the inverted index;you;postings;a lot;term frequencies
#s50	Those are the frequencies of terms in all those documents.
#c50	the frequencies;terms;all those documents
#s51	Now, if you think about what kind of values are most frequent there, you probably will be able to guess that small numbers tend to occur far more frequently than large numbers.
#c51	you;what kind;values;you;small numbers;large numbers
#s52	Why?
#s53	Think about the distribution of words and this is due to the Zipf's law, and many words occur rarely.
#c53	the distribution;words;the Zipf's law;many words
#s54	So we see a lot of small numbers.
#c54	we;a lot;small numbers
#s55	Therefore we can use fewer bits for the small but highly frequent integers  at the cost of using more bits for large integers.
#c55	we;fewer bits;the small but highly frequent integers;the cost;more bits;large integers
#s56	This is a trade off, of course.
#c56	a trade;course
#s57	If the values are distributed uniformly, this one will save us a lot of space.
#c57	the values;this one;us;a lot;space
#s58	But because we tend to see many small values, they are very frequent.
#c58	we;many small values;they
#s59	We can save on average, even though sometimes when we see a large number we have to use a lot of bits.
#c59	We;we;a large number;we;a lot;bits
#s60	What about the document IDs that we also saw in postings?
#c60	the document IDs;we;postings
#s61	They are not distributed in the skewed way right.
#c61	They;the skewed way
#s62	So how can we deal with that?
#c62	we
#s63	Well, it turns out that you can use a trick called d-gap and that is to restore the difference of this term IDs.
#c63	it;you;a trick;d;-;gap;the difference;this term IDs
#s64	And we can imagine if a term has matched many documents, then there will be long list of document IDs.
#c64	we;a term;many documents;long list;document IDs
#s65	So when we take the gap, I'm going to take the difference between adjacent the document IDs.
#c65	we;the gap;I;the difference;the document IDs
#s66	Those gaps will be small, so we will again see a lot of small numbers.
#c66	Those gaps;we;a lot;small numbers
#s67	Whereas if a term according only a few documents, then the gap would be large.
#c67	a term;only a few documents;the gap
#s68	The large numbers will not be frequent.
#c68	The large numbers
#s69	So this creates some skewed distribution that would allow us to  compress these values.
#c69	some skewed distribution;us;these values
#s70	And this is also possible because in order to uncover or uncompress these document IDs.
#c70	order;these document IDs
#s71	We have to sequential process the data.
#c71	We;the data
#s72	Because we store the difference and in order to recover the exact the document ID, we have to 1st recover the previous document IDs and then we can add the difference to the previous document ID to restore the current document ID.
#c72	we;the difference;order;the exact;the document ID;we;1st;the previous document IDs;we;the difference;the previous document ID;the current document ID
#s73	Now this was possible because we only need to have sequential access to those documents IDs.
#c73	we;sequential access;those documents;IDs
#s74	Once we look up a term we fetch all the document IDs that match the term.
#c74	we;a term;we;all the document IDs;the term
#s75	Then we sequentially process them.
#c75	we;them
#s76	So it's very natural.
#c76	it
#s77	That's why this trick works.
#c77	this trick
#s78	And there are many different methods for encoding.
#c78	many different methods;encoding
#s79	For example, the binary code is a commonly used code in programming languages where we use basically fixed length encoding.
#c79	example;the binary code;a commonly used code;programming languages;we;basically fixed length encoding
#s80	Unary code, Gamma code, and Delta code are all possibilities and there are many other possibilities.
#c80	Unary code;Gamma code;Delta code;possibilities;many other possibilities
#s81	So let's look at some of them in more detail.
#c81	's;them;more detail
#s82	Binary coding is really equal length encoding, and that's our property for randomly distributed values.
#c82	Binary coding;really equal length encoding;our property;randomly distributed values
#s83	The unary coding is a variable length encoding method.
#c83	The unary coding;a variable length encoding method
#s84	In this case, an integer that's at least one would be encoded as x-1, one bits followed by zero.
#c84	this case;an integer;at least one;x-1;one bits
#s85	So for example, three would be encoded as two ones, followed by zero, whereas five will be encoded as 4 ones, followed by zero etc.
#c85	example;two ones;4 ones
#s86	So now you can imagine how many bits do we have to use for a large number like 100.
#c86	you;how many bits;we;a large number
#s87	So how many bits do we have to use exactly the full number like 100? "
#c87	how many bits;we;exactly the full number
#s88	" Exactly, we have to use 100 bits, right?
#c88	we;100 bits
#s89	So it's the same number of bits as the value of this number, so this is very inefficient.
#c89	it;the same number;bits;the value;this number
#s90	If you will likely see some large numbers, imagine if you occasionally see a number like 1000.
#c90	you;some large numbers;you;a number
#s91	You have to use 1000 bits, so this only works well if you are absolutely sure that there will be no large numbers, mostly very, very often using very small numbers.
#c91	You;1000 bits;you;no large numbers;very small numbers
#s92	How do you decode this code?
#c92	you;this code
#s93	Since these are variable length encoding methods and you can't just count how many bits, and then they just stop.
#c93	variable length encoding methods;you;how many bits;they
#s94	You can say 8 bits or 32 bits, then you will start another code.
#c94	You;8 bits;32 bits;you;another code
#s95	There are variable lengths, so you'll have to rely on some mechanism.
#c95	variable lengths;you;some mechanism
#s96	In this case, for unary you can see it's very easy to see the boundary.
#c96	this case;you;it;the boundary
#s97	Now you can easily see zero with signal, the end of encoding.
#c97	you;signal;the end;encoding
#s98	So you just count how many ones you have seen and until you hit zero you have finished one number.
#c98	you;how many ones;you;you;you;one number
#s99	You will start another number.
#c99	You;another number
#s100	Now we just saw that the unary coding is too aggressive in rewarding small numbers.
#c100	we;the unary coding;rewarding small numbers
#s101	If you occasionally, you can see a very big number.
#c101	you;you;a very big number
#s102	It would be a disaster.
#c102	It;a disaster
#s103	So what about some other, less aggressive method?
#c103	some other, less aggressive method
#s104	Well, Gamma coding is one of them.
#c104	Gamma coding;them
#s105	And in this method we are going to use unary coding for a transformed form of the value, so it's one plus the floor of the log of X.
#c105	this method;we;unary coding;a transformed form;the value;it;the floor;the log;X.
#s106	So the magnitude of this value is much lower than the original X.
#c106	the magnitude;this value;the original X.
#s107	So that's why we can afford using unary code for that.
#c107	we;unary code
#s108	So we will first have the Unary code for this log of X. This will be followed by a uniform code or binary code, and this is basically the same uniform code and binary code are the same, and we're going to use this code to code the remaining part of the value of X.
#c108	we;the Unary code;this log;X.;a uniform code;binary code;the same uniform code;binary code;we;this code;the remaining part;the value;X.
#s109	And this is basically precise X - 1 to the floor of the log of X.
#c109	-;the floor;the log;X.
#s110	So the unary code basically coded the floor of log of X, add one there.
#c110	the unary code;the floor;log;X
#s111	But the remaining part will be using uniform code to actually code the difference between the X and this  two to the log of X.
#c111	the remaining part;uniform code;the difference;the X;the log;X.
#s112	And it's easy to show that for this value this difference, we only need to use up to this many bits and floor of log of X bits.
#c112	it;this value;we;this many bits;floor;log;X bits
#s113	This is easy to understand, if the difference is too large then we would have a higher floor of log of X. So here are some examples.
#c113	the difference;we;a higher floor;log;X.;some examples
#s114	For example, 3 is encoded as 101.
#c114	example
#s115	The first 2 digits are the unary code, right.
#c115	The first 2 digits;the unary code
#s116	So this is for the value 2.
#c116	the value
#s117	10 encodes 2 in unary coding.
#c117	10 encodes;unary coding
#s118	And so that means log of X, the floor of log of X is 1 because we want actually use unary code to encode one plus the floor of log of X. Since this is 2, then we know that the floor of log of X is actually one.
#c118	log;X;the floor;log;X;we;unary code;the floor;log;X.;we;the floor;log;X
#s119	But three is still larger than two to the one, so the difference is 1, and then one is encoded here at the end.
#c119	the one;the difference;one;the end
#s120	So that's why we have 101 for 3.
#c120	we
#s121	Similarly, 5 is encoded as 110 followed by 01 and in this case, the unary code encodes 3.
#c121	this case
#s122	And so this is the unary code 110.
#c122	the unary code
#s123	And so the floor of log of X is 2.
#c123	the floor;log;X
#s124	And that means we are going to compute the difference between 5 and 2 to the two, and that's one, and so we now have again one at the end.
#c124	we;the difference;we;the end
#s125	But this time we're going to use 2 bits, cause with this level floor of log of X we could have more numbers 5, 6, 7 they would all share the same prefix here,110.
#c125	we;2 bits;this level floor;log;X;we;more numbers;they;the same prefix
#s126	So in order to differentiate them we have to use 2 bits in the end to differentiate them.
#c126	order;them;we;2 bits;the end;them
#s127	So you can imagine six would be 10 here in the end instead of 01 after 110.
#c127	you;the end
#s128	It's also true that the form of gamma code is always: first odd number of bits and in the center in there is a zero.
#c128	It;the form;gamma code;first odd number;bits;the center
#s129	That's the end of the unary code.
#c129	the end;the unary code
#s130	And before that, on the left side of this zero, there will be all ones an on the right side of this zero, it's binary coding or uniform coding.
#c130	the left side;all ones;the right side;it;binary coding
#s131	So how can you decode such a code?
#c131	you;such a code
#s132	You again first do unary coding
#c132	You;unary coding
#s133	right
#s134	Once you hit zero, you have got the unary code.
#c134	you;you;the unary code
#s135	And this also would tell you how many bits you have to read further to decode the uniform code.
#c135	you;how many bits;you;the uniform code
#s136	So this is how you can decode Gamma code.
#c136	you;Gamma code
#s137	There was also the error code that's basically the same as gamma code, except that you replace the unary prefix with the gamma code, so that's even less conservative than gamma code in terms of rewarding the small integers.
#c137	the error code;gamma code;you;the unary prefix;the gamma code;gamma code;terms;the small integers
#s138	So that means it's OK if you occasionally see a large number.
#c138	it;you;a large number
#s139	It's OK with delta code.
#c139	It;delta code
#s140	It's also fine with gamma code, it's really a big loss for Unary code and they are all operating of course at different degrees of favoring small integers.
#c140	It;gamma code;it;a big loss;Unary code;they;course;different degrees;small integers
#s141	And that also means.
#s142	They would be appropriate for a certain distribution, but none of them is perfect for all distributions.
#c142	They;a certain distribution;none;them;all distributions
#s143	Which method works the best with have to depend on the actual distribution in your data set.
#c143	Which method;the actual distribution;your data
#s144	For inverted index compression, people have found that gamma coding seems to work well.
#c144	inverted index compression;people;gamma coding
#s145	So how do I uncompressed invert index when we just talked about this first, you decode those encode integers and we just I think discussed how we decode unary coding and gamma coding.
#c145	I;invert index;we;you;those encode integers;we;I;we;unary coding;gamma coding
#s146	So I won't repeat.
#c146	I
#s147	And what about the document IDs that might be compressed using d-gap.
#c147	the document IDs;d;-;gap
#s148	We're going to do sequential decoding.
#c148	We;sequential decoding
#s149	So suppose the encoded ID list is x1, x2, x3, etc.
#c149	the encoded ID list;x1;x2;x3
#s150	We first decode x1 to obtain the first document ID ID1.
#c150	We;first decode;x1;the first document;ID ID1
#s151	Then we would decode x2, which is actually the difference between the second ID and the first one.
#c151	we;x2;the difference;the second ID
#s152	So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this second position, right.
#c152	we;the decoder value;x2;the value;the ID;this second position
#s153	So this is where you can see the advantage of converting document IDs into integers, and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position.
#c153	you;the advantage;document IDs;integers;us;this kind;compression;we;we;all the documents;we;the document ID;the previous position;you;the document ID;the next position
410	1579f35e-a2d3-4e13-9795-f203c6d41157	1
#s1	this lecture is overview of texture retrieval methods in the previous lecture we introduced to the problem of text retrieval we explained that the main problem is the designer ranking function into ranked documents for a query in this lecture will give overview of different ways of designing this ranking function so the problem is the following we have a query that has a sequence of words and a document that that's also a sequence of words and we hope to define a function F that can compute the score based on the query and document so the main channel here is redesigning a code breaking function that can rank all the relevant documents on top of all the non random ones now clearly this means our function must be able to measure the light hold that a document that he is relevant to a query kill that also means we have to have some way to define relevance in particular in order to implement the program to do that we have to have a computational definition of relevance and we achieve this goal by designing a retrieval model which gives us a formalization of relevance now over many decades researchers have the line of many different kinds of retrieval models and they fall into different categories first one thing many of the models are based on the similarity idea basically we assume that if a document is more similar to the query then another document is then we will say the first document is more relevant than the second one so in this case the ranking function is defined as the similarity between the query and the document one well known example in this case is vector space model which we will cover more in detail later in the lecture the second kind of models are called a probabilistic models in this family of models we follow a very different a strategy where we assume that queries and documents are observations from random variables and we assume there is a binary random variable called are here to indicate whether a document is relevant to a query we then define the score of a document with respect to a query as the probability that this random variable R is equal to one given a particular document and query there are different cases of such a general idea one is classical probability model and others language model yet another is diversions from randomness the model in the letter lecture we will talk more about one case which is language model the third kind of models are based on probability inference so here the idea is do associate and certainly to inference rules and we can then quantify the probability that we can show that the query follows from the document finally there is also a family of models that are using actual medical thinking here the idea is to define a set of constraints that we hope a good retrieval function to satisfy so in this case the problem is with sick a good ranking function that can satisfy all the desire the constraints interested in although these different models are based on different thinking in the end the retrieval function tends to be very similar and these functions tend to also involve similar variables so now let's take a look at the common form of a state of that retrieval model and to examine some of the common ideas used in all these models first these models are all based on the assumption of using bag of words to represent the text and we explained this in the natural language processing laptop bag of wars were intent itching remains the main representation using all the search engines so with this assumption the score of a query like presidential campaign news with respect to a document D here would be based on scores computed based on each individual war and that means the score would depend on the score of each word such as presidential campaign and news here we can see there are three different components each corresponding to how well the document matches each of the query words inside these functions we see a number of heuristics used so for example one factor that effects the function G here is how many times does the word presidential occur in the document this is all the term frequency or TF we might also D load as C of presidential and E in general if the word occurs more frequently in the document then the value of this function would be larger another factor is how long is the document ann this is to use the talking the length for scoring in general if a term occurs in a long document that many times it's not as significant as if it occured the same number of times in the shop document becaus in a long document anytime is expected to occur more frequently finally there is this effect that called a document frequency and that is we also want to look at how often presidential occurs in the entire collection and we call this document frequency or DF of presidential and in some other models we might also use a probability to characterize this information so here i show the probability of presidential in the collection so all these are trying to characterize the popularity of the term in the crash and in general matching array item in the collection is contributing more to the overall score than matching our common term so this captures some of the main ideas using pretty much all the state of larger retrieval models so thou a natural question is which model works the best now it turns out that many models work equally well so here i list to the four major models that are generally regarded as a state of the art retrieval models pivotal and summarization BM twenty five query likely hold P L two when optimizer these models tend to perform similarly and this was discussed in detail in this reference at the end of this laptop among all these BM twenty five is probably the most popular it's most likely that this has been used in virtually order search engines and you will also often see this method discussed in research papers and we'll talk more about this method data in some other actions so to summarize the main points made in the snatch or first the design of a good ranking function pretty requires a computational definition of relevance and we achieve this goal by designing a property retrieval model second the mini models are equally effective but we don't have a single winner yet researchers are still actively working on this problem trying to find the actually optimal mitchell model finally the state of that ranking functions tend to rely on the following ideas first bag of words representation second TF and document frequency of words such information is used in waiting function to determine the overall contribution of matching or world and locking the lens these are often combine are in interesting ways and were discussed how exactly they are combined to rank documents in the lectures later there are two suggested the additional ratings if you have time the first is a paper where you can find a detailed discussion and comparison of multiple state of the art models the second is a book with a chapter that gives a broad review of different retrieval models
#c1	this lecture;overview;texture retrieval methods;the previous lecture;we;the problem;text retrieval;we;the main problem;the designer ranking function;ranked documents;a query;this lecture;overview;different ways;this ranking function;the problem;we;a query;a sequence;words;a document;a sequence;words;we;a function F;the score;the query;document;the main channel;a code breaking function;all the relevant documents;top;all the non random ones;our function;he;a query kill;we;some way;relevance;order;the program;we;a computational definition;relevance;we;this goal;a retrieval model;us;a formalization;relevance;many decades;researchers;the line;many different kinds;retrieval models;they;different categories;the models;the similarity idea;we;a document;the query;another document;we;the first document;this case;the ranking function;the similarity;the query;the document;one well known example;this case;vector space model;we;detail;the lecture;the second kind;models;a probabilistic models;this family;models;we;a very different a strategy;we;queries;documents;observations;random variables;we;a binary random variable;a document;a query;we;the score;a document;respect;a query;the probability;this random variable R;a particular document;query;different cases;such a general idea;classical probability model;diversions;randomness;the model;the letter lecture;we;more about one case;language model;the third kind;models;probability inference;the idea;associate;rules;we;the probability;we;the query;the document;a family;models;actual medical thinking;the idea;a set;constraints;we;a good retrieval function;this case;the problem;sick a good ranking function;all the desire;the constraints;these different models;different thinking;the end;the retrieval function;these functions;similar variables;'s;a look;the common form;a state;that retrieval model;the common ideas;all these models;these models;the assumption;bag;words;the text;we;the natural language processing laptop bag;wars;intent itching;the main representation;all the search engines;this assumption;the score;a query;presidential campaign news;respect;a document;D;scores;each individual war;the score;the score;each word;presidential campaign;news;we;three different components;the document;the query words;these functions;we;a number;heuristics;example;one factor;the function;how many times;the word;the document;all the term frequency;TF;we;D;C;E;the word;the document;the value;this function;another factor;the document ann;the talking the length;scoring;a term;a long document;it;it;the same number;times;the shop document becaus;a long document;this effect;a document frequency;we;how often presidential occurs;the entire collection;we;this document frequency;DF;some other models;we;a probability;this information;i;the probability;the collection;the popularity;the term;the crash;general matching array item;the collection;the overall score;our common term;the main ideas;all the state;larger retrieval models;a natural question;model;it;many models;i;the four major models;a state;the art retrieval models;BM;twenty five query;P L;these models;detail;this reference;the end;this laptop;all these BM;it;virtually order search engines;you;this method;research papers;we;this method data;some other actions;the main points;the snatch;first the design;a good ranking function;a computational definition;relevance;we;this goal;a property retrieval model;the mini models;we;a single winner;researchers;this problem;the actually optimal mitchell model;finally the state;that ranking functions;the following ideas;first bag;words;words;such information;function;the overall contribution;matching;world;the lens;interesting ways;they;documents;the lectures;the additional ratings;you;time;a paper;you;a detailed discussion;comparison;multiple state;the art models;a book;a chapter;a broad review;different retrieval models
410	1be44ca1-c700-426a-a4ea-0710a276278c	9
#s1	this lecture is about the smoothing of language models in this lecture we're going to continue talking about the probabilistic retrieval model in particular we're going to talk about smoothing of language model in the query
#c1	this lecture;the smoothing;language models;this lecture;we;the probabilistic retrieval model;we;smoothing;language model;the query
#s2	it like a whole retrieval method so you have seen this slide from a previous lecture this is the ranking function based on the query like hold here we assume that the independence of generating chick we reward and the formula would look like a following where we take a sum over all the query awards and inside the some there is a log of probability of award given by the document or spoken language model so the main task now is to estimate this document language model as we said before different methods for estimating this model would need two different retrieval functions so in this lecture we're going to look into this in more detail
#c2	it;a whole retrieval method;you;this slide;a previous lecture;the ranking function;the query;hold;we;the independence;generating chick;we;the formula;we;a sum;all the query awards;a log;probability;award;the document;the main task;this document language model;we;different methods;this model;two different retrieval functions;this lecture;we;more detail
#s3	so how do i expect this language model well the obvious choice would be the maximum macular estimated that we have seen before and that is working to normalize the word frequencies in the document and the estimator probability would look like this this is a step function here which means all the words that have the same frequency count will have identical probably there this is another frequently count that as a different probability note that for words that have not occured in the document here they all have zero probability so we know this is just like the model that we assume earlier in a lecture where we assume that the user with the simple word from the document to formulate a query and there's no chance of sampling any word that's not in the document and we know that's not good
#c3	i;this language model;the obvious choice;we;the word frequencies;the document;the estimator probability;a step function;all the words;the same frequency count;words;the document;they;zero probability;we;the model;we;a lecture;we;the user;the simple word;the document;a query;no chance;any word;the document;we
#s4	so how do we improve this well in order to assign a non zero probability to words that have not been observed in the document we would have to take away some probability mass from the words that are observing the document so for example here we have to take away some probably the mass because we need some extra problem in mass for the unseen words otherwise they want some to one so all these probabilities must send one so to make this transformation and to improve the maximum micro resonator by assigning nonzero probabilities two words that are not observed in the data we have to do smoothing and smoothing has to do with improving the estimated by considering the possibility that if the author had been written being asked to write more words for the document the the other might have retain other words if you think about this factor then smooth the language model would be more accurate representation of the actual topic imagine you have seen abstractor over research article let's say this document is abstract right if we assume and see words in this abstract that we have all our probability of zero that with me is no chance of sampling a word outside the abstract at the formula to query but imagine a user who is interested in the topic of this object the user might actually choose a word that's not in the abstract are too to use as a query so obviously if we had asked this author to write more the author would have retained A four text of that article so smoothing of the language model is attempt to try to recover the model for the whole whole article and then of course we don't have really knowledge about any words that are not observed in the abstract are so that's why smoothing is actually tricky problem
#c4	we;this well;order;a non zero probability;words;the document;we;some probability mass;the words;the document;example;we;some probably the mass;we;some extra problem;mass;the unseen words;they;all these probabilities;this transformation;the maximum micro resonator;nonzero probabilities;two words;the data;we;smoothing;smoothing;the possibility;the author;more words;the document;other words;you;this factor;the language model;more accurate representation;the actual topic;you;abstractor;research article;'s;this document;we;words;this abstract;we;all our probability;me;no chance;a word;the abstract;the formula;query;a user;who;the topic;this object;the user;a word;the abstract;a query;we;this author;the author;A four text;that article;the language model;the model;the whole whole article;course;we;really knowledge;any words;the abstract;smoothing;tricky problem
#s5	so let's talk a little more about how this mother language model and the key question here is what probably there should be assigned to those unseen words and there are many different ways of doing that one idea here that's very useful for retrieval is late the probability of unseen world be proportional to its probability given by a reference language model that means if you don't observe the word in the data set we're going to assume that its probability is kind of governed by another reference language model that would work constructed it will tell us which unseen words we have like their higher probability being the case of retrieval a natural choice would be to take the collection language model as the reference language model that is reserved for don't know observe award in the document we're going to assume that the probability of this word would be proportional to the probability of the world in the whole collection so more formally will be estimating the probability of award given a document as follows if the war is seen in the document then the probability would be a discounted the maximum like roller estimated peace obscene otherwise if the war is not seeing the document we're going to let the probability be proportional to the probability of the world in the collection and here the coefficient alpha is to control the amount of probability mass that were assigned to unseen words obviously all these probabilities must sum to one so alpha
#c5	's;how this mother language model;the key question;what;those unseen words;many different ways;that one idea;retrieval;the probability;unseen world;its probability;a reference language model;you;the word;the data;we;its probability;another reference language model;it;us;which unseen words;we;their higher probability;the case;retrieval;a natural choice;the collection language model;the reference language model;observe award;the document;we;the probability;this word;the probability;the world;the whole collection;the probability;award;a document;the war;the document;the probability;a discounted the maximum like roller estimated peace;the war;the document;we;the probability;the probability;the world;the collection;the coefficient alpha;the amount;probability mass;unseen words;all these probabilities;one so alpha
#s6	sub D is constrained in some way so what if we plugging this smoothing formula into our query lighter running function this is what we looked at in this formula you can see we have this as a sum over all the query words and note that we have returned in the form of a sum over all the vocabulary is he here this is the sum of all the words in the vocabulary but not that we have account of the world in the query so in fact we're just taking a sum of query words
#c6	D;some way;we;this smoothing formula;our query lighter running function;what;we;this formula;you;we;a sum;all the query words;we;the form;a sum;all the vocabulary;he;the sum;all the words;the vocabulary;we;account;the world;the query;fact;we;a sum;query words
#s7	right this is now a common way that we will use because of its convenience in some transformations so this is as i said this is some overall the query words in our smoothing method we assume that the words that are not observed in the document we have somewhat different form of probability name it's for this fall so we're going to then decompose this some into two parts one some is over all the query words that are matching the document that means in this sum or the words have a non zero probability in the document
#c7	a common way;we;its convenience;some transformations;i;the query words;our smoothing method;we;the words;the document;we;somewhat different form;probability name;it;this fall;we;two parts;one;all the query words;the document;this sum;the words;a non zero probability;the document
#s8	sorry it's now zero count of the world in the document they all occur in the document and they also have to of course have a non zero count in the query so these are the words that are matched these are the query words that are matching the document on the other hand this is some we are taking a sum over all the words that are not all query was not are not matched in the document so they occur in the query you do this term but they don't occur in the document in this case these words have this probability becaus of our assumption about the smoothie but that here these seem was have a different probability now we can go further by rewriting the second some as a difference of two other signs basically the first sum is actually some overall the query words now we know that the original sum is not over all the query words this is over order query was that are not matched in the document so here we pretend that they are actually over all the query it words
#c8	it;zero count;the world;the document;they;the document;they;course;a non zero count;the query;the words;the query words;the document;the other hand;we;a sum;all the words;all query;the document;they;the query;you;this term;they;the document;this case;these words;this probability becaus;our assumption;the smoothie;a different probability;we;a difference;two other signs;the first sum;the query words;we;the original sum;all the query words;order query;the document;we;they;all the query;it
#s9	so we take a sum over all the query words obviously this sum has extra terms that are this sum as extra terms that are not in this sum becaus here we're taking some overall the query was there it's not matched in the document so in order to make them equal we have to then subject to another some here and this is the sum over all the query words that are matching the document and this makes sense because here we are considering all current words and then we subtract the query words that are matched in the document that would give us the query words that not matched in the document and this is almost reversed process of the first step here and you might want to why do we want to do that well that's becaus if we do this then we have different forms of terms inside of these sums so now you can see in this some we have all the words matched the query was matching the document that was this kind of times here we have another some over the same set of terms match it queried homes in document but inside of some it's different but these two sums can clearly be merged so if we do that we'll get another form of the formula that looks like the following at the bottom yet and note that this is a very interesting formula because here we combine these two that our some over the query words matching the document in the want some here and the other son now is decomposing the two parts and these two parts look much simpler just because these are the probabilities of unseen worlds now this formula is very interesting because you can see the sum is not over all the match the query terms and just like a in the vector space model we take a son of terms that i in the intersection of query bakhtaran the document adapter so it already looks a little bit like the vector space model in fact there is even more similarity here as we explain on this slide
#c9	we;a sum;all the query words;this sum;extra terms;this sum;extra terms;this sum;becaus;we;the query;it;the document;order;them;we;the sum;all the query words;the document;sense;we;all current words;we;the query words;the document;us;the query words;the document;almost reversed process;the first step;you;we;becaus;we;we;different forms;terms;these sums;you;we;all the words;the query;the document;this kind;times;we;the same set;terms;it;homes;document;it;these two sums;we;we;another form;the formula;the following;the bottom;a very interesting formula;we;the query words;the document;the want;the other son;the two parts;these two parts;the probabilities;unseen worlds;this formula;you;the sum;all the match;the query terms;the vector space model;we;a son;terms;i;the intersection;query;it;the vector space model;fact;even more similarity;we;this slide
410	1cc2d7fa-3d11-49fa-b979-ef5e9442466f	108
#s1	This lecture is about the probabilistic latent semantic analysis or P LSA.
#c1	This lecture;the probabilistic latent semantic analysis;P LSA
#s2	In this lecture we're going to introduce probabilistic latent semantic analysis, often called the PLSA.
#c2	this lecture;we;probabilistic latent semantic analysis;the PLSA
#s3	Say this is the most basic topic model.
#c3	the most basic topic model
#s4	Also, one of the most useful topic models.
#c4	the most useful topic models
#s5	Now, this kind of models can in general be used to mine multiple topics from text documents, and PLSA is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail.
#c5	this kind;models;multiple topics;text documents;PLSA;the most basic topic models;'s;this problem;a little more detail
#s6	Here I show a sample article which is a blog article about Hurricane Katrina.
#c6	I;a sample article;a blog article;Hurricane Katrina
#s7	An I showed some sample topics, for example government response, flooding of the city in new orlean's donation and the background.
#c7	I;some sample topics;example;the city;new orlean's donation;the background
#s8	You can see in the article we use words from all these distributions.
#c8	You;the article;we;words;all these distributions
#s9	So we first for example.
#c9	So we;example
#s10	See there's a criticism of government response, and this is followed by the discussion of flooding of the city and donation, etc.
#c10	a criticism;government response;the discussion;flooding;the city;donation
#s11	We also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text.
#c11	We;background words;them;the goal;topic analysis;these topics;the text
#s12	So segment of the topics to figure out which words are from which distribution and to figure out the first one of these topics.
#c12	So segment;the topics;which words;which distribution;these topics
#s13	So how do we know there's a topic about government response?
#c13	we;a topic;government response
#s14	There is a public about the flooding of the city.
#c14	a public;the flooding;the city
#s15	So these are the tasks of topical model.
#c15	the tasks;topical model
#s16	If we can discover these topics can color this words as you see here to separate the different topics, then you can do a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc.
#c16	we;these topics;this words;you;the different topics;you;a lot;things;summarization;segmentation;the topics;clustering;sentences
#s17	So the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocabulary set.
#c17	the formal definition;the problem;mining;multiple topics;text;a slide;you;the earlier lecture;the input;the collection;the number;topics;vocabulary set
#s18	And of course, the text data right?
#c18	course;the text data
#s19	And then the output is of two kinds.
#c19	the output;two kinds
#s20	One is the topic category characterization Seedies HCI is a water distribution and
#c20	the topic category characterization;Seedies HCI;a water distribution
#s21	2nd
#s22	it's the topic coverage for each document.
#c22	it;the topic coverage;each document
#s23	These are pie some ideas
#c23	pie;some ideas
#s24	and they tell us which document covers which topic to what extent.
#c24	they;us;which document;what extent
#s25	So we hope to generate these as output because there are many useful applications if we can do that.
#c25	we;output;many useful applications;we
#s26	So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced.
#c26	the idea;PLSA;the two component mixture model;we
#s27	The only difference is that we're going to have more than two topics.
#c27	The only difference;we;more than two topics
#s28	Otherwise it's essentially the same.
#c28	it
#s29	So here I illustrate how we can generate the text that I was multiple topics.
#c29	I;we;the text;I;multiple topics
#s30	And naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function.
#c30	all cases;probabilistic modeling;the likelihood function
#s31	So we will also ask the question what's the probability of observing a world W from such a mixture model?
#c31	we;the question;what;the probability;a world W;such a mixture model
#s32	Now if you look at this picture and compare this with the picture that you have seen earlier, you will see the only difference is that we have added more topics here.
#c32	you;this picture;the picture;you;you;the only difference;we;more topics
#s33	So before we have just one topic besides the background topical, but now we have more topics.
#c33	we;just one topic;the background topical;we;more topics
#s34	Specifically we have K topics.
#c34	we;K topics
#s35	Now all these are topics that we assume that exist in the text data, so the consequences that our switch for choosing a topic now is multiway switch before it's just a two way switch.
#c35	topics;we;the text data;the consequences;that our switch;a topic;multiway switch;it;just a two way switch
#s36	Going to think of as flipping a coin.
#c36	a coin
#s37	But now we have multiple is.
#c37	we
#s38	First we can flip a coin to decide whether we will talk about the background.
#c38	we;a coin;we;the background
#s39	So it's the background.
#c39	it;the background
#s40	Lambda sub B versus non background.
#c40	Lambda sub B;non background
#s41	So this one minus Lambda B gives us the probability of actually choosing a topic.
#c41	Lambda B;us;the probability;a topic
#s42	And background help.
#c42	And background help
#s43	After we have made this decision, we have to make another decision to choose one of these K distributions.
#c43	we;this decision;we;another decision;these K distributions
#s44	So there's a key way.
#c44	a key way
#s45	Switch here, and this is characterized by the pies, and there's someone.
#c45	the pies;someone
#s46	So this is just the different design of switches, a little bit more complicated, but once we decide which distribution to use, the rest is the same.
#c46	just the different design;switches;we;which distribution;the rest
#s47	We're going to generate the world by using one of these distributions, assume here.
#c47	We;the world;these distributions
#s48	OK, so now let's look at this question about the like hold.
#c48	's;this question;the like hold
#s49	So what's the probability of observing a word from such a distribution?
#c49	what;the probability;a word;such a distribution
#s50	What do you think?
#c50	What;you
#s51	Now we've seen this problem many Times Now, and if you recall, it's generally a sum over all the different possibilities of generating the world.
#c51	we;this problem;you;it;a sum;all the different possibilities;the world
#s52	So let's first look at the how the world can be generated from the background model.
#c52	's;the world;the background model
#s53	The probability that the world is generated from the background model is Lambda multiplied by the probability of the world from the background model, right?
#c53	The probability;the world;the background model;Lambda;the probability;the world;the background model
#s54	Two things must happen.
#c54	Two things
#s55	First, we have to have chosen the background model.
#c55	we;the background model
#s56	And that's probability of Lambda sub B
#c56	probability;Lambda sub B
#s57	and then the second we must have actually obtained the world W from the background, and that's probability of W given sit out submit.
#c57	we;the world W;the background;probability;W;submit
#s58	OK, so similarly we can figure out the probability of observing the world from another topic.
#c58	we;the probability;the world;another topic
#s59	A topical theta sub k.
#c59	A topical theta;k.
#s60	Now notice that here's a product of three terms, and that's the cause.
#c60	a product;three terms;the cause
#s61	The choice of topic theta sub K only happens if two things happen.
#c61	The choice;topic theta;sub K;two things
#s62	One is we decided not to talk about background, so that's probability 1 minus Lambda
#c62	we;background;probability;Lambda
#s63	sub b. Second, we also have to actually choose.
#c63	we
#s64	Set us up.
#c64	us
#s65	K among these K topics.
#c65	K;these K topics
#s66	So that's probability of serious up cake or pie.
#c66	probability;serious up cake;pie
#s67	And similarly, the probability of generating the water from the second topic and his first topic popular like what you're seeing here and then.
#c67	And similarly, the probability;the water;the second topic;his first topic;what;you
#s68	So in the end, the probability of observing the world is just a sum of all these cases.
#c68	the end;the probability;the world;just a sum;all these cases
#s69	And I have to stress again, this is a very important formula to know because.
#c69	I;a very important formula
#s70	This is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability.
#c70	all the topic models;mixture models;you;the probability
#s71	Of W is indeed the some of these terms.
#c71	W;these terms
#s72	So next, once we have the likelihood function, we would be interested in knowing the parameters right?
#c72	we;the likelihood function;we;the parameters
#s73	So to estimate the parameters.
#c73	the parameters
#s74	But first let's put all these together to have the complete likelihood function for PLSA.
#c74	's;the complete likelihood function;PLSA
#s75	Now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as I said.
#c75	the first line;the probability;a word;the previous slide;an important formula;I
#s76	And so let's take a closer look at this.
#c76	's;a closer look
#s77	After that contains all the important parameters.
#c77	all the important parameters
#s78	So first we see Lambda sub b here.
#c78	we;Lambda sub b
#s79	This represents the percentage of background words.
#c79	the percentage;background words
#s80	That would believe exist in the text data and this can be unknown value that we set empirically.
#c80	the text data;unknown value;we
#s81	second, we see the background language model and typically we also assume this is known.
#c81	we;the background language model;we
#s82	We can use a large collection of text or use all the tests that we have available to estimate the water distribution.
#c82	We;a large collection;text;all the tests;we;the water distribution
#s83	Now next in the rest of this formula.
#c83	the rest;this formula
#s84	Excuse me, you see two interesting kinds of parameters.
#c84	me;you;two interesting kinds;parameters
#s85	Those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document.
#c85	the most important parameters;we;one;pies;the coverage;topic;the document
#s86	And the other is the word distributions that characterize all the topics.
#c86	the word distributions;all the topics
#s87	So the next line then is simply to plug this in to calculate the probability of document.
#c87	the next line;the probability;document
#s88	This is again of the familiar form where you have some and you have account of world in the document and then log of a probability.
#c88	the familiar form;you;you;account;world;the document;a probability
#s89	Now it's a little bit more complicated than the two component because now we have more components.
#c89	it;the two component;we;more components
#s90	So the sum involves more terms and then this line is just the like holder for the whole collection and it's very similar.
#c90	the sum;more terms;this line;just the like holder;the whole collection;it
#s91	Just accounting for more documents in the collection.
#c91	more documents;the collection
#s92	So what are the unknown primers?
#c92	what;the unknown primers
#s93	I already said there are two kinds on his coverage.
#c93	I;two kinds;his coverage
#s94	One is awarded distributions.
#c94	distributions
#s95	Again, it's a useful exercise for you to figure out exactly how many premise there on here.
#c95	it;a useful exercise;you
#s96	How many unknown parameters are there?
#c96	How many unknown parameters
#s97	Now trying to figure out that question would help you understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text data, and these are precisely the unknown parameters.
#c97	that question;you;the model;more detail;it;you;what;the output;we;we;PLSA;text data;the unknown parameters
#s98	So after we have obtained the likelihood function shown here, the next is to worry about parameter estimation.
#c98	we;the likelihood function;parameter estimation
#s99	And we can do the usual thing.
#c99	we;the usual thing
#s100	Maximum likelihood estimator.
#c100	Maximum likelihood estimator
#s101	So again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints.
#c101	it;a constrained optimization problem;what;we;we;a collection;text;we;more parameters;we;two constraints;different constraint;two kinds;constraints
#s102	One is awarded distributions.
#c102	distributions
#s103	All the words must have probabilities that sum to 141 distribution.
#c103	All the words;probabilities;141 distribution
#s104	The other is the topic coverage distribution.
#c104	the topic coverage distribution
#s105	Anna Document will have to cover precisely these K topics, so the probability of covering each topical would have to sum to one.
#c105	Anna Document;precisely these K topics;the probability;each topical
#s106	So at this point it's basically where they find applied math problem.
#c106	this point;it;they;applied math problem
#s107	You just need to figure out the solutions to optimization problem.
#c107	You;the solutions;optimization problem
#s108	There's a function with many variables and we need to just figure out the values of these variables to make the function which its maximum.
#c108	a function;many variables;we;the values;these variables;the function;its maximum
410	20703c3c-ced6-4410-ace1-139baa46505c	61
#s1	So I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm with converge to a local maximum.
#c1	I;you;the likelihood;it;that EM algorithm;converge;a local maximum
#s2	So here is just the illustration of what happened an A detailed explanation this.
#c2	just the illustration;what;an A detailed explanation
#s3	Require more.
#s4	Knowledge about some of the inequalities that we haven't really covered yet.
#c4	Knowledge;the inequalities;we
#s5	So here what you see is on the X dimension.
#c5	what;you;the X dimension
#s6	We have set up value.
#c6	We;value
#s7	This is the parameter that we left on the Y axis.
#c7	the parameter;we;the Y axis
#s8	We see the likelihood function.
#c8	We;the likelihood function
#s9	So this curve is reaching or like roller function, right?
#c9	this curve;roller function
#s10	So this one.
#c10	So this one
#s11	And this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this.
#c11	we;we;a set;value;this point
#s12	But in the case of mixture model, we cannot easily find the analytical solution to the problem.
#c12	the case;mixture model;we;the analytical solution;the problem
#s13	So we have to resolve a numerical algorithm.
#c13	we;a numerical algorithm
#s14	An EM algorithm is such an algorithm.
#c14	An EM algorithm;such an algorithm
#s15	It's a Hill climb algorithm that would mean you start with some random guess.
#c15	It;a Hill climb;algorithm;you;some random guess
#s16	Let's say you start from here.
#c16	's;you
#s17	That's your starting point
#c17	your starting point
#s18	and then you try to improve this by moving this to another point where you can have a higher like recorder.
#c18	you;another point;you;a higher like recorder
#s19	So that's the idea of Hill climbing.
#c19	the idea;Hill climbing
#s20	Any in the MRI was the way we achieve this is to do two things.
#c20	the MRI;the way;we;two things
#s21	First will fix a lower bound of likelihood function, so this is the lower bound you can see here.
#c21	likelihood function;you
#s22	An once we fit the lower bound we can then maximise the lower bound and of course the reason why this works is because the lower bound is much easier to optimize so we know our current gas is here an by maximizing the lower bound will move this point to the top two here.
#c22	we;we;course;we;our current gas;this point
#s23	I.
#c23	I.
#s24	And that we can then map to the original like role function.
#c24	we;the original like role function
#s25	We find this point.
#c25	We;this point
#s26	Be cause it's a lower bound, we are guaranteed to improve this gas.
#c26	it;we;this gas
#s27	Right, because we improve our lower bound and then the original lighter Holder curve which is above this lower bound will definitely be improved as well.
#c27	we;then the original lighter Holder curve
#s28	I so we already know it's improving the lower bound, so we definitely improve this original like record function which is above this lower bound.
#c28	I;we;it;we;record function
#s29	So in our example, the current gas is parameter value given by the current generation
#c29	our example;the current gas;parameter value;the current generation
#s30	and then the next guest is the RE estimated parameter values.
#c30	the next guest;the RE;parameter values
#s31	From this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck there.
#c31	this illustration;you;the next gas;the current gas;it;the maximum;it
#s32	So the two would be equal.
#s33	So the E step is basically to compute this lower bound.
#c33	the E step
#s34	And we don't direct it, just computed this likely or function, but we computed the latent variable values and.
#c34	we;it;function;we;the latent variable values
#s35	These are basically part of this lower bound.
#c35	part
#s36	This helps determine the lower bound the M step on the other hand, is to maximize the lower bound.
#c36	the M step;the other hand
#s37	It allows us to move parameters to a new point.
#c37	It;us;parameters;a new point
#s38	And that's why EML is gone.
#c38	EML
#s39	The little converge to a local maximum.
#c39	The little converge;a local maximum
#s40	Now, as you can imagine, when we have many local Maxima, we also have to repeat the EML with multiple times in order to figure out which one is the actual global maximum.
#c40	you;we;many local Maxima;we;the EML;multiple times;order;which one;the actual global maximum
#s41	And this actually in general is a difficult problem in numerical optimization.
#c41	a difficult problem;numerical optimization
#s42	So here for example, how do we start from here?
#c42	example;we
#s43	Then we gradually just climb up to this top, so that's not optimal, and we'd like to climb up all the way to here.
#c43	we;this top;we
#s44	So the only way to climb up to this here.
#c44	So the only way
#s45	This will start from somewhere here or here.
#s46	Right so.
#s47	In the EM algorithm, we generally would have to start from different points or have some other way to determine a good initial starting point.
#c47	the EM algorithm;we;different points;some other way;a good initial starting point
#s48	To summarize, in this lecture we introduce the EM algorithm.
#c48	this lecture;we;the EM algorithm
#s49	This is a general algorithm for computing.
#c49	a general algorithm;computing
#s50	Maximum regular is made of all kinds of mixture models.
#c50	all kinds;mixture models
#s51	So not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points.
#c51	our simple mixture model;algorithm;it;local maximum;it;initial points
#s52	The general idea is that we will have two steps to improve the estimate of parameters in the E step.
#c52	The general idea;we;two steps;the estimate;parameters;the E step
#s53	We roughly all augmenting our data by predicting values of useful hidden variables that we would use to simplify the estimation.
#c53	We;our data;values;useful hidden variables;we;the estimation
#s54	In our case, this is the distribution that has been used to generate the world.
#c54	our case;the distribution;the world
#s55	In the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters.
#c55	the end step;such augmented data;it;the distribution;the estimate;parameters
#s56	Here improve is guaranteed in terms of the likelihood function.
#c56	terms;the likelihood function
#s57	Note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase.
#c57	it;we;a stable converged parameter values;the likelihood function
#s58	There are some properties that have to be satisfied in order for the parameters also too.
#c58	some properties;order;the parameters
#s59	Convert it to some stable value.
#c59	it;some stable value
#s60	Now he thought data augmentation is done probabilistically.
#c60	he;data augmentation
#s61	That means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the world counts between the two distributions.
#c61	we;exactly what;the value;a hidden variable;we;a probability distribution;the possible values;these hidden variables;a split;counts;events;our case;the world;the two distributions
410	2736e0b3-cd3e-4760-b07e-e9aadcc588e2	83
#s1	This lecture is about the syntagmatic relation discovery.
#c1	This lecture;the syntagmatic relation discovery
#s2	An entropy.
#c2	An entropy
#s3	In this lecture, we're going to continue talking about word Association mining.
#c3	this lecture;we;word Association mining
#s4	In particular, we can talk about how to discover syntagmatic relations.
#c4	we;syntagmatic relations
#s5	And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations.
#c5	we;the introduction;entropy;the basis;some measures;such relations
#s6	By definition, Syntagmatic relations hold between words that have correlated Co occurrences.
#c6	definition;Syntagmatic relations;words;Co occurrences
#s7	That means when we see one word occurs in the context, we tend to see the occurrence of the other word.
#c7	we;one word;the context;we;the occurrence;the other word
#s8	So take a more specific example, here we can ask the question whenever eats occurs, but other words also tend to occur.
#c8	a more specific example;we;the question;other words
#s9	Now looking at the sentence is on the left.
#c9	the sentence;the left
#s10	We see some words that might occur together with eats like a cat, dog or fish is right.
#c10	We;some words;eats;a cat;dog;fish
#s11	But if I take them out and if you look at the right side where we only show eats and some other words.
#c11	I;them;you;the right side;we;some other words
#s12	The question that is, can you predict what other words occur?
#c12	The question;you;what;other words
#s13	To the left or to the right.
#c13	the left;the right
#s14	Right, so this would force us to think about what other words are associated with eats.
#c14	us;what;other words;eats
#s15	If they are associated with eats, they tend to occur in the context of eats.
#c15	they;eats;they;the context;eats
#s16	So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment.
#c16	our prediction problem;any text segment;a sentence;paragraph;a document;I;the question;a particular word;this segment
#s17	Right here we can ask the question about the world W is present or absent in this segment.
#c17	we;the question;the world W;this segment
#s18	Now, what's interesting is that some words are actually easier for it, in other words.
#c18	what;some words;it;other words
#s19	If you take a look at the three words shown here, meet, the and Unicorn.
#c19	you;a look;the three words;Unicorn
#s20	Which one do you think it is easier to predict?
#c20	Which one;you;it
#s21	Now, if you think about it for a moment, you might conclude that.
#c21	you;it;a moment;you
#s22	The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence.
#c22	it;I;the semtence
#s23	Unicorn is also relatively easy.
#c23	Unicorn
#s24	Because Unicorn is rare, is very rare.
#c24	Unicorn
#s25	And I can bet that it doesn't occur in this sentence.
#c25	I;it;this sentence
#s26	But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately.
#c26	meat;terms;frequency;it;it;it;it;the sentence;the segment
#s27	But it may also not occur in the segment.
#c27	it;the segment
#s28	So now let's start this problem more formally.
#c28	's;this problem
#s29	Alright, so the problem can be formally defined as predicting the value of a binary random variable.
#c29	the problem;the value;a binary random variable
#s30	Here we denoted by X sub w, w denotes a word.
#c30	we;X;sub;w;w denotes;a word
#s31	So this random variable is associated with precisely one word.
#c31	this random variable;precisely one word
#s32	When the value of the variable is 1, it means this word is present.
#c32	the value;the variable;it;this word
#s33	When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1.
#c33	it;it;the word;the probabilities
#s34	Because a word is either present or absent in the segment.
#c34	a word;the segment
#s35	There's no other choice.
#c35	no other choice
#s36	So the intuition we discussed earlier can be formally stated as follows.
#c36	the intuition;we
#s37	The more random this random variable is, the more difficult the prediction would be.
#c37	this random variable;the prediction
#s38	Now the question is, how does one quantitatively measure the randomness of a random variable like X sub w, how in general, can we quantify the randomness of a variable?
#c38	the question;the randomness;a random variable;X;sub;we;the randomness;a variable
#s39	And that's why we need a measure called entropy.
#c39	we;a measure;entropy
#s40	And this is a measure introduced in information theory to measure the randomness of X. There is also some connection with the information here, but that's beyond the scope of this course.
#c40	a measure;information theory;the randomness;X. There;some connection;the information;the scope;this course
#s41	So for our purpose we just treat the entropy function as a function defined  on a random variable.
#c41	our purpose;we;the entropy function;a function;a random variable
#s42	In this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values.
#c42	this case;it;a binary random variable;the definition;a random variable;multiple values
#s43	Now the function form looks like this.
#c43	the function form
#s44	There's a sum over all the possible values for this random variable inside the sum,  for each value we have a product of the probability that the random variable equals this value and log of this probability.
#c44	a sum;all the possible values;this random variable;the sum;each value;we;a product;the probability;the random variable;this value;log;this probability
#s45	And note that there is also an negative sign there.
#c45	an negative sign
#s46	Now, entropy in general is not negative and that can be mathematically proved.
#s47	So if we expand this sum will see the equation looks like a second one I explicitly plugged in the two values zero and one.
#c47	we;this sum;the equation;a second one;I;the two values
#s48	And sometimes when we have 0 log of 0,  we would generally find that as zero because log of 0 is undefined.
#c48	we;0 log;we;log
#s49	So this is the entropy function and this function will give a different value for different distributions of this random variable.
#c49	the entropy function;this function;a different value;different distributions;this random variable
#s50	And this clear it clearly depends on the probability that the random variable taking a value of one or zero.
#c50	it;the probability;the random variable;a value
#s51	If we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this.
#c51	we;his function;the probability;the random variable;the function
#s52	At the two ends,  That means when the probability of X = 1 is very small or very large, then the entropy function has a lower value when it's .5 in the middle that it reaches the maximum.
#c52	the two ends;the probability;X;=;the entropy function;a lower value;it;the middle;it;the maximum
#s53	Now, if we plot the function against the probability that the X  is taking a value of 0 and the function would show exactly the same curve here.
#c53	we;the function;the probability;a value;the function;exactly the same curve
#s54	And you can imagine why and so that's because the two probabilities are symmetric  and completely symmetric.
#c54	you;the two probabilities
#s55	So an interesting question.
#c55	So an interesting question
#s56	You could think about in general  here is for what kind of X?
#c56	You;what kind;X
#s57	Does the entropy reached maximum or minimum and we can in particular think about some special cases.
#c57	the entropy;we;some special cases
#s58	For example, in one case we might have a random variable that always takes the value of one,  the probability is one  or there is a random variable that Is equally likely taking a value of 1 or 0.
#c58	example;one case;we;a random variable;the value;the probability;a random variable;a value
#s59	In this case, the probability that X = 1 is .5.
#c59	this case;X;=
#s60	Now, which one has a higher entropy?
#c60	which one;a higher entropy
#s61	It's easier to look at the problem by thinking of simple example.
#c61	It;the problem;simple example
#s62	Using coin tossing,  so when we think about the random experiment like a tossing a coin, it gives us a random variable that  can represent the result.
#c62	coin tossing;we;the random experiment;a coin;it;us;a random variable;the result
#s63	It can be head or tail, so we can define a random variable X sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail.
#c63	It;head;tail;we;a random variable X sub coin;it;the coin;head;it;the coin;tail
#s64	So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing.
#c64	we;the entropy;this random variable;this entropy;it;the outcome;a coin;coin tossing
#s65	So we can think about the two cases.
#c65	we;the two cases
#s66	One is a fair coin, it's completely fair.
#c66	a fair coin;it
#s67	The coin shows up as head hotel equally likely, so the two probabilities would be,  1/2 right so both will have both equal to 1/2.
#c67	The coin;head hotel;the two probabilities
#s68	Another extreme case is completely biased coin, where the coin always shows up as head, so it's a completely biased coin.
#c68	Another extreme case;completely biased coin;the coin;head;it;a completely biased coin
#s69	Now let's think about the entropies in the two cases, and if you plug in these values you can see the entropies,  would be as follows for a fair coin we see the entropy reaches its maximum, that's one.
#c69	's;the entropies;the two cases;you;these values;you;the entropies;a fair coin;we;the entropy;its maximum
#s70	For the completely biased coin we see is 0 and that intuitively makes a lot of sense because a fair coin is most difficult to predict  whereas a completely biased coin is very easy to predict that we can always say it's a head because it is a head all the time so they can be shown on the curve as follows.
#c70	the completely biased coin;we;a lot;sense;a fair coin;a completely biased coin;we;it;a head;it;a head;they;the curve
#s71	So the fair coin corresponds to the middle point, or it's very uncertain.
#c71	the fair coin corresponds;the middle point;it
#s72	The completely biased coin corresponds to the end point.
#c72	The completely biased coin corresponds;the end point
#s73	We have a probability of 1.0 and the entropy is 0.
#c73	We;a probability;the entropy
#s74	So now let's see how we can use entropy for word prediction.
#c74	's;we;entropy;word prediction
#s75	Now the problem, let's think about our problem right, still predicted whether W is present or absolutely in this segment.
#c75	the problem;'s;our problem;W;this segment
#s76	Again, think about the three words.
#c76	the three words
#s77	Particularly, think about their entropies.
#c77	their entropies
#s78	Now we can assume high entropy words are harder to predict.
#c78	we;high entropy words
#s79	And so we will now have quantitative way to tell us which word is harder to predict.
#c79	we;quantitative way;us;which word
#s80	Now if you look at the three words, meat, the and Unicorn again.
#c80	you;the three words;meat;Unicorn
#s81	An we clearly would expect the meat to have a high entropy, then the OR Unicorn.
#c81	we;the meat;a high entropy;then the OR Unicorn
#s82	In fact, if you look at the entropy of the,  It's close to 0,  because it occurs everywhere.
#c82	fact;you;the entropy;It;it
#s83	So, it's like a completed biased coin,  therefore the entropy is 0.
#c83	it;a completed biased coin;the entropy
410	27d06808-2624-4922-a079-04dccb301dde	77
#s1	This lecture is about mixture model estimation.
#c1	This lecture;mixture model estimation
#s2	In this lecture, we're going to continue discussing probabilistic topic models.
#c2	this lecture;we;probabilistic topic models
#s3	In particular, we're going to talk about how to estimate the parameters of a mixture model.
#c3	we;the parameters;a mixture model
#s4	So let's first look at our motivation for using a mixture model
#c4	's;our motivation;a mixture model
#s5	and we hope to factor out the background words from the topic word distribution.
#c5	we;the background words;the topic word distribution
#s6	So the idea is to assume that the text data actually contain two kinds of words.
#c6	the idea;the text data;two kinds;words
#s7	One kind is from the background here.
#c7	One kind;the background
#s8	So the is away etc and the other kind is from our topic word distribution that we're interested in.
#c8	the other kind;our topic word distribution;we
#s9	So in order to solve this problem of factoring out background words, we can set up our mixture model as follows.
#c9	order;this problem;background words;we;our mixture model
#s10	We're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub D, which is our target.
#c10	We;we;the parameters;all the values;all the parameters;the mixture model;the word distribution;theta sub D;our target
#s11	So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in.
#c11	the case;a probalistic model;we;the unknown variables;we
#s12	But we are going to simplify other things.
#c12	we;other things
#s13	We are going to assume we have knowledge about others.
#c13	We;we;knowledge;others
#s14	And this is a powerful way of customizing a model for a particular need.
#c14	a powerful way;a model;a particular need
#s15	Now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words.
#c15	you;we;we;the background word distribution;this case;our goal;factor;precisely those high probability background words
#s16	So we assume the background model is already fixed.
#c16	we;the background model
#s17	And the problem here is how can we adjust theta sub D in order to maximize the probability of the observed document here and we assume all the other parameters are known.
#c17	the problem;we;theta sub D;order;the probability;the observed document;we;all the other parameters
#s18	Now, although we designed the model heuristically to try to factor out this background words.
#c18	we;the model;this background words
#s19	It's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before.
#c19	It;we;maximum likelihood estimator;we;order distribution;the common words;smaller probabilities
#s20	So now.
#s21	In this case, it turns out that the answer is yes, and when we set up the probalistic model in  this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution.
#c21	this case;it;the answer;we;the probalistic model;we;maximum likelihood estimator;we;a word distribution;the common words;the use;the background distribution
#s22	So to understand why this is so, it's useful to examine the behavior of a mixture model.
#c22	it;the behavior;a mixture model
#s23	So we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here.
#c23	we;a very, very simple case;order;some interesting behaviors;a mixture model;the observed patterns;model;it;this behavior;we;a very simple case;what;we
#s24	So specifically in this case.
#c24	this case
#s25	Let's assume that the probability of choosing each of the two models is exactly the same, so we're going to flip fair coin to decide which model to use.
#c25	's;the probability;the two models;we;fair coin;which model
#s26	Furthermore, we are going to assume there are precisely two words: the and text.
#c26	we;precisely two words;text
#s27	Obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case.
#c27	a very naive oversimplification;the actual text;it;the behavior;such a special case
#s28	So we further assume that the background model gives probability of point nine to the word the and text point one.
#c28	we;the background model;probability;the word;the and text point
#s29	Now, let's also assume that our data is extremely simple.
#c29	's;our data
#s30	The document has just the two words text and the.
#c30	The document;just the two words text
#s31	So now let's write down the likelihood function in such a case.
#c31	's;the likelihood function;such a case
#s32	First, what's the probability of text and what's the probability of the?
#c32	what;the probability;text;what;the probability
#s33	I hope by this point
#c33	I;this point
#s34	and you will be able to write it down.
#c34	you;it
#s35	So the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution.
#c35	the probability;text;the sum over 2 cases;each case;the word distribution
#s36	And it accounts for the two ways of generating text.
#c36	it;the two ways;text
#s37	An inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model.
#c37	each case;we;the probability;the model;the probability;text;that model
#s38	Similarly, the would have a probability of the same form, just with different exact probabilities.
#c38	a probability;the same form;different exact probabilities
#s39	So naturally our likelihood function is just the product of the two, so it's very easy to see that.
#c39	our likelihood function;just the product;it
#s40	Once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model.
#c40	you;what;the probability;each word;it;each word;such a mixture model
#s41	Now the interesting question now is, how can we then optimize this likelihood?
#c41	the interesting question;we;this likelihood
#s42	Well, you will notice that there were only two variables.
#c42	you;only two variables
#s43	They are precisely the two probabilities of the two words text and the given by theta sub D.
#c43	They;precisely the two probabilities;the two words text;theta sub D.
#s44	And this is because we have assumed all the other parameters are known.
#c44	we;all the other parameters
#s45	So now the question is a very simple algebra question, right?
#c45	the question;a very simple algebra question
#s46	So we have a simple expression with two variables and we hope to choose the values of these two variables to maximize this function.
#c46	we;a simple expression;two variables;we;the values;these two variables;this function
#s47	And the exercise that we have seen some simple algebra problems and note that the two probabilities must sum to one.
#c47	And the exercise;we;some simple algebra problems;the two probabilities
#s48	So there's some constraint.
#c48	some constraint
#s49	If there were no constraint, of course we would set both probabilities to their maximum value, which would be one to maximize this.
#c49	no constraint;course;we;both probabilities;their maximum value
#s50	But we can't do that because text and the must sum to one.
#c50	we;text
#s51	We can't give both a probability of 1.
#c51	We;both a probability
#s52	So now the question is how should we allocate the probability mass between the two words?
#c52	the question;we;the probability mass;the two words
#s53	What do you think?
#c53	What;you
#s54	Now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probabilities to maximize the value of this function.
#c54	it;this formula;moment;what;what;we;order;these probabilities;the value;this function
#s55	OK, if we look into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator.
#c55	we;we;some interesting behavior;the two component models;they;the probability;the observed data;the maximum likelihood estimator
#s56	But there are also competing in someway an in particular they will be competing on the words.
#c56	someway;they;the words
#s57	And they will tend to bet high probabilities on different words to avoid this competition in some sense.
#c57	they;high probabilities;different words;this competition;some sense
#s58	Or to gain advantage in this competition.
#c58	advantage;this competition
#s59	So again, looking at this objective function and we have a constraint.
#c59	this objective function;we;a constraint
#s60	On the two probabilities.
#c60	the two probabilities
#s61	Now.
#s62	If you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger than the.
#c62	you;the formula;you;you;the probability;text
#s63	And this intuition can be well supported by a mathematical fact, which is when the sum of two variables is a constant.
#c63	this intuition;a mathematical fact;the sum;two variables
#s64	Then the product of them, which is maximum when they are equal and this is a fact that we know from algebra.
#c64	the product;them;they;a fact;we;algebra
#s65	Now if we plug that in, it will  mean that we have to make the two probabilities equal.
#c65	we;it;we;the two probabilities
#s66	And when we make them equal an, if we consider the constraint that we can easy to solve this problem and the solution is the probability of text would be point nine and probability of the is point one.
#c66	we;them;we;the constraint;we;this problem;the solution;the probability;text;point;point
#s67	And as you can see, indeed the probability of text is now much larger than probability of the.
#c67	you;the probability;text;probability
#s68	This is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text.
#c68	the case;we;just one distribution;the use;the background model;a very high probability;the and low probability;text
#s69	And if you look at the equation, you will see obviously some interaction of the two distributions here.
#c69	you;the equation;you;some interaction;the two distributions
#s70	In particular, you will see in order to make them equal and then the probability assigned by theta sub D must be higher for a word that has a smaller probability given by the background.
#c70	you;order;them;then the probability;theta;D;a word;a smaller probability;the background
#s71	And, this is obvious from examining this equation because the background part is weak for text it's small.
#c71	this equation;the background part;text;it
#s72	So in order to compensate for that we must make the probability of text given by theta sub D somewhat larger so that the two sides can be balanced.
#c72	order;we;the probability;text;theta sub;the two sides
#s73	So this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution.
#c73	fact;a very general behavior;this mixture model;one distribution;a high probability;one word;, then the other distribution
#s74	Would tend to do the opposite.
#c74	the opposite
#s75	Basically it would discourage other distributions to do the same, and this is to balance them out so that we can account for all kinds of words.
#c75	it;other distributions;them;we;all kinds;words
#s76	And this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model.
#c76	a background model;high probabilities;background words;we;the unknown topic world distribution;smaller probabilities;such common words;more probability mass;the content words;the background model
#s77	Meaning that they have a very small probability from the background model, like a text here.
#c77	they;a very small probability;the background model;a text
410	2997c717-2552-411d-9dc4-7e648e16bbf0	62
#s1	So, as we explained, different textual representation tends to enable different analysis.
#c1	we;different textual representation;different analysis
#s2	In particular, we can gradually add more and more deeper analysis results to represent text data, and that would open up more interesting representation opportunities and also analysis capacities.
#c2	we;more and more deeper analysis results;text data;more interesting representation opportunities;also analysis capacities
#s3	So this table summarizes what we have just seen.
#c3	this table;what;we
#s4	So the first column shows the text recognition, the second visualizes the generality of such representation, meaning whether we can do this kind of representation accurate before all the text data, or only some of them, and third column shows the enabled analysis techniques.
#c4	the first column;the text recognition;the generality;such representation;we;this kind;representation;all the text data;them;third column;the enabled analysis techniques
#s5	And the final column shows some examples of application that can be achieved through this level of representation.
#c5	the final column;some examples;application;this level;representation
#s6	So let's take a look at them.
#c6	's;a look;them
#s7	So as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general.
#c7	a string text;stream processing algorithms;it;it
#s8	And there are still some interesting applications that can be done at this level.
#c8	some interesting applications;this level
#s9	For example, compression of text doesn't necessarily need to know the word boundaries.
#c9	example;compression;text;the word boundaries
#s10	Although knowing word boundaries might actually also help.
#c10	knowing word boundaries
#s11	Word based representation is very important level of representation.
#c11	Word based representation;very important level;representation
#s12	It's quite general and relatively robust.
#c12	It
#s13	It can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis.
#c13	It;a lot;analysis techniques;word relation analysis;topic analysis;sentiment analysis;many applications;this kind;analysis
#s14	For example, Thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text.
#c14	example;Thesaurus discovery;related words;topic;opinion related applications;example;people;the major topics;the collection;text
#s15	And this can be the case.
#c15	the case
#s16	In research literature, a scientist want to know what are the most important research topics today or customer service people might want to know what are the major complaints from their customers about by mining their email messages.
#c16	research literature;a scientist;what;the most important research topics;customer service people;what;the major complaints;their customers;their email messages
#s17	And business intelligence people might be interested in understanding consumers opinions about their products and competitors products to figure out the what are the winning features of their products.
#c17	business intelligence people;consumers;opinions;their products;competitors;products;what;the winning features;their products
#s18	And in general there are many applications that can be enabled by the representation at this level.
#c18	many applications;the representation;this level
#s19	Now moving down, we'll see we can gradually add additional representations.
#c19	we;we;additional representations
#s20	By adding syntactic structures we can enable, Of course, syntactic graph analysis.
#c20	syntactic structures;we;syntactic graph analysis
#s21	We can use graph mining algorithms to analyze Syntactic graphs.
#c21	We;graph mining algorithms;Syntactic graphs
#s22	And some applications are related to this kind of representation.
#c22	some applications;this kind;representation
#s23	For example, stylistic analysis generally requires syntactical representation.
#c23	example;stylistic analysis;syntactical representation
#s24	Syntactical structure representation.
#c24	Syntactical structure representation
#s25	We can also generate the structure based feature features and those are features that might help us classify text objects into different categories.
#c25	We;the structure;based feature features;features;us;text objects;different categories
#s26	By looking at the structures, sometimes the classification can be more accurate.
#c26	the structures;the classification
#s27	For example, if you want to classify articles into different categories corresponding to different authors want to figure out which of the K authors has actually written this article.
#c27	example;you;articles;different categories;different authors;the K authors;this article
#s28	Then you generally need to look at the syntactic structures.
#c28	you;the syntactic structures
#s29	When we add entities and relations, then we can enable lot of techniques such as knowledge graph analysis or information network analysis in general and this analysis would enable applications about entities, for example, discovery of all the knowledge and opinions about the real world energy entity.
#c29	we;entities;relations;we;lot;techniques;knowledge;analysis or information network analysis;this analysis;applications;entities;example;all the knowledge;opinions;the real world energy entity
#s30	You can also use this level representation to integrate everything about entity from scattered sources.
#c30	You;this level representation;everything;entity;scattered sources
#s31	Finally, when we add logic predicates then we would enable logic inference ofcourse, and this can be very useful for integrative analysis of scattered knowledge.
#c31	we;logic predicates;we;logic inference;integrative analysis;scattered knowledge
#s32	For example, we can also add ontology on top of the extracted information from text to make inferences.
#c32	example;we;ontology;top;the extracted information;text;inferences
#s33	A good example of application in this enabled by this level of representation is a intelligent knowledge assistant for biologists.
#c33	A good example;application;this level;representation;a intelligent knowledge assistant;biologists
#s34	And this is intended program that can help biologists manage all the relevant knowledge from literature about the research problem, such as understanding functions of genes.
#c34	And this is intended program;biologists;all the relevant knowledge;literature;the research problem;understanding functions;genes
#s35	And the computer can make inferences about some of the hypothesis that biologists might be interesting, for example, whether a gene has a certain function and then the intelligent program can read the literature to extract the relevant facts.
#c35	the computer;inferences;the hypothesis;biologists;example;a gene;a certain function;the intelligent program;the literature;the relevant facts
#s36	Doing by doing information extraction and then using a logical system to actually track that's the answers to researchers questioning about what genes are related to what functions.
#c36	information extraction;a logical system;the answers;researchers;what;genes;what functions
#s37	So in order to support this level of application, we need to go as far as logical representation.
#c37	order;this level;application;we;logical representation
#s38	Now this course is covering techniques mainly based on word based representation.
#c38	this course;techniques;word based representation
#s39	These techniques are general and robust and thus are more widely used in various applications.
#c39	These techniques;various applications
#s40	In fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level.
#c40	fact;virtually all the text mining applications;you;this level;representation;the techniques;analysis;this level
#s41	But obviously all these other levels can be combined and should be combined in order to support sophisticated applications.
#c41	all these other levels;order;sophisticated applications
#s42	So to summarize, here are the major takeaway points.
#c42	the major takeaway points
#s43	Text representation determines what kind of mining algorithms can be applied.
#c43	Text representation;what kind;mining algorithms
#s44	And there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc. "
#c44	multiple ways;text - strings;words;syntactic structures;the relation graphs;logical predicates
#s45	And these different representations should in general be combined in real applications to the extent we can.
#c45	these different representations;real applications;the extent;we
#s46	For example, if even if we cannot do accurately, this application of syntactic structures we can stick at partial structures extracted and if we can recognize some entities and that would be great.
#c46	example;we;this application;syntactic structures;we;partial structures;we;some entities
#s47	So in general we want to do as much as we can.
#c47	we;we
#s48	And when different levels are combined together, we can enable richer analysis.
#c48	different levels;we;richer analysis
#s49	More powerful analysis.
#c49	More powerful analysis
#s50	This course, however, focuses on word based representation.
#c50	This course;word based representation
#s51	Such techniques have also several advantages.
#c51	Such techniques;several advantages
#s52	First, they are general and robust, so they are applicable to any natural language.
#c52	they;they;any natural language
#s53	That's a big advantage over other approaches that rely on more fragile natural language processing techniques.
#c53	a big advantage;other approaches;more fragile natural language processing techniques
#s54	Secondly, it does not require much manual effort or sometimes it does not require any manual effort.
#c54	it;much manual effort;it;any manual effort
#s55	So that's again important benefit, because that means you can apply directly to any application.
#c55	again important benefit;you;any application
#s56	Third, these techniques are actually surprisingly powerful and effective for many applications.
#c56	these techniques;many applications
#s57	Although not all, of course, as I just explained.
#c57	course;I
#s58	Now they are very effective, partly because the words are invented by humans as basic units for communications.
#c58	they;the words;humans;basic units;communications
#s59	So they are actually quite sufficient for representing all kinds of semantics.
#c59	they;all kinds;semantics
#s60	So that makes this kind of word based representation also powerful.
#c60	this kind;word
#s61	And finally such a word based representation and the techniques enabled by such a representation can be combined with many other sophisticated approaches.
#c61	finally such a word based representation;the techniques;such a representation;many other sophisticated approaches
#s62	So they're not competing with each other.
#c62	they
410	2d0e46c7-df4e-48b3-9550-dac3fec3062d	99
#s1	This lecture is about the ordinal logistic regression for sentiment analysis.
#c1	This lecture;the ordinal logistic regression;sentiment analysis
#s2	So this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction.
#c2	our problem;a typical sentiment classification problem;rating prediction
#s3	We have an opinionated text document D as input an we want to generate as output already in the range of one through K, so it's discrete rating and thus this is a categorization problem.
#c3	We;an opinionated text document D;we;output;the range;K;it;discrete rating;a categorization problem
#s4	We have K categories here.
#c4	We;K categories
#s5	Now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories.
#c5	we;a regular text;categorization technique;this problem;such a solution;the order;dependency;the categories
#s6	Intuitively, the features that can distinguish Category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish K from K - 1.
#c6	the features;Category;K
#s7	For example, positive words generally suggest a higher rating.
#c7	example;positive words;a higher rating
#s8	Now when we train categorisation program by treating these categories as independent, we would not capture this.
#c8	we;categorisation program;these categories;we
#s9	So what's the solution?
#c9	what;the solution
#s10	In general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression.
#c10	we;order;many different approaches;we;them;the ordinal logistic regression
#s11	Now let's first think about how we use logistic regression for binary setting categorization problem.
#c11	's;we;logistic regression;binary setting categorization problem
#s12	So suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem.
#c12	we;it;it;just a two category categorization problem
#s13	So the predictors are represented as X and these are the features and there are M features altogether, which feature value is a real number, and this can be representation of a text document.
#c13	the predictors;X;the features;M features;feature value;a real number;representation;a text document
#s14	And y has two values, binary response variable {0,1}.
#c14	y;two values;binary response variable
#s15	1 means X is positive, 0 means X is negative.
#c15	X;X
#s16	And then of course, this is a standard two category categorization problem.
#c16	course;a standard two category categorization problem
#s17	We can apply logistical regression.
#c17	We;logistical regression
#s18	You may recall that in logistic regression we assume the log of probability that Y is equal to 1 is assumed to be a linear function of these features as shown here.
#c18	You;logistic regression;we;the log;probability;Y;a linear function;these features
#s19	So this would allow us to also write the probability of y = 1 given X in this equation that you are seeing on the bottom, and so that's logistical function and you can see it relates this probability to probability that
#c19	us;the probability;y;=;X;this equation;you;the bottom;logistical function;you;it;this probability;probability
#s20	y = 1 to the feature values.
#c20	y;the feature values
#s21	And of course, B_i is our parameters here.
#c21	course;B_i;our parameters
#s22	So this is just a direct application of logistical regression for binary categorization.
#c22	just a direct application;logistical regression;binary categorization
#s23	What if we have multiple categories, multiple levels?
#c23	What;we;multiple categories;multiple levels
#s24	We actually use such a binary logistic regression program to solve this multi level rating prediction.
#c24	We;such a binary logistic regression program;this multi level rating prediction
#s25	And the idea is we can introduce multiple binary classifiers and
#c25	the idea;we;multiple binary classifiers
#s26	each case we ask the classifier to predict whether the rating is J or above all the ratings lower than
#c26	we;the classifier;the rating;J;all the ratings
#s27	J. So when Y_j is equal to 1, it means rating is J or above.
#c27	Y_j;it;rating;J
#s28	When it's zero, that means the rating is lower than J.
#c28	it;the rating;J.
#s29	So basically, if we want to predict rating in the range of one through K, we first have one classifier to distinguish K versus others.
#c29	we;rating;the range;K;we;one classifier;K;others
#s30	And that's our classifier one, and then we're going to have another classifier to distinguish K - 1 from the rest.
#c30	our classifier one;we;another classifier;the rest
#s31	That's classifier two, and in the end we need a classifier to distinguish two and one
#c31	the end;we;a classifier
#s32	So altogether we'll have K - 1 classifiers.
#c32	we;K - 1 classifiers
#s33	Now if we do that of course, then we can also solve this problem, and the logistical regression program would be also very straightforward as you have just seen on the previous slide.
#c33	we;course;we;this problem;the logistical regression program;you;the previous slide
#s34	Only that here we have more parameters because for each classify we need a different set of parameters.
#c34	we;more parameters;each classify;we;a different set;parameters
#s35	So now the logistic regression classifiers indexed by J, which corresponds to a reading level.
#c35	So now the logistic regression classifiers;J;a reading level
#s36	And I have also used offer subject to replace beta 0.
#c36	I;beta
#s37	And this is to make the notation more consistent with what we can show in the ordinal logistic regression.
#c37	the notation;what;we;the ordinal logistic regression
#s38	So anyway, so here we now have basically K - 1 regular logistic regression classifiers.
#c38	we;basically K - 1 regular logistic regression classifiers
#s39	Each has its own set of parameters.
#c39	its own set;parameters
#s40	So now with this approach we can now do rating prediction as follows.
#c40	this approach;we;rating prediction
#s41	After we have trained these K - 1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision.
#c41	we;these K - 1 logistic regression classifiers;course;we;a new instance;a classifier;the decision
#s42	So first let's look at the classifier that corresponds to level of rating K.
#c42	's;the classifier;level
#s43	So this classifier will tell us whether this object should have a rating of K or above.
#c43	this classifier;us;this object;a rating;K
#s44	And if its probability according to this logistical regression classifier is larger than .5, we're going to say yes, the rating is
#c44	its probability;this logistical regression classifier;we;the rating
#s45	K. Now, what if it's not as large as .5?
#c45	it
#s46	Well, that means the reading is below K, right?
#c46	the reading;K
#s47	So now we need to invoke the next class file, which tells us whether it's above K - 1.
#c47	we;the next class file;us;it
#s48	It's at least K - 1 and if the probability is larger than .5 then will say, well, then it's K - 1.
#c48	It;the probability;it
#s49	What if it says no?
#c49	What;it
#s50	Well, that means the rating will be even below K minus one, and so we're going to just keep invoking these classifiers until we hit the end.
#c50	the rating;K;we;these classifiers;we;the end
#s51	When we need to decide whether it's two or one, so this will help us solve the problem, right?
#c51	we;it;us;the problem
#s52	So we can have a classifier that would actually give us a prediction of rating in the range of one through K, unfortunately, such a strategy is not the optimal way of solving this problem, and specifically there are two problems with this approach.
#c52	we;a classifier;us;a prediction;rating;the range;K;such a strategy;the optimal way;this problem;two problems;this approach
#s53	So these equations are the same as you have seen before.
#c53	these equations;you
#s54	Now the first problem is that there are just too many parameters.
#c54	the first problem;just too many parameters
#s55	There are many parameters.
#c55	many parameters
#s56	Now can you count how many parameters we have exactly here?
#c56	you;how many parameters;we
#s57	Now this may be interesting exercise.
#c57	interesting exercise
#s58	To do so you might want to just pause the video and try to figure out the solution how many premises we have for each classifier?
#c58	you;the video;the solution;how many premises;we;each classifier
#s59	And how many classifiers do we have?
#c59	how many classifiers;we
#s60	You can see the answer is that for each classifier we have N + 1 parameters.
#c60	You;the answer;each classifier;we;+ 1 parameters
#s61	And we have K - 1 classifiers altogether, so the total number of premises (K - 1) * (M + 1).
#c61	we;K - 1 classifiers;the total number;premises;K;(M
#s62	That's alot alot of parameters.
#c62	alot alot;parameters
#s63	So when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model?
#c63	the classifier;a lot;parameters;a lot;data;us;data;us;the optimal parameters;the this such a complex model
#s64	So that's not the idea.
#c64	the idea
#s65	The second problem is that these problems these K - 1 classifiers are not really independent.
#c65	The second problem;these problems;these K - 1 classifiers
#s66	These problems are actually dependent.
#c66	These problems
#s67	In general, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers.
#c67	words;the rating;these classifiers;all these classifiers
#s68	So we should be able to take advantage of this factor.
#c68	we;advantage;this factor
#s69	Now the idea of ordinal logistic regression is precisely that  A key idea is just the improvement over the K -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the Beta parameters these are the parameters that indicate the influence of those weights.
#c69	the idea;ordinal logistic regression;A key idea;just the improvement;the K -1 independent logistical regression classifiers;that idea;these beta parameters;we;the Beta parameters;the parameters;the influence;those weights
#s70	And we're going to assume these better values are the same for all the K - 1 premise, and this just encodes our intuition that positive words in general would make a higher rating more likely.
#c70	we;these better values;all the K - 1 premise;our intuition;positive words;a higher rating
#s71	So this is intuitively appealing assumption.
#c71	assumption
#s72	It's reasonable for our problem set up when we have this order in these categories.
#c72	It;our problem;we;this order;these categories
#s73	Now, in fact, this would allow us to have two positive benefit of one is it's going to reduce the number of parameters significantly.
#c73	fact;us;two positive benefit;it;the number;parameters
#s74	And the other is to allow us to share the training data, because all these parameters are assumed to be equal.
#c74	us;the training data;all these parameters
#s75	So these training data for different classifiers.
#c75	So these training data;different classifiers
#s76	Can then be shared to help us set the optimal value for beta.
#c76	us;the optimal value;beta
#s77	So we have more data to help us choose a good beta value.
#c77	we;more data;us;a good beta value
#s78	So what's the consequence?
#c78	what;the consequence
#s79	The formula would look very similar to what you have seen before, only that now the beta parameter has just one index that correspond to the feature and no longer has the other index that corresponds to the level of rating.
#c79	The formula;what;you;the beta parameter;just one index;the feature;the other index;the level;rating
#s80	So that means we tie them together and there's only one set of beta values for all the classifiers.
#c80	we;them;only one set;beta values;all the classifiers
#s81	However, each classifiers there has a distinct Alpha value, the Alpha parameter, the except it's different and this is of course needed to predict the different levels of ratings.
#c81	each classifiers;a distinct Alpha value;the Alpha parameter;it;course;the different levels;ratings
#s82	So apha subject is different.
#c82	apha subject
#s83	It depends on J. Different J has a different alpha, but the rest of the parameters of beta are the same.
#c83	It;J. Different J;a different alpha;the rest;the parameters;beta
#s84	So now you can also ask the question, how many parameters do we have now?
#c84	you;the question;how many parameters;we
#s85	Again, that's an interesting question to think about.
#c85	an interesting question
#s86	So if you think about it for a moment and you will see now the plan that we have far fewer parameters.
#c86	you;it;a moment;you;the plan;we;far fewer parameters
#s87	Specifically, we have N + K - 1 because we have M beta values and plus K minus one alpha values.
#c87	we;we;M;beta values
#s88	So that's just the basically that's basically the main idea of ordinal logistic regression.
#c88	the main idea;ordinal logistic regression
#s89	So now let's see how we can use such a method to actually assign ratings.
#c89	's;we;such a method;ratings
#s90	It turns out that with this.
#c90	It
#s91	Idea of tying all the parameters, the beta values, we also end up having a simpler way to make decisions, and more specifically now the criteria whether the predicted probabilities above is or at least .5 above and now is equivalent to whether the score of the object that is.
#c91	all the parameters;the beta values;we;a simpler way;decisions;the criteria;the predicted probabilities;whether the score;the object
#s92	Larger than or equal to negative of  alpha_j as shown here.
#c92	 alpha_j
#s93	Now the scoring function now is just taking linear combination of all the features weighted by beta values.
#c93	the scoring function;linear combination;all the features;beta values
#s94	So this means now we can simply make a desicion of rating by looking at the value of this scoring function and see which bracket it falls into.
#c94	we;a desicion;rating;the value;this scoring function;which bracket;it
#s95	Now you can see the General Decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object.
#c95	you;the General Decision rule;the score;the particular range;our values;we;the corresponding rating;that text object
#s96	So in sum, in this approach we're going to score the object.
#c96	sum;this approach;we;the object
#s97	By using the features and the parameter values, beta values.
#c97	the features;the parameter values;beta values
#s98	And this score will then be compared with a set of training the other values to see which range the score is in, and then using the range we can then decide which rating the object should be getting because these ranges of our values correspond to the different levels of ratings, and that's from the way we train these other values.
#c98	this score;a set;the other values;which range;the score;the range;we;the object;these ranges;our values;the different levels;ratings;the way;we;these other values
#s99	Each is tied to some level of reading.
#c99	some level;reading
410	3103be2f-681e-41cf-b0f7-21cf6ba56616	135
#s1	This lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis.
#c1	This lecture;a specific technique;contextual text mining;contextual probabilistic latent semantic analysis
#s2	In this lecture, we're going to continue discussing contextual text mining.
#c2	this lecture;we;contextual text mining
#s3	And we're going to introduce contextual probabilistic latent semantic analysis As an extension of PLSA for doing contextual text mining.
#c3	we;contextual probabilistic latent semantic analysis;an extension;PLSA;contextual text mining
#s4	Recall that in contextual text mining we hope to analyze topics in text.
#c4	contextual text mining;we;topics;text
#s5	In consideration of context so that we can associate the topics with appropriate context that we're interested in.
#c5	consideration;context;we;the topics;appropriate context;we
#s6	So in this approach contextual  probabilistic latent semantic analysis or CPLSA
#c6	this approach;CPLSA
#s7	The main idea is to explicitly add interesting context variables into a generated model.
#c7	The main idea;interesting context variables;a generated model
#s8	Recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics.
#c8	we;the text;we;we;some topics;words;some topics
#s9	But here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context.
#c9	we;context variables;the coverage;topics;also the content;topics;tight little context
#s10	Or in other words, we can do let the context influence both coverage and content of a topic.
#c10	other words;we;the context influence;both coverage;content;a topic
#s11	The consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in.
#c11	The consequences;us;contextualized topics;the topics;we;topics;a particular context;we
#s12	For example, a particular time period.
#c12	example
#s13	As extension of PLSA model, CPLSA mainly does the following changes.
#c13	extension;PLSA model;CPLSA;the following changes
#s14	Firstly it would model the conditional likelihood of text given context.
#c14	it;the conditional likelihood;text;context
#s15	That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model.
#c15	the generation;text;context;us;context;the generative model
#s16	Secondly, it makes 2 specific assumptions about the dependency of topics on context.
#c16	it;2 specific assumptions;the dependency;topics;context
#s17	One is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context.
#c17	the context;different time periods;different locations;we;different views;the topic;different versions;word distributions;a topic;this assumption;us;different variations;the same topic;different context
#s18	The other is that we assume.
#c18	we
#s19	The topic coverage also depends on the context.
#c19	The topic coverage;the context
#s20	And that means depending on the time or location, we might cover topics differently.
#c20	the time;location;we;topics
#s21	And then again this dependency would then allow us to capture the association of topics with specific context.
#c21	this dependency;us;the association;topics;specific context
#s22	We can still use the EM algorithm to solve the problem of parameter estimation.
#c22	We;the EM algorithm;the problem;parameter estimation
#s23	And in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context.
#c23	this case;the estimate premise;context variables;in particular a lot;conditional probabilities;topics;certain context
#s24	And this would allow us to do contextual text mining.
#c24	us;contextual text mining
#s25	So this is the basic idea.
#c25	the basic idea
#s26	Now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here.
#c26	we;time;this model;detail;references;you;more detail
#s27	I just want to explain the high level ideas in more detail, particularly willing to explain the generation process of text data that has context associated in such a model.
#c27	I;the high level ideas;more detail;the generation process;text data;context;such a model
#s28	So as you see here, we can assume there are still multiple topics.
#c28	you;we;multiple topics
#s29	For example, some topics might represent the themes like a government response donation or the city of New Orleans.
#c29	example;some topics;the themes;a government response donation;the city;New Orleans
#s30	Now this example is in the context of Hurricane Katrina and that hit New Orleans.
#c30	this example;the context;Hurricane Katrina;New Orleans
#s31	Now, as you can see, we assume there are different views associated with the each of the topics.
#c31	you;we;different views;the topics
#s32	And these are shown as view one, view two and view three Each view is a different version of word distributions.
#c32	view;three Each view;a different version;word distributions
#s33	And these views are tide to some context variables.
#c33	these views;tide;some context variables
#s34	For example, type to the location Texas or the time July 2005 or the occupation of the other being sociologist.
#c34	example;the location;Texas;the time;the occupation;the other being sociologist
#s35	Now on the right side you see now we assume the document has contact information, so the time is known to be July 2005, location is Texas, etc.
#c35	the right side;you;we;the document;contact information;the time;July;location;Texas
#s36	Now such context information is what we hope to model as well.
#c36	such context information;what;we
#s37	So we're not going to just model the text.
#c37	we;the text
#s38	And so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions.
#c38	one idea;the variations;topic content;different context;us;different views;the world distributions
#s39	Now on the bottom you will see the theme coverage or topic coverage might also vary according to these context.
#c39	the bottom;you;the theme coverage;topic coverage;these context
#s40	Because in the.
#s41	In the case of location like Texas, people might want to cover the red topics more at the new audience, as visualized here.
#c41	the case;location;Texas;people;the red topics;the new audience
#s42	But in a certain time period, maybe particular topic like donation will be covered more so this variation is also considered in CPLSA.
#c42	a certain time period;maybe particular topic;donation;this variation;CPLSA
#s43	So to generate such a document with context, we first also choose a view.
#c43	such a document;context;we;a view
#s44	And this view of course now could be from any of these contexts.
#c44	this view;course;these contexts
#s45	Let's say we have taken this view.
#c45	's;we;this view
#s46	That depends on the time in the middle.
#c46	the time;the middle
#s47	So now we have a specific version of word distributions.
#c47	we;a specific version;word distributions
#s48	Now you can see some probabilities of words for each topic.
#c48	you;some probabilities;words;each topic
#s49	Now, once we have chosen a view, now the situation will be very similar to what happened in standard PLSA.
#c49	we;a view;the situation;what;standard PLSA
#s50	We assume we have got a word distribution associated with each topic, right?
#c50	We;we;a word distribution;each topic
#s51	And then next to the view we choose a coverage from the bottom.
#c51	the view;we;a coverage;the bottom
#s52	So we're going to choose particular coverage and that coverage.
#c52	we;particular coverage;that coverage
#s53	Before is fixed in PLSA and it's hard to a particular document.
#c53	PLSA;it;a particular document
#s54	Each document has just one coverage distribution.
#c54	Each document;just one coverage distribution
#s55	Now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage.
#c55	we;context;the distribution;topics;the coverage;topics;the context;the coverage
#s56	So, for example, we might pick a particular coverage, let's say in this case.
#c56	example;we;a particular coverage;'s;this case
#s57	We pick We've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in PLSA.
#c57	We;We;the document;specifically coverage;the coverage;these word distributions;we;the document;exactly the same way;PLSA
#s58	So what it means we're going to use the coverage to choose a topic to choose one of these three topics.
#c58	what;it;we;the coverage;a topic;these three topics
#s59	Let's say we have picked up, let's say, the yellow topic, then withdraw a word from this particular topic on the top.
#c59	's;we;'s;the yellow topic;a word;this particular topic;the top
#s60	So we might get the word like government.
#c60	we;the word;government
#s61	And then next time we might choose a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in PLSA.
#c61	we;a different topic;we;all the words;the same process;PLSA
#s62	Now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice.
#c62	the main difference;we;the coverage;the word distributions;we;the context;our choice
#s63	So in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage.
#c63	other words;we;extra switches;this context;the choices;different views;topics;choices;coverage
#s64	And naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics.
#c64	the model;more parameters;we;those parameters;the context;we;the context;specific views;topics;context;specific coverages;topics
#s65	And this is precisely what we want in contextual text mining.
#c65	precisely what;we;contextual text mining
#s66	So here are some sample results from using such a model.
#c66	some sample results;such a model
#s67	Not necessary exactly the same model, but similar models.
#c67	exactly the same model;similar models
#s68	So on this slide you see some sample results of comparing news articles about Iraq war and Afghanistan war.
#c68	this slide;you;some sample results;news articles;Iraq war;Afghanistan war
#s69	Now we have about 30 articles.
#c69	we;about 30 articles
#s70	On Iraq war and 26 articles on Afghanistan war.
#c70	Iraq war;26 articles;Afghanistan war
#s71	Now, in this case, the goal is to.
#c71	this case;the goal
#s72	To review the common topics covered in both sets, articles and the differences or variations of the topic in each of the two collections.
#c72	the common topics;both sets;articles;the differences;variations;the topic;the two collections
#s73	So in this case, the context that is explicitly specified by the topical collection.
#c73	this case;the topical collection
#s74	And we see the results here show that.
#c74	we;the results
#s75	There is a common theme that's corresponding to cluster around here in this column.
#c75	a common theme;this column
#s76	That there is a common theme indicating that United Nations is involved in both wars is a common topic covered in both sets of articles, and that's indicated by the high probability words shown here United Nations.
#c76	a common theme;United Nations;both wars;a common topic;both sets;articles;the high probability words;United Nations
#s77	Now if you the background, of course this is not surprising and this is.
#c77	you;course
#s78	This topic is indeed very relevant, to both wars.
#c78	This topic;both wars
#s79	If you look at the column further and what's interesting is that the next two cells of word distributions actually tell US collection specific variations of the topic of United Nations.
#c79	you;the column;what;the next two cells;word distributions;US;collection specific variations;the topic;United Nations
#s80	So it indicates that in Iraq war, United Nations was more involved in weapon inspections, whereas in Afghanistan war it was more involved in maybe aid to Northern Alliance as a different variation of the topic of United Nations.
#c80	it;Iraq war;United Nations;weapon inspections;Afghanistan war;it;maybe aid;Northern Alliance;a different variation;the topic;United Nations
#s81	So this shows that by bringing the context, in this case, different wars are different collections of text.
#c81	the context;this case;different wars;different collections;text
#s82	We can have topic variations, tied to these contexts to review the differences of coverage of United Nations in the two wars.
#c82	We;topic variations;these contexts;the differences;coverage;United Nations;the two wars
#s83	Similarly, if you look at the second cluster.
#c83	you;the second cluster
#s84	Cluster 2 has to do with the killing of people and again.
#c84	Cluster;the killing;people
#s85	Not surprising if you know the background about wars or the wars involved killing of people.
#c85	you;the background;wars;the wars;killing;people
#s86	But imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles.
#c86	you;the text collections;a lot;text articles;such a technique;the common topics;both sets;articles
#s87	It can be used to review common topics in multiple sets of articles as well.
#c87	It;common topics;multiple sets;articles
#s88	If you look down, of course in that column of cluster 2 you see variations of killing of people and that correspond to in different different contexts.
#c88	you;course;that column;cluster;you;variations;killing;people;different different contexts
#s89	And here is another example of results.
#c89	another example;results
#s90	Obtain the front block articles about the Hurricane Katrina.
#c90	the front block articles;the Hurricane Katrina
#s91	Now in this case, what you see here is visualization of the. trends of topics overtime.
#c91	this case;what;you;visualization;the. trends;topics overtime
#s92	And the top one shows just the temporal chains of two topics.
#c92	just the temporal chains;two topics
#s93	One is oil price and one is.
#c93	oil price;one
#s94	about the flooding of the city.
#c94	the flooding;the city
#s95	New Orleans.
#c95	New Orleans
#s96	This these topics are obtained from block articles about the Hurricane Katrina.
#c96	these topics;block articles;the Hurricane Katrina
#s97	And people talked about these topics.
#c97	people;these topics
#s98	And in addition to some other topics.
#c98	addition;some other topics
#s99	But the visualization shows that with this technique that we can have conditional distribution of time given a topic.
#c99	the visualization;this technique;we;conditional distribution;time;a topic
#s100	So this allows us to plot this conditional probability.
#c100	us;this conditional probability
#s101	General curves like what you're seeing here.
#c101	General curves;what;you
#s102	We see that initially the two curves tracked each other very well.
#c102	We;the two curves
#s103	But later we see the topic of New Orleans was mentioned again but oil price was not and This turns out to be the time period when another Hurricane Hurricane Rita hit the region that apparently tricked more discussion about the flooding of the city.
#c103	we;the topic;New Orleans;oil price;the time period;another Hurricane Hurricane Rita;the region;more discussion;the flooding;the city
#s104	The bottom curve shows the coverage of this topic about the flooding of the city by block articles in different locations and also shows some shift of coverage.
#c104	The bottom curve;the coverage;this topic;the flooding;the city;block articles;different locations;some shift;coverage
#s105	That might be related to peoples migrating from the state of Louisiana to Texas, for example.
#c105	peoples;the state;Louisiana;Texas;example
#s106	So in this case we can see the time can be used as context to reveal trends of topics.
#c106	this case;we;the time;context;trends;topics
#s107	This is some additional result on special patterns and this.
#c107	some additional result;special patterns
#s108	In this case it's about the topic of government response.
#c108	this case;it;the topic;government response
#s109	And there was some criticism about the slow response of government in the case of Hurricane Katrina, and discussion now is covered in different locations and these visualizations show the coverage in different weeks of the event, and initially it's covered mostly in the victim states in the South, but then gradually it's spreading to other.
#c109	some criticism;the slow response;government;the case;Hurricane Katrina;discussion;different locations;these visualizations;the coverage;different weeks;the event;it;the victim states;the South;it
#s110	Locations, but in week four, which is shown on the bottom on the left, we see a pattern that's very similar to the very first week on the top left, and that's why again the hurricane Rita hit the region.
#c110	week;the bottom;the left;we;a pattern;the very first week;the top left;the hurricane Rita;the region
#s111	So such a technique would allow us to use location as context to examine variations of topics.
#c111	such a technique;us;location;context;variations;topics
#s112	And of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns.
#c112	course;the model;you;any other collections;text;spatial temporal patterns
#s113	Is yet another application of this kind of model where we look at the use of the model for event impact analysis.
#c113	another application;this kind;model;we;the use;the model;event impact analysis
#s114	So here we are looking at the research articles in information retrieval, IR, particularly SIGIR papers.
#c114	we;the research articles;information retrieval;particularly SIGIR papers
#s115	And the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left.
#c115	the topic;we;the retrieval models;you;the top word;top words;high probability;this model;the left
#s116	And then we hope to examine the impact of two events.
#c116	we;the impact;two events
#s117	One is the start of TREC for text retrieval conference.
#c117	the start;TREC;text retrieval conference
#s118	This is a major evaluation effort sponsored by US government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information retrieval.
#c118	a major evaluation effort;US government;that time;an impact;the topics;research information retrieval
#s119	The other is the publication of a Seminal paper by Croft and Ponte, and this is about the language modeling approach to information retrieval.
#c119	the publication;a Seminal paper;Croft;Ponte;the language modeling approach;information retrieval
#s120	It's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divide the time periods into a period before the event an another after this event, and then we can compare the differences of.
#c120	It;a high impact;information retrieval research;we;this kind;model;impact;the idea;the time;context;an use;these events;the time periods;a period;the event;this event;we;the differences
#s121	The topics, the coverage and variations, etc.
#c121	The topics;the coverage;variations
#s122	So in this case the results show I've seen that before TREC the study of retrieval models was mostly a vector space model, Boolean model, etc.
#c122	this case;the results;I;TREC;the study;retrieval models;a vector space model;Boolean model
#s123	But the after TREC.
#c123	TREC
#s124	Apparently the study of retrieval models have involved a lot of other words that seem to suggest some different retrieval tasks though.
#c124	the study;retrieval models;a lot;other words;some different retrieval tasks
#s125	So for example email was used in the enterprise search tasks and subtropical retrieval, with another task introduced later by TREC.
#c125	example;email;the enterprise search tasks;subtropical retrieval;another task;TREC
#s126	On the bottom we see the variations that are correlated with the publication of the language model paper.
#c126	the bottom;we;the variations;the publication;the language model paper
#s127	Before we have those classical probabilistic model logic model, Boolean model etc.
#c127	we;those classical probabilistic model logic model
#s128	But after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc.
#c128	we;clear dominance;language model;probabilistic models;we;words;a language model;estimation;parameters
#s129	So this technique here can use event as context.
#c129	this technique;event;context
#s130	To understand the impact of event again, the technique is general so you can use this to analyze the impact of any event.
#c130	the impact;event;the technique;you;the impact;any event
#s131	Here are some suggested readings.
#c131	some suggested readings
#s132	The first is paper about simple extension of PLSA to enable cross collection comparison.
#c132	paper;simple extension;PLSA;collection comparison
#s133	It's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection.
#c133	It;comparative text mining;us;the common topics;multiple collections;their variations;each collection
#s134	The second one is the main paper about the CPLSA model with a discussion of a lot of applications.
#c134	The second one;the main paper;the CPLSA model;a discussion;a lot;applications
#s135	The third one has a lot of details about this special temporal patterns for Hurricane Katrina, example.
#c135	The third one;a lot;details;this special temporal patterns;Hurricane Katrina
410	347e8785-e040-43a6-b198-2dcb12a2ce2a	8
#s1	this letter is about the specific a smoothing methods for language models used in probabilistic of retrieval model in this laptop we will continue the discussion of language models for information retrieval particular the query like rover retrieval method and wouldn't talk about the specific a small the methods used for such a retrieval function so this is a slide from a previous lecture where we show that with the query like holder ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following so this is the how the retrieval function based on this assumption is that we have discussed you can see it's a sum of all the magic query terms here and inside of some it's the count of determine the query and some weight for the term in the document and we have tearful idea of wait here
#c1	this letter;about the specific a smoothing methods;language models;probabilistic;retrieval model;this laptop;we;the discussion;language models;information retrieval;the query;rover retrieval method;the specific a small the methods;such a retrieval function;a slide;a previous lecture;we;the query;holder ranking;the collection language model;we;a retrieval function;the following;the retrieval function;this assumption;we;you;it;a sum;all the magic query terms;it;the count;the query;some weight;the term;the document;we;tearful idea;wait
#s2	and then we have another constant that here and so clearly if we want to implement this function using a programming language we still need to figure out a few variables in particular we're going to need to know how to estimate the probability of world
#c2	we;another constant;we;this function;a programming language;we;a few variables;we;the probability;world
#s3	exactly and how do we set off on so in order to answer these questions we have to think about this very specific smoothing methods and that is the main topic of this lecture wouldn't talk about the tools move methods the first is simple linear interpolation with fixed coefficient and this is also halted jelinek immersive smoothie so the idea is actually very simple this picture shows how we estimate document the language model by using maximum like flores method that gives us ward accounts normalized by the total number of words in the text the idea of using this method is to maximize the probability of the opposite of the text as a result if a word like network is not observing the text it's going to get zero probability as soon here so the idea of smoothing then is to rely on collection language model where this world is not going to have a zero probability to help us decide what non zero probability should be assigned to such a world so we can note that network has a non zero probability here so in this approaching what we do is we do linear interpolation between the maximum like harassment here and the collection language model and this is controlled by the smoothing parameter lambda which is between zero and one so this is a smoothing parameter the larger lemme lisa to the more smoothing we have we will have so by mixing them together we achieve the goal of assigning larger probabilities to award night network so let's see how it works for some of the words here for example if we compute the smooth probability for text now the maximum micro S made gives us ten over one hundred and that's going to be here but the collection probability is this so we just combine them together with this simple formula we can also see the word network which used to have a zero probability now is getting a non zero probability of this value and that's be cause the count is going to be zero four network here but this spot is now zero and that's basically how this method works if you think about this and you can easy to see now the alpha sub T in this model method is basically lemma because that's remember the coefficient in front of the probability of the world came in by the clashing language model here
#c3	we;order;these questions;we;this very specific smoothing methods;the main topic;this lecture;simple linear interpolation;fixed coefficient;jelinek immersive smoothie;the idea;this picture;we;the language model;maximum;flores;method;us;ward accounts;the total number;words;the text;the idea;this method;the probability;the opposite;the text;a result;a word;network;the text;it;zero probability;the idea;smoothing;collection language model;this world;a zero probability;us;what;non zero probability;such a world;we;network;a non zero probability;what;we;we;linear interpolation;the maximum like harassment;the collection language model;the smoothing parameter;lambda;a smoothing parameter;the larger lemme lisa;the more smoothing;we;we;them;we;the goal;larger probabilities;night network;'s;it;the words;example;we;the smooth probability;text;the maximum micro S;us;the collection probability;we;them;this simple formula;we;the word network;a zero probability;a non zero probability;this value;the count;zero four network;this spot;this method;you;you;the alpha;sub T;this model method;the coefficient;front;the probability;the world;the clashing language model
#s4	right OK so this is the first model method the signal one is similar but it has a dynamical coefficient for linear interpolation is often called the original apply or bayesian smoothie so again here we face the problem of zero probability for unseen world like a network again we will use the collection language model but in this case we're going to combine them in somewhat different ways the formula first can be seen as an interpolation of the left my car is made and the collection and model as before as in the james multimethod only that the coefficient now is not a lambda a fixed number but that dynamically coefficient in this form where mu is a parameter it's a non negative value and you can see if we set mutua constant the fact is that a long document with actually get a smaller coefficient in here because a long document we have longer length therefore the coefficient is actually smaller and so a long document would have less smoothing as we would expect so this since to make more sense than a fixed for coefficient smoothie of course this pot would be of this form so that the two coefficients would someone now this is one way to understand that this is more
#c4	the first model method;the signal one;it;a dynamical coefficient;linear interpolation;the original apply;bayesian smoothie;we;the problem;zero probability;unseen world;a network;we;the collection language model;this case;we;them;somewhat different ways;the formula;an interpolation;the left;my car;the james multimethod;the coefficient;a lambda;a fixed number;this form;mu;a parameter;it;a non negative value;you;we;mutua;the fact;a long document;a smaller coefficient;a long document;we;longer length;the coefficient;so a long document;we;more sense;coefficient smoothie;course;this pot;this form;the two coefficients;someone;one way
#s5	the basically it means it's a dynamical coefficient interpolation there is another way to understand this formula which is even easier to remember and that's on this side
#c5	it;it;a dynamical coefficient interpolation;another way;this formula;this side
#s6	so it's easy to see we can rewrite the smoothing method in this form now in this form we are easy to see what change we have made it to the maximum micro estimator which would be this part right so normalize account by the documents so in this form we can see what we did is we add this to the count of every word so what does this mean well this is basically something related to the probability of the world in the collection
#c6	it;we;the smoothing method;this form;this form;we;what change;we;it;the maximum micro estimator;this part;account;the documents;this form;we;what;we;we;the count;every word;what;something;the probability;the world;the collection
#s7	and we multiply that by the parameter mu and when we combine this with the count here essentially we are adding pseudocounts to the observed that text we pretend every word has got this many pseudo count so the total count would be the sum of these pseudo counts and the actual count of the war in the document as a result in total we would have added at this many pseudocounts why because if you take a some of this this one overall the words then we see the probability of the words would someone
#c7	we;the parameter mu;we;the count;we;pseudocounts;the observed;that text;we;every word;this many pseudo count;the total count;the sum;these pseudo counts;the actual count;the war;the document;a result;total;we;this many pseudocounts;you;the words;we;the probability;the words;someone
#s8	and that gives us just mute so this is a total number of pseudo comes that we added and so these probabilities would still something to one so in this case we can easily see the method is essentially too added these as a pseudo count to this day to pretend we actually augment the data by including some pseudo there are defined by the collection language model as a result we have more counts based the total counts for word award it would be like this and as a result evening for word has zero counter here let's say if we have zero come here and it would still have non zero account cause of this part so this is how this method works let's also take a look at the some specific example here i thought for text again will have ten as original count that we actually observe but we also added some pseudo count and so the probability of texture would be of this form naturally the probability of network would be just this part and so here you can also see what's offers up the ear can you see it if you want to think about you can pause the video have you notice that this part is basically of a sub D so we can see this case other submitters depend on the document because this length depends on the document whereas in the linear interpolation that james move method that this is a constant
#c8	us;a total number;pseudo;we;these probabilities;this case;we;the method;a pseudo count;this day;we;the data;some pseudo;the collection language model;a result;we;more counts;the total counts;word award;it;a result evening;word;zero counter;'s;we;it;non zero account cause;this part;this method;'s;a look;the some specific example;i;text;original count;we;we;some pseudo count;the probability;texture;this form;the probability;network;just this part;you;what;the ear;you;it;you;you;the video;you;this part;a sub D;we;this case;other submitters;the document;this length;the document;the linear interpolation
410	380a7417-6702-4df8-9818-5aceba7cde2b	68
#s1	This lecture is  about topic mining and analysis.
#c1	This lecture;topic mining;analysis
#s2	"We "As you see on this roadmap, we have just "we have just "about the language namely discovery of word  associations such as paradigmatic relations relations and syntagmatic relations.
#c2	"We;you;this roadmap;we;we;the language;namely discovery;word;  associations;paradigmatic relations relations;syntagmatic relations
#s3	Now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about.
#c3	this lecture;we;another kind;knowledge;content mining;knowledge
#s4	The main topics.
#c4	The main topics
#s5	In the text.
#c5	the text
#s6	And we call that topic mining and analysis.
#c6	we;that topic mining;analysis
#s7	In this lecture we're going to talk about its motivation and the task definition.
#c7	this lecture;we;its motivation;the task definition
#s8	So first, let's look at the concept of topic.
#c8	's;the concept;topic
#s9	So topic is something that we all understand, I think, but it's actually not that easy to formally define it.
#c9	topic;something;we;I;it;it
#s10	Roughly speaking, topic is the main idea discussed in text data, and you can think of this as a theme or subject of discussion or conversation.
#c10	topic;the main idea;text data;you;a theme;subject;discussion;conversation
#s11	It can also have different granularities.
#c11	It;different granularities
#s12	For example, we can talk about the topic of a sentence.
#c12	example;we;the topic;a sentence
#s13	A topic of article topic of a paragraph, or the topics of all the research articles in the digital library.
#c13	A topic;article topic;a paragraph;the topics;all the research articles;the digital library
#s14	So different granularities of topics obviously have different applications.
#c14	different granularities;topics;different applications
#s15	Indeed, there are many applications that require discovery of topics in text and then analyze them.
#c15	many applications;discovery;topics;text;them
#s16	Here are some examples.
#c16	some examples
#s17	For example, we might be interested in knowing what are Twitter users talking about today?
#c17	example;we;what;Twitter users;today
#s18	Are they talking about NBA sports or talking about some international events, etc.
#c18	they;NBA sports;some international events
#s19	Or we are interested in knowing about the research topics.
#c19	we;the research topics
#s20	For example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago.
#c20	example;one;what;the current research topics;data mining;they
#s21	Now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past.
#c21	discovery;topics;data mining;literatures;we;topics;today's literature;the past
#s22	And then we can make a comparison.
#c22	we;a comparison
#s23	We might be also interested in knowing what do people like about some product like iPhone 6 and what do they dislike?
#c23	We;what;people;some product;iPhone;what;they
#s24	And this involves discovering topics in positive opinions about iPhone 6 and also negative reviews about it.
#c24	topics;positive opinions;iPhone;also negative reviews;it
#s25	Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election?
#c25	we;what;the major topics;2012 presidential election
#s26	And all these have to do is discovering topics in texts and analyzing them, and we're going to talk about a lot of techniques for doing this.
#c26	topics;texts;them;we;a lot;techniques
#s27	In general, we can view topic as some knowledge about the world.
#c27	we;topic;some knowledge;the world
#s28	So from text that we expected to discover a number of topics and then this topic generally provide the description about the world and it tells us something about the world, about the product, about the person, etc.
#c28	text;we;a number;topics;this topic;the description;the world;it;us;something;the world;the product;the person
#s29	Now when we have some non-text data then we can have more context for analyzing the topics.
#c29	we;some non-text data;we;more context;the topics
#s30	For example, we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc.
#c30	example;we;the time;the text data;locations;the text data;the authors;text;the sources;the text
#s31	All such meta data or context variables can be associated with the topics that we discover.
#c31	All such meta data or context variables;the topics;we
#s32	And then we can use these context variables to help us analyze patterns of topics.
#c32	we;these context variables;us;patterns;topics
#s33	For example, looking at topics overtime, we would be able to discover whether there's a trending topic or some topics might be fading away.
#c33	example;topics;we;a trending topic;some topics
#s34	Similar looking at the topics in different locations, we might know some insights about people's opinions in different locations.
#c34	the topics;different locations;we;some insights;people's opinions;different locations
#s35	So that's why mining topics is very important.
#c35	mining topics
#s36	Now let's look at the tasks of topic mining and analysis.
#c36	's;the tasks;topic mining;analysis
#s37	In general, it would involve first discovering a lot of topics.
#c37	it;a lot;topics
#s38	In this case K topics.
#c38	this case
#s39	And then we also would like to know which topics are covered in which documents, to what extent.
#c39	we;topics;which documents;what extent
#s40	So for example in.
#c40	example
#s41	Document one we might see that topic 1 is covered a lot, topic 2 and topic k are covered with a small portion.
#c41	we;that topic;topic;topic k;a small portion
#s42	And other topics perhaps are not covered.
#c42	other topics
#s43	Document 2, on the other hand,  covered topic 2 very well, but it did not cover topic 1 at all and also covers topic K to some extent.
#c43	Document;the other hand;covered topic;it;topic;topic K;some extent
#s44	etc., right?
#s45	So now you can see there are generally two different tasks or subtasks.
#c45	you;two different tasks;subtasks
#s46	The first is to discover K topics from a collection of text data.
#c46	K topics;a collection;text data
#s47	What are these K topics?
#c47	What;these K topics
#s48	OK, major topics in the text data.
#c48	OK, major topics;the text data
#s49	The second task is to figure out which documents cover which topics to what extent.
#c49	The second task;which documents;what extent
#s50	So more formally, we can define the problem as follows.
#c50	we;the problem
#s51	First, we have as input a collection of N text documents.
#c51	we;a collection;N text documents
#s52	Here we can denote that text connection.
#c52	we;that text connection
#s53	as C.
#c53	C.
#s54	And denote a text article as di
#c54	a text article
#s55	and we generally also need to have as input the number of topics
#c55	we;the number;topics
#s56	K.
#c56	K.
#s57	But there may be techniques that can automatically suggest a number of topics, but in the techniques that we will discuss which are also the most useful techniques, we often need to specify a number of topics.
#c57	techniques;a number;topics;the techniques;we;the most useful techniques;we;a number;topics
#s58	Now the output would then be the K topics that we would like to discover denoted as theater sub one through theta sub k.
#c58	the output;the K topics;we;theater sub;theta sub;k.
#s59	Also we want to generate the coverage of topics in each document d
#c59	we;the coverage;topics;each document
#s60	sub i
#s61	and this is denoted by π sub i j and "π sub i j document d sub i covering topic theta sub j.
#c61	π sub;i;i;j;document;i;topic theta
#s62	So obviously for each document we have a set of such values indicate.
#c62	each document;we;a set;such values
#s63	To what extent did the document covers each topic.
#c63	what extent;the document;each topic
#s64	An we can assume that these probabilities sum to one, because a document won't be able to cover other topics outside the topics that we discussed we discovered.
#c64	we;these probabilities;a document;other topics;the topics;we;we
#s65	So now the question is how do we define theta sub i?
#c65	the question;we;theta sub;i
#s66	How do we define the topic now?
#c66	we;the topic
#s67	This problem has not been completely defined until we define what is exactly theta.
#c67	This problem;we;what
#s68	So in the next a few lectures, we're going to talk about different ways to define theta.
#c68	the next a few lectures;we;different ways;theta
410	384d42d5-644d-4649-b3be-c744c3d84e1a	176
#s1	This lecture is about the feedback in the language modeling approach.
#c1	This lecture;the feedback;the language modeling approach
#s2	In this lecture we will continue the discussion of feedback in text retrieval.
#c2	this lecture;we;the discussion;feedback;text retrieval
#s3	In particular, we're going to talk about the feedback in language modeling approaches.
#c3	we;the feedback;language modeling approaches
#s4	So we derive the query likelihood ranking function by making various assumptions.
#c4	we;the query likelihood ranking function;various assumptions
#s5	As a basic retrieval function, that formula of those formulas worked well, but if we think about the feedback information, it's a little bit of awkward to use query like hold too.
#c5	a basic retrieval function;that formula;those formulas;we;the feedback information;it;a little bit;query;hold
#s6	Perform feedback because a lot of times the feedback information is additional information about the query.
#c6	feedback;a lot;times;the feedback information;additional information;the query
#s7	But we assume that the query is generated by assembling words from a language model in the query likelihood method.
#c7	we;the query;words;a language model;the query likelihood method
#s8	It's kind of a natural to sample words that form feedback documents as a result, then researchers proposed a way to generalize query like hold function and it's called a callback labeler divergance retrieval model.
#c8	It;kind of a natural;words;feedback documents;a result;researchers;a way;query;hold function;it;a callback labeler divergance retrieval model
#s9	And this model is actually going to make the query likelihood retrieval function much closer to vector space model.
#c9	this model;the query likelihood retrieval function;vector space model
#s10	Yet, this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query likelihood as a special case.
#c10	this form;the language model;a generalization;query;hold;the sense;it;query likelihood;a special case
#s11	And in this case, then, feedback can be achieved through simple query model estimation or updating.
#c11	this case;feedback;simple query model estimation;updating
#s12	This is very similar to Rock Hill which updates the query vector.
#c12	Rock Hill;the query vector
#s13	So let's see.
#c13	's
#s14	What is this KL Divergence retrieval model?
#c14	What;this KL Divergence retrieval model
#s15	So on the top, what you see is a query like hold.
#c15	the top;what;you;a query;hold
#s16	Retrieval function right this one.
#c16	Retrieval function
#s17	And then kill diversions or also called cross entropy.
#c17	diversions;cross entropy
#s18	Whichever model is basically to generalize.
#c18	Whichever model
#s19	The frequency part here.
#c19	The frequency part
#s20	Into a language model.
#c20	a language model
#s21	So basically it's the difference.
#c21	it;the difference
#s22	Given by the probabilistic model here to characterize what the user is looking for versus the count of query words there.
#c22	the probabilistic model;what;the user;the count;query words
#s23	And this difference allows us to plug in various different ways to estimate this, so this can be estimated in many different ways, including using feedback information.
#c23	this difference;us;various different ways;many different ways;feedback information
#s24	Now this is called KL divergence because This can be interpreted as measuring the care divergent of two distributions.
#c24	KL divergence;the care divergent;two distributions
#s25	One is the query model denoted by this distribution.
#c25	the query model;this distribution
#s26	One is talking the language model.
#c26	the language model
#s27	Here smoothly with collection language model, of course an.
#c27	collection language model;course
#s28	We're not going to talk about the detail of and then find it in some references.
#c28	We;the detail;it;some references
#s29	It's also called a cross entropy, because in effect we can ignore some terms in the care divergent function and we will end up having actually cross entropy that both our terms in information theory.
#c29	It;a cross entropy;effect;we;some terms;the care divergent function;we;entropy;both our terms;information theory
#s30	But anyway for.
#s31	Our purpose here you can just receive the two formulas look almost identical, except that here we have a probability of award given by a query language model.
#c31	you;the two formulas;we;a probability;award;a query language model
#s32	All this.
#s33	And here the sum is over all the words that are in the document and also with the non zero probability for the query model.
#c33	the sum;all the words;the document;the non zero probability;the query model
#s34	So it's kind of again a generalization of some overall the match query words.
#c34	it;a generalization;some overall the match query words
#s35	Now you can also easy to see we can recover the query like hold retrieval function by simply setting this query model to the relative frequency of award in the query.
#c35	you;we;the query;retrieval function;this query model;the relative frequency;award;the query
#s36	This is very easy to see once you plug this into.
#c36	you
#s37	Here you can eliminate this query Lance.
#c37	you;this query
#s38	That's a constant and then you get exactly like that.
#c38	you
#s39	So you can see the equivalence.
#c39	you;the equivalence
#s40	And that's also why this care divergent model can be regarded as a generalization of query, like whole because we can cover query like Rd as a special case.
#c40	this care divergent model;a generalization;query;we;query;Rd;a special case
#s41	But it would also allow such rule much more than that.
#c41	it;such rule
#s42	So this is how we can use the care divergent model to them the feedback the picture shows that we first estimate the document language model.
#c42	we;the care divergent model;them;the feedback;the picture;we;the document language model
#s43	Then we estimate the query name model and we compute the KL diversions, as is often denoted by AD here.
#c43	we;the query name model;we;the KL diversions;AD
#s44	But this basically means this is exactly like a vector space model 'cause we computer vector for the document the computer, another vector for the query
#c44	a vector space model;we;vector;the document;the computer;another vector;the query
#s45	and then we compute the distance only that these vectors are of special forms their probability distributions.
#c45	we;the distance;these vectors;special forms;their probability distributions
#s46	And then we got the results and we can find some feedback documents.
#c46	we;the results;we;some feedback documents
#s47	Let's assume they are most inactive.
#c47	's;they
#s48	Sorry, mostly positive documents, although we could also consider both kinds of documents.
#c48	Sorry, mostly positive documents;we;both kinds;documents
#s49	So what we could do is like in rock you'll ever know computer another language model, coder feedback, language model here.
#c49	what;we;rock;you;computer;another language model;feedback;language model
#s50	Again, this is going to be another vector, just like a computing central about the in Rock Hill.
#c50	another vector;a computing central;Rock Hill
#s51	And then this model can be combined with the original query model.
#c51	this model;the original query model
#s52	Using a linear interpolation.
#c52	a linear interpolation
#s53	And this would then give us a update model just like again in Rock Hill.
#c53	us;a update model;Rock Hill
#s54	So here we can see the parameter Alpha can control the amount of feedback if it's set to 0, then it says here there's no feedback after set to one, we got 4 feedback.
#c54	we;the parameter;Alpha;the amount;feedback;it;it;no feedback;we;4 feedback
#s55	If we ignore the original query and this is generally not desirable, right?
#c55	we;the original query
#s56	So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important.
#c56	you;you;a lot;relevant documents;the query terms
#s57	So of course the main question here is how do you compute this single F?
#c57	course;the main question;you;this single F
#s58	This is the big question here, and once you can do that, the rest is easy.
#c58	the big question;you;the rest
#s59	So here we will talk about one of the approaches and there are many approaches.
#c59	we;the approaches;many approaches
#s60	Of course this approach is based on generated model.
#c60	this approach;generated model
#s61	And I'm going to show you how it works.
#c61	I;you;it
#s62	This is the user generated mixable.
#c62	the user
#s63	So this picture shows that we have this model.
#c63	this picture;we;this model
#s64	Here.
#s65	The feedback model that we want to estimate.
#c65	The feedback model;we
#s66	And will the basis is the feedback documents.
#c66	the basis;the feedback documents
#s67	Let's say we are observing the positive documents.
#c67	's;we;the positive documents
#s68	These are the click the documents by users or relevant documents judged by users or simply top ranked blocking that we assume to be relevant.
#c68	the click;the documents;users;relevant documents;users;we
#s69	Now imagine how we can compute the centroid for these documents by using language Model 1.
#c69	we;the centroid;these documents;language Model
#s70	Approach is simply to assume these documents are generated from this language model as we did before.
#c70	Approach;these documents;this language model;we
#s71	What we could do is do just normalize the word frequency here
#c71	What;we;the word frequency
#s72	and then we got this world distribution.
#c72	we;this world distribution
#s73	Now the question is whether this distribution is good for feedback.
#c73	the question;this distribution;feedback
#s74	But you can imagine.
#c74	you
#s75	The top ranked the world would be what?
#c75	The top;the world;what
#s76	What do you think?
#c76	What;you
#s77	Those words would be common words, right?
#c77	Those words;common words
#s78	As we always see in a language model, the top ranked words are actually common words like the etc.
#c78	we;a language model;the top ranked words;common words
#s79	So it's not very good for feedback because we would be adding a lot of such words to our query when we interpret this with the original query model.
#c79	it;feedback;we;a lot;such words;our query;we;the original query model
#s80	So this is not good.
#s81	So when it do something in particular will are trying to get rid of those common words and we are we have seen actually one way to do that by using background language model.
#c81	it;something;those common words;we;we;one way;background language model
#s82	In the case of learning the associations with words, words that are related to the water computer.
#c82	the case;the associations;words;words;the water computer
#s83	We could do that and there will be another way to do this, but here we are going to talk about another approach which is more principled approach.
#c83	We;another way;we;another approach;more principled approach
#s84	In this case, we're going to state, well, you said that there are common words here in this.
#c84	this case;we;state;you;common words
#s85	These documents that should not belong to this topic model, right?
#c85	this topic model
#s86	So now what we can do is to assume that, well, those words are generated from background language model, so they were generated those words like the.
#c86	what;we;those words;background language model;they;those words
#s87	Example.
#c87	Example
#s88	And if we use maximum likelihood estimator, note that if all the words here must be generated from this model, then.
#c88	we;maximum likelihood estimator;all the words;this model
#s89	This model is forced to assign high probabilities to award like that because it occurs so frequently here.
#c89	This model;high probabilities;it
#s90	Note that in order to reduce its probability in this model.
#c90	order;its probability;this model
#s91	We have to have another model which is this one to help explain the world.
#c91	We;another model;this one;the world
#s92	The here and in this case it's not appropriate to use the background language model to achieve this goal, because this model would assign high probabilities to these common words.
#c92	this case;it;the background language model;this goal;this model;high probabilities;these common words
#s93	So in this approach, then we assume this machine that was generated.
#c93	this approach;we;this machine
#s94	These words would work as follows.
#c94	These words
#s95	We have a source controller here.
#c95	We;a source controller
#s96	Imagine we flip a coin here to decide what distribution to use with probability of Lambda.
#c96	we;a coin;what distribution;probability;Lambda
#s97	The coin shows up as head and we're going to use the background language model and we can do that simple word from that model with probability of 1 minus them.
#c97	The coin;head;we;the background language model;we;that simple word;that model;probability;them
#s98	Now will do decide to use the unknown.
#s99	Topic model here that we would like to estimate and we're going to then generate award here.
#c99	Topic model;we;we;award
#s100	If we make this assumption and this whole thing would be just one model and we call this mixture model 'cause there are two distributions that are mixed together and we actually don't know when each distribution is used.
#c100	we;this assumption;this whole thing;just one model;we;this mixture model;two distributions;we;each distribution
#s101	So again, think of this whole thing as one model.
#c101	this whole thing;one model
#s102	And we can still ask for words, and it will still give us a word in a random manner, right?
#c102	we;words;it;us;a word;a random manner
#s103	And of course, which word will show up would depend on both this distribution and that distribution.
#c103	course;which word;both this distribution;that distribution
#s104	In addition, it would also depend on this Lambda, because if you say Lambda is very high and it's going to always use the background distribution, you'll get different words than if you say lemme's very small, we're going to use this.
#c104	addition;it;this Lambda;you;Lambda;it;the background distribution;you;different words;you;lemme;we
#s105	Right, so all these are parameters.
#c105	parameters
#s106	In this model.
#c106	this model
#s107	And then if you think in this way, basically we can do exactly the same as what we did before.
#c107	you;this way;we;what;we
#s108	We are going to use maximum likelihood estimator to adjust this model to estimate the parameters.
#c108	We;maximum likelihood estimator;this model;the parameters
#s109	Basically we're going to adjust well this parameter.
#c109	we
#s110	So that we can best explain all the data.
#c110	we;all the data
#s111	The difference now is that we are not asking this model alone to explain this.
#c111	The difference;we;this model
#s112	But rather, we're going to ask this whole model mixture model to explain the data becauses there has got some help from the background model.
#c112	we;this whole model mixture model;the data becauses;some help;the background model
#s113	It doesn't have to assign high probabilities towards like the.
#c113	It;high probabilities
#s114	As a result, it would then assign higher probabilities to other words that are common here.
#c114	a result;it;higher probabilities;other words
#s115	But not having high probability here.
#c115	high probability
#s116	So those will be common here.
#s117	I.
#c117	I.
#s118	And if they are common, they would have to have high probabilities according to maximum likelihood estimator.
#c118	they;they;high probabilities;maximum likelihood estimator
#s119	An if they are rare here.
#c119	they
#s120	Right, so if they are rare here.
#c120	they
#s121	Then you don't get much help from this background model.
#c121	you;much help;this background model
#s122	As a result, this topic model must assign high probabilities, so the high probability words according to the topic model would be those that are common here, but rare in the background.
#c122	a result;this topic model;high probabilities;the high probability words;the topic model;the background
#s123	OK, so this is basically a little bit a IDF waiting here.
#c123	a little bit;a IDF
#s124	But this would allow us to achieve the effect of removing this top awards that are meaningless in the feedback.
#c124	us;the effect;this top awards;the feedback
#s125	So mathematically, what we have is to compute the like hold again local, like hold of the feedback documents and.
#c125	what;we;the like hold;hold;the feedback documents
#s126	And note that we also have another parameter Lambda here, but we assume that the Lambda denotes the noise in the feedback document.
#c126	we;another parameter;Lambda;we;the Lambda;the noise;the feedback document
#s127	So we are going to, let's say set this to a parameter that say 50% of the words are noise or 9% are noise and this can be assumed to be fixed if we assume this is fixed.
#c127	we;'s;a parameter;50%;the words;noise;9%;noise;we
#s128	Then We only have these probabilities as parameters, just like in the simplest unigram language model.
#c128	We;these probabilities;parameters;the simplest unigram language model
#s129	We have end parameters and is the number of words.
#c129	We;parameters;the number;words
#s130	And then the likelihood function would look like this.
#c130	the likelihood function
#s131	It's very similar to the likelihood function local.
#c131	It;the likelihood function
#s132	I can hold a function we see before, except that inside the logarithm there's a some here, and this sum is becausw.
#c132	I;a function;we;the logarithm;this sum;becausw
#s133	We consider two distributions.
#c133	We;two distributions
#s134	And which one is used would depend on Lambda
#c134	one;Lambda
#s135	and that's why we have this form.
#c135	we;this form
#s136	But mathematically, this is their function with theater as unknown variables, right?
#c136	their function;theater;unknown variables
#s137	So this is just a function or the other values are known except for this guy.
#c137	just a function;the other values;this guy
#s138	So we can then choose this probability distribution to maximize this locali code.
#c138	we;this probability distribution;this locali code
#s139	The same idea as the maximum, like Horace made it as a mathematical problem.
#c139	The same idea;Horace;it;a mathematical problem
#s140	We just we just have to solve this optimization problem.
#c140	We;we;this optimization problem
#s141	We essentially would try all the theater values and until we find one that gives this whole thing the maximum probability.
#c141	We;all the theater values;we;this whole thing;the maximum probability
#s142	So it's a well defined math problem.
#c142	it;a well defined math problem
#s143	Once we have done that, will obtain the serial F that can be there, interpreted with the original query model to do feedback.
#c143	we;the serial F;the original query model;feedback
#s144	So here are some examples of the feedback model learned from a Web document collection, and we do sudo feedback.
#c144	some examples;the feedback model;a Web document collection;we;feedback
#s145	Are we just use the top ten documents and we use this mixture model so the query is airport security?
#c145	we;the top ten documents;we;this mixture model;the query;airport security
#s146	What we do is we first retrieve 10 documents from the web database.
#c146	What;we;we;10 documents;the web database
#s147	And this is of course a pseudo feedback.
#c147	course;a pseudo feedback
#s148	I and then we're going to feed that mixture model to this 10.
#c148	I;we;that mixture model
#s149	Document set.
#c149	Document set
#s150	And these are the words learned using this approach.
#c150	the words;this approach
#s151	This is the probability of award given by the feedback model in both cases.
#c151	the probability;award;the feedback model;both cases
#s152	So in both cases you can see the highest probability words include very relevant words to the query, so airport security, for example.
#c152	both cases;you;the highest probability words;very relevant words;the query;airport security;example
#s153	This query words still show up as high probabilities in each case naturally becausw they occur frequently in the top ranked documents, but we also see beverage, alcohol, bomb, terrorists, etc.
#c153	This query words;high probabilities;each case;they;the top ranked documents;we;beverage;alcohol;bomb;terrorists
#s154	So these are relevant to this topic and they if combined with the original query.
#c154	this topic;they;the original query
#s155	Can help us match more accurately documents and also they can help us bring up a documents that only imagine the some of these other words.
#c155	us;documents;they;us;a documents;these other words
#s156	Maybe for example just the airport and then bomb for example this.
#c156	example;example
#s157	So this is how single feedback works.
#s158	Issues that this model really works and picks up some related words to the query.
#c158	Issues;this model;some related words;the query
#s159	What's also interesting is that if you look at the two tables here and you compare them and you see in this case when Lambda is set to a small value and we still see some common words here.
#c159	What;you;the two tables;you;them;you;this case;Lambda;a small value;we;some common words
#s160	And that means.
#s161	When we don't use the background more often, remember Lambda can use the probability of using the background model to generate the text.
#c161	we;the background;Lambda;the probability;the background model;the text
#s162	If we don't rely much on background model, we still have to use this topic model to account for the common words, whereas if we set Lambda to a very high value, we will use the background model very often to explain these words.
#c162	we;background model;we;this topic model;the common words;we;Lambda;a very high value;we;the background model;these words
#s163	Then there's no burden on explaining those common words in the feedback documents by the topic model.
#c163	no burden;those common words;the feedback documents;the topic model
#s164	So as a result of the topic model, here is very discriminant if it contains all the relevant words without common words.
#c164	a result;the topic model;it;all the relevant words;common words
#s165	So this can be added to the original query to achieve feedback.
#c165	the original query;feedback
#s166	So to summarize, in this lecture we have talked about the feedback in language model approach.
#c166	this lecture;we;the feedback;language model approach
#s167	In general, feedback is to learn from examples.
#c167	feedback;examples
#s168	These examples can be assumed, examples can be sued, examples like.
#c168	These examples;examples;examples
#s169	Assume that the top ten documents that are assumed to be random in there could be based on using fractions like a feedback based on pixels or implicit feedback.
#c169	the top ten documents;fractions;a feedback;pixels;implicit feedback
#s170	We talked about the three major feedback scenarios, relevance feedback, sooner feedback, and in principle feedback.
#c170	We;the three major feedback scenarios;relevance feedback;sooner feedback;principle feedback
#s171	We talked about how to use Rock You to do feedback in vector space model and how to use query model is missing for feedback in language model and we briefly talked about the mixture model and the basic idea.
#c171	We;Rock You;feedback;vector space model;query model;feedback;language model;we;the mixture model;the basic idea
#s172	There are many other methods, for example, the relevance model is a very effective model for estimating query model.
#c172	many other methods;example;the relevance model;a very effective model;query model
#s173	So you can read more about these methods in the references that listed at the end of this lecture.
#c173	you;these methods;the references;the end;this lecture
#s174	So there are two additional readings here.
#c174	two additional readings
#s175	The first one is a book that has a systematic review and discussion of language models for information retrieval and signal.
#c175	a book;a systematic review;discussion;language models;information retrieval;signal
#s176	One is important research paper that's about relevance based language models, and it's a very effective way of computing query model.
#c176	important research paper;about relevance based language models;it;a very effective way;computing query model
410	3956403f-f159-448a-9514-5dc69f314c5a	130
#s1	This lecture is about the evaluation of taxable categorization.
#c1	This lecture;the evaluation;taxable categorization
#s2	So we've talked about many different methods for taxi categorisation, but how do you which method works better?
#c2	we;many different methods;taxi categorisation;you;which method
#s3	And for a particular application, how do you this is the best way of solving your problem.
#c3	a particular application;you;the best way;your problem
#s4	To understand these will have to.
#s5	How to we have to know how to evaluate categorisation results?
#c5	we;categorisation results
#s6	So first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called Cranfield Evaluation Methodology.
#c6	the evaluation;evaluation;this kind;empirical tasks;categorisation;we;methodology;information retrieval researchers;Cranfield Evaluation Methodology
#s7	The basic idea is to help humans to create test collection.
#c7	The basic idea;humans;test collection
#s8	Where we already every document is tagged with the desired categories, or in the case of search for which query, which documents should have been retrieved and this is called ground truth.
#c8	we;already every document;the desired categories;the case;search;which query;documents;ground truth
#s9	Now with this ground truth test collection, we can then reduce the collection to test many different systems and compare different systems.
#c9	this ground truth test collection;we;the collection;many different systems;different systems
#s10	We can also turn off some component in system to see what's going to happen.
#c10	We;some component;system;what
#s11	Basically it provides.
#c11	it
#s12	A way to do controlled experiments to compare different methods.
#c12	A way;controlled experiments;different methods
#s13	So this methodology has been virtually used for all the tasks that involve empirically defined problems.
#c13	this methodology;all the tasks;empirically defined problems
#s14	So in our case, then we're going to compare our systems categorization results with the categorisation ground truth created by humans.
#c14	our case;we;our systems categorization results;the categorisation ground truth;humans
#s15	And we're going to compare our systems decisions on which documents should get which category with what.
#c15	we;our systems decisions;documents;which category;what
#s16	Categories have been assigned to those documents by humans and we want to quantify the similarity of these decisions.
#c16	Categories;those documents;humans;we;the similarity;these decisions
#s17	Or equivalently, to measure the difference between the system output and desired ideal output generated by the humans?
#c17	the difference;the system output;ideal output;the humans
#s18	So obviously the higher similarity is, the better the results are.
#c18	the higher similarity;the results
#s19	The similarity can be measured in different ways.
#c19	The similarity;different ways
#s20	And that would lead to different measures, and sometimes it's desirable also to measure the similarity from different perspectives just to have a better understanding of the results in detail.
#c20	different measures;it;the similarity;different perspectives;a better understanding;the results;detail
#s21	For example, it might be also interested in knowing which category performs better, which category is easy to categorize, etc.
#c21	example;it;which category;which category
#s22	In general, different categorization mistakes, however, have different costs for a specific application, so some errors might be more serious than others.
#c22	general, different categorization mistakes;different costs;a specific application;some errors;others
#s23	So ideally we would like to model such differences.
#c23	we;such differences
#s24	But if you read many papers in texture catalyzation, you will see that they don't generally do that, and instead they will use a simplified measure.
#c24	you;many papers;texture catalyzation;you;they;they;a simplified measure
#s25	And that's the cause.
#c25	the cause
#s26	It's often OK not to consider such a cost variation when we compare different methods.
#c26	It;such a cost variation;we;different methods
#s27	An we when we are interested in knowing the relative difference of these methods.
#c27	An we;we;the relative difference;these methods
#s28	So it's OK to introduce some bias as long as the bias is not correlated with a particular method.
#c28	it;some bias;the bias;a particular method
#s29	And then we should still expect the more effective method to perform better than a less effective one, even though the measure is not perfect.
#c29	we;the more effective method;the measure
#s30	So the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions.
#c30	the first measure;we;classification accuracy;the percentage;corrective decisions
#s31	So here you show that here you see that there are K categories denoted by C1 through CK and there are N documents in order by D1 through DN an for each pair of a category on the document that we can then look at the situation.
#c31	you;you;K categories;C1;CK;N documents;order;D1;DN an;each pair;a category;the document;we;the situation
#s32	And see if the system has said yes to despair.
#c32	the system;despair
#s33	Basically has assigned this category to this document or no, so this is denoted by Y or N.
#c33	this category;this document;Y;N.
#s34	That's the system to decision.
#c34	the system
#s35	And similarly we can look at the humans decision.
#c35	we;the humans decision
#s36	Also, if the human has assigned a category to the document, there will be a plus sign here.
#c36	the human;a category;the document;a plus sign
#s37	That's just that just means a human would think this assignment is correct an if the incorrect, and then there's a minus.
#c37	a human;this assignment;a minus
#s38	So we will see.
#c38	we
#s39	All combinations of these ends yes and Nos with minus and plus.
#c39	All combinations;these ends;Nos;minus
#s40	So there are four combinations in total and two of them are correct and when we have Y plus or minus
#c40	four combinations;total;them;we;Y
#s41	and then there are also two kinds of errors.
#c41	two kinds;errors
#s42	So the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made.
#c42	the measure;classification accuracy;these decisions;the total number;decisions;we
#s43	So we know that the total number of decisions is.
#c43	we;the total number;decisions
#s44	In multiplied by K. And the number of characters decisions obviously are basically of two kinds.
#c44	K.;the number;characters decisions;two kinds
#s45	One is why pluses and the other is N minus is
#c45	pluses;N minus
#s46	and we just put together the account.
#c46	we;the account
#s47	Now this is a very convenient measure that will give us a one number to characterize performance of method and the higher the better of course.
#c47	a very convenient measure;us;a one number;performance;method;course
#s48	But the method I also had some problems.
#c48	I;some problems
#s49	First it has treated all the decisions equally so, but in reality there's some decision errors are more serious than others.
#c49	it;all the decisions;reality;some decision errors;others
#s50	For example, it may be more important to get the decisions right on some documents than others, and or maybe more important to get the divisions right on some categories than others, and this would call for some detailed evaluation of this results to understand.
#c50	example;it;the decisions;some documents;others;the divisions;some categories;others;some detailed evaluation;this results
#s51	The strengths and weaknesses of different methods.
#c51	The strengths;weaknesses;different methods
#s52	And to understand the performance of these methods in detail.
#c52	the performance;these methods;detail
#s53	In APA category or per document basis?
#c53	APA category;document basis
#s54	One example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem.
#c54	One example;the desicion errors;different causes;spam filtering;a two category categorization problem
#s55	Missing a legitimate email is all is 1 type of error.
#c55	a legitimate email;1 type;error
#s56	But letting us ma'am to come into your folder is another type of error.
#c56	us;your folder;another type;error
#s57	The two types of errors are clearly very different because it's very important not to miss a legitimate email.
#c57	The two types;errors;it;a legitimate email
#s58	It's OK to occasionally let us spam email to come into your inbox, so the error of the first missing a legitimate email is very high cost.
#c58	It;us;email;your inbox;the error;a legitimate email;very high cost
#s59	It's very serious mistake.
#c59	It;very serious mistake
#s60	And classification error classification accuracy does not address this issue.
#c60	classification error classification accuracy;this issue
#s61	There's also another problem with imbalanced tests at the Imagine there's a skew.
#c61	another problem;imbalanced tests;the Imagine;a skew
#s62	The test set where most instances are in category one.
#c62	The test;most instances;category
#s63	And 98% of instances are in category one only 2% are in category Two.
#c63	98%;instances;category;only 2%;category
#s64	In such a case, we can have a very simple baseline that actually performs very, and the baseline would Simply put all instances in the major category.
#c64	such a case;we;a very simple baseline;the baseline;all instances;the major category
#s65	That would give us 98% accuracy.
#c65	us;98% accuracy
#s66	In this case, it's going to be appearing to be very effective, but in reality this is obviously not a good result.
#c66	this case;it;reality;a good result
#s67	And so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced.
#c67	we;classification accuracy;a measure;we;the classes
#s68	And we wonder about equal number of instances.
#c68	we;equal number;instances
#s69	For example, in each class the minority categories or classes tend to be overlooked in the evaluation of classification accuracy.
#c69	example;each class;the minority categories;classes;the evaluation;classification accuracy
#s70	How to address these problems?
#c70	these problems
#s71	We of course would like to also evaluate the results in other ways and in different ways.
#c71	We;course;the results;other ways;different ways
#s72	As I said, it's beneficial to look at the actual must multiple perspectives.
#c72	I;it;must multiple perspectives
#s73	So for example, we can look at the perspective from each document perspective based on each document.
#c73	example;we;the perspective;each document perspective;each document
#s74	So the question here is,
#c74	the question
#s75	how could other divisions on this document?
#c75	how could other divisions;this document
#s76	Now, as in the general cases of all decisions, we can think about four combinations of possibilities.
#c76	the general cases;all decisions;we;about four combinations;possibilities
#s77	Depending on whether the system has said yes, and depending on whether the human has said it correctly or incorrectly, or say yes or no, and so the four combinations are first.
#c77	the system;the human;it;the four combinations
#s78	When both the human system said yes and that's true positives when the system says yes, it's actually positive.
#c78	both the human system;true positives;the system;it
#s79	So when the system says yes, it's a positive.
#c79	the system;it
#s80	But when the human confirmed that it is indeed correct, that becomes true positive.
#c80	the human;it
#s81	When the system says yes, but human says no, that's incorrect.
#c81	the system;human
#s82	That's a false positive FP.
#c82	a false positive FP
#s83	And when the system says no, but the human says yes, then it's a false negative.
#c83	the system;the human;it;a false negative
#s84	We missed one assignment.
#c84	We;one assignment
#s85	When does the system and human said no?
#c85	the system;human
#s86	Then that's also corrected vision.
#c86	vision
#s87	That's true negatives.
#c87	true negatives
#s88	Alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall.
#c88	we;some meshes;the performance;these phone numbers;so 2 popular measures;precision
#s89	And these are also proposed by information retrieval researchers in 19, six days for evaluating searching results.
#c89	information retrieval researchers;six days;searching results
#s90	But now they have become a standard measure used everywhere.
#c90	they;a standard measure
#s91	So when the system says yes, we can ask the question how many are correct?
#c91	the system;we;the question
#s92	What's the percentage of correct decisions when the system says yes?
#c92	What;the percentage;correct decisions;the system
#s93	That's called precision.
#c93	precision
#s94	It's a true positive divided by all the cases when the system says yes all the positives.
#c94	It;all the cases;the system
#s95	The other recall the other meshes called Recall an this measures.
#c95	the other meshes;Recall;an this measures
#s96	Whether the document that has called all the categories it should have.
#c96	Whether the document;all the categories;it
#s97	So in this case it's divided the true positive by true positives and false negatives.
#c97	this case;it;true positives;false negatives
#s98	So these are all the cases where this human says the document should have this category.
#c98	all the cases;this human;the document;this category
#s99	So this represents the old categories that it should have got an.
#c99	the old categories;it
#s100	So recall tells us whether the system has actually indeed assigned all the categories that it should have to this document.
#c100	us;the system;all the categories;it;this document
#s101	This gives us a detailed view of the decision on each document.
#c101	us;a detailed view;the decision;each document
#s102	Then we can aggregate them later.
#c102	we;them
#s103	And if you're interested in some documents and this would tell us how well we did that those documents a subset of them might be more interesting than others.
#c103	you;some documents;us;we;those documents;a subset;them;others
#s104	For example, and this allows us to analyze errors in more detail as well.
#c104	example;us;errors;more detail
#s105	We can separate the documents of certain characteristic from others and then look at the errors.
#c105	We;the documents;certain characteristic;others;the errors
#s106	You might see a pattern here for this kind of documents along documents it doesn't do as well as.
#c106	You;a pattern;this kind;documents;documents;it
#s107	For short documents.
#c107	short documents
#s108	And this gives you some insight for improving the better.
#c108	you;some insight
#s109	Similarly, we can look at the popular category valuation.
#c109	we;the popular category valuation
#s110	This.
#s111	In this case we're going to look at the how good are the decision on a particular category.
#c111	this case;we;the decision;a particular category
#s112	And as in the previous case, we can define precision and recall and it will just basically answer the questions from a different perspective.
#c112	the previous case;we;precision;recall;it;the questions;a different perspective
#s113	I saw when the system says yes, how many are corrected that means looking at this category to see if all the documents that are assigned with this category are indeed in this category.
#c113	I;the system;this category;all the documents;this category;this category
#s114	An recall would tell us has the category being actually assigned to all the documents that should have this category.
#c114	An recall;us;the category;all the documents;this category
#s115	Is sometimes also useful to combine precision and recall as one measure, and this is often done by using if mesh.
#c115	precision;one measure;if mesh
#s116	And this is just the harmonic mean of precision and recall defined on this slide.
#c116	just the harmonic mean;precision;recall;this slide
#s117	Ann It's also controlled by a parameter beta two to indicate the weather precision is more important, or recall is more important when beta is set to one, we have a measure called F1, and in this case we just take a equal weight on both precision and recall.
#c117	It;a parameter beta;the weather precision;recall;beta;we;a measure;F1;this case;we;a equal weight;both precision
#s118	If one is very often used as a measure for categorisation.
#c118	one;a measure;categorisation
#s119	Now, as in all cases when we combine results, you always should think about the best way of combining them.
#c119	all cases;we;results;you;the best way;them
#s120	So in this case I don't know if you have thought about it
#c120	this case;I;you;it
#s121	and we could have combining them just with the arithmetic mean, right?
#c121	we;them;the arithmetic mean
#s122	So that would still give it the same range of values.
#c122	it;the same range;values
#s123	But obviously there's a reason why we didn't do that and why.
#c123	a reason;we
#s124	If one is more popular and it's actually useful to think about difference.
#c124	one;it;difference
#s125	And if you think about that, you will see that there is indeed some difference and sum.
#c125	you;you;some difference;sum
#s126	Undesirable property of this arithmetic mean.
#c126	Undesirable property
#s127	Basically, it would be obvious to you if you think about a case when the system says yes for all the category and nothing appears.
#c127	it;you;you;a case;the system;all the category;nothing
#s128	And even tried to compute the precision and recall in that case and see what would happen.
#c128	the precision;that case;what
#s129	I basically this kind of measure will not the arithmetic mean is not going to be as reasonable FF1, which tends to prefer a tradeoff between precision and recall.
#c129	I;this kind;measure;reasonable FF1;a tradeoff;precision
#s130	So that the two values are about equal, so we if there's an extreme case where you have 041 value and one for the other, than F1 will be low, but the arithmetic mean would still be reasonably high.
#c130	the two values;we;an extreme case;you;041 value;F1
410	39d13817-de51-4195-a33a-985b0b54e64d	115
#s1	This lecture is about the text categorization.
#c1	This lecture;the text categorization
#s2	In this lecture we're going to talk about the text categorization.
#c2	this lecture;we;the text categorization
#s3	This is a very important technique for a text, data mining and analytics.
#c3	a very important technique;a text;data mining;analytics
#s4	It is relevant to discovery of various different kinds of knowledge as shown here.
#c4	It;discovery;various different kinds;knowledge
#s5	First is related to topic mining analysis.
#c5	topic mining analysis
#s6	And that's because it has to do with analyzing text data based on some predefined topics.
#c6	it;text data;some predefined topics
#s7	Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor.
#c7	it;opinion mining and sentiment analysis;knowledge;the observer
#s8	Because we can categorize the authors, for example, based on the content of the articles that they have written.
#c8	we;the authors;example;the content;the articles;they
#s9	We can in general categorize the observer based on the content.
#c9	We;the observer;the content
#s10	That they produce.
#c10	they
#s11	Finally, it's also related to text based prediction.
#c11	it;based prediction
#s12	Because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data.
#c12	we;text categorization techniques;some variables;the real world;text data
#s13	And so this is a very important technique for text data mining.
#c13	a very important technique;text data mining
#s14	This is the overall plan for covering the topic.
#c14	the overall plan;the topic
#s15	First we're going to talk about what is text categorization and why we are interested in doing that in this lecture.
#c15	we;what;text categorization;we;this lecture
#s16	And then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so.
#c16	we;text categorisation;the categorisation results
#s17	The problem of texture categorisation is defined as follows.
#c17	The problem;texture categorisation
#s18	We're given a set of predefined categories.
#c18	We;a set;predefined categories
#s19	Possibly forming a hierarchy so.
#c19	a hierarchy
#s20	And often also a set of training examples or training set of labeled text objects.
#c20	And often also a set;training;examples;training set;labeled text objects
#s21	Which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories.
#c21	text objects;known categories;the task;any tax object;these predefined categories
#s22	So the picture on the slide shows what happens.
#c22	the picture;the slide;what
#s23	When we do text categorization, we have a lot of text objects to be processed by a categorisation system.
#c23	we;categorization;we;a lot;text objects;a categorisation system
#s24	And the system will in general assign categories to these documents as shown on the right.
#c24	the system;general assign categories;these documents;the right
#s25	And the categorisation results.
#c25	the categorisation
#s26	And we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in different categories, and this would further help the system then learn how to recognize.
#c26	we;the availability;training examples;the documents;known categories;these examples;the system;patterns;different categories;the system
#s27	The categories of new tax objects that it has not seen.
#c27	The categories;new tax objects;it
#s28	So here are some specific examples of text categorization and In fact, there are many examples.
#c28	some specific examples;text categorization;fact;many examples
#s29	Here are just a few.
#s30	So first text objects can vary, so we can categorize a document.
#c30	text objects;we;a document
#s31	Or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can vary a lot, so this creates a lot of possibilities.
#c31	a passage;sentence;collections;text;the case;the units;a lot;possibilities
#s32	Secondly, categories can also vary, and we can generally distinguish two kinds of categories.
#c32	categories;we;two kinds;categories
#s33	One is internal categories.
#c33	internal categories
#s34	These are categories that characterize content of text object.
#c34	categories;content;text object
#s35	For example, topic categories.
#c35	example
#s36	Or sentiment categories and they generally have to do with the content of the tax objects, direct Characterization of the content.
#c36	Or sentiment categories;they;the content;the tax objects;direct Characterization;the content
#s37	The other kind is external categories that can characterize the entity associated with the text object.
#c37	The other kind;external categories;the entity;the text object
#s38	For example, authors or entities associated with the content that they produce.
#c38	example;authors;entities;the content;they
#s39	And so we can use their content, determine which author has written which part, for example, and that's called author attribution.
#c39	we;their content;author;which part;example;author attribution
#s40	Or we can have any other meaningful categories associated with text data, as long as.
#c40	we;any other meaningful categories;text data
#s41	There is a. There are, there's a meaningful connection between the entity and text data.
#c41	a.;a meaningful connection;the entity and text data
#s42	For example, we might collect a lot of reviews about a restaurant.
#c42	example;we;a lot;reviews;a restaurant
#s43	Or a lot of reviews about the product.
#c43	Or a lot;reviews;the product
#s44	And then these text data can help us infer properties of product or a restaurant.
#c44	these text data;us;properties;product;a restaurant
#s45	In that case, we can treat this as a categorization problem.
#c45	that case;we;a categorization problem
#s46	We can categorize restaurants or categorize products based on their corresponding reviews.
#c46	We;restaurants;categorize products;their corresponding reviews
#s47	So this is example of external category.
#c47	example;external category
#s48	Here are some specific examples of applications.
#c48	some specific examples;applications
#s49	News categorization is very common, has been stuided.
#c49	News categorization
#s50	A lot.
#c50	A lot
#s51	News agencies would like to assign predefined categories to categorize news generated every day.
#c51	News agencies;predefined categories;news
#s52	And literature article categorizations another important task, for example, in biomedical domain, Is this mesh annotations , mesh stands for medical subject heading.
#c52	literature article;another important task;example;biomedical domain;this mesh annotations;mesh;medical subject heading
#s53	And this is ontology of terms characterize content of literature articles in detail.
#c53	ontology;terms;content;literature articles;detail
#s54	Another example of application spam, email detection or filtering right?
#c54	Another example;application spam;email detection
#s55	So we often have a spam filter to help us distinguish spam from legitimate emails, and this is clearly a binary classification problem.
#c55	we;a spam filter;us;spam;legitimate emails;a binary classification problem
#s56	Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize content into positive or negative or positive and negative or neutral.
#c56	Sentiment categorization;product reviews;tweets;yet another kind;applications;we;content
#s57	so you can have the same sentiment categories assigned.
#c57	you;the same sentiment categories
#s58	to text content.
#c58	content
#s59	Another application is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category.
#c59	Another application;routing;sorting;you;your emails;different folders;one application;text categorization;each folder;a category
#s60	There is also another important kind of applications of routing emails to the right person to handle.
#c60	another important kind;applications;emails;the right person
#s61	So in helpdesk email messages generally routed to a particular person to handle different people attempt to handle different kinds of requests and in many cases a person will manually assign the messages to the right people.
#c61	helpdesk email messages;a particular person;different people;different kinds;requests;many cases;a person;the messages;the right people
#s62	But you can imagine you can build automatic text categorization system to help routing a request.
#c62	you;you;automatic text categorization system;a request
#s63	And this is to classify the incoming request in to one of the categories where each category actually corresponds to a person to handle the request.
#c63	the incoming request;the categories;each category;a person;the request
#s64	And finally, author Attribution.
#c64	And finally, author Attribution
#s65	As I just mentioned, is yet another application, and it's another example of using text to actually infer properties of some other entities.
#c65	I;another application;it;another example;text;properties;some other entities
#s66	And there are also many variants of the problem formulation and so first we have the simplest case, which is a binary categorization where there are only two categories and there are many examples like that information retrieval or search engine applications would want to.
#c66	many variants;the problem formulation;we;the simplest case;a binary categorization;only two categories;many examples;that information retrieval;search engine applications
#s67	Distinguish it relevant documents from non relevant documents for a particular query.
#c67	it;non relevant documents;a particular query
#s68	Spam filter is interesting.
#c68	Spam filter
#s69	Distinguishing spams from non spam.
#c69	spams;non spam
#s70	So also two categories.
#c70	So also two categories
#s71	Sometimes classification of opinions can be in two categories, positive and negative.
#c71	classification;opinions;two categories
#s72	A more general case would be K-category categorization and there are also many applications like that.
#c72	A more general case;K-category categorization;many applications
#s73	There could be more than two categories, so topical categorisation is often such example where you can have multiple topics.
#c73	more than two categories;topical categorisation;such example;you;multiple topics
#s74	Email routing would be another example when you may have multiple folders, or if you route the email to the right person to handle it, then there are multiple people, to clasify so in all these cases there are more than two kinds of categories.
#c74	Email routing;another example;you;multiple folders;you;the email;the right person;it;multiple people;all these cases;more than two kinds;categories
#s75	And another variation to have hierarchical categorization, where categories form hierarchy, again, topical hierarchy is very common.
#c75	another variation;hierarchical categorization;categories;hierarchy;topical hierarchy
#s76	Yet another variation is joint categorization.
#c76	another variation;joint categorization
#s77	That's when you have multiple categorization tasks that are related.
#c77	you;multiple categorization tasks
#s78	And then you hope to kind of do joint categorization.
#c78	you;joint categorization
#s79	Try to leverage the dependents of these tasks to improve accuracy for each individual task.
#c79	the dependents;these tasks;accuracy;each individual task
#s80	Now among all these, binary categorization is most fundamental and partly also because it's simple and partly it's cause it can actually be used to perform all the other categorization tasks.
#c80	binary categorization;it;it;it;all the other categorization tasks
#s81	For example, K category categorisation task can be actually performed by using binary categorization.
#c81	example;K category categorisation task;binary categorization
#s82	And basically we can look at each category separately and then the binary categorization problem is whether object is in this category or not.
#c82	we;each category;the binary categorization problem;object;this category
#s83	Meaning in other categories.
#c83	other categories
#s84	And the hierarchical category categorisation can also be done by progressively doing flat categorisation at each level.
#c84	the hierarchical category categorisation;flat categorisation;each level
#s85	So we can first categorize all the objects in tune.
#c85	we;all the objects;tune
#s86	It's a small number of high level categories an inside each category.
#c86	It;a small number;high level categories;each category
#s87	We can further categorize into sub categories etc.
#c87	We;sub categories
#s88	So why is text categories important well, I already showed you several applications, but in general there are several reasons.
#c88	text categories;I;you;several applications;several reasons
#s89	One is text Categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis.
#c89	text Categorization;us;text representation;more understanding;text data;text analysis
#s90	So now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks.
#c90	categorisation;text;multiple levels;keyword bag;words;a lot;text processing tasks
#s91	But we can also add categories and they provide 2 levels of representation.
#c91	we;categories;they;2 levels;representation
#s92	Semantic categories assigned can also be directly or indirectly useful for application.
#c92	Semantic categories;application
#s93	So for example, sentiment categories could be already very useful, or author Attribution might be directly useful.
#c93	example;sentiment categories;author;Attribution
#s94	And.
#s95	Another example is when semantic categories can facilitate aggregation of tax content, and this is another case of.
#c95	Another example;semantic categories;aggregation;tax content;another case
#s96	Applications of text categorisation.
#c96	Applications;text categorisation
#s97	For example, we if we want to know the overall opinions about the product, we could first categorize the opinions in each individual review as positive or negative, and then that would allow us to easily aggregate all the sentiments and it will tell us about 70% of the views positive and 30% are negative, etc.
#c97	example;we;we;the overall opinions;the product;we;the opinions;each individual review;us;all the sentiments;it;us;about 70%;the views;30%
#s98	So without doing categorization it will be much harder to aggregate such opinions.
#c98	categorization;it;such opinions
#s99	So it provides a concise way of coding text in some sense based on our vocabulary.
#c99	it;a concise way;coding text;some sense;our vocabulary
#s100	And sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary.
#c100	you;some applications;text;categorization;a text coding encoding;some controller vocabulary
#s101	The second kind of reasons is to use text categorization to infer properties of entities.
#c101	The second kind;reasons;text categorization;properties;entities
#s102	And text categorisation allows us to infer the properties of such entities that are associated with text data.
#c102	text categorisation;us;the properties;such entities;text data
#s103	So this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities.
#c103	we;text categorization;knowledge;the world;we;the entity;text data;we;the text data;the corresponding entities
#s104	So it's useful to think about the information network that will connect the other entities with text data.
#c104	it;the information network;the other entities;text data
#s105	The obvious entities that can be directly connected are authors, but you can also imagine the authors affiliations or the authors ages and other things can be actually connected to text data indirectly.
#c105	The obvious entities;authors;you;the authors affiliations;the authors ages;other things;data
#s106	Once we can make the connection, then we can make predictions about those values.
#c106	we;the connection;we;predictions;those values
#s107	So this is a general way to allow us to use text mining tool.
#c107	a general way;us;text mining tool
#s108	Sorry, text categorization to discover knowledge about the world.
#c108	Sorry, text categorization;knowledge;the world
#s109	Very useful, especially in big text data.
#c109	big text data
#s110	Analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors.
#c110	Analytics;we;text data;extra sensor data;humans;certain desicion factors
#s111	Often together with non text data specifically to text.
#c111	non text data;text
#s112	For example, we can also think of examples of inferring properties of entities.
#c112	example;we;examples;inferring properties;entities
#s113	For example discovery of non native speakers of a language and this can be done by categorizing the content of.
#c113	example;non native speakers;a language;the content
#s114	Speakers Another example is to predict the party affiliation of a politician based on the political speech at this is again example of using text data to infer some knowledge about real world.
#c114	Speakers;Another example;the party affiliation;a politician;the political speech;example;text data;some knowledge;real world
#s115	In nature this all the problems are all the same and that's as we defined and it's a text categorization problem.
#c115	nature;all the problems;we;it;a text categorization problem
410	3eca1e42-a7a7-433c-ba9f-6ee90351395f	45
#s1	So we talked about page rank as a way to. "
#c1	we;page rank;a way
#s2	" Capture the authorities now we also looked at the some other examples where a hub might be interesting, so there is another algorithm called hits and that's going to compute the scores for authorities and hubs.
#c2	the authorities;we;the some other examples;a hub;another algorithm;hits;the scores;authorities;hubs
#s3	Intuitions are pages that are wider.
#c3	Intuitions;pages
#s4	Sites are good authorities, then where pages that cite many other pages are good hubs, right?
#c4	Sites;good authorities;pages;many other pages;good hubs
#s5	But the I think the most interesting idea of this algorithm hits is.
#c5	the I;the most interesting idea
#s6	It's going to use reinforcement mechanism to kind of help improve the scoring for hubs and authorities, and here.
#c6	It;reinforcement mechanism;kind of help;the scoring;hubs;authorities
#s7	So here's the idea.
#c7	the idea
#s8	It would assume that good authorities are cited by good hubs.
#c8	It;good authorities;good hubs
#s9	That means if you're cited by many pages with good hub scores, then that increases your authority score and similarly good hubs are those that pointed to good authorities.
#c9	you;many pages;good hub scores;your authority score;similarly good hubs;good authorities
#s10	So if you get you pointed to a lot of good authority pages, then your hub score will be increased.
#c10	you;you;a lot;good authority pages;your hub score
#s11	So then we can iterate, reinforce each other 'cause you can point to some good hubs so that you can point to some good authorities to get a good hub score, whereas those authoritie scores.
#c11	we;you;some good hubs;you;some good authorities;a good hub score
#s12	Would be also improved because they are pointed to by a good hub and this algorithm is also general.
#c12	they;a good hub;this algorithm
#s13	It can have many applications in graph and network analysis.
#c13	It;many applications;graph;network;analysis
#s14	So just briefly, here's how it works.
#c14	it
#s15	We first also construct the matrix, but this time we're going to construct the adjacency matrix and we're not going to normalize the values.
#c15	We;the matrix;we;the adjacency matrix;we;the values
#s16	So if there's a link, there is one.
#c16	a link
#s17	If there's no link that's zero again it's the same graph.
#c17	no link;it;the same graph
#s18	And then we're going to define the Hub score of page as the sum of the authority scores of all the pages that it points to.
#c18	we;the Hub score;page;the sum;the authority scores;all the pages;it
#s19	So whether you are a hub really depends on whether you're pointing to a lot of good authority pages.
#c19	you;a hub;you;a lot;good authority pages
#s20	That's what it says in the first equation.
#c20	what;it;the first equation
#s21	In the second equation, we define the authorities score over page as the sum of the hub scores of all those pages that the point to you.
#c21	the second equation;we;the authorities;page;the sum;the hub scores;all those pages;you
#s22	So whether you are good authoritie would depend on whether those pages that are pointing to you are good hubs so you can see this forms iterative reinforcement mechanism.
#c22	you;good authoritie;those pages;you;good hubs;you;this forms
#s23	Now these two equations can be also written in the matrix format.
#c23	these two equations;the matrix format
#s24	I saw what we get here is then the hub vector is equal to the product of the edges of the adjacency matrix and the authority vector.
#c24	I;what;we;the hub vector;the product;the edges;the adjacency matrix;the authority vector
#s25	And this is basically the first equation right?
#c25	the first equation
#s26	And similarly the second equation can be returned as the authoritie vector is equal to the product of A transpose multiplied by the hub vector and these are just different ways of expressing these equations.
#c26	the second equation;the authoritie vector;the product;A transpose;the hub vector;just different ways;these equations
#s27	But what's interesting is that if you look at the matrix form, you can also plug in the authority equation.
#c27	what;you;the matrix form;you;the authority equation
#s28	Into the first one.
#s29	So if you do that, you can actually then eliminate the authoritie vector completely and you get the equation of only hub scores, right?
#c29	you;you;the authoritie vector;you;the equation;only hub scores
#s30	The Hub score vector is equal to A multiplied by A transpose multiplied by the hub score vector again.
#c30	The Hub score vector;A transpose;the hub score vector
#s31	And similarly, we can do a transformation to have equation for just the authority scores.
#c31	we;a transformation;equation;just the authority scores
#s32	So although we framed the problem as computing hubs and authorities, we can actually eliminate one of them to obtain equation just for one of them.
#c32	we;the problem;computing hubs;authorities;we;them;equation;them
#s33	The difference between this and page rank is that now the matrix is actually a multiplication of the edges in the matrix and its transpose, so this is different from page rank.
#c33	The difference;this and page rank;the matrix;a multiplication;the edges;the matrix;its transpose;page rank
#s34	But mathematically, then we will be computing the same problem.
#c34	we;the same problem
#s35	So in hits we typically would initialize the values that said, one for all these values and then we would iteratively apply these equations, essentially and This is equivalent to multiply that by the Matrix A and A transpose.
#c35	hits;we;the values;all these values;we;these equations;the Matrix A;A transpose
#s36	So the algorithm is exactly the similar page rank, but here because the adjacency matrix is not normalized.
#c36	the algorithm;exactly the similar page rank;the adjacency matrix
#s37	So what we have to do, what we have to do is after each iteration, we're going to normalize and this would allow us to control the growth of value, otherwise they would grow larger and larger.
#c37	what;we;what;we;each iteration;we;us;the growth;value;they
#s38	And if we do that and then we'll basically get hits algorithm to compute the hub scores an authority scores for all the pages.
#c38	we;we;hits algorithm;an authority scores;all the pages
#s39	And these scores can then be used in ranking just like a page rank scores.
#c39	these scores;a page rank scores
#s40	So to summarize, in this lecture we have seen that link information is very useful.
#c40	this lecture;we;link information
#s41	In particular, the anchor text is very useful to increase the.
#c41	the anchor text
#s42	The text representation of a page and we also talk about page rank and hits on as two major link analysis algorithms.
#c42	The text representation;a page;we;page rank;two major link analysis algorithms
#s43	Both can generate scores for web pages that can be used in the ranking function.
#c43	scores;web pages;the ranking function
#s44	Note that page rank
#c44	that page rank
#s45	and it's also very general algorithms, so they have many applications in analyzing other graphs or networks.
#c45	it;very general algorithms;they;many applications;other graphs;networks
410	3f166a89-1603-4016-909f-cac980864478	4
#s1	this letter is about learning to rank in this lecture we're going to continue talking about web search in particular we're going to talk about the using machine learning to combine different features to improve the ranking function so the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results in the previous lectures we have talked about a number of ways to rank documents we have talked about some retrieval models like a VM twenty five or query like code they can generate the content basis course for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranging now the question now is how can we combine all these features an potential million other features to do ranking and this will be very useful for ranking web pages not only just to improve accuracy but also the improve the robustness of the ranking function so that it's not easy for a spammer to just perturb a one or a few features to promote a page so the general idea of learning to rank is to use machine learning to combine these features were optimized the weights on different features to generate the optimal ranking function so we will assume that the given a query document pair Q M D we can define a number of features and these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as VM twenty five or query like whole door pivot condense formalization or P L two etc it can also be linked based on score like page ranks for it can be also application of retrieval models to the anchor text of the page like those are the types of descriptions of links that pointed to this page so these can all be cruise about whether this document is relevant or not we can even include a feature such as whether the URL has theater because this might be an indicator of home page or engine page so all these features can then be combined together through generated ranking function the question is of course how can we combine them in this approach if we simply hypothesize that the probability that this document is relevant to this query is a function of all these features so we can hypothesize is that the probability of relevance is related to these features through a particular form of the function that has some parameters these parameters can control the inference of different features on the final relevance this is of course just assumption whether this is something really makes sense is the big question still have to empirically evaluate the function but by hypothesizing that relevance is related to these features in the particular way we can then combine these features to generate potentially more powerful ranking function more robust ranking function naturally the next question is how do we estimate those parameters and how do we know which features we should have a higher weight an which features will have low weight
#c1	this letter;this lecture;we;web search;we;different features;the ranking function;the question;we;this lecture;we;many features;a single ranking function;search results;the previous lectures;we;a number;ways;documents;we;some retrieval models;a VM;code;they;the content;basis course;documents;a query;we;the link based approaches;page rank;additional scores;us;the question;we;an potential million other features;ranking;ranking web pages;accuracy;the robustness;the ranking function;it;a spammer;a one or a few features;a page;the general idea;these features;the weights;different features;the optimal ranking function;we;a query document;pair Q M D;we;a number;features;these features;content based features;a score;the document;respect;the query;a retrieval function;VM;whole door pivot condense formalization;P L;it;score;page;it;application;retrieval models;the anchor text;the page;the types;descriptions;links;this page;cruise;this document;we;a feature;the URL;theater;an indicator;home page;engine page;all these features;generated ranking function;the question;course;we;them;this approach;we;the probability;this document;this query;a function;all these features;we;the probability;relevance;these features;a particular form;the function;some parameters;these parameters;the inference;different features;the final relevance;course;just assumption;something;sense;the big question;the function;hypothesizing;that relevance;these features;the particular way;we;these features;potentially more powerful ranking function more robust ranking function;the next question;we;those parameters;we;we;a higher weight;an which features;low weight
#s2	so this is the task of training or learning so in this approach what we will do is to use some training data those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be rounded high for which queries
#c2	the task;training;this approach;what;we;some training data;the data;users;we;the relevance judgments;we;which documents
#s3	and this information can be based on real judgments by users or this can also be approximated by just using click through information where we can assume the click the documents are better than the skip the documents or click the document are relevant than the skip your documents are not relevant so in general with the fit such a hypothesized ranking function through the training data meaning that we will try to optimize its retrieval accuracy on the training data we adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as map or NDC G so the training data would look like a table of couples each tuple has three elements the query the document and the judgment
#c3	this information;real judgments;users;information;we;the click;the documents;the skip;the documents;the document;the skip;your documents;the fit;such a hypothesized ranking function;the training data;we;its retrieval accuracy;the training data;we;these parameters;we;the performance;the function;the training data;terms;some measures;map;NDC G;the training data;a table;couples;each tuple;three elements;the query;the document;the judgment
#s4	so it looks very much like our relevance judgment that we talked about evaluation of retrieval systems
#c4	it;our relevance judgment;we;evaluation;retrieval systems
410	3fa6d76f-2285-4cde-94db-6531698b9c21	202
#s1	This lecture is about natural language content analysis.
#c1	This lecture;natural language content analysis
#s2	As you see from this picture, this is really the first step to process any text data, text data in natural languages.
#c2	you;this picture;the first step;any text data;text data;natural languages
#s3	So computers have to understand natural language to some extent in order to make use of the data.
#c3	computers;natural language;some extent;order;use;the data
#s4	So that's the topic of this lecture.
#c4	the topic;this lecture
#s5	We're going to cover three things.
#c5	We;three things
#s6	First, what is natural language processing?
#c6	what;natural language processing
#s7	Which is the main technique for processing natural language to obtain understanding?
#c7	the main technique;natural language;understanding
#s8	The second is the state of the art in NLP, which stands for natural language processing.
#c8	the state;the art;NLP;natural language processing
#s9	Finally, we're going to cover the relation between natural language processing and text retrieval.
#c9	we;the relation;natural language processing and text retrieval
#s10	First what is NLP?
#c10	what;NLP
#s11	Well the best way to explain it is to think about if you see a text in a foreign language that you can understand.
#c11	Well the best way;it;you;a text;a foreign language;you
#s12	Now what do you have to do in order to understand that text?
#c12	what;you;order;that text
#s13	This is basically what computers are facing, right?
#c13	what computers
#s14	So looking at the simple sentence like a dog is chasing a boy on the playground.
#c14	the simple sentence;a dog;a boy;the playground
#s15	We don't have any problem with understanding this sentence.
#c15	We;any problem;this sentence
#s16	But imagine what the computer would have to do in order to understand it, or in general it would have to do the following.
#c16	what;the computer;order;it;it;the following
#s17	First we have to know dogs are a noun chasing is a verb etc.
#c17	we;dogs;a noun chasing;a verb
#s18	So this is called lexical analysis or part of speech tagging.
#c18	lexical analysis;part;speech tagging
#s19	And we need to figure out the syntactic categories of those words.
#c19	we;the syntactic categories;those words
#s20	So that's the first step.
#c20	the first step
#s21	After that, we're going to figure out the structure of the sentence.
#c21	we;the structure;the sentence
#s22	So for example, here it shows that A and a dog would go together to form a noun phrase.
#c22	example;it;A;a dog;a noun phrase
#s23	And we won't have dog and is to go first, and there are some structures that are not just right.
#c23	we;dog;some structures
#s24	But this structure shows what we might get if we look at the sentence and try to interpret the sentence.
#c24	this structure;what;we;we;the sentence;the sentence
#s25	Some words would go together 1st and then they will go together with other words.
#c25	Some words;they;other words
#s26	So here we show we have noun phrases as intermediate components and then verbal phrases.
#c26	we;we;noun phrases;intermediate components;then verbal phrases
#s27	Finally we have a sentence.
#c27	we;a sentence
#s28	And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser.
#c28	this structure;we;something;a syntactic analysis;we;a parser
#s29	A computer program that would automatically create this structure.
#c29	A computer program;this structure
#s30	Now at this point you would know the structure of this sentence, but still you don't know the meaning of the sentence, so we have to go further to semantic analysis.
#c30	this point;you;the structure;this sentence;you;the meaning;the sentence;we;semantic analysis
#s31	In our mind, we usually can map such a sentence to what we already know in our knowledge base.
#c31	our mind;we;such a sentence;what;we;our knowledge base
#s32	And for example, you might imagine a dog that looks like that there's a boy and there's some activity here.
#c32	example;you;a dog;a boy;some activity
#s33	But for a computer would have to use symbols to denote that, right?
#c33	a computer;symbols
#s34	So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground.
#c34	we;a symbol D1;a dog;B1;a boy;then P1;the playground;, playground
#s35	Now there is also chasing activity that's happening here, so we have a relation chasing here that connects all these symbols.
#c35	activity;we;a relation chasing;all these symbols
#s36	So this is how computer would obtain some understanding of this sentence.
#c36	computer;some understanding;this sentence
#s37	Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference.
#c37	this representation;we;some other things;we;something;we;the text;inference
#s38	So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared.
#c38	example;you;someone;this person;this rule;you;computers;this boy
#s39	So this is some extra knowledge that you would infer based on understanding of the text.
#c39	some extra knowledge;you;understanding;the text
#s40	You can even go further to understand why the person said this sentence, so this has reduced the use of language.
#c40	You;the person;this sentence;the use;language
#s41	This is called.
#s42	Pragmatic analysis.
#c42	Pragmatic analysis
#s43	In order to understand the speech actor of a sentence.
#c43	order;the speech actor;a sentence
#s44	Like we say something to basically achieve some goal.
#c44	we;something;some goal
#s45	There's some purpose there, and this has to do with the use of language.
#c45	some purpose;the use;language
#s46	In this case, the person who said this sentence might be reminding another person to bring back the dog.
#c46	this case;who;this sentence;another person;the dog
#s47	That could be one possible intent to reach this level of understanding would require all these steps.
#c47	one possible intent;this level;understanding;all these steps
#s48	And a Computer would have to go through all these steps in order to completely understand this sentence.
#c48	a Computer;all these steps;order;this sentence
#s49	Yet we humans have no trouble with understanding that, we instantly will get everything.
#c49	we humans;no trouble;understanding;we;everything
#s50	And there is a reason for that.
#c50	a reason
#s51	That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence.
#c51	we;a large knowledge base;our brain;we;common sense knowledge;the sentence
#s52	Computers unfortunately, are hard to obtain such understanding.
#c52	Computers;such understanding
#s53	They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties.
#c53	They;such a knowledge base;they;reasoning;uncertainties
#s54	So that makes natural language processing difficult for computers.
#c54	natural language processing;computers
#s55	But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers.
#c55	the fundamental reason;a natural language processing;computers;natural language;computers
#s56	They natural languages are designed for us to communicate.
#c56	They natural languages;us
#s57	There are other languages designed for computers.
#c57	other languages;computers
#s58	For example program languages.
#c58	example
#s59	Those are harder for us, so natural languages is designed to make our communication efficient.
#c59	us;natural languages;our communication
#s60	As a result, we omit a lot of common sense knowledge because we assume everyone knows about that.
#c60	a result;we;a lot;common sense knowledge;we;everyone
#s61	We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context.
#c61	We;a lot;ambiguities;we;the receiver;the hearer;ambiguous word;the knowledge;the context
#s62	There's no need to invent the different words for different meanings.
#c62	no need;the different words;different meanings
#s63	We could overload the same word with different meanings without the problem.
#c63	We;the same word;different meanings;the problem
#s64	Because of these reasons, this makes every step in natural language processing difficult.
#c64	these reasons;every step;natural language processing
#s65	For computers, ambiguity is the main difficulty.
#c65	computers;ambiguity;the main difficulty
#s66	And common sense reasoning is often required.
#c66	common sense reasoning
#s67	That's also hard.
#s68	So let me give you some examples of challenges here.
#c68	me;you;some examples;challenges
#s69	Consider the word level ambiguity.
#c69	the word level ambiguity
#s70	The same word can have different syntactic categories.
#c70	The same word;different syntactic categories
#s71	For example, design can be a noun or a verb.
#c71	example;design;a noun;a verb
#s72	The word root may have multiple meanings, so square root in math sense, or the root of a plant.
#c72	The word root;multiple meanings;so square root;math sense;the root;a plant
#s73	You might be able to think of other meanings.
#c73	You;other meanings
#s74	There are also syntactical ambiguities, for example.
#c74	syntactical ambiguities;example
#s75	The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure.
#c75	The main topic;this lecture;natural language processing;two ways;terms;the structure
#s76	Think for a moment to see if you can figure that out.
#c76	a moment;you
#s77	We usually think of this as processing of natural language.
#c77	We;processing;natural language
#s78	But you could also think of this as you say, language processes is natural.
#c78	you;you;language processes
#s79	Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words.
#c79	an example;syntactic ambiguity;we;different structures;the same sequence;words
#s80	Another common example of an ambiguous sentence is the following.
#c80	Another common example;an ambiguous sentence;the following
#s81	A man saw a boy with a telescope.
#c81	A man;a boy;a telescope
#s82	Now in this case, the question is who had the telescope?
#c82	this case;the question;who;the telescope
#s83	Right, this is called a prepositional phrase attachment.
#c83	a prepositional phrase attachment
#s84	Ambiguity, or PP attachment ambiguity.
#c84	Ambiguity;PP attachment ambiguity
#s85	Now we generally don't have a problem with these ambiguities.
#c85	we;a problem;these ambiguities
#s86	Because we have a lot of background and knowledge to help us disambiguate the ambiguity.
#c86	we;a lot;background;knowledge;us;the ambiguity
#s87	Another example of difficulties is anaphora resolution, so think about the sentence like John persuaded Bill to buy a TV for himself.
#c87	Another example;difficulties;anaphora resolution;the sentence;John;Bill;a TV;himself
#s88	The question here is does himself refer to John or Bill?
#c88	The question;himself;John;Bill
#s89	So again, this is something that you have to use some background or the context to figure out.
#c89	something;you;some background;the context
#s90	Finally, presupposition is another problem.
#c90	presupposition;another problem
#s91	Consider the sentence.
#c91	the sentence
#s92	He has quit smoking.
#c92	He;smoking
#s93	This obviously implies that he smoked before.
#c93	he
#s94	So imagine a computer wants to understand all these subtle differences and meanings.
#c94	a computer;all these subtle differences;meanings
#s95	It would have to use a lot of knowledge to figure that out.
#c95	It;a lot;knowledge
#s96	It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world.
#c96	It;a large knowledge knowledge base;all the meanings;words;they;our common sense knowledge;the world
#s97	So This is why it's very difficult.
#c97	it
#s98	So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers.
#c98	a result;we;fact;natural language;computers
#s99	So this slide sort of gives simplified view of state of the art technologies.
#c99	this slide sort;simplified view;state;the art technologies
#s100	We can do part of speech tagging pretty well, so I showed 97% accuracy here.
#c100	We;part;speech tagging;I;97% accuracy
#s101	Now this number is obviously based on a certain data set, so don't take this literally.
#c101	this number;a certain data
#s102	It just shows that we can do it pretty well, but it's still not perfect.
#c102	It;we;it;it
#s103	In terms of parsing, we can do partial parsing pretty well.
#c103	terms;we
#s104	That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure.
#c104	we;noun phrase structures;verbal phrases structures;some segment;the sentence;terms;the structure
#s105	An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences.
#c105	some evaluation results;we;90% accuracy;terms;partial parsing;sentences
#s106	Again, I have to say these numbers are relative to the data set in some other data sets.
#c106	I;these numbers;the data;some other data sets
#s107	The numbers might be lower.
#c107	The numbers
#s108	Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data.
#c108	the existing work;news data;a lot;these numbers;news data
#s109	Think about the social media data, the accuracy likely is lower.
#c109	the social media data;the accuracy
#s110	In terms of semantic analysis.
#c110	terms;semantic analysis
#s111	We are far from being able to do a complete understanding of a sentence.
#c111	We;a complete understanding;a sentence
#s112	But we have some techniques that would allow us to do partial understanding of the sentence.
#c112	we;some techniques;us;partial understanding;the sentence
#s113	So I could mention some of them.
#c113	I;them
#s114	For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles.
#c114	example;we;techniques;us;the entities;relations;text articles
#s115	For example, recognizing the mentions of people, locations, organisations, etc in text.
#c115	example;the mentions;people;locations;organisations;text
#s116	So this is called entity extraction.
#c116	entity extraction
#s117	We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company.
#c117	We;the relations;example;this person;that place;this person;that person;this company;another company
#s118	Such relations can be extracted by using the current natural language processing techniques.
#c118	Such relations;the current natural language processing techniques
#s119	They're not perfect, but they can do well for some entities.
#c119	They;they;some entities
#s120	Some entities are harder than others.
#c120	Some entities;others
#s121	We can also do word sense disambiguation to some extent.
#c121	We;word;sense;disambiguation;some extent
#s122	We can figure out whether this word in this sentence would have certain meaning in another context.
#c122	We;this word;this sentence;certain meaning;another context
#s123	The computer could figure out it has a different meaning.
#c123	The computer;it;a different meaning
#s124	Again, it's not perfect, but you can do something in that direction.
#c124	it;you;something;that direction
#s125	We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative.
#c125	We;analysis;the weather sentence
#s126	This is especially useful for review analysis, for example.
#c126	review analysis;example
#s127	So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences.
#c127	examples;semantic analysis;they;us;partial understanding;the sentences
#s128	It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful.
#c128	It;us;a complete understanding;I;it;this sentence;it;us;understanding;the content
#s129	In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties.
#c129	terms;inference;we;the general difficulty;inference;uncertainties
#s130	This is a general challenging in artificial intelligence.
#c130	artificial intelligence
#s131	That's partly also because we don't have complete semantic representation for natural language text, so this is hard yet in some domains, perhaps in limited domains, when you have a lot of restrictions on the word uses, you maybe do may be able to perform inference.
#c131	we;complete semantic representation;natural language text;some domains;limited domains;you;a lot;restrictions;the word;you;inference
#s132	To some extent, but in general we cannot really do that.
#c132	some extent;we
#s133	reliably.
#s134	Speech Act analysis is also far from being done, and we can only do that analysis for various special cases.
#c134	Speech Act analysis;we;that analysis;various special cases
#s135	So this roughly gives you some idea about the state of the art.
#c135	you;some idea;the state;the art
#s136	And then we also talk a little bit about what we can't do.
#c136	we;what;we
#s137	And so we can't even do one hundred percent part of speech tagging.
#c137	we;one hundred percent part;speech tagging
#s138	Now this looks like a simple task, but think about the example here.
#c138	a simple task;the example
#s139	The two users of off may have different syntactic categories.
#c139	The two users;different syntactic categories
#s140	If you try to make a fine grained distinguishing, it's not that easy to figure out such differences.
#c140	you;a fine;it;such differences
#s141	It's also hard to do general, complete parsing, and again this same sentence that you saw before is example.
#c141	It;complete parsing;again this same sentence;you;example
#s142	This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope.
#c142	This ambiguity;you;example;you;a lot;knowledge;the context;the sentence;the background;order;who;the telescope
#s143	So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long.
#c143	the sentence;it;cases;the sentence
#s144	Imagine it has four or five prepositional phrases, and there are even more possibilities to figure out.
#c144	it;four or five prepositional phrases;even more possibilities
#s145	It's also hard to do precise deep semantic analysis, so here's example in the sentence.
#c145	It;precise deep semantic analysis;example;the sentence
#s146	John owns a restaurant.
#c146	John;a restaurant
#s147	How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers.
#c147	we;exactly the word;something;we;it;the meaning;computers
#s148	So as a result, we have robust and general natural language processing techniques that can process a lot of text data.
#c148	a result;we;robust and general natural language processing techniques;a lot;text data
#s149	In a shallow way, meaning we only do superficial analysis.
#c149	a shallow way;we;superficial analysis
#s150	For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence.
#c150	example;parts;speech tagging;deep understanding;we;the exact meaning;a sentence
#s151	On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text.
#c151	the other hand;the deeper understanding techniques;they;some unrestricted text
#s152	And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well.
#c152	you;the text domain;the use;words;these techniques
#s153	They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well.
#c153	They;techniques;the data;the training data;the program
#s154	The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing.
#c154	The data;the training data;the state;the art;natural language processing
#s155	Of course, within such a short amount of time, we can't really give you a complete view of NLP, which is big field an either expect to see multiple courses on natural language processing.
#c155	such a short amount;time;we;you;a complete view;NLP;big field;multiple courses;natural language processing
#s156	topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background.
#c156	topic;its relevance;the topic;we;it;you;the background
#s157	In case you haven't been exposed to that.
#c157	case;you
#s158	So what does that mean for text retrieval?
#c158	what;text retrieval
#s159	In text retrieval, we're dealing with all kinds of text.
#c159	text retrieval;we;all kinds;text
#s160	It's very hard to restrict the text to a certain domain.
#c160	It;the text;a certain domain
#s161	And we also often dealing with a lot of text data.
#c161	we;a lot;text data
#s162	So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval.
#c162	the NLP techniques;we;fairly shallow and NLP techniques;text retrieval
#s163	In fact, most search engines today use something called a bag of words representation.
#c163	fact;most search engines;something;a bag;words;representation
#s164	Now, this is probably the simplest representation you can possibly think of.
#c164	the simplest representation;you
#s165	That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words.
#c165	text data;simply a bag;words;we;individual words;all the orders;words
#s166	And we'll keep duplicated occurrences of words.
#c166	we;duplicated occurrences;words
#s167	So this is called a bag of words representation.
#c167	a bag;words;representation
#s168	When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order.
#c168	you;the text;this way;you;a lot;other information;it;the exact meaning;a sentence;we;the order
#s169	But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult.
#c169	this representation;most search tasks;the search task
#s170	If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions.
#c170	you;matching;the query words;a text document;chances;that document;the topic;exceptions
#s171	So in comparison, some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong.
#c171	comparison;some other tasks;example machine translation;you;the language;the translation
#s172	So in comparison, search task is all relatively easy.
#c172	comparison;search task
#s173	Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using.
#c173	Such a representation;the representation;a Google;Bing
#s174	Of course I put in parentheses is here, but not all.
#c174	I;parentheses
#s175	Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done.
#c175	many queries;the current search engines;they;a representation;the bag;words;more natural language processing
#s176	There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP.
#c176	another reason;we;the sophisticated NLP techniques;modern search engines;some retrieval techniques;the problem;NLP
#s177	So one example is word sense disambiguation.
#c177	one example;word sense disambiguation
#s178	Think about the world like Java.
#c178	the world;Java
#s179	It could mean coffee, or could mean program language.
#c179	It;coffee;program language
#s180	If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words.
#c180	you;the world;it;the user;the word;the query;other words
#s181	For example, I'm looking for usage of Java applet.
#c181	example;I;usage;Java applet
#s182	When I have applet there that implies.
#c182	I
#s183	Java Means program language.
#c183	Java Means program language
#s184	And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee.
#c184	that context;us;documents;Java;program language;those documents;Java;the document;a way;it;coffee
#s185	Then you would never match applet or with very small probability, right?
#c185	you;very small probability
#s186	So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation.
#c186	the case;some retrieval techniques;the goal;word sense disambiguation
#s187	Another example is.
#c187	Another example
#s188	Some technical code feedback which we will talk about later in some of the lectures.
#c188	Some technical code feedback;we;the lectures
#s189	This technical code would allow us to add additional words to the query and those additional words could be related to the query words.
#c189	This technical code;us;additional words;the query;those additional words;the query words
#s190	And these words can help matching documents where the original query words have not occurred.
#c190	these words;documents;the original query words
#s191	So this achieves to some extent.
#c191	some extent
#s192	Semantic matching of terms.
#c192	Semantic matching;terms
#s193	So those techniques also helped us bypass some of the difficulties in natural language processing.
#c193	those techniques;us;the difficulties;natural language processing
#s194	However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks.
#c194	the long run;we;deeper natural language processing techniques;order;the accuracy;the current search engines;it;complex search tasks
#s195	Or for question answering.
#c195	question
#s196	Google has recently launched Knowledge Graph and this is one step toward that goal.
#c196	Google;Knowledge Graph;one step;that goal
#s197	'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly.
#c197	knowledge graph;entities;their relations;the simple bag;words;representation;such technique;us;the search engine utility
#s198	Although this is still an open topic for research and exploration.
#c198	an open topic;research;exploration
#s199	In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines.
#c199	summary;this lecture;we;what;an NLP;We;the state;the art techniques;what;we;what;we;we;the why bag;words;representation;the dominant representation;modern search engines;deeper NLP;future search engines
#s200	If you want to know more, you can take a look at some additional readings.
#c200	you;you;a look;some additional readings
#s201	I only cited one here and that's a good starting point.
#c201	I;a good starting point
#s202	Thanks.
#c202	Thanks
410	402b4679-f01e-4d07-89a3-6c9817710ada	6
#s1	this lecture is about vector space between the model we're going to give introduction to its basic idea in the last lecture we talked about the different ways of designing our retrieval model which would give us a different the ranking function in this lecture whether they talk about the specific way of designing a ranking function called a vector space retrieval model and we're going to give a brief introduction to the basic idea after space model is a special case of similarity based models as we discussed before which means we assume relevance is roughly similarity between the document and the query now whether this assumption is true is actually a question but in order to solve a search problem we have to convert the vague notion of relevance into a more precise definition that can be implemented with programming languages so in this process we have to make a number of assumptions this is the first is something that we make here basically we assume that if a document is more similar to a query there another document then the first document would be assuming it'll be more relevant and then the second one and this is the basis for ranking documents in this approach again it's questionable whether this is really the best definition for relevance as we will see later there are other ways to model relevance the basic idea of vectors ways retrieval model is actually very easy to understand imagine or high dimensional space where each dimension corresponds to a term so here i issue a three dimensional space with three words programming library and presidential so each term here defines one dimension now we can consider vectors in this three dimensional space and we're going to assume that all our documents and the query will be placed in this vector space so for example one document might be represented by this vector D one
#c1	this lecture;vector space;the model;we;introduction;its basic idea;the last lecture;we;the different ways;our retrieval model;us;a different the ranking function;this lecture;they;the specific way;a ranking function;a vector space retrieval model;we;a brief introduction;the basic idea;space model;a special case;similarity based models;we;we;relevance;roughly similarity;the document;the query;this assumption;a question;order;a search problem;we;the vague notion;relevance;a more precise definition;programming languages;this process;we;a number;assumptions;something;we;we;a document;a query;another document;the first document;it;the basis;ranking documents;this approach;it;the best definition;relevance;we;other ways;relevance;the basic idea;vectors;retrieval model;imagine;high dimensional space;each dimension;a term;i;a three dimensional space;three words programming library;each term;one dimension;we;vectors;this three dimensional space;we;all our documents;the query;this vector space;example;one document;this vector D
#s2	now this means this document probably covers library and presidential but it doesn't really talk about programming right what does this mean in terms of representation of document that just means we're going to look at our document from the perspective of this vector we're going to ignore everything else basically what we see here is only the vector representation of the document of course the document has other information for example the orders of words are simply ignored and that's be cause we assume that the bag of words with temptation so with this representation you have already see T one central suggest a topic like a press agent library now this is different from another document which might be represented as a different about the D two here in this case the document covers programming and library but it doesn't talk about the presidential so what does this remind you well you can probably guess the topic is likely about programming language in the library is software library so this shows that by using this vector space recommendation week actually capture the differences between topics of documents now you can also imagine there are other vectors for example these three is pointing to that direction that might be about the presidential program and in fact that we can place all the documents in this vector space and there will be pointing to all kinds of directions and similarly we're going to place our query also in this space as another factor
#c2	this document;library;it;programming;what;terms;representation;document;we;our document;the perspective;this vector;we;everything;what;we;only the vector representation;the document;course;the document;other information;example;the orders;words;we;the bag;words;temptation;this representation;you;a topic;a press agent library;another document;the D;this case;the document;programming;library;it;the presidential;what;you;you;the topic;programming language;the library;software library;this vector space recommendation week;the differences;topics;documents;you;other vectors;example;that direction;the presidential program;fact;we;all the documents;this vector space;all kinds;directions;we;our query;this space;another factor
#s3	and then we're going to measure the similarity between the query vector an every document vector so in this case for example we can easily see D two seems to be the closest for two this query vector and therefore the two would be ranked above others so this was basically the main idea of the battle space model so to be more probably precise to be more precise that is based model is a framework in this framework we make the following assumptions first we represent a document that and query by a term vector so here are term can be any basic concept for example a word or a phrase or even engram of characters those are just sequence of characters inside the world each term is assumed that would define one dimension therefore end terms in our vocabulary with divine N dimensional space appear a vector would consist of a number of elements corresponding to the weights on different terms each document of actor is also similar it has a number of elements and each value of it your element his indicating the weight of the corresponding term here you can see we assume there are N dimensions therefore there are elements each corresponding to the weight on the particular term so the relevance in this case would be assumed to be the similarity between the two vectors therefore our ranking function is also defined as the similarity between the query vector and document vector now if i ask you to write a program to implement this approach in a search engine you would realize that this is far from clear right we haven't to say that a lot of things in detail therefore it's impossible to actually write a program to implement this that's why i said this is a framework and this has to be refined in order to actually suggest a particular ranking function that you can implement on a computer so what is this framework not self
#c3	we;the similarity;the query vector;this case;example;we;D;two this query vector;others;the main idea;the battle space model;model;a framework;this framework;we;the following assumptions;we;a document;query;a term vector;term;any basic concept;example;a word;a phrase;even engram;characters;sequence;characters;the world;each term;one dimension;terms;our vocabulary;divine N dimensional space;a vector;a number;elements;the weights;different terms;each document;actor;it;a number;elements;each value;it;your element;his;the weight;the corresponding term;you;we;N dimensions;elements;the weight;the particular term;the relevance;this case;the similarity;the two vectors;our ranking function;the similarity;the query vector;document vector;i;you;a program;this approach;a search engine;you;we;a lot;things;detail;it;a program;i;a framework;order;a particular ranking function;you;a computer;what;this framework
#s4	well it actually had in the set up many things that would be required in order to implement this function first it did not say how we should define or select the basic concepts exactly we clearly assume the concepts are orthogonal otherwise there will be redundancy for example if two synonyms are somehow distinguish it as a two different concepts then there would be defining two different line messages and that would clearly cause redundancy here over emphasizing of matching this concept because it would be as if you match the tool dimensions
#c4	it;the set;many things;order;this function;it;we;the basic concepts;we;the concepts;redundancy;example;two synonyms;it;a two different concepts;two different line messages;redundancy;this concept;it;you;the tool dimensions
#s5	well you actually match one semantic concept secondly it did not say how we exactly should place documents and the query in this space basically i showed you some examples of query and document vectors but where exactly should the vector for particular document point two so this is equivalent to how to define the term weights how do you computer those element values in those vectors now this was a very important question becaus tom waits in the query vector indicates importance of term so depending on how you assign the weight you might prefer some terms to be matched all the others similarly the term weather in the document that is also very meaningful with indicates how well the term characterizes the document if you got it wrong then you clearly don't represent this document accurately finally how to define the similarity measure is also not cater so these questions must be addressed before we can have operational function that we can actually implement using a programming language
#c5	you;one semantic concept;it;we;documents;the query;this space;i;you;some examples;query;document;vectors;where exactly should the vector;particular document point;the term;you;those element values;those vectors;the query vector;importance;term;you;the weight;you;some terms;all the others;the term weather;the document;the term;the document;you;it;you;this document;the similarity measure;these questions;we;operational function;we;a programming language
#s6	so how do we solve these problems is the main topic of the next election
#c6	we;these problems;the main topic;the next election
410	40831a36-c960-4eef-8c60-8ab212b52f8c	12
#s1	so average precision is computer for just one query but we generally experimented with many different queries and this is to avoid the variance across queries depending on the queries you use you might make different conclusions
#c1	average precision;computer;just one query;we;many different queries;the variance;queries;the queries;you;you;different conclusions
#s2	so it's better to use more queries if you use more queries than you would also have to take the average of the average precision over all these queries so how can we do that well you can naturally it would think of just doing arithmetic mean as we always tend to to think in this way so this would give us what's called a mean average precision or map in this case we take arithmetic mean of all the average precisions over a set of queries or topics
#c2	it;more queries;you;more queries;you;the average;the average precision;all these queries;we;you;it;we;this way;us;what;a mean average precision;map;this case;we;arithmetic mean;all the average precisions;a set;queries;topics
#s3	but as i just mentioned in another lecture is this good we call that we talked about the different ways of combining precision and recall an we conclude that the arithmetic me is not as good as the F measure
#c3	i;another lecture;we;we;the different ways;precision;we;the arithmetic;me;the F measure
#s4	but here it's the same we can also think about the alternative ways of aggregating the numbers don't just automatically assume that that's just to take a terrorist metheny of the average precision over these queries let's think about what's the best way of aggregating if you think about different ways naturally you would probably be able to think about another way which is joe match coming
#c4	it;we;the alternative ways;the numbers;a terrorist metheny;the average precision;these queries;'s;what;the best way;you;different ways;you;another way;joe match
#s5	and we called this kind of average G map but this is another way so now once you think about the two different ways of doing the same thing the natural question to ask is which one is better so so you use map orgy map again that's important question imagine you all again testing a new algorithms in by comparing it with your old algorithm in the search engine now you tested multiple topics now you've got the average precisions of all these topics now you are thinking of looking at the overall performance you have to take the average but which which is strategy would you use now first you should also think about the question would it make a difference can you think of scenarios where using one of them would make a difference that is they would give different rankings of those methods and that also means depending on the way you average or you take the average of these average positions you will get different conclusions this makes the question become even more important right so which one would you use well again if you look at the difference between these different ways of aggregating the average position you will realize in arithmetic me the sum is dominated by large values so what does the LG magic value have need it means the query is relatively easy you can have a hyper average position where is jim abbott tends to be affected more by little values and those are the queries that don't have good performance the average precision is low
#c5	we;this kind;average G map;another way;you;the two different ways;the same thing;the natural question;one;you;map orgy map;you;a new algorithms;it;your old algorithm;the search engine;you;multiple topics;you;the average precisions;all these topics;you;the overall performance;you;the average;strategy;you;you;the question;it;a difference;you;scenarios;them;a difference;they;different rankings;those methods;the way;you;you;the average;these average positions;you;different conclusions;the question;even more important right;which one;you;you;the difference;these different ways;the average position;you;arithmetic me;the sum;large values;what;the LG magic value;it;the query;you;a hyper average position;jim abbott;little values;the queries;good performance;the average precision
#s6	so if you think about improving the search engine for those difficult queries then G map would be preferred on the other hand that if you just want to have improvement all overall the kinds of queries or particularly popular queries that might be easy and you want to make the perfect and maybe map would be them prefer so again the answer depends on your users will users tasks and their preferences so the point that here is to think about multiple ways to solve the same problem and then come here then and think carefully about differences and which you want makes more sense often in one of them might make sense in one situation an another might make more sense in a different situation so it's important to freak out and what situations one is preferred as a special case of the mean average precision we can also think about the case where there is precisely one relevant document and this happens often for example in what's called a no item search where you know a target page let's say you wanted to find the amazon home page you have one random document there
#c6	you;the search engine;those difficult queries;G map;the other hand;you;improvement;the kinds;queries;particularly popular queries;you;the perfect and maybe map;them;the answer;your users;the point;multiple ways;the same problem;differences;you;more sense;them;sense;one situation;more sense;a different situation;it;what situations;a special case;the mean average precision;we;the case;precisely one relevant document;example;what;a no item search;you;a target page;'s;you;the amazon home page;you;one random document
#s7	and you hope to find it that's called a known item search in that case this precise the one relevant document or in another application like a question answering maybe there's only one answer there
#c7	you;it;a known item search;that case;this precise the one relevant document;another application;a question;only one answer
#s8	so if you rank the answers then your goal is rank that one particular answer on top so in this case you can easily verify the average position we basically boil down two reciprocal rank that is one over R where R is the rank position of that single relevant document so if that document is ranked on the very top eyes one and then it's one for reciprocal rank if it's ranked at the second then it's one over two etc and then we can also take a average of all these average casino reciprocal rank over a set of topics and that would give us something called a mean receiver core rank at a very popular value for no item search any random problem where you have just one rather than the item now again here you can see this all actually is meaningful here and this are is basically indicating how much effort a user would have to make in order to find that relevant document if it's rendered on the top there's no effort that you have to make a little effort but if it is ranked at one hundred then you actually have to read presumably one hundred documents in order to find it so in this sense all is also meaningful measure and the reciprocal rank will take the reciprocal of our instead of using all directly so one natural question here is why not simply using all the imagine fewer designer measure to measure performance of a ranking system when there is only one relevant item you might have thought about using all directly as the measure after all that matches the users effort right but think about if you take an average of this over a large number of topics again it would make a difference for one single topic using our or using one over R wouldn't make any difference it's the same larger are with correspond to a small one overall but the difference would only show when show up when you have many topics so again think about the average of mean reciprocal rank versus average of just on what's the difference do you see any difference with this difference change the order of systems in our conclusion and this it turns out that there is actually a big difference and if you think about it if you want to think about it
#c8	you;the answers;your goal;one particular answer;top;this case;you;the average position;we;two reciprocal rank;R;R;the rank position;that single relevant document;that document;the very top eyes;it;reciprocal rank;it;it;we;a average;all these average casino reciprocal rank;a set;topics;us;something;a mean receiver core rank;a very popular value;any random problem;you;the item;you;how much effort;a user;order;that relevant document;it;the top;no effort;you;a little effort;it;you;presumably one hundred documents;order;it;this sense;meaningful measure;the reciprocal rank;our;one natural question;all the imagine fewer designer measure;performance;a ranking system;only one relevant item;you;the measure;the users;you;an average;a large number;topics;it;a difference;one single topic;our;R;any difference;it;correspond;the difference;you;many topics;the average;mean reciprocal rank;average;what;the difference;you;any difference;this difference;the order;systems;our conclusion;it;a big difference;you;it;you;it
#s9	and then yourself then pause the video basically the difference is if you take some of our directory then again will be dominated by large values of our
#c9	yourself;the video;the difference;you;our directory;large values;our
#s10	so what are those values those are basically large values that indicate the lowly ranked results that means the relevant item is ranked very low down on the list and the sum the orders also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list but from a user 's perspective we care more about the highly ranked documents so by taking this transformation by using reciprocal rank here we emphasize more on the difference on the top and think about the difference between one and two it will make a big difference in one over R gotta think about the one hundred and one hundred aware and one the one make much difference if you use this but if you use this there will be a big difference in one hundred and one thousand
#c10	what;those values;large values;the lowly ranked results;the relevant item;the list;the orders;the average;those relevant documents;the lower portion;the ranked list;a user 's perspective;we;the highly ranked documents;this transformation;reciprocal rank;we;the difference;the top;the difference;it;a big difference;R;the one;much difference;you;you;a big difference
#s11	right
#s12	so this is not the desirable on the other hand while two won't make much difference so this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense so to summarize we shows that the precision recall curve can characterize the overall accuracy of a ranked list and we emphasize that the actual utility over ranked list it depends on how many top random results are user would actually examine some users will examine more than others an average prison is the standard measure for comparing two ranking methods it combines precision and recall and it's a sensitive to the rank of every random the document
#c12	the other hand;much difference;another case;multiple choices;the same thing;you;one;more sense;we;the precision recall curve;the overall accuracy;a ranked list;we;the actual utility;ranked list;it;how many top random results;user;some users;others;an average prison;the standard measure;two ranking methods;it;precision;recall;it;the rank
410	435befd6-e92e-4529-813e-e9bcacc58a6d	29
#s1	So to summarize, our discussion of recommender systems in some sense, the filtering task or recommending task is easy and in some other senses, and the task is actually difficult, so it's easy because the users expectations, though in this case the system it takes initiative to push the information to the user so the user doesn't really make.
#c1	our discussion;recommender systems;some sense;the filtering task;recommending task;some other senses;the task;it;the users expectations;this case;it;initiative;the information;the user;the user
#s2	An effort, so any recommendation is better than nothing, right?
#c2	An effort;any recommendation;nothing
#s3	So unless you recommend the order, noisy items or useless documents, if you can recommend some useful information, users general would appreciate it, so that's in that sense that's easy.
#c3	you;the order;you;some useful information;users;it;that sense
#s4	However, filtering is actually much harder task than retrieval because it has you have to make a binary decision and you can't afford waiting for a lot of items.
#c4	filtering;much harder task;retrieval;it;you;a binary decision;you;a lot;items
#s5	And then you're going to see.
#c5	you
#s6	whether 1 item is better than others.
#c6	1 item;others
#s7	You have to make a decision when you see this item.
#c7	You;a decision;you;this item
#s8	The thing about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user.
#c8	The thing;the news filtering;you;the news;you;the news;a user
#s9	If you wait for a few days.
#c9	you;a few days
#s10	Even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased.
#c10	you;accurate recommendation;the most relevant news;the utility
#s11	Another reason why it's hard, it's because the data sparseness.
#c11	it;it
#s12	If you think of this as a learning problem in collaborative filtering, for example, it's purely based on learning from the past ratings, so if you don't have many ratings, this really not that much you can do.
#c12	you;a learning problem;collaborative filtering;example;it;the past ratings;you;many ratings;you
#s13	And yeah, just mentioned this cold start problem.
#c13	this cold start problem
#s14	This is actually a very serious serious problem, but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem.
#c14	a very serious serious problem;course;strategies;the problem;different strategies;you;the problem
#s15	You can use for example, more user information to assess their similarity instead of using the preferences of these users on these items, there may be additional information available about the user.
#c15	You;example;more user information;their similarity;the preferences;these users;these items;additional information;the user
#s16	etc and.
#s17	And we also talked about the two strategies for filtering task one is content based, where we look at item similarity.
#c17	we;the two strategies;filtering task;one;we;item similarity
#s18	The other is collaborative filtering where we look at the user similarity and they obviously can be combined in a practical system.
#c18	collaborative filtering;we;the user similarity;they;a practical system
#s19	You can imagine the general would have to be combined so that will give us a hybrid strategy for filtering.
#c19	You;the general;us;a hybrid strategy;filtering
#s20	And we also could recall that we talked about push versus pull as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are certain users in the pull mode.
#c20	we;we;push;pull;two strategies;access;the text data;recommended system;users;the push mode;search engines;certain users;the pull mode
#s21	Obviously the tool should be combined and they can be combined to have a system that can support the user with multiple mode information access.
#c21	the tool;they;a system;the user;multiple mode information access
#s22	So in future we could anticipate the such a system to be more useful to user.
#c22	future;we;the such a system;user
#s23	An either this is the active research area, so there are a lot of new algorithms being proposed all the time.
#c23	the active research area;a lot;new algorithms
#s24	In particular, those new algorithms tend to use a lot of context information.
#c24	those new algorithms;a lot;context information
#s25	Now the context here could be the context of the user and that it could be also context of documents or items.
#c25	the context;the context;the user;it;context;documents;items
#s26	The items are not isolated and they are connected in many ways.
#c26	The items;they;many ways
#s27	The users might form social network as well, so there's a rich context there that we can leverage in order to really solve the problem well and then that's an active research area where also machine learning algorithms that have been applied.
#c27	The users;social network;a rich context;we;order;the problem;an active research area;algorithms
#s28	Here are some additional readings in the Handbook called Recommender Systems and has a collection of a lot of good.
#c28	some additional readings;the Handbook;Recommender Systems;a collection;a lot;good
#s29	Articles that can give you an overview of a number of specific approaches to recommended systems.
#c29	Articles;you;an overview;a number;specific approaches;recommended systems
410	4453a049-7597-4df4-9b9b-67c2d124a116	87
#s1	We can compute this maximum regular estimated by using the EM algorithm.
#c1	We;this maximum;the EM algorithm
#s2	So in the E-step, we now have to introduce more hidden variables because we have more topics.
#c2	the E;-;step;we;more hidden variables;we;more topics
#s3	So our hidden variable Z now, which is a topic indicator, can take more than two values.
#c3	our hidden variable;Z;a topic indicator;more than two values
#s4	Specifically, will take a K plus one values with B denoting the background and one through K to denote all the K topics.
#c4	a K;one values;B;the background;K;all the K topics
#s5	So now the E step as you can recall is augmented data and by predicting the values of the hidden variable.
#c5	the E step;you;augmented data;the values;the hidden variable
#s6	So we're going to predict for word whether the word has come from one of these K+1 distributions.
#c6	we;word;the word;these K+1 distributions
#s7	This equation allows us to predict the probability that the word W in Document "D is generated from topic theta sub j
#c7	This equation;us;the probability;Document;topic theta;j
#s8	And the bottom one is the predicted probability that this word has been generated from the background.
#c8	the bottom one;the predicted probability;this word;the background
#s9	Note that we use Document D here to index the word.
#c9	we;Document D;the word
#s10	Why?
#s11	Because Whether a word is from a particular topic, actually depends on the document.
#c11	a word;a particular topic;the document
#s12	Can you see why?
#c12	you
#s13	Well, it's through the pi.
#c13	it;the pi
#s14	The pis are tied to each document.
#c14	The pis;each document
#s15	Each document can have a potentially different pis, right?
#c15	Each document;a potentially different pis
#s16	The pis will then affect our prediction, so the pis are here, and this depends on the document.
#c16	The pis;our prediction;the pis;the document
#s17	And that might give a different guess of word for word in different documents, and that's desirable.
#c17	a different guess;word;word;different documents
#s18	In both cases we are using the bayes rule as I explained, basically assessing the likelihood of generating word in from each distribution and is normalized.
#c18	both cases;we;the bayes rule;I;the likelihood;word;each distribution
#s19	What about the M-step?
#c19	the M-step
#s20	Well, we may recall the M step is to take advantage of the inferred Z values to split the counts and then collect the right counts to re estimate parameters.
#c20	we;the M step;advantage;the inferred Z values;the counts;the right counts;parameters
#s21	So in this case we can re estimate our coverage probability and this is re estimated based on collecting all the words in the document.
#c21	this case;we;our coverage probability;all the words;the document
#s22	And that's why we have the count of the word in document and sum over all the words.
#c22	we;the count;the word;document;all the words
#s23	And then we're going to look at the to what extent this word belongs to the topic's theta sub-j, and this part is our guess from E-step.
#c23	we;what extent;this word;the topic's theta;sub;-;j;this part;our guess;E;-;step
#s24	This tells us how likely this word is actually from theta sub-j, and when we multiply them together we get the discounted count that's allocated for topic theta sub-j and we normalize this over all the topics we get the distribution over all the topics to indicate the coverage.
#c24	us;this word;theta sub;-;j;we;them;we;the discounted count;topic theta;sub;-;j;we;all the topics;we;the distribution;all the topics;the coverage
#s25	And similarly, the bottom one is to re-estimate the probability of word for topic.
#c25	the bottom one;the probability;word;topic
#s26	In this case we're using exactly the same count.
#c26	this case;we;exactly the same count
#s27	You can see this is the same discounted count, it tells us to what extent we should allocate this word to topic theta sub-j.
#c27	You;the same discounted count;it;us;what extent;we;this word;theta;sub;-;j.
#s28	But the normalization is different because in this case we are interested in the word distribution.
#c28	the normalization;this case;we;the word distribution
#s29	So we simply normalize this over all the words.
#c29	we;all the words
#s30	This is different.
#s31	In contrast, here we normalized among all the topics.
#c31	contrast;we;all the topics
#s32	It would be useful to take a comparison between the two.
#c32	It;a comparison
#s33	This gives us different distributions and these tells us how to improve the parameters?
#c33	us;different distributions;us;the parameters
#s34	And as I just explained in both E step formulas, we have a maximum likelihood estimator based on the allocated word "counts to "topic theta sub-j.
#c34	I;both E step formulas;we;a maximum likelihood estimator;topic theta;sub;-;j.
#s35	Now this phenomena is actually general phenomenon in all the EM algorithms in the M step, you generate computed expected count of event based on the E step result and then you just collect the relevant counts for a particular parameter.
#c35	this phenomena;general phenomenon;all the EM algorithms;the M step;you;computed expected count;event;the E step result;you;the relevant counts;a particular parameter
#s36	and re-estimate with normalizing.
#s37	Typically.
#s38	So in terms of computation of the EM algorithm, we can.
#c38	terms;computation;the EM algorithm;we
#s39	Actually, just keep counting various events and then normalize them.
#c39	various events;them
#s40	And when we think in this way, we also have a more concise way of presenting the EM algorithm.
#c40	we;this way;we;a more concise way;the EM algorithm
#s41	It actually helps us better understand the formulas.
#c41	It;us;the formulas
#s42	So I'm going to go over this in some detail.
#c42	I;some detail
#s43	So as the algorithm, we first initialize all the unknown parameters randomly.
#c43	the algorithm;we;all the unknown parameters
#s44	In our case we are interested in all those coverage parameters-- pis--and word distributions, thetas.
#c44	our case;we;all those coverage parameters-- pis;word distributions;thetas
#s45	And we just randomly normalize them.
#c45	we;them
#s46	This is the initialization step, and then we will repeat until likelihood converges.
#c46	the initialization step;we;likelihood converges
#s47	Now how do we know whether likelihood converges we're going to compute likelihood at each step and compare the current likelihood with the previous likelihood if it doesn't change much and we're going to say stop right?
#c47	we;we;likelihood;each step;the current likelihood;the previous likelihood;it;we
#s48	So in each step we can do E step and M step in the E step we're going to augment the data by predicting the hidden variable values.
#c48	each step;we;E step;M step;the E step;we;the data;the hidden variable values
#s49	In this case the hidden variable Z sub DW indicates whether word in W in D is from topic or background, an if it's from a topic which topic?
#c49	this case;the hidden variable Z sub;DW;word;W;D;topic;background;it;a topic
#s50	So if you look at the E step formulas essentially we're actually normalizing these counts.
#c50	you;the E step;we;these counts
#s51	At all, sorry, these are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution.
#c51	probabilities;the word;each distribution;you;the prediction;word;topic theta;sub;-;j;the probability;that theta sub;j;a word distribution;the world;the probability;the word;that distribution
#s52	And I said it's proportional to this because in completing the implementation of EM algorithm you can just keep count counter for this quantity and then in the end you just normalize it.
#c52	I;it;the implementation;EM algorithm;you;count counter;this quantity;the end;you;it
#s53	So the normalization here is over all the topics and then you will get a probability.
#c53	the normalization;all the topics;you;a probability
#s54	Now in the M step we do the same and we are going to collect these.
#c54	the M step;we;we
#s55	Allocated counts for each topic.
#c55	Allocated counts;each topic
#s56	And we split words among the topics.
#c56	we;words;the topics
#s57	And then we're going to normalize them in different ways to obtain the re-estimate.
#c57	we;them;different ways;the re;-;estimate
#s58	So, for example, we can normalize among all the topics to get re estimate of Pi the coverage.
#c58	example;we;all the topics;estimate;Pi;the coverage
#s59	Or we can renormalize based on the.
#c59	we
#s60	For all the words and that would give us a word distribution.
#c60	all the words;us;a word distribution
#s61	So it's useful to think of the algorithm in this way, because when you implement, you can just use.
#c61	it;the algorithm;this way;you;you
#s62	Variables to keep track of these quantities in each case.
#c62	Variables;track;these quantities;each case
#s63	And then you just normalize these variables to make them a distribution.
#c63	you;these variables;them
#s64	Now I did not put the constraint for this one
#c64	I;the constraint;this one
#s65	and I intentionally leave this as exercise for you
#c65	I;exercise;you
#s66	and you can see what's the normalizer for this one.
#c66	you;what;the normalizer;this one
#s67	It's of a slightly different form, but it's essentially the same as the one that you have seen here.
#c67	It;a slightly different form;it;the one;you
#s68	Namely this one.
#c68	Namely this one
#s69	So in general, in the implementation of EM algorithm you will see you accumulated counts various counts and then you normalize them.
#c69	the implementation;EM algorithm;you;you;various counts;you;them
#s70	So to summarize, we introduced the PLSA model, which is a mixture model with K unigram language models representing K topics.
#c70	we;the PLSA model;a mixture model;K unigram language models;K topics
#s71	And we also added a predetermined background language model to help discover discriminating topics.
#c71	we;a predetermined background language model;discriminating topics
#s72	Because this background language model can help attract the common terms.
#c72	this background language model;the common terms
#s73	And, We show that with maximum likelihood estimator we can discover topical knowledge from text data.
#c73	We;maximum likelihood estimator;we;topical knowledge;text data
#s74	In this case PLSA allows us to discover two things.
#c74	this case;PLSA;us;two things
#s75	One is k-word distributions, each representing a topic and the other is the proportion of each topic in each document.
#c75	k-word distributions;a topic;the proportion;each topic;each document
#s76	And such detailed characterization of coverage of topics in documents can enable a lot of further analysis.
#c76	such detailed characterization;coverage;topics;documents;a lot;further analysis
#s77	For example, we can aggregate the documents in the particular time period to assess the coverage of a particular topic in a time period that would allow us to generate the temporal chains of topics.
#c77	example;we;the documents;the particular time period;the coverage;a particular topic;a time period;us;the temporal chains;topics
#s78	We can also aggregate topics covered in documents associated with a particular author, and then we can characterize the topics written by this author, etc.
#c78	We;topics;documents;a particular author;we;the topics;this author
#s79	And in addition to this, we can also cluster terms and cluster documents.
#c79	addition;we;terms;cluster documents
#s80	In fact, each topic can be regarded as a cluster, so we already have term clusters.
#c80	fact;each topic;a cluster;we;term clusters
#s81	And the higher probability words can be regarded as in belonging to one cluster.
#c81	the higher probability words;one cluster
#s82	Represented by the topic.
#c82	the topic
#s83	Similarly documents can be clustered in the same way.
#c83	documents;the same way
#s84	We can assign a document to the topic cluster that's covered most in the document.
#c84	We;a document;the topic cluster;the document
#s85	So remember pis indicate to what extent each topic is covered in the document.
#c85	pis;what extent;each topic;the document
#s86	We can assign the document to the topic cluster that has the highest pi.
#c86	We;the document;the topic cluster;the highest pi
#s87	And in general, there are many useful applications of this technique.
#c87	many useful applications;this technique
410	44df41bc-04d3-41ca-ac51-dbd22dc98305	77
#s1	In general, we can use the empirical counts of events in the observed data to estimate probabilities.
#c1	we;the empirical counts;events;the observed data;probabilities
#s2	and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts.
#c2	a commonly used technique;a maximum likelihood estimate;we;the observed accounts
#s3	So if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we simply normalize the counts of segments that contain this word.
#c3	we;we;we;these probabilities;the probability;we;a word;segment;we;the counts;segments;this word
#s4	So let's first take a look at the data here.
#c4	's;a look;the data
#s5	On the right side you see I listed some hypothesizes that data these are segments.
#c5	the right side;you;I;some hypothesizes;segments
#s6	And in some segments you see both words occur.
#c6	some segments;you;both words
#s7	Their indicator as once for both columns.
#c7	Their indicator;both columns
#s8	In some other cases, only one word occurs, so only that column has one and the other column has zero.
#c8	some other cases;only one word;that column;the other column
#s9	And of course in some other cases, none of the words occur, so they are both zeros.
#c9	course;some other cases;none;the words;they;zeros
#s10	And For estimating these probabilities, we simply need to collect the three counts.
#c10	these probabilities;we;the three counts
#s11	So the three counts of 1st, the count of W. 1
#c11	So the three counts;1st;the count;W.
#s12	and that's the total number of segments that contain world W one.
#c12	the total number;segments
#s13	It's just the ones in the column of W one we can just count how many ones we have seen there.
#c13	It;just the ones;the column;we;how many ones;we
#s14	The second counter is for word 2 and we just count the ones in the second column.
#c14	The second counter;word;we;the ones;the second column
#s15	And these this would give us a total number of segments that contain W2.
#c15	us;a total number;segments;W2
#s16	The third account is when both words occurred, so this is time we're going to count the segments where both columns have ones.
#c16	The third account;both words;time;we;the segments;both columns;ones
#s17	And then so this would give us the total number of segments where we have seen both W and W2.
#c17	us;the total number;segments;we;both W;W2
#s18	Once we have these counts, we can just normalize.
#c18	we;these counts;we
#s19	These counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information.
#c19	These counts;n;the total number;segments;us;the probabilities;we;mutual information
#s20	Now there is a small problem.
#c20	a small problem
#s21	When we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sample and in general we would believe that it's potentially possible for award to occur in any context.
#c21	we;zero counts;this case;we;a zero probability;our data;maybe a small sample;we;it;award;any context
#s22	So to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we don't get a zero probability in any case.
#c22	this problem;we;a technique;smoothing;some small constant;discounts;we;a zero probability;any case
#s23	Now, the best way to understand the smoothing is imagine that we actually.
#c23	the best way;the smoothing;we
#s24	Observed more data than we actually have.
#c24	more data;we
#s25	We will pretend we observe some pseudo segments that are illustrated on the top on the right side of the slide and these pseudo segments would contribute additional counts of these words so that no event will have zero probability probability.
#c25	We;we;some pseudo segments;the top;the right side;the slide;these pseudo segments;additional counts;these words;no event;zero probability probability
#s26	Now, in particular, we introduce the four pseudo segments.
#c26	we;the four pseudo segments
#s27	Each is weighted 1/4.
#s28	And these represent the four different combinations of occurrences of these words.
#c28	the four different combinations;occurrences;these words
#s29	So now each event, each combination will have at least one count or at least non zero counter.
#c29	each combination;at least one count;at least non zero counter
#s30	From these pseudo segment.
#c30	these pseudo segment
#s31	So in the actual segments that we observed, it's OK if we haven't observed all the combinations.
#c31	the actual segments;we;it;we;all the combinations
#s32	So more specifically, you can see the point of five.
#c32	you;the point
#s33	Here actually comes from the two ones in the two pseudo segments, because each is weighted 1/4, we added them up.
#c33	the two ones;the two pseudo segments;we;them
#s34	We get .5.
#c34	We
#s35	And similarly this .05 comes from one single pseudo segment that indicates the two words occur together.
#c35	one single pseudo segment;the two words
#s36	And of course, in the denominator we add the total number of pseudo segments that we added.
#c36	course;the denominator;we;the total number;pseudo segments;we
#s37	In this case we added a 4th through the segments.
#c37	this case;we;a 4th;the segments
#s38	Each is weighted 1/4, so the total the sum is actually one.
#c38	the total;the sum
#s39	So that's why in the denominator you still want there.
#c39	the denominator;you
#s40	So this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery.
#c40	the discussion;the mutual information;syntagmatic relation discovery
#s41	No.
#s42	So, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words.
#c42	the cinematic relation;correlations;occurrences;two words
#s43	We introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable X conditional entropy, which measures the entropy of X. Given we know why.
#c43	We;the three concepts;information theory;random variable X conditional entropy;the entropy;X.;we
#s44	And mutual information of X&Y which matches the entropy reduction of X. Due to knowing why or entropy reduction of why do too knowing eggs?
#c44	And mutual information;X&Y;the entropy reduction;X.;reduction;eggs
#s45	They are the same, so these three concepts are actually very useful for other applications as well.
#c45	They;these three concepts;other applications
#s46	That's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations.
#c46	we;some time;detail;syntagmatic relations
#s47	In particular, mutual information is a principled way for discovering such a relation.
#c47	mutual information;a principled way;such a relation
#s48	It allows us to have values computer on different pairs of words that are comfortable, and so we can rank these pairs and discover the strongest cinematical relationship from collection of documents.
#c48	It;us;values;different pairs;words;we;these pairs;the strongest cinematical relationship;collection;documents
#s49	Now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery.
#c49	some relation;syntactic medical relation discovery;paradigmatically relation discovery
#s50	So we already discussed the possibility of using BM 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word.
#c50	we;the possibility;BM;terms;the context;the candidates;medical relations;the candidate word
#s51	But here, once we use mutual information to discover Syntagmatic relations, we can also represent the context with this mutual information as weights.
#c51	we;mutual information;Syntagmatic relations;we;the context;this mutual information;weights
#s52	So this would give us another way to represent the context.
#c52	us;another way;the context
#s53	Of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity.
#c53	a word;a cat;we;all the words;we;these words;computer similarity;these words;their context similarity
#s54	So this provides yet another way to do term waiting for paradigmatic.
#c54	yet another way;term
#s55	A relation discovery an.
#c55	A relation discovery;an
#s56	So to summarize, this whole part about word Association mining, we introduce the two basic associations, called Paradigmatic and Syntagmatic relations.
#c56	word Association mining;we;the two basic associations;Paradigmatic and Syntagmatic relations
#s57	These are fairly general.
#s58	They can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities.
#c58	They;any items;any language;the units;they;phrases;entities
#s59	Are we introduced multiple statistical approaches for discovering them?
#c59	we;multiple statistical approaches;them
#s60	Then it showing that pure statistical approaches are visible?
#c60	it;pure statistical approaches
#s61	Available for discovering both kinds of relations, and they can be combined to perform.
#c61	both kinds;relations;they
#s62	Join the analysis as well.
#c62	the analysis
#s63	These approaches can be applied to any text with no helmet human effort.
#c63	These approaches;any text;no helmet human effort
#s64	And mostly becausw.
#c64	And mostly becausw
#s65	They are based on counting of words.
#c65	They;counting;words
#s66	Yet they can actually discover interesting relations of words.
#c66	they;interesting relations;words
#s67	We can also use different ways to define context and segment and this would lead to some interesting variations of applications.
#c67	We;different ways;context;segment;some interesting variations;applications
#s68	For example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations.
#c68	example;the context;a few words;a word;sentence;maybe paragraphs;different contexts;you;different flavors;paradigmatic relations
#s69	And similarly, counting Co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations.
#c69	Co occurrences;'s;mutual information;syntagmatic relations;we;the segment;the segment;an arrow;text window;longer text article;us;different kinds;associations
#s70	These discovery associations can support them.
#c70	These discovery associations;them
#s71	Any other applications in both information retrieval and text data mining.
#c71	Any other applications;both information retrieval;text data mining
#s72	So here are some recommended readings.
#c72	some recommended readings
#s73	If you want to know more about the topic, the 1st is a book with a chapter on locations which is quite relevant to the topic of these lectures.
#c73	you;the topic;the 1st;a book;a chapter;locations;the topic;these lectures
#s74	The second is the article about the using various statistical measures to discover lexical atoms.
#c74	the article;various statistical measures;lexical atoms
#s75	Those are phrases that are non composition compositional or for example hot dog is not really a dog that's hot.
#c75	phrases;example;hot dog;a dog
#s76	Blue chip is not a chip that's blue, and the paper has a discussion about to some techniques for discovering such phrases.
#c76	Blue chip;a chip;the paper;a discussion;some techniques;such phrases
#s77	The third one is new paper on unified way to discover both paradigmatic a relation and select medical relations using random walks on world graphs.
#c77	The third one;new paper;unified way;a relation and select medical relations;random walks;world graphs
410	48b37a2f-5ca3-4b7b-9bfc-d841da37c566	83
#s1	So now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters.
#c1	's;the problem;'s;the two different ways;parameters
#s2	One is called maximum likelihood estimate that I already just mentioned.
#c2	maximum likelihood estimate;I
#s3	The other is Bayesian estimation.
#c3	Bayesian estimation
#s4	So in Maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here.
#c4	Maximum likelihood estimation;we;the data likelihood;it;this expression
#s5	Where we define the estimate as  arg max of the probability of X given Theta.
#c5	we;the estimate;  arg max;the probability;X;Theta
#s6	And so arg max here just means it's actually a function that would return the argument that gives the function maximum value as the value, so the value of arg max is not the value of this function, but rather the argument that has made the function reach maximum.
#c6	arg max;it;a function;the argument;the function maximum value;the value;the value;arg max;the value;this function;the function
#s7	So in this case the value of argmax is Theta.
#c7	this case;the value;argmax;Theta
#s8	It's the Theta that makes the probability of X given Theta reach its maximum, so this estimate intuitively also makes sense, and it's often very useful, and it seeks the parameters that best explain the data.
#c8	It;the Theta;the probability;X;given Theta;its maximum;this estimate;sense;it;it;the parameters;the data
#s9	But it has a problem when the data is too small, because when the data points are too small, there are very few data points.
#c9	it;a problem;the data;the data points;very few data points
#s10	The sample is small, then if we trust data entirely and try to fit the data and then we will be biased.
#c10	The sample;we;data;the data;we
#s11	So in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability.
#c11	the case;text data;'s;our observed 100 words;another word;text mining;our maximum likelihood estimator;that word;zero probability
#s12	Because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data.
#c12	a non zero probability;probability mass;some observed words;terms;the likelihood;the observed data
#s13	But this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining.
#c13	this zero probability;all the unseen words;we;the distribution;the topic;text mining
#s14	So one way to address this problem is actually to use Bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters.
#c14	one way;this problem;Bayesian estimation;we;both the data;all our prior knowledge;the parameters
#s15	We assume that we have some prior belief about the parameters.
#c15	We;we;some prior belief;the parameters
#s16	Now in this case, of course, so we are not going to look at just the data, but also look at the prior so the prior here is defined by P of Theta.
#c16	this case;course;we;just the data;the prior;P;Theta
#s17	And this means we will impose some preference on certain Thetas over others.
#c17	we;some preference;certain Thetas;others
#s18	And by using Bayes rule that I have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter.
#c18	Bayes rule;I;we;the likelihood function;the prior;us;this posterior probability;the parameter
#s19	Now a full explanation of Bayes Rule and some of these things related to Bayesian reasoning would be outside the scope of this course, but I just give a brief introduction because this is a general knowledge that might be useful for you, so the Bayes rule is basically defined here.
#c19	a full explanation;Bayes Rule;these things;Bayesian reasoning;the scope;this course;I;a brief introduction;a general knowledge;you;the Bayes rule
#s20	And allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X.
#c20	us;one conditional probability;X;Y;terms;the conditional probability;Y;X.
#s21	And you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often the rule is used for making inferences of a variable.
#c21	you;the two probabilities;two conditional probabilities;the order;the two variables;the rule;inferences;a variable
#s22	So let's take a look at it again, we can assume that P of X encodes our prior belief about the X. That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others.
#c22	's;a look;it;we;P;X encodes;our prior belief;the X.;we;any other data;our belief;X;what;we;some X values;higher probability;others
#s23	And this probability of X given Y is a conditional probability, and this is our posterior belief about X, because this is our belief about X values after we have observed Y. Given that we have observed Y, now what do we believe about X now, do we believe some values have high probabilities than others?
#c23	this probability;X;Y;a conditional probability;our posterior belief;X;our belief;X values;we;Y.;we;Y;what;we;X;we;some values;high probabilities;others
#s24	Now, the two probabilities are related through this can be regarded as the probability of the observed evidence Y here given a particular X.
#c24	the two probabilities;the probability;the observed evidence;Y;a particular X.
#s25	So you can think about X as our hypothesis.
#c25	you;X;our hypothesis
#s26	And we have some prior belief about which hypothesis to choose and after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior here and the likelihood of observing this Y if X is indeed true.
#c26	we;some prior belief;which hypothesis;we;Y;we;our belief;this updating formula;the combination;our prior;the likelihood;this Y;X
#s27	So much for a detour about Bayes Rule.
#c27	a detour;Bayes Rule
#s28	So in our case, what we're interested in is inferring the theta values so we have a prior here.
#c28	our case;what;we;the theta values;we
#s29	That includes our prior knowledge about the parameters.
#c29	our prior knowledge;the parameters
#s30	And then we have the data likelihood here that would tell us which parameter value can explain the data well.
#c30	we;the data likelihood;us;which parameter value;the data
#s31	The posterior probability combines both of them.
#c31	The posterior probability;them
#s32	So it represents a compromise of the two preferences.
#c32	it;a compromise;the two preferences
#s33	And in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability.
#c33	such a case;we;this posterior probability;a theta;this posterior probability
#s34	And this estimator is called the Maximum a Posteriori or MAP estimate.
#c34	this estimator;the Maximum;a Posteriori or MAP estimate
#s35	And this estimate is a more general estimate than the maximum likelihood estimate.
#c35	this estimate;a more general estimate;the maximum likelihood estimate
#s36	Because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here.
#c36	we;our prior;a noninformative prior meaning;it;all the theta values;no preference;we;the maximum likelihood estimator;such a case;it;this likelihood value
#s37	The same as here.
#s38	OK, but if we have some informative prior, some bias towards certain values, then MAP estimate can allow us to incorporate that, but the problem here of course is how to define the prior.
#c38	we;some bias;certain values;, then MAP estimate;us;the problem;course;the prior
#s39	There's no free lunch, and if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable.
#c39	no free lunch;you;the problem;more knowledge;we;that knowledge;knowledge
#s40	Otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate.
#c40	your estimate;maximum likelihood estimate
#s41	Now let's look at the Bayesian estimation in more detail.
#c41	's;the Bayesian estimation;more detail
#s42	OK, so I show the theta values as just one dimension value and that's a simplification of course.
#c42	I;the theta values;just one dimension value;a simplification;course
#s43	So we're interested in which value of data is optimal.
#c43	we;which value;data
#s44	So now first we have the prior.
#c44	we;the prior
#s45	The prior tells us some theta values are more likely than others.
#c45	us;some theta values;others
#s46	We believe, for example, these values are more likely than the values like here or here or other places.
#c46	We;example;these values;the values;other places
#s47	So this is our prior.
#c47	our prior
#s48	And then we have our data likelihood.
#c48	we;our data likelihood
#s49	In this case, the data also tells us which values of theta are more likely and that just means those theta values can best explain our data.
#c49	this case;the data;us;which values;theta;those theta values;our data
#s50	And then when we combine the two, we get the posterior distribution and that's just a compromise of the two.
#c50	we;we;the posterior distribution;just a compromise
#s51	It would say that it's somewhere in between, so we can now look at some interesting point estimates of theta.
#c51	It;it;we;some interesting point estimates;theta
#s52	Now this point represents the mode of prior.
#c52	this point;the mode
#s53	That means the most likely parameter value according to our prior before we observe any data.
#c53	the most likely parameter value;we;any data
#s54	This point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability.
#c54	This point;the maximum likelihood estimate;the theta;the data;the maximum probability
#s55	Now this point is interesting.
#c55	this point
#s56	It's the posterior mode, it's the.
#c56	It;the posterior mode;it
#s57	It's the most likely value of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate.
#c57	It;the most likely value;theta;the posterior distribution;it;a good compromise;the prior mode;the maximum likehood estimate
#s58	In general, in Bayesian inference we are interested in the distribution of all these parameter values.
#c58	Bayesian inference;we;the distribution;all these parameter values
#s59	As you see, here is there's a distribution over Theta values that you can see here P of theta given X.
#c59	you;a distribution;Theta values;you;P;theta;X.
#s60	So the problem of Bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on Theta.
#c60	the problem;Bayesian inference;this posterior distribution;other interesting quantities;Theta
#s61	So I showed F of Theta here as an interesting variable that we want to compute.
#c61	I;F;Theta;an interesting variable;we
#s62	But in order to compute this value, we need to know the value of Theta.
#c62	order;this value;we;the value;Theta
#s63	In Bayesian inference, we treat data as uncertain variable.
#c63	Bayesian inference;we;data;uncertain variable
#s64	So we think about all the possible values of Theta.
#c64	we;all the possible values;Theta
#s65	Therefore we can estimate the value of this function F as the expected value of F according to the posterior distribution of data given the observed evidence X. As a special case, we can assume F of Theta is just equal to Theta.
#c65	we;the value;this function;the expected value;F;the posterior distribution;data;the observed evidence;X.;a special case;we;F;Theta;Theta
#s66	In this case we get the expected value of Theta.
#c66	this case;we;the expected value;Theta
#s67	That's basically the posterior mean that gives us also one point of Theta.
#c67	us;one point;Theta
#s68	  
#s69	And it's sometimes the same as posterior mode, but it's not always the same, so it gives us another way to estimate the parameters.
#c69	it;posterior mode;it;it;us;another way;the parameters
#s70	So this is a general illustration of Bayesian estimation and Bayesian inference.
#c70	a general illustration;Bayesian estimation;Bayesian inference
#s71	inference.
#c71	inference
#s72	And later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics.
#c72	you;topic mining;we;some prior knowledge;the topics
#s73	So to summarize, we introduced the language model which is basically probability distribution over text.
#c73	we;the language model;probability distribution;text
#s74	It's also called a generative model for text data.
#c74	It;a generative model;text data
#s75	The simplest language model is unigram language model.
#c75	The simplest language model;unigram language model
#s76	It's basically a word distribution.
#c76	It;a word distribution
#s77	We introduced the concept of likelihood function which is the probability of data given some model.
#c77	We;the concept;likelihood function;the probability;data;some model
#s78	And this function is very important.
#c78	this function
#s79	Given a particular set of parameter values, this function can tell us which X, which data point has a higher likelihood, higher probability.
#c79	a particular set;parameter values;this function;us;which X;data point;a higher likelihood;higher probability
#s80	Given a data point, sorry, given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate.
#c80	a data point;a data sample X;we;this function;parameter values;the probability;the observed data;the maximum likelihood estimate
#s81	We also talked about the Bayesian estimation or influence.
#c81	We;the Bayesian estimation;influence
#s82	In this case we must define a prior on the parameters P of Theta, and then we're interested in computing the posterior distribution of the parameters which is proportional to the prior and the likelihood.
#c82	this case;we;a prior;the parameters;P;Theta;we;the posterior distribution;the parameters;the likelihood
#s83	And this kind of distribution would allow us, then, to infer any derived values from Theta.
#c83	this kind;distribution;us;any derived values;Theta
410	4a54f790-991c-44bb-ab62-713cbef84ad1	88
#s1	This lecture is about the sentiment classification.
#c1	This lecture;the sentiment classification
#s2	If we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case.
#c2	we;the elements;the opinion representation;maybe just the sentiment classification;this case
#s3	So suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion.
#c3	we;who;the opinion holder;what;the opinion target;the content;context;the opinion
#s4	Then we mainly need to decide the opinion sentiment of the review.
#c4	we;the opinion sentiment;the review
#s5	So this is a case of just using sentiment classification for understanding opinion.
#c5	a case;sentiment classification;understanding opinion
#s6	Sentiment classification can be defined more specifically as follows:
#c6	Sentiment classification
#s7	The input is opinionated text object.
#c7	The input;opinionated text object
#s8	The output is typically, a sentiment label or sentiment tag, and that can be designed in two ways.
#c8	The output;a sentiment label;sentiment tag;two ways
#s9	One is polarity analysis where we have categories such as positive, negative or neutral.
#c9	polarity analysis;we;categories
#s10	The other is emotion analysis.
#c10	emotion analysis
#s11	That can go beyond polarity to characterize the feeling of the opinion holder.
#c11	polarity;the feeling;the opinion holder
#s12	In the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web.
#c12	the case;polarity analysis;we;numerical ratings;you;some reviews;the web
#s13	Five might denote the most positive and one maybe at most negative, for example.
#c13	example
#s14	In general you have just discrete categories to characterize the sentiment.
#c14	you;just discrete categories;the sentiment
#s15	In emotion analysis, of course, there are also different ways to design the categories.
#c15	emotion analysis;course;different ways;the categories
#s16	The six most frequently used categories are happy, sad, fearful, angry, surpised and disgusted.
#c16	The six most frequently used categories
#s17	So as you can see, the task is essentially a classification task or categorisation task.
#c17	you;the task;a classification task;categorisation task
#s18	As we've seen before, so it's a special case of text categorization.
#c18	we;it;a special case;text categorization
#s19	This also means any text categorization method can be used to do sentiment classification.
#c19	any text categorization method;classification
#s20	Now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique.
#c20	course;you;the accuracy;sentiment classification;some improvement;regular text categorization technique;simple text categorization technique
#s21	In particular, it needs two kinds of improvements.
#c21	it;two kinds;improvements
#s22	One is to use more sophisticated features that may be more appropriate for sentiment tagging, as I will discuss more in a moment.
#c22	more sophisticated features;sentiment tagging;I;a moment
#s23	The other is to consider the order of these categories.
#c23	the order;these categories
#s24	An especially polarity analysis, very clear that order here and so these categories are not all that independent.
#c24	An especially polarity analysis;that order;these categories
#s25	There is order among them, and so it's useful to consider the order.
#c25	order;them;it;the order
#s26	For example, we could use ordinal regression to do, and that's something that will talk more about later.
#c26	example;we;ordinal regression;something
#s27	So now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis.
#c27	's;some features;text categorization;text mining;them;sentiment analysis
#s28	So let's start from the simplest one, which is character n-grams.
#c28	's;the simplest one;character;grams
#s29	You can just have a sequence of characters as a unit, and they can be mixed with different n(s),  different lengths.
#c29	You;a sequence;characters;a unit;they;different n(s;different lengths
#s30	And this is a very general way, and a very robust way to represent the text data.
#c30	a very general way;a very robust way;the text data
#s31	You could do that for any language pretty much.
#c31	You;any language
#s32	And this is also robust to spelling errors or recognition errors, right?
#c32	errors;recognition errors
#s33	So if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly.
#c33	you;the word;1 character;this representation;you;this word;it;the text
#s34	So misspelled word and the correct form can be matched because they contain some common n-grams of characters.
#c34	So misspelled word;the correct form;they;some common n;grams;characters
#s35	But of course such a representation would not be as discriminative as words.
#c35	course;such a representation;words
#s36	So next we have word n-grams, a sequence of words and again we can mix them with different lengths.
#c36	we;word;n;grams;a sequence;words;we;them;different lengths
#s37	Uni Grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly.
#c37	Uni Grams;a lot;text processing tasks;words;well designed features;humans;communication;they;many tasks;it;sentiment analysis
#s38	For example, we might see a sentence like it's not good or it's not as good as something else.
#c38	example;we;a sentence;it;it;something
#s39	So in such a case, if you just take a good and that would suggest positive, it's not good, so it's not accurate, but if you take the bigram, not good together, and then it's more accurate, so longer n-grams are generally more discriminative and they are more specific.
#c39	such a case;you;a good;it;it;you;the bigram;it;-grams;they
#s40	If you match it and it says a lot
#c40	you;it;it;a lot
#s41	and it's accurate.
#c41	it
#s42	It's unlikely, very ambiguous.
#c42	It
#s43	But it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories.
#c43	it;overfitting;the machine learning program;such features;such unique features;categories
#s44	An obviously that kind of classifier won't generalize well to future data when such discriminating features will not necessarily occur.
#c44	An obviously that kind;classifier;future data;such discriminating features
#s45	So that's a problem of overfitting.
#c45	a problem;overfitting
#s46	That's not desirable.
#s47	We can also consider part of speech tag n-grams, if we can do part of speech tagging and for example, adjective, noun could form a pair.
#c47	We;part;speech tag;grams;we;part;speech tagging;example;noun;a pair
#s48	We can also mix N grams of words and N grams of part of speech tags.
#c48	We;N grams;words;N grams;part;speech tags
#s49	For example, the word great might be followed by a noun and this could become a feature, a hybrid feature.
#c49	example;the word;a noun;a feature;a hybrid feature
#s50	That could be useful for sentiment analysis.
#c50	sentiment analysis
#s51	So next we can also have word classes, so these classes can be syntactic like a part of speech tags.
#c51	we;word classes;these classes;a part;speech tags
#s52	Or could be semantic and they might represent concepts in the thesaurus or ontology like word net.
#c52	they;concepts;the thesaurus;ontology;word net
#s53	Or they can be recognized the named entities like people or place and these categories can be used to enrich the representation as additional features.
#c53	they;the named entities;people;place;these categories;the representation;additional features
#s54	We can also learn word clusters empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words.
#c54	We;word clusters;example;we;mining associations;words;we;cluster;paradigmatically related words;sementically related words
#s55	And these clusters can be features to supplement the word based representation.
#c55	these clusters;the word;based representation
#s56	Furthermore, we can also have frequent pattern syntax and these could be frequent word set.
#c56	we;frequent pattern syntax
#s57	The words that formed a pattern do not necessarily occur together or next to each other.
#c57	The words;a pattern
#s58	But we also have locations where the words might occur more closely together.
#c58	we;locations;the words
#s59	And such patterns provide a more discriminative features than words, obviously, and they may also generalize better than just the regular n-grams because they are frequent, so you can expect them to occur also in test data so they have a lot of advantages, but they might still face the problem of overfitting as the features become more complex.
#c59	such patterns;a more discriminative features;words;they;just the regular n;grams;they;you;them;test data;they;a lot;advantages;they;the problem;overfitting;the features
#s60	This is the problem in general, and the same is true for parse tree based features where you can use a parse tree to derive features such as frequent subtrees or paths, and those are even more discriminating, but they also are more likely to cause overfitting.
#c60	the problem;parse tree based features;you;a parse tree;features;frequent subtrees;paths;they;overfitting
#s61	And in General, Patton discovery algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful.
#c61	General;Patton discovery algorithms;feature construction;they;us;a larger space;possible features;words
#s62	So in general, natural language processing is very important to derive complex features.
#c62	natural language processing;complex features
#s63	They can enrich text representation.
#c63	They;text representation
#s64	So for example, this is a simple sentence that I showed you long time ago, and in another lecture.
#c64	example;a simple sentence;I;you;another lecture
#s65	So from these words we can only derive simple world n-grams representations or character n-grams.
#c65	these words;we;simple world n-grams representations;character n;grams
#s66	But with NLP we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act.
#c66	NLP;we;the representation;a lot;other information;part;speech tags;trees;entities
#s67	Now with such enriched information, of course, then we can generate a lot of other features, more complex features, like a mixed grams of word and part of speech tags.
#c67	such enriched information;course;we;a lot;other features;, more complex features;a mixed grams;word;part;speech tags
#s68	Or even a part of parse tree.
#c68	Or even a part;parse tree
#s69	So in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application.
#c69	feature design;categorization accuracy;it;a very important part;any machine learning application
#s70	In general, I think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features.
#c70	I;it;you;machine learning;error analysis;domain knowledge;designing features
#s71	So first you want to use domain knowledge and your understanding of the problem to design seed features.
#c71	you;domain knowledge;your understanding;the problem;seed features
#s72	And you can also define a basic feature space with a lot of possible features for the Machine learning program to work on.
#c72	you;a basic feature space;a lot;possible features;the Machine learning program
#s73	And machine learning can be applied to select the most effective features or construct the new features that feature learning.
#c73	machine learning;the most effective features;the new features
#s74	And these features can then be further analyzed by humans through error analysis.
#c74	these features;humans;error analysis
#s75	And you can look at the categorization errors and then further analyze what features can help you recover from those errors or what features cause overfitting and cause those errors, and so this can lead to feature validation that would revise the feature set
#c75	you;the categorization errors;what;features;you;those errors;what;overfitting;those errors;feature validation;the feature
#s76	and then you can iterate and we might consider using a different feature space.
#c76	you;we;a different feature space
#s77	So NLP enriches text representation.
#c77	So NLP enriches text representation
#s78	As I just said and because it enriches the feature space.
#c78	I;it;the feature space
#s79	It allows much larger search space of features.
#c79	It;much larger search space;features
#s80	And there are also many meaningful features that can be very useful for a lot of tasks.
#c80	many meaningful features;a lot;tasks
#s81	But be careful not to use a lot of complicated features because it can cause overfitting or otherwise you have to do the training carefully, not to let overfitting happen.
#c81	a lot;complicated features;it;overfitting;you;the training;overfitting
#s82	So a main challenge in designing features, a common challenge is to optimize the tradeoff between exhaustivity and specificity.
#c82	a main challenge;designing features;a common challenge;the tradeoff;exhaustivity;specificity
#s83	And this trade off, it turns out to be very difficult.
#c83	it
#s84	Now, exhaustivity means we want the features to actually have high coverage of a lot of documents.
#c84	exhaustivity;we;the features;high coverage;a lot;documents
#s85	And so in that sense, you wanted features to be frequent.
#c85	that sense;you;features
#s86	Specificity requires the feature to be discriminative, so naturally infrequent features tend to be more discriminating, so this really caused tradeoff between frequent versus infrequent features, and that's why feature design is generally an art.
#c86	Specificity;the feature;infrequent features;tradeoff;features;feature design;an art
#s87	That's perhaps the most important part in applying machine learning to any problem in particular.
#c87	the most important part;machine;any problem
#s88	In our case, for text categorization, or more specifically, sentiment classification.
#c88	our case;text categorization;classification
410	4b0bd537-3a1f-4cae-a68f-f2845ecf4f35	161
#s1	This lecture is about the implementation of text retrieval systems.
#c1	This lecture;the implementation;text retrieval systems
#s2	In this lecture we will discuss how we can implement text retrieval method to build a search engine.
#c2	this lecture;we;we;text retrieval method;a search engine
#s3	The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries.
#c3	The main challenge;a lot;text data;a query;many queries
#s4	This is a typical text retrieval system architecture.
#c4	a typical text retrieval system architecture
#s5	We can see the documents are first processor via tokenizer to get that tokenized units, for example words, and then these words or tokens will be processed by a indexer that would create the index which is a data structure for the search engine to use to quickly answer query.
#c5	We;the documents;first processor;tokenizer;that tokenized units;example words;these words;tokens;a indexer;the index;a data structure;the search engine;query
#s6	And the query will be going through a similar process in step, so that tokenizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other.
#c6	the query;a similar process;step;that tokenizer;the query;the text;the same way;the same units
#s7	And the queries representation would then be given to the scorer which would use the index to quickly answer a users query by scoring the documents and then ranking them.
#c7	the queries representation;the scorer;the index;a users query;the documents;them
#s8	The results will be given to the user and then the user can look at the results and provide some feedback that can be expressed judgments about which documents are good, which documents are bad or implicit feedback such as click slows so the user doesn't have to do any, anything extra the user would just look at the results and skip some and click on some results to view.
#c8	The results;the user;the user;the results;some feedback;judgments;documents;documents;click;the user;the user;the results;some results
#s9	So these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skiped ones.
#c9	these interaction signals;the system;the ranking accuracy;the view;the documents;the skiped ones
#s10	So a search engine system then can be divided into 3 parts.
#c10	a search engine system;3 parts
#s11	The first part is the indexer, and the second part is a scorer that responds to the users query, and the third part is a feedback mechanism.
#c11	The first part;the indexer;the second part;a scorer;the users query;the third part;a feedback mechanism
#s12	Now typically the indexer is done in the offline manner, so you can preprocess the collected data and to build the inverted index which we will introduce in the moment.
#c12	the indexer;the offline manner;you;the collected data;the inverted index;we;the moment
#s13	And this data structure can then be used by the online module, which is a scorer to process users query dynamically and quickly generate search results.
#c13	this data structure;the online module;a scorer;users query;search results
#s14	The feedback mechanism can be done online or offline depending on the method.
#c14	The feedback mechanism;the method
#s15	The implementation of the indexer, and the scorer is fairly standard and this is the main topic of this lecture.
#c15	The implementation;the indexer;the scorer;the main topic;this lecture
#s16	In the next few lectures, the feedback mechanism, on the other hand, has variations depends on which method is used.
#c16	the next few lectures;the feedback mechanism;the other hand;variations;which method
#s17	So that is usually down in the algorithm specific way.
#c17	the algorithm specific way
#s18	Let's first talk about the tokenizer.
#c18	's;the tokenizer
#s19	Tokenization is to normalize lexical units into the same form so that semantically similar words can be matched with each other.
#c19	Tokenization;lexical units;the same form;semantically similar words
#s20	In the language like English.
#c20	the language;English
#s21	Stemming is often used and this is where map all the inflectional forms of words into the same root form.
#c21	Stemming;all the inflectional forms;words;the same root form
#s22	So for example, computer computation and computing can all be matched to the root form compute.
#c22	example;computer computation;computing;the root form compute
#s23	This way all these different forms of computing can be matched with each other.
#c23	all these different forms;computing
#s24	Normally this is good idea to increase the coverage of documents that matched with this query, but it's also not always beneficial because sometimes the subtle of this difference between computer and computation might still suggest the difference in the coverage of the content, but in most cases stemming seems to be beneficial.
#c24	good idea;the coverage;documents;this query;it;the subtle;this difference;computer;computation;the difference;the coverage;the content;most cases
#s25	When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries, because it's not obvious where the boundary is as there's no space to separate them.
#c25	we;the text;some other languages;example;we;some special challenges;the text;the word boundaries;it;the boundary;no space;them
#s26	So here, of course we have to use some language specific natural language processing techniques.
#c26	course;we;some language specific natural language processing techniques
#s27	Once we do tokenization then we would index the text documents and that is to convert the documents into some data structure that can enable fast search.
#c27	we;tokenization;we;the text documents;the documents;some data structure;fast search
#s28	The basic idea is do pre-compute as much as we can basically.
#c28	The basic idea;we
#s29	So the most commonly used indexes, is called inverted index.
#c29	the most commonly used indexes;inverted index
#s30	And this has been used to in many search engines to support basically search algorithms, sometimes other indices, for example, a document index might be needed in order to support the feedback.
#c30	many search engines;basically search algorithms;sometimes other indices;example;a document index;order;the feedback
#s31	Like I said, in this kind of techniques are not really standard in that they vary a lot according to feedback methods.
#c31	I;this kind;techniques;they;feedback methods
#s32	To understand why we want to use inverted index, it would be useful for you to think about how you would respond to a single term query quickly.
#c32	we;inverted index;it;you;you;a single term query
#s33	So if you want to use more time to think about that, pause the video.
#c33	you;more time;the video
#s34	So think about how you can preprocess the text data so that you can quickly respond to a query with just one word?
#c34	you;the text data;you;a query;just one word
#s35	If you have thought about that question, you might realize that what the best is to simply create a list of documents that match every term in the vocabulary.
#c35	you;that question;you;what;a list;documents;every term;the vocabulary
#s36	In this way you can basically pre construct the answers.
#c36	this way;you;the answers
#s37	So when you see a term, you can simply just fetch the ranked list of documents for that term and return the list to the user.
#c37	you;a term;you;the ranked list;documents;that term;the list;the user
#s38	So that's the fastest way to respond to a single term query.
#c38	the fastest way;a single term query
#s39	Now the idea of inverted indexes actually basically like that, we're going to pre construct the such a index that would allow us to quickly find the all the documents that match a particular term.
#c39	the idea;inverted indexes;we;the such a index;us;the all the documents;a particular term
#s40	So let's take a look at this example.
#c40	's;a look;this example
#s41	We have three documents here and these are the documents that you have seen in some previous lectures.
#c41	We;three documents;the documents;you;some previous lectures
#s42	Suppose we want to create inverted index for these documents.
#c42	we;inverted index;these documents
#s43	Then we would maintain a dictionary in the dictionary we will have one entry for each term, and we're going to store some basic statistics about the term.
#c43	we;a dictionary;the dictionary;we;one entry;each term;we;some basic statistics;the term
#s44	For example, the number of documents that match the term or the total number of  frequency of the term, which means we would count the duplicated occurrences of the term.
#c44	example;documents;the term;the total number;  frequency;the term;we;the duplicated occurrences;the term
#s45	And so, for example news.
#c45	example news
#s46	This term ocurred in all the three documents.
#c46	This term;all the three documents
#s47	So the count of documents is 3.
#c47	the count;documents
#s48	And you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model.
#c48	you;we;this count;documents;document frequency;some statistics;the vector space model
#s49	Can you think of that?
#c49	you
#s50	So what weighting heuristic would need this count?
#c50	this count
#s51	That's IDF, right?
#c51	IDF
#s52	Inverse document frequency.
#c52	Inverse document frequency
#s53	So IDF is the property over turn and we can compute it right here.
#c53	IDF;the property;turn;we;it
#s54	So with the document account here, it's easy to compute the IDF, either at this time or when we build index or running time when we see a query.
#c54	the document account;it;the IDF;this time;we;index;time;we;a query
#s55	Now, in addition to these basic statistics, we also store all the documents that match the news, and these entries are stored in a file called postings.
#c55	addition;these basic statistics;we;all the documents;the news;these entries;a file;postings
#s56	So in this case it matched three documents and store information about these three documents here.
#c56	this case;it;three documents;store information;these three documents
#s57	This is the document ID, document one, and the frequency is 1, the TF is 1 for news.
#c57	the document ID;the frequency;the TF;news
#s58	In the second document, it's also 1 etc.
#c58	the second document;it
#s59	So from this list that we can get all the documents that match at the term news and we can also know the frequency of news in these documents.
#c59	this list;we;all the documents;the term news;we;the frequency;news;these documents
#s60	So if the query has just one word news and we can easily look up this table to find the entry and go quickly to the postings and fetch all the documents that match news.
#c60	the query;just one word news;we;this table;the entry;the postings;all the documents;news
#s61	So let's take a look at another term.
#c61	's;a look;another term
#s62	This time, let's take a look at the word presidential.
#c62	's;a look;the word
#s63	This word occured in only one document, document three, so the document frequency is one.
#c63	This word;only one document;document;the document frequency
#s64	But it occurred twice in this document, so the frequency count is 2 and the frequency count is useful in some other retrieval method where we might use the frequency to assess the popularity of a term in the collection, and similarly will have a pointer to the postings here.
#c64	it;this document;the frequency count;the frequency count;some other retrieval method;we;the frequency;the popularity;a term;the collection;a pointer;the postings
#s65	And in this case there is only one entry here, because, the term occured in just one document, and that's here.
#c65	this case;only one entry;the term;just one document
#s66	The document ID is 3 an it occured it twice.
#c66	The document ID;it;it
#s67	So this is the basic idea of inverted index.
#c67	the basic idea;inverted index
#s68	It's actually pretty simple, right?
#c68	It
#s69	With this structure we can easily fetch all the documents that match a term and this will be the basis for scoring documents for a query.
#c69	this structure;we;all the documents;a term;the basis;documents;a query
#s70	Now sometimes we also want to store the positions of these terms.
#c70	we;the positions;these terms
#s71	So, in many of these cases, the term occured just once in the document, so there's only one position.
#c71	these cases;the term;the document;only one position
#s72	For example in this case.
#c72	example;this case
#s73	But in this case, the time occurred twice, so we will store two positions.
#c73	this case;the time;we;two positions
#s74	Now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say 5 words or 10 words or whether the matching of the two query terms is in fact a phrase of two words.
#c74	the position information;the matching;query terms;a small window;'s;5 words;10 words;the matching;the two query terms;fact;a phrase;two words
#s75	This can all be checked quickly by using the position information.
#c75	the position information
#s76	So why is inverted index good for fast search?
#c76	fast search
#s77	We just talked about the possibility of using it to answer a single word query.
#c77	We;the possibility;it;a single word query
#s78	And that's very easy.
#s79	What about the multiple term queries?
#c79	the multiple term
#s80	Let's first look at the some special cases of the Boolean query.
#c80	's;the some special cases;the Boolean query
#s81	Boolean query is basically boolean expression like this.
#c81	Boolean query;basically boolean expression
#s82	So I want the relevant document to match both term A and the term B right?
#c82	I;the relevant document;both term A;B
#s83	So that's one conjunctive query?
#c83	one conjunctive query
#s84	Or I want the relevant documents to match term a or term b.
#c84	I;the relevant documents;term
#s85	That's a disjunctive query.
#c85	a disjunctive query
#s86	Now how can we answer such a query by using inverted index?
#c86	we;such a query;inverted index
#s87	If you think a bit about it would be obvious cause we had simply fetch all the documents that match term a, and also fetch all the documents that match term B and then just take the intersection to answer a query A&B or to take the Union to answer the query A or
#c87	you;a bit;it;we;all the documents;term;all the documents;term B;the intersection;a query A&B;the Union;the query;A
#s88	B.
#c88	B.
#s89	So this is all very easy to answer.
#s90	It's going to be very quick now.
#c90	It
#s91	What about the multi term keyword query?
#c91	the multi term keyword query
#s92	We talked about vector space model for example and we would match such query with document generated score and the score is based on aggregated term weights.
#c92	We;vector space model;example;we;such query;document generated score;the score;aggregated term weights
#s93	So in this case it's not a Boolean query.
#c93	this case;it;a Boolean query
#s94	But the scoring can be acted out in a similar way.
#c94	the scoring;a similar way
#s95	Basically, it's similar to disjunctive Boolean query.
#c95	it;disjunctive Boolean query
#s96	Basically, it's like a or b. We take the union of all the documents that match at least one query term, and then we would aggregate the term weights.
#c96	it;b.;We;the union;all the documents;at least one query term;we;the term weights
#s97	So this is a basic idea of using that inverted index for scoring documents in general, and we're going to talk about this in more detail later, but for now, let's just look at the question why is inverted index a good idea.
#c97	a basic idea;that inverted index;documents;we;more detail;'s;the question;inverted index;a good idea
#s98	Basically why is it more efficient than sequentially just scanning documents?
#c98	it;documents
#s99	Like, this is the obvious approach.
#c99	the obvious approach
#s100	You can just compute the score for each document and then you can then score them.
#c100	You;the score;each document;you;them
#s101	Sorry, you can then sort them.
#c101	you;them
#s102	This is a straight forward method, but this is going to be very slow.
#c102	a straight forward method
#s103	Imagine the web it has a lot of documents.
#c103	the web;it;a lot;documents
#s104	If you do this then it would take a long time to answer your query.
#c104	you;it;a long time;your query
#s105	So the question now is why would the inverted index be much faster it has to do with the word distribution in text.
#c105	the question;the inverted index;it;the word distribution;text
#s106	So here's some common phenomenon of word distribution in text.
#c106	some common phenomenon;word distribution;text
#s107	There are some language independent patterns that seem to be stable.
#c107	some language independent patterns
#s108	And these patterns are basically characterized by the following pattern of few words, like the common words the, a, or we occur very frequently in text.
#c108	these patterns;the following pattern;few words;the common words;the, a;we;text
#s109	So they account for a large percent of occurrences of words.
#c109	they;a large percent;occurrences;words
#s110	But mostly words would occur just rarely.
#c110	words
#s111	There are many words that occur just once, let's say in the document or once in the collection.
#c111	many words;'s;the document;the collection
#s112	There are many such singletons.
#c112	many such singletons
#s113	It's also true that most frequently words in one corpus may have to be rare in another.
#c113	It;most frequently words;one corpus
#s114	That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context.
#c114	the general phenomenon;many cases;the exact words;context;context
#s115	So this phenomenon is characterized by what's called Zipf's law.
#c115	this phenomenon;what;Zipf's law
#s116	This law says that the rank of word multiplied by the frequency of the word is roughly a constant.
#c116	This law;the rank;word;the frequency;the word
#s117	So formally, if we use F(w) to denote the frequency, r(w) to denote the rank of a word, then this is the formula.
#c117	we;the frequency;r(w;the rank;a word;the formula
#s118	It basically says the same thing, just mathematical term we will see is basically a constant, right?
#c118	It;the same thing;just mathematical term;we
#s119	So as so there is also parameter Alpha that might be adjusted to better fit any empirical observations.
#c119	parameter Alpha;any empirical observations
#s120	So if I plot the word frequencies in sorted order, then you can see this more easily.
#c120	I;the word frequencies;sorted order;you
#s121	The X axis is basically the world rank and this is r(w), Y axis is a word frequency or F(w).
#c121	The X axis;the world rank;Y axis;a word frequency
#s122	Now, this curve basically shows that the product of the two is roughly the constant.
#c122	this curve;the product
#s123	Now if you look at these words, we can see they can be separated into three groups.
#c123	you;these words;we;they;three groups
#s124	In the middle it's the intermediate frequency words.
#c124	the middle;it;the intermediate frequency words
#s125	These words tend to occur in quite a few documents, but they are not like those most frequent words and they're also not very rare.
#c125	These words;quite a few documents;they;those most frequent words;they
#s126	So they tend to be often used in queries, and they also tend to have high TF IDF weights, these intermediate frequency words.
#c126	they;queries;they;high TF IDF weights;these intermediate frequency words
#s127	But if you look at the left part of the curve.
#c127	you;the left part;the curve
#s128	These are the highest frequency words they occur very frequently.
#c128	the highest frequency words;they
#s129	They are usually stop words like the, a, we, of, etc.
#c129	They;words;the, a, we
#s130	Those words are very, very frequent.
#c130	Those words
#s131	They are in fact too frequent to be discriminated and they are generally not very useful for retrieval.
#c131	They;fact;they;retrieval
#s132	So they, are often removed and this is called stop words removal, so you can use pretty much just the count of words in the collection to kind of infer what words might be stop words.
#c132	they;words;you;just the count;words;the collection;what words;words
#s133	Those are basically the highest frequency words and they also occupy a lot of space in the inverted index.
#c133	the highest frequency words;they;a lot;space;the inverted index
#s134	You can imagine the posting entries for such a word would be very long, and therefore if you can remove such words, you can save a lot of space in the inverted index.
#c134	You;the posting entries;such a word;you;such words;you;a lot;space;the inverted index
#s135	We also show that the tail part, which has a lot of rare words.
#c135	We;a lot;rare words
#s136	Those words don't occur very frequently and there are many such words.
#c136	Those words;many such words
#s137	Those words are actually very useful for search.
#c137	Those words;search
#s138	Also, if a user happens to be interested in such a topic, but because they're rare.
#c138	a user;such a topic;they
#s139	It's often true that the users are unnecessary interested in those words, but retain them would allow us to match such a document accurately, and they generally have very high IDFs.
#c139	It;the users;those words;them;us;such a document;they;very high IDFs
#s140	So what kind of data structures should we use to store inverted index?
#c140	what kind;data structures;we;inverted index
#s141	It has two parts, right?
#c141	It;two parts
#s142	If you recall, we have a dictionary and we also have postings.
#c142	you;we;a dictionary;we;postings
#s143	The dictionary has modest size, although for the web it's still going to be very large, but compared with postings, it's modest.
#c143	The dictionary;modest size;the web;it;postings;it
#s144	And we also need to have fast random access to the entries 'cause we want to look up with a query term very quickly.
#c144	we;fast random access;the entries;we;a query term
#s145	So therefore we prefer to keep such a dictionary in memory if it's possible, or if the collection is not very large, this is feasible; but the collection is very large, then it's in general not possible if the vocabulary size is very large.
#c145	we;such a dictionary;memory;it;the collection;the collection;it;the vocabulary size
#s146	Obviously we can't do that so, but in general that's our goal, so the data structures that we often use for storing dictionary would be directly accessed data structures like hash table, or B-tree if we can't store everything in memory, we can use this and  try to build a structure that would allow you to quickly look up  entries.
#c146	we;our goal;the data structures;we;dictionary;data structures;hash table;B-tree;we;everything;memory;we;a structure;you;  entries
#s147	For postings, they're huge.
#c147	postings;they
#s148	And in general we don't have to have direct access to a specific entry we generate with.
#c148	we;direct access;a specific entry;we
#s149	Just look up a sequence of document IDs and frequencies for all the documents that match  a query term, so we would read those entries sequentially.
#c149	a sequence;document IDs;frequencies;all the documents;a query term;we;those entries
#s150	And therefore, because it's large, we generally have store postings on disk, so they have to stay on disk.
#c150	it;we;store postings;disk;they;disk
#s151	And they would contain information such as document IDs, term frequencies or term positions etc.
#c151	they;information;document IDs;term frequencies;term positions
#s152	Now, because they are very large, compression is often desirable.
#c152	they;compression
#s153	Now this is not only to save disk space, and this is of course one benefit of compression.
#c153	disk space;course;one benefit;compression
#s154	It's not going to occupy that much space.
#c154	It;that much space
#s155	But it's also to help improving speed.
#c155	it;speed
#s156	Can you see why?
#c156	you
#s157	We know that input and output would cost a lot of time in comparison with the time taken by CPU, so CPU is much faster.
#c157	We;input;output;a lot;time;comparison;the time;CPU;CPU
#s158	But IO takes time, and so by compressing the inverted index, the posting files will become smaller and the entries that we have to read into memory to process query time would be smaller and then so we can reduce the amount of trafficing IO and that can save a lot of time.
#c158	IO;time;the inverted index;the posting files;the entries;we;memory;query time;we;the amount;IO;a lot;time
#s159	Of course we have to then do more processing of the data when we uncompress the data in the memory.
#c159	we;more processing;the data;we;the data;the memory
#s160	But as I said in the CPU is fast, so overall we can still save time.
#c160	I;the CPU;we;time
#s161	So compression here is both to save disk space and through speed up the loading of inverted index.
#c161	compression;disk space;speed;the loading;inverted index
410	4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	93
#s1	So this is indeed a general idea of the expectation maximization, or EM algorithm.
#c1	a general idea;the expectation maximization;EM algorithm
#s2	So in all the EM algorithms, we introduce a hidden variable to help us solve the problem more easily.
#c2	all the EM algorithms;we;a hidden variable;us;the problem
#s3	In our case, the hidden variable is a binary variable for each occurrence of word.
#c3	our case;the hidden variable;a binary variable;each occurrence;word
#s4	And this binary variable would indicate whether the world has been generated from theta on Sunday or theater
#c4	this binary variable;the world;theta;Sunday
#s5	Super B.
#c5	Super B.
#s6	And here we show some possible values of these variables.
#c6	we;some possible values;these variables
#s7	For example for the it's from Background, Z value is 1 and text on the other hand is from the topic.
#c7	example;it;Background;Z value;text;the other hand;the topic
#s8	Then it's 0 for Z etc.
#c8	it
#s9	Now, of course we don't observe those Z values.
#c9	course;we;those Z values
#s10	We just imagine there are such a social values of Z attached to all the words.
#c10	We;such a social values;Z;all the words
#s11	And that's why we call these hidden variables.
#c11	we;these hidden variables
#s12	Now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this.
#c12	the idea;we;the word distribution;the general;the world;it
#s13	The value of this hidden variable.
#c13	The value;this hidden variable
#s14	And So.
#s15	The algorithm, the EM algorithm then would work as follows.
#c15	The algorithm;the EM algorithm
#s16	First will initialize all the parameters with random values.
#c16	all the parameters;random values
#s17	In our case the parameters are mainly the probability of a word given by status update.
#c17	our case;the parameters;the probability;a word;status update
#s18	So this is the initialization stage.
#c18	the initialization stage
#s19	It is initialized values would allow us to use Bayes rule to take a guess of these Z values.
#c19	It;values;us;Bayes rule;a guess;these Z values
#s20	So will guess these values we can say for sure whether taxes from background or not, but we can have our guesses.
#c20	these values;we;taxes;background;we;our guesses
#s21	This is given by this formula.
#c21	this formula
#s22	It's called E-step.
#c22	It;E;-;step
#s23	And so the algorithm would then try to use the E Step 2 gas.
#c23	the algorithm;the E Step 2 gas
#s24	These Z values.
#c24	These Z values
#s25	After that it would then invoke another spec step called M-step.
#c25	it;another spec step;M-step
#s26	In this step we simply take advantage of the inferred values and then just group words that are in the same distribution like this from background, including this as well.
#c26	this step;we;advantage;the inferred values;then just group words;the same distribution;background
#s27	We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters.
#c27	We;the count;the probabilities;our estimate;the parameters
#s28	So let me also illustrate we can group the words that are believed to have come from Cedar sub D and as text mining algorithm for example and clustering.
#c28	me;we;the words;Cedar sub D;text mining algorithm;example;clustering
#s29	And we had group them together.
#c29	we;group;them
#s30	To help us re estimate the parameters.
#c30	us;the parameters
#s31	That were interested in so these will help us re estimate these parameters.
#c31	us;these parameters
#s32	But note that before we just set these parameter values randomly, But with this guess we will have a somewhat improved estimate of this.
#c32	we;these parameter values;this guess;we;a somewhat improved estimate
#s33	Of course, we don't know exactly whether it's zero or one, so we're not going to really do the split in hardware, but rather we can do those soft split and this is what happened here.
#c33	we;it;we;the split;hardware;we;those soft split;what
#s34	So we're going to adjust the count.
#c34	we;the count
#s35	By the probability that we believe this would has been generated by using the theta sub d.
#c35	the probability;we;the theta sub d.
#s36	And you can see this.
#c36	you
#s37	Where does this come from?
#s38	Well, this has come from here right from the E step.
#c38	the E step
#s39	So the EM algorithm with iteratively improve our initial estimate of parameters by using E-step first and then M step.
#c39	So the EM algorithm;our initial estimate;parameters;E;-;step;then M step
#s40	the E step is to augment the data with additional information like Z. And the M step is to take advantage of the additional information to separate the data to split the data accounts and then collect the right data counts.
#c40	the E step;the data;additional information;Z.;the M step;advantage;the additional information;the data;the data accounts;the right data counts
#s41	re estimate our parameters.
#c41	our parameters
#s42	And then once we have a new generation of parameters, we're going to repeat this.
#c42	we;a new generation;parameters;we
#s43	We're going to use the E-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters.
#c43	We;the E;-;step;our estimate;the hidden variables;another generation;the parameters
#s44	For the word distribution that we're interested in.
#c44	the word distribution;we
#s45	OK, so as I said, the bridge between the two is really variable Z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d.
#c45	I;the bridge;really variable Z hidden variable;this world;the topic word distributions;theta sub d.
#s46	So this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of EM algorithm.
#c46	this slide;a lot;content;you;the video;it;the essence;EM algorithm
#s47	Start with initial values that are often randomly set.
#c47	initial values
#s48	And then we invoke E step followed by M step to get an improved setting of parameters, and then we repeat this.
#c48	we;E step;M step;an improved setting;parameters;we
#s49	So this is a hill climbing algorithm that would gradually improve the estimate of parameters and as I will explain later, there's some guarantee for reaching a local maximum of the likelihood function.
#c49	a hill;algorithm;the estimate;parameters;I;some guarantee;a local maximum;the likelihood function
#s50	So let's take a look at the computation for specific case.
#c50	's;a look;the computation;specific case
#s51	So these formulas are the EM formulas that you see before, and you can also see there are superscripts here N to indicate the generation of parameters.
#c51	these formulas;the EM formulas;you;you;superscripts;the generation;parameters
#s52	I go here.
#c52	I
#s53	For example, we have N + 1.
#c53	example;we;N
#s54	That means we have improved parameters from here to.
#c54	we;parameters
#s55	here we have improvement.
#c55	we;improvement
#s56	So in this setting we have assumed that the two models have equal probabilities and the background model is known.
#c56	this setting;we;the two models;equal probabilities;the background model
#s57	So what are the relevant statistics?
#c57	what;the relevant statistics
#s58	Well, these are the word counts.
#c58	the word counts
#s59	So assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the.
#c59	we;just 4 words;their counts;our background model;high probabilities;common words
#s60	An in the first iteration you can picture what would happen.
#c60	the first iteration;you;what
#s61	Well, we first we initialize all the values.
#c61	we;we;all the values
#s62	So here this probability that we're interested in is normalized into an uniform distribution over all the words.
#c62	we;an uniform distribution;all the words
#s63	And then the E step would give us a guess.
#c63	the E step;us;a guess
#s64	Of the distribution that has been used to generate each word, we can see we have different probabilities for different words.
#c64	the distribution;each word;we;we;different probabilities;different words
#s65	Why that's be cause these words have different probabilities in the background.
#c65	these words;different probabilities;the background
#s66	So even though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities.
#c66	the two distributions;our initialization;uniform distribution;the difference;the background world distribution;we;different guest probabilities
#s67	So these words are believed to be more likely from the topic.
#c67	these words;the topic
#s68	These, on the other hand, are less likely probably from background.
#c68	the other hand;background
#s69	So once we have the Z values, we know in the E step these probabilities would be used to adjust the counts.
#c69	we;the Z values;we;the E step;these probabilities;the counts
#s70	So 4 must be multiplied by this point three three in order to get the allocated counts toward the topic.
#c70	this point;order;the allocated counts;the topic
#s71	And this is done by this multiplication.
#c71	this multiplication
#s72	Note that if our guess says this is 100%.
#c72	our guess;100%
#s73	If this is 1.0 Then we just get the full Council of this word for this topic.
#c73	we;the full Council;this word;this topic
#s74	But in general, as I said, it's not going to be 1.0, so we're going to just get some percentage of the counts toward this topic, and then we simply normalize these counts.
#c74	I;it;we;some percentage;the counts;this topic;we;these counts
#s75	To have a new generation of practice mate so you can see, compare this with the old one which is here.
#c75	a new generation;practice mate;you;the old one
#s76	So compare this with this one and will see at the probability is different.
#c76	this one;the probability
#s77	Not only that, we also see some words that are believed to have come from the topic.
#c77	we;some words;the topic
#s78	We have high probability like this one text.
#c78	We;high probability;this one text
#s79	And of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values.
#c79	course;this new generation;parameters;us;the infer;the latent variable;hidden variable values
#s80	So we have a new generation of values because of the E step based on the new generation of parameters.
#c80	we;a new generation;values;the E step;the new generation;parameters
#s81	And this these new in further values of these will give us then another generation of the estimate of probabilities of the words.
#c81	further values;us;another generation;the estimate;probabilities;the words
#s82	And so on so forth.
#s83	So this is what would actually happen when we compute these probabilities using the EM algorithm.
#c83	what;we;these probabilities;the EM algorithm
#s84	And as you can see in the last rule where we showed the log like code and the likelihood is increasing as we do the iteration.
#c84	you;the last rule;we;the log;code;the likelihood;we;the iteration
#s85	And note that these log likelihood is negative becausw the probability is between zero and one when you take logarithm, it becomes a negative value.
#c85	these log likelihood;the probability;you;logarithm;it;a negative value
#s86	What's also interesting is do not last column, and these are the inferred word split, and these are the probabilities that a word is believed to have come from one distribution.
#c86	What;do not last column;the inferred word split;the probabilities;a word;one distribution
#s87	In this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right?
#c87	this case;you;our main goal;these word distribution
#s88	So this is our primary goal.
#c88	our primary goal
#s89	We hope to have a more discriminating world distribution.
#c89	We;a more discriminating world distribution
#s90	But the last column is also by product and this actually can also be very useful and you can think about that.
#c90	the last column;product;you
#s91	And one use is to.
#c91	one use
#s92	For example, is made to what extent this document has covered background words.
#c92	example;what extent;this document;background words
#s93	And this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explained well by the background.
#c93	we;the average;what extent;it;background;content;words;the background
410	4cac96b9-b869-4523-99b0-e36c0cca95b4	57
#s1	In this lecture we give an overview of text mining and analytics.
#c1	this lecture;we;an overview;text mining;analytics
#s2	First, let's define the term text mining and the term text analytics.
#c2	's;the term text mining;the term text analytics
#s3	The title of this course is called Text Mining and Analytics, but the two terms text mining and text analytics are actually roughly the same.
#c3	The title;this course;Text Mining;Analytics;the two terms;text mining and text analytics
#s4	So we are not going to really distinguish them, and we're going to use them interchangeably.
#c4	we;them;we;them
#s5	But the reason why we have chosen to use both terms in the title is because there is also some subtle difference if you look at the two phrases literally.
#c5	the reason;we;both terms;the title;some subtle difference;you;the two phrases
#s6	Mining emphasizes more on the process, so it gives us an algorithmic view of the problem.
#c6	Mining;the process;it;us;an algorithmic view;the problem
#s7	Analytics on the other hand emphasizes more on the result or having a problem in mind.
#c7	Analytics;the other hand;the result;a problem;mind
#s8	We're going to look at the text data to help us solve a problem.
#c8	We;the text data;us;a problem
#s9	But again, as I said, we can treat these two terms roughly the same, and I think in the literature you probably will find the same.
#c9	I;we;these two terms;I;the literature;you
#s10	So we're not going to really distinguish them in the course.
#c10	we;them;the course
#s11	Both text mining and text analytics mean that we want to turn text data into high quality information or actionable knowledge.
#c11	Both text mining and text analytics;we;text data;high quality information;actionable knowledge
#s12	So in both cases we have the problem of dealing with a lot of text data and we hope to turn these text data into something more useful to us than the raw text data.
#c12	both cases;we;the problem;a lot;text data;we;these text data;something;us;the raw text data
#s13	And here we distinguish two different results one is high quality information, the other is actionable knowledge.
#c13	we;two different results;high quality information;actionable knowledge
#s14	Now, sometimes the boundary between the two is not so clear, but I also want to say a little bit about these two different angles of the result of text mining.
#c14	the boundary;I;a little bit;these two different angles;the result;text mining
#s15	In the case of high quality information we refer to more concise information about the topic, which might be much easier for humans to digest than the raw text data.
#c15	the case;high quality information;we;more concise information;the topic;humans;the raw text data
#s16	For example, you might face a lot of reviews of a product.
#c16	example;you;a lot;reviews;a product
#s17	The more concise form of the information would be very concise summary of the major opinions about the features of the product.
#c17	The more concise form;the information;very concise summary;the major opinions;the features;the product
#s18	Positive about, let's say, battery life of a laptop.
#c18	's;a laptop
#s19	Now, this kind of results are very useful to help people digest text data, and so this is to minimize the human effort in consuming text data in some sense.
#c19	this kind;results;people;text data;the human effort;text data;some sense
#s20	The other kind of output is actionable knowledge.
#c20	The other kind;output;actionable knowledge
#s21	Here we emphasize the utility of the information or knowledge we discover from text data.
#c21	we;the utility;the information;knowledge;we;text data
#s22	It's actionable knowledge for some decision problem, or some actions to take.
#c22	It;actionable knowledge;some decision problem;some actions
#s23	For example, we might be able to determine which product is more appealing to us all, a better choice for a shopping decision.
#c23	example;we;which product;us;a better choice;a shopping decision
#s24	Now, such an outcome could be called actionable knowledge because a consumer can take the knowledge and make a decision and act on it.
#c24	such an outcome;actionable knowledge;a consumer;the knowledge;a decision;it
#s25	So in this case, text mining supplies knowledge for optimal decision making.
#c25	this case;knowledge;optimal decision making
#s26	But again, the two are not so clearly distinguished, so we don't necessarily have to make a distinction.
#c26	we;a distinction
#s27	Text mining is also related to text retrieval, which is an essential component in any text mining systems.
#c27	Text mining;text retrieval;an essential component;any text mining systems
#s28	Now text retrieval refers to finding relevant information from a large amount of text data.
#c28	text retrieval;relevant information;a large amount;text data
#s29	So I've taught another separate MOOC on text retrieval and search engines, where we discussed various techniques for text retrieval.
#c29	I;another separate MOOC;text retrieval and search engines;we;various techniques;text retrieval
#s30	If you have taken that MOOC, you will find some overlap and it would be useful to know the background of text retrieval for understanding some of the topics in text mining.
#c30	you;that MOOC;you;some overlap;it;the background;text retrieval;the topics;text mining
#s31	But if you have not taken that MOOC it's also fine, because in this more context mining and analytics we're going to repeat some of the key concepts that are relevant for text mining.
#c31	you;it;this more context mining;analytics;we;the key concepts;text mining
#s32	But at the high level, let me also explain the relation between text retrieval and text mining.
#c32	the high level;me;the relation;text retrieval;text mining
#s33	Text retrieval is very useful for text mining in two ways: First, text retrieval can be a pre-processor for text mining, meaning that it can help us turn big text data into a relatively small amount of most relevant text data, which is often what's needed for solving a particular problem.
#c33	Text retrieval;text mining;two ways;text retrieval;-;processor;text mining;it;us;big text data;a relatively small amount;most relevant text data;what;a particular problem
#s34	And in this sense, text retrieval also helps minimize human effort.
#c34	this sense;text retrieval;human effort
#s35	Text retrieval is also needed for knowledge provenance and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge.
#c35	Text retrieval;knowledge provenance;the interpretation;text mining;text data;actionable knowledge
#s36	Once we find the patterns in text data or actionable knowledge, we generally would have to verify the knowledge by looking at the original text data so the users would have to have some text retrieval support to go back to the original text data to interpret the pattern, or to better understand the knowledge or to verify whether the pattern is really reliable.
#c36	we;the patterns;text data;actionable knowledge;we;the knowledge;the original text data;the users;some text retrieval support;the original text data;the pattern;the knowledge;the pattern
#s37	So this is a high level introduction to the concept of text mining and the relation between text mining and retrieval.
#c37	a high level introduction;the concept;text mining;the relation;text mining;retrieval
#s38	Next, let's talk about text data as a special kind of data.
#c38	's;text data;a special kind;data
#s39	Now it's interesting to view text data as data generated by humans as subjective sensors.
#c39	it;text data;data;humans;subjective sensors
#s40	So this slide shows an analogy between text data and non text data and between humans as subjective sensors and physical sensors such as network sensor or thermometer.
#c40	this slide;an analogy;text data;non text data;humans;subjective sensors;physical sensors;network sensor;thermometer
#s41	So in general, a sensor will monitor the real world in some way it will sense some signal from the real world and then would report the signal as data in various forms, for example, a thermometer would watch the temperature of real world and then will report the temperature in particular format.
#c41	a sensor;the real world;some way;it;some signal;the real world;the signal;data;various forms;example;a thermometer;the temperature;real world;the temperature;particular format
#s42	Similarly a geo sensor would sense the location and then report the location specification, for example in the form of longitude value and lattitude value.
#c42	a geo sensor;the location;the location specification;example;the form;longitude value;lattitude value
#s43	Network sensor would monitor network traffic or activities in the network and report some digital format of data.
#c43	Network sensor;network traffic;activities;the network;some digital format;data
#s44	Similarly, we can think of humans as subjective sensors that would observe the real world from some perspective, and then humans would express what they have observed in the form of text data.
#c44	we;humans;subjective sensors;the real world;some perspective;humans;what;they;the form;text data
#s45	So in this sense human is actually a subjective sensor that would also sense what's happening in the world and then express what's observed in the form of data, in this case text data.
#c45	this sense;a subjective sensor;what;the world;what;the form;data;this case
#s46	Now looking at the text data in this way has the advantage of being able to integrate all kinds of data together, and that's indeed needed in most data mining problems.
#c46	the text data;this way;the advantage;all kinds;data;most data mining problems
#s47	So here we are looking at the general problem of data mining, and in general we would be dealing with a lot of data about our world that are related to a problem.
#c47	we;the general problem;data mining;we;a lot;data;our world;a problem
#s48	And in general would be dealing with both non text data and text data and of course the non text data are usually produced by physical sensors.
#c48	both non text data;text data;course;the non text data;physical sensors
#s49	And those non text data can be also of different formats -  numerical data or categorical or relational data or multimedia data like a video or speech.
#c49	those non text data;different formats;numerical data;data;multimedia;data;a video;speech
#s50	So, these non text data are often very important in some problems.
#c50	these non text data;some problems
#s51	But text data is also very important, mostly because they contain a lot of semantic content and they often contain knowledge about the users, especially preferences and opinions of users.
#c51	text data;they;a lot;semantic content;they;knowledge;the users;especially preferences;opinions;users
#s52	So, but by treating text data as the data observed from human sensors, we can treat all these data together in the same framework.
#c52	text data;the data;human sensors;we;all these data;the same framework
#s53	So, data mining problem is basically to turn such data, turn all the data into actionable knowledge that we can take the advantage to change the real world, of course, for better.
#c53	data mining problem;such data;all the data;actionable knowledge;we;the advantage;the real world;course
#s54	So this means that data mining problem is basically taking a lot of data as input and giving actionable knowledge as output.
#c54	data mining problem;a lot;data;actionable knowledge;output
#s55	Inside the data mining module you can also see we have a number of different kinds of mining algorithms and this is because for different kinds of data we generally need different algorithms for mining the data.
#c55	the data mining module;you;we;a number;different kinds;mining algorithms;different kinds;data;we;different algorithms;the data
#s56	For example, video data might require computer vision to understand video content and that would facilitate the more effective mining and we also have a lot of general algorithms that are applicable to all kinds of data, and those algorithms of course are very useful, although for a particular kind of data we generally want to also develop special algorithms.
#c56	example;video data;computer vision;video content;the more effective mining;we;a lot;general algorithms;all kinds;data;those algorithms;course;a particular kind;data;we;special algorithms
#s57	So this course will cover specialized algorithms that are particularly useful for mining text data.
#c57	this course;specialized algorithms;mining text data
410	4da6283d-6903-4be9-8bfc-ad5d330343c6	155
#s1	This lecture is about the discriminative classifiers for text categorization.
#c1	This lecture;the discriminative classifiers;text categorization
#s2	In this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches.
#c2	this lecture;we;text categorization;discriminative approaches
#s3	This is a slide that you have seen from the discussion of Naive Bayes classifier, where we have shown that although naive Bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here.
#c3	a slide;you;the discussion;Naive Bayes classifier;we;naive Bayes classifier;the generation;text data;each categories;we;bayes rule;the scoring function;you;this slide;this scoring function;a weighted combination;a lot;word;the feature values;word count;the feature weights;the log;probability ratios;the word;two distributions
#s4	Now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector.
#c4	this kind;scoring function;a general scoring function;we;text data;a feature vector
#s5	Of course the features don't have to be all the words and their features can be other signals that we want to use.
#c5	the features;all the words;their features;other signals;we
#s6	And we mentioned that this is precisely similar to logistic regression.
#c6	we;logistic regression
#s7	So in this lecture we're going to introduce some discriminative classifiers.
#c7	this lecture;we;some discriminative classifiers
#s8	They try to model the conditional distribution of labels given the data directly rather than using Bayes rule to compute that indirectly.
#c8	They;the conditional distribution;labels;the data;Bayes rule
#s9	As we have seen in naive bayes.
#c9	we;naive bayes
#s10	So the general idea of logistical regression is to model the dependency of the binary response variable Y here,
#c10	the general idea;logistical regression;the dependency;the binary response variable Y
#s11	On some predictors.
#c11	some predictors
#s12	That are denoted as X.
#c12	X.
#s13	So here we have also changed the notation to X For feature values you may recall in the previous slide we have used Fi to represent the feature values.
#c13	we;the notation;X;feature values;you;the previous slide;we;Fi;the feature values
#s14	An here we use the notation of X vector, which is more common when we Introduce such machine learning algorithms, so X is our input, it's a vector.
#c14	we;the notation;X vector;we;algorithms;X;our input;it;a vector
#s15	And with M features.
#c15	M features
#s16	And each feature has a value X sub I here and our goal is model the dependency of this binary response variable on all these features.
#c16	each feature;a value;X sub;I;our goal;the dependency;this binary response variable;all these features
#s17	So in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the Y value to denote the two categories.
#c17	our categorization problem;we;two categories;theta;we;the Y value;the two categories
#s18	And when Y is 1 it means the category of the documents first class theta 1
#c18	Y;it;the category;the documents first class theta
#s19	Now the goal here is to model the conditional probability of Y given X directly as opposed to model the generation of X&Y as in the case of Naive Bayes.
#c19	the goal;the conditional probability;Y;X;the generation;X&Y;the case;Naive Bayes
#s20	And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector.
#c20	another advantage;this kind;approach;it;many other features;words;this vector
#s21	Since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization.
#c21	we;the generation;this vector;we;any signals;we;advantages;text categorization
#s22	So most specifically, in logistic regression the assumed functional form of y depending on X is the following, and this is very closed, closely related to the log or log odds that I introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide.
#c22	logistic regression;y;X;the following;the log;odds;I;the naive bayes;log;probability ratio;the two categories;you;the previous slide
#s23	So that this is what I meant, right?
#c23	what;I
#s24	So in the case of Naive Bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this.
#c24	the case;Naive Bayes;we;bayes rule;we;a formula
#s25	That looks like this.
#s26	But here we actually would assume explicitly that we would model our Probability of Y given X. As directly as a function of these features.
#c26	we;we;our Probability;Y;X.;a function;these features
#s27	So most specifically, we assume that log of the ratio of probability of y = 1 and the probability of y = 0.
#c27	we;that log;the ratio;probability;y;=;the probability;y;=
#s28	Is a function of X.
#c28	a function;X.
#s29	And so it's a function of X, and it's a linear combination of these feature values, controlled by beta values.
#c29	it;a function;X;it;a linear combination;these feature values;beta values
#s30	And since we know that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way.
#c30	we;that probability;y;=;1 minus probability;y;=;this way
#s31	So this is a log odds ratio.
#c31	a log odds ratio
#s32	Here.
#s33	And so in logistic regression, we basically assume that the probability of y = 1  given X is dependent on this linear combination of all these features.
#c33	logistic regression;we;the probability;y;X;this linear combination;all these features
#s34	So it's just one of the many possible ways of assuming that the dependency,
#c34	it;the many possible ways
#s35	But this particular form has been quite useful, and it has also has some nice properties.
#c35	this particular form;it;some nice properties
#s36	So if we rewrite this question to actually express the probability of Y given X in terms of X by taking by getting rid of the logarithm and we get this functional form and this is called a logistical function, it's a transformation of X into Y. As you see.
#c36	we;this question;the probability;Y;X;terms;X;the logarithm;we;this functional form;a logistical function;it;a transformation;X;Y.;you
#s37	on the right side here.
#c37	the right side
#s38	So that the Xs will be mapped into a range of values from zero to 1.0.
#c38	the Xs;a range;values
#s39	You can see, and that's precisely what we want.
#c39	You;precisely what;we
#s40	Since we have a probability here.
#c40	we;a probability
#s41	And the function form looks like this.
#c41	the function form
#s42	So this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization.
#c42	the basic idea;logistic regression;it;a very useful classifier;a lot;classification tasks;text categorization
#s43	So as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs.
#c43	all cases;model;we;the parameters;fact;all the machine learning programs
#s44	Once you set up the model set of objective function.
#c44	you;the model;objective function
#s45	To model the classifier, then the next step is to compute the parameter values.
#c45	the classifier;the next step;the parameter values
#s46	In general, we're going to adjust these parameter values, optimize the performance of classifier on the training data.
#c46	we;these parameter values;the performance;classifier;the training data
#s47	So in our case, let's assume we have training data.
#c47	our case;'s;we;training data
#s48	The training data here, X i and Y i and each pair is basically feature vector of X and a known label for that X Y, either one or zero.
#c48	The training data;X i;Y;i;each pair;feature vector;X;a known label;that X Y
#s49	So in our case we are interested in maximizing this conditional likelihood.
#c49	our case;we;this conditional likelihood
#s50	The condition likelihood here is basically to model y given the observed X.
#c50	The condition likelihood;y;the observed X.
#s51	So it's not like a.
#c51	it;a.
#s52	It's not like a modeling X, but rather we're going to model this.
#c52	It;a modeling X;we
#s53	Note that this is a conditional probability of Y given X.
#c53	a conditional probability;Y;X.
#s54	And this is also precisely what we want for classification.
#c54	precisely what;we;classification
#s55	Now, so the likelihood function would be just a product over all the training cases.
#c55	the likelihood function;just a product;all the training cases
#s56	And in each case, this is the modeled probability of observing this particular training case.
#c56	each case;the modeled probability;this particular training case
#s57	So given a particular XI, how likely we are going to observe the corresponding Y i of course, Y I could be one or zero and in fact the function form here would vary depending on whether Y sub I is one or zero.
#c57	a particular XI;we;the corresponding Y;i;course;I;fact;the function form;I
#s58	If it's one will be taking this form.
#c58	it;one;this form
#s59	And that's basically the logistical regression function.
#c59	the logistical regression function
#s60	But what about this if it's 0?
#c60	it
#s61	Well, if it's zero then we have to use a different form and that's this one.
#c61	it;we;a different form;this one
#s62	Now how do we get this one?
#c62	we;this one
#s63	That's just one minus the probability of y = 1, right?
#c63	just one minus the probability;y;=
#s64	And you can easily see this now.
#c64	you
#s65	The key point here is that the function form here depends on the observed.
#c65	The key point;the function form;the observed
#s66	Y I if it's one, it has a different form than when it's 0.
#c66	Y I;it;it;a different form;it
#s67	And if you think about when we want to maximize this probability we will basically going to want this probability to be as high as possible when the label is one.
#c67	you;we;this probability;we;this probability;the label
#s68	That means the document is in topic one.
#c68	the document;topic
#s69	But if the document is not we are going to maximize this value, and what's going to happen is actually to make this value as small as possible.
#c69	the document;we;this value;what;this value
#s70	Because they sum to one.
#c70	they
#s71	When I maximize this one.
#c71	I;this one
#s72	It's equivalent to minimize this one.
#c72	It;this one
#s73	So you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible.
#c73	you;we;the conditional likelihood;we;the prediction;the training data
#s74	So, as in other cases, when compute the maximum likelihood estimator Basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood.
#c74	other cases;the maximum likelihood estimator;a beta value;a set;beta values;this conditional likelihood
#s75	And this again then gives us a standard optimization problem.
#c75	us;a standard optimization problem
#s76	In this case, it can be also solved in many ways.
#c76	this case;it;many ways
#s77	Newtons method is a popular way to solve this problem.
#c77	Newtons method;a popular way;this problem
#s78	There are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined scoring function to help us classify a document right?
#c78	other methods;the end;we;the set;beta values;we;the beta values;we;a well defined scoring function;us;a document
#s79	So what's the function?
#c79	what;the function
#s80	Well, it's this one.
#c80	it;this one
#s81	If we have all the betavalues already known, all we need is to compute The Xi's for that document.
#c81	we;all the betavalues;we;The Xi;that document
#s82	And then plugging those values that will give us a estimate.
#c82	those values;us;a estimate
#s83	The probability that the document is in category one.
#c83	The probability;the document;category
#s84	OK, so much for logistical regression.
#c84	logistical regression
#s85	Let's also introduce another discriminative classifier called K nearest neighbors.
#c85	's;another discriminative classifier;K nearest neighbors
#s86	Now in general, I should say there are many such approaches.
#c86	I;many such approaches
#s87	And thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them.
#c87	thorough introduction;them;the scope;this course;you;a machine learning course;them
#s88	Here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization.
#c88	the basic introduction;the most commonly used classifiers;you;them;text categorization
#s89	So the second classifier, is called k nearest neighbors.
#c89	the second classifier;k nearest neighbors
#s90	In this approach, we're going to also estimate the conditional probability of label.
#c90	this approach;we;the conditional probability;label
#s91	Given data, but in a very different way.
#c91	data;a very different way
#s92	So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set
#c92	the idea;all the training examples;we;a text object;we;we;the K examples;the training
#s93	and that are most similar to this text object.
#c93	this text object
#s94	Basically this is to find the neighbors of this text object in the training data set.
#c94	the neighbors;this text object;the training data
#s95	So once we found we found the neighborhood and found the objects that are close to the.
#c95	we;we;the neighborhood;the objects
#s96	The object we're interested in classifying and say we have found the K nearest neighbors.
#c96	The object;we;we;the K nearest neighbors
#s97	That's why this method is called K nearest neighbors.
#c97	this method;K nearest neighbors
#s98	And then we're going to assign the category that's most common in these neighbors.
#c98	we;the category;these neighbors
#s99	Basically, we're going to allow these neighbors to vote for the category of the object that we're interested in classifying.
#c99	we;these neighbors;the category;the object;we
#s100	Now that means if most of them have a particular category, lets say category 1 then we're going to say this current object will have category one.
#c100	them;a particular category;category;we;this current object
#s101	This approach, can also be improved by considering the distance of a neighbor and the current object.
#c101	This approach;the distance;a neighbor;the current object
#s102	Basically, we can assume a close neighbor will have more saying about the category of this object, so we can have we can give such a neighbor more influence on the vote and we can take weighted sum of their votes based on the distances.
#c102	we;a close neighbor;more saying;the category;this object;we;we;such a neighbor;more influence;the vote;we;weighted sum;their votes;the distances
#s103	But the general idea is to look at the neighborhood and then try to assess the category based on the categories of the neighbors.
#c103	the general idea;the neighborhood;the category;the categories;the neighbors
#s104	Intuitively, this makes a lot of sense.
#c104	a lot;sense
#s105	But mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is P of Y given X.
#c105	a way;the conditional probability;label;data;P;Y;X.
#s106	Now I'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work.
#c106	I;this intuition;the moment;we;me;we;a similarity function;order;this work
#s107	I note that in naive base classifier we did not need a similarity function.
#c107	I;naive base classifier;we;a similarity function
#s108	An in logistical regression, we did not talk about the similarity function either.
#c108	logistical regression;we;the similarity function
#s109	But here we explicitly requires a similarity function.
#c109	we;a similarity function
#s110	Now this similarity function.
#c110	Now this similarity function
#s111	Actually is a good opportunity for us to inject any of our insights about features.
#c111	a good opportunity;us;our insights;features
#s112	Basically, effective features are those that would make the objects that are in the same category look more similar, but distinguishing objects in different categories.
#c112	effective features;the objects;the same category;distinguishing objects;different categories
#s113	So the design of this similarity function is closely tied to the design of the features in logistic regression.
#c113	the design;this similarity function;the design;the features;logistic regression
#s114	and other classifiers, so let's illustrate how K-NN works.
#c114	and other classifiers;'s;K-NN
#s115	Suppose we have a lot of training instances here.
#c115	we;a lot;training instances
#s116	And I've colored them differently and to show just different categories.
#c116	I;them;just different categories
#s117	Now suppose we have a new object in the center that we want to classify.
#c117	we;a new object;the center;we
#s118	So according to this approach we're going to  find the neighbors.
#c118	this approach;we;the neighbors
#s119	And let's first think of a special case of finding just one neighbor, the closest neighbor.
#c119	's;a special case;just one neighbor;the closest neighbor
#s120	Now in this case, if the, let's assume the closest neighbor is the box filled with diamonds and so then we're going to say.
#c120	this case;'s;the closest neighbor;the box;diamonds;we
#s121	Well, since this is in, this object is in category of diamonds.
#c121	this object;category;diamonds
#s122	Let's say then we're going to say, well, we're going to assign the same Category to our text object.
#c122	's;we;we;the same Category;our text object
#s123	But let's also look at the another possibility of finding a larger neighborhood.
#c123	's;the another possibility;a larger neighborhood
#s124	So let's think about the four neighbors.
#c124	's;the four neighbors
#s125	In this case, we're going to include a lot of other solid field boxes in red or pink.
#c125	this case;we;a lot;other solid field boxes
#s126	So in this case now we  are going to notice that among the four neighbors there are actually three neighbors in a different category.
#c126	this case;we;the four neighbors;three neighbors;a different category
#s127	So if we take a vote, then we'll conclude the object is actually of a different category.
#c127	we;a vote;we;the object;a different category
#s128	So this both illustrates how K nearest neighbor works and also illustrates some potential problems of this classifier.
#c128	K nearest neighbor;some potential problems;this classifier
#s129	Basically the results might depend on the K and indeed K is an important parameter to optimize.
#c129	the results;the K;K;an important parameter
#s130	Now you can intuitively imagine if we have a lot of neighbors around this object and then we'll be OK because we have a lot of neighbors for help us decide categories.
#c130	you;we;a lot;neighbors;this object;we;we;a lot;neighbors;help;us;categories
#s131	But if we have only a few, then the decision may not be reliable.
#c131	we;the decision
#s132	So on the one hand we want to find more neighbors right?
#c132	the one hand;we;more neighbors
#s133	And then we have more votes, but on the other hand as we try to find the more neighbors, we actually could risk on getting neighbors that are not really similar to this instance, they might be actually far away as you try to get more neighbors, so although you get more neighbors to vote, but those neighbors aren't necessary so helpful because they are not very similar to the object.
#c133	we;more votes;the other hand;we;the more neighbors;we;neighbors;this instance;they;you;more neighbors;you;more neighbors;those neighbors;they;the object
#s134	So the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation.
#c134	the parameter;you;such a parameter;cross validation
#s135	Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose.
#c135	you;your training data;two parts;you;one part;you
#s136	The The parameter K here or some other parameters in other classifiers, and then you're going to assume this number that works well on your training set would be actually the best for your future data.
#c136	The The parameter K;some other parameters;other classifiers;you;this number;your training set;your future data
#s137	So as I mentioned that KNN can be actually regarded as estimate of conditional probability of Y given X, and that's why we put this in the category of discriminative approaches.
#c137	I;KNN;estimate;conditional probability;Y;X;we;the category;discriminative approaches
#s138	So the key assumption that we made in this approach is that the distribution of the label given the document or probability of a category given document.
#c138	the key assumption;we;this approach;the distribution;the label;the document;probability;a category;document
#s139	For example, probability of theta I given document D is locally smoothed and that just means we're going to assume that this probability is the same for all the documents in this region.
#c139	example;probability;theta;I;document D;we;this probability;all the documents;this region
#s140	R  here.
#c140	R
#s141	And suppose we draw a neighborhood and we're going to assume in this neighborhood, since the data instances  are very similar, we're going to assume that the conditional distribution of the label, given the data, would be roughly the same.
#c141	we;a neighborhood;we;this neighborhood;the data instances;we;the conditional distribution;the label;the data
#s142	If D is not different, very different than we're going to assume that the probability of theta given D would be also similar, and so that's a very key assumption, and that that's.
#c142	D;we;the probability;theta;D;a very key assumption
#s143	That's actually important assumption that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function.
#c143	actually important assumption;us;a lot;machine learning;reality;course;we;similarity;the neighborhood;our similarity function
#s144	If our similarity function captures objects that do follow similar distributions, then this assumption is OK.
#c144	our similarity function;similar distributions;this assumption
#s145	But if our similarity function could not capture that.
#c145	our similarity function
#s146	Obviously the assumption would be a problem, and then the classifier would not be accurate.
#c146	the assumption;a problem;the classifier
#s147	Let's proceed with this assumption.
#c147	's;this assumption
#s148	Then what we are saying is that in order to estimate the probability of a category given a document, we can try to estimate the probability of the category given that entire region.
#c148	what;we;order;the probability;a category;a document;we;the probability;the category;that entire region
#s149	Now this has the benefit of course, of bringing additional data points to help us estimate this probability.
#c149	the benefit;course;additional data points;us;this probability
#s150	And so this is precise idea of KNN.
#c150	precise idea;KNN
#s151	Basically now we can use the known categories of all the documents in this region to estimate this probability.
#c151	we;the known categories;all the documents;this region;this probability
#s152	And I have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region.
#c152	I;a formula;you;we;the topics;this region;the total number;documents;the region
#s153	So the numerator that you see here c of Theta and R is a count of the documents in region R with category Theta I. Since these are training documents, we know they're categories.
#c153	the numerator;you;c;Theta;R;a count;the documents;region R;category Theta I.;documents;we;they;categories
#s154	We can simply count how many times we have seen sports here, how many times we have seen science etc.
#c154	We;we;sports;we
#s155	And then denominator is just a total number of documents training documents in this region, so this gives us a rough estimate of which category is most popular in this neighborhood, and we're going to assign the popular category to our data objective since it falls into this region.
#c155	denominator;just a total number;documents;documents;this region;us;a rough estimate;which category;this neighborhood;we;the popular category;our data objective;it;this region
410	4f58ef76-9ef1-4e19-8556-b84295a2afb3	121
#s1	This lecture is about the smoothing of language models.
#c1	This lecture;the smoothing;language models
#s2	In this lecture, we're going to continue talking about probabilistic retrieval model.
#c2	this lecture;we;probabilistic retrieval model
#s3	In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method.
#c3	we;the smoothing;language model;the query likelihood retrieval method
#s4	So you have seen this slide from the previous lecture.
#c4	you;this slide;the previous lecture
#s5	This is the ranking function based on the query likelihood.
#c5	the ranking function;the query likelihood
#s6	Here we assume that the independence of generating each query word.
#c6	we;each query word
#s7	  
#s8	And the formula would look like the following where we take a sum over all the query words and inside the sum.
#c8	the formula;we;a sum;all the query words;the sum
#s9	There is a log of probability of word  given by the document or document language model.
#c9	a log;probability;word;the document or document language model
#s10	So the main task now is to estimate this document language model.
#c10	the main task;this document language model
#s11	As we said before, different methods for estimating this model would lead to different retrieval functions.
#c11	we;this model;different retrieval functions
#s12	So in this lecture we're going to look into this in more detail.
#c12	this lecture;we;more detail
#s13	So how do we estimate this language model?
#c13	we;this language model
#s14	The obvious choice would be the maximum likelihood estimate that we have seen before, and that is we're going to normalize the word frequencies in the document.
#c14	The obvious choice;the maximum likelihood estimate;we;we;the word frequencies;the document
#s15	And the estimated probability would look like this.
#c15	the estimated probability
#s16	But this is a step function here.
#c16	a step function
#s17	Which means all the words that have the same frequency count will have identical probability.
#c17	all the words;the same frequency count;identical probability
#s18	Right, this is another frequent account that has a different probability.
#c18	another frequent account;a different probability
#s19	Note that for words that have not occurred in the document here they all have zero probability, so we this know this is just like the model that we assumed earlier in the lecture, where we assume that the user would sample word from the document.
#c19	words;the document;they;zero probability;we;the model;we;the lecture;we;the user;word;the document
#s20	To formulate a query.
#c20	a query
#s21	And there's no chance of sampling any word that's not in the document, and we know that's not good.
#c21	no chance;any word;the document;we
#s22	So how do we improve this, well?
#c22	we
#s23	In order to assign a non zero probability to words that have not been observed in the document.
#c23	order;a non zero probability;words;the document
#s24	We would have to take away some probability mass from the words that are observed in the document.
#c24	We;some probability mass;the words;the document
#s25	So for example here we have to take away some probability mass because we need some extra probability mass for the unseen words.
#c25	example;we;some probability mass;we;some extra probability mass;the unseen words
#s26	Otherwise they want to sum to one.
#c26	they
#s27	So all these probabilities must sum to one.
#c27	all these probabilities
#s28	So to make this transformation and to improve the maximum likelihood estimate by assigning non zero probabilities to words that are not observed in the data.
#c28	this transformation;the maximum likelihood estimate;non zero probabilities;words;the data
#s29	We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author had been written had been asked to write more words.
#c29	We;smoothing;smoothing;the estimate;the possibility;the author;more words
#s30	For the document, the author might have written other words.
#c30	the document;the author;other words
#s31	If you think about this factor, then a smoother language model would be more accurate representation of the actual topic.
#c31	you;this factor;a smoother language model;more accurate representation;the actual topic
#s32	Imagine you have seen an abstract of a research article.
#c32	you;an abstract;a research article
#s33	Let's say this document is abstract.
#c33	's;this document
#s34	Right, if we assume.
#c34	we
#s35	unseen words in this abstract.
#c35	unseen words;this abstract
#s36	We have all probability of zero.
#c36	We;all probability
#s37	That would mean there is no chance of sampling a word outside the abstract to formulate a query.
#c37	no chance;a word;the abstract;a query
#s38	But imagine a user who is interested in the topic of this subject.
#c38	a user;who;the topic;this subject
#s39	The user might actually choose a word that's not in the abstract to use as query.
#c39	The user;a word;the abstract;query
#s40	So obviously if we had asked this author to write more, the author would have written the full text of that article.
#c40	we;this author;the author;the full text;that article
#s41	So smoothing of the language model is attempt to try to recover the model for the whole article and then of course we don't have really knowledge about any words that are not observed in the abstract there.
#c41	the language model;attempt;the model;the whole article;course;we;really knowledge;any words;the abstract
#s42	So that's why smoothing is actually tricky problem.
#c42	smoothing;tricky problem
#s43	So let's talk a little more about how to smooth the language model.
#c43	's;the language model
#s44	And the key question here is what probability should be assigned to those unseen words.
#c44	the key question;what probability;those unseen words
#s45	And there are many different ways of doing that.
#c45	many different ways
#s46	One idea here that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by reference language model.
#c46	retrieval;the probability;unseen word;its probability;reference language model
#s47	That means if we don't observe the word in the data set, we're going to assume that it's probability is kind of governed by another reference language model that we will construct.
#c47	we;the word;the data;we;it;probability;another reference language model;we
#s48	It will tell us which unseen words will have likely higher probability.
#c48	It;us;which unseen words;likely higher probability
#s49	In the case of retrieval, a natural choice would be to take the collection language model as the reference language model.
#c49	the case;retrieval;a natural choice;the collection language model;the reference language model
#s50	That is to say if we don't observe a word in the document.
#c50	we;a word;the document
#s51	We're going to assume that the probability of this word would be proportional to the probability of the word in the whole collection.
#c51	We;the probability;this word;the probability;the word;the whole collection
#s52	So more formally, we will be estimating the probability of a word given  a document as follows.
#c52	we;the probability;a word;a document
#s53	If the word is seen in the document.
#c53	the word;the document
#s54	Then the probability would be a discounted.
#c54	the probability
#s55	maximum likelihood estimate P sub seen here.
#c55	maximum likelihood estimate;P sub
#s56	Otherwise.
#s57	If the word is not seen in the document, we're going to let its probability be proportional to the probability of the word in the collection.
#c57	the word;the document;we;its probability;the probability;the word;the collection
#s58	And here the coefficient Alpha.
#c58	And here the coefficient Alpha
#s59	Is to control the amount of probability mass that we assign to unseen words.
#c59	the amount;probability mass;we;unseen words
#s60	Obviously, all these probabilities must sum to one, so Alpha sub D is constrained in some way.
#c60	all these probabilities;Alpha sub D;some way
#s61	So what if we plug in this smoothing formula into our query likelihood running function?
#c61	we;this smoothing formula;our query likelihood running function
#s62	This is what we will get.
#c62	what;we
#s63	Right, in this formula you can see.
#c63	this formula;you
#s64	Right, we have.
#c64	we
#s65	This as a sum over all the query words and note that we have written in the form of a sum over all the vocabulary.
#c65	a sum;all the query words;we;the form;a sum;all the vocabulary
#s66	Can see here this is a sum over all the words in the vocabulary, but note that we have a count of the word in the query.
#c66	a sum;all the words;the vocabulary;we;a count;the word;the query
#s67	So in effect we are just taking sum of query words right?
#c67	effect;we;sum;query words
#s68	This is now.
#s69	A common way that we will use.
#c69	A common way;we
#s70	Because of its convenience.
#c70	its convenience
#s71	In some transformations.
#c71	some transformations
#s72	So this is as I said, this is some of all the query words.
#c72	I;all the query words
#s73	In our smoothing method, we assume that the words that are not observed in the document we have somewhat different form of probability namely it's for this form.
#c73	our smoothing method;we;the words;the document;we;somewhat different form;probability;it;this form
#s74	So we're going to then decompose this sum into two parts.
#c74	we;this sum;two parts
#s75	One sum is over all the query words that are matching the document.
#c75	One sum;all the query words;the document
#s76	That means in this sum, all the words have a non-zero probability in the document, sorry it's the non 0 count of the word in the document.
#c76	this sum;all the words;a non-zero probability;the document;it;the non 0 count;the word;the document
#s77	They all occurred in the document.
#c77	They;the document
#s78	And they also have to of course have a non 0 count in the query, so these are the words that are matched.
#c78	they;course;a non 0 count;the query;the words
#s79	These are the query words that are matching the document.
#c79	the query words;the document
#s80	But on the other hand, in this sum we are taking sum over all the words that are not.
#c80	the other hand;this sum;we;sum;all the words
#s81	All query words that are not matched in the document.
#c81	All query words;the document
#s82	So they occur in the query.
#c82	they;the query
#s83	Due to this this term, but they don't occur in the document.
#c83	this term;they;the document
#s84	In this case, these words have this probability because of our assumption about the smoothing.
#c84	this case;these words;this probability;our assumption;the smoothing
#s85	But that here.
#s86	These seen words have a different probability.
#c86	These seen words;a different probability
#s87	Now we can go further by rewriting the second sum.
#c87	we;the second sum
#s88	As a difference of two other sums, basically the first sum is actually sum, over all the query words.
#c88	a difference;two other sums;the first sum;all the query words
#s89	We know that the original sum is not over all the query words.
#c89	We;the original sum;all the query words
#s90	This is over all the  query words that are not matched in the document.
#c90	all the  query words;the document
#s91	So here we pretend that they are actually.
#c91	we;they
#s92	Over all the query words, so we take a sum over all the query words.
#c92	all the query words;we;a sum;all the query words
#s93	Obviously this sum has extra terms that are.
#c93	this sum;extra terms
#s94	This sum has extra terms that are not in this sum.
#c94	This sum;extra terms;this sum
#s95	Because here we are taking sum over all the query words there.
#c95	we;sum;all the query words
#s96	It's not matched in the document.
#c96	It;the document
#s97	So in order to make them equal, we will have to then subtract another sum here.
#c97	order;them;we;another sum
#s98	And this is the sum over all the query words that are matching the document.
#c98	the sum;all the query words;the document
#s99	And this makes sense, because here we are considering all query words and then we subtract the query words that are matched in the document.
#c99	sense;we;all query words;we;the query words;the document
#s100	That would give us the query words that not matched in the document.
#c100	us;the query words;the document
#s101	And this is almost reverse process of the first step here.
#c101	almost reverse process;the first step
#s102	And you might want to, why do we want to do that?
#c102	you;we
#s103	Well, that's cause.
#c103	cause
#s104	If we do this, then we have different forms of terms inside these sums.
#c104	we;we;different forms;terms;these sums
#s105	So now you can see in this sum we have.
#c105	you;this sum;we
#s106	All the words match the query words matched in the document and with this kind of terms.
#c106	All the words;the query words;the document;this kind;terms
#s107	Here we have another sum.
#c107	we;another sum
#s108	Over the same set of terms.
#c108	the same set;terms
#s109	match the query terms in document but inside the sum it's different.
#c109	the query terms;document;the sum;it
#s110	But these two sums can clearly be merged.
#c110	these two sums
#s111	So if we do that, we'll get another form of the formula that looks like the following.
#c111	we;we;another form;the formula;the following
#s112	At the bottom here.
#c112	the bottom
#s113	And note that this is a very interesting formula because here we combined these two.
#c113	a very interesting formula;we
#s114	That our sum.
#c114	That our sum
#s115	Over the query words matched in the document in the one sum here.
#c115	the query words;the document;the one sum
#s116	And the other sum now is decomposed into two parts.
#c116	the other sum;two parts
#s117	And these two parts look much simpler just because these are the probabilities of unseen worlds.
#c117	these two parts;the probabilities;unseen worlds
#s118	Now this formula is very interesting because you can see the sum is now over all the matched query terms.
#c118	this formula;you;the sum;all the matched query terms
#s119	And just like in the vector space model, we take a sum of terms that are in the intersection of query vector and the document vector.
#c119	the vector space model;we;a sum;terms;the intersection;query vector;the document vector
#s120	So it all already looks a little bit like the vector space model.
#c120	it;the vector space model
#s121	In fact, there's even more similarity here as we explain on this slide.
#c121	fact;even more similarity;we;this slide
410	500e19a0-d0de-4358-8f74-427cf05f3c3e	90
#s1	And here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of.
#c1	we;basic strategy;similarity;users;the rating
#s2	An object via active user.
#c2	An object;active user
#s3	Using the ratings of similar users to this active user, this is called a memory based approach.
#c3	the ratings;similar users;this active user;a memory based approach
#s4	Because it's a little similar to storing all the user information and when we are considering a particular user, we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that user information about those users to predict the preference of this user.
#c4	it;all the user information;we;a particular user;we;the relevant users;the similar users;this user case;that user information;those users;the preference;this user
#s5	So here's the general idea, and we use some notations here so X_ij denotes the rating of object o_j by user u_i.
#c5	the general idea;we;some notations;X_ij;the rating;object;o_j;user;u_i
#s6	And n_i is the average rating of all objects by this user.
#c6	n_i;the average rating;all objects;this user
#s7	So.
#s8	This n_i is needed because we would like to normalize the ratings of objects by this user.
#c8	This n_i;we;the ratings;objects;this user
#s9	So how do you do normalization?
#c9	you;normalization
#s10	Well, we're going to just subtract the average rating from all the ratings.
#c10	we;the average rating;all the ratings
#s11	Now this is it will normalize these ratings so that the ratings from different users would be comparable.
#c11	it;these ratings;the ratings;different users
#s12	Because some users might be more generous and they generally given high ratings.
#c12	some users;they;high ratings
#s13	But some others might be more critical, so their ratings cannot be directly compared with each other or aggregate them together.
#c13	some others;their ratings;them
#s14	So we need to do this normalization.
#c14	we;this normalization
#s15	Now the prediction of the rating on the item by another user or active user.
#c15	Now the prediction;the rating;the item;another user;active user
#s16	u_ a here.
#c16	_
#s17	Can be based on the average ratings of similar users.
#c17	the average ratings;similar users
#s18	So the user u_a is the user that we're interested in recommending items to and We now are interested in recommending this o_j, so we're interested in knowing how likely this user will like this objec.
#c18	the user;the user;we;items;We;this o_j;we;this user
#s19	How do we know that?
#c19	we
#s20	The idea here is to look at whether similar users to this user have liked this object.
#c20	The idea;similar users;this user;this object
#s21	So mathematically, this is to say the predicted the rating of this user on this object user A on object
#c21	the rating;this user;this object user;A;object
#s22	o_j is basically combination of the normalized ratings of different users.
#c22	o_j;combination;the normalized ratings;different users
#s23	And in fact here we're taking a sum over all the users.
#c23	fact;we;a sum;all the users
#s24	But not all users contribute equally to the average, and this is controlled by the weights.
#c24	not all users;the average;the weights
#s25	So this.
#s26	Weight Controls the influence of user on the prediction.
#c26	Weight Controls;the influence;user;the prediction
#s27	And of course, naturally, this way that should be related to the similarity between u_a and this particular user u_i.
#c27	course;the similarity;u_a;this particular user;u_i
#s28	The more similar they are, then the more contribution would like a user UI to make in predicting the preference of u_a
#c28	they;the more contribution;a user UI;the preference;u_a
#s29	So the formula is extremely simple.
#c29	the formula
#s30	You can see it's a sum over all the possible users.
#c30	You;it;a sum;all the possible users
#s31	And inside the sum, We have their ratings, well, their normalized ratings as I just explained, the ratings need to be normalized in order to be comparable with each other.
#c31	the sum;We;their ratings;their normalized ratings;I;the ratings;order
#s32	And then these ratings are weighted by their similarity.
#c32	these ratings;their similarity
#s33	So you can imagine W of a an I is just a similarity of user A and user
#c33	you;W;a an;I;just a similarity;user A;user
#s34	I.
#c34	I.
#s35	Now what's k here, well k is simply normalizer, it's just it's just one over the sum of all the weights.
#c35	what;k;k;normalizer;it;it;the sum;all the weights
#s36	over all the users.
#c36	all the users
#s37	And so this means basically, if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users.
#c37	you;the weight;K;we;coefficients;weights;all the users
#s38	And it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction.
#c38	it;just a normalization strategy;you;rating;the same range;the these ratings;we;the prediction
#s39	Right, so this is basically the main idea of memory based approaches for collaborative filtering.
#c39	the main idea;memory based approaches;collaborative filtering
#s40	Once we make this prediction.
#c40	we;this prediction
#s41	We also would like to map back to the rating that the user would actually make and this is to Further add the mean rating or average rating of this user u_a to the predicted value.
#c41	We;the rating;the user;the mean rating;average rating;this user;the predicted value
#s42	This would recover a meaningful rating for this user.
#c42	a meaningful rating;this user
#s43	So if this user is generous than the average would be somewhat high and when we add that the rating will be adjust to a relatively high rating.
#c43	this user;the average;we;the rating;a relatively high rating
#s44	Now when you recommend.
#c44	you
#s45	Item to a user.
#c45	Item;a user
#s46	This actually doesn't really matter 'cause you're interested in.
#c46	you
#s47	Basically the normalized rating that's more meaningful, but when they evaluate these collaborative filtering approaches.
#c47	Basically the normalized rating;they;these collaborative filtering approaches
#s48	is that typically assume the actual ratings of the user on these objects to be unknown, and then you do the prediction and then you compare the Predicted ratings with their actual ratings so they you do have access to their actual ratings, but then you pretend you don't know.
#c48	the actual ratings;the user;these objects;you;the prediction;you;the Predicted ratings;their actual ratings;they;you;access;their actual ratings;you;you
#s49	And then you compare your system's predictions with the actual ratings.
#c49	you;your system's predictions;the actual ratings
#s50	In that case, obviously the systems prediction would have to be adjusted to match the actual ratings of the user, and this is what's happening here, basically.
#c50	that case;the systems prediction;the actual ratings;the user;what
#s51	OK, so this is the memory based approach.
#c51	the memory based approach
#s52	Now of course if you look at the formula if you want to write the program to implement it, you still face the problem of determining what is this W function right?
#c52	course;you;the formula;you;the program;it;you;the problem;what;this W function
#s53	Once you know the W function, then the formula is very easy to implement.
#c53	you;the W function;the formula
#s54	So indeed there are many different ways to compute this function or this weight, w and specific approaches generally differ in how this is computed.
#c54	many different ways;this function;this weight;approaches
#s55	So here are some possibilities and you can imagine.
#c55	some possibilities;you
#s56	There are many other possibilities.
#c56	many other possibilities
#s57	One popular approach is to use the Pearson correlation coefficient.
#c57	One popular approach;the Pearson correlation coefficient
#s58	This would be a sum over commonly rated items and the formula is a standard Pearson correlation coefficient formula as shown here.
#c58	a sum;commonly rated items;the formula;a standard Pearson correlation coefficient formula
#s59	So this basically measures whether the two users tend to all give higher ratings to similar items, or lower ratings to similar items.
#c59	the two users;higher ratings;similar items;lower ratings;similar items
#s60	Another measure is the cosine measure, and this is to treat the rating vectors as vectors in the vector space.
#c60	Another measure;the cosine measure;the rating vectors;vectors;the vector space
#s61	And then we're going to measure the angle and then compute the cosine of the angles of the two vectors, and this measure has been used in the vector space model for retrieval as well.
#c61	we;the angle;the cosine;the angles;the two vectors;this measure;the vector space model;retrieval
#s62	So as you can imagine, there are many different ways of doing that.
#c62	you;many different ways
#s63	In all these cases, note that the user similarity is based on their preferences on Items and we did not actually use any content information of these items.
#c63	all these cases;the user similarity;their preferences;Items;we;any content information;these items
#s64	It didn't matter what these items are.
#c64	It;what;these items
#s65	They can be movies they can book so they can be products.
#c65	They;movies;they;they;products
#s66	They can be text documents.
#c66	They;text documents
#s67	We just didn't care about the content.
#c67	We;the content
#s68	And So this allows such approach to be applied to a wide range of problems.
#c68	such approach;a wide range;problems
#s69	Now, in some new approaches, of course we would like to use more information about the user.
#c69	some new approaches;course;we;more information;the user
#s70	Clearly we know more about the user, not just these preferences on these items.
#c70	we;the user;not just these preferences;these items
#s71	So, in the actual filtering system using collaborative filtering.
#c71	the actual filtering system;collaborative filtering
#s72	We could also combine that with content based filtering.
#c72	We;content based filtering
#s73	We could use more context information and those are all interesting approaches that people are still studying.
#c73	We;more context information;interesting approaches;people
#s74	There are new approaches proposed, but this memory based approach.
#c74	new approaches;this memory based approach
#s75	It has been shown to work reasonably well and it's easy to implement and in practical application this could be a starting point to see if the strategy works well for your application.
#c75	It;it;practical application;a starting point;the strategy;your application
#s76	So there are some obvious ways to also improve this approach.
#c76	some obvious ways;this approach
#s77	And mainly we would like to improve the user similarity measure and there are some practical issues to deal with here as well.
#c77	we;the user similarity measure;some practical issues
#s78	So for example, there will be a lot of missing values.
#c78	example;a lot;missing values
#s79	What do you do with them?
#c79	What;you;them
#s80	Or you can set them to default values, or the average ratings of the user and that would be a simple solution, but there are the advanced approaches that can actually try to predict those missing values.
#c80	you;them;values;the average ratings;the user;a simple solution;the advanced approaches;those missing values
#s81	And then use the predicted values to improve the similarity.
#c81	the predicted values;the similarity
#s82	So in fact the memory based approach you can predict those missing values right?
#c82	fact;you;those missing values
#s83	So you can imagine you have iterative approach where you first do some preliminary prediction and then you can use the predicted values to further improve the similarity function.
#c83	you;you;iterative approach;you;some preliminary prediction;you;the predicted values;the similarity function
#s84	So this is.
#s85	Here is a way to solve the problem and the strategy obviously would affect the performance of collaborative filtering.
#c85	a way;the problem;the strategy;the performance;collaborative filtering
#s86	Just like any other heuristics to improve these similarity functions, another idea which is actually very similar to the idea of IDF that we have seen in Text research is called inverse user frequency.
#c86	any other heuristics;these similarity functions;another idea;the idea;IDF;we;Text research;inverse user frequency
#s87	Or IUF Now here the idea is to look at the where the two users share similar ratings.
#c87	IUF;the idea;the two users;similar ratings
#s88	If the item is a popular item that has been viewed by many people, and seeing these two people.
#c88	the item;a popular item;many people;these two people
#s89	Interested in this item May not be so interesting, but if it's a rare item it has not been viewed by many users, but these two users viewed this item and they give similar ratings and that says more about their similarity, right?
#c89	this item;it;a rare item;it;many users;these two users;this item;they;similar ratings;their similarity
#s90	So it's kind of to emphasize more on similarity on items that are not viewed by many users.
#c90	it;similarity;items;many users
410	506d605d-bcb9-448f-84b8-d0d53a6936fe	100
#s1	  This lecture is about the natural language content analysis.
#c1	This lecture;the natural language content analysis
#s2	Natural language content analysis is the foundation of text mining.
#c2	Natural language content analysis;the foundation;text mining
#s3	So we are going to first talk about this.
#c3	we
#s4	And in particular, natural language processing.
#c4	particular, natural language processing
#s5	with a factor how we can represent text data?
#c5	a factor;we;text data
#s6	And this determines what algorithms can be used to analyze and mine text data.
#c6	what algorithms;text data
#s7	We're going to take a look at the basic concepts in natural language first.
#c7	We;a look;the basic concepts;natural language
#s8	I'm going to explain these concepts using a simple example that you are seeing here.
#c8	I;these concepts;a simple example;you
#s9	A dog is chasing a boy on the playground.
#c9	A dog;a boy;the playground
#s10	Now this is a very simple sentence.
#c10	a very simple sentence
#s11	When we read such a sentence, we don't have to think about it to get meaning of it.
#c11	we;such a sentence;we;it;meaning;it
#s12	But when a computer has to understand the sentence, the computer has to go through several steps.
#c12	a computer;the sentence;the computer;several steps
#s13	First, the computer needs to know what are the words, how to segment the words.
#c13	the computer;what;the words;the words
#s14	In English this is very easy as we can just look at the space and then the computer would need to know the categories of these words, syntactical categories.
#c14	English;we;the space;the computer;the categories;these words;syntactical categories
#s15	So for example Dog is a noun, chasing is the verb, boy is another noun, etc.
#c15	example;Dog;a noun;the verb;boy;another noun
#s16	And this is called a lexical analysis.
#c16	a lexical analysis
#s17	In particular tagging these words with these syntactic categories is called a part of speech tagging.
#c17	these words;these syntactic categories;a part;speech tagging
#s18	After that, the computer also needs to figure out the relation between these words.
#c18	the computer;the relation;these words
#s19	So A and the dog will form a noun phrase.
#c19	A;the dog;a noun phrase
#s20	On the playground would be a  prepositional phrase, etc.
#c20	the playground;a  prepositional phrase
#s21	And there are certain way for them to be connected together in order to generate the meaning.
#c21	certain way;them;order;the meaning
#s22	Some other combinations may not make sense.
#c22	Some other combinations;sense
#s23	And this .....
#s24	This is called syntactic parsing.
#c24	syntactic parsing
#s25	Or syntactical analysis or parsing of natural language sentence.
#c25	Or syntactical analysis;parsing;natural language sentence
#s26	The outcome is parse tree that you're seeing here.
#c26	The outcome;parse tree;you
#s27	That tells us a structure of the sentence so that we know how we can interpret the sentence.
#c27	us;a structure;the sentence;we;we;the sentence
#s28	But this is not semantics yet.
#c28	semantics
#s29	So in order to get the meeting would have to map these phrases and these structures into some real world entities that we have in our mind.
#c29	order;the meeting;these phrases;these structures;some real world entities;we;our mind
#s30	So dog is a concept that we know an A boy, the concept that we know.
#c30	dog;a concept;we;an A boy;the concept;we
#s31	So connecting these phrases with what we know is understanding.
#c31	these phrases;what;we
#s32	For computer, would have to formally represent these entities by using symbols.
#c32	computer;these entities;symbols
#s33	So dog d1 means d1 is a dog, boy b1 means b1 refers to a boy, etc.
#c33	dog;d1;d1;a dog;boy b1;b1;a boy
#s34	And we also represented the chasing action as a predicate.
#c34	we;the chasing action;a predicate
#s35	So chasing is predic here with three arguments, d1, b1 and p1,  which is a playground, right?
#c35	chasing;three arguments;d1;b1;p1;a playground
#s36	So this is a formal representation of the semantics of this sentence.
#c36	a formal representation;the semantics;this sentence
#s37	Once we reach that level of understanding, we might also make inferences.
#c37	we;that level;understanding;we;inferences
#s38	For example, if we assume there's a rule that says if someone is being chased than a person can get scared, then we can infer this boy might be scared.
#c38	example;we;a rule;someone;a person;we;this boy
#s39	This is the inferred meaning based on our additional knowledge.
#c39	the inferred meaning;our additional knowledge
#s40	And finally, we might even further to infer... might further infer what  This sentence is requesting or why the person who said the sentence is saying this sentence.
#c40	we;This sentence;who;the sentence;this sentence
#s41	And so this has to do with understanding the purpose of saying this sentence, and this is called SPEECH Act analysis or pragmatic analysis.
#c41	the purpose;this sentence;SPEECH Act analysis;pragmatic analysis
#s42	Which refers to the use of language.
#c42	the use;language
#s43	So in this case, person saying this might be reminding another person to bring back the dog.
#c43	this case;another person;the dog
#s44	So this means when saying a sentence, the person actually takes the action.
#c44	a sentence;the person;the action
#s45	So the action here is to make a request.
#c45	the action;a request
#s46	Now, this slide clearly shows that in order to really understand the sentence, there are a lot of things that the computer has to do now.
#c46	this slide;order;the sentence;a lot;things;the computer
#s47	In general, it's very hard for the computer to do everything, especially if we wanted to do everything correctly.
#c47	it;the computer;everything;we;everything
#s48	This is the very difficult.
#s49	Now the main reason why a natural language processing is very difficult because it's designed to make human communications efficient.
#c49	Now the main reason;a natural language processing;it;human communications
#s50	As a result, for example, we omit a lot of common sense knowledge.
#c50	a result;example;we;a lot;common sense knowledge
#s51	Because we assume all the..... all of us have this knowledge, there's no need to encode this knowledge.
#c51	we;us;this knowledge;no need;this knowledge
#s52	And that makes communication efficient.
#c52	communication
#s53	We also keep a lot of ambiguities,  like ambiguities of words.
#c53	We;a lot;ambiguities;ambiguities;words
#s54	And this is again because we assume that we have the ability to disambiguate a word, so there's no problem with having the same word to mean, possibly different things in different context.
#c54	we;we;the ability;a word;no problem;the same word;possibly different things;different context
#s55	Yet, for a computer this would be very difficult because the computer does not have the common sense knowledge that we do, so the computer would be confused indeed, and this makes it hard for natural language processing.
#c55	a computer;the computer;the common sense knowledge;we;the computer;it;natural language processing
#s56	Indeed, it makes it very hard for every step in the slide that I showed you earlier.
#c56	it;it;every step;the slide;I;you
#s57	Ambiguity is a main killer, meaning that in every step there are multiple choices and the computer would have to decide what's the right choice and that decision can be very difficult, as you will see also in a moment.
#c57	Ambiguity;a main killer;every step;multiple choices;the computer;what;the right choice;that decision;you;a moment
#s58	And in general we need common sense reasoning.
#c58	we;common sense reasoning
#s59	In order to fully understand the natural language and computers today don't yet have that, and that's why it's very hard for computers to precisely understanding natural language  at this point.
#c59	order;the natural language;computers;it;computers;natural language;this point
#s60	so here are some specific examples of challenges.
#c60	some specific examples;challenges
#s61	Think about the word level ambiguity a word like design can be a noun or a verb, so we've got ambiguous part of speech tag.
#c61	the word level ambiguity;a word;design;a noun;a verb;we;ambiguous part;speech tag
#s62	Root has also multiple meanings.
#c62	Root;multiple meanings
#s63	It can be of mathematical sense, like in square root.
#c63	It;mathematical sense;square root
#s64	Or it can be the root of a plant.
#c64	it;the root;a plant
#s65	Syntactic ambiguity refers to different interpretations of the sentence in terms of structures.
#c65	Syntactic ambiguity;different interpretations;the sentence;terms;structures
#s66	So for example, natural language processing can actually be interpreted in two ways.
#c66	example;natural language processing;two ways
#s67	So one is.
#c67	one
#s68	The ordinary meaning that we will be getting.
#c68	The ordinary meaning;we
#s69	As well talking about this topic so it's processing of natural language.
#c69	this topic;it;processing;natural language
#s70	But there is also another possible interpretation, which is to say language processing is natural.
#c70	another possible interpretation;language processing
#s71	Now we don't generally have this problem, but imagine for once a computer to determine the structure, the computer would actually have to make a choice between the two.
#c71	we;this problem;a computer;the structure;the computer;a choice
#s72	Another classic example is a man saw a boy with a telescope.
#c72	Another classic example;a man;a boy;a telescope
#s73	This ambiguity lies in  the question who had the telescope.
#c73	This ambiguity;the question;who;the telescope
#s74	This  is called a prepositional phrase attachment ambiguity meaning... where to attach this prepositional phrase with a telescope?
#c74	a prepositional phrase attachment ambiguity;this prepositional phrase;a telescope
#s75	Should it modify the boy or should it be modifying saw, the verb?
#c75	it;the boy;it;modifying saw;the verb
#s76	Another problem Anaphora resolution """John " persuaded Bill to buy a TV for "himself."
#c76	Another problem;Anaphora resolution;Bill;a TV;himself
#s77	"" Does himself referred to John or Bill?
#c77	himself;John;Bill
#s78	Pre supposition is another difficulty.
#c78	Pre supposition;another difficulty
#s79	He has quit Smoking implies that he smoked before and we need to have such knowledge in order to understand the languages.
#c79	He;Smoking;he;we;such knowledge;order;the languages
#s80	Because of these problems, the state of the art natural language processing techniques cannot do anything perfectly, even for the simplest part of speech tagging, we still cannot solve the whole problem.
#c80	these problems;the state;the art natural language processing techniques;anything;the simplest part;speech tagging;we;the whole problem
#s81	The accuracy that I listed here just about 97% was just taken from some studies earlier, and these studies obviously have to be using particular datasets, so the numbers here are not really meaningful if you take it out of the context of the data set that are used for evaluation, but I show these numbers may need to give you some sense about the accuracy or how well we can do things like this.
#c81	The accuracy;I;some studies;these studies;particular datasets;the numbers;you;it;the context;the data;evaluation;I;these numbers;you;some sense;the accuracy;we;things
#s82	It doesn't mean on any data set to the accuracy will be precisely  97%.
#c82	It;any data;the accuracy
#s83	But in general we can do part of speech tagging fairly well, although not perfectly.
#c83	we;part
#s84	Parsing would be more difficult, but for partial parsing, meaning to get that some phrases correct, we can probably achieve 90% or better accuracy.
#c84	Parsing;partial parsing;some phrases;we;90%;better accuracy
#s85	But to get the complete parse tree correctly is still very very difficult.
#c85	the complete parse tree
#s86	For semantic analysis, we can also do some aspects of semantic analysis, particularly extraction of entities and relations.
#c86	semantic analysis;we;some aspects;semantic analysis;particularly extraction;entities;relations
#s87	For example, recognizing this is the person, that's the  location, this person and that person met in some place etc.
#c87	example;the person;the  location;this person;that person;some place
#s88	it can also do a word sense disambiguation for some extent.
#c88	it;a word;sense disambiguation;some extent
#s89	We can figure out the occurrence of root in this sentence refers to the mathematical sense etc.
#c89	We;the occurrence;root;this sentence;the mathematical sense
#s90	Sentiment analysis is another aspect of semantic analysis that we can do.
#c90	Sentiment analysis;another aspect;semantic analysis;we
#s91	That means we can tag the sentences general positive when it's talking about product.
#c91	we;the sentences;it;product
#s92	Or talking about the person.
#c92	the person
#s93	Influence, however, is very hard and we generally cannot do that for any big domain, and it's only feasible for very limited domain.
#c93	Influence;we;any big domain;it;very limited domain
#s94	And that's a generally difficult problem in artificial intelligence.
#c94	a generally difficult problem;artificial intelligence
#s95	Speech Act analysis is also very difficult, and we can only do this properly for very specialized cases with a lot of help from human.
#c95	Speech Act analysis;we;very specialized cases;a lot;help;human
#s96	To annotate enough data for the computer to learn from.
#c96	enough data;the computer
#s97	So the slides also shows that computers are far from being able to understand natural language precisely, and that also explains why the text mining problem is difficult, because we cannot rely on mechanical approaces or computational methods to understand the language precisely.
#c97	the slides;computers;natural language;the text mining problem;we;mechanical approaces;computational methods;the language
#s98	Therefore, we have to use whatever we have today particular statistical machine learning methods, or.
#c98	we;we;particular statistical machine learning methods
#s99	Statistical analysis methods to try to get as much meaning out from the text as possible.
#c99	Statistical analysis methods;as much meaning;the text
#s100	And later you will see that there are actually many such algorithms that can indeed extract the interesting knowledge from text, even though we cannot really fully understand the meaning of all the natural language sentences precisely.
#c100	you;many such algorithms;the interesting knowledge;text;we;the meaning;all the natural language sentences
410	50879266-52ce-4800-8022-5f2c69d13eea	55
#s1	This lecture is about the collaborative filtering.
#c1	This lecture;the collaborative filtering
#s2	In this lecture, we're going to continue the discussion of recommender systems.
#c2	this lecture;we;the discussion;recommender systems
#s3	In particular, we're going to look at the approach of collaborative filtering.
#c3	we;the approach;collaborative filtering
#s4	You have seen this slide before when we talked about the two strategies to answer the basic question will user U like item X.
#c4	You;this slide;we;the two strategies;the basic question;will user U;item X.
#s5	In the previous lecture, we looked at the item similarity.
#c5	the previous lecture;we;the item similarity
#s6	That's content-based filtering.
#c6	content-based filtering
#s7	In this lecture, we will look at the user similarity.
#c7	this lecture;we;the user similarity
#s8	This is a different strategy called a collaborative filtering.
#c8	a different strategy;a collaborative filtering
#s9	So first, what is collaborative filtering?
#c9	what;collaborative filtering
#s10	It is to make filtering decisions for individual user based on the judgments of other users.
#c10	It;filtering decisions;individual user;the judgments;other users
#s11	And that is, we say, we will infer individual's interest or preferences from that of other similar users.
#c11	we;we;individual's interest;preferences;other similar users
#s12	So the general idea is the following.
#c12	the general idea;the following
#s13	Given a user U, we're going to 1st find the similar users u_1 through u_m.
#c13	a user;U;we;1st;the similar users;u_m
#s14	And then we're going to predict the user preferences based on the preferences of these similar users.
#c14	we;the user preferences;the preferences;these similar users
#s15	u_1 through u_m.
#c15	u_m
#s16	Now the user similarity here can be judged based on their similarity in preferences on a common set of items.
#c16	the user similarity;their similarity;preferences;a common set;items
#s17	Now here you can see the exact content of item doesn't really matter.
#c17	you;the exact content;item
#s18	We're going to look at the only the relation between the users and items.
#c18	We;the only the relation;the users;items
#s19	So this means this approach is very general.
#c19	this approach
#s20	It can be applied to any items.
#c20	It;any items
#s21	Not just the text options, so this approach it would work well under the following assumptions.
#c21	it;the following assumptions
#s22	1st.
#c22	1st
#s23	Users with the same interest where have similar preferences.
#c23	Users;the same interest;similar preferences
#s24	Second, the users with similar preferences probably share the same interest.
#c24	the users;similar preferences;the same interest
#s25	So for example.
#c25	example
#s26	If the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers.
#c26	the interest;the user;information retrieval;we;the user;SIGIR papers
#s27	Alright, so those who are interested in information retrieval research probably all favor SIGIR papers.
#c27	who;information retrieval research;SIGIR papers
#s28	That's the assumption that we make and if this assumption is true, Then it would help collaborative filtering to work well.
#c28	the assumption;we;this assumption;it;collaborative filtering
#s29	We can also assume that if we see people favor SIGIR papers, then we can infer their interest is probably information retrieval.
#c29	We;we;people;SIGIR papers;we;their interest;information retrieval
#s30	So in this simple examples it seems to make sense.
#c30	this simple examples;it;sense
#s31	And in many cases, such assumption actually does make sense.
#c31	many cases;such assumption;sense
#s32	So another assumption we have to make is that there are sufficiently large number of user preferences available to us.
#c32	another assumption;we;sufficiently large number;user preferences;us
#s33	So for example, if you see a lot of ratings of users for movies, and those indicate their preferences for movies, and if you have a lot of such data, then collaborative filtering can be very effective.
#c33	example;you;a lot;ratings;users;movies;their preferences;movies;you;a lot;such data;collaborative filtering
#s34	If not, there will be a problem, and that's often called cold start problem.
#c34	a problem;cold start problem
#s35	That means you don't have many preferences available, so the system could not afford to take advantage of collaborative filtering yet.
#c35	you;many preferences;the system;advantage;collaborative filtering
#s36	So let's look at the collaborative filtering problem in a more formal way, and so this picture shows that we are in general considering a lot of users showing we're showing M users here.
#c36	's;the collaborative filtering problem;a more formal way;this picture;we;a lot;users;we;M users
#s37	So u_1 through u_m
#c37	So u_1
#s38	and we also considering a number of objects, let's say N objects denoted as o_1 through o_n
#c38	we;a number;objects;'s;N objects;o_1;o_n
#s39	and then we will assume that the users will be able to judge those objects and the user could for example give ratings for those items, for example, those items could be movies.
#c39	we;the users;those objects;the user;example;ratings;those items;example;those items;movies
#s40	Could be products and then the users would give ratings 1 through 5 let's say.
#c40	products;the users;ratings;'s
#s41	So what you see here is that we have shown some ratings available for some combinations, so some users have watched some movies they have rated those movies.
#c41	what;you;we;some ratings;some combinations;some users;some movies;they;those movies
#s42	They obviously won't be able to watch all the movies and some users may actually only watch a few movies.
#c42	They;all the movies;some users;a few movies
#s43	So this is in general a sparse matrix.
#c43	a sparse matrix
#s44	So many items, many entries.
#c44	So many items;many entries
#s45	Have unknown values.
#c45	unknown values
#s46	And what's interesting here is we could potentially infer the value of an element in this matrix based on other values, and that's actually the central question in collaborative filtering, and that is we assume there's unknown function here F that would map a pair of a user and an object to the rating.
#c46	what;we;the value;an element;this matrix;other values;the central question;collaborative filtering;we;unknown function;F;a pair;a user;an object;the rating
#s47	And we have observed some values of this function.
#c47	we;some values;this function
#s48	And we want to infer the value of this function for other pairs, That with that don't have values available here.
#c48	we;the value;this function;other pairs;values
#s49	So this is very similar to other machine learning problems where we know the values of the function on some training data sets and we hope to predict the values of this function on some test data, right?
#c49	problems;we;the values;the function;some training data sets;we;the values;this function;some test data
#s50	So this is a function approximation.
#c50	a function approximation
#s51	And how can we figure out the function based on the observed ratings?
#c51	we;the function;the observed ratings
#s52	So this is the setup.
#c52	the setup
#s53	Now there are many approaches to solving this problem, and in fact this is a very active research area, a reason there are special conferences there are special conferences dedicated to the problem.
#c53	many approaches;this problem;fact;a very active research area;a reason;special conferences;special conferences;the problem
#s54	R is the major conference devotes to the problem.
#c54	R;the major conference devotes;the problem
#s55	the problem.
#c55	the problem
410	5190e288-54f7-4021-9083-8e8ceac11345	84
#s1	This lecture is about the latent Dirichlet allocation or LDA.
#c1	This lecture;the latent Dirichlet allocation;LDA
#s2	In this lecture, we're going to continue talking about topic models.
#c2	this lecture;we;topic models
#s3	In particular, we are going to talk about some extensions of PLSA, and one of them is LDA or latent Dirichlet allocation.
#c3	we;some extensions;PLSA;them;LDA;Dirichlet allocation
#s4	So the plan for this lecture is to cover two things.
#c4	the plan;this lecture;two things
#s5	One is to extend the PLSA with prior knowledge that would allow us to have in some sense a user controlled PLSA, so it doesn't blindly just listen to data but also would listen to our needs.
#c5	the PLSA;prior knowledge;us;some sense;a user controlled PLSA;it;data;our needs
#s6	The second is to extend the PLSA as a generative model fully generated model.
#c6	the PLSA;a generative model fully generated model
#s7	This has led to the development of Latent Dirichlet Allocation or LDA.
#c7	the development;Latent Dirichlet Allocation;LDA
#s8	So first let's talk about the PLSA with prior knowledge.
#c8	's;the PLSA;prior knowledge
#s9	In practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis.
#c9	practice;we;PLSA;text data;we;additional knowledge;we;the analysis
#s10	The standard PLSA is going to blindly listen to the data by using maximum likelihood estimator.
#c10	The standard PLSA;the data;maximum likelihood estimator
#s11	We are going to just fit data as much as we can and get some insight about data.
#c11	We;data;we;some insight;data
#s12	This is also very useful, but sometimes a user might have some expectations about which topics to analyze.
#c12	a user;some expectations;topics
#s13	For example, we might expect to see retrieval models as a topic in information retrieval.
#c13	example;we;retrieval models;a topic;information retrieval
#s14	We also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects.
#c14	We;certain aspects;battery;memory;the opinions;the laptop;the user;these aspects
#s15	Now, a user may also have knowledge about the topic coverage.
#c15	a user;knowledge;the topic coverage
#s16	And we may know which topic is definitely not covered in which document or is covered in the document.
#c16	we;which topic;which document;the document
#s17	For example, we might have seen those tags topic tags assigned to documents.
#c17	example;we;those tags;topic tags;documents
#s18	And those tag could be treated as topics if we do that, then a document that can only be generated using topics corresponding to the tags already assigned to the document.
#c18	those tag;topics;we;topics;the tags;the document
#s19	If the document is not assigned to a tag, we're going to say there's no way for using that topic to generate the document.
#c19	the document;a tag;we;no way;that topic;the document
#s20	The document must be generated by using the topics corresponding to the assigned tags.
#c20	The document;the topics;the assigned tags
#s21	So the question is, how can we incorporate such knowledge into PLSA?
#c21	the question;we;such knowledge;PLSA
#s22	It turns out that there's a. A very elegant way of doing that, and that's all incorporated such knowledge as priors on the models.
#c22	It;a. A very elegant way;such knowledge;priors;the models
#s23	And you may recall in Bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen.
#c23	you;Bayesian inference;we;data;parameters;what
#s24	So in this case we can use maximum a posteriori estimate, also called map estimate, and the formula is given here.
#c24	this case;we;a posteriori estimate;map estimate;the formula
#s25	Basically is to maximize the posterior distribution probability and this is a combination of the likelihood of data and the prior.
#c25	the posterior distribution probability;a combination;the likelihood;data;the prior
#s26	So what would happen is that we're going to have an estimate that listens to the data and also listens to our prior preferences.
#c26	what;we;an estimate;the data;our prior preferences
#s27	We can use this prior, which is denoted as P of Lambda to encode.
#c27	We;P;Lambda
#s28	All kinds of preferences and constraints.
#c28	All kinds;preferences;constraints
#s29	So for example, we can use this to encode the need of having precisely 1 background the topic.
#c29	example;we;the need;precisely 1 background;the topic
#s30	Now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model.
#c30	a prior;we;the prior;the parameters;the plan;one topic;the background language model
#s31	In other words, in other cases if it's not like that, we're going to say supplier says it's impossible.
#c31	other words;other cases;it;we;supplier;it
#s32	So the probability of that kind of model setting would be 0 according to our prior.
#c32	the probability;that kind;model setting;our prior
#s33	So now we can also, for example use the prior to force particular choice of topic to have a probability of a certain number.
#c33	we;example;the prior;particular choice;topic;a probability;a certain number
#s34	For example, we can force the document D to choose topic one with probability of 1/2.
#c34	example;we;the document;D;topic;probability
#s35	Or we can prevent a topic from being used in generated document.
#c35	we;a topic;generated document
#s36	So we can say the third topic should not be user generated.
#c36	we;the third topic;user
#s37	Document D will set to the Pi value to 0 for that topic.
#c37	Document D;the Pi value;that topic
#s38	We can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words.
#c38	We;the prior;favor set;parameters;topics;high probabilities;some particular words
#s39	In this case, we're not going to say it's impossible, but we're going to just strongly favor certain kind of distributions.
#c39	this case;we;it;we;certain kind;distributions
#s40	And you will see example later.
#c40	you;example
#s41	The map can be computed using a similar EM algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences.
#c41	The map;a similar EM algorithm;we;that maximum likelihood estimator;just some modification;smallest parameters;the prior preferences
#s42	And in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the prior will be similar to the data.
#c42	such a estimate;we;a special form;the prior called conjugate;the functional form;the prior;the data
#s43	As a result, we can combine the two and the consequences that you can basically convert the influence of the prior into the influence of having additional pseudo data because the two functional forms are the same and they can be combined.
#c43	a result;we;the consequences;you;the influence;the prior;the influence;additional pseudo data;the two functional forms;they
#s44	So the effect is as if we had more data.
#c44	the effect;we;more data
#s45	And this is convenient for computation.
#c45	computation
#s46	It doesn't mean conjugate prior is the best way to define the prior.
#c46	It;the best way;the prior
#s47	So now let's look at the specific example.
#c47	's;the specific example
#s48	Suppose the user is particularly interested in battery life of a laptop, and we're analyzing reviews.
#c48	the user;battery life;a laptop;we;reviews
#s49	So the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life.
#c49	the prior;the distribution;one distribution;high probabilities;battery
#s50	So we could do say there's a distribution that's entirely concentrated on battery life and we all priors is that one of your distributions should be very similar to this.
#c50	we;a distribution;battery life;we all priors;your distributions
#s51	Now if we use map estimator with the conjugated prior, which is Dirichlet prior Dirichlet distribution based on this preference, then the only difference in the EM algorithm is in the M step.
#c51	we;map estimator;Dirichlet prior Dirichlet distribution;this preference;the only difference;the EM algorithm;the M step
#s52	When we re estimate word distributions, we are going to add.
#c52	we;word distributions;we
#s53	Additional counts to reflect our prior right?
#c53	Additional counts
#s54	So here you can see the pseudocounts are defined the based on the probability of words in our prior.
#c54	you;the pseudocounts;the probability;words;our prior
#s55	So battery obviously will have a high pseudocounts similar life would have also high pseudocounts or the other words.
#c55	battery;a high pseudocounts;similar life;high pseudocounts;the other words
#s56	We have 0 pseudocounts because their probability is zero in the prior and when you see this is also controlled by a parameter mu
#c56	We;0 pseudocounts;their probability;the prior;you;a parameter mu
#s57	and We're going to add mu multiplied by the probability of W given our prior distribution to the connected counts.
#c57	We;mu;the probability;W;our prior distribution;the connected counts
#s58	When we re estimate the when we re estimate the this world distribution right?
#c58	we;we;the this world distribution
#s59	So this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been generated from this topic.
#c59	the only step;the changes;we;the counts;words;we;this topic
#s60	But now we force this distribution.
#c60	we;this distribution
#s61	To give more probabilities to these words by adding them to the pseudocounts so to artificially in effect, we artificially inflated their probabilities and to make this distribution we also need to add this many pseudocounts to the denominator.
#c61	more probabilities;these words;them;the pseudocounts;effect;we;their probabilities;this distribution;we;this many pseudocounts;the denominator
#s62	This is the total sum of all the pseudocounts we have added for all the words.
#c62	the total sum;all the pseudocounts;we;all the words
#s63	This would make this again a distribution.
#c63	a distribution
#s64	Now, this is a intuitively very reasonable way of modifying the EM algorithm and theoretically speaking, this deal works, and it computes the map estimator.
#c64	a intuitively very reasonable way;the EM algorithm;this deal;it;the map estimator
#s65	It's useful to think about two specific extreme cases of Mu.
#c65	It;two specific extreme cases;Mu
#s66	Now can you picture.
#c66	you
#s67	Think about what would happen if we set Mu to Zero.
#c67	what;we;Mu;Zero
#s68	Well, that's essentially to remove this prior, so mu in some sense indicates our strength on prior.
#c68	mu;some sense;our strength
#s69	Now what would happen if we set Mu to positive Infinity?
#c69	what;we;Mu;Infinity
#s70	Well, that's to say this price is so strong that we're not going to listen to the data at all.
#c70	this price;we;the data
#s71	So in the end you can see in this case we can do make one distribution fixed to the prior.
#c71	the end;you;this case;we;one distribution;the prior
#s72	You see why?
#c72	You
#s73	When mu is Infinity, we basically let this one dominate.
#c73	mu;Infinity;we
#s74	In fact, we are going to set this one.
#c74	fact;we;this one
#s75	to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution.
#c75	this distribution;this case;it;this distribution;we;the background language model;fact;a way;a prior;we;one distribution;what;we;the background distribution
#s76	So in this case we can even force the distribution to entirely focused on battery life.
#c76	this case;we;the distribution;battery life
#s77	But of course this won't work well 'cause it cannot attract other words, it would affect the accuracy of counting.
#c77	course;it;other words;it;the accuracy;counting
#s78	Topics about the battery life so in practice mu is set somewhere in between, of course.
#c78	Topics;the battery life;practice mu;course
#s79	So this is one way to impose our prior.
#c79	one way;our prior
#s80	We can also impose some other constraints.
#c80	We;some other constraints
#s81	For example, we can set any parameters for constraints, including zero as needed.
#c81	example;we;any parameters;constraints
#s82	For example, we may want to set one of the pis to 0.
#c82	example;we;the pis
#s83	And this would mean we don't allow that topic to participate in generating that document.
#c83	we;that topic;that document
#s84	And this is only reasonable, of course, when we have prior knowledge that strongly suggests this.
#c84	course;we;prior knowledge
410	51a1afdf-e864-49de-a80a-760bda4173a7	21
#s1	this lecture is about the basic a measures for evaluation of text retrieval systems in this latter we're going to discuss how we design basically measures to quantitatively compare to regional systems this is the slide that you have seen earlier in the lecture
#c1	this lecture;the basic a measures;evaluation;text retrieval systems;we;we;measures;regional systems;the slide;you;the lecture
#s2	well we talked about the cranfield evaluation methodology we can have that uh scratching that consists of queries documents and relevance judgments we can then run in two systems on these datasets to quantitatively evaluate their performance and we raised the question about which set of results is better in the system a better or system be better so let's now talk about how to actually quantify their performance suppose we have a total of ten relevant documents in the collection for this query now the relevance judgments shown on the right did not include the all the time obviously and we have only seen three relevant documents there but we can imagine there are other relevant documents in charger for this query so now intuitively we thought that system a is better becaus it did not have much noise and in particular we have seen among the three results two of them are relevant but in system be we have five results and only three of them are relevant
#c2	we;the cranfield evaluation methodology;we;queries documents;relevance judgments;we;two systems;these datasets;their performance;we;the question;which set;results;the system;'s;their performance;we;a total;ten relevant documents;the collection;this query;the relevance judgments;the right;the all the time;we;three relevant documents;we;other relevant documents;charger;this query;we;that system;better becaus;it;much noise;we;the three results;them;system;we;five results;them
#s3	so intuitively it looks like system is more accurate and this intuition can be captured by a measure called precision where we simply compute to what extend order retrieval results are relevant if you have one hundred percent precision that would mean all the retrieval documents are relevant so in this case the system a has a precision of two out of three system B has three over five and this shows that system a is better by cuisine
#c3	it;system;this intuition;a measure;precision;we;what;order retrieval results;you;one hundred percent precision;all the retrieval documents;this case;the system;a precision;two out of three system B;that system;cuisine
#s4	but we also talked about system be might be preferred by some other users hold like to retrieve as many relevant documents as possible so in that case we have to compare the number of relevant documents that retrieve and there is another measure total recall this measures the completeness of coverage of relevant documents in your retriever result so we just assume that there are ten relevant documents in the collection an here we've got the two of them in system a so the recoil is two out of ten
#c4	we;system;some other users;as many relevant documents;that case;we;the number;relevant documents;another measure;total recall;this measures;the completeness;coverage;relevant documents;your retriever result;we;ten relevant documents;the collection;we;them;system;the recoil
#s5	whereas system be has got a three
#c5	system
#s6	so it's a three out of ten now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines and they are very important because they are also widely used in many other task evaluation problems for example if you look at the applications of machine learning you tend to see precision recall numbers being reported for all kinds of tasks
#c6	it;we;recall system B;these two measures;the very basic measures;search engines;they;they;many other task evaluation problems;example;you;the applications;machine learning;you;precision recall numbers;all kinds;tasks
#s7	OK
#s8	so now let's define these two measures more precisely and these measures are to evaluate a set of retrieved documents so that means we are considering that approximation of the set of relevant documents we could distinguish it four cases depending on the situation of the document a document that can be retrieved or not retrieved right because we're talking about the set of results a document can be also relevant or non relevant depending on whether the user thinks this is useful document so we can now count of documents in each of the four categories we can have a to represent the number of documents that are retrieved and relevant the for documents that are not retriever but relevant etc now with this table then we have defined precision as the ratio of the random in the retrieval documents A to the total number of retrieve the documents so this is just a divided by the sum of A and C by the sum of this column similar records defined by dividing a by the sum of A and B
#c8	's;these two measures;these measures;a set;retrieved documents;we;that approximation;the set;relevant documents;we;it;four cases;the situation;the document;a document;we;the set;results;a document;the user;useful document;we;documents;the four categories;we;the number;documents;documents;this table;we;precision;the ratio;the random;the retrieval documents;the total number;retrieve;the documents;the sum;A;C;the sum;this column;similar records;the sum;B
#s9	so that's again the divider eight by the sum of the rule instead of the column
#c9	the divider;the sum;the rule;the column
#s10	right so we gotta see precision recall is all focused on looking at eight that's the number of retrieval relevant documents
#c10	we;precision recall;the number;retrieval relevant documents
#s11	but we're going to use different denominators
#c11	we;different denominators
#s12	OK
#s13	so what would be an ideal without well you can easy to see in the ideal case we have precision and recall all to be one point zero that needs we have got one percent of all the relevant documents in our results and all the results that will return all relevant at least there's no single not ready not in return he reality however high record hence to be associated with low precision and you can imagine why that's the case as you go down the list to try to get as many relevant documents as possible you tend to encounter a lot of non relevant documents so the precision will go down note that this set can also be defined by a cut off in the ranked list that's why although these two measures are defined for a set of retrieved documents they are actually very useful for evaluating a ranked list there are fundamental measures in terms of retrieval and many other tasks we often are interested in the position at ten documents for web search this means we look at the how many documents among the top ten results are actually relevant now this is a very meaningful measure becaus it tells us how many random documents are user can expect to see on the first page of search results where they typically shoot ten results so precision and recall the basically measures an we need to use them to further evaluate a search engine but there are the building blocks really we just to say that there tends to be a tradeoff between brazilian recalls so naturally it would be interesting to combine them and here's one measure that's often used called F measure anne
#c13	what;an ideal;you;the ideal case;we;precision;one point;we;one percent;all the relevant documents;our results;all the results;return;he;however high record;low precision;you;the case;you;the list;as many relevant documents;you;a lot;non relevant documents;the precision;this set;a cut;the ranked list;these two measures;a set;retrieved documents;they;a ranked list;fundamental measures;terms;retrieval;many other tasks;we;the position;ten documents;web search;we;the how many documents;the top ten results;a very meaningful measure becaus;it;us;how many random documents;user;the first page;search results;they;ten results;the basically measures;we;them;a search engine;the building blocks;we;a tradeoff;it;them;one measure;F measure anne
#s14	it's harmonic mean of precision and recall is defined on this slide so you can see it first compute the adverse of are in the P here
#c14	it;harmonic mean;precision;recall;this slide;you;it;the adverse;the P
#s15	and then it would interpret the two by using the coefficients depending on a prem that beta and after some transformation you can easy to see it would be of this form and in any case is interesting combination of precision and recall and beta is the parameter that's often set to one it can control the emphasis on precision or recall when we set bait out to one we end up having a special case of F measure often called F one this is a popular measure that's often used little combined precision and recall at the formula looks very simple
#c15	it;the coefficients;a prem;that beta;some transformation;you;it;this form;any case;interesting combination;precision;recall;beta;the parameter;it;the emphasis;precision;we;we;a special case;F measure;F one;a popular measure;little combined precision;the formula
#s16	it's just this here now it's easy to see that if you have a larger precision or larger recall then F magic would be high but what's interesting is that the trade off between precision and recall is the capturing the interesting way in F one so in order to understand that we can first look at the natural question why not just combining them using a simple arithmetic mean as official here now would be likely the most natural way of combining them so what do you think if you want to think more you can post a video so why is this not as good as F one or what's the problem with this now if you think about the arithmetic mean you can see this is the sum of multiple terms in this case this is some of precision and recall in the case of the sum that total value tends to be dominated by the large values that means if you have a very high P or very high are then you really don't care about whether the other value is low so the whole sum would be higher now this is not desirable becaus one can easily have a perfect the recall we can have perfect recall easily can you imagine how it's probably very easy to imagine that we simply retrieve all the document in the collection and then we have a perfect recall and this will give us point five as the average but such results are clearly not very useful for users even though the the average using this formula would be relatively high in contrast you can see if one would reward the case where precision and recall are roughly similar so it would paralyze a case where you have extremely high value for one of them so this means FY encodes a different the tradeoff between that this example shows actually a very important methodology here but he tried to solve a problem you might naturally think of one solution let's say in this case it's arithmetic mean
#c16	it;it;you;a larger precision;larger recall;F magic;what;the trade;precision;recall;the interesting way;F one;order;we;the natural question;them;a simple arithmetic;the most natural way;them;what;you;you;you;a video;F one;what;the problem;you;you;the sum;multiple terms;this case;precision;the case;the sum;total value;the large values;you;a very high P;you;the other value;the whole sum;desirable becaus;one;a perfect the recall;we;perfect recall;you;it;we;all the document;the collection;we;a perfect recall;us;such results;users;the the average;this formula;contrast;you;one;the case;precision;recall;it;a case;you;extremely high value;them;FY encodes;this example;a very important methodology;he;a problem;you;one solution;'s;this case;it
#s17	but it's important not to settle on this solution it's important to think whether you have other ways to combine them and once you think about the multiple variants it's important to analyze their difference
#c17	it;this solution;it;you;other ways;them;you;the multiple variants;it;their difference
#s18	and i think above which one makes more sense in this case if you think more carefully you will feel that F one probably makes more sense than the simple arithmetic mean although in other cases there may be different results but in this case the arithmetic mean seems not reasonable
#c18	i;one;more sense;this case;you;you;F one;more sense;the simple arithmetic;other cases;different results;this case
#s19	but if you don't pay attention to these subtle differences you might just take easy way to combine them and then go ahead with it and here later you will find that measure doesn't seem to work
#c19	you;attention;these subtle differences;you;easy way;them;it;you;that measure
#s20	well i saw at this methodology is after very important in general in solving problems and try to think about the best solution try to understand the problem very well
#c20	i;this methodology;problems;the best solution;the problem
#s21	and then no why you needed this measure and why you need to combine precision and recall and then use that to guide you in finding a good way to solve the problem to summarize we talked about precision which addresses the question other retrieval results all relevant we also talk about the recall which address to the question have all the relevant documents being retrieved these two are the two basic measures in text retrieval evaluation they are useful for many other tasks so as well we talked about the F mash as a way to combine precision and recall we also talked about the trade off between precision recall at this turns out to depend on the users search tasks and will discuss this point more in the later lecture
#c21	you;this measure;you;precision;you;a good way;the problem;we;precision;the question;other retrieval results;we;the recall;which address;the question;all the relevant documents;the two basic measures;text retrieval evaluation;they;many other tasks;we;the F mash;a way;precision;we;the trade;precision recall;the users search tasks;this point;the later lecture
410	51be74e8-eb10-47c2-a768-b688605de1e0	89
#s1	This lecture is about the syntagmatic relation discovery and conditional entropy.
#c1	This lecture;the syntagmatic relation discovery;conditional entropy
#s2	In this lecture, we're going to continue the discussion of word association mining an analysis.
#c2	this lecture;we;the discussion;word association;an analysis
#s3	We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations.
#c3	We;the conditional entropy;syntagmatic relations
#s4	Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word.
#c4	we;entropy;it;the presence;absence;a word
#s5	Now we address the different scenario where we assume that we know something about the text segment.
#c5	we;the different scenario;we;we;something;the text segment
#s6	So now the question is, suppose we know eats occured in the segment, how would that help us predict the presence or absence of a word like meat?
#c6	the question;we;the segment;us;the presence;absence;a word;meat
#s7	And in particular we want to know whether the presence of eats has helped us predict the presence of meat.
#c7	we;the presence;eats;us;the presence;meat
#s8	And if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat.
#c8	we;entropy;we;the presence;eats;uncertainty;the meat;the entropy;the random variable;the presence;absence;meat
#s9	We can also ask the question, what if we know of the absence of eats?
#c9	We;the question;we;the absence;eats
#s10	Would that also help us predict the presence or absence of meat.
#c10	us;the presence;absence;meat
#s11	So these questions can be addressed by using.
#c11	these questions
#s12	Another concept, called the conditional entropy.
#c12	Another concept;the conditional entropy
#s13	So to explain this concept, let's first look at the scenario we had before where we know nothing about the segment.
#c13	this concept;'s;the scenario;we;we;nothing;the segment
#s14	So we have these probabilities indicating whether a word like meat occurs or does not occur in the segment, and we have the entropy function that looks like what you see on the slide.
#c14	we;these probabilities;a word;meat;the segment;we;the entropy function;what;you;the slide
#s15	I suppose we know eats is present, so now know the value of another random variable that denotes eats.
#c15	I;we;the value;another random variable;denotes
#s16	Now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat.
#c16	all these probabilities;conditional probabilities;we;the presence;absence;meat
#s17	Given that we know eats occured in the context.
#c17	we;the context
#s18	So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy.
#c18	a result;we;these probabilities;their corresponding conditional probabilities;the entropy function;we;the conditional entropy
#s19	So this equation now here.
#c19	So this equation
#s20	Would be.
#s21	The conditional entropy conditioned on the presence of eats.
#c21	The conditional entropy;the presence;eats
#s22	Right?
#s23	So you can see this is essentially the same entropy function as you have seen before, except that we all the probabilities now have a condition.
#c23	you;the same entropy function;you;we;all the probabilities;a condition
#s24	And this then tells us the entropy of meat after we have known eats occurring in the segment.
#c24	us;the entropy;meat;we;eats;the segment
#s25	And of course, we can also define this conditional entropy for the scenario where we don't see eats.
#c25	course;we;this conditional entropy;the scenario;we;eats
#s26	So if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition.
#c26	we;eats;the segment;this conditional entropy;the uncertainty;meat;that content;that condition
#s27	So now putting different scenarios together, we have the complete definition of conditional entropy as follows.
#c27	different scenarios;we;the complete definition;conditional entropy
#s28	Basically.
#s29	We're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1.
#c29	We;both scenarios;the value;eats;us;the probability
#s30	Basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario.
#c30	course;the entropy;conditional entropy;meat;that particular scenario
#s31	So if you expand this entropy, then you have the following equation.
#c31	you;this entropy;you;the following equation
#s32	Where you see the involvement of those conditional probabilities.
#c32	you;the involvement;those conditional probabilities
#s33	Now in general, for any discrete random variables X&Y we have.
#c33	any discrete random variables;X&Y;we
#s34	The conditional entropy is no larger than the entropy of the variable X, so basically this is upper bound for the conditional entropy.
#c34	The conditional entropy;the entropy;the variable X;the conditional entropy
#s35	That means by knowing more information about the segment, we won't be able to increase the uncertainty.
#c35	more information;the segment;we;the uncertainty
#s36	We can only reduce uncertainty, and that intuitively makes sense because as we know more information, it should always help us.
#c36	We;uncertainty;sense;we;more information;it;us
#s37	Make the prediction and it cannot hurt the prediction in any case.
#c37	the prediction;it;the prediction;any case
#s38	Now what's interesting here is also to think about what's the minimum possible value of this conditional entropy.
#c38	what;what;the minimum possible value;this conditional entropy
#s39	Now we know that the maximum value is the entropy of X. But what about the minimum?
#c39	we;the maximum value;the entropy;X.;the minimum
#s40	So what do you think?
#c40	what;you
#s41	I hope you can reach the conclusion that the minimum possible value would be 0
#c41	I;you;the conclusion;the minimum possible value
#s42	and it will be interesting to think about and in what situation will achieve this.
#c42	it;what situation
#s43	So let's see how we can use conditional entropy to capture syntagmatic relations.
#c43	's;we;conditional entropy;syntagmatic relations
#s44	Now, of course this conditional entropy gives us directly one way to measure the association of two words.
#c44	course;this conditional entropy;us;one way;the association;two words
#s45	Because it tells us to what extent we can predict the one word given that we know the presence or absence of another word.
#c45	it;us;what extent;we;the one word;we;the presence;absence;another word
#s46	Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself.
#c46	we;the intuition;conditional entropy;syntagmatic relations;it;a very special case;the conditional entropy;the word;itself
#s47	So, here we listed the this conditional entropy in the middle.
#c47	we;the this conditional entropy;the middle
#s48	So it's here.
#c48	it
#s49	So what is the value of this?
#c49	what;the value
#s50	Now.
#s51	This means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence.
#c51	we;meat;the sentence;we;the meat;the sentence
#s52	Now of course this is zero because there's no uncertain there anymore Once we know whether the word occurs in the segment we will already know the answer for the prediction.
#c52	course;we;the word;the segment;we;the answer;the prediction
#s53	So this is 0.
#s54	And that's also when this conditional entropy reaches the minimum.
#c54	this conditional entropy;the minimum
#s55	So now let's look at some other cases.
#c55	's;some other cases
#s56	So this is a case of.
#c56	a case
#s57	Knowing the and trying to predict the meat and this is the case of knowing eats and trying to predict the meat.
#c57	the meat;the case;knowing eats;the meat
#s58	Which one do you think is smaller?
#c58	Which one;you
#s59	Note that a smaller entropy means easier for prediction.
#c59	a smaller entropy;prediction
#s60	Which one do you think is higher?
#c60	Which one;you
#s61	Which one is smaller?
#c61	Which one
#s62	If you look at the uncertainty, then in the first case the doesn't really tell us much about the meat, so knowing the occurrence of the doesn't really help us reduce the entropy that match, so it stays as fairly close to the original entropy of meat.
#c62	you;the uncertainty;the first case;us;the meat;the occurrence;us;the entropy;it;the original entropy;meat
#s63	Whereas in the case of eats, eats is related to meet, so knowing presence of eats or absence of eats would help us predict wether meat occurs so it can help us reduce entropy of meat, so we should expect the second term, namely, this one to have a smaller entropy.
#c63	the case;eats;so knowing presence;eats;absence;eats;us;wether meat;it;us;entropy;meat;we;the second term;namely, this one;a smaller entropy
#s64	And that means there is a stronger association between meat and eats.
#c64	a stronger association;meat;eats
#s65	So we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0?
#c65	we;this w;this meat;the entropy conditional entropy;its minimum
#s66	And for what kind of words would it reach its maximum?
#c66	what kind;words;it;its maximum
#s67	Well, that's when this W is not really related to meat.
#c67	this W;meat
#s68	like the, for example, it would be very close to the maximum, which is the entropy of meat itself.
#c68	example;it;the maximum;the entropy;meat;itself
#s69	So this suggests that we can use conditional entropy for mining syntagmatic relations.
#c69	we;conditional entropy;syntagmatic relations
#s70	The algorithm would look as follows.
#c70	The algorithm
#s71	For each word W1, we're going to enumerate the overall other words W2, and then we can compute the conditional entropy of W1 given W2.
#c71	each word;W1;we;the overall other words W2;we;the conditional entropy;W1;W2
#s72	And we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word W1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with W1.
#c72	we;all the candidate words;order;the conditional entropy;we;a word;a small entropy;it;us;the target word W1;we;the top;the candidate words;words;potential syntagmatic relations;W1
#s73	Note that we need to use a threshold to find these words.
#c73	we;a threshold;these words
#s74	The threshold can be the number of top candidates to take or absolute value for the conditional entropy.
#c74	The threshold;the number;top candidates;absolute value;the conditional entropy
#s75	Now this would allow us to mine the most strongly correlated words with a particular word W1 here.
#c75	us;the most strongly correlated words;a particular word W1
#s76	But this algorithm does not help us mine the strongest K syntagmatic relations from entire collection.
#c76	this algorithm;us;the strongest K syntagmatic relations;entire collection
#s77	Because in order to do that, we have to ensure that these conditional entropies are  comparable across different words.
#c77	order;we;these conditional entropies;different words
#s78	In this case of discovering Syntagmatic relations for a target word like W1, we only need to compare the conditional entropies For W1 given different words.
#c78	this case;Syntagmatic relations;a target word;W1;we;the conditional entropies;W1;different words
#s79	And in this case they all comparable right?
#c79	this case;they
#s80	So the conditional entropy of W1 given W2 and conditional entropy of W1 given  W3 are comparable.
#c80	the conditional entropy;W1;W2;conditional entropy;W1;  W3
#s81	They all measure how hard it is to predict W1.
#c81	They;it;W1
#s82	But if we think about the two pairs where we share W2 in the same condition and we try to predict the W1&W3, then the conditional entropies are actually not comperable.
#c82	we;the two pairs;we;W2;the same condition;we;the W1&W3;the conditional entropies
#s83	And you can think about this question, why?
#c83	you;this question
#s84	So Why are they not comparable?
#c84	they
#s85	Well, that was because they have a different upper bounds, right?
#c85	they;a different upper bounds
#s86	So those upper bounds are precisely the entropy of W1 and the entropy of W3.
#c86	those upper bounds;the entropy;W1;the entropy;W3
#s87	And they have different upper bounds, so we cannot really compare them in this way.
#c87	they;different upper bounds;we;them;this way
#s88	So how do we address this problem?
#c88	we;this problem
#s89	Later we'll discuss we can use mutual information to solve this problem.
#c89	we;we;mutual information;this problem
410	5350ccd0-beab-48fc-8484-d8e6a38c4cbf	64
#s1	Now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies.
#c1	's;the another behavior;mixture model;this case;'s;their response;the data frequencies
#s2	OK,
#s3	So what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1.
#c3	what;you;the likelihood function;the two word document;we;this case;the solution;text;a probability;the probability
#s4	Now it's interesting to think about a scenario where we start adding more words to the document.
#c4	it;a scenario;we;more words;the document
#s5	So what would happen if we add many the's to the document?
#c5	what;we;many the's;the document
#s6	Now this will change the game, right?
#c6	the game
#s7	So how?
#s8	Well, picture what would the likelihood function look like now?
#c8	Well, picture;what;the likelihood function
#s9	It started with the likelihood function for the two words.
#c9	It;the likelihood function;the two words
#s10	As we add more words, we know that,  we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the.
#c10	we;more words;we;we;the likelihood function;additional terms;the additional occurrences
#s11	Since in this case all the additional terms are the, we're going to just multiply by this term for the probability of the.
#c11	this case;all the additional terms;we;this term;the probability
#s12	An if we have another occurrence of the, we multiply again by the same term and so on, so forth until we add as many terms as the number of the's that we added to the document
#c12	we;another occurrence;we;the same term;we;as many terms;the number;the's;we;the document
#s13	D prime.
#c13	D prime
#s14	Now this obviously changes the likelihood function, so what's interesting is now to think about how would that change our solution.
#c14	the likelihood function;what;our solution
#s15	So what's the optimal solution now?
#c15	what;the optimal solution
#s16	Intuitively, you would know the original solution.
#c16	you;the original solution
#s17	0.9 and  0.1 will no longer be optimal for this new function, right?
#c17	this new function
#s18	But the question is how should we change it?
#c18	the question;we;it
#s19	Well in general they sum to one.
#c19	they
#s20	So in order to change it, we must take away some probability mess from one word.
#c20	order;it;we;some probability mess;one word
#s21	An added the probability mass to the other word.
#c21	An added the probability mass;the other word
#s22	The question is which word to have a reduced the probability and which word to have a larger probability?
#c22	The question;which word;a reduced the probability;which word;a larger probability
#s23	And in particular, let's think about the probability of the.
#c23	's;the probability
#s24	Should it be increased to be more than 0.1 or should we decrease it to less than  0.1?
#c24	it;we;it
#s25	What do you think?
#c25	What;you
#s26	Now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator.
#c26	you;the video;this question;understanding;important behavior;a mixture model
#s27	Now if you look at the formula for a moment then you will see.
#c27	you;the formula;a moment;you
#s28	It seems that now the objective function is more influenced by the than text before each contributed one turn.
#c28	It;the objective function;the than text;one turn
#s29	So now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the.
#c29	you;it;sense;a smaller probability;text;room;a larger probability
#s30	Why?
#s31	Because the is repeated many times if we increase it a little bit, it will have more positive impact, whereas a slight decrease of text.
#c31	we;it;it;more positive impact;text
#s32	We have relatively small impact because it occurs just once.
#c32	We;relatively small impact;it
#s33	Right, so this means there is another behavior that we observe here that is high frequency words generally will have high probabilities from all the distributions.
#c33	another behavior;we;high frequency words;high probabilities;all the distributions
#s34	And this is no surprise at all, because after all we are maximizing the likelihood of the data.
#c34	no surprise;we;the likelihood;the data
#s35	So all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function.
#c35	all the more word;it;it;more sense;such a word;a high probability;the impact;the likelihood function
#s36	This is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term.
#c36	fact;a very general phenomenon;all the maximum likelihood estimator;this case;we;we;more occurrences;term
#s37	It also encourages the unknown distribution theta sub
#c37	It;the unknown distribution theta
#s38	d to assign somewhat higher probability to this word.
#c38	somewhat higher probability;this word
#s39	Now it's also interesting to think about the impact of probability of theta sub
#c39	it;the impact;probability;theta sub
#s40	B. The probability of choosing one of the two component models.
#c40	B. The probability;the two component models
#s41	Now, we've being so far, assuming that each model is equally likely and that gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the probability of choosing a background model.
#c41	we;each model;us;you;your function;what;we;the probability;a background model
#s42	Now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficient in front of 0.9 which is now 0.5 would be even larger.
#c42	you;these terms;a different form;the probability;the background;a high probability;the word;the coefficient;front
#s43	When this is larger the overall result would be larger and that also makes them less important for theta sub D to increase the probability for the because it's already very large so the impact here of increasing the probability of the is somewhat regulated by this coefficient 0.5.
#c43	the overall result;them;theta sub D;the probability;it;the impact;the probability;this coefficient
#s44	If it's a larger on the background then it becomes less important to increase the value so.
#c44	it;the background;it;the value
#s45	So.
#s46	This means the behavior here, which is high frequency words tend to get higher probabilities are affected or regularised somewhat by the probability of choosing each component.
#c46	the behavior;high frequency words;higher probabilities;the probability;each component
#s47	The more likely a component that is being chosen, it's more important than to have higher values for these frequent words.
#c47	it;higher values;these frequent words
#s48	If you have a very small probability of being chosen, than the incentive is less.
#c48	you;a very small probability;the incentive
#s49	So to summarize, we have just discussed the mixture model
#c49	we;the mixture model
#s50	and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions.
#c50	we;the estimation problem;mixture model;we;some general behavior;the estimate;we;the our estimator;these intuitions
#s51	1st
#c51	1st
#s52	Every component component model attempts to assign high probabilities to high frequency words in the data.
#c52	Every component component model attempts;high probabilities;high frequency words;the data
#s53	And this is to collaboratively maximize likelihood.
#c53	likelihood
#s54	Second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize the likelihood.
#c54	different component models;high probabilities;different words;competition;waste;probability;them;the likelihood
#s55	3rd, the probability of choosing each component regulates the collaboration and competition between the component models.
#c55	the probability;each component;the collaboration;competition;the component models
#s56	It would allow some component models to respond more to the change, for example of frequency of data point in the data.
#c56	It;some component models;the change;example;frequency;data point;the data
#s57	We also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents.
#c57	We;the special case;one component;a background word distribution;this distribution;a collection;documents
#s58	A large collection of English documents, by using just one distribution
#c58	A large collection;English documents;just one distribution
#s59	and then we'll just have normalized frequencies of terms to give us the probabilities of all these words.
#c59	we;frequencies;terms;us;the probabilities;all these words
#s60	Now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component.
#c60	we;such a specialized mixture model;we;we;background words;the other component
#s61	And that would make the discovered  topic more discriminative.
#c61	the discovered  topic
#s62	This is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in Bayesian estimation and this prior would allow us to favor a model that's consistent with our prior.
#c62	an example;a prior;the model parameters;one model;the background language model;you;what;we;Bayesian estimation;us;a model;our prior
#s63	In fact, if it's not consistent, we're going to say the model is impossible, so it has a zero prior probability, and that effectively excludes such a scenario.
#c63	fact;it;we;the model;it;a zero prior probability;such a scenario
#s64	This is also an issue that we will talk more later.
#c64	an issue;we
410	54ab232c-85cb-4829-abd0-6cbaed5f3fc8	85
#s1	This lecture is about an overview of statistical language models which cover probabilistic topic models as special cases.
#c1	This lecture;about an overview;statistical language models;probabilistic topic models;special cases
#s2	In this lecture we're going to give an overview of statistical language models.
#c2	this lecture;we;an overview;statistical language models
#s3	These models are general models that cover probabilistic topic models as special cases.
#c3	These models;general models;probabilistic topic models;special cases
#s4	So first, what is the statistical language model?
#c4	what;the statistical language model
#s5	A statistical language model is basically the probability distribution over word sequences.
#c5	A statistical language model;the probability distribution;word sequences
#s6	So, for example, we might have a distribution that gives """Today is Wednesday"" a probability of 0.001" It might give """Today Wednesday is"" which is a non" grammatical sentence very, very small probability as shown here. "
#c6	example;we;a distribution;Today;Wednesday;" a probability;It;a non" grammatical sentence;very, very small probability
#s7	And similarly another sentence, ""The" "eigenvalue is positive"", might get a" probability of 0.00001
#c7	And similarly another sentence;The" "eigenvalue;a" probability
#s8	So as you can see, such a distribution clearly is context dependent.
#c8	you;such a distribution
#s9	It depends on the context of discussion.
#c9	It;the context;discussion
#s10	Some word sequences might have higher probabilities than others.
#c10	Some word sequences;higher probabilities;others
#s11	But the same sequence of words might have a different probability in a different context.
#c11	the same sequence;words;a different probability;a different context
#s12	And so this suggests that such a distribution can actually characterize topic.
#c12	such a distribution;topic
#s13	Such a model can also be regarded as a probabilistic mechanism for generating text.
#c13	Such a model;a probabilistic mechanism;text
#s14	And that just means we can view text data as data observed from such a model.
#c14	we;text data;data;such a model
#s15	For this reason, we also call such a model generative model.
#c15	this reason;we;such a model generative model
#s16	So now Given a model, we can then sample sequences of words.
#c16	a model;we;sequences;words
#s17	So for example, based on the distribution that I have shown here on this slide, we might, let's say, sample "a sequence like ""today is Wednesday""" because it has a relatively high probability, we might often get such a sequence.
#c17	example;the distribution;I;this slide;we;'s;a sequence;today;Wednesday;it;a relatively high probability;we;such a sequence
#s18	"We might also get ""the eigenvalue is" "positive"", sometimes with a smaller" probability.
#c18	We;the eigenvalue;a smaller" probability
#s19	Very, very occasionally, "we might get ""today" "Wednesday is"" because the probability is" so small.
#c19	we;Wednesday;the probability
#s20	So in general, in order to characterize such a distribution, we must specify probability values for all these different sequences of words.
#c20	order;such a distribution;we;probability values;all these different sequences;words
#s21	Obviously it's impossible to specify that, because it's impossible to enumerate all the possible sequences of words.
#c21	it;it;all the possible sequences;words
#s22	So in practice we will have to simplify the model in some way.
#c22	practice;we;the model;some way
#s23	So the simplest language model is called a unigram language model.
#c23	the simplest language model;a unigram language model
#s24	In such a case, we simply assume that text is generated by generating each word independently.
#c24	such a case;we;text;each word
#s25	Now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model.
#c25	the words;we;this assumption;we;the language model
#s26	Basically now the probability of a sequence of words w_1 through w_n would be just a product of each.
#c26	the probability;a sequence;words;w_n;just a product
#s27	The probability of each word.
#c27	The probability;each word
#s28	So for such a model we have as many parameters as the number of words in our vocabulary.
#c28	such a model;we;as many parameters;the number;words;our vocabulary
#s29	So here we assume we have N words, so we have N probabilities, one for each word, and they sum to one.
#c29	we;we;N words;we;probabilities;each word;they
#s30	So now we can assume our text is a sample drawn according to this word distribution.
#c30	we;our text;a sample;this word distribution
#s31	That just means we're gonna draw a word each time
#c31	we;a word
#s32	and then eventually we'll get a text.
#c32	we;a text
#s33	So for example now again.
#c33	example
#s34	We can try to sample words according to a distribution.
#c34	We;words;a distribution
#s35	We might get Wednesday often or today often and some other words like eigenvalue might have a small probability, etc.
#c35	We;Wednesday;some other words;eigenvalue;a small probability
#s36	Now, with this we actually can also compute the probability of every sequence, even though our model only specifies the probabilities of words.
#c36	we;the probability;every sequence;our model;the probabilities;words
#s37	This is because of the independence assumption.
#c37	the independence assumption
#s38	So specifically we can compute the "probability of ""today is Wednesday""."
#c38	we;the "probability;today;Wednesday
#s39	Because it's just a product of the probability of today, probability of is and probably Wednesday.
#c39	it;just a product;the probability;today;probability
#s40	For example, I showed some fake numbers here and we might then multiply these numbers together to get the probability "of ""today is Wednesday""."
#c40	example;I;some fake numbers;we;these numbers;the probability;today;Wednesday
#s41	So as you can see, with N probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model.
#c41	you;N probabilities;each word;we;the probability distribution;all kinds;sequences;words;a very simple model
#s42	Ignore the word order, so it may not be effective for some problems such as speech recognition, where you may care about the order of words.
#c42	the word order;it;some problems;speech recognition;you;the order;words
#s43	But it turns out to be quite sufficient for many tasks that involve topic analysis, and that's also what we're interested in here.
#c43	it;many tasks;topic analysis;what;we
#s44	So when we have a model, we generally have two problems that we can think about.
#c44	we;a model;we;two problems;we
#s45	One is given a model.
#c45	a model
#s46	How likely we'll observe certain kind of data points.
#c46	we;certain kind;data points
#s47	That is, we're interested in the sampling process.
#c47	we;the sampling process
#s48	The other is the estimation process and that is to figure out the parameters of the model given some observed data, and we're going to talk about that in a moment.
#c48	the estimation process;the parameters;the model;some observed data;we;a moment
#s49	Let's first talk about the sampling.
#c49	's;the sampling
#s50	So here I show two examples of word distributions or unigram language models.
#c50	I;two examples;word distributions;unigram language models
#s51	The first one has higher probabilities for words,  text, mining, association, etc.
#c51	The first one;higher probabilities;words;  text;mining;association
#s52	Now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context.
#c52	a topic;text mining;we;words;such a distribution;we;words;text mining context
#s53	So in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course.
#c53	this case;we;the question;what;the probability;a particular document;we;text;a text mining paper;course
#s54	...
#s55	The text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero.
#c55	The text;we;words;this distribution;the probability;a text mining paper publishing;the top conference
#s56	Assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents.
#c56	no word;a zero probability;the distribution;we;all kinds;text documents;very meaningful text documents
#s57	The second distribution show on the bottom has different words that with higher probability.
#c57	The second distribution show;the bottom;different words;higher probability
#s58	Food, nutrition and healthy, diet etc.
#c58	Food;nutrition
#s59	So this clearly indicates a different topic and in this case it's probably about health.
#c59	a different topic;this case;it;health
#s60	So if we sample words from such distribution, then the probability of observing a text mining paper would be very very small.
#c60	we;words;such distribution;the probability;a text mining paper
#s61	On the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher.
#c61	the other hand;the probability;a text;a food nutrition paper
#s62	So that just means given a particular distribution, different text will have different probabilities.
#c62	that just means;a particular distribution;different text;different probabilities
#s63	Now let's look at the estimation problem.
#c63	's;the estimation problem
#s64	Now, in this case, we're going to assume that we have observed data.
#c64	this case;we;we;data
#s65	We know exactly what the text data looks like.
#c65	We;exactly what;the text data
#s66	In this case, let's assume we have a text mining paper.
#c66	this case;'s;we;a text mining paper
#s67	In fact, it's abstract of the paper, so the total number of words is 100, and I've shown some counts of individual words here.
#c67	fact;it;the paper;the total number;words;I;some counts;individual words
#s68	If we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model?
#c68	we;the question;what;the most likely language model;this text data;the text;some language model;what;our best guess;this language model
#s69	OK, so the problem now is just the estimated probabilities of these words as I've shown here.
#c69	the problem;just the estimated probabilities;these words;I
#s70	So what do you think?
#c70	what;you
#s71	What would be your guess?
#c71	What;your guess
#s72	Would you guess text that has a very very small probability or relatively large probability?
#c72	you;text;a very very small probability;relatively large probability
#s73	What about the query?
#c73	the query
#s74	Your guess probably will be dependent on how many times we have observed this word in the text data, right?
#c74	Your guess;we;this word;the text data
#s75	And if you think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100.
#c75	you;it;a moment;you;many others;you;text;a probability
#s76	Because I've observed text 10 times in the text that has a total of 100 words.
#c76	I;text;the text;a total;100 words
#s77	And similarly, mining has five out of 100.
#c77	mining
#s78	And query as a relatively small probability, just observd once.
#c78	And query;a relatively small probability
#s79	So it's one out of 100.
#c79	it
#s80	Right, so that, intuitively, is a reasonable guess, but the question is Is this our best guess or best estimate of the parameters?
#c80	a reasonable guess;the question;our best guess;best estimate;the parameters
#s81	Of course, in order to answer this question we have to define what we mean by best.
#c81	order;this question;we;what;we
#s82	In this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate.
#c82	this case;it;our guesses;some sense;maximum likelihood estimate
#s83	And it's the best in that it would give our observed data the maximum probability.
#c83	it;it;our observed data;the maximum probability
#s84	Meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller.
#c84	you;the estimate;the probability;the observed text data
#s85	And this is called a maximum likelihood estimate.
#c85	a maximum likelihood estimate
410	59806251-1c41-4c2c-9207-e95bc0618a0e	130
#s1	This lecture is about a probabilistic retrieval model.
#c1	This lecture;a probabilistic retrieval model
#s2	In this lecture, we're going to continue the discussion of tax retrieval methods.
#c2	this lecture;we;the discussion;tax retrieval methods
#s3	We can do look at the another kind of very different way to design ranking functions than the vector space model that we discussed before.
#c3	We;the another kind;very different way;ranking functions;the vector space model;we
#s4	Being probabilistic models, we define the ranking function based on the probability that this document is relevant to this query.
#c4	probabilistic models;we;the ranking function;the probability;this document;this query
#s5	In other words, we introduce a binary random variable here.
#c5	other words;we;a binary random variable
#s6	This is the variable R here.
#c6	the variable R
#s7	And we also assume that the query and the documents are observations from random variables.
#c7	we;the query;the documents;observations;random variables
#s8	Note that in the vector space model we assume they are vectors, but here we are assumed.
#c8	the vector space model;we;they;vectors;we
#s9	We assume they are the data observed from random variables.
#c9	We;they;the data;random variables
#s10	And so the problem of retrieval now becomes two estimated.
#c10	the problem;retrieval
#s11	probability of relevance.
#c11	probability;relevance
#s12	In this category of models there are different variants.
#c12	this category;models;different variants
#s13	The classical problem is model has led to the BM 25 retrieval function which we discussed in the vector space model.
#c13	The classical problem;model;the BM 25 retrieval function;we;the vector space model
#s14	Because it's a form is actually similar to objectives space model.
#c14	it;a form;objectives space model
#s15	In this lecture, we will discuss another subclass in this.
#c15	this lecture;we;another subclass
#s16	Big class.
#c16	Big class
#s17	Called a language modeling approaches to retrieval.
#c17	a language;modeling approaches;retrieval
#s18	In particular, we're going to discuss the query likelihood retrieval model.
#c18	we;the query likelihood retrieval model
#s19	Which is one of the most effective models in probabilistic models.
#c19	the most effective models;probabilistic models
#s20	There is also another line called a divergent from randomness model which has led to.
#c20	another line;a divergent;randomness model
#s21	The PL-2 function.
#c21	The PL-2 function
#s22	It's also one of the most effective state of the other travel functions.
#c22	It;the most effective state;the other travel functions
#s23	In query likelihood Our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance.
#c23	query;Our assumption;this probability;relevance;the probability;query;a document;relevance
#s24	So intuitively this probability.
#c24	So intuitively this probability
#s25	Just captures the following probability and that is if a user likes document D. How likely would the user enter query Q in order to retrieve documenting?
#c25	the following probability;a user;document D.;the user;query Q;order
#s26	So assume that the user likes D. Because we have a relevance value here and then we asked the question about the how likely will see this particular query from this user.
#c26	the user;D.;we;a relevance value;we;the question;this particular query;this user
#s27	So this is the basic idea.
#c27	the basic idea
#s28	Not to understand this idea, let's take a look at the general idea or the basic idea of probabilistic retrieval models.
#c28	this idea;'s;a look;the general idea;the basic idea;probabilistic retrieval models
#s29	So here are listed at some.
#s30	Imagine the relevance of status values or relevance judgments.
#c30	the relevance;status values;relevance judgments
#s31	Often queries and documents.
#c31	documents
#s32	For example, in this line it shows that query one.
#c32	example;this line;it;that query
#s33	is A query that the user typed in and the D1 is a document the user has seen and one means the user thinks the one is relevant to Q1.
#c33	A query;the user;the D1;a document;the user;the user;the one;Q1
#s34	So this R here can be also approximated by the click through data that a search engine can collect by watching how you interact with the search results.
#c34	this R;the click;data;a search engine;you;the search results
#s35	So in this case, let's say the user clicked on this document, so there's one here.
#c35	this case;'s;the user;this document
#s36	Similarly.
#s37	The user clicked on D2 also, so there is 1 here.
#c37	The user;D2
#s38	In other words, D2 is assumed to be relevant to Q1.
#c38	other words;D2;Q1
#s39	On the other hand, D3  is non  relevant.
#c39	the other hand
#s40	There's a 0 here.
#s41	At the voice down relevant and then  D5 is again relevant.
#c41	the voice;then  D5
#s42	And so on so forth.
#s43	And this part.
#c43	And this part
#s44	Maybe data collected from a different user.
#c44	Maybe data;a different user
#s45	So this user typing Q1 and then found that D1 is actually not useful.
#c45	this user;Q1;D1
#s46	So divine is actually non relevant.
#s47	In contrast here we see it's relevant.
#c47	contrast;we;it
#s48	And all this could be the same query typing by.
#c48	the same query
#s49	The same user at different times.
#c49	The same user;different times
#s50	But D2  is also relevant, ET cetera.
#c50	D2;relevant, ET cetera
#s51	And here we can see more data.
#c51	we;more data
#s52	Then what about other queries?
#c52	other queries
#s53	Now we can imagine we have a lot of such data.
#c53	we;we;a lot;such data
#s54	We can ask the question, how can we then estimate the probability of relevance?
#c54	We;the question;we;the probability;relevance
#s55	Right, so how can we compute this probability of relevance?
#c55	we;this probability;relevance
#s56	Or intuitively that just means?
#s57	If we look at the all the entries where we see this particular D and this particular Q, how likely will see a one on the third column?
#c57	we;the all the entries;we;this particular D;this particular Q;a one;the third column
#s58	So basically that just means we can just collect those accounts.
#c58	we;those accounts
#s59	We can first count the how many times we have seen Q&D as a pair.
#c59	We;the how many times;we;Q&D;a pair
#s60	in this table and then count how many times we actually have also seen one in the third column.
#c60	this table;we;the third column
#s61	So and then we just.
#s62	Compute the ratio.
#c62	the ratio
#s63	So let's take a look at some specific examples.
#c63	's;a look;some specific examples
#s64	Suppose we're trying to compute this probability for D1D2 and D3 for Q1.
#c64	we;this probability;D1D2;D3;Q1
#s65	What is the estimated probability?
#c65	What;the estimated probability
#s66	Now think about that.
#s67	You can pause the video if needed.
#c67	You;the video
#s68	Try to take a look at the table.
#c68	a look;the table
#s69	And try to give your estimate of the probability.
#c69	your estimate;the probability
#s70	Have you seen that if we are interested in Q1 and D1 will be looking at these two pairs?
#c70	you;we;Q1;D1;these two pairs
#s71	And in both cases.
#c71	both cases
#s72	Actually, in one of the cases.
#c72	the cases
#s73	The user has said This is why this is relevant, so R is equal to 1 in only one of the two cases.
#c73	The user;R;the two cases
#s74	In the other case it's 0.
#c74	the other case;it
#s75	So that's one out of two.
#s76	What about the D1 and D2?
#c76	the D1;D2
#s77	"" They are here.
#c77	They
#s78	 in both cases.
#c78	both cases
#s79	In this case R  is equal to 1, so it's two out of two.
#c79	this case;it
#s80	And so on, so forth.
#s81	So you can see with this approach, we can actually score these documents for the query, right?
#c81	you;this approach;we;these documents;the query
#s82	We now have a score for D1D2 and D3.
#c82	We;a score;D1D2;D3
#s83	For this query we can simply rank them based on these probabilities, and so that's the basic idea of probabilistic retrieval model, and you can see it makes a lot of sense.
#c83	this query;we;them;these probabilities;the basic idea;probabilistic retrieval model;you;it;a lot;sense
#s84	In this case it's going to rank D2 above all the other documents, because in all the cases when you have seen D1 and D2.
#c84	this case;it;D2;all the other documents;all the cases;you;D1;D2
#s85	Eyes equals one the user clicked on this document.
#c85	Eyes;the user;this document
#s86	So this also.
#s87	Should.
#s88	Show that with a lot of click through data, a search engine can learn a lot from the data to improve their search engine.
#c88	a lot;click;data;a search engine;a lot;the data;their search engine
#s89	This is a simple example that shows that with even a small number of entries here we can already estimate some probabilities.
#c89	a simple example;even a small number;entries;we;some probabilities
#s90	These probabilities would give us some sense about which document might be more relevant or more useful to a user who typing this query.
#c90	These probabilities;us;some sense;which document;a user;who;this query
#s91	Now of course the problems that we don't observe all the queries and all the documents and all the relevance values.
#c91	course;we;all the queries;all the documents;all the relevance values
#s92	There will be a lot of unseen documents.
#c92	a lot;unseen documents
#s93	In general we only collected data from the documents that we have shown to the users.
#c93	we;data;the documents;we;the users
#s94	There are even more unseen queries because you cannot predict what queries would be typing by users.
#c94	even more unseen queries;you;what queries;users
#s95	So obviously this approach won't work if we apply it to unseen queries or unseen documents.
#c95	this approach;we;it;unseen queries;unseen documents
#s96	Nevertheless, this shows the basic idea of problems control model and it makes sense intuitively.
#c96	the basic idea;problems;model;it;sense
#s97	So what do we do in such a case when we have a lot of unseen documents and then some queries where the solutions that we have to approximate in somewhere, right?
#c97	what;we;such a case;we;a lot;unseen documents;we
#s98	So in this particular case code query like whole retrieval model, we just approximate this by another conditional probability.
#c98	this particular case code query;whole retrieval model;we;another conditional probability
#s99	P of Q given D an R is equal to 1.
#c99	P;Q;D;an R
#s100	So the condition part.
#c100	So the condition part
#s101	We assume that the user likes the document because we have seen that the user clicked on this document.
#c101	We;the user;the document;we;the user;this document
#s102	And this part shows that we're interested in how likely the user would actually enter this query.
#c102	this part;we;the user;this query
#s103	How likely will see this query in the same role?
#c103	this query;the same role
#s104	So no data here.
#c104	So no data
#s105	We have made an interesting assumption here.
#c105	We;an interesting assumption
#s106	Basically, we can do assume that whether the user types in this query has something to do with whether user likes the document.
#c106	we;the user types;this query;something;user;the document
#s107	In other words, we actually make the following assumption.
#c107	other words;we;the following assumption
#s108	And that is a user formula to query based on an imaginary relevant document.
#c108	a user formula;query;an imaginary relevant document
#s109	If you just look at this is conditional probability.
#c109	you;conditional probability
#s110	It's not obvious we're making this assumption.
#c110	It;we;this assumption
#s111	So what I really meant is that.
#c111	what;I
#s112	To use this new conditional probability to help us score, then this knew conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table.
#c112	this new conditional probability;us;conditional probability;this conditional probability;this big table
#s113	Otherwise we would be having similar problems as before an by making this assumption, we have some way to bypass this big table and try to just model how the user formulates the query.
#c113	we;similar problems;this assumption;we;some way;this big table;the user;the query
#s114	OK, so this is how you can simplify the general model so that we can derive a specific Iranian function later.
#c114	you;the general model;we;a specific Iranian function
#s115	So let's look at how this model work for our example, and basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query.
#c115	's;this model;our example;what;we;this case;the following question;these documents;the imaginary relevant document;the user's mind;the user;this query
#s116	So we ask this question and we quantify the probability and this probability is conditional probability of.
#c116	we;this question;we;the probability;this probability;conditional probability
#s117	Observing this query if a particular document is infected, imaginary relevant document in the user's mind.
#c117	this query;a particular document;the user's mind
#s118	Here you can see we compute all these query likelihood probabilities.
#c118	you;we;all these query likelihood probabilities
#s119	The likelihood of queries given each document.
#c119	The likelihood;queries;each document
#s120	Once we have these values, we can then rank these documents based on these values.
#c120	we;these values;we;these documents;these values
#s121	So to summarize, the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable R here, and then let's a scoring function be defined based on this conditional probability.
#c121	the general idea;modern relevance;the probabilistic model;we;a binary random variable R;'s;this conditional probability
#s122	We also talked about the approximate in this by using the query likelihood.
#c122	We;the approximate;the query likelihood
#s123	And in this case we have a ranking function that's basically based on the probability of a query given the document, and this probability should be interpreted as the probability that a user who likes document D would pose queria
#c123	this case;we;a ranking function;the probability;a query;the document;this probability;the probability;a user;who;document D;queria
#s124	Q. Now the question of course, is how do we compute this conditional probability?
#c124	Q.;the question;course;we;this conditional probability
#s125	At this, in general has to do with how to compute the probability of text, because Q is attached.
#c125	the probability;text;Q
#s126	And this has to do with.
#s127	Model called the Language model and this kind of models are proposed to model text.
#c127	Model;the Language model;this kind;models;text
#s128	So more specifically, we would be very interested in the following conditional probability as issuing this here if the user.
#c128	we;the following conditional probability
#s129	This document how likely the user would oppose this query.
#c129	This document;the user;this query
#s130	Ann In the next lecture working through, give an introduction to language models that we can see how we can model text with the probabilistic model in general.
#c130	the next lecture;an introduction;language models;we;we;text;the probabilistic model
410	5bb813bd-6b7d-4f77-8156-21995f5944ad	144
#s1	This lecture is about the similarity based approaches to text for clustering.
#c1	This lecture;the similarity based approaches;clustering
#s2	In this lecture, we're going to continue the discussion of how to do a text clustering.
#c2	this lecture;we;the discussion;a text clustering
#s3	In particular, we're going to cover a different kind of approaches than generative models.
#c3	we;a different kind;approaches;generative models
#s4	And that is similarity based approaches.
#c4	similarity based approaches
#s5	So the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects.
#c5	the general idea;similarity based clustering;a similarity function;the similarity;2:00 text objects
#s6	Now this is in contrast with a generative model where we implicitly define the clustering bias.
#c6	contrast;a generative model;we;the clustering bias
#s7	By using a particular objective function like a likelihood function.
#c7	a particular objective function;a likelihood function
#s8	The whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program.
#c8	The whole process;we;a review;what;we;it;us;any particular view;similarity;the clustering program
#s9	So once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups.
#c9	we;a similarity function;we;the data;clusters;different groups
#s10	Anne, try to maximize the intragroup similarity and minimize the intergroup similarity.
#c10	the intragroup similarity;the intergroup similarity
#s11	That is, to ensure the objects that are put in the same group to be similar, but the objects that are put into different groups to be not similar, and these are the general goals of clustering.
#c11	the objects;the same group;the objects;different groups;the general goals;clustering
#s12	And there's often a tradeoff between achieving both goals.
#c12	a tradeoff;both goals
#s13	Now, there are many different methods for doing similarity based clustering.
#c13	many different methods;similarity based clustering
#s14	In general, I think we can distinguish two strategies at high level.
#c14	I;we;two strategies;high level
#s15	One is to progressively construct the hierarchy of clusters.
#c15	the hierarchy;clusters
#s16	And so this often leads to hierarchical clustering an we can further distinguishes two ways to construct the hierarchy depending on whether we started with the collection to divide the collection or start with individual objects and gradually group them together.
#c16	hierarchical clustering;we;two ways;the hierarchy;we;the collection;the collection;individual objects;them
#s17	So one is bottom up that can be called agglomerative, where we gradually Group A similar object into larger and larger clusters until we group everything together.
#c17	one;we;larger and larger clusters;we;everything
#s18	The other is top down or divisive.
#s19	In this case we gradually partitioning the whole data set into smaller and smaller clusters.
#c19	this case;we;the whole data;smaller and smaller clusters
#s20	The other general strategy is to start with the initial tentative clustering and then iteratively improve it and this often leads to a flat clustering.
#c20	The other general strategy;the initial tentative clustering;it;a flat clustering
#s21	One example is K means.
#c21	One example;K means
#s22	So as I just said, there are many different clustering methods available and.
#c22	I;many different clustering methods
#s23	A full coverage of all these custom methods would be beyond the scope of this course.
#c23	A full coverage;all these custom methods;the scope;this course
#s24	But here we can talk about the two representative methods and.
#c24	we;the two representative methods
#s25	In some detail.
#c25	some detail
#s26	One is hierarchical agglomerative clustering or agency, the other is K-MEANS.
#c26	hierarchical agglomerative clustering;agency;K-MEANS
#s27	So first let's look at the agglomerative hierarchical clustering.
#c27	's;the agglomerative hierarchical clustering
#s28	In this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and then we can stop when some stopping criterions that.
#c28	this case;we;a similarity function;similarity;two objects;we;similar objects;a bottom up profession;larger and larger groups;they;a hierarchy;we
#s29	I could be either some number of classes has been achieved, or the threshold for similarity has been reached.
#c29	I;either some number;classes;the threshold;similarity
#s30	There are different variations here and there mainly differ in the ways to computer group similarity based on the individual object similarity.
#c30	different variations;the ways;computer group similarity;the individual object similarity
#s31	So let's illustrate how can induce a structure based on just similarity.
#c31	's;a structure;just similarity
#s32	So start with all the text objects and we can then measure the similarity between them.
#c32	all the text objects;we;the similarity;them
#s33	Of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together.
#c33	the provider similarity function;we;which pair;the highest similarity;them
#s34	And then was going to see which pair is.
#c34	which pair
#s35	The next one to group.
#c35	group
#s36	Maybe these two now have the highest similarity.
#c36	the highest similarity
#s37	And then we can gradually group them together in every time we're going to pick the highest similarity similarity pairs to group.
#c37	we;them;every time;we;the highest similarity similarity pairs;group
#s38	This will give us a binary tree eventually to group everything together.
#c38	us;a binary tree;everything
#s39	Now depending our applications, we can use the whole hierarchy as structure for browsing for example, or we can choose the cut off at say come here to get four clusters.
#c39	our applications;we;the whole hierarchy;structure;example;we;the cut;four clusters
#s40	Or we can use the threshold to cut or we can cut at this high level to get just the two clusters.
#c40	we;the threshold;we;this high level;just the two clusters
#s41	So this is a general idea.
#c41	a general idea
#s42	Now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity.
#c42	you;this algorithm;you;we;everything;the group similarity
#s43	We are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups.
#c43	We;the similarity function;two objects;we;we;the similarity;two groups
#s44	And there are also different ways to do that, and there's the three popular methods are single link complete link an average link?
#c44	different ways;the three popular methods;single link complete link;an average link
#s45	So given two groups and singling algorithm is going to define the group similarity as the similarity of the closest repair of the two groups.
#c45	two groups;algorithm;the group similarity;the similarity;the closest repair;the two groups
#s46	Complete Link defines the similarity of two groups as the similarity of the father sister pair.
#c46	Complete Link;the similarity;two groups;the similarity;the father sister pair
#s47	Average link defines the similarity as average of similarity of all the pairs of the two groups.
#c47	Average link;the similarity;similarity;all the pairs;the two groups
#s48	So it's much easier to understand these methods by illustrating them.
#c48	it;these methods;them
#s49	So here are two groups G1 and G2 with some objects in each group, and we know how to compute the similarity between two objects.
#c49	two groups G1;G2;some objects;each group;we;the similarity;two objects
#s50	But the question now is, how can we compute the similarity between the two groups?
#c50	the question;we;the similarity;the two groups
#s51	And then we can in general basis on the similarities of the objects in the two groups.
#c51	we;general basis;the similarities;the objects;the two groups
#s52	So in terms of single link and we're just looking at the closest pair.
#c52	terms;single link;we;the closest pair
#s53	So in this case these two pairs objects would define the similarity of the two groups.
#c53	this case;these two pairs objects;the similarity;the two groups
#s54	As long as they are very close orders, say the two groups are very.
#c54	they;very close orders;the two groups
#s55	Close so it's optimistic view of similarity.
#c55	it;optimistic view;similarity
#s56	The complete link, on the other hand, will be in some sense pessimistic and by taking the similarity of the two farthest appear as the similarity for the two groups.
#c56	The complete link;the other hand;some sense;the similarity;the similarity;the two groups
#s57	So we're going to make sure that if the two groups are  having a high similarity, then every pair of the two the objects in the group will be insured to have high similarity.
#c57	we;the two groups;a high similarity;then every pair;the objects;the group;high similarity
#s58	Every link is in between, so it takes average of all these pairs.
#c58	Every link;it;all these pairs
#s59	Now, these different ways of computing group similarities will need to different clustering algorithms, and they will generally give different results.
#c59	these different ways;computing group similarities;different clustering algorithms;they;different results
#s60	Now, so it's useful to take a look at their differences and to make comparison.
#c60	it;a look;their differences;comparison
#s61	Our first single link.
#c61	Our first single link
#s62	Can be expected to generate the loose clusters.
#c62	the loose clusters
#s63	The reason is becausw.
#c63	The reason;becausw
#s64	As long as two objects are very similar in the two groups, it would bring the two groups together.
#c64	two objects;the two groups;it;the two groups
#s65	If you think about this is similar to having parties with people, then it just means two groups of two groups of people would be putting together as long as each group there is a person that is well connected with the other group.
#c65	you;parties;people;it;two groups;two groups;people;each group;a person;the other group
#s66	So the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups.
#c66	the two leaders;the two groups;a good relationship;they;the two groups
#s67	In this case, the cluster is rules because there's no guarantee that other members of the two groups are actually very close to each other.
#c67	this case;the cluster;rules;no guarantee;other members;the two groups
#s68	Sometimes they may be very far away.
#c68	they
#s69	Now in this case it's also based on individual decision, so it could be sensitive to outliers.
#c69	this case;it;individual decision;it;outliers
#s70	The complete linker is in the opposite situation where we can expect the clusters to be tight.
#c70	The complete linker;the opposite situation;we;the clusters
#s71	Anne, it's also based on individual decision, so it can be sensitive to outliers.
#c71	it;individual decision;it;outliers
#s72	Again, to continue the analogy to having a party of people then complete the link would mean when two groups come together they want to ensure that even the.
#c72	the analogy;a party;people;the link;two groups;they
#s73	Even the people that are unlikely to talk to each other would be comfortable with talking to each other, so ensure the whole class to be coherent.
#c73	Even the people;the whole class
#s74	The average link, of course is in between and group decision, so it's going to be insensitive to outliers.
#c74	The average link;course;decision;it;outliers
#s75	In practice, which one is the best?
#c75	practice;one
#s76	Well, this will depend on the application and sometimes you need a loose classes and to aggressively on cluster objects together.
#c76	the application;you;a loose classes;cluster objects
#s77	Then maybe simple English good.
#c77	Then maybe simple English good
#s78	But other times you might need a tight clusters, then completely completely better, but in general you have to empirically evaluate these methods for your application to know which one is better.
#c78	you;a tight clusters;you;these methods;your application;which one
#s79	Now let's look at another example of method for similarity based classroom in this case.
#c79	's;another example;method;similarity based classroom;this case
#s80	Which is called K means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects.
#c80	K;means clustering;each text object;a term vector;similarity function;objects
#s81	Now we're going to start with some tentative clustering result by just selecting Kate randomly selected vectors as centroids of K clusters and treat them as sentence as they represent each cluster.
#c81	we;some tentative clustering result;Kate;centroids;K clusters;them;sentence;they;each cluster
#s82	So this is.
#s83	This gives us the initial tentative classroom.
#c83	us;the initial tentative classroom
#s84	And then we're going to iteratively improve it, and the process goes like this.
#c84	we;it;the process
#s85	And once we have these Central Eastside, we're going to assign a vector to the cluster hosts entry that is closest to the current vector.
#c85	we;these Central Eastside;we;a vector;the cluster hosts entry;the current vector
#s86	So basically we're going to measure the distance between this vector and each of the centroids, and see which one is closest to this one, and then just put this class this object into that cluster.
#c86	we;the distance;this vector;the centroids;which one;this one;this class;this object;that cluster
#s87	Now this is to have tentative.
#c87	tentative
#s88	Assignment of objects into clusters and we're going to partition or the objects into K clusters based on our tentative clustering centroids.
#c88	Assignment;objects;clusters;we;partition;the objects;K clusters;our tentative clustering centroids
#s89	And then we're going to recovery, compute the centroid based on the allocated objects in each cluster.
#c89	we;recovery;the centroid;the allocated objects;each cluster
#s90	And this is.
#s91	To adjust the centroid and then we had repeated this process until the similarity based on objective function.
#c91	the centroid;we;this process;the similarity;objective function
#s92	In this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective function.
#c92	this case;it;cluster sum;squares;we;this process;cluster sum;squares;objective function
#s93	Given K clusters.
#c93	K clusters
#s94	So it can be also shown this process will converge to a local minimum.
#c94	it;this process;a local minimum
#s95	I think about this process for a moment.
#c95	I;this process;a moment
#s96	It might remind you the EM algorithm for mixture model.
#c96	It;you;the EM algorithm;mixture model
#s97	Indeed, this algorithm is very similar to the EM algorithm for the mixture model for clustering.
#c97	this algorithm;the EM algorithm;the mixture model;clustering
#s98	So more specifically, we also initialize these.
#c98	we
#s99	Predators in the EM algorithm, so the random inner inner initialization is similar.
#c99	Predators;the EM algorithm;the random inner inner initialization
#s100	And then in the EML with them, you may recall that we're going to repeat eastep and M step to improved our primary destinations.
#c100	the EML;them;you;we;eastep;our primary destinations
#s101	In this case, we're going to improve the clustering result iteratively by also doing 2 steps, and in fact the two steps are very similar to EM algorithm.
#c101	this case;we;the clustering result;2 steps;fact;the two steps;EM;algorithm
#s102	In that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model.
#c102	we;vector;the clusters;our tentative clustering;it;the distribution;the document;the mixture model
#s103	So it's essentially similar to eastep.
#c103	it;eastep
#s104	Also, what's the difference?
#c104	what;the difference
#s105	While the differences here, we don't make a probabilistic on location as in the case of the step.
#c105	the differences;we;a probabilistic;location;the case;the step
#s106	But rather we make a choice.
#c106	we;a choice
#s107	We're going to make a call if this data point is closest to cluster two that were going to say you are in class too.
#c107	We;a call;this data point;you;class
#s108	So there's no choice, and we're not going to say you are 70% belonging to class too.
#c108	no choice;we;you;70%;class
#s109	And so we're not going to have a probability, but we're going to just put one object into precisely one cluster.
#c109	we;a probability;we;one object;precisely one cluster
#s110	In the E step, however, we do a probabilistic location, so we split the counts.
#c110	the E step;we;a probabilistic location;we;the counts
#s111	And we're not going to say exactly which distribution has been used to generate the data point.
#c111	we;exactly which distribution;the data point
#s112	Now the next we're going to adjust the centroid, and this is very similar to M step where we re estimate the parameters.
#c112	we;the centroid;M step;we;the parameters
#s113	That's when we'll have a better estimate of the parameter.
#c113	we;a better estimate;the parameter
#s114	So here we have a better clustering result by adjusting the centroid.
#c114	we;a better clustering result;the centroid
#s115	And note that the central is adjusted based on the average of the vectors in the.
#c115	the central;the average;the vectors
#s116	A cluster, so this is also similar to the M step, where we do counts pull together counter and normalize them, or the difference of course is also because of the difference in the instep, and we're not going to consider probabilities when we count the points in this case, for K means we're going to only count the objects allocated to this cluster, and this is only a subset of data points.
#c116	A cluster;the M step;we;counts;counter;them;the difference;course;the difference;the instep;we;probabilities;we;the points;this case;K;we;the objects;this cluster;only a subset;data points
#s117	But in the EM algorithm, we in principle consider all the data points.
#c117	the EM algorithm;we;principle
#s118	Based on probabilistic allocations.
#c118	probabilistic allocations
#s119	But in nature they are very similar and that's why it's also maximizing where defined objective function
#c119	nature;they;it;where defined objective function
#s120	and it's guaranteed to convert converted local minimum.
#c120	it;local minimum
#s121	So to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model.
#c121	our discussion;clustering methods;we;the model based approaches;mainly the mixture model
#s122	And here we use is implicitly similarity function.
#c122	we;similarity function
#s123	To define the clustering bias, there's no explicit definer similarity function.
#c123	the clustering bias;no explicit definer similarity function
#s124	The model defines clustering bias.
#c124	The model;clustering bias
#s125	And the clustering structure is built into a generated model.
#c125	the clustering structure;a generated model
#s126	That's why we can use potentially a different model to recover different instruction.
#c126	we;a different model;different instruction
#s127	Complex generative models can be used to discover complex clustering structures.
#c127	Complex generative models;complex clustering structures
#s128	We did not talk about it, but we can easily design generated model to generate a hierarchical clusters.
#c128	We;it;we;generated model;a hierarchical clusters
#s129	We can also use prior to further customize clustering algorithm to for example, control the topic of 1 cluster or multiple clusters.
#c129	We;algorithm;example;the topic;1 cluster;multiple clusters
#s130	However, one disadvantage of this approach is that there is no easy way to direct or control the similarity measure.
#c130	one disadvantage;this approach;no easy way;the similarity measure
#s131	Sometimes we want to do that, but it's very hard to inject the such a explicit definition of similarity into such a model.
#c131	we;it;the such a explicit definition;similarity;such a model
#s132	We also talked about the similarity based approaches.
#c132	We;the similarity based approaches
#s133	These approaches are more flexible.
#c133	These approaches
#s134	Directly specify similarity functions.
#c134	similarity functions
#s135	But one potential disadvantage is that their object function is not always very clear.
#c135	one potential disadvantage;their object function
#s136	The K means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach.
#c136	The K;algorithm;a clearly defined the objective function;it;a model based approach
#s137	The hierarchical clustering algorithm, on the other hand, is.
#c137	The hierarchical clustering;algorithm;the other hand
#s138	It's harder to.
#c138	It
#s139	To specify the objective function so it's not clear what exactly is being optimized.
#c139	the objective function;it;what
#s140	Both approaches can and generate the term clusters and document clusters.
#c140	Both approaches;the term clusters;document clusters
#s141	An term clusters can be in general generated by representing each term with some text content.
#c141	An term clusters;each term;some text content
#s142	For example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning.
#c142	example;the context;each term;a representation;each term;we;paradigmatic relation learning
#s143	And then we can certainly cluster terms based on actually their tax representations.
#c143	we;terms;their tax representations
#s144	Of course, term clusters can be generated by using generative models as well as we have seen.
#c144	term clusters;generative models;we
410	5e5aba04-1aee-4c63-9ca6-f9e1d0390689	24
#s1	so let's plugging these smoothing methods into the ranking function to see what we will get OK this is the general smoothing sorry general ranking function for smoothing with crashing and model you have seen this before and now we have a very specific and smoothing method the james move method so now let's see what value for our sub D here an what's the value for peace obscene here so we may need to decide this in order to figure out the exact form of the ranking function and we also need to figure out of course alpha so let's see well this ratio is basically this
#c1	's;these smoothing methods;the ranking function;what;we;sorry general ranking function;crashing;model;you;we;a very specific and smoothing method;the james move method;'s;our sub D;an what;the value;peace;we;order;the exact form;the ranking function;we;course;alpha;'s;this ratio
#s2	right so here this is the probability of singled on the top and this is the probability of unseen world or in other words lemma is basically the awful
#c2	the probability;the top;the probability;unseen world;other words;lemma
#s3	here this
#s4	so it's easy to see that this can be them riveting is this very simple
#c4	it;them
#s5	so we can plug this into here
#c5	we
#s6	and then here what's the value for other what do you think it'll be just lamp anne what would happen if we plug in this value here if this is number what can we say about this does it depend on the document
#c6	what;the value;you;it;just lamp anne;what;we;this value;number;what;we;it;the document
#s7	no so it can be ignored right
#c7	it
#s8	so we end up having this ranking function shown here and in this case you can easy to see this is precisely vector space model becaus this part is the sum over all the matched queried homes this is the element of the query about what do you think is element of the document about there
#c8	we;this ranking function;this case;you;precisely vector space model becaus;this part;the sum;all the matched queried homes;the element;the query;what;you;element;the document
#s9	well it's this so that's all of document that the element and let's further examine what's inside this is always so one plus this
#c9	it;document;'s;what
#s10	so it's going to be done negative log of this it's going to be at least one right
#c10	it;negative log;it;at least one right
#s11	and these this is a parameter so lemme biased parameter and let's look at this
#c11	a parameter;so lemme biased parameter;'s
#s12	and this is a TF now we see very clearly this TF waiting here and the larger the company 's the higher the waiting with me we also see idea of waiting which is given by this and with your document lance normalization here so all these heuristics are captured in this formula what's interesting that we kind of have got this waiting function automatically by making various assumptions whereas in the vector space model we had to go through those heuristic a design in order to get this and in this case note that there is a specific form and wanted to see whether this form actually makes sense i saw what do you think is the denominator here this is the length of document total number of words multiplied by the probability of the world given by the collection so this actually can be interpreted as expected account of award if we're going to draw a word from the collection language model and willing to draw as many as the number of words in the document if you do that the expected account of award W would be precisely a given by this denominator so this ratio basically is comparing the actual count here the actual count of the word in the document with the expected account given by this product if the world is in fact the following the distribution in the collection this
#c12	a TF;we;this TF;the company;me;we;idea;your document lance normalization;all these heuristics;this formula;what;we;this waiting function;various assumptions;the vector space model;we;those heuristic a design;order;this case;a specific form;this form;sense;i;what;you;the denominator;the length;document total number;words;the probability;the world;the collection;expected account;award;we;a word;the collection language model;the number;words;the document;you;the expected account;award W;this denominator;this ratio;the actual count;the actual count;the word;the document;the expected account;this product;the world;fact;the distribution;the collection
#s13	and if this counter is larger than the expected counter in this part this ratio it would be larger than one so that's actually a very interesting interpretation it's very natural and intuitively it makes a lot of sense and this is why advantage of using this kind of probabilistic reasoning well we have made it explicit assumptions and we know precisely why we have a logarithm here and why we have these probabilities are here and we also have a formula that intuitive that makes a lot of sense and those TF IDF weighting and document instrumentation let's look at the dealership price mosey is very similar to the case of james moving in this case the smoothing parameter is mule and that's different from lemma that we saw before but the format looks very similar the form of the function looks very similar so we still have linear interpolation here and when we compute this ratio while we defined that is that the ratio is equal to this now what's interesting here is that we're doing another comparison here now we're comparing the actual count which is the expected account of the war if we sample mule words according to the collection water probability so not that it's interesting we don't even see document lanthier and like it in the james morning so this of course should be plugged into this part so you might want us where is document lens interestingly the documents is here in office of the so this would be plugging this spot as a result what we get is the following function here and this is again a sum over all the match the query words
#c13	this counter;the expected counter;this part;it;a very interesting interpretation;it;it;a lot;sense;why advantage;this kind;probabilistic reasoning;we;it;we;we;a logarithm;we;these probabilities;we;a formula;a lot;sense;those TF IDF weighting and document instrumentation;'s;the dealership price mosey;the case;james;this case;the smoothing parameter;lemma;we;the format;the form;the function;we;linear interpolation;we;this ratio;we;the ratio;what;we;another comparison;we;the actual count;the expected account;the war;we;mule words;the collection water probability;it;we;document lanthier;it;the james morning;course;this part;you;us;document lens;the documents;office;this spot;a result;what;we;the following function;a sum;all the match
#s14	and we again see the query from frequency here and you can interpret this as the enemy of a document vector but this is no longer simple dot product right be cause we have this pot and note that end is the lens of the query
#c14	we;the query;frequency;you;the enemy;a document vector;simple dot product;we;this pot;that end;the lens;the query
#s15	i
#c15	i
#s16	so that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document
#c16	we;this function;we;a sum;all the query words;some adjustment;the score;the document
#s17	but it's still it's still clear that it does documents formalisation becaus this lens is in the denominator so a longer document will have a lower weight here and we can still see it has TF here and now idea only that this time the form of the formula is different from the previous one in james smooth but intuitively is still implements TF IDF weighting and documents from addition again the form of the function is dictated by the probabilistic reasoning and assumptions that we have made now their roles disadvantages of this approach and that is this look around here that such a form of the formula would actually work well so it look about at this material function although it's TF IDF weighting and stocking with anthem relishing for example it's unclear whether we have sub near transformation unfortunately and we can see here there is a logarithm function here so we do have also it's here so we do have some linear transformation
#c17	it;it;it;documents formalisation becaus;this lens;the denominator;a longer document;a lower weight;we;it;TF;the form;the formula;james smooth;TF IDF weighting;documents;addition;the form;the function;the probabilistic reasoning;assumptions;we;their roles disadvantages;this approach;this look;such a form;the formula;it;this material function;it;TF IDF weighting;stocking;anthem;example;it;we;sub;transformation;we;a logarithm function;we;it;we;some linear transformation
#s18	but we did not intentionally do that that means there's no guarantee that will end up in this way suppose we don't have logarithm then there's no sub linear transformation as we discussed before perhaps a formula is not going to work so well
#c18	we;no guarantee;this way;we;logarithm;no sub linear transformation;we;a formula
#s19	so that's example of the gap between formal model like this an the relevance that we have to model which is really subjective machine that is titled to users so it doesn't mean we cannot fix this for example imagine if we did not have this logarithm
#c19	example;the gap;formal model;an the relevance;we;subjective machine;users;it;we;example;we;this logarithm
#s20	right so we can heuristically add one or we can even add a double logarithm
#c20	we;we;a double logarithm
#s21	but then it would mean that the function is no longer probably small also the concequence of the modification is no longer as predictable as what we have been doing now so that's also why for example PM twenty five remains very competitive and still open challenger how to use probabilistic model to derive better model than BM twenty five in particular how do we use query like code to derive a model and that would work consistently better than BM twenty five currently we still cannot do that studio interesting open question so to summarize spot we've talked about the two smoothing methods generate mercer which is doing fixed coefficient of linear interpolation the richard applier this is will add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient that would be larger for shorter documents in both cases we can see by using these smoothing methods we would be able to reach a retrieval function where the assumptions are clearly articulated so they're less heuristic experiment results also show that these retrieval functions also are very effective and they are compara bulto BM twenty five or pivot in advance normalization so this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design yet in the end that we naturally implement the TF IDF weighting and documents on rotation each of these functions also has precisely one smoothing parameter in this case of course we still need to set the smoothing parameter but there are also methods that can be used to estimate these parameters so overall at this shows by using probabilistic model we follow very different strategy than the vector space model yet in the end we end up with some retrieval functions that look very similar to vectors based model with some advantages in having assumptions clearly stated and then the form dictated by probabilistic model now this also concludes our discussion of the query like holder problems with model anne this recall what assumptions we have made in order to derive the functions that we have seen in this lecture
#c21	it;the function;the concequence;the modification;what;we;example;PM;challenger;probabilistic model;better model;BM;we;query;code;a model;BM;we;that studio;open question;spot;we;the two smoothing methods;mercer;fixed coefficient;linear interpolation;the richard applier;a pseudo counts;every word;adaptive interpolation;the coefficient;shorter documents;both cases;we;these smoothing methods;we;a retrieval function;the assumptions;they;less heuristic experiment results;these retrieval functions;they;compara bulto BM;advance normalization;a major advantage;probabilistic model;we;a lot;heuristic design;the end;we;the TF IDF weighting;documents;rotation;these functions;precisely one smoothing parameter;this case;course;we;the smoothing parameter;methods;these parameters;this shows;probabilistic model;we;very different strategy;the vector space model;the end;we;some retrieval functions;vectors;based model;some advantages;assumptions;the form;probabilistic model;our discussion;the query;holder problems;model anne;what assumptions;we;order;the functions;we;this lecture
#s22	well we basically have made four assumptions that i listed here the first assumption is that the relevance can be more than by the query michael and the second assumption with maddie 's query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of all the words in the query
#c22	we;four assumptions;i;the first assumption;the relevance;the query michael;the second assumption;maddie 's query words;us;the probability;the whole query;a product;probabilities;all the words;the query
#s23	and then the third is something that we have made is if award is not seeing the document that we're going to let its probability but proportional to its probability in the collection of the smoothing was a classy language model and find that they have made one of these two assumptions about the smoothing so we either use james moving authorship rise moving if we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier fortunately the function has a nice property in that implements TF IDF weighting and documents on releasing an these functions also work very well so in that sense these functions are less heuristic compared with the vector space model and there are many extensions this basically model
#c23	something;we;award;the document;we;its probability;its probability;the collection;the smoothing;a classy language model;they;these two assumptions;the smoothing;we;james;authorship rise;we;these four assumptions;we;no choice;the form;the retrieval function;we;the function;a nice property;TF IDF weighting;documents;an these functions;that sense;these functions;the vector space model;many extensions
#s24	and you can find the discussion of them in the reference at the end of this laptop
#c24	you;the discussion;them;the reference;the end;this laptop
410	60d307bb-e329-4e74-9d8a-5ffd6ceed306	5
#s1	in this lecture we continue the discussion of vectors why is smaller in particular we're going to talk about the TF transformation in the previous lecture we have derived TF idea of weighting formula using the vectors space model and we have assumed that this model actually works pretty well for these examples as shown on this slide except for D five which has received very high score indeed it has received the highest score amount all these documents but this document is intuitively non relevant so this is not desirable in this lecture or going to talk about how we can use TF transformation to solve this problem before we discuss the details let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high school so this is the formula and if you look at the formula careful it and you will see it involves a sum over all the match the query terms an insider some each metric query term has a particular weight and this way it is TF IDF weighting so it has an idea of component where we see two variables one is the total number of documents in the collection and that is M the other is the document frequency this is a number of documents that contain this word W the other variables in involved in the formula include the count of the query tom W in the query and the count of the word in the document if you look at this document again now it's not hard to realize that the reason why it hasn't received the highest score isba cause it has a very high count of campaign so the count of campaigning in this document is a four which is much higher than the other documents and has contributed to the high score of this document so intuitively in order lower the score for this document we need to somehow restrict the contribution of the matching of this term in the document and if you think about matching of terms in a document carefully you actually would realize we probably should know reward multiple occurrences so generously and by that i mean the first recurrence of term says a lot about the matching of this time because it goes from zero count to account of one and then increase means a lot once we see a word in the document it's very likely the document is talking about this world if we see a extra occurrence on top of the first recurrence that is to go from one to two then we also can say that well the second occurrence kind of confirmed that it's not accidental managing of the world now we are more sure that this document is talking about this world but imagine we have seen let's say fifty times of the world in the document then adding one extra occurrence is not going to tell us more about evidence 'cause we already sure that this document is about this or so if you think in this way it seems that we should restrict the contribution of high count of term and that is the idea of tearful transformation so this transformation function is going to turn the raw counts of word into a term frequency wait for the word in the document so here i show in X access roll count and
#c1	this lecture;we;the discussion;vectors;we;the TF transformation;the previous lecture;we;TF idea;weighting formula;the vectors space model;we;this model;these examples;this slide;D;very high score;it;the highest score amount;all these documents;this document;this lecture;we;TF transformation;this problem;we;the details;'s;a look;the formula;this simple TF IDF;ranking function;this document;such a high school;the formula;you;the formula;it;you;it;a sum;all the match;the query;an insider;some each metric query term;a particular weight;it;TF IDF weighting;it;an idea;component;we;two variables;one;the total number;documents;the collection;M;the document frequency;a number;documents;this word;W;the other variables;the formula;the count;the query tom W;the query;the count;the word;the document;you;this document;it;the reason;it;the highest score isba;it;a very high count;campaign;the count;campaigning;this document;the other documents;the high score;this document;order;the score;this document;we;the contribution;the matching;this term;the document;you;terms;a document;you;we;multiple occurrences;i;the first recurrence;term;a lot;the matching;this time;it;zero count;account;a lot;we;a word;the document;it;the document;this world;we;a extra occurrence;top;the first recurrence;we;it;accidental managing;the world;we;this document;this world;we;'s;fifty times;the world;the document;one extra occurrence;us;evidence;we;this document;you;this way;it;we;the contribution;high count;term;the idea;tearful transformation;this transformation function;the raw counts;word;a term frequency;the word;the document;i;X access roll count
#s2	Y axis i show that um frequence of weight so in the previous ranking functions we actually have implicitly used some kind of transformation so for example in the zero one bit vector representation we actually use researcher transformation function as shown here basically if the count is zero then it has zero weight otherwise it would have a way to one is a flat now what about the using term count as TF wait well that's the linear function
#c2	i;um frequence;weight;the previous ranking functions;we;some kind;transformation;example;the zero one bit vector representation;we;researcher transformation function;the count;it;zero weight;it;a way;the using term count;TF;the linear function
#s3	right so it has just exactly the same weight as the count now we have just seen that this is not desirable so what we want is something like this so for example with the logarithm function we can have a sub linear transformation that looks like this and this will control the influence of really high weight because it's going to lower its inference yet it will retain the inference of small counts or we might want to even then the curve more by applying logarithm twice now people have tried all these methods and they are indeed working better than than linear form of the transformation but so far what works the best seems to be this special transformation called a BM twenty five transformation VM stands for best matching now in this transformation you can see there's a parameter came here and this K controls the upper bound of this function it's easy to see this function has upper bound because if you look at the X divided by X plus K where K is nonnegative number then the numerator will never be able to exceed the denominator so it's upper bounded by K plus one this is also difference between this transformation function and the logarithm transformation which it doesn't have upper bound uh furthermore one interesting property of this function is that as we vary K we can actually simulate different transformation functions including the two extremes that i've shown here that is a zero one bit transformation and the linear transformation so for example if we set K two zero now you can see the function value would be one so we precisely recover the zero one bit transformation if you said kate were very large number on the other hand is going to look more like the linear transformation function so in this sense this transformation is very flexible it allows us to control the shape of the transformation it also has a nice property of the upper bound and this upper bound is useful to control the inference of a particular time and so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time in other words this outbound might also ensure that all terms would be counted when we aggregate the weights to compute the score as i said this transformation function has worked well so far so to summarize this lecture the main point is that we need to do some linear T hop TF transformation and this is needed to capture the intuition of diminishing return from higher term counts it's also to avoid dominance by one single term over others this PM twenty five transmission transformation that we talked about is very interesting it's so far one of the best performing TF trans morning formation formulas it has upper bound and social robust and effective
#c3	it;just exactly the same weight;the count;we;what;we;something;example;the logarithm function;we;a sub linear transformation;the influence;really high weight;it;its inference;it;the inference;small counts;we;even then the curve;logarithm;people;all these methods;they;linear form;the transformation;what;this special transformation;a BM twenty five transformation VM;this transformation;you;a parameter;this K;this function;it;this function;you;the X;X;K;K;nonnegative number;the numerator;the denominator;it;K;difference;this transformation function;the logarithm transformation;it;furthermore one interesting property;this function;we;K;we;different transformation functions;the two extremes;i;a zero one bit transformation;the linear transformation;example;we;K;you;the function value;we;the zero one bit transformation;you;kate;very large number;the other hand;the linear transformation function;this sense;this transformation;it;us;the shape;the transformation;it;a nice property;the inference;a particular time;we;a spammer;the count;one term;all queries;this time;other words;this outbound;all terms;we;the weights;the score;i;this transformation function;this lecture;the main point;we;some linear T hop;TF transformation;the intuition;return;higher term;it;dominance;one single term;others;twenty five transmission transformation;we;it;formation;it
#s4	my food plucking this function into our TF IDF weighting vectors based model
#c4	my food;this function;our TF IDF weighting vectors;based model
#s5	then we would end up having the following ranking function which has a VM twenty five TF component now this is already very close two a state of that ranking function called BM twenty five anne will discuss how we can further improve this formula in the next rapture
#c5	we;the following ranking function;a VM twenty five TF component;two a state;that ranking function;BM;twenty five anne;we;this formula;the next rapture
410	628ab0a9-bfa9-4be2-96c8-42ecabcf6816	159
#s1	In this lecture we're going to talk about how to instantiate vector space model so that we can get a very In this lecture, we are going to talk about how to instantiate vector space model so that we can get a very specific ranking function.
#c1	this lecture;we;vector space model;we;this lecture;we;vector space model;we;a very specific ranking function
#s2	So this is to continue the discussion of the vector space model, which is one particular approach to design ranking function.
#c2	the discussion;the vector space model;one particular approach;ranking function
#s3	And we're going to talk about how we use the general framework of the vector space model as a guidance to instantiate the framework to derive a specific ranking function.
#c3	we;we;the general framework;the vector space model;a guidance;the framework;a specific ranking function
#s4	And we're going to cover the simplest instantiation of the framework.
#c4	we;the simplest instantiation;the framework
#s5	So as we discussed in the previous lecture, the Vector Space model is really a framework.
#c5	we;the previous lecture;the Vector Space model;a framework
#s6	It didn't say.
#c6	It
#s7	As we discussed in the previous lecture.
#c7	we;the previous lecture
#s8	Vector space model is really a framework.
#c8	Vector space model;a framework
#s9	It doesn't say many things.
#c9	It;many things
#s10	So for example, here it shows that it did not say how we should define the dimension.
#c10	example;it;it;we;the dimension
#s11	It also did not say how we place a document vector in this space.
#c11	It;we;a document vector;this space
#s12	It did not say how we place a query vector in this vector space.
#c12	It;we;a query vector;this vector space
#s13	And finally, it did not say how we should measure the similarity between the query vector and the document vector.
#c13	it;we;the similarity;the query vector;the document vector
#s14	So you can imagine in order to implement this model we have to say, specifically, how we compute these vectors?
#c14	you;order;this model;we;we;these vectors
#s15	What is exactly Xi and what is exactly Yi?
#c15	What;exactly Xi;what;Yi
#s16	This will determine where we place a document vector, where we place a query vector.
#c16	we;a document vector;we;a query vector
#s17	And of course, we also need to say exactly what should be the similarity function.
#c17	course;we;exactly what;the similarity function
#s18	So if we can provide a definition of the concepts that would define the dimensions and these Xi's or Yi's, namely weights of terms for query and document, then we will be able to place document vectors and query vector in this well defined space and then if we also specify similarity function then we'll have a well defined the ranking function.
#c18	we;a definition;the concepts;the dimensions;these Xi;Yi;namely weights;terms;query;document;we;document vectors;query vector;this well defined space;we;similarity function;we;a well defined the ranking function
#s19	So let's see how we can do that and think about the simplest instantiation.
#c19	's;we;the simplest instantiation
#s20	Actually, I would suggest you to pause the lecture at this point, spend a couple of minutes to think about.
#c20	I;you;the lecture;this point;a couple;minutes
#s21	Suppose you are asked to implement this idea.
#c21	you;this idea
#s22	You come up with the idea of vector space model.
#c22	You;the idea;vector space model
#s23	But you still have to figure out how to compute these vectors.
#c23	you;these vectors
#s24	Exactly how to define the similarity function?
#c24	the similarity function
#s25	What would you do?
#c25	What;you
#s26	So think for.
#s27	A couple of minutes and then proceed.
#c27	A couple;minutes
#s28	So let's think about the some simplest ways of instantiating this vector space model?
#c28	's;the some simplest ways;this vector space model
#s29	First, how do we define dimension where the obvious choice is to use each word in our vocabulary to define the dimension?
#c29	we;dimension;the obvious choice;each word;our vocabulary;the dimension
#s30	And here we show that there are N words in our vocabulary, therefore there are N dimensions.
#c30	we;N words;our vocabulary;N dimensions
#s31	Each word defines one dimension and this is basically the bag of words representation.
#c31	Each word;one dimension;the bag;words;representation
#s32	Now let's look at how we place vectors in this space.
#c32	's;we;vectors;this space
#s33	Again here the simplest strategy is to use a bit vector to represent both the query and a document.
#c33	the simplest strategy;a bit vector;both the query;a document
#s34	And that means each element Xi and Yi would be taking a value of either zero or one.
#c34	each element Xi;Yi;a value
#s35	When it's one, it means the corresponding word is present in the document or in query.
#c35	it;it;the corresponding word;the document;query
#s36	When it's zero it's going to mean that it's absent.
#c36	it;it;it
#s37	So you can imagine if the user types in a few words in the query, then the query vector will only have a few ones, many many zeros.
#c37	you;the user types;a few words;the query;the query vector;a few ones;many many zeros
#s38	The document vector in general will have more ones of course.
#c38	The document vector;more ones;course
#s39	But it will also have many zeros, since the vocabulary is generally very large.
#c39	it;many zeros;the vocabulary
#s40	Many words don't really occur in any document.
#c40	Many words;any document
#s41	Many words will only occasionally occur in the document.
#c41	Many words;the document
#s42	A lot of words will be absent in a particular document.
#c42	A lot;words;a particular document
#s43	So now we have placed the documents and the query in the vector space.
#c43	we;the documents;the query;the vector space
#s44	Let's look at how we measure the similarity.
#c44	's;we;the similarity
#s45	So a commonly used similarity measure here is dot product.
#c45	a commonly used similarity measure;dot product
#s46	The dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors.
#c46	The dot product;two vectors;the sum;the products;the corresponding elements;the two vectors
#s47	So here we see that it's the product of X1 and Y1.
#c47	we;it;the product;X1;Y1
#s48	So here.
#s49	And then X2 * Y2 and then finally XN multiplied by YN
#c49	And then X2 * Y2;then finally XN;YN
#s50	and then we take a sum here.
#c50	we;a sum
#s51	So that's the dot product.
#c51	the dot product
#s52	Now we can represent this in a more general way using a sum here.
#c52	we;a more general way;a sum
#s53	So this is only one of the many different ways of measuring the similarity.
#c53	the many different ways;the similarity
#s54	So now we see that we have defined the the dimensions we have defined the vectors and we have also defined the similarity function, so now we finally have the simplest of vector space model.
#c54	we;we;the the dimensions;we;the vectors;we;the similarity function;we;vector space model
#s55	Which is based on the bit vector represntation dot product similarity and bag of words representation.
#c55	the bit vector represntation dot product similarity;bag;words
#s56	And the formula looks like this.
#c56	the formula
#s57	So this is our formula, and that's actually particular retrieval function, a ranking function, right?
#c57	our formula;actually particular retrieval function;a ranking function
#s58	Now we can find the implement this function using a programming language and then rank documents for query.
#c58	we;the implement;this function;a programming language;documents;query
#s59	Now at this point you should again pause the lecture.
#c59	this point;you;the lecture
#s60	So think about how we can interpret this score.
#c60	we;this score
#s61	So we have gone through the process of modeling the retrieval problem.
#c61	we;the process;the retrieval problem
#s62	Using a vector space model
#c62	a vector space model
#s63	and then we make assumptions about how we place vectors in the vector space and how we define the similarity.
#c63	we;assumptions;we;vectors;the vector space;we;the similarity
#s64	So in the end that we've got a specific retrieval function, shown here.
#c64	the end;we;a specific retrieval function
#s65	Now the next step is to think about whether this retrieval function actually makes sense.
#c65	the next step;this retrieval function;sense
#s66	Can we expect this function to actually perform well when we used to rank the documents for users' queries?
#c66	we;this function;we;the documents;users' queries
#s67	So it's worth thinking about.
#c67	it
#s68	What is this value that we'll calculate?
#c68	What;this value;we
#s69	So in the end we get a number, but what does this number mean?
#c69	the end;we;a number;what;this number
#s70	Is it meaningful?
#c70	it
#s71	So spend a couple of minutes to think about that.
#c71	a couple;minutes
#s72	And of course the general question here is: Do you believe this is a good ranking function would they actually work?
#c72	course;the general question;you;a good ranking function;they
#s73	So again, think about how to interpret this value.
#c73	this value
#s74	Is it actually meaningful?
#c74	it
#s75	Does it mean something?
#c75	it;something
#s76	It's related to how well the document matches the query.
#c76	It;the document;the query
#s77	So in order to assess whether this simplest vector space model actually works well, let's look at the example.
#c77	order;this simplest vector space model;'s;the example
#s78	So here I show some sample documents and a simple query.
#c78	I;some sample documents;a simple query
#s79	The query is news about the presidential campaign and we have 5 documents here.
#c79	The query;news;the presidential campaign;we;5 documents
#s80	They cover different terms in the query.
#c80	They;different terms;the query
#s81	And if you look at the these documents for a moment, you may realize that some documents are probably relevant and some others are probably non relevant.
#c81	you;the these documents;a moment;you;some documents;some others
#s82	Now, if I ask you to rank these documents how would you rank them?
#c82	I;you;these documents;you;them
#s83	This is basically our ideal ranking: when humans can examine the documents and then try to rank them.
#c83	our ideal ranking;humans;the documents;them
#s84	So think for a moment and take a look at this slide and perhaps by pausing the lecture.
#c84	a moment;a look;this slide;the lecture
#s85	So I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well.
#c85	I;you;d4;d3;others;they;the query
#s86	They match news, presidential and campaign.
#c86	They;news;campaign
#s87	So it looks like these documents are probably better than the others, so they should be ranked on top.
#c87	it;these documents;the others;they;top
#s88	An the other three D2, D1, and D5 are really non relevant, so we can also say differently.
#c88	An the other three D2;D1;D5;we
#s89	D4 and D3 are relevant documents and D1, D2, and D5 are non relevant.
#c89	D4;D3;relevant documents;D1;D2;D5
#s90	So now let's see if our simplest vector space model could do the same or could do something closer.
#c90	's;our simplest vector space model;something
#s91	So let's first think about how we actually use this model to score documents.
#c91	's;we;this model;documents
#s92	Right here I show 2 documents D1 and D3 and we have the query also here.
#c92	I;2 documents;D1;D3;we;the query
#s93	In the vectors space model of course we want to 1st compute the vectors for these documents and the query.
#c93	the vectors space model;course;we;1st;the vectors;these documents;the query
#s94	Now I show the vocabulary here as well, so these are the N dimensions that will be thinking about.
#c94	I;the vocabulary;the N dimensions
#s95	So what do you think is the vector representation for the query?
#c95	what;you;the vector representation;the query
#s96	Note that we are assuming that we only use zero and one to indicate whether the term is absent or present in the query or in the document, so these are 0/1 bit vectors.
#c96	we;we;the term;the query;the document;0/1 bit vectors
#s97	So what do you think is the query vector?
#c97	what;you;the query vector
#s98	The query has four words here, so for these four words there will be one and for the rest will be 0.
#c98	The query;four words;these four words;the rest
#s99	Now, what about the documents?
#c99	the documents
#s100	It's the same, so T1 has two words news and about.
#c100	It;T1;two words news
#s101	So there are two ones here and the rest of zeros.
#c101	two ones;the rest;zeros
#s102	Similarly.
#s103	So.
#s104	Now that we have the two vectors.
#c104	we;the two vectors
#s105	Let's compute the similarity.
#c105	's;the similarity
#s106	And we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements, right?
#c106	we;dot product;you;we;dot product;we;the corresponding elements
#s107	So these two will be.
#s108	forming, forming product and these two will generate another product and these two will generate yet another product and so on so forth.
#c108	product;another product;yet another product
#s109	Now you can easy to see if we do that.
#c109	you;we
#s110	We actually don't have to care about.
#c110	We
#s111	These zeros.
#c111	These zeros
#s112	Because if whenever we have zero the product will be 0.
#c112	we;the product
#s113	So when we take a sum over all these pairs then the zero entries will be gone.
#c113	we;a sum;all these pairs;the zero entries
#s114	As long as you have 1 zero then the product will be 0.
#c114	you;the product
#s115	So in effect we're just counting how many Pairs of one and one, but in this case we have seen two, so the result would be two.
#c115	effect;we;how many Pairs;this case;we;the result
#s116	So what does that mean?
#c116	what
#s117	Well, that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document.
#c117	this number;the value;this scoring function;the count;how many unique query terms;the document
#s118	Because if a document, if a term is matched in the document, then there will be 2 ones.
#c118	a document;a term;the document;2 ones
#s119	If it's not then there will be 0 on the document side.
#c119	it;the document side
#s120	Similarly, if the document has a term, but the term is not in the query, there will be a zero in the query vector, so those don't count.
#c120	the document;a term;the term;the query;the query vector
#s121	So as a result this scoring function basically matches how many unique query terms are matched in the document.
#c121	a result;this scoring function;how many unique query terms;the document
#s122	This is how we interpret this score.
#c122	we;this score
#s123	Now we can also take a look at the D3.
#c123	we;a look;the D3
#s124	In this case you can see the result is 3 because D3 matched three distinct query words: news, presidential, campaign.
#c124	this case;you;the result;D3;three distinct query words;news;presidential, campaign
#s125	Whereas D1 only match two.
#c125	D1
#s126	Now in this case, it seems reasonable to rank D3 on top of D1 and this simplest vector space model indeed does that, so that looks pretty good.
#c126	this case;it;D3;top;D1;this simplest vector space model
#s127	However, if we examine this model in detail, we likely will find some problems.
#c127	we;this model;detail;we;some problems
#s128	So here I'm going to show all the scores for these five documents.
#c128	I;all the scores;these five documents
#s129	And you can easily verify their correct because we're basically counting the number of unique query terms matched in each document.
#c129	you;we;the number;unique query terms;each document
#s130	Now note that this matching actually that makes sense, right?
#c130	this matching;sense
#s131	It basically means if a document matches more unique query terms then the document will be assumed to be more relevant, and that seems to make sense.
#c131	It;a document;more unique query terms;the document;sense
#s132	The only problem is here we can notice that there are three documents D2, D3, and D4 and they tied with a 3 as a score.
#c132	The only problem;we;three documents D2;D3;D4;they;a score
#s133	So that's a problem, because if you look at them carefully, it seems that D4 should be ranked above D3 because D3 only mentioned presidential once, but D4 mentioned it multiple times.
#c133	a problem;you;them;it;D4;D3;D3;D4;it
#s134	In the case of D3 presidential, could be an extended matching.
#c134	the case;D3 presidential;an extended matching
#s135	But D4 is clearly about the presidential campaign.
#c135	D4;the presidential campaign
#s136	Another problem is that D2 and D3 also have the same score.
#c136	Another problem;D2;D3;the same score
#s137	But if you look at the three words that are matched in the case of D2 it matched the news, about, and campaign.
#c137	you;the three words;the case;D2;it;the news
#s138	But in the case of D3, it matched news, presidential, and campaign.
#c138	the case;D3;it;news;campaign
#s139	So intuitively D3 is better because matching presidential is more important than matching about even though about and presidential above in the query.
#c139	D3;matching presidential;the query
#s140	So intuitively, we would like D3 we ranked above D2.
#c140	we;D3;we;D2
#s141	But this model doesn't do that.
#c141	this model
#s142	So that means this model is still not good enough.
#c142	this model
#s143	We have to solve these problems.
#c143	We;these problems
#s144	To summarize, in this lecture we talked about how to instantiate a vector space model.
#c144	this lecture;we;a vector space model
#s145	We may need to do three things.
#c145	We;three things
#s146	One is to define the dimension.
#c146	the dimension
#s147	The second is to decide how to place documents as vectors in the vector space.
#c147	documents;vectors;the vector space
#s148	And to also place a query in the vector space as a vector.
#c148	a query;the vector space;a vector
#s149	And 3rd is to define the similarity between two vectors, particularly the query vector and the document vector.
#c149	3rd;the similarity;two vectors;particularly the query vector;the document vector
#s150	We also talk about a very simple way to instantiate vector space model.
#c150	We;a very simple way;vector space model
#s151	Indeed, that's probably the simplest vector space model that we can derive.
#c151	the simplest vector space model;we
#s152	In this case, we use each word to define a dimension.
#c152	this case;we;each word;a dimension
#s153	When user 0/1 bit vector to represent a document or a query.
#c153	When user 0/1 bit vector;a document;a query
#s154	In this case, we basically only care about word presence or absence.
#c154	this case;we;word presence;absence
#s155	We ignore the frequency.
#c155	We;the frequency
#s156	And we use the dot product as the similarity function.
#c156	we;the dot product;the similarity function
#s157	And with such a instantiation, and we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document.
#c157	such a instantiation;we;the scoring function;a document;the number;distinct query words;the document
#s158	We also show that such a such simple vector space model still doesn't work well and we need to improve it.
#c158	We;such a such simple vector space model;we;it
#s159	And this is a topic that we're going to cover in the next lecture.
#c159	a topic;we;the next lecture
410	6382e23f-d54e-4ece-a231-8df819983fb5	75
#s1	This lecture is continued discussion of evaluation of textual categorisation.
#c1	This lecture;continued discussion;evaluation;textual categorisation
#s2	Earlier we have introduced measures that can be used to compute the precision and recall for each category and each document.
#c2	we;measures;the precision;each category;each document
#s3	Now in this lecture we're going to do further  examine how to combine the performance on these different categories or different documents.
#c3	this lecture;we;the performance;these different categories;different documents
#s4	How do we aggregate them?
#c4	we;them
#s5	How do we take average?
#c5	we
#s6	You see on the title here, I indicated it's called a macro average and this is in contrast to micro average that will talk more about that later.
#c6	You;the title;I;it;a macro average;contrast
#s7	So.
#s8	Again, for each category, we can compute the precision recall and F1 so for example, for category C one.
#c8	each category;we;the precision recall;F1;example;category C one
#s9	We have precision  P1 recall R1 and F value F1 and similarly we can do that for Category 2 and all the other categories.
#c9	We;precision  P1;R1;F value F1;we;Category;all the other categories
#s10	Once we compute that, then we can aggregate them.
#c10	we;we;them
#s11	So for example, we can aggregate all the precision values for all the categories to compute the overall precision and this is often very useful.
#c11	example;we;all the precision values;all the categories;the overall precision
#s12	To summarize what we have seen in the whole data set and the aggregation can be done in many different ways.
#c12	what;we;the whole data;the aggregation;many different ways
#s13	Again, as I said, in case when you need to to aggregate different values.
#c13	I;case;you;different values
#s14	It's always good to think about what's the best way of doing the aggregation.
#c14	It;what;the best way;the aggregation
#s15	For example, you can consider arithmetic mean, which is very commonly used.
#c15	example;you
#s16	Or you can use geometric mean which would have different behavior depending on the way you aggregate.
#c16	you;geometric mean;different behavior;the way;you
#s17	You might have got different conclusions.
#c17	You;different conclusions
#s18	In terms of which method works better, so it's important to consider these differences and choosing the right one or more suitable one for your task.
#c18	terms;method;it;these differences;your task
#s19	So the difference, for example between arithmetic mean and geometric mean is that the arithmetic mean would be dominated by high values, whereas geometric mean would be more affected by low values, and so whether you want to emphasize low values or high values would be a question related to your application.
#c19	the difference;example;geometric mean;the arithmetic mean;high values;geometric mean;low values;you;low values;high values;a question;your application
#s20	And similar we can do that for recall and F score, so that's how we can then generate the overall precision, recall and F score.
#c20	we;recall;F score;we;the overall precision;recall;F score
#s21	Now we can do the same for aggregation over all the documents, right?
#c21	we;aggregation;all the documents
#s22	So it's exactly the same situation for each document or computer precision.
#c22	it;exactly the same situation;each document;computer precision
#s23	Recall and F.
#c23	Recall;F.
#s24	And then after we have completed the computations for all these documents we were going to aggregate them to generate the overall precision, overall recall and overall F score.
#c24	we;the computations;all these documents;we;them;the overall precision
#s25	These are again examining the results from different angles and which one is more useful would depend on your application.
#c25	the results;different angles;one;your application
#s26	In general, it's beneficial to look at the results from all these perspectives, and especially if you compare different methods in different dimensions.
#c26	it;the results;all these perspectives;you;different methods;different dimensions
#s27	It might reveal which method is better, in which measure or in what situations, and this provides insight for understanding the strength of a method or weakness, and this provides further insight for improving them.
#c27	It;which method;which measure;what situations;insight;the strength;a method;weakness;further insight;them
#s28	So as I mentioned, there is also micro averaging in contrast to the macro average that we talked about earlier.
#c28	I;contrast;the macro average;we
#s29	In this case, what we do is to pull together all the decisions.
#c29	this case;what;we;all the decisions
#s30	An then compute the precision and recall.
#c30	the precision
#s31	So we can compute the overall precision and recall by just counting how many cases are in true positive, how many cases in false positive, etc.
#c31	we;the overall precision;how many cases;, how many cases
#s32	Basically computing the values to fill in this contingency table and then we can compute precision recall just once.
#c32	the values;this contingency table;we;precision
#s33	Now, in contrast, in macro averaging we're going to do that for each category 1st and then aggregate over these categories.
#c33	contrast;we;each category;these categories
#s34	Or we do that for each document and then aggregate over all the documents.
#c34	we;each document;all the documents
#s35	But here we pulled them together.
#c35	we;them
#s36	Now this will be very similar to the classification accuracy that we introduced earlier, and one problem here of course, is to treat all the instances, all the decisions equally.
#c36	the classification accuracy;we;one problem;course;all the instances;all the decisions
#s37	And, this may not be desirable.
#s38	But it may be appropriate for some applications, especially if we associate, for example, the cost for each combination.
#c38	it;some applications;we;example;each combination
#s39	Then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision.
#c39	we;example;classification accuracy;you;the different cost;utility;each specific decision
#s40	So there could be variations of these methods that would be more useful, but in general macro average tends to be more informative than micro averaging just because it might reflect the need for understanding performance on each category or performance on each document which are needed in many applications.
#c40	variations;these methods;general macro average;it;the need;performance;each category;performance;each document;many applications
#s41	But the macro averaging and micro averaging, they're both very common and you might see both reported in research papers on text categorisation.
#c41	the macro averaging;micro averaging;they;you;research papers;text categorisation
#s42	Also, sometimes categorisation results might actually be evaluated from ranking perspective.
#c42	categorisation results;ranking perspective
#s43	Now this is because.
#s44	Categorisation results are sometimes or often indeed passed to human for various purposes.
#c44	Categorisation results;various purposes
#s45	For example, it might be passed to humans for further editing.
#c45	example;it;humans;further editing
#s46	For example, news articles can be tentatively categorized by using the system and then human editors would then correct them.
#c46	example;news articles;the system;human editors;them
#s47	And all the email messages might be routed to the right person for handling in the help desk, and in such a case the categorizations do help prioritizing the task for a particular customer service person.
#c47	all the email messages;the right person;the help desk;such a case;the categorizations;the task;a particular customer service person
#s48	So in this case, the results have to be prioritized.
#c48	this case;the results
#s49	And if the system can give a score to the categorisation decision or confidence, then we can use the scores to rank these decisions and then evaluate the results as a ranked list, just as in search engine evaluation, where you rank the documents in response to the query.
#c49	the system;a score;the categorisation decision;confidence;we;the scores;these decisions;the results;a ranked list;search engine evaluation;you;the documents;response;the query
#s50	So for example, discovery of spam emails can be evaluated, based on ranking emails for the spam category and this is useful if you want people to verify whether this is really spam, right?
#c50	example;discovery;spam emails;ranking emails;the spam category;you;people;spam
#s51	The person would then take the ranked list to check one by one and then verify whether this is indeed a spam.
#c51	The person;the ranked list;a spam
#s52	So to reflect the utility for humans in such a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again.
#c52	the utility;humans;such a task;it;the ranking accuracy
#s53	And in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem.
#c53	such a case;the problem;a ranking problem;categorization problem
#s54	So for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful.
#c54	example;ranking documents;the search engine;a binary categorization problem;relevant documents;users
#s55	But typically we frame this as a ranking problem and we evaluated as a ranked list.
#c55	we;a ranking problem;we;a ranked list
#s56	That's be cause people tend to examine the results sequentially, so ranking evaluation more reflects the utility from users perspective.
#c56	people;the results;so ranking evaluation;the utility;users perspective
#s57	So, to summarize, categorization evaluation, first evaluation is always very important for all these tasks, so get it right.
#c57	first evaluation;all these tasks;it
#s58	If you don't get it right, you might get misleading results an you might be misled to believe one method is better than the other, which is in fact not true.
#c58	you;it;you;misleading results;you;one method;fact
#s59	So it's very important to get it right.
#c59	it;it
#s60	Measures must also reflect the intended use of the results for particular application.
#c60	Measures;the intended use;the results;particular application
#s61	For example, in spam filtering and news categorization results are used in maybe  different ways.
#c61	example;spam filtering;news categorization results;different ways
#s62	So then we would need to consider the difference and design measures appropriately.
#c62	we;the difference and design measures
#s63	We generally need to consider how will the results be further processed by a user and then think from a user's perspective what quality is important.
#c63	We;the results;a user;a user's perspective;what quality
#s64	What aspect of quality is important.
#c64	What aspect;quality
#s65	Sometimes there are tradeoffs between multiple aspects, like precision and recall, and then, so we need to know for this application is high recall more important or high precision is more important.
#c65	tradeoffs;multiple aspects;precision;we;this application;more important or high precision
#s66	Ideally we associate the different cost with each different decision error and this of course has to be designed in application specific away.
#c66	we;the different cost;each different decision error;course;application
#s67	Some commonly used measures for relative comparison of different methods or the following classification accuracy is very commonly used for especially balanced tester set.
#c67	Some commonly used measures;relative comparison;different methods;the following classification accuracy;especially balanced tester set
#s68	Precision, recall, and F scores are commonly reported to characterize the performances in different angles, and there are some also variations like per document based evaluation, per category evaluation and then take average of all of them in different ways.
#c68	Precision;F scores;the performances;different angles;some also variations;document based evaluation;category evaluation;average;them;different ways
#s69	Micro versus macro averaging.
#c69	macro averaging
#s70	In general, you want to look at the results from multiple perspectives and for particular application in some perspectives would be more important than others, but
#c70	you;the results;multiple perspectives;particular application;some perspectives;others
#s71	for diagnosis, analysis of categorization methods and it's generally useful to look at as many perspectives as possible to see subtle differences between methods or to see where a method might be weak, from which you can obtain insights for improving a method.
#c71	diagnosis;categorization methods;it;as many perspectives;subtle differences;methods;a method;you;insights;a method
#s72	Finally, sometimes ranking may be more appropriate, so be careful.
#s73	Sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well.
#c73	a ranking task;machine learning methods;ranking measures
#s74	So here are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures.
#c74	two suggested readings;some chapters;this book;you;more discussion;evaluation measures
#s75	The second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.
#c75	a paper;the comparison;different approaches;categorization;it;excellent discussion;the text categorisation
410	64839f96-5182-452f-ae9f-1fa3e52fce70	11
#s1	so we talked about page rank as a way to capture the authorities now we also looked at some other examples where a hub might be interesting so there is another algorithm called hits and that's going to compute the scores for authorities and hubs the intuitions are pages that are wider site could sort of this then whereas pages at the site many other pages are good apps
#c1	we;page rank;a way;the authorities;we;some other examples;a hub;another algorithm;hits;the scores;authorities;the intuitions;pages;wider site;pages;the site;many other pages;good apps
#s2	but i think the most interesting idea of this algorithm hits is it's going to use reinforcement mechanism for kind of help improve the scoring for haps and the authorities an here
#c2	i;the most interesting idea;algorithm;it;reinforcement mechanism;kind of help;the scoring;haps;the authorities
#s3	so here's the idea it will assume that could authorities are cited by could hubs that means if you're cited by many pages with good hub scores then that increases your authority score and similarly could helps all those that pointed to
#c3	the idea;it;authorities;hubs;you;many pages;good hub scores;your authority score
#s4	could authorities' so if you get you pointed to a lot of good authoritie pages then your help score will be increased
#c4	you;you;a lot;good authoritie pages;your help score
#s5	so then we can iteratively reinforce each other BIH cause you can point to some good hubs also that you can point to some good authorities to get a good hub score whereas those also the scores would be also improved becaus they are pointed to by a good hub and this algorithm is also general it can have many applications in graphene network analysis so just briefly here's how it works with first also construct the metrics
#c5	we;each other BIH;you;some good hubs;you;some good authorities;a good hub score;those also the scores;becaus;they;a good hub;this algorithm;it;many applications;graphene network analysis;it;the metrics
#s6	but this time we're going to construct the adjacent symmetrics
#c6	we;the adjacent symmetrics
#s7	and we're not going to normalize the values so if there's a link there is one if there's no link that's zero again it's the same graph
#c7	we;the values;a link;no link;it;the same graph
#s8	and then we're going to define the hub score of page as the sum of the authorities scores of all the pages that it points to so whether you are happy it really depends on whether you're pointing to a lot of good authoritie pages that's what it says in the first equation in the second equation we define the authorities' score of a page as a sum of the hub scores of all those pages that appointed to you so whether you are good authors that would depend on whether those pages that are pointing you are good house so you can see this forms iterative reinforcement mechanism now these two equations can be also return in the metrics of all format so what we get here is then the hub vector is equal to the product of the edges and the metrics and the authoritie vector and this is basically the first equation a similar the second question can be returned as the authoritie vector is equal to the product of a transpose multiplied by the top vector and these are just different ways of expressing these equations but was interesting that if you look at the metrics form you can also plug in the authority equation into the first one so if you do that you have children that in limited authoritie raptor completely and you get the equation of only hub scores the hub score vector is equal to a multiplied by a transpose multiplied by the hub score vector again and similarly we can do a transformation to have equation for just the authorities calls so although with frame with a problem as computing haxan authorities' week after enumerate the one of them to obtain equation just for one of them now the difference between this end page ranger is that not the matrix is actually a multiplication of the edges in the metrics and its transfers so this is different from page rank but mathematics voted then we will be computing the same problem so it hits with typical will initialize the values let's say one for all these values and then with the iterative the apply these these equations essentially this is equivalent to multiply that by the metrics and a transpose and so the algorithm is exactly the same debate rank
#c8	we;the hub score;page;the sum;the authorities scores;all the pages;it;you;it;you;a lot;good authoritie pages;what;it;the first equation;the second equation;we;the authorities' score;a page;a sum;the hub scores;all those pages;you;you;good authors;those pages;you;good house;you;this forms;these two equations;the metrics;all format;what;we;the hub vector;the product;the edges;the metrics;the authoritie vector;the first equation;a similar the second question;the authoritie vector;the product;a transpose;the top vector;just different ways;these equations;you;the metrics form;you;the authority equation;you;you;children;limited authoritie raptor;you;the equation;only hub scores;the hub score vector;a transpose;the hub score vector;we;a transformation;equation;just the authorities;frame;a problem;computing haxan authorities' week;enumerate;them;equation;them;the difference;this end page ranger;the matrix;a multiplication;the edges;the metrics;its transfers;page rank;mathematics;we;the same problem;it;the values;'s;one;all these values;the iterative;these these equations;the metrics;a transpose;the algorithm;exactly the same debate rank
#s9	but here be cause the edges in the metrics is not normalized so what we have to do what we have to do is after each iteration working to normalize and this would allow us to control the growth of value otherwise there were the grew larger and larger and if we do that
#c9	the edges;the metrics;what;we;what;we;each iteration;us;the growth;value;we
#s10	and then we're basically get hits algorithm to compute the hub scores and also this calls for all the pages and these are scores can then be use ranching just like a pager in schools so to summarize in this lecture we have seen that link information is very useful in particular the anchor text is very useful to increase the the text representation of a page and will spoke about the page rank and hits art and has two major link analysis algorithms both can generate the scores for web pages that can be used in the renton function loads that page rank
#c10	we;hits algorithm;the hub scores;all the pages;scores;use;a pager;schools;this lecture;we;link information;the anchor text;the the text representation;a page;the page rank;art;two major link analysis algorithms;the scores;web pages;that page rank
#s11	and it's also very general algorithms so they have many applications in analyzing other graphs or networks
#c11	it;very general algorithms;they;many applications;other graphs;networks
410	6691b6ff-51d5-4d89-8bc8-bd882398be6a	21
#s1	this lecture is about natural language of content (test 51) analysis as you see from this picture this is really the first step to process any text today to text data are in natural languages so computers have to natural language to some extent in order to make use of the data so that's the topic of this laptop we're going to cover three things first what is natural language processing which is the main technique for processing natural language to obtain understanding the second is the state of lot of NLP which is stands for natural language processing finally we're going to cover the relation between natural language processing and the texture retrieval first what is NLP well the best way to explain it is to think about if you see a text in a foreign language that you can understand now what you have to do in order to understand that text this is basically what computers are phasing so looking at the simple sentence like a dog is chasing a boy on the playground we don't have any problem with understanding this sentence but imagine what the computer would have to do in order to understand it well in general it would have to do the following first it will have to know doggies are now chasing the verb etc
#c1	this lecture;natural language;content;you;this picture;the first step;any text;data;natural languages;computers;natural language;some extent;order;use;the data;the topic;this laptop;we;three things;what;natural language processing;the main technique;natural language;understanding;the state;lot;NLP;natural language processing;we;the relation;natural language processing;the texture retrieval;what;NLP;the best way;it;you;a text;a foreign language;you;what;you;order;that text;what computers;the simple sentence;a dog;a boy;the playground;we;any problem;this sentence;what;the computer;order;it;it;it;doggies;the verb
#s2	so this is the code lexical analysis or part of speech tagging and we need to figure out the syntactic categories of those words
#c2	the code lexical analysis;part;speech tagging;we;the syntactic categories;those words
#s3	so that's the first step after that we're going to figure out the structure of the centers so for example here it shows that a anna dog would go together to form a noun phrase and we won't have dogan ease to go first and there are some structures that are not just right but this structure shows what we might get if we look at the sentence and try to interpret the sentence some words would go together first
#c3	the first step;we;the structure;the centers;example;it;a anna dog;a noun phrase;we;dogan ease;some structures;this structure;what;we;we;the sentence;the sentence;some words
#s4	and then they will go together with other words so here we show we have non phrases as intermediate components and
#c4	they;other words;we;we;non phrases;intermediate components
#s5	then verbal phrases finally we have a sentence and you get this structure we need to do something called a syntactic analysis or pausing and we may have a pazer computer program that would automatically create this structure at this point that you would know the structure of this sentence but still you don't know the meaning of the sentence so we have to go further to semantic analysis in our mind we usually can map such a sentence to what we already know in our knowledge base and for example you might imagine a dog that looks like that there's a boy and there's some activity here but for computer would have to use symbols denote that so we would use a symbol T one to denote a dog and P want to denote a boy and then P one to know the playground playground now there is also chasing activity that's happening here so we have a relation chasing here that connects all these symbols
#c5	verbal phrases;we;a sentence;you;this structure;we;something;a syntactic analysis;pausing;we;a pazer computer program;this structure;this point;you;the structure;this sentence;you;the meaning;the sentence;we;semantic analysis;our mind;we;such a sentence;what;we;our knowledge base;example;you;a dog;a boy;some activity;computer;symbols;we;a symbol T one;a dog;P;a boy;then P;the playground;activity;we;a relation chasing;all these symbols
#s6	so this is how computer would obtain some understanding of this sentence now from this representation we could also further infer some other things and we might indeed naturally think of something else when we read the text and this is called inference so for example if you believe that if someone is being chased and this person might be scared with this rule you can see computers could also infer that this boy may be scaled so this is some extra knowledge that you would infer based on understanding of the text you can even go further to understand why the person said this sentence so this has to do with the use of language this is called pragmatic analysis in order to understand the speech actor of a sentence we say something too basically achieve some goal there's some purpose there and this has to do with the use of language in this case the person who said this sentence might be reminding another person to bring back the dog that could be one possible intent to reach this level of understanding would require all these steps and a computer would have to go through all these steps in order to completely understand this sentence yet we humans have no trouble with understand that we instantly we get everything and there is a reason for that that's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence Computers unfortunately are hard to obtain such understanding they don't have such a knowledge base there are still incapable of doing reasoning under uncertainties so that makes natural language processing difficult for computers but the fundamental reason why a natural language processing is difficult for computers is simply becaus natural language it has not been designed for computers the natural languages are designed for us to communicate there are other than we just design a full computers for example programming languages those are harder for us so natural languages is designed to make our communication efficient as a result we admit a lot of common sense knowledge becaus we assume everyone knows about that we also keep a lot of ambiguities becaus we assume the receiver or the hero could no how to decembrie good ambiguous word based on the knowledge or the complex there's no need to invent the different words for different meanings we could overload the same words with different meanings without the problem because of these reasons this makes every step in natural language processing difficulty for computers and B guild is the main difficulty and common sense reasoning is often required
#c6	computer;some understanding;this sentence;this representation;we;some other things;we;something;we;the text;inference;example;you;someone;this person;this rule;you;computers;this boy;some extra knowledge;you;understanding;the text;you;the person;this sentence;the use;language;pragmatic analysis;order;the speech actor;a sentence;we;something;some goal;some purpose;the use;language;this case;the person;who;this sentence;another person;the dog;one possible intent;this level;understanding;all these steps;a computer;all these steps;order;this sentence;we humans;no trouble;understand;we;we;everything;a reason;we;a large knowledge base;our brain;we;common sense knowledge;the sentence;Computers;such understanding;they;such a knowledge base;reasoning;uncertainties;natural language processing;computers;the fundamental reason;a natural language processing;computers;becaus natural language;it;computers;the natural languages;us;we;a full computers;example programming languages;us;natural languages;our communication;a result;we;a lot;common sense knowledge becaus;we;everyone;we;a lot;ambiguities;we;the receiver;the hero;good ambiguous word;the knowledge;the complex;no need;the different words;different meanings;we;the same words;different meanings;the problem;these reasons;every step;natural language processing difficulty;computers;B guild;the main difficulty;common sense reasoning
#s7	that's also hard so let me give you some examples of challenges here consider the word level ambiguity the same word came different syntactic categories for example design can be a noun or a verb the war route may have multiple meanings so square root in math sense or the root of a plant you might be able to think of other meanings there also syntactical ambiguities for example the main topic of this lecture natural language processing cap should be interpreted in two ways in terms of the structure think for a moment to see if you can figure that out we usually think of this as processing of natural language
#c7	me;you;some examples;challenges;the word level ambiguity;the same word;different syntactic categories;example design;a noun;a verb;the war route;multiple meanings;so square root;math sense;the root;a plant;you;other meanings;there also syntactical ambiguities;example;the main topic;this lecture natural language processing cap;two ways;terms;the structure;a moment;you;we;processing;natural language
#s8	but you could also think of this as you say language processes natural
#c8	you;you;language
#s9	so this is example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words another common example of ambiguous sentence is the following a man so a boy with the telescope now in this case the question is who had the telescope
#c9	example;syntactic ambiguity;we;different structures;the same sequence;words;another common example;ambiguous sentence;a man;a boy;the telescope;this case;the question;who;the telescope
#s10	right this is hold a prepositional phrase attachment and ability or PP attachment ambiguity not we generally don't have a problem with these ambiguities becaus
#c10	a prepositional phrase attachment;ability;PP attachment ambiguity;we;a problem;these ambiguities
#s11	we have a lot of background knowledge to help us disambiguate the ambiguity another example of difficulties anaphora resolution so think about the sentence like a jam persuaded appeal to buy a TV for himself the question here is does himself refer to jam or bill so again this is something that you have to use some background or the context to figure out finally presupposition is another problem consider the sentence he has quit smoking now this obviously implies that he smoked before so imagine a computer wants to understand all these subtle differences and meanings it would have to use a lot of knowledge to fix that loud it also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world so this is why it's very difficult so as a result we are step not perfect in fact the far from perfect in understanding natural language using computers so this slide sort of gives simplified view of state of large technologies we can do part of speech tagging pretty well
#c11	we;a lot;background knowledge;us;the ambiguity;another example;difficulties;resolution;the sentence;a jam;appeal;a TV;himself;the question;himself;jam;bill;something;you;some background;the context;finally presupposition;another problem;the sentence;he;smoking;he;a computer;all these subtle differences;meanings;it;a lot;knowledge;it;a large knowledge knowledge base;all the meanings;words;they;our common sense knowledge;the world;it;a result;we;step;fact;natural language;computers;this slide sort;simplified view;state;large technologies;we;part;speech tagging
#s12	so i showed that ninety seven percent accuracy here now this number is obviously based on a certain data set so don't take this literally this just shows that we can do it pretty well
#c12	i;ninety seven percent accuracy;this number;a certain data;we;it
#s13	but it's still not perfect in terms of pausing we can do partial pausing for the world that means we can get noun phrase structures or verbal phrases structure or some segment of the sentence and this dude correctly in terms of the structure an in some evaluation results we have seen about ninety percent accuracy in terms of partial pausing of sentences again i have to say these numbers are relative to the data set in some other data sets the numbers might be lower most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data think about the social media data the accuracy likely is lower in terms of semantic analysis we are far from being able to do a complete understanding of a sentence but we have some techniques that would allow us to do partial understanding of the sentence so i could mention some of them for example we have techniques that can allow us to extract the entities and relations mentioning tax articles for example recognizing the mentions of people locations organisations etc in text
#c13	it;terms;pausing;we;the world;we;noun phrase structures;verbal phrases structure;some segment;the sentence;this dude;terms;the structure;some evaluation results;we;about ninety percent accuracy;terms;partial pausing;sentences;i;these numbers;the data;some other data;the numbers;the existing work;news data;a lot;these numbers;news data;the social media data;the accuracy;terms;semantic analysis;we;a complete understanding;a sentence;we;some techniques;us;partial understanding;the sentence;i;them;example;we;techniques;us;the entities;relations;tax articles;example;the mentions;people locations organisations;text
#s14	so this is called entity extraction we may be able to recognize the relations for example this person visited that place or this person met that person or this company acquired another company such relations can be extracted by using the current that natural language processing techniques they're not perfect but they can do well for some men today's so many days are harder than others we can also do word sense disambiguation to some extent we can figure out the weather this word in this sentence would have sort of meaning in another context the computer could figure out it has a different meaning again it's not perfect but you can do something in that direction we can also do sentiment analysis meaning to figure out the weather sentence is positive or negative this is especially useful for review analysis for example so these are examples of semantic analysis and they help us to obtain partial understanding of the sentences it's not giving us a complete understanding as i showed it before for this sentence but it would still help us gain understanding of the content and these can be useful in terms of inference we are not there yet probably be cause of the general difficulty of inference and uncertainties this is a general challenger in artificial intelligence that partly also be cause we don't have complete semantical representation for natural language text so this is hard yet in some domains perhaps in limited domains when you have a lot of restrictions on the world uses you maybe do may be able to perform inference to some extent but in general we cannot really do that reliable speech act analysis is also far from being down and we can only do that analysis for various special cases so this roughly gives you some idea about the state of the art and then we also talk a little bit about what we can't do and so we can't even do one hundred percent part of speech tagging now this looks like a simple task but think about the example here the two uses of off may have different syntactic categories if you try to make a fine grained distinctions it's not that easy to figure out the such differences it's also hard to do general complete pausing and again this same sentence that you saw before is example this ambiguity can be very hard to disambiguate and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope so although the sentence looks very simple it actually is pretty hard and in cases when the sentence is very long imagine it has four or five prepositional phrases and there are even more possibilities to figure out it's also harder to do precise deep semantic analysis so here's example in the sentence journal owns a restaurant how do we define owns exactly the word is something that we understand but it's very hard to precisely describe the meaning of for computers so as a result we have robust and general natural language processing techniques that can process a lot of text data in a shallow way meaning we only do superficial analysis for example a policy of speech tagging or party or passing or recognizing sentiment and those are not deep understanding be cause we're not really understanding the exact the meaning of a sentence on the other hand of the deep understanding techniques ten not to scale up a well meaning that they would fail on some unrestricted a text ann if you don't restrict the text domain or the use of words then these techniques tend not to work well they may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on but the general wouldn't work well on the there are that are very different from the training data so this pretty much summarizes the state of the art of natural language processing of course within such a short amount of time we can really give you complete view of NLP which is big field an either expect that to to see multiple causes on natural language processing topic itself but becaus of its relevance to the topic of we talk about it's useful for you know the background in case you haven't been exposed to that so what does that mean for text retrieval well in text retrieval we're dealing with all kinds of text
#c14	entity extraction;we;the relations;example;this person;that place;this person;that person;this company;another company;such relations;that natural language processing techniques;they;they;some men;today's so many days;others;we;word;sense;disambiguation;some extent;we;the weather;this word;this sentence;another context;the computer;it;a different meaning;it;you;something;that direction;we;analysis;the weather sentence;review analysis;example;examples;semantic analysis;they;us;partial understanding;the sentences;it;us;a complete understanding;i;it;this sentence;it;us;understanding;the content;terms;inference;we;cause;the general difficulty;inference;uncertainties;a general challenger;artificial intelligence;we;complete semantical representation;natural language text;some domains;limited domains;you;a lot;restrictions;the world;you;inference;some extent;we;that reliable speech act analysis;we;that analysis;various special cases;you;some idea;the state;the art;we;what;we;we;one hundred percent part;a simple task;the example;the two uses;different syntactic categories;you;a fine;it;the such differences;it;general;pausing;you;example;this ambiguity;you;example;you;a lot;knowledge;the context;the sentence;the background;order;who;the telescope;the sentence;it;cases;the sentence;it;four or five prepositional phrases;even more possibilities;it;deep semantic analysis;example;the sentence journal;a restaurant;we;exactly the word;something;we;it;the meaning;computers;a result;we;robust and general natural language processing techniques;a lot;text data;a shallow way;we;superficial analysis;example;a policy;speech tagging;party;sentiment;deep understanding;we;the exact;the meaning;a sentence;the other hand;the deep understanding techniques;a well meaning;they;some unrestricted a text ann;you;the text domain;the use;words;these techniques;they;techniques;the data;the training data;the program;the general;the training data;the state;the art;natural language processing;course;such a short amount;time;we;you;complete view;NLP;big field;multiple causes;natural language processing topic;itself;becaus;its relevance;the topic;we;it;you;case;you;what;text retrieval;text retrieval;we;all kinds;text
#s15	it's very hard to restrict the texture to a certain domain and we also often dealing with a lot of text data so that means the NLP techniques must be general robust and efficient and that just implies today we can only use fairly shallow NLP techniques for text retrieval in fact most search engines do they use something called a bag of words representation now this is probably the simplest representation you can possibly think of that is the current text data into simply a bag of words meaning we will keep individual words
#c15	it;the texture;a certain domain;we;a lot;text data;the NLP techniques;we;fairly shallow NLP techniques;text retrieval;fact;most search engines;they;something;a bag;words;representation;the simplest representation;you;the current text data;simply a bag;words;we;individual words
#s16	but we'll ignore all the orders of words and we'll keep duplicated occurrences of words so this is called a bag of words repent nation when you represent the text in this way you ignore a lot of other information an that just makes it harder to understand the exact the meaning of a sentence becaus we've lost the order
#c16	we;all the orders;words;we;duplicated occurrences;words;a bag;words;repent nation;you;the text;this way;you;a lot;other information;it;the exact;the meaning;a sentence becaus;we;the order
#s17	but yet this replenishing tends to after the work pretty well for most search tasks and this is partly be cause the search taskbar is not all that difficult if you see matching of some of the query words in a text document changes all that that document is about the topic although there are exceptions so in comparison some other tasks for example machine translation would require you to understand the language accurately otherwise the translation would be wrong so in comparison search task is all relatively easy such a representation is often sufficient and that's also their condition that the major search engines today like a google or bing are using of course i put in princess here but not all of course there are many queries that are not answered away by the current search engines and they do require representation that would go beyond bag of words representation that would require more natural language processing to be done there is another reason why we have not used the sophisticated NLP techniques in modern search engines and let's be cause some retrieval techniques actually naturally solve the problem of NLP so one example is water sense disambiguation think about the world like java it could mean coffee or could mean program language if you look at the world alone it will be ambiguous but when the user uses the word in the query usually there are other words for example i'm looking for usage of java applet when i have applet there that implies java means programming language and that context can help us naturally prefer documents where java is referring to program language becaus those documents would probably match app late as well if java a cursing that documented way the means coffee then you would never match applet with various more probability
#c17	the work;most search tasks;the search taskbar;you;the query words;that document;the topic;exceptions;comparison;some other tasks;example machine translation;you;the language;the translation;comparison search task;such a representation;their condition;the major search engines;a google;bing;course;i;princess;many queries;the current search engines;they;representation;bag;words;representation;more natural language processing;another reason;we;the sophisticated NLP techniques;modern search engines;'s;some retrieval techniques;the problem;NLP;one example;water sense disambiguation;the world;it;coffee;program language;you;the world;it;the user;the word;the query;other words;example;i;usage;java applet;i;programming language;that context;us;documents;program language becaus;those documents;app;the means coffee;you;various more probability
#s18	so this is the case when some retrieval techniques naturally achieve the goal of water sense disambiguation another example is some technique called feedback we will talk about later in some of the lectures this technique technique it would allow us to add additional words to the query and those additional words could be related to the query words and these words can't help matching documents where the original query words have not occured so this achieves to some extent semantic matching of terms so those techniques also helped us bypass some of the difficulties in natural language processing however in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines
#c18	the case;some retrieval techniques;the goal;water sense disambiguation;another example;some technique;feedback;we;the lectures;this technique technique;it;us;additional words;the query;those additional words;the query words;these words;documents;the original query words;some extent semantic matching;terms;those techniques;us;the difficulties;natural language processing;the long run;we;a deeper natural language processing techniques;order;the accuracy;the current search engines
#s19	and it's particularly therefore complex search tasks or for question answering google has recently launched knowledge graph
#c19	it;therefore complex search tasks;question;google;knowledge graph
#s20	and this is one step toward that goal becaus knowledge graph would contain entities and their relations and this goes beyond the simple bag of words representation and such techniques should help us improve the search engine utility significantly although this is there open topic of all research and exploration in some in this actual we've talked about what is NLP ann we've talked about the state of the art techniques what we can do what we cannot do
#c20	one step;that goal;becaus knowledge graph;entities;their relations;the simple bag;words;representation;such techniques;us;the search engine utility;open topic;all research;exploration;we;what;NLP ann;we;the state;the art techniques;what;we;what;we
#s21	and finally we also explain the white bag of words representation remains the dominant replantation used in modern search engines even though deeper NLP would be needed for future search engines if you want to know more you can take a look at some additional readings i only cited one here and that's a good starting point thanks
#c21	we;the white bag;words;representation;the dominant replantation;modern search engines;deeper NLP;future search engines;you;you;a look;some additional readings;i;a good starting point
410	675158b2-3979-4126-a098-e4d7a73c11e2	26
#s1	so let's take a look at this in detail so in this random surfing model at any page would assume random sofa would choose the next page to visit so this is a small graph here that's of course over simplification of the complicated web but let's say there are four documents here I T one T two T three and T four and let's assume that a random server or random walker can be on any of these pages and then the random suffer could decide to just randomly jumping to any page or follow a link and then visited the next page so if the random server is at A T one then and with some probability that random so far will follow the links now there are two out links here one is pointing to the three the other is pointing to default so the random so far could pick any of these two to reach E three and E four
#c1	's;a look;detail;this random surfing model;any page;random sofa;the next page;a small graph;course;simplification;the complicated web;'s;four documents;I;T one T two T;T;'s;a random server;random walker;these pages;the random suffer;any page;a link;the next page;the random server;A T one;some probability;the links;two out links;one;E
#s2	but it also assumes that the random sofa might get bored sometimes so the random server would decide to ignore the actual links and simply randomly jump to any page on the web so if it does that would be able to reach any of the other pages even though there's no link directly from the wild to that page so this is assumed random surfing model imagine and random server is really doing surfing like this then we can ask the question how likely on average the server would actually reach particular page in fact D one or D two or D three that's the average probability of visiting a particular page and this probability is precisely what page ranger computes so the page rank score of the document that is the average probability that the sofa visits a particular page now intuitively this would basically capture the in link account why becaus if a page has a lot of inlinks then it would have a higher chance of being visited because there will be more opportunities of having the sofa the following link to come to this page and this is why the random surfing model actually captures the idea of counting the inlinks loads that it also considers the interacting links why becaus if the pages that point too you have themselves a lot of innings that with me the random server we are very likely reach one of them and therefore it increases the chance of visiting you so this is a nice way to capture both indirect and direct links so mathematically how can we compute this probability in order to see that we need to take a look at how this probability is computed so first let's take a look at the transition matrix here and this is just the metrics with values indicating how likely the random server will go from one page to another so each row stands for a starting page for example low one would indicate the probability of going to any other four pages from he won and here we see there are only nine two nine zero entries each is one over two half so this is be cause if you look at the graph D Y is pointing to the three and D four is no link from D one to D one cell for D two
#c2	it;the random sofa;the random server;the actual links;any page;the web;it;the other pages;no link;the wild;that page;random surfing model;random server;surfing;we;the question;the server;particular page;fact;D;the average probability;a particular page;this probability;precisely what page ranger computes;the page rank score;the document;the average probability;the sofa;a particular page;link;a page;a lot;inlinks;it;a higher chance;more opportunities;the sofa;the following link;this page;the random surfing model;the idea;the inlinks;it;the interacting links;the pages;you;themselves;a lot;innings;me;we;them;it;the chance;you;a nice way;both indirect and direct links;we;this probability;order;we;a look;this probability;'s;a look;the transition matrix;just the metrics;values;the random server;one page;each row;a starting page;example;low one;the probability;any other four pages;he;we;only nine two nine zero entries;you;the graph;D Y;no link;D;one;D;one cell;D
#s3	so we've got zeros for the first of two columns and point five four E three and E four in general the element in this metrics M sub idea is the probability of going from D
#c3	we;zeros;two columns;point;the element;this metrics M sub idea;the probability;D
#s4	I two DJ an obviously for each rule the values should sum to one becaus the server would have to go to precisely one of these other pages so this is a transition matrix now how can we compute the probability of a surfer visiting a page
#c4	I;DJ;each rule;the values;one becaus;the server;these other pages;a transition matrix;we;the probability;a surfer;a page
#s5	well if you look at the surf model then basically we can compute the probability of reaching a page as follows so here on the left hand side you see it's the probability of visiting page DJ at time T plus one
#c5	you;the surf model;we;the probability;a page;the left hand side;you;it;the probability;visiting page DJ;time T
#s6	so it's the next time point on the right hand side you can see the equation involves the probability of at page D
#c6	it;the next time point;the right hand side;you;the equation;the probability;page D
#s7	I at time T so you can see the subscript index T here and that indicates that the probability that the server was at a document at time T so the equation basically captures the two possibilities of reaching a DJ at the time three plus one what are these two possibilities but one is through random surfing and one is through following a link as we just explained so the first part captures the probability that the random server would reach this page by following a link and you can see the random sofa chooses this strategy with probability one minus R file as we assume and so there is a factor of one minus are here but the main part is really some over all the possible pages that server could have been at time T there are N pages
#c7	I;time;T;you;the subscript index T;the probability;the server;a document;time;T;the equation;the two possibilities;a DJ;the time;what;these two possibilities;one;random surfing;one;a link;we;the first part;the probability;the random server;this page;a link;you;the random sofa;this strategy;probability one minus R file;we;a factor;one minus;the main part;all the possible pages;server;time;T;N pages
#s8	so it's a sum over all the possible in pages inside the sum is a product of two probabilities one is the probability that the server was at the at the time T that's P sub T of the eye the other is the transition probability from D
#c8	it;a sum;pages;the sum;a product;two probabilities;the probability;the server;the time;T;P sub T;the eye;the transition probability;D
#s9	I two DJ
#c9	I;DJ
#s10	and so in order to reach this DJ page the server must first be at D
#c10	order;this DJ page;the server;D
#s11	I at the time team
#c11	I;the time team
#s12	and then also would have to follow the link to go from D I to DJ
#c12	the link;D;I;DJ
#s13	so the probability is the probability of being at D
#c13	the probability;the probability;D
#s14	I at time T multiplied by the probability of going from that page to the target page DJ here the second pod is a similar sound the only difference is that now the transition probability is a uniform transition probability of one over N and this pilot captures the probability of reaching this page through random jumpy
#c14	I;time;T;the probability;that page;the target page DJ;the second pod;a similar sound;the only difference;the transition probability;a uniform transition probability;N;this pilot;the probability;this page;random jumpy
#s15	right so the form is exactly the same and this also allows us to see why pagerank essentially assumed smoothing of the transition matrix if you think about this one over N as coming from another transition matrix that has all the elements being one over N the uniform metrics then you can see very clearly essentially we can merge the two parts becaus they all the same form we can imagine there's a different metrics that's a combination of this M and that uniform matrix where every element is one over N and in this sense page rank it uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix now of course this is time dependent calculation of the probabilities now we can imagine if we're on the computer average probabilities the average probability is properly with the steps file this equation without considering the time index
#c15	the form;us;pagerank;smoothing;the transition matrix;you;N;another transition matrix;all the elements;N;the uniform metrics;you;we;the two parts becaus;they;all the same form;we;a different metrics;a combination;this M;that uniform matrix;every element;N;this sense page rank;it;this idea;no zero entry;such a transition matrix;course;time dependent calculation;the probabilities;we;we;the computer average probabilities;the average probability;the steps file;this equation;the time index
#s16	so let's drop the time index and just assume that there will be equal now this would give us any equations becaus for each page we have such equation and if you look at the what variables we have in these equations there also precisely N variables right so this basically means we now have a system of any equations within variables and these are linear equations so basically now the problem boils down to solve this system of equations and here i also show the equations in the metrics form it's the vector P here equals a metrics well the transpose of the metrics here and multiply by the vector again now if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for eigenvector right when you multiply the metrics by this vector you get the same value as this vector and this can be solved by using iterative algorithm so the question is here on the bob basically taken from the previous slide
#c16	's;the time index;us;any equations;becaus;each page;we;such equation;you;what variables;we;these equations;we;a system;any equations;variables;linear equations;the problem;this system;equations;i;the equations;the metrics;it;the vector P;a metrics;the transpose;the metrics;the vector;you;some knowledge;you;linear algebra;you;the equation;eigenvector;you;the metrics;this vector;you;the same value;this vector;iterative algorithm;the question;the bob;the previous slide
#s17	so you see the relationship between the PG rating scores of different pages an in this iterative approach or power approach we simply start with randomly initialized the vector P
#c17	you;the relationship;the PG rating scores;different pages;this iterative approach or power approach;we;the vector P
#s18	and then we repeatedly just updated this P by multiplying the metrics here by this P vector
#c18	we;this P;the metrics;this P vector
#s19	so i also show a concrete are example here so you can see this now if we assume are far is point two then with the example that we show here on this slide we have the original transition matrix here by that in croel that encodes the graph the actual links and we have this is smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with the linear interpolation to form another metrics that would be like this so essentially we can imagine now the web it looks like this can be captured by that there are virtual links between all the pages now so the page ranking algorithm would just initialize the P vector first and then just computed the updating of this P vector by using this metrics multiplication now if you rewrite this metrics model multiplication in terms of just individual questions you will see this and this is basically the updating formula for this particular page is page rank score so you can also see you if you want to compute the value of this updated score for D one you basically multiply this rule right by this column and we take the dot product of the two that will give us the value for this value so this is how we updated the vector we started with some initial values for these guys for for this
#c19	i;a concrete;example;you;we;point;the example;we;this slide;we;the original transition matrix;croel;the graph;the actual links;we;transition matrix uniform transition matrix;random jumping;we;them;the linear interpolation;another metrics;we;the web;it;virtual links;all the pages;the page;algorithm;the P vector;the updating;this P vector;this metrics multiplication;you;this metrics model multiplication;terms;just individual questions;you;the updating formula;this particular page;page rank score;you;you;you;the value;this updated score;D one;you;this rule;this column;we;the dot product;us;the value;this value;we;the vector;we;some initial values;these guys
#s20	and then we just to revise the scores which generate a new set of scores and the updating formula is this one so we just repeatedly apply this and here it converged and when the metrics is like this where there's no zero values and it can be guaranteed to converge and at that point that we were just have the page rank scores for all the pages now we typically set the initial values just to one over N so interestingly this updating formula can be also interpreter as propagating scores over the graph can you see why well if you look at this formula and then compare that with this graph and can you imagine how we might be able to interpret this as essentially propagating scores over the graph i hope you will see that indeed that we can imagine we have values initializer on each of these pages so we can have values here let's say that's one over four for each and then we're going to use this metrics to update this this calls
#c20	we;the scores;a new set;scores;the updating formula;this one;we;it;the metrics;no zero values;it;that point;we;the page rank scores;all the pages;we;the initial values;N;this updating formula;scores;the graph;you;you;this formula;this graph;you;we;scores;the graph;i;you;we;we;values;these pages;we;values;'s;we;this metrics
#s21	and if you look at the equation here this one basically we're going to combine the scores of the pages that possibly would lead through reaching this page so will look at all the pages that are pointing to this page and then combine their schools and propagate the score some of the scores to this document D one
#c21	you;the equation;we;the scores;the pages;this page;all the pages;this page;their schools;the score;the scores;this document
#s22	so we look at the scores that represent the probability that the random server would be visiting the other pages before it reaches the D one
#c22	we;the scores;the probability;the random server;the other pages;it
#s23	and then just do the propagation to simulate the probability of reaching this page D one
#c23	the propagation;the probability;this page;D
#s24	so there are two interpretations one is just the metrics multiplication repeated multiply vector by this matrix the other is to just think of it as propagating the scores repeatedly on the web so in practice the computation of pagerank score is actually efficient becaus the metrics is a sparse and there are some ways we transform the question so that you avoid actually literally computing the values for all those elements sometimes you may also normalize equation and that would give you a somewhat different form of the equation
#c24	two interpretations;one;this matrix;it;the scores;the web;practice;the computation;pagerank score;actually efficient becaus;the metrics;a sparse;some ways;we;the question;you;the values;all those elements;you;equation;you;a somewhat different form;the equation
#s25	but then the ranking of pages will not change the results of this potential problem of zero out link problem in that case if the a page does not have any out link then the probability of these pages would not something that basically the probability of reaching the next page from this page will not someone mainly because we have lost some probability mass when we assume there's some probability that the server will try to follow links
#c25	the ranking;pages;the results;this potential problem;zero out link problem;that case;the a page;any out link;the probability;these pages;basically the probability;the next page;this page;someone;we;some probability mass;we;some probability;the server;links
#s26	but then there's no link follow and one possible solution is simply to use a PP specific attempting factor and that could easy to fix this basically that's to say awful would be one point zero four page with no out linger in that case the server with just have to randomly jump through another page instead of trying to follow a link so there are many extensions of page rank one extension is topic specific page rank noted that page rank it doesn't really use the query information so we can make page rank query specific however so for example in the topic specific page rank we can simply assume when the server is board the server is not going to randomly jump to any page on the web instead it's going to jump to only those pages that are relevant to a query for example if the queries about the sports then we could assume that when it's doing random jumping is going to randomly jump to a sports page by doing this then we can bias in the page rank to topic line sports and then if you know the current query is about disposal anyone use this specialized for page rank score to wrangle documents that would be better than if you use the generic page rank so page rank is also a general algorithm that can be used in many other applications for network analysis particular example of social networks you can imagine if you compute the page rank scores for social network where link might indicate a friendship relation you will get some meaningful scores for people
#c26	no link follow;one possible solution;a PP specific attempting factor;one point zero four page;no out linger;that case;the server;another page;a link;many extensions;page rank;one extension;topic specific page rank;it;the query information;we;page rank query;example;the topic specific page rank;we;the server;board;the server;any page;the web;it;only those pages;a query;example;the queries;the sports;we;it;random jumping;a sports page;we;the page rank;topic line sports;you;the current query;disposal;anyone;this specialized;page rank score;documents;you;the generic page rank;page rank;a general algorithm;many other applications;network analysis;particular example;social networks;you;you;the page rank scores;social network;link;a friendship relation;you;some meaningful scores;people
410	68b9ad8e-db63-49c2-b060-46046c7dfac0	39
#s1	this latter is about the implementation of text retrieval systems in this lecture we will discuss how we can implement a text retrieval method to build a search engine the main channing is to manage a lot of the test data and to enable a query to be answered very quickly and to respond too many queries this is a typical text retrieval system architecture we can see the documents are first processor by a tokenizer to get tokenized units for example words and then these words or tokens will be processed by a indexer that would create index which is a data structure for the search engine to use to quickly ends or query and the query will be going through a similar process step so that organizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other an the queries representation would then be given to the scorer which would use index too quickly and so users query by scoring the documents and then ranking them results will be given to the user and then the user can look at the results and provide some feedback that can be expressed with judgments about which documents are good which documents are bad or implicit feedback such as click slows so user doesn't have to do any anything extra the user would just look at the results an skip some and click on some results to view so these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skip the ones so a search engine system then can be divided into three parts the first part is the indexer and the second part is a scorer that response to the users query in the third parties of feedback mechanism now typically the index is down in the offline manner so you can preprocess the collected data and to build the inverted index which will introduce in a moment and this data structure can then be used by the online module which is a scorer two processor users query dynamically quickly generate search results the feedback mechanism can be done online or offline depending on the method the implementation of the index and the score is a fairly standard and this is the main topic of this lecture and the next few lectures the feedback mechanism on the other hand has variations it depends on which method is used so that is usually down in algorithm specific
#c1	this latter;the implementation;text retrieval systems;this lecture;we;we;a text retrieval method;a search engine;the main channing;a lot;the test data;a query;too many queries;a typical text retrieval system architecture;we;the documents;first processor;a tokenizer;tokenized units;example words;these words;tokens;a indexer;index;a data structure;the search engine;the query;a similar process step;that organizer;the query;the text;the same way;the same units;an the queries representation;the scorer;index;users;the documents;them;results;the user;the user;the results;some feedback;judgments;documents;documents;click;user;any anything;the user;the results;an skip;some results;these interaction signals;the system;the ranking accuracy;the view;the documents;the skip;the ones;a search engine system;three parts;the first part;the indexer;the second part;a scorer;response;the users query;the third parties;feedback mechanism;the index;the offline manner;you;the collected data;the inverted index;a moment;this data structure;the online module;a scorer two processor users query;search results;the feedback mechanism;the method;the implementation;the index;the score;the main topic;this lecture;the next few lectures;the feedback mechanism;the other hand;variations;it;which method
#s2	away let's first talk about that at night position is the normalized lexical units into the same form so that semantically similar words can be matched with each other our in the language like english stemming is often use
#c2	's;night position;the normalized lexical units;the same form;semantically similar words;our;the language;english stemming
#s3	and this is what map all the inflectional forms of words into the same root form so for example computer computation in computing can all be matched to root form compute this way all these different forms of computing can be matched with each other and normally this is good idea to increase the coverage of documents that are matched with this query
#c3	all the inflectional forms;words;the same root form;example;computer computation;computing;root form compute;all these different forms;computing;good idea;the coverage;documents;this query
#s4	but it's also not always beneficial because sometimes the settle this difference between computer and computation might still suggest that the difference in the coverage of the content but in most cases stemming since will be beneficial when we tokenize the texting some other languages for example
#c4	it;the settle;this difference;computer;computation;the coverage;the content;most cases;we;the texting;some other languages;example
#s5	chinese we might face some special challenges in segmenting the text to find the water boundaries because it's not obvious where the boundary is as there's no space to separate them so here of course we have to use some languages specifically natural language processing techniques once we do tokenization then we would index the text documents and that is it'll convert the documents into some there are structure that can enable fast search the basic idea is to precompute as much as we can basically so the most commonly used indexes called inverted index and this has been used to in many search engines to support basically search algorithms sometimes other indexes for example a document in that might be needed in order to support feedback like i said and this this kind of techniques are not really standard in that they vary a lot according to feedback methods to understand why we want to use invert index it would be useful for you to think about how you would respond to a single term query quickly so if you want to use more time to think about that post the video so think about how you can preprocess the text there are so that you can quickly responded to a query with just one word well if you have thought about that question you might realize that or the best is to simply create a list of documents that match every term in the vocabulary in this way you can basically preconstructed benzos so when you see your term you can simply just fetch the random list of documents for that term every turn the disk to the user
#c5	we;some special challenges;the text;the water boundaries;it;the boundary;no space;them;course;we;some languages;specifically natural language processing techniques;we;tokenization;we;the text documents;it;the documents;structure;the basic idea;we;so the most commonly used indexes;inverted index;many search engines;algorithms;example;a document;order;feedback;i;this kind;techniques;they;feedback methods;we;invert index;it;you;you;a single term query;you;more time;the video;you;the text;you;a query;just one word;you;that question;you;a list;documents;every term;the vocabulary;this way;you;benzos;you;your term;you;the random list;documents;that term;the disk;the user
#s6	so that's the fastest way to respond to a single term query now the idea of the inverted index is actually basically like that we can do pre construct the such index that would allow us to quickly find the older documents that match a particular term
#c6	the fastest way;a single term query;the idea;the inverted index;we;the such index;us;the older documents;a particular term
#s7	so let's take a look at this example we have three documents here and these are the documents that you have seen in some previous lectures suppose we want to create invert index for these documents then we would maintain a dictionary in the dictionary will have one entry for each term and we're going to store some basically statistics about the term for example the number of documents that match the term or the total number of total frequency over the term which means we would counter duplicated occurrences of the term and so for example news this term occured in all the three documents so the count of documents is three
#c7	's;a look;this example;we;three documents;the documents;you;some previous lectures;we;invert index;these documents;we;a dictionary;the dictionary;one entry;each term;we;some basically statistics;the term;example;the number;documents;the term;the total number;total frequency;the term;we;duplicated occurrences;the term;example;this term;all the three documents;the count;documents
#s8	and you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model can you think of that
#c8	you;we;this count;documents;document frequency;some statistics;the vector space model;you
#s9	so what weighting heuristic would need this count well that's the idea right inverse document frequency
#c9	what;this count;the idea right inverse document frequency
#s10	so IDF is the property of the term and we can compute it right here so with the document that count here it's easy to compute the idea of either at this time when we build an index or running time when we see your query now in addition to these basically statistics we also store all the documents that match the news and these entries are store in fire called postings so in this case it meant three documents and store information about these three documents here this is the document ID document of one and the frequency is one the TF is one four news in the second document it's also well etc so from this list that we can get all the documents that match the term news and we can also know the frequency of news in these documents
#c10	IDF;the property;the term;we;it;the document;it;the idea;this time;we;an index;we;your query;addition;these basically statistics;we;all the documents;the news;these entries;store;fire;postings;this case;it;three documents;store information;these three documents;the document ID document;the frequency;the TF;one four news;the second document;it;this list;we;all the documents;the term news;we;the frequency;news;these documents
#s11	so if the query has just the one world news and we are easily look up this table to find the entry and go quickly through the postings perfect all the documents that matching yours so let's take a look at another term this time let's take a look at the world presidential this war occured in only one document document three so the document frequency is one
#c11	the query;just the one world news;we;this table;the entry;the postings;all the documents;yours;'s;a look;another term;'s;a look;the world;this war;only one document document;the document frequency
#s12	but it occur to twice in this document so the frequency count is two an the frequency com is useful in some other retrieval method where we might use the frequency to assess the popularity of term in the collection and similarly will have a pointer to the postings here and in this case there is only one entry here becaus the term according just one document
#c12	it;this document;the frequency count;an the frequency com;some other retrieval method;we;the frequency;the popularity;term;the collection;a pointer;the postings;this case;only one entry;the term;just one document
#s13	and that's here i talking ID is three
#c13	i;ID
#s14	an it could toys so this is the basic idea of inverted index it's actually pretty simple right with this structure we can easily fetch all the documents that match it on and this will be the basis for scoring documents for query now sometimes we also want to store the positions of these tones so in many of these cases the term occur just once in the document so there's only one position for example in this case but in this case the term occur twice so we would store two positions now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say five words or ten words or whether the matching of the two query terms is in fact a phrase of towards that this can we check the quickly by using the position information so why is inverted index good for faster search
#c14	an it;the basic idea;inverted index;it;this structure;we;all the documents;it;the basis;documents;query;we;the positions;these tones;these cases;the term;the document;only one position;example;this case;this case;the term;we;two positions;the position information;the matching;query terms;a small window;'s;five words;ten words;the matching;the two query terms;fact;a phrase;we;the position information;faster search
#s15	well we just talked about the possibility of using it to ends or single water clearly and that's very easy what about the multiple term queries well let's first look at the some special cases over the boolean query a boolean query is basically boolean expression like this
#c15	we;the possibility;it;the multiple term;'s;the some special cases;the boolean query;a boolean query;basically boolean expression
#s16	so i want the relative in the document to match both term A and term be so that's one conjunctive query
#c16	i;the relative;the document;both term A;term;one conjunctive query
#s17	or i want the relevant documents to match term A or B that's a disadvantage query how can we answer such a query by using vert index
#c17	i;the relevant documents;term A;B;a disadvantage query;we;such a query;vert index
#s18	well if you think a bit about it it would be obvious it cause we had simply fetch all the documents that matched ma an also fetch all the documents that match tom be and then just to take the intersection to answer query like A N B or to take the union to answer the query A or B
#c18	you;it;it;it;we;all the documents;ma;all the documents;tom;the intersection;query;A N B;the union;the query;A;B
#s19	so this is all very easy to when it's going to be very quick now what about the multi term keyword query we talked about vectors based model for example and we would match such query with document and generative score and the score is based on aggregated term weights so in this case it's not a boolean query but the scoring can be acted out in a similar way basically it's similar to disjunctive pony and query basically it's like a RV we take the union of all the documents that match it at least one query term
#c19	it;the multi term keyword query;we;vectors;model;example;we;such query;document;generative score;the score;aggregated term weights;this case;it;a boolean query;the scoring;a similar way;it;disjunctive pony;query;it;a RV;we;the union;all the documents;it;at least one query term
#s20	and then we would aggregate the term weights so this is basic idea of using that index for scoring documents in general
#c20	we;the term;weights;basic idea;that index;documents
#s21	and we're going to talk about this in more detail later but for now let's just look at the question why is inverted index a good idea basically why is it more efficiently than sequential register scanning documents this is obvious approach you can just compute the score for each document and then you can scroll them sorry you can then sort them this is a straightforward method but this is going to be very slow imagine the web it has a lot of documents if you do this then it will take a long time to answer your query so the question now is why would the in the inverted index we've much faster than it has to do with the water distribution in text
#c21	we;more detail;'s;the question;inverted index;a good idea;it;sequential register;documents;obvious approach;you;the score;each document;you;them;you;them;a straightforward method;the web;it;a lot;documents;you;it;a long time;your query;the question;the inverted index;we;it;the water distribution;text
#s22	so here's some common phenomenon of water distribution in text there are some language independent the patterns that seem to be stable and these patterns are basically characterized by the following pattern a few words like the common words another
#c22	some common phenomenon;water distribution;text;some language;the patterns;these patterns;the following pattern;the common words
#s23	or we occur very very frequently in text so they account for a large extent of occurrences of words
#c23	we;text;they;a large extent;occurrences;words
#s24	but most are words would occur just rarely there are many words that occur just once let's say in the document or once in a collection there are many such singletons it's also true that the most frequent the words in one coppers layer to be raring another that means although the general phenomenon is applicable or is observed in many cases exactly words that are common may vary from context to context so this phenomena is characterized by what's called a zip of slaw this law says that the rank of word multiplied by the frequency of the world is roughly a constant so formally if we use F of WT node the frequency of W D noted the rank of world then this is the formula it basically says the same thing just mathematical term we'll see is basically a constant
#c24	words;many words;'s;the document;a collection;many such singletons;it;the most frequent the words;one coppers layer;the general phenomenon;many cases;exactly words;context;this phenomena;what;a zip;slaw;this law;the rank;word;the frequency;the world;we;F;WT;node;the frequency;W D;the rank;world;the formula;it;the same thing;just mathematical term;we
#s25	so 's so there is also parameter over that might be adjusted to better fit any empirical observations
#c25	parameter;any empirical observations
#s26	so if i plot the word frequencies in sorted order now you can see this more easily the X axis is basically the world rank and this is all of W and Y axis is a word frequency or F of W now this curve with shows that the product of the two is roughly the consonant now if you look at these words that we can see they can be separated into three groups in the middle it's the intermediate frequency words these words tend to occur in quite a few documents
#c26	i;the word frequencies;sorted order;you;the X axis;the world rank;W and Y axis;a word frequency;F;W;this curve;shows;the product;you;these words;we;they;three groups;the middle;it;the intermediate frequency words;these words;quite a few documents
#s27	but they're not like those most frequent awards
#c27	they;those most frequent awards
#s28	and they also not very rare so they tend to be often used in queries
#c28	they;they;queries
#s29	and they also tend to have high TF IDF whites these intermediate frequently words but if you look at the left path of the curve these are the highest frequency words they occur very frequently they are usually stopovers because the we of etc those words are very very frequently there are in fact the tool frequently to be discriminate him and they are generally not very useful for for retrieval so they are often removed and this is gotta stop awards removal so you can use pretty much just the counter words in the collection to kind of infer what words might be stopped with those are basically the heister frequency words and they also occupy a lot of space in the inverted index you can imagine the posting entries for such a world would be very long and therefore if you can remove such words you can save a lot of space in the inverted index we also show the tear apart which has a lot of rare words those was long a covered frequently and there are many such words those words are actually very useful for search also if a user happens to be interested in such a topic but be cause they're rare it's often true that the users are unnecessary interest in those words but retain them allow us to match such a document accurate and they generally have very high ID FS
#c29	they;high TF IDF whites;these intermediate frequently words;you;the left path;the curve;the highest frequency words;they;they;the we;those words;fact;the tool;him;they;retrieval;they;awards removal;you;just the counter words;the collection;what words;the heister frequency words;they;a lot;space;the inverted index;you;the posting entries;such a world;you;such words;you;a lot;space;the inverted index;we;the tear;a lot;rare words;many such words;those words;search;a user;such a topic;they;it;the users;unnecessary interest;those words;them;us;such a document;they;very high ID FS
#s30	so what kind of data structures should we use to slow inverted index
#c30	what kind;data structures;we;inverted index
#s31	well it has two parts right if you recall we have a dictionary
#c31	it;two parts;you;we
#s32	and we also have postings the dictionary has mostly size although for the web it's still going to be very large but computer was postings it's modest and we also need to have fast random access to the entries 'cause we want to look up with the query term very quickly so therefore we prefer to keep such a dictionary in memory if it's possible or if the connection is not very large this is visible but the connection is very large then it's in general not possible with the vocabulary size is very large obviously we can't do that so in general that's our goal so the data structures that we often used for storing dictionary it would be directly access data structures like hash table or P G if we can't store everything in memory we can use disco and
#c32	we;postings;the dictionary;mostly size;the web;it;computer;postings;it;we;fast random access;the entries;we;the query term;we;such a dictionary;memory;it;the connection;the connection;it;the vocabulary size;we;our goal;we;dictionary;it;directly access data structures;hash table;P G;we;everything;memory;we;disco
#s33	but they try to build a structure that would allow you to quickly look up her entries i for postings they are huge an in general we don't have to have direct access to specific entry we generate with just look up a sequence of document i desana frequencies for all the documents that match it or
#c33	they;a structure;you;her entries;i;postings;they;we;direct access;specific entry;we;a sequence;document;i;frequencies;all the documents;it
#s34	query term
#c34	query term
#s35	so we were the read those entries sequential it and therefore be'cause its large and we generate store postings on disk so they have to stay on this an they would contain information such as document IDS tone frequencies or compositions etc now because they are very large compression is often desirable now this is not only to save disk space
#c35	we;the read;those entries;it;we;store postings;disk;they;they;information;document IDS tone frequencies;compositions;they;very large compression;disk space
#s36	and this is of course one benefit of compression it's not an occupied that much space
#c36	course;one benefit;compression;it;an occupied that much space
#s37	but it's also to help her improving speed can you see why well we know that input and output with cost a lot of time in comparison with time taken by CPU so CPU is much faster
#c37	it;her;speed;you;we;that input;output;cost;a lot;time;comparison;time;CPU;CPU
#s38	but i owe takes time so by compressing the inverted index the pulsing files will become smaller and the entries that we have to read into memory to process a query time with would be smaller and then so we can reduce the amount of traffic in I O
#c38	i;time;the inverted index;the pulsing files;the entries;we;memory;a query time;we;the amount;traffic;I
#s39	and that can save a lot of time of course we have to then do more processing of the data when we uncompressed the data in the memory but as i say the CPU is fast so overall we can still save time so compression here is both a safe distance space and to speed up the loading of number three index
#c39	a lot;time;course;we;more processing;the data;we;the data;the memory;i;the CPU;we;time;compression;both a safe distance space;the loading;number three index
410	6962b043-7dd8-4050-bad0-bbdb13e2c302	235
#s1	This lecture is about how to use generative probabilistic models for text categorization.
#c1	This lecture;generative probabilistic models;text categorization
#s2	There are in general are two kinds of approaches to text categorization by using machine learning.
#c2	two kinds;approaches;categorization;machine learning
#s3	One is generative probabilistic models, the other is discriminative approaches.
#c3	generative probabilistic models;discriminative approaches
#s4	In this lecture, we're going to talk about the generative models.
#c4	this lecture;we;the generative models
#s5	In the next lecture, we're going to talk about discriminative approaches.
#c5	the next lecture;we;discriminative approaches
#s6	So the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster.
#c6	the problem;text categorization;we;each document;one category;one cluster
#s7	Main difference is that in clustering we don't really know what are the predefined categories or what are the clusters.
#c7	Main difference;clustering;we;what;the predefined categories;what;the clusters
#s8	In fact, that's the goal of text clustering.
#c8	fact;the goal;text clustering
#s9	We want to find such clusters in the data.
#c9	We;such clusters;the data
#s10	But in the case of categorization, we are given the categories.
#c10	the case;categorization;we;the categories
#s11	So we kind of have predefined categories and.
#c11	we;categories
#s12	then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories.
#c12	these categories;training data;we;a document;these categories;sometimes multiple categories
#s13	But because of the similarity of the two problems, we can actually adapt document clustering models for text categorization.
#c13	the similarity;the two problems;we;document clustering models;text categorization
#s14	Or we can understand how we can use generative models to do text categorization from the perspective of clustering.
#c14	we;we;generative models;text categorization;the perspective;clustering
#s15	And so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions.
#c15	a slide;we;text clustering;we;multiple topics;word distributions
#s16	Each topic is 1 cluster.
#c16	Each topic;1 cluster
#s17	So once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this question boils down to decide which theta i has been used to generate D. Suppose D has L words represent represent as Xi here.
#c17	we;such model;we;the problem;which cluster document;this question;which theta;i;D. Suppose D;L words;represent;Xi
#s18	Now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document?
#c18	you;the probability;i;this document
#s19	In general, we use bayes rule to make this inference.
#c19	we;bayes;rule;this inference
#s20	And you can see this Prior information here.
#c20	you;this Prior information
#s21	That we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster, so we should favor such a cluster.
#c21	we;a topic;cluster;it;the document;this cluster;we;such a cluster
#s22	The other is a likelihood part, that is this part.
#c22	a likelihood part;this part
#s23	And this has to do with whether the topic word distribution can explain the content of this document well.
#c23	the topic word distribution;the content;this document
#s24	And we want to pick a topic that's high by both values.
#c24	we;a topic;both values
#s25	So more specifically, we just multiply them together and then choose which topic has the highest product.
#c25	we;them;which topic;the highest product
#s26	So more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document.
#c26	what;we;we;the topic;this posterior probability;the topic;the document
#s27	Get posterior becausw this one P of Theta i is the prior, that's our belief about which topic is more likely.
#c27	this one P;Theta;i;our belief;which topic
#s28	Before we observe any document.
#c28	we;any document
#s29	But this conditional probability here Is the posterior probability of the topic after we have observed the document d. And Bayes rule allows us to update this probability based on the prior and I shown the details.
#c29	this conditional probability;the posterior probability;the topic;we;the document d.;Bayes rule;us;this probability;the prior;I;the details
#s30	Below here you can see how the prior here is related to the posterior on the left hand side.
#c30	you;the posterior;the left hand side
#s31	And this is related to how well this word distribution explains the document here, and the two are related in this way.
#c31	this word distribution;the document;this way
#s32	So to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course.
#c32	the topic;the highest posterior probability;it;this product;we;this course
#s33	An we can then change the probability of document in your product of the probability of each word and that's just because we've made the assumption about the independence in generating each word OK.
#c33	we;the probability;document;your product;the probability;each word;we;the assumption;the independence;each word
#s34	So this is just something that you have seen in document clustering.
#c34	just something;you;document clustering
#s35	An we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories.
#c35	we;we;a documentary;a category;the information;word distributions;these categories;these categories
#s36	So this idea can be directly adapted to do categorization and This is precisely what Naive Bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if Theta i represents category I accurately that means the word distribution characterizes the content of documents in category i accurately.
#c36	this idea;categorization;precisely what;Naive Bayes classifier;it;the same information;we;the categorization problem;we;i;category;I;the word distribution;the content;documents;category;i
#s37	Then what we can do is precisely like what we did for text clustering.
#c37	what;we;what;we;text clustering
#s38	Namely, we are going to assign document D to the category that has the highest probability of generating this document.
#c38	we;document D;the category;the highest probability;this document
#s39	In other words, we're going to maximize this posterior probability as well.
#c39	other words;we;this posterior probability
#s40	And this is related to the prior and the likelihood an as you have seen on the previous slide.
#c40	the likelihood;you;the previous slide
#s41	And so naturally, we can then decompose this likelihood into a product.
#c41	we;this likelihood;a product
#s42	As you see here now here I changed the notation so that we will write down the product as product over all the words in the vocabulary and even if even though the document doesn't contain all the words and the product is there accurately representing the product of all the words in the document.
#c42	you;I;the notation;we;the product;product;all the words;the vocabulary;the document;all the words;the product;the product;all the words;the document
#s43	because of this count here.
#c43	this count
#s44	when a word doesn't occur in the document.
#c44	a word;the document
#s45	The count would be 0, so this count would just disappear.
#c45	The count;this count
#s46	So effectively we're just have the product over all the words in the document.
#c46	we;the product;all the words;the document
#s47	So basically with naive Bayes classifier, we're going to score each category for a document by this function.
#c47	naive Bayes classifier;we;each category;a document;this function
#s48	Now you may notice that here It involves the product of a lot of small probabilities and this can cause underflow problem.
#c48	you;It;the product;a lot;small probabilities;underflow problem
#s49	So one way to solve the problem is to take logarithm of this function, which doesn't change the order of these categories, but would help us preserve precision and so this is often the.
#c49	one way;the problem;logarithm;this function;the order;these categories;us;precision
#s50	This is often the function that we actually use to score each category, and then we're going to choose the category that has the highest score by this function.
#c50	the function;we;each category;we;the category;the highest score;this function
#s51	So this is called a Naiyes Bayes classifier.
#c51	a Naiyes Bayes classifier
#s52	Now the keyword Bayes is understandable because we are applying a Bayes rule here.
#c52	the keyword Bayes;we;a Bayes rule
#s53	When we go from the posterior probability of the topic to a product of the likelihood and the prior.
#c53	we;the posterior probability;the topic;a product;the likelihood;the prior
#s54	Now it's also called a Naive because We've made an assumption that every word in the document is generated independently, and this is indeed a naive assumption, because in reality they are not generated independently.
#c54	it;a Naive;We;an assumption;every word;the document;a naive assumption;reality;they
#s55	Once you see some word and other words will more likely occur.
#c55	you;some word;other words
#s56	For example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear And if you have not seen text.
#c56	example;you;a word;a text;it;categorization;you;text
#s57	But this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks.
#c57	this assumption;us;the problem;it;many text categorization tasks
#s58	But you should know that this kind of model doesn't have to make this assumption.
#c58	you;this kind;model;this assumption
#s59	We could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model.
#c59	We;example;the words;it
#s60	And of course you can even use a mixture model to model what the document looks like in each category.
#c60	course;you;a mixture model;what;the document;each category
#s61	So in nature they will be all using Bayes rule to do classification, but the actual generative model for documents in each category.
#c61	nature;they;Bayes rule;classification;the actual generative model;documents;each category
#s62	Can vary, and here we just talk about a very simple case.
#c62	we;a very simple case
#s63	Perhaps the simplest case.
#c63	Perhaps the simplest case
#s64	So now the question is, how can we make sure each theta i actually represents category i accurate?
#c64	the question;we;i;category;i
#s65	Now, in clustering we learned this category i or the word distributions for category i from the data.
#c65	clustering;we;this category;i;the word distributions;category;i;the data
#s66	But in our case what can we do to make sure this theta i represents indeed category i?
#c66	our case;what;we;this theta;i;i
#s67	If you think about the question and you're likely to come up with the idea of using the training data right.
#c67	you;the question;you;the idea;the training data
#s68	Indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category.
#c68	text categorization;we;training data;the documents;which category
#s69	In other words, these are the documents with known categories assigned, and of course human experts must do that.
#c69	other words;the documents;known categories;course;human experts
#s70	And here you see that T1 represents the set of documents that are known to have been generated from category one, and T2 represents the documents that are known to have been generated from category two, etc.
#c70	you;T1;the set;documents;category;T2;the documents;category
#s71	Now if you look at this picture, you see that the model here is really a simplified unigram language model.
#c71	you;this picture;you;the model;a simplified unigram language model
#s72	It is no longer mixture model.
#c72	It;mixture model
#s73	Why?
#s74	Because already know which distribution has been used to generate which documents.
#c74	which distribution;which documents
#s75	There's no uncertainty here.
#c75	no uncertainty
#s76	There's no mixing of different categories here.
#c76	no mixing;different categories
#s77	So the estimation problem of course would be simplified, but in general you can imagine what we want to do is to estimate these probabilities that I marked here and what are the probabilities that we have to estimate in order to do categorization where there are two kinds.
#c77	the estimation problem;course;you;what;we;these probabilities;I;what;the probabilities;we;order;categorization;two kinds
#s78	So one is the prior.
#c78	one;the prior
#s79	The probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category.
#c79	The probability;theta;i;each category;we;the document;that category
#s80	The other kind is word distributions and we want to know what words have high probabilities for each category.
#c80	The other kind;word distributions;we;what words;high probabilities;each category
#s81	So the idea then is to just use the observed training data to estimate these two probabilities.
#c81	the idea;the observed training data;these two probabilities
#s82	And in general we can do this separately for different categories.
#c82	we;different categories
#s83	That's just because these documents are known to be generated from a specific category, so once we know that it's in some sense irrelevant what other categories we are also dealing with.
#c83	these documents;a specific category;we;it;some sense;what other categories;we
#s84	So now this is statistical estimation problem.
#c84	statistical estimation problem
#s85	We have observed some data from some model and we want to guess the parameters of this model.
#c85	We;some data;some model;we;the parameters;this model
#s86	We want to take our best guess of the parameters.
#c86	We;our best guess;the parameters
#s87	And this is the problem that you have seen.
#c87	the problem;you
#s88	Also several times in this course.
#c88	Also several times;this course
#s89	Now, if you haven't thought about that this problem, haven't seen  naive Bayes classifier, it would be very useful for you to pause the video for a moment and to think about how to solve this problem.
#c89	you;this problem;  naive Bayes classifier;it;you;the video;a moment;this problem
#s90	So let me state the problem again, so let's just think about category One.
#c90	me;the problem;'s;category
#s91	We know there is one word distribution that has been used to generate documents.
#c91	We;one word distribution;documents
#s92	And we generated each word in the document independently and we know that we have observed the set of N sub one documents in the set of T1.
#c92	we;each word;the document;we;we;the set;N;one documents;the set;T1
#s93	These documents have been all generated from category one, namely have been all generated using this same word distribution.
#c93	These documents;this same word distribution
#s94	Now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category?
#c94	the question;what;your guess;estimate;the probability;each word;this distribution;what;your guess;the prior probability;this category
#s95	Of course, this second probability depends on how likely that you will see documents in other categories.
#c95	this second probability;you;documents;other categories
#s96	Right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these K categories.
#c96	a moment;you;all these training data;all these documents;these K categories
#s97	To estimate all these parameters.
#c97	all these parameters
#s98	Now if you spend some time to think about this and it would help you understand the following few slides.
#c98	you;some time;it;you;the following few slides
#s99	So do spend some time to make sure that you can try to solve this problem or do your best to solve the problem yourself.
#c99	some time;you;this problem;the problem
#s100	Now, if you have thought about it and then you will realize the following intuition.
#c100	you;it;you;the following intuition
#s101	First, what's the basis for estimating the prior or the probability of each category?
#c101	what;the basis;the probability;each category
#s102	Well, this has to do with whether you have observed a lot of documents from that category.
#c102	you;a lot;documents;that category
#s103	Intuitively, if you have seen a lot of documents in sports and very few in medical science, then your guess is that the probability of sports category is larger or your prior on the category would be larger.
#c103	you;a lot;documents;sports;medical science;your guess;the probability;sports category;your prior;the category
#s104	And what about the basis for estimating the probability of word in each category?
#c104	the basis;the probability;word;each category
#s105	Well, the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category.
#c105	you;that words;the documents;a category
#s106	will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do.
#c106	higher probability;just the maximum likelihood estimator;what;we
#s107	So to estimate the probability of each category.
#c107	the probability;each category
#s108	And to answer the question which category is most popular, then we can simply normalize the count of documents in each category.
#c108	the question;which category;we;the count;documents;each category
#s109	So here you see n sub I denotes the number of documents in each category.
#c109	you;I;the number;documents;each category
#s110	And we simply just normalize this count to make this a probability.
#c110	we;this count
#s111	In other words, we make this probability proportional to the size of training dataset in each category.
#c111	other words;we;this probability;the size;training;each category
#s112	That's the size of the set T sub i. Now, what about the word distribution?
#c112	the size;the set T sub i.;the word distribution
#s113	Well, we do the same again.
#c113	we
#s114	This time we can do this for each category.
#c114	we;each category
#s115	So let's say we are considering category I or Theta I.
#c115	's;we;category I;Theta I.
#s116	So which word has higher probability?
#c116	which word;higher probability
#s117	Well, we simply count the word occurrences in the documents that are known to be generated from theta i.
#c117	we;the word;occurrences;the documents;theta i.
#s118	And then we put together all the all the counts of the same word in this set.
#c118	we;all the all the counts;the same word;this set
#s119	And then we just normalize these counts to make this distribution of all the words make all the probabilities of all these words sum to one.
#c119	we;these counts;this distribution;all the words;all the probabilities;all these words
#s120	So in this case you can see this is a proportional to the count of the word in the collection of training documents.
#c120	this case;you;a proportional;the count;the word;the collection;training documents
#s121	T sub I
#c121	T sub
#s122	and that's denoted by C of w and T sub I.
#c122	C;w;T;sub;I.
#s123	Now you may notice that we often write down a probability estimate in the form of being proportional to certain number, and this is often sufficient.
#c123	you;we;a probability estimate;the form;certain number
#s124	Becausw we have some constraints on these distributions and so the normalizer is dictated by the constraint.
#c124	we;some constraints;these distributions;the normalizer;the constraint
#s125	So in this case it will be useful for you to think about what are the constraints on these two kinds of probabilities.
#c125	this case;it;you;what;the constraints;these two kinds;probabilities
#s126	So once you figure out the answer to this question and you will know how to normalize, this counts and so this is a good exercise to work on it if it's not obvious to you.
#c126	you;the answer;this question;you;this counts;a good exercise;it;it;you
#s127	There is another issue in Naive Bayes which is a smoothing.
#c127	another issue;Naive Bayes;a smoothing
#s128	In fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data.
#c128	fact;the smoothing;a general problem;all the estimate;language models;what;you;a small amount;data
#s129	So smoothing is the important technique to address data sparseness.
#c129	smoothing;the important technique;data sparseness
#s130	In our case the training data set can be small and one data set is small.
#c130	our case;the training data;one data
#s131	When we use maximum likelihood estimator we often face the problem of zero probability.
#c131	we;maximum likelihood estimator;we;the problem;zero probability
#s132	That means if the event is not observed.
#c132	the event
#s133	Then the estimated probability would be 0 in this case if we have not seen a word in the training documents for, let's say, category I, then our estimate would be 0 for the probability of this word in this category.
#c133	the estimated probability;this case;we;a word;the training documents;'s;our estimate;the probability;this word;this category
#s134	And this is generally not accurate.
#s135	So we have to do smoothing to make sure it's not zero probability.
#c135	we;smoothing;it;zero probability
#s136	The other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing.
#c136	The other reason;smoothing;a way;prior knowledge;a lot;situations;smoothing
#s137	When the data set is small, we tend to rely on some prior knowledge to to solve the problem.
#c137	the data;we;some prior knowledge;the problem
#s138	So in this case our prior knowledge  says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability.
#c138	this case;our prior knowledge;no words;zero probability;smoothing;us;no word;a zero probability
#s139	There is also a third reason, which is sometimes not very obvious, but we'll explain that in a moment and that is to help achieve discriminative weighting of terms.
#c139	a third reason;we;a moment;discriminative weighting;terms
#s140	And this is also called IDF weighting inverse document frequency weighting that you have seen in mining word relations.
#c140	IDF weighting;inverse document frequency;weighting;you;mining word relations
#s141	So how do we do smoothing?
#c141	we
#s142	Well in general we added pseudo counts to these events.
#c142	we;pseudo counts;these events
#s143	We'll make sure that no event has zero count.
#c143	We;no event;zero count
#s144	So one possible way of smoothing the probability of category is to simply add small nonnegative constant Delta to the count.
#c144	one possible way;the probability;category;small nonnegative constant Delta;the count
#s145	We pretend that every category has actually some extra number of documents represented by Delta.
#c145	We;every category;some extra number;documents;Delta
#s146	And in the denominator we also add K multiplied by Delta because we want the probability to sum to one.
#c146	the denominator;we;K;Delta;we;the probability
#s147	So in total we've added Delta K Times because we have K categories.
#c147	total;we;Delta K Times;we;K categories
#s148	Therefore in the sum we have to also add K multiplied by Delta as a total pseudo counts that we add to the estimate.
#c148	the sum;we;K;Delta;a total pseudo counts;we;the estimate
#s149	Now it's interesting to think about the influence of delta.
#c149	it;the influence;delta
#s150	Obvious Delta is a smoothing parameter here, meaning that the larger delta is and the more we will do smoothing and that means we'll more rely on pseudo counts and we might indeed ignore the actual counts if delta is set to Infinity.
#c150	Obvious Delta;a smoothing parameter;the larger delta;we;smoothing;we;pseudo counts;we;the actual counts;delta;Infinity
#s151	Imagine what would happen if delta approaches positive Infinity?
#c151	what;delta;positive Infinity
#s152	Well, we're going to say every word has infinity amount of sorry, not every word every category has.
#c152	we;every word;infinity amount;sorry, not every word;every category
#s153	infinity amount of documents, and then there's no distinction between them, so it becomes just a uniform.
#c153	infinity amount;documents;no distinction;them;it;just a uniform
#s154	What if Delta is zero?
#c154	What;Delta
#s155	Well we just go back to the original estimate based on the observed training data to estimate the probability of each category.
#c155	we;the original estimate;the observed training data;the probability;each category
#s156	Now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words.
#c156	we;the word distribution;this case;we;we;it;a non-uniform pseudo counts;the words
#s157	So here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model.
#c157	you;we;pseudocounts;each word;mu;the probability;the world;a background language model
#s158	Theta sub
#c158	Theta sub
#s159	b Now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model.
#c159	background model;a large collection;text;this case;we;the whole set;all the training data;this background language model
#s160	But if we don't have to use this one, we can use larger text data that are available from somewhere else.
#c160	we;this one;we;larger text data
#s161	Now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts.
#c161	we;such a background language model;pseudocounts;we;some words;more pseudocounts
#s162	So what are those words?
#c162	what;those words
#s163	Well those are the common words.
#c163	the common words
#s164	Because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts.
#c164	they;higher probability;the background language model;the pseudocounts;such words;rare words;the other hand;smaller pseudocounts
#s165	Now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model.
#c165	this addition;background model;nonuniform smoothing;this word distributions;we;the probability;those common words;a higher level;the background model
#s166	Now this helps make the difference of the probability of such words smaller across categories.
#c166	the difference;the probability;such words;categories
#s167	Because every category has some help from their background for words, like the, a which have high probabilities.
#c167	every category;some help;their background;words;high probabilities
#s168	Therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities.
#c168	it;each category;documents;such a lot;occurrences;such word;the estimate;the background model;the consequences;we;categorization;such words;the decision;words;small probabilities
#s169	From the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories.
#c169	the background language model;those words;some help;the background language model;the difference;the differences;the occurrences;the training documents;different categories
#s170	You also see another smoothing parameter mu here, which controls the amount of smoothing, just like delta does for the other probability.
#c170	You;another smoothing parameter mu;the amount;smoothing;delta;the other probability
#s171	And you can easily understand why we add mu to the denominator because that represents the sum of all the pseudo counts that we add for all the words.
#c171	you;we;mu;the denominator;the sum;all the pseudo counts;we;all the words
#s172	So mu is also non-negative constant and it's  empirically set to control smoothing.
#c172	mu;it;smoothing
#s173	There are some interesting special cases to think about as well.
#c173	some interesting special cases
#s174	First, let's think about when mu approaches Infinity.
#c174	's;mu;Infinity
#s175	What would happen?
#c175	What
#s176	Or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model.
#c176	this case;the estimate;the background language model;the background language model;we;every word distribution;the same background language model
#s177	And that essentially removes the difference between these categories.
#c177	the difference;these categories
#s178	Obviously we don't want to do that.
#c178	we
#s179	The other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary.
#c179	The other special cases;we;the background model;we;the two uniform distribution;'s;one;the size;the vocabulary
#s180	So each word has the same probability.
#c180	each word;the same probability
#s181	Then this smoothing formula is going to be very similar to the one on the top.
#c181	this smoothing formula;the one;the top
#s182	When we add Delta because we're going to add a constant pseudo count to every word.
#c182	we;Delta;we;a constant pseudo count;every word
#s183	So in general, in naiyes bayes categorization we have to do such smoothing and   once we have these probabilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier.
#c183	naiyes;categorization;we;such smoothing;we;these probabilities;we;the score;each category;a document;the category;the highest score;we
#s184	Now it's useful to further understand whether the naive Bayes scoring function actually makes sense, so to understand that.
#c184	it;the naive Bayes scoring function;sense
#s185	And also to understand why adding a background language model will actually achieve the effect of idea of IDF weighting and to penalize common words.
#c185	a background language model;the effect;idea;IDF weighting;common words
#s186	Right, so it's suppose we have just two categories and we're going to score based on their Ratio of probability, so this is Ann let's say this is our scoring function for two categories.
#c186	it;we;just two categories;we;their Ratio;probability;Ann;'s;our scoring function;two categories
#s187	So this is a score of a document for these two categories.
#c187	a score;a document;these two categories
#s188	And we're going to score based on this probability ratio.
#c188	we;this probability ratio
#s189	So if the ratio is larger  then it means it's more likely to be in category one, so the larger the score is, the more likely the document is in category One.
#c189	the ratio;it;it;category;the score;the document;category
#s190	So by using bayes rule we can write down this ratio as follows and you have seen this before.
#c190	bayes rule;we;this ratio;you
#s191	Now, we generally take logarithm of this ratio and to avoid small probabilities, and this would then give us this formula in the second line.
#c191	we;logarithm;this ratio;small probabilities;us;this formula;the second line
#s192	And here we see something really interesting, because this is our scoring function for deciding between the two categories.
#c192	we;something;our scoring function;the two categories
#s193	And if you look at this function, we'll see it has several parts.
#c193	you;this function;we;it;several parts
#s194	The first part here is actually log of prior probability ratio and so this is the category bias.
#c194	The first part;prior probability ratio;the category bias
#s195	So it doesn't really depend on the document, it just says which category is more likely and then would.
#c195	it;the document;it;which category
#s196	We would then favor this category slightly.
#c196	We;this category
#s197	So the second part has a sum of all the words.
#c197	the second part;a sum;all the words
#s198	Right, so these are the words that are observed in the document, but in general we can consider all the words in the vocabulary.
#c198	the words;the document;we;all the words;the vocabulary
#s199	So here we're going to collect evidence about which category is more likely.
#c199	we;evidence;which category
#s200	So inside the sum you can see there is product of two things.
#c200	the sum;you;product;two things
#s201	The first is count of the word.
#c201	count;the word
#s202	And this count of the word serves as a feature and to represent the document.
#c202	this count;the word;a feature;the document
#s203	And this is what we can collect from document.
#c203	what;we;document
#s204	The second part is the weight of this feature.
#c204	The second part;the weight;this feature
#s205	Here it's the weight on each word and this weight.
#c205	it;the weight;each word;this weight
#s206	Tells us.
#c206	us
#s207	To what extent observing this word helps contributing to our decision to put this document in Category One.
#c207	what extent;this word;our decision;this document;Category
#s208	I remember the higher the scoring function is more likely it's in category one.
#c208	I;the scoring function;it;category
#s209	Now if you look at this ratio basically or sorry this weight It's basically based on the ratio of the probability of the word from of the two distributions.
#c209	you;this ratio;It;the ratio;the probability;the word;the two distributions
#s210	Essentially we are comparing the probability of the word from the two distributions and if it's higher according to theta one, then according to theta 2 then this weight would be positive and therefore it means when we observe such a word.
#c210	we;the probability;the word;the two distributions;it;theta;this weight;it;we;such a word
#s211	We'll say that it's more likely to be from category One, and the more we observe such a word, the more likely the document will be classified as theta one.
#c211	We;it;category;we;such a word;the document;theta
#s212	If, on the other hand, the probability of the word from theta one is smaller than the probability of the word from theta 2, then you can see this weight is negative.
#c212	the other hand;the probability;the word;theta;one;the probability;the word;theta;you;this weight
#s213	Therefore this is the negative evidence for supporting category one.
#c213	the negative evidence;category
#s214	That means the more we observe such a word, the more likely the document is actually from theta 2.
#c214	we;such a word;the document;theta
#s215	So this formula now makes a lot of sense, so we're going to aggregate all the evidence from the document.
#c215	this formula;a lot;sense;we;all the evidence;the document
#s216	We take a sum over all the words we can call this the features.
#c216	We;a sum;all the words;we;the features
#s217	That we collect from the document that would help us make the decision and that each feature has a weight that tells us how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio.
#c217	we;the document;us;the decision;each feature;a weight;us;this feature support category;support;the category;the log;probability ratio
#s218	Here in naive Bayes.
#c218	naive Bayes
#s219	And then finally we have this constant of bias here, so that formula actually is a formula that can be generalized to accommodate more features.
#c219	we;this constant;bias;that formula;a formula;more features
#s220	And that's why I've introduced some other symbols here.
#c220	I;some other symbols
#s221	So introduce the beta zero to denote the bias and Fi to denote each feature, and then beta sub i, to denote the weight on which feature.
#c221	the bias;Fi;each feature;then beta sub;i;the weight
#s222	Now if we do this generalization, what we see is that in general we can represent the document by feature vector F, FI here.
#c222	we;this generalization;what;we;we;the document;feature vector F;FI
#s223	Of course in this case FI is the count of a word, but in general we can put any features that we think are relevant for categorization.
#c223	this case;FI;the count;a word;we;any features;we;categorization
#s224	For example document length or the font size or counts of other patterns in the document.
#c224	example document length;the font size;counts;other patterns;the document
#s225	And then our scoring function can be defined as a sum of constant beta zero and sum of the feature weights over all the features.
#c225	our scoring function;a sum;sum;the feature weights;all the features
#s226	So if HF sub I is a feature value then we multiply value by the corresponding weight beta sub
#c226	I;a feature value;we;value;the corresponding weight beta
#s227	i and we just take sum and this is to aggregate.
#c227	i;we;sum
#s228	All evidence that we can collect from all these features.
#c228	All evidence;we;all these features
#s229	And of course there are parameters here.
#c229	course;parameters
#s230	So what are the parameters?
#c230	what;the parameters
#s231	Well These betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents.
#c231	These betas;the weights;appropriate settings;weights;we;the such a scoring function;documents
#s232	Just like in the case of Naive Bayes we can clearly see naive Bayes classifier is a special case of this general classifier.
#c232	the case;Naive Bayes;we;naive Bayes classifier;a special case;this general classifier
#s233	Actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification.
#c233	this general form;a classifier;logistical regression;those conditional approaches;discriminative approaches;classification
#s234	And we are going to talk more about such approaches later, but here I want you to know that there's a strong connection close connection between the two kinds of approaches, and this slide shows how naive Bayes classifier can be connected to a logistic regression.
#c234	we;such approaches;I;you;a strong connection close connection;the two kinds;approaches;this slide;how naive Bayes classifier;a logistic regression
#s235	And you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to solve the problem.
#c235	you;discriminative classifiers;a more general form;the bottom;we;more features;the problem
410	69980b1f-e40c-4015-a85a-54ab27fbac41	111
#s1	This lecture is about how to mine text data with social network as context.
#c1	This lecture;text data;social network;context
#s2	In this lecture, we're going to continue discussing contextual text mining.
#c2	this lecture;we;contextual text mining
#s3	In particular, we're going to look at the social network of authors of text as context.
#c3	we;the social network;authors;text;context
#s4	So first some motivation for using network context for analysis of text.
#c4	So first some motivation;network context;analysis;text
#s5	The context of a text article can form a network.
#c5	The context;a text article;a network
#s6	For example, the authors of research articles might form a collaboration network.
#c6	example;the authors;research articles;a collaboration network
#s7	Or authors of social media content might also form social networks.
#c7	authors;social media content;social networks
#s8	For example, in Twitter.
#c8	example;Twitter
#s9	People might follow each other, or in Facebook.
#c9	People;Facebook
#s10	Some people might claim as friends of others etc.
#c10	Some people;friends;others
#s11	So such context connects the content.
#c11	such context;the content
#s12	Often the others.
#c12	Often the others
#s13	Similarly, locations associated with text can also be connected to form geographical network, but in general you can imagine the meta data of the text data can form some kind of network if they have some relations.
#c13	locations;text;geographical network;you;the meta data;the text data;some kind;network;they;some relations
#s14	Now there is some benefit in jointly analyzing text and its social network context or network context in general.
#c14	some benefit;text;its social network context;network context
#s15	And that's because we can use network to impose some constraints on topics text.
#c15	we;network;some constraints;topics text
#s16	So for example, it's reasonable to assume that authors connected in collaboration network tend to write about the similar topics.
#c16	example;it;authors;collaboration network;the similar topics
#s17	So such heuristic can be used to guide us in analyzing topics.
#c17	us;topics
#s18	Text also can help characterize the content associated with each subnetwork and this is to say that both kinds of data, the network and text can help each other.
#c18	Text;the content;each subnetwork;both kinds;data;the network;text
#s19	So for example, the difference in opinions expressed in two subnetworks let's say two social networks can be revealed by doing this kind of joint analysis.
#c19	example;the difference;opinions;two subnetworks;'s;two social networks;this kind;joint analysis
#s20	So here we are going to briefly introduce.
#c20	we
#s21	a model called Network supervised topic model.
#c21	a model;Network;supervised topic model
#s22	And, this in this slide we're going to give some general ideas, and then in the next slide will give some more details.
#c22	this slide;we;some general ideas;the next slide;some more details
#s23	But in general, in this part of the course we don't have enough time to cover these frontier topics in detail, but we provide references that would allow you to.
#c23	this part;the course;we;enough time;these frontier topics;detail;we;references;you
#s24	To read more about the topic.
#c24	the topic
#s25	To know the details.
#c25	the details
#s26	But it should still be useful to know the general ideas and to know what they can do to know when you might be able to use them.
#c26	it;the general ideas;what;they;you;them
#s27	So the general idea of network supervised topic model modeling is the following.
#c27	the general idea;network;the following
#s28	Let's start with.
#c28	's
#s29	Viewing the regular topic models like PLSA or LDA as solving optimization problem.
#c29	the regular topic models;PLSA;LDA;optimization problem
#s30	Of course, in this case the optimization objective function is the likelihood function.
#c30	this case;the optimization objective function;the likelihood function
#s31	So we often use maximum likelihood estimator to obtain the parameters and these parameters would give us useful information that we want to obtained from text data.
#c31	we;maximum likelihood estimator;the parameters;these parameters;us;useful information;we;text data
#s32	For example topics.
#c32	example
#s33	So we want to maximize the probability of text data given the parameters generated denoted by Lambda here.
#c33	we;the probability;text data;the parameters;Lambda
#s34	Now the main idea of incorporating network is to say that to think about the constraints that can be imposed based on the network.
#c34	the main idea;incorporating network;the constraints;the network
#s35	In general, the idea is to use the network to impose some constraints on the model parameters Lambda here.
#c35	the idea;the network;some constraints;the model parameters;Lambda
#s36	For example, the text at adjacent nodes of the network can be assumed to cover similar topics.
#c36	example;the text;adjacent nodes;the network;similar topics
#s37	Indeed, in many cases they tend to cover similar topics.
#c37	many cases;they;similar topics
#s38	We so we may be able to smooth the topic distributions on the graph on the network so that adjacent nodes will have very similar topic distributions, so you.
#c38	We;we;the topic distributions;the graph;the network;adjacent nodes;very similar topic distributions
#s39	They will share common distribution of the topics or have just slight variations of the topic distributions or topic coverage.
#c39	They;common distribution;the topics;just slight variations;the topic distributions;topic coverage
#s40	So technically what we can do is simply to add a network induced regularizers to the likelihood objective function as shown here.
#c40	what;we;a network;regularizers;the likelihood objective function
#s41	So instead of just optimizing the probability of text data given parameters, Lambda.
#c41	the probability;text data;parameters;Lambda
#s42	We're going to optimize another function F.
#c42	We;another function
#s43	This function combines the likelyhood with a regularizer function called r here, and the regularizer is defined the on the parameters Lambda and the network and tells us basically what kind of parameters are preferred from network constraint perspective, so you can easy to see this is in effect implemented the idea of imposing a prior on the model parameters only that will not necessarily having a probabilistic model.
#c43	This function;the likelyhood;a regularizer function;r;the regularizer;the parameters;Lambda;the network;us;what kind;parameters;network constraint perspective;you;effect;the idea;a prior;the model parameters;a probabilistic model
#s44	But the idea is the same.
#c44	the idea
#s45	We're going to combine the two in one single objective function.
#c45	We;one single objective function
#s46	So the advantage of this idea is that it's quite general here the topic model can be any generative model for text.
#c46	the advantage;this idea;it;the topic model;any generative model;text
#s47	Right, it doesn't have to be PLSA or LDA or the current topic models.
#c47	it;PLSA;LDA;the current topic models
#s48	And similarly, the network can be also any network.
#c48	the network;any network
#s49	Any graph that connects these text objects.
#c49	Any graph;these text objects
#s50	This regularizer can also be any regularizer.
#c50	This regularizer;any regularizer
#s51	We can be flexible in capturing different heuristics that we want to capture.
#c51	We;different heuristics;we
#s52	And finally, the function F can also vary, so there can be many different with combined them.
#c52	the function;F;combined them
#s53	So this general idea is actually quite powerful.
#c53	this general idea
#s54	Office general approach to combining these different types of data in a single optimization framework.
#c54	Office general approach;these different types;data;a single optimization framework
#s55	And this general idea clearly can be applied for many problems, but here in this paper referenced here.
#c55	this general idea;many problems;this paper
#s56	A particular instantiation, called NetPLSA was started in this case it's just an extension of PLSA to incorporate some simple constraints imposed by network.
#c56	A particular instantiation;NetPLSA;this case;it;just an extension;PLSA;some simple constraints;network
#s57	And the prior here is the neighbors on the network must have similar topic distribution.
#c57	the neighbors;the network;similar topic distribution
#s58	They must cover similar topics in similar ways, and that's basically what it says in English.
#c58	They;similar topics;similar ways;what;it;English
#s59	So technically we just have a modified object function here as defined on both the text collection C and the network graph G here.
#c59	we;a modified object function;both the text collection C;the network;graph;G
#s60	And if you look at this formula and you can actually recognize some part fairly familiar, because are they should be fairly familiar to you by now.
#c60	you;this formula;you;some part;they;you
#s61	So can you recognize which part?
#c61	you;which part
#s62	Is the likelihood for the text data given by a topic model?
#c62	the likelihood;the text data;a topic model
#s63	If you look at it that you will see this part is precisely the PLSA log likelihood that we want to maximize when we estimate the parameters for PLSA alone.
#c63	you;it;you;this part;precisely the PLSA log likelihood;we;we;the parameters;PLSA
#s64	But the second equation shows some additional constraints on the parameters.
#c64	the second equation;some additional constraints;the parameters
#s65	And in particular we see here.
#c65	we
#s66	It's to measure the difference between the topic coverage at node U and
#c66	It;the difference;the topic coverage;node U
#s67	the V.
#c67	the V.
#s68	The two adjacent nodes on the network.
#c68	The two adjacent nodes;the network
#s69	We want their distributions to be similar, so they here we are computing the square of their differences and we want to minimize this difference.
#c69	We;their distributions;we;the square;their differences;we;this difference
#s70	And note that there is a negative sign in front of this sum.
#c70	a negative sign;front;this sum
#s71	This whole Sum.
#c71	This whole Sum
#s72	Here.
#s73	So this makes it possible to find the parameters that are that are both to maximize the PLSA log likelihood.
#c73	it;the parameters;the PLSA log likelihood
#s74	That means the parameters will fit the data well and also to respect this constraint from the network.
#c74	the parameters;the data;this constraint;the network
#s75	And this is an active sign that I just mentioned, because there's a negative sign when we maximize this objective function we will actually minimize this second term here.
#c75	an active sign;I;a negative sign;we;this objective function;we;this second term
#s76	So if we look further in this picture, will see there is also weight of edge between u and v here and that's based on our network.
#c76	we;this picture;weight;edge;u;our network
#s77	If we have a weight that says these two nodes are strong collaborators of researchers, or these two are strong.
#c77	we;a weight;these two nodes;strong collaborators;researchers
#s78	Connections between two people in a social network and then they will have a high weight then that means it will be more important to make sure that their topic coverages are similar and that's basically what it says here.
#c78	Connections;two people;a social network;they;a high weight;it;their topic coverages;what;it
#s79	And then finally use your parameter Lambda.
#c79	your parameter;Lambda
#s80	Here.
#s81	This is a new parameter to control the influence of network constraints.
#c81	a new parameter;the influence;network constraints
#s82	We can see easily if Lambda is set to zero, we just go back to the standard PLSA.
#c82	We;Lambda;we;the standard PLSA
#s83	But when the lambda is set to a larger value, then we'll let the network influence the estimate models more so as you can see, the effect here is that we can do basically PLSA but we're going to also try to make the topic coverages on the two nodes that are.
#c83	the lambda;a larger value;we;the network;the estimate models;you;the effect;we;PLSA;we;the topic;the two nodes
#s84	strongly connected to be similar and we ensure their coverages are more similar.
#c84	we;their coverages
#s85	So here are some sample results from that paper, and this slide shows the regular results of using PLSA and the data here is DBLP data.
#c85	some sample results;that paper;this slide;the regular results;PLSA;the data;DBLP data
#s86	bibliographic data about the research articles.
#c86	bibliographic data;the research articles
#s87	And the experiments have to do with using a four communities of publications.
#c87	the experiments;a four communities;publications
#s88	IR information retrieval.
#c88	IR information retrieval
#s89	That means DM, stand for data mining, ML for machine learning and web.
#c89	DM;data mining;ML;machine learning;web
#s90	There are four communities of articles and we will.
#c90	four communities;articles;we
#s91	Hoping to see that the topic mining can help us uncover these four communities, but from these sample topics that you are seeing here that are generated by PLSA and PLSA is unable to generate the four communities that correspond to our intuition and the reason was because they are all mixed together and there are many words that are shared by these communities, so it's not that easy to use the four topics.
#c91	the topic mining;us;these four communities;these sample topics;you;PLSA;PLSA;the four communities;our intuition;the reason;they;many words;these communities;it;the four topics
#s92	to separate them.
#c92	them
#s93	If we use more topics, perhaps will have more coherent topics.
#c93	we;more topics;more coherent topics
#s94	But what's interesting is that if we use the NetPLSA where the network the collaboration network in this case of others is used to impose constraints.
#c94	what;we;the network;the collaboration network;this case;others;constraints
#s95	And this, in this case we also use four topics, but NetPLSA would give much more meaningful topics.
#c95	this case;we;four topics;NetPLSA;much more meaningful topics
#s96	So here we see that these topics correspond well to the four communities.
#c96	we;these topics;the four communities
#s97	The first is the information retrieval second is data mining.
#c97	the information retrieval;second;data mining
#s98	Third is machine learning the fourth is web.
#c98	machine;web
#s99	So that separation was.
#c99	that separation
#s100	Mostly because of the influence of network where we leverage the collaboration network information.
#c100	the influence;network;we;the collaboration network information
#s101	Essentially the people that form a collaboration network would then be kind of assumed to write about similar topics and that's why we can have more coherent topics.
#c101	the people;a collaboration network;similar topics;we;more coherent topics
#s102	And if you just listen to text data alone based on the Co occurrences you won't get such a coherent topics, even though a topic model PLSA or LDA also.
#c102	you;text data;you;such a coherent topics
#s103	should be able to pick up a Co occurring words, so in general the topics that they generate represent words that Co occur with each other.
#c103	a Co occurring words;in general the topics;they;words;Co
#s104	But still they cannot generate such coherent results as NetPLSA is showing that the network context is very useful here.
#c104	they;such coherent results;the network context
#s105	A similar model could have been also used to characterize the content associated with each subnetwork of collaborations.
#c105	A similar model;the content;each subnetwork;collaborations
#s106	So a more general view of text mining in the context of network is to treat text as living in the rich information network environment.
#c106	a more general view;text mining;the context;network;text;the rich information network environment
#s107	That means we can connect all the related data together as a big network and text data out can be associated with a lot of structures in the network, for example text.
#c107	we;all the related data;a big network;text data;a lot;structures;the network;example
#s108	It can be associated with the nodes of the network, and that's basically what we just discussed in the NetPLSA But text data can be associated with edges as well, or paths or even sub networks and such way to represent taxes are in the big environment of all the context.
#c108	It;the nodes;the network;what;we;text data;edges;paths;even sub networks;such way;taxes;the big environment;all the context
#s109	Information is very powerful because it allows us to analyze all the data, all the information together.
#c109	Information;it;us;all the data
#s110	And so, in general analysis of text there should be using the entire network of information that's related to the text data.
#c110	general analysis;text;the entire network;information;the text data
#s111	So here's one suggested reading and this is the paper about the NetPLSA where you can find more details about the model and how to estimate such a model.
#c111	one suggested reading;the paper;you;more details;the model;such a model
410	6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	106
#s1	This lecture is about the probabilistic topic models for topic mining and analysis.
#c1	This lecture;the probabilistic topic models;topic mining;analysis
#s2	In this lecture we're going to continue talking about the top mining and analysis.
#c2	this lecture;we;the top mining;analysis
#s3	We're going to introduce probabilistic topic models.
#c3	We;probabilistic topic models
#s4	So this is a slide that you have seen earlier where we discussed the problems with using a term as a topic.
#c4	a slide;you;we;the problems;a term;a topic
#s5	So to solve these problems intuitively we need to use more words to describe the topic and this would address the problem of lack of expressive power.
#c5	these problems;we;more words;the topic;the problem;lack;expressive power
#s6	When we have more words that we can use to describe the topic, we can describe complicated topics, to address the second problem, we need to introduce weights of words.
#c6	we;more words;we;the topic;we;complicated topics;the second problem;we;weights;words
#s7	This would allow you to distinguish subtle differences in topics and to introduce semantically related words in the fuzzy manner.
#c7	you;subtle differences;topics;semantically related words;the fuzzy manner
#s8	Finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic.
#c8	the problem;word ambiguity;we;an ambiguous word;we;its topic
#s9	It turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic.
#c9	It;a probabilistic topic model;we;a lot;lectures;this topic
#s10	So the basic idea here is improved representation of topic as a word distribution.
#c10	the basic idea;improved representation;topic;a word distribution
#s11	So what you see now is the old representation, where we represent each topic with just one word or one term or one phrase.
#c11	what;you;the old representation;we;each topic;just one word;one term;one phrase
#s12	But now we're going to use a word distribution to describe the topic.
#c12	we;a word distribution;the topic
#s13	So here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary.
#c13	you;sports;we;a word distribution;all the words;our vocabulary
#s14	So for example, the high probability words here are sports, game, basketball, football, play, star, etc.
#c14	example;the high probability words;sports;game;basketball;football;play;star
#s15	These are sports-related terms and of course it would also give a non zero probability to some other words like """travel"" which might be related to" sports.
#c15	sports-related terms;course;it;a non zero probability;some other words;"""travel;sports
#s16	But in general not so much related to the topic.
#c16	the topic
#s17	In general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one.
#c17	we;a non zero probability;all the words;some words;very very small probabilities;these probabilities
#s18	So that it forms a distribution of all the words.
#c18	it;a distribution;all the words
#s19	Now intuitively, this distribution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports.
#c19	this distribution;a topic;we;words;the distribution;we;words;sports
#s20	You can also see it as a very special case if the probability mass is concentrated entire of just one word.
#c20	You;it;a very special case;the probability mass;just one word
#s21	Let's sports, and this basically degenerates to the simple representation of topic with just one word.
#c21	's;the simple representation;topic;just one word
#s22	But as a distribution, this topic representation can in general involve many words to describe the topic and can model subtle differences in semantics of the topic.
#c22	a distribution;this topic representation;many words;the topic;subtle differences;semantics;the topic
#s23	Similarly, we can model travel and science with their respective distributions.
#c23	we;travel;science;their respective distributions
#s24	So in the distribution for travel we "see top words like ""attraction, trip," "flight, hotel etc."
#c24	the distribution;travel;we;top words;"attraction, trip
#s25	"" "Whereas in science, we see ""scientist," "spaceship, telescope or genomics"" and new" science-related terms, now, that doesn't mean sports-related terms  necessary have zero probabilities for science in general, we can imagine all these words.
#c25	science;we;"scientist;spaceship;telescope;genomics;new" science-related terms;sports-related terms;zero probabilities;science;we;all these words
#s26	We have non zero probabilities, it's just that for a particular topic of some words we have very very small probabilities.
#c26	We;non zero probabilities;it;a particular topic;some words;we;very very small probabilities
#s27	Now you can also see there are some words that are shared by these topics.
#c27	you;some words;these topics
#s28	Well, when I say shared, that just means even with some probability threshold you can still see one word to occur in multiple topics.
#c28	I;some probability threshold;you;one word;multiple topics
#s29	In this case I marked them in black so "you can see ""travel""," for example, occured in all the three topics here, but with different probabilities.
#c29	this case;I;them;black;you;""travel;example;all the three topics;different probabilities
#s30	It has the highest probability for the travel topic 0.05.
#c30	It;the highest probability;the travel topic
#s31	But with much smaller probabilities for sports and science, which makes sense. "
#c31	much smaller probabilities;sports;science;sense
#s32	And similarly you can see ""star"" also" occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics.
#c32	you;""star;sports;science;reasonably high probabilities;they;the two topics
#s33	So with this representation it addresses the three problems that mentioned earlier.
#c33	this representation;it;the three problems
#s34	First, it now uses multiple words that describe topic, so it allows us to describe fairly complicated topics.
#c34	it;multiple words;topic;it;us;fairly complicated topics
#s35	Second, it assigns weights to terms, so now we can model several differences of semantics and you can bring in related words together to model topic.
#c35	it;weights;terms;we;several differences;semantics;you;related words;topic
#s36	Third, because we have probabilities for the same word in different topics.
#c36	we;probabilities;the same word;different topics
#s37	We can disambiguate the sense of word in the text to decode its underlying topic, so we address all these three problems with this new way of representing a topic.
#c37	We;the sense;word;the text;its underlying topic;we;all these three problems;this new way;a topic
#s38	So now, of course, our problem definition has been refined just slightly.
#c38	course;our problem definition
#s39	The slide is very similar to what you have seen before, except that we have added refinement for what the topic is.
#c39	The slide;what;you;we;refinement;what;the topic
#s40	So now each topic is word distribution.
#c40	each topic;word distribution
#s41	And for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary.
#c41	each word distribution;we;all the probabilities;all the words;the vocabulary
#s42	So you see a constraint here
#c42	you;a constraint
#s43	and we still have another constraint on the topic coverage, namely pis.
#c43	we;another constraint;the topic coverage;namely pis
#s44	So all the pis of ij's must sum to one for the same document.
#c44	all the pis;ij;the same document
#s45	So how do we solve this problem?
#c45	we;this problem
#s46	Well, let's look at this problem as a computation problem now.
#c46	's;this problem;a computation problem
#s47	So we clearly specify the input and output as illustrated here on this side.
#c47	we;the input;output;this side
#s48	The input, of course is our text data C is the collection, but we also generally assume we know the number of topics K or we hypothesize a number and then try to mine K topics, even though we don't know the exact topics that exist in the collection and these vocabulary set.
#c48	The input;course;our text data C;the collection;we;we;the number;topics K;we;a number;K topics;we;the exact topics;the collection;these vocabulary set
#s49	As a set of words that determines what units would be treated as the basic units for analysis.
#c49	a set;words;what units;the basic units;analysis
#s50	In most cases, we use words as the basis.
#c50	most cases;we;words;the basis
#s51	For analysis, and that means each word is a unit.
#c51	analysis;each word;a unit
#s52	Now the output would consist of as first a set of topics represented by Theta i's Each theta_i is a word distribution.
#c52	the output;as first a set;topics;Theta;i;Each theta_i;a word distribution
#s53	And We also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before.
#c53	We;the coverage;topics;each document;the same pi_ij;we
#s54	So given a set of text data, we would like to compute all these distributions and all these coverages as you have seen on this slide.
#c54	a set;text data;we;all these distributions;all these coverages;you;this slide
#s55	Now of course, there may be many different ways of solving this problem.
#c55	course;many different ways;this problem
#s56	Indeed, you can write a heuristic program to solve this problem, but here we're going to introduce a general way of solving this problem called  generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here I dim the picture that you have seen before in order to show the generation process.
#c56	you;a heuristic program;this problem;we;a general way;this problem;  generative model;fact;very general idea;it;a principle way;statistical modeling;text mining problems;I;the picture;you;order;the generation process
#s57	So the idea of this approach is actually to 1st design a model for our data.
#c57	the idea;this approach;1st;a model;our data
#s58	So we design a probabilistic model to model how the data are generated.
#c58	we;a probabilistic model;the data
#s59	Of course this is based on our assumption.
#c59	our assumption
#s60	The actual data aren't necessary generated this way, so that would give us a probability distribution of the data that you are seeing on this slide given a particular model and parameters that are denoted by Lambda.
#c60	The actual data;us;a probability distribution;the data;you;this slide;a particular model;parameters;Lambda
#s61	So this capital lambda actually consists of all the parameters that we're interested in.
#c61	this capital lambda;all the parameters;we
#s62	And these parameters in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others.
#c62	these parameters;the behavior;the probabilistic model;you;these parameters;different values;it;some data;higher probabilities;others
#s63	Now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters.
#c63	this case;course;our tax mining problem;more precisely topic mining problem;we;the following parameters
#s64	First, we have theta_i's Each is a word distribution
#c64	we;theta_i;a word distribution
#s65	and then we have a set of pi's for each document.
#c65	we;a set;pi;each document
#s66	And since we have N documents so we have N sets of pis.
#c66	we;N documents;we;N sets;pis
#s67	And each set of the pi values will sum to one.
#c67	each set;the pi values
#s68	So this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions.
#c68	we;we;these word distributions;coverage numbers;we;we;data;such distributions
#s69	So how do we model the data in this way?
#c69	we;the data;this way
#s70	And we assume that data are actually samples drawn from such a model that depends on these parameters.
#c70	we;data;samples;such a model;these parameters
#s71	Now one interesting question here is to think about how many parameters are there in total.
#c71	one interesting question;how many parameters;total
#s72	Now obviously we can already see N * K parameters for pi's.
#c72	we;N * K parameters;pi
#s73	We also see K theta_i's, but each theta_i is actually a set of probability values.
#c73	We;K theta_i;each theta_i;a set;probability values
#s74	Right?
#s75	It's a distribution over words.
#c75	It;a distribution;words
#s76	So I leave this as exercise for you to figure out exactly how many parameters there are here.
#c76	I;exercise;you;exactly how many parameters
#s77	Now, once we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data.
#c77	we;a model;we;the model;our data;we;the parameters;the parameters;the data
#s78	In other words, we would like to adjust these parameter values until we give our data set the maximum probability.
#c78	other words;we;these parameter values;we;our data;the maximum probability
#s79	I just say that depending on the parameter values, some data points will have higher probabilities than others.
#c79	I;the parameter values;some data points;higher probabilities;others
#s80	What we're interested in here is what parameter values will give our data set the highest probability.
#c80	What;we;what;parameter values;our data;the highest probability
#s81	So I also illustrate the problem with the picture that you see here.
#c81	I;the problem;the picture;you
#s82	On the X axis, I just illustrate the Lambda, the parameters as one dimensional variable.
#c82	the X axis;I;the Lambda;the parameters;one dimensional variable
#s83	It's oversimplification obviously, but it suffices is to show the idea and the Y axis shows the probability of the data observe.
#c83	It;oversimplification;it;the idea;the Y axis;the probability
#s84	This probability obviously depends on the setting of Lambda, so that's why it varies as you change the value of Lambda.
#c84	This probability;the setting;Lambda;it;you;the value;Lambda
#s85	What we're interested in here is to find the Lambda star that would maximize the probability of the observed data.
#c85	What;we;the Lambda star;the probability;the observed data
#s86	So this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm.
#c86	our estimate;the parameters;these parameters;precisely what;we;text data;these parameters;actually the outcome;the output;the data mining;algorithm
#s87	So this is a general idea of using a generative model for text mining.
#c87	a general idea;a generative model;text mining
#s88	First, we design a model with some parameters that we are interested in, and then we model the data.
#c88	we;a model;some parameters;we;we;the data
#s89	We adjust the parameters to fit the data as well as we can.
#c89	We;the parameters;the data;we
#s90	After we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data.
#c90	we;data;we;some parameter values;this specific parameter values;the output;the algorithm;we;the discovered knowledge;text data
#s91	By varying the model, of course we can discover different knowledge.
#c91	the model;course;we;different knowledge
#s92	So to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic.
#c92	we;a new way;a topic;word distribution;advantage;multiple words;a complicated topic
#s93	It also allows us to assign weights on words so we can model subtle variations of semantics.
#c93	It;us;weights;words;we;subtle variations;semantics
#s94	We talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles.
#c94	We;the task;topic mining;analysis;we;a topic;a distribution;the input;a collection;text articles
#s95	The number of topics and vocabulary set and the output is a set of topics.
#c95	The number;topics;vocabulary set;the output;a set;topics
#s96	Each is word distribution.
#c96	word distribution
#s97	And also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters.
#c97	And also the coverage;all the topics;each document;theta_i;pi_i;we;two constraints;these parameters
#s98	The first is the constraint on the word distributions.
#c98	the constraint;the word distributions
#s99	In each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary.
#c99	each world distribution;the probabilities;all the words;all the words;the vocabulary
#s100	The second constraint is on the topic coverage in each document.
#c100	The second constraint;the topic coverage;each document
#s101	A document is not allowed to cover a topic outside the set of topics that we are discovering.
#c101	A document;a topic;the set;topics;we
#s102	So the coverage of each of these K topics would sum to one for a document.
#c102	the coverage;these K topics;a document
#s103	We also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data.
#c103	We;the general idea;a generative model;text mining;the idea;a model;the generation;data
#s104	We simply assume that they are generated this way and inside the model, we embed some parameters that were interested in denoted by Lambda.
#c104	We;they;the model;we;some parameters;Lambda
#s105	And then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the Lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text.
#c105	we;the most likely parameter values;a particular data set;we;the Lambda star;knowledge;the text;our problem;we;the design;the model;parameters;various kinds;knowledge;text
#s106	As you will see later in the other lectures.
#c106	you;the other lectures
410	6a9b1334-5f53-407b-8864-2cff7edbc603	98
#s1	This lecture is the first one about the text clustering.
#c1	This lecture;the text clustering
#s2	This is very important that technique for doing topic mining an analysis.
#c2	that technique;topic mining;an analysis
#s3	In particular, in this lecture organ to start with some basic questions about the clustering: What is text clustering and why we are interested in text clustering?
#c3	this lecture organ;some basic questions;the clustering;What;text clustering;we;text clustering
#s4	In the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. "
#c4	the following lectures;we;text clustering;the clustering results
#s5	So what is text
#c5	what;text
#s6	  
#s7	Clustering actually is a very general technique for data mining.
#c7	Clustering;a very general technique;data mining
#s8	As you might have learned in some other courses.
#c8	you;some other courses
#s9	The idea is to discover natural structures in the data.
#c9	The idea;natural structures;the data
#s10	In other words, we want to group similar objects together.
#c10	other words;we;similar objects
#s11	In our case, these objects are of course texture objects.
#c11	our case;these objects;course;texture objects
#s12	For example, they can be documents, turns, passages, sentences or websites.
#c12	example;they;documents;passages;sentences;websites
#s13	And then our goal is to group similar texture objects together.
#c13	our goal;similar texture objects
#s14	So let's see a example here.
#c14	's;a example
#s15	You don't really see text objects, but I just use some shapes to denote objects that can be grouped together.
#c15	You;text objects;I;some shapes;denote objects
#s16	Now, if I ask you what are some natural structures or natural groups well you, if you look at it, you might agree that we can group these objects based on shapes or their locations on this 2 dimensional space.
#c16	I;you;what;some natural structures;natural groups;you;you;it;you;we;these objects;shapes;their locations;this 2 dimensional space
#s17	So we got the three clusters in this case.
#c17	we;the three clusters;this case
#s18	And then may not be so much disagreement about these three clusters, but it really depends on the perspective to look at the objects.
#c18	so much disagreement;these three clusters;it;the perspective;the objects
#s19	Maybe some of you have also seen it in a different way, so we might get different clusters.
#c19	you;it;a different way;we;different clusters
#s20	And you will see another example about this ambiguity more clearly, but the main point here is the problem is actually not so well defined.
#c20	you;another example;this ambiguity;the main point;the problem
#s21	And the problem lies in how to define similarity.
#c21	the problem;similarity
#s22	What do you mean by similar objects?
#c22	What;you;similar objects
#s23	Now this problem.
#c23	Now this problem
#s24	Has to be clearly defined in order to have well defined clustering problem.
#c24	order;well defined clustering problem
#s25	And the problem is in general that any two objects can be similar that depending on how you look at them.
#c25	the problem;any two objects;you;them
#s26	So for example.
#c26	example
#s27	Let's look at the two words like car and horse.
#c27	's;the two words;car;horse
#s28	So are the two words similar It depends on how you look at it.
#c28	the two words;It;you;it
#s29	If you look at the physical.
#c29	you;the physical
#s30	Physical properties of car and horse.
#c30	Physical properties;car;horse
#s31	They are very different.
#c31	They
#s32	But if you look at the them functionally, a car in the horse can both be transportation tool, so in that sense they may be similar.
#c32	you;the them;a car;the horse;transportation tool;that sense;they
#s33	So as you can see, it really depends on our perspective to look at the objects and so in order to make the clustering problem well defined, a user must define the perspective.
#c33	you;it;our perspective;the objects;order;the clustering problem;a user;the perspective
#s34	For assessing similarity.
#c34	similarity
#s35	And we call this perspective the clustering bias.
#c35	we;this perspective;the clustering bias
#s36	And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be used to group similar objects 'cause otherwise.
#c36	you;a clustering problem;it;your perspective;similarity;the similarity;similar objects
#s37	Similarity is not well defined.
#c37	Similarity
#s38	An one can have different ways to group objects.
#c38	An one;different ways;group objects
#s39	So let's look at a concrete example.
#c39	's;a concrete example
#s40	Here you are seeing some objects or some shapes that are very similar to what you have seen on the 1st slide.
#c40	you;some objects;some shapes;what;you;the 1st slide
#s41	But if I ask you to group these objects again, you might.
#c41	I;you;these objects;you
#s42	Might.
#c42	Might
#s43	Feel there's more uncertainty here than on the previous slide.
#c43	more uncertainty;the previous slide
#s44	For example.
#c44	example
#s45	You might think, well, we can still group by shapes, so that would give us cluster that looks like this.
#c45	You;we;shapes;us;cluster
#s46	However, you might also feel that.
#c46	you
#s47	Maybe the objects can be grouped based on the sizes, so that would give us a different way to cluster the data.
#c47	the objects;the sizes;us;a different way;the data
#s48	If we look at the size and look at the similarity in size.
#c48	we;the size;the similarity;size
#s49	So as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective.
#c49	you;the perspective;different clustering results;us;order;the clustering result;we;perspective
#s50	Without perspective, it's very hard to define what is the best clustering result.
#c50	perspective;it;what;the best clustering result
#s51	So there are many examples of text clustering.
#c51	many examples;text clustering
#s52	Set up.
#s53	And so, for example, we can cluster documents in the whole text collection.
#c53	example;we;documents;the whole text collection
#s54	So in this case documents are the units to be clustered.
#c54	this case;documents;the units
#s55	We may be able to cluster terms in this case.
#c55	We;terms;this case
#s56	Terms are objects.
#c56	Terms;objects
#s57	And Cluster of terms can be used to define the concept or theme or topic.
#c57	Cluster;terms;the concept;theme;topic
#s58	In fact, the topic models that you have seen some previous lectures.
#c58	fact;you;some previous lectures
#s59	Can give you cluster of terms in some sense.
#c59	you;cluster;terms;some sense
#s60	If you take the terms with high probabilities from world distribution.
#c60	you;the terms;high probabilities;world distribution
#s61	Another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects.
#c61	Another example;just a cluster;any texts segments;example;passages;sentences;any segments;you;a large text objects
#s62	For example, we might extract all the text segments about the topic, let's say by using a topic model.
#c62	example;we;all the text segments;the topic;'s;a topic model
#s63	Now, once we've got those text objects, then we can cluster.
#c63	we;those text objects;we
#s64	The segments that we've got to discover interesting clusters that might also represent the subtopics.
#c64	The segments;we;interesting clusters;the subtopics
#s65	So this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve.
#c65	a case;some other techniques;you;a lot;text mining algorithms;a flexible way
#s66	The goal of doing more sophisticated mining and analysis of text data.
#c66	The goal;more sophisticated mining;analysis;text data
#s67	We can also cluster fairly large text law gets, and by that I just mean text objects may contain a lot of documents.
#c67	We;fairly large text law;I;text objects;a lot;documents
#s68	So for example we might cluster websites.
#c68	example;we;websites
#s69	Each website is actually composed of multiple documents.
#c69	Each website;multiple documents
#s70	Similarly, we can also cluster articles written by the same author, for example.
#c70	we;articles;the same author;example
#s71	So we can treat all the articles published by author as one unit for Clustering.
#c71	we;all the articles;author;one unit;Clustering
#s72	In this way, we might group authors together based on whether they are published papers or similar.
#c72	this way;we;authors;they;papers
#s73	Furthermore, text clusters can also be further clustered.
#c73	text clusters
#s74	Regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels.
#c74	the hierarchy;we;any text object;different levels
#s75	So more generally, why is text clustering interesting?
#s76	Well, it's brcause
#c76	it;brcause
#s77	it's a very useful technique for text mining, particularly exploratory text analysis.
#c77	it;a very useful technique;text mining;particularly exploratory text analysis
#s78	And so a typical scenario is that you are getting a lot of text data.
#c78	a typical scenario;you;a lot;text data
#s79	Let's say all the email messages from customers in some time period, or all the literature, articles, etc.
#c79	's;all the email messages;customers;some time period;all the literature;articles
#s80	And then you hope to get the sense about what are the overall content of the collection.
#c80	you;the sense;what;the overall content;the collection
#s81	So, for example, you might be interested in getting.
#c81	example;you
#s82	A sense about the major topics or what are some typical or representative document in the collection?
#c82	A sense;the major topics;what;some typical or representative document;the collection
#s83	And clustering help us achieve this goal.
#c83	clustering;us;this goal
#s84	We sometimes also want to link similar text objects together and these.
#c84	We;similar text objects
#s85	These objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents.
#c85	These objects;content;example;that case;such a technique;us;redundancy;duplicated documents
#s86	Sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic.
#c86	they;about the same topic;them;we;more complete coverage;the topic
#s87	We may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing.
#c87	We;text;the clustering;a structure;the text data;we;a hierarchy;structures
#s88	We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the feature value would be one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later.
#c88	We;text clustering;additional features;text data;we;documents;we;each cluster;a feature;we;a document;this cluster;the feature value;a document;this cluster;the future value;additional discrimination;texture classification;we
#s89	So there are in general many applications of text clustering any.
#c89	general many applications;text
#s90	I just saw it with two very specific ones.
#c90	I;it;two very specific ones
#s91	One is to cluster search results for example and You can imagine a search engine can cluster the search results so that user can see overall structure of those.
#c91	cluster search results;example;You;a search engine;the search results;user;overall structure
#s92	Results returned for a query.
#c92	Results;a query
#s93	And when the query is ambiguous, this is particularly useful.
#c93	the query
#s94	Becausw clusters likely represent different senses of ambiguous word.
#c94	Becausw clusters;different senses;ambiguous word
#s95	Another application is to understand the major complaints from customers based on their emails, right?
#c95	Another application;the major complaints;customers;their emails
#s96	So in this case we can cluster email messages and then find the major clusters.
#c96	this case;we;email messages;the major clusters
#s97	From there.
#s98	We can understand what other major complaints about.
#c98	We;what
410	6cf0ae0d-8ed2-4731-9da3-bcf60af9a356	13
#s1	this lecture is about a probabilistic retrieval model in this lecture we're going to continue the discussion of the text retrieval methods where can do look at another kind of very different the way to design ranking functions then the vectors based model that we discussed it before being probabilistic models we define the ranking function based on the probability that this document is relevant to this query in other words we are we introduce a binary random variable here this is the variable are here and we also assume that the query and the documents are observations from random variables note that in the vector space model we assume they're vectors but here we are assumed we assume they are the data observed from random variables and so the problem mod retrieval becomes to estimate the probability of relevance in this category of models there are different variants the classical problem is model has led to the BM twenty five retrieval function which we discuss in the vectors based model because it's a form is actually similar to affective space model in this lecture we will discuss another subclass in this big class call the language modeling approaches to retrieval in particular whether the discuss the query like hold retrieval model which is one of the most effective models in probabilistic models there is also another line called a divergent from randomness model which has led to the P L two function it's also one of the most effective state of the other retrieval functions in query like called our assumption is that this probability of relevance can be approximated at by the probability of query given a document and randomness so intuitively this probability just captures the following probability and that is if the user likes document D how likely would the user enter query Q in order to retrieve document so we assume that the user likes T because we have at relevance value here and then we asked the question about the how likely we will see this particular query from this user
#c1	this lecture;a probabilistic retrieval model;this lecture;we;the discussion;the text retrieval methods;another kind;ranking functions;the vectors;model;we;it;probabilistic models;we;the ranking function;the probability;this document;this query;other words;we;we;a binary random variable;the variable;we;the query;the documents;observations;random variables;the vector space model;we;they;vectors;we;we;they;the data;random variables;the problem;mod retrieval;the probability;relevance;this category;models;different variants;the classical problem;model;the BM twenty five retrieval function;we;the vectors;model;it;a form;affective space model;this lecture;we;another subclass;this big class call;the language;modeling approaches;retrieval;the query;retrieval model;the most effective models;probabilistic models;another line;a divergent;randomness model;the P L two function;it;the most effective state;the other retrieval functions;query;our assumption;this probability;relevance;the probability;query;a document;randomness;this probability;the following probability;the user;document D;the user;query Q;order;document;we;the user;T;we;relevance value;we;the question;we;this particular query;this user
#s2	so this is the basic idea not to understand this idea let's take a look at the general idea or the basic idea of probabilistic retrieval models so here i listed at some imagine the relevance status values or relevance judgments of queries and documents for example in this line shows that query one is a query that the user tightly and T one is a document that user has a scene and one means the user thinks D one is relevant to kill one so this are here can be also approximator by the click through data that a search engine can collect it by watching how you interact with the search results so in this case let's say the user clicked on this document so there is a one here similarly the user clicked on D two also so there is one here in other words
#c2	the basic idea;this idea;'s;a look;the general idea;the basic idea;probabilistic retrieval models;i;some imagine;the relevance status values;relevance judgments;queries;documents;example;this line;query;a query;T one;a document;user;a scene;one;the user;approximator;the click;data;a search engine;it;you;the search results;this case;'s;the user;this document;a one;the user;D;other words
#s3	D two is assumed to be relevant to kill one on the other hand these three is non relevant there's a zero here at the voiced non relevant and then D five is again relevant and so on so false and this part of maybe there are collected from a different user so this user type thing kill one and then found that the T one is actually not useful so D wines actually non relevant in contrast here we see it's relevant and or this could be the same query typing by the same user at different times but the two is also relevant etc
#c3	D;the other hand;the voiced non;D;this part;a different user;this user type thing;the T one;so D wines;contrast;we;it;the same query;the same user;different times
#s4	and then here we can see more data about other queries not we can imagine we have a lot of such data now we can ask the question how can we then estimate the probability of relevance
#c4	we;more data;other queries;we;we;a lot;such data;we;the question;we;the probability;relevance
#s5	right so how can we compute this probability of relevance or intuitively that just means if we look at it all the entries where we see this particular T and this particular Q how likely will see a one on the third column so basically that just means we can just collect these accounts we can first recount how many times we have seen Q and V as a pair in this table and then count how many times we actually have also seen one in the third column so
#c5	we;this probability;relevance;we;it;all the entries;we;this particular T;this particular Q;a one;the third column;we;these accounts;we;we;Q;V;a pair;this table;we;the third column
#s6	and then we just compute the ratio so let's take a look at some specific examples suppose we're trying to compute this probability for D one D two and D three for Q one what is the estimated power penalty now think about that you can pause the video if needed try to take a look at the table and try to give your estimate of the probability have you seen that if we are interested in Q one and one will be looking at these two pairs and in both cases well actually in one of the cases the user has said this is why this is random
#c6	we;the ratio;'s;a look;some specific examples;we;this probability;D;one D;D;Q;one;what;the estimated power penalty;you;the video;a look;the table;your estimate;the probability;you;we;Q one;these two pairs;both cases;the cases;the user
#s7	and so i is equal to one in only one of the two cases in the other case it's zero so that's one out of two what about T one and T two
#c7	i;the two cases;the other case;it;T;T
#s8	well the idea here windy two D one D two in both cases in this case size equal to one
#c8	the idea;two D;D;both cases;this case size
#s9	so it's a two out of two and so on so forth so you can see with this approach we can actually score these documents for the query
#c9	it;you;this approach;we;these documents;the query
#s10	right we now have a score for D one D two and three for this query we can simply rank them based on these probabilities and so that's the basic idea of probabilistically retrieval model and you can see it makes a lot of sense in this case it's going to rank D two above all the other documents becaus in all the cases when you have seen Q one and Y two is equal to one the user clicked on this document so this also should show that with a lot of click through there are a search engine can learn a lot from the data to improve their search engine this is a simple example that shows that with even small number of entries here we can already estimate some probabilities these probabilities would give us some sense about which document might be more relevant or more useful to a user typing this query now of course the problems that we don't observe all the queries in all the documents and all the relevance values there will be a lot of unseen documents in general he only collected data from the documents that we have shown to the users and there are even more unseen queries becaus you cannot predict the word queries would be typing by users so obviously this approach won't work if we apply to unseen queries or anything documents nevertheless this shows the basic idea of problems
#c10	we;a score;D;one D;this query;we;them;these probabilities;the basic idea;probabilistically retrieval model;you;it;a lot;sense;this case;it;D;all the other documents;all the cases;you;Q;Y;the user;this document;a lot;click;a search engine;a lot;the data;their search engine;a simple example;even small number;entries;we;some probabilities;these probabilities;us;some sense;which document;a user;this query;course;we;all the queries;all the documents;all the relevance values;a lot;unseen documents;he;data;the documents;we;the users;you;the word;users;this approach;we;unseen queries;anything documents;the basic idea;problems
#s11	material model
#c11	material model
#s12	and it makes sense intuitively so what do we do in such a case when we have a lot of unseen documents an answering queries or the solutions that we have to approximate in some way so in this particular case called query likelihood retrieval model we just approximate this by another conditional probability P of Q given the an all is equal to one so in the condition part we assume that the user likes the document because we have seeing that the user clicked on this document and this part shows that we're interested in how likely the user would actually enter this query how likely will see this query in the same role to note that here we have made an interesting assumption here basically we're going to assume that whether the user types in this query has something to do with whether user likes the document in other words we actually make the following assumption and that is a user formulas a query based on an imaginary realm in the document well if you just look at this this is conditional probability it's not obvious we're making this assumption so what i really meant is that to use this new conditional probability to help us score then this knew conditional probability we have to somehow be able to estimate this conditional probability without relying on this big table otherwise we would be having similar problems as before and by making this assumption we have some way to bypass this dick table and try to just model how the user formulas a query
#c12	it;sense;what;we;such a case;we;a lot;unseen documents;an answering queries;the solutions;we;some way;this particular case;query likelihood retrieval model;we;another conditional probability;P;Q;the condition part;we;the user;the document;we;the user;this document;this part;we;the user;this query;this query;the same role;we;an interesting assumption;we;the user types;this query;something;user;the document;other words;we;the following assumption;a user;a query;an imaginary realm;the document;you;conditional probability;it;we;this assumption;what;i;this new conditional probability;us;conditional probability;we;this conditional probability;this big table;we;similar problems;this assumption;we;some way;this dick table;the user;a query
#s13	OK so this is how you can simplify the the general model so that we can derive a specific training function later so let's look at the how this model work for our example basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user 's mind when the user formulates this query so we asked this question can we quantify the probability in this probability is conditional probability of observing this query if a particular document it is in fact the imagine relevant document in the user 's mind here you can see we compute all these query likelihood probabilities the likelihood of queries given each document once we have these values we can then rank these documents based on these values so to summarize the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable are here and then let's a scoring function be defined based on this conditional probability we also talked about approximating this by using the query likely hold and in this case we have a ranking function that's basically based on the probability of a query given the document and this probability should be interpreted as the probability that a user who like stocking the D would pose query Q now the question of course is how do we compute this conditional probability at this in general has to do with how to compute the probability of text because Q is attacks and this has to do with model called and language model and this kind of models are proposed to model text so more specifically we will be very interested in the following conditional probability 's issuing this here if the user like this document how likely the user would oppose this query ann in the next lecture working to give introduction to language model so that we can see how we can model text with the probabilistic model in general
#c13	you;the the general model;we;a specific training function;'s;this model;our example;what;we;this case;the following question;these documents;the imaginary relevant document;the user 's mind;the user;this query;we;this question;we;the probability;this probability;conditional probability;this query;a particular document;it;fact;the imagine relevant document;the user 's mind;you;we;all these query likelihood probabilities;the likelihood;queries;each document;we;these values;we;these documents;these values;the general idea;modern relevance;the probabilistic model;we;a binary random variable;'s;a scoring function;this conditional probability;we;the query;this case;we;a ranking function;the probability;a query;the document;this probability;the probability;a user;who;the D;query Q;the question;course;we;this conditional probability;the probability;text;Q;attacks;model;model;this kind;models;text;we;the following conditional probability;the user;this document;the user;this query ann;the next lecture;introduction;language model;we;we;text;the probabilistic model
410	6ea11c1e-e924-4648-9ba7-3c1596bb0a5b	88
#s1	This lecture is about the future of web search.
#c1	This lecture;the future;web search
#s2	In this lecture we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general.
#c2	this lecture;we;some possible future trends;web search;intelligent information retrieval systems
#s3	In order to further improve the accuracy of search engines, it's important to consider special cases of information need, so one particular trend could be to have more and more specialized and customized search engines and they can be called vertical search engines.
#c3	order;the accuracy;search engines;it;special cases;information;one particular trend;more and more specialized and customized search engines;they;vertical search engines
#s4	These vertical search engines can be expected to be more effective than the current general search engines because they could assume that the users are a special group of users that might have a common information need, and then the search engine can be customized to serve such users.
#c4	These vertical search engines;the current general search engines;they;the users;a special group;users;a common information need;the search engine;such users
#s5	And because of the customization is also possible to do personalization, so search can be personalized.
#c5	the customization;personalization;search
#s6	Because we have a better understanding of the users.
#c6	we;a better understanding;the users
#s7	Because of the restriction of the domain, we also have some advantages in handling the documents because we can have better understanding of documents.
#c7	the restriction;the domain;we;some advantages;the documents;we;better understanding;documents
#s8	For example, particular words may not be ambiguous in such a domain, so we can bypass the problem of ambiguity.
#c8	example;particular words;such a domain;we;the problem;ambiguity
#s9	Another trend that we can expect to see is The search engine will be able to learn over time.
#c9	Another trend;we;The search engine;time
#s10	It's like a lifetime learning or lifelong learning.
#c10	It;a lifetime learning;lifelong learning
#s11	And this is of course very attractive, because that means the search engine will self improve itself as more people are using it.
#c11	course;the search engine;itself;more people;it
#s12	Search engines will become better and better and this is already happening because search engines can learn from the implicit feedback, more users use it and the quality of the search results for the popular queries that are typed in by many users will likely become better.
#c12	Search engines;search engines;the implicit feedback;more users;it;the quality;the search results;the popular queries;many users
#s13	So this is another feature that we would see.
#c13	another feature;we
#s14	The third trend might be the integration of multi modes of information access.
#c14	The third trend;the integration;multi modes;information access
#s15	So search navigation and recommendation or filtering might be combined to form a full fledged information management system.
#c15	navigation;recommendation;filtering;a full fledged information management system
#s16	And in the beginning of this course we talked about the push versus pull.
#c16	the beginning;this course;we;the push;pull
#s17	These are different modes of information access, but these modes can be combined.
#c17	different modes;information access;these modes
#s18	And similarly, in the pull mode, querying and browsing could also be combined then in fact we're doing that basically today with the current search engines we are querying.
#c18	the pull mode;fact;we;the current search engines;we
#s19	Sometimes browsing, clicking on links.
#c19	links
#s20	Sometimes we've got some information recommended, although most of the cases information recommended is because of advertising.
#c20	we;some information;the cases;information;advertising
#s21	But in the future you can imagine seamlessly integrate the system with multi mode for information access.
#c21	the future;you;the system;multi mode;information access
#s22	And that would be convenient for people.
#c22	people
#s23	Another trend is that we might see systems that try to go beyond, search it supports the user tasks.
#c23	Another trend;we;systems;it;the user tasks
#s24	After all, the reason why people want to search is to solve a problem or to make a decision or perform a task.
#c24	the reason;people;a problem;a decision;a task
#s25	For example, consumers might search for opinions about products in order to purchase a product and choose a good product to buy.
#c25	example;consumers;opinions;products;order;a product;a good product
#s26	So in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product.
#c26	this case;it;the whole workflow;a product;a product
#s27	In this area, after the current search engines already provided good support, for example, you can sometimes look at the reviews and then if you want to buy it and you can just click on the button to go to the shopping site directly, get it done.
#c27	this area;the current search engines;good support;example;you;the reviews;you;it;you;the button;the shopping site;it
#s28	But it does not provide a good task support for many other tasks.
#c28	it;a good task support;many other tasks
#s29	For example for researchers, you might want to find the relevant literature or site of the literature, and then there's no support for finishing tasks such as writing a paper.
#c29	example;researchers;you;the relevant literature;site;the literature;no support;tasks;a paper
#s30	So in general, I think there are many opportunities to innovate, and so in the following a few slides I would be talking a little bit more about some specific ideas or thoughts that hopefully you can help you imagine new application possibilities.
#c30	I;many opportunities;the following a few slides;I;some specific ideas;thoughts;you;you;new application possibilities
#s31	Some of them might be already relevant to what you are currently working on.
#c31	them;what;you
#s32	In general, you can think about any intelligent system, especially intended information system, as being specified by these three nodes, and so if we connect these three into a triangle, then we'll be able to specify information system, and I call this data user service triangle.
#c32	you;any intelligent system;especially intended information system;these three nodes;we;a triangle;we;information system;I;this data user service triangle
#s33	So basically the three questions you ask would be who are you serving and what kind of data you are managing.
#c33	the three questions;you;who;you;what kind;data;you
#s34	And what kind of service you provide?
#c34	what kind;service;you
#s35	Right there, this would help us basically specify your system and there are many different ways to connect them, depending on how you connect them, you will have a different kind of system, so let me give you some example.
#c35	us;your system;many different ways;them;you;them;you;a different kind;system;me;you;some example
#s36	On the top, you can see different kinds of users, on the left side you can see different types of data or information, and on the bottom you can see different service functions.
#c36	the top;you;different kinds;users;the left side;you;different types;data;information;the bottom;you;different service functions
#s37	Now imagine we can connect all these in different ways.
#c37	we;different ways
#s38	So for example if you can connect everyone with web pages.
#c38	example;you;everyone;web pages
#s39	And the support search and browsing.
#c39	And the support search
#s40	What do you get?
#c40	What;you
#s41	That's web search, right?
#c41	web search
#s42	What if we connect the UIUC employees with organization documents or enterprise documents to support the search and browsing.
#c42	What;we;the UIUC employees;organization documents;enterprise documents;the search
#s43	That's enterprise search.
#c43	enterprise search
#s44	If you connect the scientists with literature information to provide all kinds of service, including search, browsing or alert of new relevant documents, or mining, analyzing research trends, or provide the task support or decision support, for example.
#c44	you;the scientists;literature information;all kinds;service;search;browsing;alert;new relevant documents;mining;research trends;the task support;decision support;example
#s45	You might be might be able to provide a support for automatically generating related work section for research paper, and this would be closer to task support, right?
#c45	You;a support;automatically generating related work section;research paper;task support
#s46	So then we can imagine this would be a literature assistant if we connect to online shoppers with blog articles or product reviews, then we can help these people to improve shopping experience so we can provide for example data mining capabilities to analyze the reviews to Compare products, compare sentiment of products and to provide a task support or decision support to help them choose what product to buy.
#c46	we;a literature assistant;we;online shoppers;blog articles;product reviews;we;these people;shopping experience;we;example;data mining capabilities;the reviews;Compare products;compare sentiment;products;a task support;decision support;them;what product
#s47	Or we can connect the customer service people with emails from the customers.
#c47	we;the customer service people;emails;the customers
#s48	And we can imagine a system that can provide analysis of these emails to find the major complaints of the customers.
#c48	we;a system;analysis;these emails;the major complaints;the customers
#s49	We can imagine the system could provide task support by automatically generating a response to the customer email, or maybe intelligently attach also promotion.
#c49	We;the system;task support;a response;the customer email;promotion
#s50	Message if appropriate if the detector that's a positive message, not a complaint, and then you might to take this opportunity to attach some promotion information.
#c50	Message;a positive message;a complaint;you;this opportunity;some promotion information
#s51	Whereas if it's a complaint that you might be able to.
#c51	it;a complaint;you
#s52	Automatically generate some generic response first and tell the customer that he or she can expect the detailed response later, etc.
#c52	some generic response;the customer;he;she;the detailed response
#s53	All these are trying to help people to improve the productivity.
#c53	people;the productivity
#s54	So this shows that the opportunities are really a lot, it's just only restricted by our imagination.
#c54	the opportunities;a lot;it;our imagination
#s55	So this picture shows the trend of the technology and also it characterizes the intelligent information system in three angles you can see in the center there is a triangle that connects keyword queries to search and bag of words representation.
#c55	this picture;the trend;the technology;it;the intelligent information system;three angles;you;the center;a triangle;keyword;bag;words;representation
#s56	That means the current search engines basically provides search support.
#c56	the current search engines;search support
#s57	Users and mostly model users based on keyword queries.
#c57	Users;users;keyword queries
#s58	And it sees the data through a bag of words representation.
#c58	it;the data;a bag;words;representation
#s59	So it's a very simple approximation of the actual information in the documents.
#c59	it;a very simple approximation;the actual information;the documents
#s60	But that's what the current system does.
#c60	what;the current system
#s61	It connects these three nodes in such a simple way.
#c61	It;these three nodes;such a simple way
#s62	Or it only provides you basic search function and doesn't really understand the user.
#c62	it;you basic search function;the user
#s63	And it doesn't really understand it that much information in the documents.
#c63	it;it;that much information;the documents
#s64	Now I showed some trends to push each node.
#c64	I;some trends;each node
#s65	Toward more advanced function, so think about the user node here so we can go beyond the keyword queries.
#c65	more advanced function;the user node;we;the keyword
#s66	Look at the user search history and then further model the user completely to understand the users task, environment, task need context or other information.
#c66	the user search history;then further model;the user;the users task;environment;task;context;other information
#s67	So this is pushing for personalization and complete the user modeling and this is a major direction in research.
#c67	personalization;the user modeling;a major direction;research
#s68	In order to build intelligent information systems.
#c68	order;intelligent information systems
#s69	On the document side.
#c69	the document side
#s70	We can also see we can go beyond bag of words representation to have entity relation representation.
#c70	We;we;bag;words;entity relation representation
#s71	This means we'll recognize peoples names their relations, locations etc
#c71	we;peoples;their relations;locations
#s72	and this is already feasible with today's natural language processing technique and Google's recent initiative on the Knowledge Graph.
#c72	today's natural language processing technique;Google's recent initiative;the Knowledge Graph
#s73	If you have heard of it, it's a good step toward this direction and once we can get to that level of representation in a robust manner at large scale, it can enable the search engine to provide much better service.
#c73	you;it;it;a good step;this direction;we;that level;representation;a robust manner;large scale;it;the search engine;much better service
#s74	In the future, we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent.
#c74	the future;we;knowledge representation;we;perhaps inference rules;the search engine
#s75	So this calls for large scale semantic analysis, and perhaps this is more feasible for vertical search engines.
#c75	large scale semantic analysis;vertical search engines
#s76	It's easier to make progress in a particular domain.
#c76	It;progress;a particular domain
#s77	Now on the service side, we see that we need to go beyond the search if support information access in general, so search is only one way to get access to information as well.
#c77	the service side;we;we;the search;information access;search;only one way;access;information
#s78	Recommender systems and push and pull are different ways to get access to relevant information, but going beyond access we also need to help people digest information.
#c78	Recommender systems;different ways;access;relevant information;access;we;people;information
#s79	Once the information is found and this step has to do with analysis of information or data mining.
#c79	the information;this step;analysis;information;data mining
#s80	We have to find the patterns or converted the text information into your knowledge that can be used in application or actionable knowledge that can be used for decision making.
#c80	We;the patterns;the text information;your knowledge;application;actionable knowledge;decision making
#s81	And furthermore the knowledge would be used to help user to improve productivity in finishing a task.
#c81	the knowledge;user;productivity;a task
#s82	For example, a decision making task.
#c82	example;task
#s83	So this is a trend and so basically in this dimension we anticipate in the futur.
#c83	a trend;this dimension;we;the futur
#s84	Intelligent Information systems will provide intelligent and interactive task support.
#c84	Intelligent Information systems;intelligent and interactive task support
#s85	Now I should also emphasize interactive here because it's important to optimize the combined intelligence of the users and the system so we can get some help from users in some natural way
#c85	I;it;the combined intelligence;the users;the system;we;some help;users;some natural way
#s86	and we don't have to assume the system has to do everything when the human user and the machine can collaborate in an intelligent way in an efficient way, then the combined intelligence will be high and in general we can minimize the user's overall effort in solving problem.
#c86	we;the system;everything;the human user;the machine;an intelligent way;an efficient way;the combined intelligence;we;the user's overall effort;problem
#s87	So this is the big picture of future intelligent information systems.
#c87	the big picture;future intelligent information systems
#s88	And this hopefully can provide us some insights about how to make further innovations on top of what we can do today.
#c88	us;some insights;further innovations;top;what;we
410	6ff3d617-b0b3-4f95-8b77-3aef7d9e0e76	9
#s1	so i showed you how we rewrite the query like hold mutual function into a form that looks like the formula from this slide after we make the assumption about the smoothing the language model based on the collection language model if you look at the this rewriting it actually would give us two benefits the first benefit is helps us better than standard this ranking function in particular we're going to show that from this formula we can see small thing with the collection language model with give us something like a TF IDF weighting and thence normalization the second benefit is that it also allows us to compute the query like order more efficiently in particular we see that the main part of the formula is a sum over the matching query terms so this is much better than if we take a sum over all the words after we spoke the document language model we essentially have nonzero probabilities for the words so this new form of the formula is much easier to score to compute it's also interesting to note that the last of time here is actually independent of the document since our goal is to rank the documents for the same query we can ignore this term for engin becaus
#c1	i;you;we;the query;mutual function;a form;the formula;this slide;we;the assumption;the language model;the collection language model;you;it;us;two benefits;the first benefit;us;this ranking function;we;this formula;we;small thing;the collection language model;us;something;a TF IDF weighting and thence normalization;the second benefit;it;us;the query;order;we;the main part;the formula;a sum;the matching query terms;we;a sum;all the words;we;the document language model;we;nonzero probabilities;the words;this new form;the formula;it;time;the document;our goal;the documents;the same query;we;this term;engin becaus
#s2	it's going to be the same for all the documents ignore it wouldn't affect the order of the documents inside of the sun we also see that each magical query at home would contribute wait
#c2	it;all the documents;it;the order;the documents;the sun;we;each magical query;home
#s3	and this wait actually is very interesting becaus it looks like a TF IDF weighting first we can already see it has a frequency of the world in the query just like in the vector space model when we take a top product we see the word frequency in the query to show up in such a song and so naturally this pot would correspond to the vector element from the document back there and here indeed we can see it actually encodes a weight that has similar factor too TF idea winning i let you examine it can you see it can you see which part is captured in TF and which part is capturing idea of waiting so if you want you can pause the video to think more about it so have you noticed that this piece obscene is related to the term frequency in the sense that if a word occurs very frequently in the document then the S major probability here would tend to be larger so this means this term is really doing something like a TF way now have you also notice that this time in the denominator is actually achieving the factor of idea why becaus this is the popularity of the term in the collection
#c3	very interesting becaus;it;a TF IDF weighting;we;it;a frequency;the world;the query;the vector space model;we;a top product;we;the word frequency;the query;such a song;this pot;the vector element;the document;we;it;a weight;similar factor;too TF idea;i;you;it;you;it;you;which part;TF;which part;idea;you;you;the video;it;you;this piece obscene;the term frequency;the sense;a word;the document;the S major probability;this term;something;a TF way;you;this time;the denominator;the factor;idea;the popularity;the term;the collection
#s4	but it's in the denominator so if the probability in the collection is larger than the width is after the smaller
#c4	it;the denominator;the probability;the collection;the width
#s5	and this means a popular term we actually have a smaller weight and this is precisely what idea of waiting is doing only that we now have a different form of TF an idea remember ideas has a log logarithm of document frequency
#c5	a popular term;we;a smaller weight;precisely what idea;waiting;we;a different form;TF;an idea;ideas;a log logarithm;document frequency
#s6	but here we have something different but intuitively it achieves similar fact interesting it we also have something related to the length normalization again can you see which factor is related documents in this formula well i just say that this term is related to IDF weighting this this collection probability
#c6	we;something;it;similar fact;it;we;something;the length normalization;you;which factor;related documents;this formula;i;this term;IDF
#s7	but it turns out that this time here is after related to document lands normalization in particular offers of D might be related to document land length so it encodes how much probability mass we want to give to unseen words how much smoothing do we want to do intuitively if documents along then we need to do less mosey becaus it we can assume the data is large enough we probably have observed all the words that the author could have written but the document is a short then offers update could be expected to be to be large we need to do more smoothing it's like that there are words that have not been written yet by the other so this term appears to penalize long document in that the office update would tend to be longer than larger than before long document but note that offers of the also occurs here
#c7	it;this time;document lands;normalization;particular offers;D;document land length;it;how much probability mass;we;unseen words;how much smoothing;we;documents;we;less mosey becaus;it;we;the data;we;all the words;the author;the document;update;we;it;words;this term;long document;the office update;long document;the also occurs
#s8	and so this may not actually be necessary paralyzing long documents that effect is not so clear
#c8	long documents;that effect
#s9	but as we will see later when we consider some specifically smoothing methods it turns out that they do penalize long documents just like in TF IDF weighting and documents normalization formulas in the vector space model so that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing we just need to assume that if we smooth with this collection language model then we would have a formula that looks like a TF IDF weighting and documents normalization what's also interesting that we have very fixed form of the ranking function and see we have not the heuristic lee put a logarithm here in fact that you can think about why we will have logarithm here if you look at the assumptions that we have made will be clear it's be cause we have i used logarithm of query like hold for scoring and we turned the product into a sum of logarithm of probability and that's why we have this lower note that if we only want to heuristically implement the TF waiting an idea of waiting we don't necessarily have to have along with them here imagine if we drop this logarithm we would still have TF an idea of waiting but what's nice with probabilistic modeling is that we are automatically given a logarithm function here and that's basically a fixed reform of the formula that we did not really have to heuristic design and in this case if you try to drop this logarithm the model probably work as well as if you keep along with so a nice property of probabilistic modeling is that by following some assumptions and probability rules will get a formula automatically and the formula would have a particular form like in this case and if we heuristic it designed the formula we may not necessarily end up having such a specific form so to summarize we talked about the need for smoothing a document language model otherwise with gave zero probability for unseen words in the document and that's not good for scoring a query with such a unseen world it's also necessary in general to improve the accuracy of estimating the model represent the topic of this document the general idea of smoothing in retrieval is it will use the collecting language model two give us some cruel power the witch and seen was should have a higher probability that is the probability of an cemboard is assumed that would be proportional to its probability in the collection with this assumption we've shown that we can derive a general ranking formula for query like hold that has the effect of TF IDF weighting and documents normalization we also see that through some rewriting the scoring of such a ranking function is primary based on sum of weights on match query times just like in the vector space model but the actual ranking function is given us automatically by the probability rules and assumptions that we have made and liking the vector space model where we have to heuristically think about the form of the function however we still need to address the question how exactly we should SMS a document language model how exactly we should use reference language model based on the collection to adjust to the probability of the maximum micro S maiden and this is the topic of the next election
#c9	we;we;some specifically smoothing methods;it;they;long documents;TF IDF weighting and documents normalization formulas;the vector space model;a very interesting observation;it;we;the specific way;we;we;this collection language model;we;a formula;a TF IDF weighting and documents normalization;what;we;very fixed form;the ranking function;we;the heuristic lee;a logarithm;fact;you;we;logarithm;you;the assumptions;we;it;we;i;logarithm;query;hold;scoring;we;the product;a sum;logarithm;probability;we;this lower note;we;the TF;an idea;we;them;we;we;TF;an idea;what;probabilistic modeling;we;a logarithm function;a fixed reform;the formula;we;heuristic design;this case;you;the model;you;a nice property;probabilistic modeling;some assumptions;probability rules;a formula;the formula;a particular form;this case;we;it;the formula;we;such a specific form;we;the need;a document language model;zero probability;unseen words;the document;a query;such a unseen world;it;the accuracy;the model;the topic;this document;the general idea;retrieval;it;the collecting language model;us;some cruel power;the witch;a higher probability;the probability;an cemboard;its probability;the collection;this assumption;we;we;a general ranking formula;query;hold;the effect;TF IDF weighting and documents normalization;we;the scoring;such a ranking function;sum;weights;match query times;the vector space model;the actual ranking function;us;the probability rules;assumptions;we;the vector space model;we;the form;the function;we;the question;we;a document language model;we;reference language model;the collection;the probability;the maximum micro S maiden;the topic;the next election
410	72313ed1-d189-4ee8-9d34-7e28faae4ce2	108
#s1	This lecture is about the recommender systems.
#c1	This lecture;the recommender systems
#s2	So far we have talked about a lot of aspects of search engines.
#c2	we;a lot;aspects;search engines
#s3	We have talked about the problem of search and ranking problem, different methods for ranking implementation of a<br> search engine and how to evaluate the search engine etc.
#c3	We;the problem;search;ranking problem;different methods;ranking implementation;a<br> search engine
#s4	The.
#s5	This is probably because of we know that web search engines are by far the most important applications of text retrieval and they are the most useful tools to help people convert big raw text data into a small set of relevant documents.
#c5	we;web search engines;the most important applications;text retrieval;they;the most useful tools;people;big raw text data;a small set;relevant documents
#s6	Another reason why we spend so many lectures on search engines is because many techniques used in search engines are actually also very useful for recommender systems, which is the topic of this lecture.
#c6	Another reason;we;so many lectures;search engines;many techniques;search engines;recommender systems;the topic;this lecture
#s7	And so overall, the two systems are actually well connected and there are many techniques that are shared by them.
#c7	the two systems;many techniques;them
#s8	So this is a slide that you have seen before when we talked about the two different modes of text access - pull and push.
#c8	a slide;you;we;the two different modes;text access - pull;push
#s9	An we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take initiative to recommend the information to user or push a relevant information to the user.
#c9	we;recommender systems;the main systems;users;the push mode;the systems;initiative;the information;user;a relevant information;the user
#s10	And this often works well when the user has a relatively stable information need.
#c10	the user;a relatively stable information need
#s11	When the system has a good knowledge about what the user wants.
#c11	the system;a good knowledge;what;the user
#s12	So a recommender system is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the useless articles.
#c12	a recommender system;a filtering system;it;useful items;people;the useless articles
#s13	And so in this sense they are kind of similar.
#c13	this sense;they
#s14	And in all these cases, the system must make a binary decision, and usually there's a dynamic source of information items.
#c14	all these cases;the system;a binary decision;a dynamic source;information items
#s15	And you have some knowledge about this user's interest and then the system would make a delivery decision whether this item is interesting to the user and then if he's interested in, then the system would recommend the article to the user.
#c15	you;some knowledge;this user's interest;the system;a delivery decision;this item;the user;he;the system;the article;the user
#s16	So the basic of filtering question here is really, will this user like this item.
#c16	the basic;filtering question;this user;this item
#s17	Will you like item X?
#c17	you;item X
#s18	And there are two ways to answer this question.
#c18	two ways;this question
#s19	If you think about it, I wanted to look at what items you like.
#c19	you;it;I;what items;you
#s20	And then we can see if X actually like those items.
#c20	we;those items
#s21	The other is to look at the who likes X
#c21	who;X
#s22	and we can see if this user looks like a one of those users.
#c22	we;this user;those users
#s23	or like most of those users.
#c23	those users
#s24	And these strategies can be combined if we follow the first strategy that look at item similarity.
#c24	these strategies;we;the first strategy;item similarity
#s25	In the case of recommending text objects.
#c25	the case;recommending text objects
#s26	Then we are talking about the content based filtering or content based recommendation.
#c26	we;the content based filtering;recommendation
#s27	If we look at the second strategy, then it will compare users and in this case will exploit the user similarity and the technique is often called collaborative filtering.
#c27	we;the second strategy;it;users;this case;the user similarity;the technique;collaborative filtering
#s28	So let's first look at the content based filtering system.
#c28	's;the content based filtering system
#s29	This is what the system would look like.
#c29	what;the system
#s30	Inside the system there will be a binary classifier that would have some knowledge above the users‘ interest.
#c30	the system;a binary classifier;some knowledge;the users;interest
#s31	And it's called a user interest profile.
#c31	it;a user interest profile
#s32	It maintains this profile to keep track of the users‘ interest.
#c32	It;this profile;track;the users;interest
#s33	And then there was a utility function to guide the user to make decisions, and I explained the utility function in the moment.
#c33	a utility function;the user;decisions;I;the utility function;the moment
#s34	It helps the system decide where to set the threshold.
#c34	It;the system;the threshold
#s35	And then the accepted document will be those that have passed the threshold according to the classifier.
#c35	the accepted document;the threshold;the classifier
#s36	There should be also an initialization module that would take a users input, maybe from a users specified keywords or chosen category etc.
#c36	an initialization module;a users input;a users;keywords
#s37	And this will be to feed the system with the initial user profile.
#c37	the system;the initial user profile
#s38	There is also typically a learning module that will learn from users feedback overtime.
#c38	a learning module;users;feedback overtime
#s39	Now.
#s40	Note that in this case typical users information need is stable, so the system would have a lot of opportunities to observe the users, if the user has taken a recommended item has viewed that, and this is the signal to indicate that the recommended item may be relevant if the user discarded it, it's not relevant, and so such feedback can be a long term feedback and can last for a long time.
#c40	this case;typical users information;the system;a lot;opportunities;the users;the user;a recommended item;the signal;the recommended item;the user;it;it;such feedback;a long term feedback;a long time
#s41	And the system Clock collect a lot of information about these users interest and this can then be used to improve the classifier.
#c41	the system;Clock;a lot;information;these users interest;the classifier
#s42	Now what's the criteria for evaluating such a system?
#c42	what;the criteria;such a system
#s43	How do we know this filtering system actually performs well?
#c43	we;this filtering system
#s44	Now, in this case we cannot use the ranking evaluation measures like a map because we can afford waiting for a lot of documents and then rank the documents to make a decision for the user.
#c44	this case;we;the ranking evaluation measures;a map;we;a lot;documents;the documents;a decision;the user
#s45	And so the system must make a decision in real time in general to decide whether the item is above the threshold or not.
#c45	the system;a decision;real time;the item;the threshold
#s46	So in other words, we're trying to decide the absolute relevance.
#c46	other words;we;the absolute relevance
#s47	So in this case, one commonly used strategies is user utility function to evaluate the system.
#c47	this case;one commonly used strategies;user utility function;the system
#s48	So here I show a linear utility function that's defined as for example 3 multiplied by the number of good items that you delivered minus 2 multiplied by the number of bad items you deliver.
#c48	I;a linear utility function;example;the number;good items;you;the number;bad items;you
#s49	So in other words, we could kind of just.
#c49	other words;we
#s50	treat this as almost a in a gambling game.
#c50	a gambling game
#s51	If you delete, if you deliver one good item, let's say you win $3, you gain $3.
#c51	you;you;one good item;'s;you;you
#s52	But if you deliver a better one and you will lose $2 and this utility function basically kind of measures how much money you will get by doing this kind of game.
#c52	you;a better one;you;measures;how much money;you;this kind;game
#s53	And so it's clear that if you want to maximize this utility function, your strategy should be to deliver as many good articles as possible and to minimize the delivery of bad articles, that's obvious.
#c53	it;you;this utility function;your strategy;as many good articles;the delivery;bad articles
#s54	One interesting question here is how should we set these coefficients?
#c54	One interesting question;we;these coefficients
#s55	Now I just showed 3 and negative 2 as a possible coefficients.
#c55	I;a possible coefficients
#s56	But one can ask the question, are they reasonable?
#c56	one;the question;they
#s57	So what do you think?
#c57	what;you
#s58	Do you think that's a reasonable choice?
#c58	you;a reasonable choice
#s59	What about the other choices?
#c59	the other choices
#s60	And also for example we can have 10 and minus one.
#c60	example;we
#s61	Or one minus ten.
#s62	What's the difference?
#c62	What;the difference
#s63	What do you think?
#c63	What;you
#s64	How would this utility function affect the system's threshold decision?
#c64	this utility function;the system's threshold decision
#s65	But you can think of these two extreme cases, 10&nbsp; -1 versus 1 -10.
#c65	you;these two extreme cases
#s66	Which one do you think it would encourage the system to overdeliver and which one would encourage the system to be conservative?
#c66	Which one;you;it;the system;one;the system
#s67	If you think about it, they will see that when we get a big award for delivering a good document, you incur only a small penalty for delivering bad one.
#c67	you;it;they;we;a big award;a good document;you;only a small penalty
#s68	Intuitively, you would be encouraged to deliver more right?
#c68	you
#s69	And you can try to deliver more.
#c69	you
#s70	In hopes of getting a good one delivered, and then you'll get a big award.
#c70	hopes;a good one;you;a big award
#s71	I saw, on the other hand, if you choose 1 -- 10, you don't really get such a big price if you deliver deliver a good document.
#c71	I;the other hand;you;you;such a big price;you;a good document
#s72	On the other hand, you will have a big loss if you deliver a bad one.
#c72	the other hand;you;a big loss;you;a bad one
#s73	You can imagine that the system would be very reluctant to deliver a lot of documents.
#c73	You;the system;a lot;documents
#s74	It has to be absolutely sure that it's not a non-relevant one.
#c74	It;it;a non-relevant one
#s75	So this utility function has to be designed based on the specific application.
#c75	this utility function;the specific application
#s76	Three basic problems in content based filtering are the following.
#c76	Three basic problems;content based filtering;the following
#s77	Frst it has to make a filtering decision so it has to be a binary decision maker binary classifier given a text.
#c77	it;a filtering decision;it;a binary decision maker;binary classifier;a text
#s78	Text document and profile description of the user.
#c78	Text document and profile description;the user
#s79	It has to say yes or no, whether this document should be delivered or not.
#c79	It;this document
#s80	So that's the decision module and there should be a initialization module.
#c80	the decision module;a initialization module
#s81	As you have seen earlier and this is to get the system started.
#c81	you;the system
#s82	And we have to initialize the system based on only very limited text description or very few examples from the user.
#c82	we;the system;only very limited text description;very few examples;the user
#s83	And the third component is a learning module which he had to be able to learn from limited relevance judgments.
#c83	the third component;a learning module;he;limited relevance judgments
#s84	Because we count in learn from the user about their preferences on the delivered documents if we don't deliver document to the user, we would never know would never be able to know whether the user likes it or not.
#c84	we;learn;the user;their preferences;the delivered documents;we;document;the user;we;the user;it
#s85	And we can accumulate a lot of documents and learn from the entire history and all these modules would have to be optimized to maximize the utility.
#c85	we;a lot;documents;the entire history;all these modules;the utility
#s86	So how can we build such a system?
#c86	we;such a system
#s87	And there are many different approaches.
#c87	many different approaches
#s88	here.
#s89	Here we're going to talk about how to extend retrieval system.
#c89	we;retrieval system
#s90	A search engine for information filtering.
#c90	A search engine;information filtering
#s91	Again, here's why.
#s92	We've spent a lot of time to talk about the search engines because it's actually not very hard to extend the search engine for information filtering.
#c92	We;a lot;time;the search engines;it;the search engine;information filtering
#s93	So here's the basic idea for extending a retrieval system for information filtering.
#c93	the basic idea;a retrieval system;information filtering
#s94	First we can reuse a lot of retrieval techniques to do scoring, right, so we know how to score documents against queries, etc.
#c94	we;a lot;retrieval techniques;scoring;we;documents;queries
#s95	We can measure the similarity between profile text, description and document, and then we can use the score threshold for the filtering decision.
#c95	We;the similarity;profile text;description;document;we;the score threshold;the filtering decision
#s96	We do retrieval, and then we kind of find the scores of documents and then we apply a threshold to see whether document is passing the threshold or not, and if it's passing the threshold we are going to say it's relevant, and we're going to deliver it to the user.
#c96	We;retrieval;we;the scores;documents;we;a threshold;document;the threshold;it;the threshold;we;it;we;it;the user
#s97	And another component that we have to add is for is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring.
#c97	another component;we;course;the history;we;the traditional feedback techniques;scoring
#s98	And we know Rocchio can be used for scoring improvement.
#c98	we;Rocchio;scoring improvement
#s99	And but we have to develop a new approaches to learn how to set the threshold
#c99	we;a new approaches;the threshold
#s100	and we need to set it initially
#c100	we;it
#s101	and then we have to learn how to update the threshold overtime.
#c101	we;the threshold overtime
#s102	So here's what the system might look like if we just generalize the vector space model for filtering problems.
#c102	what;the system;we;the vector space model;filtering problems
#s103	Right, so you can see the document vector could be fed into a scoring module which is already exists in a search engine that implements a vector space model and the profile will be treated as a query essentially
#c103	you;the document vector;a scoring module;a search engine;a vector space model;the profile;a query
#s104	and then the profile vector can be matched with the document vector to generate the score.
#c104	the profile vector;the document vector;the score
#s105	And then this score would be fed into a threshold module that would say yes or no, and then the evaluation would be based on utility for the filtering results.
#c105	this score;a threshold module;the evaluation;utility;the filtering results
#s106	If it says yes and then the documents will be sent to the user and then the user could give some feedback and the feedback information would have been ,would be used to both adjust to the threshold and to adjust the vector representation so the vector learning is essentially the same as query modification or feedback.
#c106	it;the documents;the user;the user;some feedback;the feedback information;the threshold;the vector representation;the vector learning;query modification;feedback
#s107	in the case of search.
#c107	the case;search
#s108	The threshold of learning is new component that we need to talk a little bit more about.
#c108	The threshold;new component;we
410	73582b0c-e010-4f39-891d-0c7837768c71	22
#s1	and here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of an object to buy active user using the ratings of similar users to this active user this is called memory based approach becaus its deliberative similar to storing all the using information and when we are considering a particular user we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that uses information about those users to predict the preference of this user
#c1	we;basic strategy;similarity;users;the rating;an object;active user;the ratings;similar users;this active user;memory based approach becaus;all the using information;we;a particular user;we;the relevant users;the similar users;this user case;information;those users;the preference;this user
#s2	so here's the general idea
#c2	the general idea
#s3	and we used some notations here so X sub by JD notes rating of object OJ by user UI and ends up i is average rating of all objects by this user so this N I is needed becaus we would like to normalize the railings of objects by this user
#c3	we;some notations;X sub;JD;object;user UI;i;average rating;all objects;this user;this N;I;becaus;we;the railings;objects;this user
#s4	so how do you do normalization well we're going to just subtract the average rating from all the ratings now this is it'll normalize these ratings so that the ratings from different users would be comfortable because some users might be more generous and they generally have high ratings but some others might be more critical so their ratings cannot be directly compared with each other or aggregated them together
#c4	you;normalization;we;the average rating;all the ratings;it;these ratings;the ratings;different users;some users;they;high ratings;some others;their ratings;them
#s5	so we need to do this normalization now the prediction of the rating on the item by another user or active user use up a here can be based on the average ratings of similar users so the user you subway is the user that we are interested in recommending items to an we now interested in recommending this old subjects
#c5	we;this normalization;the prediction;the rating;the item;another user;active user;the average ratings;similar users;the user;you;the user;we;items;an we;this old subjects
#s6	so we're interested in knowing how likely this user will like this object now how do we know that where the idea here is to look at the weather similar users to this user have liked this object so mathematically this is to say well the predicted rating of this user on this app object user a on object OJ is basically combination of the normalized the ratings of different users and in fact here we're taking a sum over all the users but not all users contribute equally to the average and this is controlled by the weights so this wait controls are inference of user on the prediction and of course naturally this way that should be related to the similarity between UA and
#c6	we;this user;this object;we;the idea;the weather;similar users;this user;this object;the predicted rating;this user;this app object user;object;OJ;combination;the normalized the ratings;different users;fact;we;a sum;all the users;not all users;the average;the weights;controls;inference;user;the prediction;course;this way;the similarity;UA
#s7	this particular user UI the more similar they are then the more contribution would like user UI to make in predicting the preference of your a so the formula is extremely simple you can see it's a sum of all the possible users an insider some we have their ratings weather normalize ratings as i just explained the regions needs to be normalized in order to be comfortable with each other and then these ratings are weighted by their similarity so you can imagine W of an eye is just a similarity of user and user i now what's K here where katie is simply normalizer
#c7	this particular user;UI;they;the more contribution;user UI;the preference;your;the formula;you;it;a sum;all the possible users;an insider;we;their ratings weather;ratings;i;the regions;order;these ratings;their similarity;you;W;an eye;just a similarity;user;user;i;what;K;katie;normalizer
#s8	it's just it's just one over the sum of all the weights of all the users
#c8	it;it;the sum;all the weights;all the users
#s9	and so this means basically if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users
#c9	you;the weight;K;we;coefficients;weights;all the users
#s10	and it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction
#c10	it;just a normalization strategy;you;rating;the same range;the these ratings;we;the prediction
#s11	right
#s12	so this is basically the main idea of memory based approaches for collaborative filtering once we make this prediction we also would like to map back to the rating that the user the user would actually make and this is true further add the mean rating or average rating of this user use of a through the predicted value this would recover a meaning for rating for this user
#c12	the main idea;memory based approaches;collaborative filtering;we;this prediction;we;the rating;the user;the user;the mean rating;average rating;this user use;the predicted value;a meaning;rating;this user
#s13	so if this user is generous then the average it would be is somewhat high and when we added that the rating will be adjusted to a relatively high rating now when you recommend the items we use this actually doesn't really matter 'cause you are interested in basically the normalized rating that's more meaningful but when they evaluate these collaborative filtering approach is that typically assume the actual ratings of the user on these objects to be unknown and then you do the prediction and then you compare the predictor ratings with their actual ratings so they do have access to their actual ratings but then you pretend you don't know
#c13	this user;the average;it;we;the rating;a relatively high rating;you;the items;we;you;basically the normalized rating;they;these collaborative filtering approach;the actual ratings;the user;these objects;you;the prediction;you;the predictor ratings;their actual ratings;they;access;their actual ratings;you;you
#s14	and then you compare your systems predictions with the actual ratings in that case obviously the systems predicting would have to be adjusted to match the actual ratings of the user and this is what's happening here basically OK
#c14	you;your systems predictions;the actual ratings;that case;the systems;the actual ratings;the user;what
#s15	so this is the memory based approach now of course if you look at the formula if you want to write the program to implement it you still face the problem of determining what is this W function right once you know the W function then the formula is very easy to implement so indeed there are many different ways to compute this function for this weight W and specific approaches general differ in how this is computed so here are some possibilities an you can imagine there are many other possibilities one popular approach is we use the pearson correlation coefficient this would be a sum over common rated items and the formula is standard a pearson correlation coefficient formula as assume here so this basically meshes weather two users tend to all gave higher ratings two similar items or lower ratings two similar items another measure is the cosine measure and this is retrieved the rating vectors as vectors in the vector space
#c15	the memory based approach;course;you;the formula;you;the program;it;you;the problem;what;this W function;you;the W function;the formula;many different ways;this function;this weight W and specific approaches;some possibilities;you;many other possibilities;one popular approach;we;the pearson correlation coefficient;a sum;common rated items;the formula;standard a pearson correlation coefficient formula;assume;weather;two users;higher ratings;two similar items;lower ratings;two similar items;another measure;the cosine measure;the rating vectors;vectors;the vector space
#s16	and then we're going to measure the angle an computer the cosine of the angle of the two vectors and this measure has been using the vectors based model for retrieval as well so as you can imagine there are many different ways of doing that in all of these cases note that the user similarity is based on their preferences on items and we did not actually use any content information of these items it didn't matter waht these items are they can be movies we can be books they can be product they can be tax documents we just didn't care about the content ann
#c16	we;the angle;an computer;the cosine;the angle;the two vectors;this measure;the vectors;model;retrieval;you;many different ways;these cases;the user similarity;their preferences;items;we;any content information;these items;it;these items;they;movies;we;books;they;product;they;tax documents;we;the content ann
#s17	so this allows such approach it will be applied through a wide range of problems now in some new approaches of course we would like to use more information about the user clearly we know more about the user not just these preferences on these items i saw in a actual filtering system is in collaborative filtering we could also combine that with content based filtering we could use more context information and those are all interesting approaches that people are still starting there are new approaches and propose but this memory based approach it has been shown to work reasonably well
#c17	such approach;it;a wide range;problems;some new approaches;course;we;more information;the user;we;the user;not just these preferences;these items;i;a actual filtering system;collaborative filtering;we;content based filtering;we;more context information;interesting approaches;people;new approaches;propose;it
#s18	and it's easy to implement in practical applications this could be a starting point to see if the strategy works well for your application so there are some obvious ways to also improve this approach an maybe would like to improve the user similarity measure
#c18	it;practical applications;a starting point;the strategy;your application;some obvious ways;this approach;the user similarity measure
#s19	and there are some practical issues with deal with here as well so for example there will be a lot of missing values what do you do with them you can set them to default values or the average ratings of the user and that would be a simple solution
#c19	some practical issues;deal;example;a lot;missing values;what;you;them;you;them;values;the average ratings;the user;a simple solution
#s20	but there are the monster approaches that can actually try to predict those missing values and then use the predicted values through improved the similarity so in fact the memory based approach it can predict those missing values right so you can imagine you have iterative approach where you first do some pre memory prediction and then you can use the predictor values to further improve the similarity function so this is here is the way to solve the problem and the square is obviously would affect the performance of collaborative filtering just like any other heroes heuristic to improve these similarity functions not idea which is actually very similar to the idea of IDF that we've seen text research is called inverse user frequency or I U F now here the idea is to look at the where the two users share similar ratings if the item is a popular item that has been viewed by many people an seeing these two people interested in this item may not be so interesting
#c20	the monster approaches;those missing values;the predicted values;the similarity;fact;the memory based approach;it;those missing values;you;you;iterative approach;you;some pre memory prediction;you;the predictor values;the similarity function;the way;the problem;the square;the performance;collaborative filtering;any other heroes;these similarity functions;not idea;the idea;IDF;we;text research;inverse user frequency;the idea;the two users;similar ratings;the item;a popular item;many people;these two people;this item
#s21	but if it's a rare item it has not been viewed by many users but these two users viewed this item and they gave similar ratings and that says more about their similarity
#c21	it;a rare item;it;many users;these two users;this item;they;similar ratings;their similarity
#s22	so it's kind of to emphasize more on similarity on items that are not viewed by many users
#c22	it;similarity;items;many users
410	74e36652-0c1a-4d5f-a708-0f55c1eb651f	126
#s1	So let's take a look at this in detail.
#c1	's;a look;detail
#s2	So in this random surfing model.
#c2	this random surfing model
#s3	At any page would assume random surfer would choose the next page to visit, so this is a small graph here.
#c3	any page;random surfer;the next page;a small graph
#s4	That's of course oversimplification of the complicated web, but let's say there are four documents here, d1 d2 d3 and d4, and let's assume that a random surfer or random walker can be on any of these pages.
#c4	course;oversimplification;the complicated web;'s;four documents;d1 d2 d3;d4;'s;a random surfer;random walker;these pages
#s5	And then the random surfer could decide to just randomly jump into any page.
#c5	the random surfer;any page
#s6	Or follow a link and then visit the next page.
#c6	a link;the next page
#s7	So if the random server is at d1.
#c7	the random server;d1
#s8	Then With some probability that random surfer will follow the links.
#c8	some probability;random surfer;the links
#s9	Now there are two out links here.
#c9	two out links
#s10	One is pointing to d3, the other is pointing to d4, so the random surfer could pick any of these two to reach d3 and d4.
#c10	d4;the random surfer;d3;d4
#s11	But it also assumes that the random surfer might get bored sometimes, so the random surfer will decide to ignore the actual links and simply randomly jump to any page on the web.
#c11	it;the random surfer;the random surfer;the actual links;any page;the web
#s12	So if it does that, it would be able to reach any of the other pages, even though there's no link directly from d1 to that page.
#c12	it;it;the other pages;no link;d1;that page
#s13	So this is assumed random surfing model.
#c13	random surfing model
#s14	Imagine a random surfer is really doing a surfing like this.
#c14	a random surfer;a surfing
#s15	Then we can ask the question how likely on average the surfer would actually reach a particular page like d1 or d2 or d3 really, that's the average probability of visiting a particular page.
#c15	we;the question;the surfer;a particular page;d1;d2;d3;the average probability;a particular page
#s16	And this probability is precisely what Pagerank computes.
#c16	this probability;precisely what
#s17	So the Pagerank score of the document is the average probability that the surfer visits a particular page.
#c17	the Pagerank score;the document;the average probability;the surfer;a particular page
#s18	Now, intuitively, this would basically capture the in link account.
#c18	link
#s19	Why?
#s20	Because if a page has a lot of in links, then it would have a higher chance of being visited because there will be more opportunities of having the surfer to follow a link to come to this page.
#c20	a page;a lot;links;it;a higher chance;more opportunities;the surfer;a link;this page
#s21	And this is why.
#s22	The random surfing model actually captures the idea of counting the in links.
#c22	The random surfing model;the idea;links
#s23	Note that it also considers the indirect in links.
#c23	it;links
#s24	Why?
#s25	Because if the pages that point to you have themselves a lot of in links.
#c25	the pages;you;themselves;a lot;links
#s26	That would mean the random surfer will very likely reach one of them, and therefore it increases the chance of visiting you.
#c26	the random surfer;them;it;the chance;you
#s27	So this is a nice way to capture both indirect and direct links.
#c27	a nice way;both indirect and direct links
#s28	So mathematically, how can we compute this probability in order to see that we need to take a look at how this probability is computed.
#c28	we;this probability;order;we;a look;this probability
#s29	So first, let's take a look at the transition matrix here.
#c29	's;a look;the transition matrix
#s30	And this is just the metrics with values indicating how likely I ran.
#c30	just the metrics;values;I
#s31	The random surfer will go from one page to another, so each row stands for a starting page.
#c31	The random surfer;one page;each row;a starting page
#s32	For example, row one would indicate the probability of going to any other 4 pages from d1, and here we see there are only two non zero entries, each is 1 over 2.
#c32	example;row;one;the probability;any other 4 pages;d1;we;only two non zero entries
#s33	So, this is because if you look at the graph d1 is pointing to d3 and d4, there is no link from d1 to d1 itself or d2, so we've got zeros for the first 2 columns and .5 for d3 and d4.
#c33	you;the graph;d1;no link;d1;itself;d2;we;zeros;the first 2 columns;d3;d4
#s34	In general, the element in this matrix M sub i, j is the probability of going from d,i to d,j and obviously for each row the values should sum to 1 because the surfer would have to go to precisely one of these other pages, right?
#c34	the element;this matrix;M sub;i;j;the probability;d;i;d;each row;the values;the surfer;these other pages
#s35	So this is the transition matrix.
#c35	the transition matrix
#s36	Now, how can we compute the probability of a surfer visiting a page?
#c36	we;the probability;a surfer;a page
#s37	If you look at the surf model then basically we can compute the probability of reaching a page as follows.
#c37	you;the surf model;we;the probability;a page
#s38	So.
#s39	Here on the left hand side you see it's the probability of visiting page d,j at time T + 1, so it's the next time point.
#c39	the left hand side;you;it;the probability;visiting page d;time;it;the next time point
#s40	On the right hand side you can see the equation involves the probability of at Page d,i at time T.
#c40	the right hand side;you;the equation;the probability;Page d;i;time
#s41	So you can see the subscript index t here, and that indicates that the probability that the surfer was at a document at time t.
#c41	you;the subscript index t;the probability;the surfer;a document;time
#s42	So.
#s43	The equation basically captures the two possibilities of reaching at d,j at time T + 1.
#c43	The equation;the two possibilities;d;time
#s44	What are these two possibilities?
#c44	What;these two possibilities
#s45	One is through random surfing and one is through following a link as we just explained.
#c45	random surfing;one;a link;we
#s46	So the first part captures the probability that the random surfer would reach this page by following a link, and you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume and so there is a factor of 1 minus alpha here, but the main part is really sum over all the possible pages that the surfer could have been at time t. There are N pages, so it's a sum over all the possible N pages.
#c46	the first part;the probability;the random surfer;this page;a link;you;the random surfer;this strategy;probability;alpha;we;a factor;1 minus alpha;the main part;all the possible pages;the surfer;time;t.;N pages;it;a sum;all the possible N pages
#s47	Inside the sum is a product of two probabilities.
#c47	the sum;a product;two probabilities
#s48	One is the probability that the surfer was at d,i the time t. That's p sub t of d,i.
#c48	the probability;the surfer;d;i;t.;p;sub t;d;i.
#s49	The other is the transition probability from the d,i to d,j. And so in order to reach this d,j page, the surfer must first be at d,i at time t and then also would have to follow the link to go from the d,i to d,j.
#c49	the transition probability;the d;i;d;j.;order;this d;j page;the surfer;d;i;time t;the link;the d;i;d;j.
#s50	So the probability is the probability of being at d,i at time t multiplied by the probability of going from that page to the target page.
#c50	the probability;the probability;d;i;time t;the probability;that page;the target page
#s51	d,j here.
#c51	j
#s52	The second part is a similar sound.
#c52	The second part;a similar sound
#s53	The only difference is that now the transition probability is a uniform transition probability of 1 / N and this part of captures the probability of reaching this page through random jumping.
#c53	The only difference;the transition probability;a uniform transition probability;1 / N;captures;the probability;this page;random jumping
#s54	Right, so the form is exactly the same and is.
#c54	the form
#s55	This also allows us to see why Pagerank essentially assume the smoothing of the transition matrix.
#c55	us;Pagerank;the smoothing;the transition matrix
#s56	If you think about this 1 / N as coming from another transition matrix that has all the elements being 1 / N The uniform matrix, then you can see very clearly.
#c56	you;this 1 / N;another transition matrix;all the elements;1 / N;you
#s57	Essentially we can merge the two parts.
#c57	we;the two parts
#s58	And because they are of the same form, we can imagine there's a different matrix that's a combination of this M and that uniform matrix, where every element is 1 / N, and in this sense Pagerank uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix.
#c58	they;the same form;we;a different matrix;a combination;this M;that uniform matrix;every element;1 / N;this sense;Pagerank;this idea;no zero entry;such a transition matrix
#s59	Now of course this is time dependent calculation of probabilities.
#c59	course;time dependent calculation;probabilities
#s60	Now we can imagine if we want to compute the average probabilities, the average probabilities probably would satisfy this equation without considering the time index.
#c60	we;we;the average probabilities;the average probabilities;this equation;the time index
#s61	So let's drop the time index and just assume that they will be equal.
#c61	's;the time index;they
#s62	Now this would give us N equations because for each page we have such equation and if you look at the what variables will have in these equations there are also precisely N variables.
#c62	us;N equations;each page;we;such equation;you;what variables;these equations;precisely N variables
#s63	Right?
#s64	So this basically means we now have a system of N equations with N variables.
#c64	we;a system;N equations;N variables
#s65	And these are linear equations.
#c65	linear equations
#s66	So basically the problem boils down to solve this system of equations.
#c66	the problem;this system;equations
#s67	And here I also showed the equations in the matrix form.
#c67	I;the equations;the matrix form
#s68	It's the vector p here.
#c68	It;the vector p
#s69	Equals a metrics of the transverse of the matrix here.
#c69	a metrics;the transverse;the matrix
#s70	And multiply by the vector again.
#c70	And multiply;the vector
#s71	Now, if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for item vector, right when you multiply the matrix by this vector, you get the same value as this vector.
#c71	you;some knowledge;you;linear algebra;you;the equation;item vector;you;the matrix;this vector;you;the same value;this vector
#s72	And this can be solved by using iterative algorithm.
#c72	iterative algorithm
#s73	So the equations here on above are basically taken from the previous slide, so you see the relation between the.
#c73	the equations;the previous slide;you;the relation
#s74	The Pagerank scores of different pages and in this iterative approach or power approach, we simply start with.
#c74	The Pagerank scores;different pages;this iterative approach or power approach;we
#s75	Randomly initialized vector p
#c75	vector p
#s76	and then we repeatedly just updated this p by multiplying the matrix here by this p vector.
#c76	we;this p;the matrix;this p vector
#s77	So I also show a concrete example here.
#c77	I;a concrete example
#s78	So you can see this now if we assume alpha is .2, then with the example that we show here on this slide we have the original transition matrix here.
#c78	you;we;alpha;the example;we;this slide;we;the original transition matrix
#s79	Right?
#s80	That includes the graph, the actual links, and we have this smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with a linear interpolation to form another matrix.
#c80	the graph;the actual links;we;this smoothing transition matrix uniform transition matrix;random jumping;we;them;a linear interpolation;another matrix
#s81	That would be like this.
#s82	So essentially we can imagine now the web looks like this.
#c82	we;the web
#s83	Can be captured by that there are virtual links between all the pages now.
#c83	virtual links;all the pages
#s84	So the Pagerank algorithm would just initialize the p vector first and then just compute the updating of this p vector by using this matrix multiplication.
#c84	the Pagerank algorithm;the p vector;the updating;this p vector;this matrix multiplication
#s85	Now if you rewrite this matrix model multiplication in terms of just, individual equations, you will see this.
#c85	you;this matrix model multiplication;terms;just, individual equations;you
#s86	And this is Basically the updating formula for this particular pages Pagerank score so you can also see you if you want to compute the value of this updated score for d1 you basically multiply this rule.
#c86	the updating formula;this particular pages;Pagerank score;you;you;you;the value;this updated score;d1;you;this rule
#s87	Right, by this column.
#c87	this column
#s88	And we take the dot product of the two.
#c88	we;the dot product
#s89	That would give us the value for this value.
#c89	us;the value;this value
#s90	So this is how we updated the vector.
#c90	we;the vector
#s91	We started with some initial values for these guys.
#c91	We;some initial values;these guys
#s92	For this and then we just revise the scores we generate a new set of scores and the updating formula is this one.
#c92	we;the scores;we;a new set;scores;the updating formula;this one
#s93	So we just repeatedly apply this and here it converges and when the metrics is like this where there's no zero values and it can be guaranteed to converge.
#c93	we;it;the metrics;no zero values;it
#s94	And at that point that we will just have the Pagerank scores for all the pages.
#c94	that point;we;the Pagerank scores;all the pages
#s95	Now we typically set the initial values just to 1 / N.
#c95	we;the initial values;1 / N.
#s96	So Interestingly, this updating formula can be also interpreted as propagating scores over the graph.
#c96	this updating formula;scores;the graph
#s97	Can you see why?
#c97	you
#s98	If you look at this formula and then compare that with this graph?
#c98	you;this formula;this graph
#s99	And can you imagine how we might be able to interpret this as essentially propagating scores over the graph?
#c99	you;we;scores;the graph
#s100	I hope you will see that indeed we can imagine we have values initialized on each of these pages, so we can have values here.
#c100	I;you;we;we;values;these pages;we;values
#s101	Let's say that's 1 /4 for each, and then we're going to use this matrix to update these scores.
#c101	's;1 /4;we;this matrix;these scores
#s102	And if you look at the equation here.
#c102	you;the equation
#s103	This one.
#c103	This one
#s104	Basically, we're going to combine the scores of the pages that possibly would lead to reaching this page, so we'll look at all the pages that are pointing to this page and then combine their scores and propagate the score.
#c104	we;the scores;the pages;this page;we;all the pages;this page;their scores;the score
#s105	The sum of the scores to this document d1.
#c105	The sum;the scores;this document;d1
#s106	So we look at the scores that represent the probability that the random surfer will be visiting the other pages before it reached d1, and then just do the propagation to simulate the probability of reaching this page.
#c106	we;the scores;the probability;the random surfer;the other pages;it;d1;the propagation;the probability;this page
#s107	d1.
#c107	d1
#s108	So there are two interpretations.
#c108	two interpretations
#s109	One is just the matrix multiplication and repeatedly multiply the vector by this matrix, but the other is to just think of it as propagating the scores repeatedly on the web.
#c109	just the matrix multiplication;the vector;this matrix;it;the scores;the web
#s110	So in practice the computation of Pagerank score is actually efficient because the matrix is sparse and there are some ways to transform the equation so that you avoid actually literally computing the values for all those elements.
#c110	practice;the computation;Pagerank score;the matrix;some ways;the equation;you;the values;all those elements
#s111	Sometimes you may also normalize the equation and that would give you a somewhat different form of the equation, but then the ranking of pages will not change.
#c111	you;the equation;you;a somewhat different form;the equation;the ranking;pages
#s112	The results of this potential problem of zero out link problem.
#c112	The results;this potential problem;zero out link problem
#s113	In that case, if the page does not have any out link then the probability of these pages would not sum to one basically the probability of reaching the next page from this page will not sum to one.
#c113	that case;the page;any out link;the probability;these pages;one basically the probability;the next page;this page
#s114	Mainly because we have lost some probability mass when we assume there's some probability that the surfer will try to follow links, but then there's no link to follow.
#c114	we;some probability mass;we;some probability;the surfer;links;no link
#s115	And one possible solution is simply to use a page specific damping factor and that could easily fix this.
#c115	one possible solution;a page specific damping factor
#s116	Basically, that's to say alpha would be 1.0 for a page with no out out link.
#c116	alpha;a page;no out out link
#s117	In that case the surfer would just have to randomly jump through another page instead of trying to follow a link.
#c117	that case;the surfer;another page;a link
#s118	So there are many extensions of page rank.
#c118	many extensions;page rank
#s119	One extension is to do topic specific Pagerank.
#c119	One extension;topic;specific Pagerank
#s120	Noted that Pagerank doesn't really use the query information so.
#c120	Pagerank;the query information
#s121	So we can make Pagerank query specific, however, so for example in the topic specific Pagerank we can simply assume when the surfer is bored the surfer is not going to randomly jump to any page on the web.
#c121	we;Pagerank query;example;the topic specific Pagerank;we;the surfer;the surfer;any page;the web
#s122	Instead it's going to jump to only those pages that are relevant to a query.
#c122	it;only those pages;a query
#s123	For example, if the queries is about the sports, then we could assume that when it's doing random jumping it's going to randomly jump to a sports page.
#c123	example;the queries;the sports;we;it;random jumping;it;a sports page
#s124	By doing this, then we can bias and Pagerank to topic like sports and then if you know the current query is about sports and then you can use this specialized Pagerank score to rank documents that would be better than if you use a generic Pagerank score.
#c124	we;Pagerank;sports;you;the current query;sports;you;this specialized Pagerank score;documents;you;a generic Pagerank score
#s125	Pagerank is also a general algorithm that can be used in many other applications for network analysis, particularly for example social networks.
#c125	Pagerank;a general algorithm;many other applications;network analysis;example;social networks
#s126	You can imagine if you compute the Pagerank scores for social network where a link might indicate friendship relation, you'll get some meaningful scores for people.
#c126	You;you;the Pagerank scores;social network;a link;friendship relation;you;some meaningful scores;people
410	77f30708-8474-41f5-89e9-d0cbabc4f20c	116
#s1	This lecture is about the web indexing.
#c1	This lecture;the web indexing
#s2	In this lecture we will continue talking about the web search and we're going to talk about how to create a web scale index.
#c2	this lecture;we;the web search;we;a web scale index
#s3	So once we crawled the web, we've got a lot of web pages.
#c3	we;the web;we;a lot;web pages
#s4	The next step is to use the indexer to Create the inverted index.
#c4	The next step;the indexer;the inverted index
#s5	In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture.
#c5	we;the standard information retrieval techniques;the index;what;we;the previous lecture
#s6	But there are new challenges that we have to solve for web scale indexing and the two main challenges.
#c6	new challenges;we;web scale indexing;the two main challenges
#s7	Our scalability and efficiency.
#c7	Our scalability;efficiency
#s8	The index would be so large that it cannot actually fit in into any single machine or a single disk, so we have to store the data on multiple machines.
#c8	The index;it;any single machine;a single disk;we;the data;multiple machines
#s9	Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly.
#c9	the data;it;the data;parallel;we;the index
#s10	Now, to address these challenges, Google has made a number of innovations.
#c10	these challenges;Google;a number;innovations
#s11	One is the Google file system that's a general distributed file system that can help programmers manage files stored on a cluster of machines.
#c11	the Google file system;a general distributed file system;programmers;files;a cluster;machines
#s12	The second is MapRecuce.
#c12	MapRecuce
#s13	This is a general software framework for supporting parallel computation.
#c13	a general software framework;parallel computation
#s14	Hadoop is the most known open source implementation of map reduce, now used in many applications.
#c14	Hadoop;the most known open source implementation;map;many applications
#s15	So this is the architecture of the Google File System.
#c15	the architecture;the Google File System
#s16	It uses very simple centralized management mechanism to manage it all the specific locations of files, so that maintains the file name, space and look up a table to know where exactly each file is stored.
#c16	It;very simple centralized management mechanism;it;all the specific locations;files;the file name;space;a table;each file
#s17	The application client would then talk to this GFS master and then obtain specific locations of the files that they want to process.
#c17	The application client;this GFS master;specific locations;the files;they
#s18	And once the GFS client obtained the.
#c18	the GFS client
#s19	The specific information about the files, then the application client can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network.
#c19	The specific information;the files;the application client;the specific servers;the data;you;other nodes;the network
#s20	So when this file system stores the files on machines the system also would create a fixed size of chunks so that data files are separated into many chunks.
#c20	this file system;the files;machines;the system;a fixed size;chunks;data files;many chunks
#s21	Each chunk is 64 MB, so it's pretty big, and that's a property for large data processing.
#c21	Each chunk;64 MB;it;a property;large data processing
#s22	These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about.
#c22	These chunks;reliability;something;the programmer
#s23	It's all taken care of by this fire system, so from the application perspective, the programmer would see this as if it's a normal file.
#c23	It;care;this fire system;the application perspective;the programmer;it;a normal file
#s24	The program doesn't have to know where exactly it's stored and can just invoke high level operators to process the file.
#c24	The program;it;high level operators;the file
#s25	(And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense.
#c25	Another feature;the data transfers;application;chunk servers;it;this sense
#s26	On top of the Google File System and Google also proposed map reduce as a general framework for parallel programming.
#c26	top;the Google File System;Google;map;a general framework;parallel programming
#s27	Now this is very useful to support a task like a building inverted index.
#c27	a task;a building inverted index
#s28	So this framework is hiding a lot of low level features from the program.
#c28	this framework;a lot;low level features;the program
#s29	As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel.
#c29	a result;the programmer;a minimum effort;a application;large cluster;parallel
#s30	And so some of the low level details hidden in the framework, including the specific network communications or load balancing or where the tasks are executed.
#c30	the low level details;the framework;the specific network communications;the tasks
#s31	All these details are hidden from the programmer.
#c31	All these details;the programmer
#s32	There is also a nice feature which is the built-in fault tolerance.
#c32	a nice feature;the built-in fault tolerance
#s33	If one server is broken, let's say service down and then some tasks may not be finished, then the map reduce mechanism would know that the task has not been done, so automatically dispatches the task on other servers that can do the job and therefore again the program doesn't have to worry about that.
#c33	one server;'s;service;some tasks;the map;mechanism;the task;the task;other servers;the job;the program
#s34	So here's how MapReduce works.
#c34	MapReduce
#s35	The input data will be separated into a number of key value pairs.
#c35	The input data;a number;key value pairs
#s36	Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel.
#c36	what;the value;the data;it;a fairly general framework;you;the data;different parts;each part;parallel
#s37	Each key value pair would be send to a map function.
#c37	Each key value pair;a map function
#s38	The programmer would write map function of course.
#c38	The programmer;map function;course
#s39	And then the map function would then process this key value pair and would generator a number of other key value pairs.
#c39	the map function;this key value pair;a number;other key value pairs
#s40	Of course the new key is usually different from the old key that's given to the map as input.
#c40	the new key;the old key;the map;input
#s41	And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected.
#c41	these key value pairs;the output;the map function;all the outputs;all the map functions
#s42	And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together.
#c42	the sorted;the key;the result;all the values;the same key
#s43	So now we've got a pair of a key and a set of values that are attached to this key.
#c43	we;a pair;a key;a set;values;this key
#s44	So this will then be sent to a Reduce function.
#c44	a Reduce function
#s45	Now of course, each Reduce function will handle a different  key, so we will send these output values to multiple, reduce functions, each handling unique key.
#c45	course;each Reduce function;a different  key;we;these output values;functions;unique key
#s46	A reduce function would then process the input.
#c46	A reduce function;the input
#s47	Which is a key and a set of values to produce another set of key values as the output.
#c47	a key;a set;values;another set;key values;the output
#s48	So these output values will be then collected together to form the final output.
#c48	these output values;the final output
#s49	Right, so this is the general framework of MapReduce.
#c49	the general framework;MapReduce
#s50	Now the programmer only needs to write the Map function and the Reduce function.
#c50	the programmer;the Map function;the Reduce function
#s51	Everything else is actually taken care of by the MapReduce framework.
#c51	Everything;care;the MapReduce framework
#s52	So you can see the program really only needs to do minimum work.
#c52	you;the program;minimum work
#s53	And with such a framework the input data can be partitioned into multiple parts, each is processed in parallel, first by map, and then in the process after we reach the reduce stage, then multiple reduce functions can also further process the different keys and their associated values in parallel, so it achieves (some)
#c53	such a framework;the input data;multiple parts;parallel;map;the process;we;the reduce stage;multiple reduce functions;the different keys;their associated values;parallel;it
#s54	It achieves the purpose of parallel processing of large data set.
#c54	It;the purpose;parallel processing
#s55	So let's take a look at a simple example and that's what accounting.
#c55	's;a look;a simple example;what
#s56	How the input is files containing words.
#c56	the input;files;words
#s57	And the output that we want to generate is the number of occurrences of each word, so it's the word account.
#c57	the output;we;the number;occurrences;each word;it;the word account
#s58	We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging.
#c58	We;this kind;counting;example;the popularity;a word;a large collection;effect;IDF weiging
#s59	Or search.
#s60	So how can we solve this problem?
#c60	we;this problem
#s61	One natural thought is that.
#c61	One natural thought
#s62	This task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts, and that's precisely the idea of what we can do with MapReduce.
#c62	This task;parallel;different parts;the file;parallel;the end;we;all the counts;precisely the idea;what;we;MapReduce
#s63	We can parallelize on lines in this input file.
#c63	We;lines;this input file
#s64	So more specifically, we can assume the input to each map function is key value pair that represents the line number and the stream on that line.
#c64	we;the input;each map function;key value pair;the line number;the stream;that line
#s65	So the first line, for example, has the "key of 1 and the value is ""Hello World""" " ""Bye World""" and just 4 words.
#c65	the first line;example;the "key;the value;""Hello World;"Bye World;just 4 words
#s66	On that line so this key value pair will be sent to a map function.
#c66	that line;this key value pair;a map function
#s67	The map function would then just count the words in this line, and in this case of course there are only four words.
#c67	The map function;the words;this line;this case;course;only four words
#s68	Each word gets a count of one, and these are the output that you see here on this slide.
#c68	Each word;a count;the output;you;this slide
#s69	From this map function.
#c69	this map function
#s70	So the map function is really very simple if you look at the what the pseudocode looks like on the right side you see it simply needs to iterate over all the words in this line and then just call the collect function, which means it would then send the world and the counter to the collector.
#c70	the map function;you;what;the pseudocode;the right side;you;it;all the words;this line;the collect function;it;the world;the counter;the collector
#s71	The collector would then try to sort all these.
#c71	The collector
#s72	key value pairs from different Map functions, so the function is very simple and the programmer specifies this function as a way to process each part of the data.
#c72	key value;different Map functions;the function;the programmer;this function;a way;each part;the data
#s73	Of course, the second line will be handled by a different map function, which will produce a similar output.
#c73	the second line;a different map function;a similar output
#s74	OK, now the output from the map functions will be then send to a collector and the collector will do the internal grouping or sorting.
#c74	the output;the map functions;a collector;the collector;the internal grouping;sorting
#s75	So at this stage you can see we have collected multiple pairs, each pair is a word and its count in the line.
#c75	this stage;you;we;multiple pairs;each pair;a word;its count;the line
#s76	So once we see all these pairs then we can sort them based on the key which is the word.
#c76	we;all these pairs;we;them;the key;the word
#s77	So we will collect all the counts of "a word like a ""Bye"" here together."
#c77	we;all the counts;a word;a ""Bye
#s78	An similarly we do that "for other words like ""Hadoop"", ""Hello"" etc."
#c78	we;other words;""Hadoop
#s79	So each world now is attached to a number of values, a number of counts.
#c79	each world;a number;values;counts
#s80	And these counts represent the occurrences of this word in different lines.
#c80	these counts;the occurrences;this word;different lines
#s81	So now we have got a new pair of a key and a set of values and this pair will then be feeding to reduce function.
#c81	we;a new pair;a key;a set;values;this pair;function
#s82	So the reduce function now would have to finish the job of counting the total occurrences of this word.
#c82	the reduce function;the job;the total occurrences;this word
#s83	Now it has already got all these partial accounts, so all it needs to do is similarly to add them up so the reduce function shown here is very simple as well.
#c83	it;all these partial accounts;it;them;the reduce function
#s84	You have a counter and then iterate over all the words that you see in this array, and then you just accumulated the count.
#c84	You;a counter;all the words;you;this array;you;the count
#s85	And then finally output the key and the total count, and that's precisely what we want as the output of this whole program.
#c85	the key;the total count;precisely what;we;the output;this whole program
#s86	So you can see this is already very similar to building an inverted index, and if you think about it, the output here is indexed by world and we have already got the dictionary.
#c86	you;an inverted index;you;it;the output;world;we;the dictionary
#s87	Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel.
#c87	we;the counts;what;the document IDs;the specific frequency counts;words;those documents;we;inverted index;parallel
#s88	So here's one way to do that.
#c88	one way
#s89	So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document.
#c89	this case;we;the input;function;a pair;a key;the document ID;the value;the stream;that document
#s90	So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example.
#c90	it;all the words;that document;the map function;something;what;we;the word count example
#s91	It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs.
#c91	It;all the counts;this word;this document;it;the set;key value pairs
#s92	Each key is a word.
#c92	Each key;a word
#s93	And the value is the count of this orld in this document, plus the document ID.
#c93	the value;the count;this orld;this document;the document ID
#s94	Now you can easily see why we need to add document ID here.
#c94	you;we;document ID
#s95	Of course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later.
#c95	the inverted index;we;this information;the map function;track;it;the reduce function
#s96	Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just "a key like ""Java"" associated with all the" documents that match this key or the "documents where ""Java"" occurred."
#c96	another document D2;the same way;the end;a sorting mechanism;them;we;just "a key;"Java;all the" documents;this key;the "documents;Java
#s97	And the counts.
#c97	And the counts
#s98	So the counts of Java in those documents.
#c98	So the counts;Java;those documents
#s99	And this will be collected together and this will be also fed into the reduce function.
#c99	the reduce function
#s100	So now you can see the reduce function has already got input that looks like a inverted index entry, right?
#c100	you;the reduce function;input;a inverted index entry
#s101	So it's just the word and all the documents that contain the word and the frequencies of the word in those documents.
#c101	it;just the word;all the documents;the word;the frequencies;the word;those documents
#s102	So all it needs to do is simply to concatenate them into a continuous chunk of data, and this can be then written into a file system.
#c102	it;them;a continuous chunk;data;a file system
#s103	So basically the reduce function is going to do very minimum work.
#c103	the reduce function;very minimum work
#s104	So this is pseudocode for inverted index construction.
#c104	inverted index construction
#s105	Here we see two functions.
#c105	we;two functions
#s106	Procedure Map and procedure Reduce.
#c106	Procedure Map;procedure Reduce
#s107	And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described.
#c107	a program;the specify;these two functions;top;map;you;they;what;I
#s108	In the case of map, it's going to count the occurrences of word using associative array and will output the old accounts together with the document ID here.
#c108	the case;map;it;the occurrences;word;associative array;the old accounts;the document ID
#s109	So this is the reduce function On the other hand, simply concatenates all the input that it has been given and then put them together as one single entry for this key.
#c109	the reduce function;the other hand;all the input;it;them;one single entry;this key
#s110	So this is a very simple MapReduce function, yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines.
#c110	a very simple MapReduce function;it;us;the inverted index;very large scale;the data;different machines
#s111	The program doesn't have to take care of the details.
#c111	The program;care;the details
#s112	So this is how we can do parallel index construction for web search.
#c112	we;index construction;web search
#s113	So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system.
#c113	web scale indexing;some new techniques;the standard traditional indexing techniques;the index;multiple machines;file system;a Google File System;a distributed file system
#s114	And secondly, it requires creating the index in parallel because it's so large it takes a long time to create the index for all the documents.
#c114	it;the index;parallel;it;it;a long time;the index;all the documents
#s115	So if we can do it in parallel it will be much faster and this is done by using the MapReduce framework.
#c115	we;it;parallel;it;the MapReduce framework
#s116	Note that the post the GFS and MapReduce frameworks are very general so they can also support many other applications.
#c116	the post;the GFS and MapReduce frameworks;they;many other applications
410	7be943e1-7bec-499e-9d91-b157f9ecb80d	88
#s1	So average precision is computed for just one query.
#c1	average precision;just one query
#s2	But we generally experiment with many different queries and this is to avoid the variance across queries.
#c2	we;many different queries;the variance;queries
#s3	Depending on the queries you use, you might make different conclusions, so it's better to use more queries.
#c3	the queries;you;you;different conclusions;it;more queries
#s4	If you use more queries then you would also have to take average of the average precision over all these queries.
#c4	you;more queries;you;the average precision;all these queries
#s5	So how can we do that?
#c5	we
#s6	You can naturally think of just doing arithmetic mean as we know.
#c6	You;we
#s7	Always tend to think in this way.
#c7	this way
#s8	So this would give us what is called Mean Average Precision or MAP.
#c8	us;what;Mean Average Precision;MAP
#s9	In this case we take arithmetic mean of all the average precisions over set of queries or topics.
#c9	this case;we;arithmetic mean;all the average precisions;set;queries;topics
#s10	But as I just mentioned in another lecture, is this good?
#c10	I;another lecture
#s11	Recall that we talked about the different ways of combining precision and recall.
#c11	we;the different ways;precision
#s12	And we conclude that the arithmetic mean is not as good as the F measure.
#c12	we;the F measure
#s13	But here it's the same.
#c13	it
#s14	We can also think about the alternative ways of aggregating the numbers.
#c14	We;the alternative ways;the numbers
#s15	Don't just automatically assume that.
#s16	Let's just take the arithmetic mean of the average precision over these queries.
#c16	's;the arithmetic mean;the average precision;these queries
#s17	Let's think about what's the best way of aggregating.
#c17	's;what;the best way;aggregating
#s18	If you think about different ways, naturally you would probably be able to think about another way, which is geometric mean.
#c18	you;different ways;you;another way;geometric mean
#s19	And we called this kind of average gMAP map.
#c19	we;this kind;average gMAP map
#s20	This is another way.
#c20	another way
#s21	So now, once you think about the two different ways of doing the same thing, the natural question to ask is which one is better so.
#c21	you;the two different ways;the same thing;the natural question;one
#s22	So do you use MAP or gMAP?
#c22	you;MAP;gMAP
#s23	Again, that's important question.
#c23	important question
#s24	Imagine you are again testing a new algorithms by comparing it with your old algorithm in the search engine.
#c24	you;a new algorithms;it;your old algorithm;the search engine
#s25	Now you test it on multiple topics.
#c25	you;it;multiple topics
#s26	Now you've got the average precisions for all these topics.
#c26	you;the average precisions;all these topics
#s27	Now you are thinking of looking at the overall performance you have to take average.
#c27	you;the overall performance;you
#s28	But which which strategy would you use?
#c28	which strategy;you
#s29	Now first you should also think about the question, would it make a difference?
#c29	you;the question;it;a difference
#s30	Can you think of scenarios where using one of them would make a difference?
#c30	you;scenarios;them;a difference
#s31	That is, they would give different the rankings of those methods.
#c31	they;the rankings;those methods
#s32	And that also means depending on the way you average, or you take the average of these average precisions, you will get different conclusions.
#c32	the way;you;you;the average;these average precisions;you;different conclusions
#s33	This makes the question become even more important.
#c33	the question
#s34	So which one would you use?
#c34	which one;you
#s35	Again, if you look at the difference between these different ways of aggregating the average position, you will realize in arithmetic mean the sum is dominant by large values.
#c35	you;the difference;these different ways;the average position;you;the sum;large values
#s36	So what does a large menu value here mean?
#c36	what;a large menu value
#s37	It means the query is relatively easy.
#c37	It;the query
#s38	You can have a high average precision, where as gMAP tends to be affected more by lower values and those are the queries that don't have good performance.
#c38	You;a high average precision;gMAP;lower values;the queries;good performance
#s39	The average precision is low.
#c39	The average precision
#s40	So if you think about improving the search engine for those difficult queries than gMAP would be preferred.
#c40	you;the search engine;those difficult queries;gMAP
#s41	On the other hand, that if you just want to have improvement over all the kinds of queries or particular popular queries, that might be easy and you want to make the perfect and maybe MAP would be them preferred.
#c41	the other hand;you;improvement;all the kinds;queries;particular popular queries;you;MAP;them
#s42	So again, the answer depends on your users, your user's tasks, and their preferences.
#c42	the answer;your users;your user's tasks;their preferences
#s43	So the point that here is.
#c43	So the point
#s44	To think about the multiple ways to solve the same problem and then compare them and think carefully about differences and which one makes more sense.
#c44	the multiple ways;the same problem;them;differences;one;more sense
#s45	Often in one of them might make sense in one situation and another might make more sense in a different situation, so it's important to figure out under what situations one is preferred.
#c45	them;sense;one situation;more sense;a different situation;it;what situations
#s46	As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document.
#c46	a special case;the mean average precision;we;the case;precisely one relevant document
#s47	And this happens often.
#s48	For example, in what's called a known item search, where you know a target page.
#c48	example;what;a known item search;you;a target page
#s49	Let's say you want to find the Amazon home page, you have one relevant document there, and you hope to find it.
#c49	's;you;the Amazon home page;you;one relevant document;you;it
#s50	And that's called the known item search.
#c50	the known item search
#s51	In that case, there is precisely one relevant document, or in another application like a question answering.
#c51	that case;precisely one relevant document;another application;a question
#s52	Maybe there's only one answer there, so if you rank the answers, then your goal is ranked at one particular answer on top right?
#c52	only one answer;you;the answers;your goal;one particular answer;top
#s53	So in this case, you can easily verify, the average precision will basically boil down two reciprocal rank, that is one over R, where R is the rank position of that single relevant document.
#c53	this case;you;the average precision;two reciprocal rank;R;R;the rank position;that single relevant document
#s54	So if that document is ranked on the very top, R is 1
#c54	that document;the very top;R
#s55	and then it's one for reciprocal rank.
#c55	it;reciprocal rank
#s56	If it's ranked at the second, then it's 1 / 2 etc.
#c56	it;it
#s57	And then we can also take a average of all these average position or reciprocal rank over a set of topics and that would give us something called Mean Reciprocal Rank.
#c57	we;a average;all these average position;reciprocal rank;a set;topics;us;something;Mean Reciprocal Rank
#s58	It is a very popular value for known item search or any ranking problem where you have just one relevant item.
#c58	It;a very popular value;known item search;any ranking problem;you;just one relevant item
#s59	Now again, here you can see this R actually is meaningful here, and this R is basically indicating how much effort an user would have to make in order to find that relevant document.
#c59	you;this R;this R;how much effort;an user;order;that relevant document
#s60	If it's ranked on the top is no effort that you have to make or little effort, but if it's ranked at 100 then you actually have to read presumably 100 documents in order to find it.
#c60	it;the top;no effort;you;little effort;it;you;presumably 100 documents;order;it
#s61	So in this sense, R is also meaningful measure and the reciprocal rank will take the reciprocal of R instead of using R directly.
#c61	this sense;R;meaningful measure;the reciprocal rank;R;R
#s62	So one natural question here is, why not simply using R?
#c62	one natural question;R
#s63	Now imagine if you are to design a measure to measure performance of the ranking system when there is only one relevant item.
#c63	you;a measure;performance;the ranking system;only one relevant item
#s64	You might have thought about using r directly as the measure.
#c64	You;r;the measure
#s65	After all that measures the users effort, right?
#c65	the users
#s66	But think about, if you take the average of this over a large number of topics, again, it would make a difference right, for one single topic using R or using one overall wouldn't make any difference.
#c66	you;the average;a large number;topics;it;a difference;one single topic;R;any difference
#s67	It's the same larger R with correspond to a small one overall, but the difference would only show when show up when you have many topics.
#c67	It;the same larger R;correspond;the difference;you;many topics
#s68	So again think about average of mean reciprocal rank versus average of just R.
#c68	mean reciprocal rank;average;just R.
#s69	What's the difference?
#c69	What;the difference
#s70	Do you see any difference?
#c70	you;any difference
#s71	And would this difference change the order of systems in our conclusion?
#c71	this difference;the order;systems;our conclusion
#s72	And it turns out that there is actually a big difference, and if you think about it, if you want to think about it and then yourself, then pause the video.
#c72	it;a big difference;you;it;you;it;the video
#s73	Basically the difference is if you take some of R directly, then again will be dominated by large values of R.
#c73	the difference;you;R;large values;R.
#s74	So what are those values?
#c74	what;those values
#s75	Those are basically large values that indicate the lowly ranked results.
#c75	large values;the lowly ranked results
#s76	That means the relevant item is ranked very low down on the list and the sum, the audacity.
#c76	the relevant item;the list;the audacity
#s77	Also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list, but from a user's perspective we care more about the highly ranked documents.
#c77	the average;those relevant documents;the lower portion;the ranked list;a user's perspective;we;the highly ranked documents
#s78	So by taking this transformation by using reciprocal rank, here we emphasize more on the difference on the top and think about the difference between one and two.
#c78	this transformation;reciprocal rank;we;the difference;the top;the difference
#s79	It will make a big difference.
#c79	It;a big difference
#s80	In one over R, but think about 100 and 101 and one it won't make much difference if you use this.
#c80	R;it;much difference;you
#s81	But if you use this, there will be a big difference.
#c81	you;a big difference
#s82	Being 100 and let's say 1000.
#c82	's
#s83	Right, so this is not the desirable.
#s84	On the other hand, one and two won't make much difference, so this is yet another case where there may be multiple choices of doing the same thing, and then you need to figure out which one makes more sense.
#c84	the other hand;much difference;another case;multiple choices;the same thing;you;one;more sense
#s85	So to summarize, we show the Precision recall curve, can characterize the overall accuracy of a ranked list.
#c85	we;the Precision recall curve;the overall accuracy;a ranked list
#s86	And we emphasized that the actual utility over ranking list that depends on how many top rankings results are user would actually examine.
#c86	we;the actual utility;ranking list;how many top rankings;results;user
#s87	Some users will examine more than others and average precision is the standard measure for comparing two ranking methods.
#c87	Some users;others;average precision;the standard measure;two ranking methods
#s88	It combines precision and recall and it's sensitive to the rank of every relevant the document.
#c88	It;precision;recall;it;the rank;every relevant the document
410	7c6ead79-a73f-4c15-ab06-7d76fada8172	20
#s1	but there are some interesting challenges in threshold in learning the filtering problem so here i show the historical data that you can collect in the filtering system so you can see the scores and the status of relevance so the first one has a score thirty six point five
#c1	some interesting challenges;threshold;the filtering problem;i;the historical data;you;the filtering system;you;the scores;the status;relevance;the first one;a score
#s2	and it's relevant the second one is not relevant etc of course we have a lot of documents for which we don't know the status 'cause we have never delivered them through the user so as you can see here we only see the judgments of documents delivered to the user
#c2	it;the second one;we;a lot;documents;we;we;them;the user;you;we;the judgments;documents;the user
#s3	so this is not a random sample
#c3	a random sample
#s4	so it's a sense of the data it's kind of biased so that creates some difficulty for learning and signal there are in general very little label data an very few random in the data
#c4	it;a sense;the data;it;some difficulty;signal;very little label data;the data
#s5	so it's it's also challenging for machine learning approaches typically they require require more training data in the extreme case at the beginning we don't even have any label data as well the system dear has to make a decision so that's very difficult problem at the beginning finally there is also this issue of exploration versus exploitation tradeoff now this means we also want to explore the document space A little bit and to see if the user might be interested in documents that we haven't been able so in other words we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently not matching the users interests so well
#c5	it;it;machine learning approaches;they;more training data;the extreme case;the beginning;we;any label data;the system dear;a decision;very difficult problem;the beginning;this issue;exploration;exploitation tradeoff;we;the document space;the user;documents;we;other words;we;the space;user interests;the user;some other documents;the users interests
#s6	so how do we do that
#c6	we
#s7	well we could lower the threshold a little bit
#c7	we;the threshold
#s8	and we just deliver some near misses to the user to see what the user would respond to see how the user would respond to this extra document and this is a tradeoff becaus on the one hand you want to explore but on the other hand you don't want to really explore too much 'cause then you'd over deliver non relevant information so exploitation means you would exploit what you learn about the user let's say you know the user is interested in this particular topic so you don't want to deviate that much
#c8	we;some near misses;the user;what;the user;the user;this extra document;a tradeoff becaus;the one hand;you;the other hand;you;you;non relevant information;exploitation;you;what;you;the user;'s;you;the user;this particular topic;you
#s9	but if you don't deviate at all then you don't explore at all that's also not good you might miss opportunity another interest of the user
#c9	you;you;you;opportunity;another interest;the user
#s10	so this is a dynama and that's also a difficult problem to solve now how do we solve these problems in general i think one can use the empirical utility optimization strategy and this is strategy is basically to optimize the threshold based on historical data that says you have seen on the previous slide so you can just compute the utility on the training data for each candidate the score threshold pretend what before i cut at this point what if i can cut at the different scoring threshold point what would happen what's utility since these are training there are we can kind of compute the utility we know there are relevant status or we assume that we know relevant status based on approximation of click throws
#c10	a dynama;a difficult problem;we;these problems;i;one;the empirical utility optimization strategy;strategy;the threshold;historical data;you;the previous slide;you;the utility;the training data;each candidate;the score threshold;i;this point;what;i;the different scoring threshold point;what;what;utility;we;the utility;we;relevant status;we;we;relevant status;approximation;click
#s11	so then we can just choose the threshold and that gives the maximum utility on the training data but this of course there's an account for exploration that we just talked about and there is also the difficulty of bias training sample as we mentioned so in general we can only get the upper bound of the true optimal thresh code 'cause the the threshold might be actually lower than this so it's possible that the discarded item might be actually interesting to the user
#c11	we;the threshold;the maximum utility;the training data;course;an account;exploration;we;the difficulty;bias training sample;we;we;the true optimal thresh code;the the threshold;it;the discarded item;the user
#s12	so how do we solve this problem where we generate as i said we can lower the threshold to explore a little bit so here's one particular protein called a better gamma threshold and learning so the idea is following so here i show rent list of all the training documents that we have seen so far
#c12	we;this problem;we;i;we;the threshold;a little bit;one particular protein;a better gamma threshold;the idea;i;rent list;all the training documents;we
#s13	and they're ranked by their positions an on the Y axis we show the utility of course this function depends on how you specify the coefficient in the utility function but we can then imagine depending on the cut off position we will have a utility that means suppose i cut at this position and that would be the utility so we can for example identify some cutting cut off point the optimal point theater optimal is the point when we would achieve the maximum utility if we had chosen this thrash code and there is also zero threshold O utilities rush code and you can see at this cut off the utility is zero now what does that mean that means if i lower the threshold a little bit and i reach this threshold the utility would be lower
#c13	they;their positions;the Y axis;we;the utility;course;this function;you;the coefficient;the utility function;we;the cut off position;we;a utility;i;this position;the utility;we;example;point;the optimal point theater;the point;we;the maximum utility;we;this thrash code;zero threshold O utilities rush code;you;this cut;the utility;what;i;the threshold;i;this threshold;the utility
#s14	but it's still it's still non elective at least so it's not as high as the optimal utility
#c14	it;it;it;the optimal utility
#s15	but it gives us a safe point to explore the threshold as i just explained it's desirable to explore the interest of space so it's desirable to lower the thresh code based on your training data so that means in general we want to set the threshold somewhere in this range let's say we can use offer to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of zero utility threshold and the optimal between the threshold now the question is how how should we set alpha when should we deviate a more from the optimal utility point
#c15	it;us;a safe point;the threshold;i;it;the interest;space;it;the thresh code;your training data;we;the threshold;this range;'s;we;the deviation;the optimal utility point;you;the formula;the threshold;just the interpolation;zero utility threshold;the threshold;the question;we;alpha;we;the optimal utility point
#s16	well this can depend on multiple factors and the one way to solve the problem is to encourage this thresholding mechanism to explore up to the zero point
#c16	multiple factors;the one way;the problem;this thresholding mechanism;the zero point
#s17	and that's a safe point
#c17	a safe point
#s18	but we're not going to necessarily reach all the way to the zero point
#c18	we;the zero point
#s19	but rather we're going to use other parameters to further define alpha and this specifically is as follows so there will be a a beta parameter to control the deviation from the optimal threshold and this can be based on can be accounting for over fitting to the training data let's say an so this can be just adjustment factor
#c19	we;other parameters;alpha;a a beta parameter;the deviation;the optimal threshold;fitting;the training data;'s;just adjustment factor
#s20	but what's more interesting is this gamma parameter here and you can see in this formula game isaac controlling the inference of the number of examples in training data set so you can see in this formula as N which denotes the number of training examples becomes bigger than it would actually encourage less exploration in other words when is very small it would try to explore more and that just means if we have seen few examples we're not sure but we have exhausted the space of interests so we would explore but as we have seen many examples from the user many data points then we feel that we probably don't have to explore more so this gives us a dynamical strategy for exploration the more examples we have seen the less expressing we're going to do so the threshold will be closer to the optimal threshold so that's the basic idea of this approach obvious approach it actually has been working well in some evaluation studies and paracle effective and also can work on arbitrary utility with a proper their lower bound and expressively address this is exploration exploitation tradeoff the kind of uses the O utility threshold point as safeguard for exploring and exploiting tradeoff were not never going to explore further than the zero utility point so if you take the analogy of gambling and you don't want to risk on losing money so it's a safe square root for the conservative strategy for exploration and the problem is of course this approach is purely heuristic and the zero utility lower bound is also often too conservative and there are of course more advanced machine learning approaches that have been proposed for solving these problems and this is the active research area so to summarize there are two strategies for recommender systems or filtering systems one is content based which is looking at the item similarity the other is collaborative filtering which is looking at the user similarity in this lecture we've covered content based filtering approach in the next lecture we're going to talk about the collaborative filtering income hand basically filtering system we generally have to solve several problems related to filtering decisioning and learning etc and such a system character to be built based on a search engine system by adding a threshold mechanism and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user
#c20	what;this gamma parameter;you;this formula game isaac;the inference;the number;examples;training data;you;this formula;the number;training examples;it;less exploration;other words;it;we;few examples;we;we;the space;interests;we;we;many examples;the user;we;we;us;a dynamical strategy;exploration;the more examples;we;we;the threshold;the optimal threshold;the basic idea;this approach obvious approach;it;some evaluation studies;paracle;arbitrary utility;exploration exploitation;the kind;the O utility threshold point;safeguard;tradeoff;the zero utility point;you;the analogy;gambling;you;money;it;a safe square root;the conservative strategy;exploration;the problem;course;this approach;the zero utility;course;more advanced machine learning approaches;these problems;the active research area;two strategies;recommender systems;filtering systems;one;the item similarity;collaborative filtering;the user similarity;this lecture;we;content based filtering approach;the next lecture;we;the collaborative filtering income hand;basically filtering system;we;several problems;filtering decisioning;such a system character;a search engine system;a threshold mechanism;adaptive learning algorithm;the system;long-term feedback;the user
410	7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	262
#s1	This lecture is a continued discussion of.
#c1	This lecture;a continued discussion
#s2	Discriminative classifiers for text categorization.
#c2	Discriminative classifiers;text categorization
#s3	So in this lecture will introduce yet another discriminative classifier called a support vector machine or VM, which is a very popular classification method, and there has been also shown to be effective for text categorization.
#c3	this lecture;discriminative classifier;a support vector machine;VM;a very popular classification method;text categorization
#s4	So to introduce this classifier, let's also think about the simple case of two categories
#c4	this classifier;'s;the simple case;two categories
#s5	and we have two public categories, season one and Season 2 here.
#c5	we;two public categories;season
#s6	An we want to classify documents into these two categories and we're going to represent again a document by a feature vector X here.
#c6	we;documents;these two categories;we;a document;a feature vector X
#s7	Now the idea of this classifier is do design.
#c7	the idea;this classifier;design
#s8	Also a linear separator.
#c8	Also a linear separator
#s9	Here that you see and it's very similar to what you have seen or just for logistic regression.
#c9	you;it;what;you;logistic regression
#s10	And we're going to also say that if the sign of this function value is positive, then we're going to say the object is in Category 1.
#c10	we;the sign;this function value;we;the object;Category
#s11	Otherwise, we're going to say it's in Category 2, so that makes 0 value.
#c11	we;it;Category;0 value
#s12	The decision boundary between two categories.
#c12	The decision boundary;two categories
#s13	So in general in high dimensional space such a zero point corresponds to a hyperplane.
#c13	high dimensional space;such a zero point;a hyperplane
#s14	I show you a simple case of two dimensional space with just X1 and X2.
#c14	I;you;a simple case;two dimensional space;just X1;X2
#s15	In this case this corresponds to a line that you can see here.
#c15	this case;a line;you
#s16	So this is.
#s17	A line defined by just three parameters "here beta0  Now this line.
#c17	A line;just three parameters;beta0
#s18	Is the heading in this direction, so it shows that as we increase X1, X2 will also increase.
#c18	the heading;this direction;it;we;X1;X2
#s19	So know that beta1 and beta2 have different signs or one is negative and there is positive.
#c19	beta1;beta2;different signs
#s20	I so let's just assume that beta one is negative and beta two is positive.
#c20	I;'s;beta one
#s21	Now it's interesting to examine then the data instances on the two sides of this line, so here that there are incidences are visualized as circles for one class and diamonds for the other class.
#c21	it;the data instances;the two sides;this line;incidences;circles;one class;the other class
#s22	Now one question is to take a point like this one and to ask the question what's the value of this expression or this classifier for this data point.
#c22	one question;a point;this one;the question;what;the value;this expression;this classifier;this data point
#s23	So what do you think?
#c23	what;you
#s24	Basically working to evaluate its value by using this function.
#c24	its value;this function
#s25	And as we said, if this value is positive we're gonna say this is in category one, and if it's negative it's going to be in category Two.
#c25	we;this value;we;category;it;it;category
#s26	Intuitively, this line separates these two categories, so we expect the points on one side would be positive and points on the other side would be negative.
#c26	this line;these two categories;we;the points;one side;the other side
#s27	Or the question is under the assumption that I just mentioned, let's examine a particular point like this one.
#c27	the question;the assumption;I;'s;a particular point;this one
#s28	So what do you think is the sign of this expression?
#c28	what;you;the sign;this expression
#s29	To examine the sign, we can simply look at this expression.
#c29	the sign;we;this expression
#s30	Here we can compare this with, let's say, value on the line.
#c30	we;'s;the line
#s31	Let's say compare this with this point.
#c31	's;this point
#s32	They have identical X one, but then one has a higher value for its too.
#c32	They;one;a higher value
#s33	Now let's look at the sign of the coefficient for X2, where we know this is a positive.
#c33	's;the sign;the coefficient;X2;we
#s34	So what that means is that the F value for this point should be higher than the F value for this point on the line.
#c34	what;the F value;this point;the F value;this point;the line
#s35	That means this will be positive, right?
#s36	So we know in general for all the points on this side, the.
#c36	we;all the points;this side
#s37	Functions about it would be positive.
#c37	Functions;it
#s38	And you can also verify all the points on this side would be negative, and so this is how this kind of linear classifier or linear separator can then separate the points in the two categories.
#c38	you;all the points;this side;this kind;linear classifier;separator;the points;the two categories
#s39	So now the natural question is, which linear separate is the best?
#c39	the natural question
#s40	Now I've again she want lying here that can separate the two classes.
#c40	I;she;the two classes
#s41	And this line, of course, is determined by the vector beta, the coefficients, different coefficient will give us a different line.
#c41	this line;course;the vector beta;the coefficients;different coefficient;us;a different line
#s42	So we could imagine there are other lines that can do the same job.
#c42	we;other lines;the same job
#s43	So gamma, for example, could give us another line that can also separate these instances.
#c43	gamma;example;us;another line;these instances
#s44	And of course there are also lines that won't separate them, and those are bad lines.
#c44	course;lines;them;bad lines
#s45	But the question is when we have multiple lines that can separate the both clauses, which line is the best?
#c45	the question;we;multiple lines;the both clauses;which line
#s46	In fact, you can imagine there are many different ways of choosing the line.
#c46	fact;you;many different ways;the line
#s47	So the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best.
#c47	the logistical regression classifier;you;some criteria;this line;it;a conditional likelihood;the training data;which line
#s48	But in this VM, we're going to look at another criteria for determining which lines best and this time the criteria is more tide to the classification error.
#c48	this VM;we;another criteria;which lines;the criteria;more tide;the classification error
#s49	As you will see.
#c49	you
#s50	So the basic idea is to choose the separator.
#c50	the basic idea;the separator
#s51	To maximize the margin.
#c51	the margin
#s52	So what is the margin?
#c52	what;the margin
#s53	Well, I choose.
#c53	I
#s54	So I've shown some daughter lines here to indicate the boundaries of those data points in.
#c54	I;some daughter lines;the boundaries;those data points
#s55	In each class and the margin is simply the distance between the line, the separator and the closest points from each class.
#c55	each class;the margin;the distance;the line;the separator;the closest points;each class
#s56	So you can see the margin of this side is as I've shown here.
#c56	you;the margin;this side;I
#s57	And you can also define the margin on the other side.
#c57	you;the margin;the other side
#s58	And in order for the separate to maximizing the margin, it has to be kind of in the middle of the two boundaries, and you don't want this separator to be very close to one side.
#c58	order;the margin;it;the middle;the two boundaries;you;this separator;one side
#s59	And then that inducing intuitively makes a lot of sense.
#c59	a lot;sense
#s60	So this is the basic idea of ecfmg.
#c60	the basic idea;ecfmg
#s61	We're going to choose a linear separator to maximize the margin.
#c61	We;a linear separator;the margin
#s62	Now on this slide I've also changed the notation so that I'm not going to use beta.
#c62	this slide;I;the notation;I;beta
#s63	Didn't know the parameters and, but instead I'm going to use W, although W was used to denote the words before.
#c63	the parameters;I;W;W;the words
#s64	So don't be confused here.
#s65	W here is actually wait set of weights.
#c65	W;weights
#s66	And.
#s67	So I'm also using locates be to denote beta zero, the bias constant.
#c67	I;locates
#s68	And there are instances do represented as X.
#c68	instances;X.
#s69	And I also use the vector form of multiplication here.
#c69	I;the vector form;multiplication
#s70	So we see transpose of W vector multiplied by the feature vector.
#c70	we;transpose;W vector;the feature vector
#s71	So P is a biased constant and W is a set of weights and with one wait for each feature we have M features and so have aim weights and are represented as a vector.
#c71	P;W;a set;weights;one wait;each feature;we;M features;so have aim;weights;a vector
#s72	An similarly the data instance.
#c72	An similarly the data instance
#s73	Here the text object is represented by also a feature vector of the same number of elements.
#c73	the text object;a feature vector;the same number;elements
#s74	XI is future value.
#c74	XI;future value
#s75	For example word count.
#c75	example
#s76	I can you can verify when we multiply these two vectors together, take the dot product that we get the same form of the NIA separate as you have seen before.
#c76	I;you;we;these two vectors;the dot product;we;the same form;the NIA;you
#s77	It's just a different way of representing this.
#c77	It;just a different way
#s78	Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM.
#c78	I;this way;it;what notations;people;they;SVM
#s79	This way you can.
#c79	This way;you
#s80	Better connected the slides with some other readings you might do.
#c80	the slides;some other readings;you
#s81	OK.
#s82	So.
#s83	When we maximize the margins of separate, it just means with the boundary of.
#c83	we;the margins;it;the boundary
#s84	The separate is only determined by a few data points, and these are the data points that we call support vectors.
#c84	a few data points;the data points;we;support vectors
#s85	So here are illustrated to support vectors for one class and two for the other class.
#c85	vectors;one class;the other class
#s86	At this, porters define the margin basically.
#c86	porters;the margin
#s87	And you can imagine once we know which are support vectors, then this center separate line will be determined by them so.
#c87	you;we;support vectors;this center separate line;them
#s88	The other data points actually don't really matter that much.
#c88	The other data points
#s89	And you can see if they you change other data points, it won't really affect the margin, so the separate with the stay the same mainly affected by the support vector machines.
#c89	you;they;you;other data points;it;the margin;the stay;the support vector machines
#s90	Sorry it's mainly affected by the support vectors and that's why it is called a support vector machine.
#c90	it;the support vectors;it;a support vector machine
#s91	OK, so.
#s92	The next question is of course, how can we set it up to optimize the line?
#c92	The next question;course;we;it;the line
#s93	How can we actually find the line?
#c93	we;the line
#s94	Or the separator.
#c94	Or the separator
#s95	Now this is equivalent to finding values for W&B because they would determine where exactly the separator is.
#c95	values;W&B;they;the separator
#s96	So in the simplest case, the linear osfm is just a simple optimization problem.
#c96	the simplest case;the linear osfm;just a simple optimization problem
#s97	So again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights
#c97	we;'s;our classifier;such a linear separator;we;weights;all the features;the main goal;these weights
#s98	W&B.
#c98	W&B.
#s99	And the classifier will say X is in category one if it's positive.
#c99	the classifier;X;category;it
#s100	Otherwise it's going to say it's in the other category.
#c100	it;it;the other category
#s101	So this is our assumption or setup.
#c101	our assumption;setup
#s102	So in the linear is UVM, we're going to then seek these parameter values to optimize the margins and then the training error.
#c102	the linear;UVM;we;these parameter values;the margins;then the training error
#s103	The training laid out would be basically like a in other classifiers we have a set of training points where we know the X vector
#c103	The training;other classifiers;we;a set;training points;we;the X vector
#s104	and then we also the corresponding label, why I?
#c104	we;also the corresponding label
#s105	An here we define why I as two values, but these two values are not 01 as you have seen before, but rather negative one and positive one and their corresponding to these two categories as I've shown here.
#c105	we;why I;two values;these two values;you;these two categories;I
#s106	Now you might wonder why we don't define them as zero and one, but instead of having negative 11 and this is purely for mathematical convenience, as you will see in a moment.
#c106	you;we;them;mathematical convenience;you;a moment
#s107	So the goal of optimization first is to make sure the labeling on training data is all correct.
#c107	the goal;optimization;the labeling;training data
#s108	So that just means if Yi, the known label, for instance XI is one we would like this classify value to be large.
#c108	Yi;the known label;instance XI;we;this classify value
#s109	And here we just choose threshold one here.
#c109	we;threshold;one
#s110	But if you use another threshold, you can see you can easily affect that constant into the parameter values B&W to make the right hand side.
#c110	you;another threshold;you;you;the parameter values;B&W;the right hand side
#s111	Just one.
#s112	Now, if, on the other hand, why I is negative one that means it's in a different class then we want this classifier to give us a very small value.
#c112	the other hand;I;negative one;it;a different class;we;this classifier;us;a very small value
#s113	In fact a negative value.
#c113	fact
#s114	And we want this value to be less than or equal to negative one.
#c114	we;this value
#s115	These are the two different instances, different kinds of cases and how can we combine them together now.
#c115	the two different instances;different kinds;cases;we;them
#s116	This is where it's convenient when we have chosen why I as negative one for the other category cause it turns out that we can easily combine the two into one constraint.
#c116	it;we;I;the other category;it;we;one constraint
#s117	Why I multiplied by the classifier value must be larger than or equal to 1?
#c117	I;the classifier value
#s118	An obviously when?
#s119	Why is just one you see.
#c119	you
#s120	This is the same as the constraint on the left hand side.
#c120	the constraint;the left hand side
#s121	But when Yi is negative one you also see a new.
#c121	Yi;negative one;you
#s122	This is equivalent to the other inequality, so this one actually captures both constraints in a unified way, and that's a convenient way of capturing these constraints.
#c122	the other inequality;this one;both constraints;a unified way;a convenient way;these constraints
#s123	What's our second goal?
#c123	What;our second goal
#s124	That's true.
#s125	Maximizing margin, right?
#c125	margin
#s126	So we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin.
#c126	we;the training data;all the cases;we;the data;we;the largest margin
#s127	Now the margin can be shown to be related to the magnitude of the weights.
#c127	the margin;the magnitude;the weights
#s128	The sum of squares of all those weights.
#c128	The sum;squares;all those weights
#s129	So this to have a small value for this expression.
#c129	a small value;this expression
#s130	It means all the eyes must be small.
#c130	It;all the eyes
#s131	So we've just assume that we have a constraint for the getting the data on the training set to be classified correctly.
#c131	we;we;a constraint;the data;the training
#s132	Now we also have the objective that's Tide to maximization of margin and this is simply to maximize sorry to minimize W transpose multiplied by W
#c132	we;the objective;Tide;maximization;margin;W transpose;W
#s133	and we often denote this by file
#c133	we;file
#s134	W.
#c134	W.
#s135	So now you can see this is basically optimization problem, right?
#c135	you;optimization problem
#s136	We have some variables to optimize and these are the weights and B and we have some constraints.
#c136	We;some variables;the weights;B;we;some constraints
#s137	These are linear constraints and the objective function is a quadratic function of the weights.
#c137	linear constraints;the objective function;a quadratic function;the weights
#s138	So this is a quadratic program with linear constraints and there are standard algorithms that are available for solving this problem.
#c138	a quadratic program;linear constraints;standard algorithms;this problem
#s139	And once we solve, the problem, will obtain the weights W&B
#c139	we;the problem;the weights
#s140	and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects.
#c140	us;a well defined the classifier;we;this classifier;any new texture objects
#s141	Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linearly separable.
#c141	the previous formulation;any error;the classification;the data
#s142	That means they may not look as nice as you have seen on the previous slide where align can separate all of them.
#c142	they;you;the previous slide;align;them
#s143	And what would happen if we.
#c143	what
#s144	Allow some errors.
#c144	some errors
#s145	The principle can stay right, so we want to minimize the training error, but try to also maximize the margin.
#c145	The principle;we;the training error;the margin
#s146	But in this case we have a soft margin because the data points may not be a completely separate bowl.
#c146	this case;we;a soft margin;the data points;a completely separate bowl
#s147	So it turns out that we can easily modify it as VM to accommodate this.
#c147	it;we;it;VM
#s148	So what you see here is very similar to what you have seen before, but we have introduced the extra variables.
#c148	what;you;what;you;we;the extra variables
#s149	Cassie I an we in fact will have one for each data instance and this is going to model the error that will allow for each instance.
#c149	Cassie I;we;fact;each data instance;the error;each instance
#s150	But the optimization problem will be very similar.
#c150	the optimization problem
#s151	So specifically, you will see we have added something to the optimization problem.
#c151	you;we;something;the optimization problem
#s152	First we have added some.
#c152	we
#s153	Some error to the constraint so that now we allow.
#c153	Some error;the constraint;we
#s154	Allow the classifier to make some mistakes here, so this KCI is allowed error if we set KCI to 0, then we go back to the original constraint.
#c154	the classifier;some mistakes;this KCI;error;we;KCI;we;the original constraint
#s155	We want every instance we classified accurately, but if we allow this to be.
#c155	We;every instance;we;we
#s156	Zero, then we allow some errors here.
#c156	we;some errors
#s157	In fact, the one CI is very large.
#c157	fact;the one CI
#s158	The error can be very, very large, so naturally we don't want this to happen.
#c158	The error;we
#s159	So we want to then also minimize this CI.
#c159	we;this CI
#s160	So Cassie, I needs to be minimized in order to control the error.
#c160	I;order;the error
#s161	And so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the errors as you see here, we simply take a sum over all the instances.
#c161	a result;the objective function;we;we;the weights;the errors;you;we;a sum;all the instances
#s162	Each one has a CI to model the error allowed for that instance an when we combine them together, we basically want to minimize the errors on.
#c162	Each one;a CI;the error;that instance;we;them;we;the errors
#s163	All of them.
#c163	them
#s164	Now you see there's a parameter.
#c164	you;a parameter
#s165	See here and that's a constant to control the tradeoff between minimizing the errors and maximizing the region of the margin if C is set to zero, you can see we go back to the original object function where we only maximize margin.
#c165	a constant;the tradeoff;the errors;the region;the margin;C;you;we;the original object function;we;margin
#s166	And we don't really optimize the training errors and then see I can be set to a very large value to make the constraints easy to satisfy.
#c166	we;the training errors;I;a very large value;the constraints
#s167	That's not very good of course, so see should be set to a non 0 value and a positive value.
#c167	course;a non 0 value;a positive value
#s168	But when she is settled very, very large value would see the objective function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role.
#c168	she;, very large value;the objective function;the training errors;the optimization;margin;a secondary role
#s169	So if that happens, what would happen?
#c169	what
#s170	What would happen is then we will try to do our best to minimize the training errors.
#c170	What;we;the training errors
#s171	But then we're not going to take care of the margin and that affects the generalization capacity of the classifier for future data.
#c171	we;care;the margin;the generalization capacity;the classifier;future data
#s172	So it's also not good.
#c172	it
#s173	So apparently this parameter C has to be actually set.
#c173	this parameter C
#s174	Carefully, and this is just like in the case of nearest neighbor way you need to optimize the number of neighbors.
#c174	the case;nearest neighbor way;you;the number;neighbors
#s175	Here you need to optimize the C and this is the general also achievable by doing cross validation.
#c175	you;the C;the general;cross validation
#s176	Basically you look at the empirical data to see what values should be set to in order to optimize the performance.
#c176	you;the empirical data;what values;order;the performance
#s177	Now with this modification in the problem, is there a quadratic program with linear constraints, so the optimization algorithm can be actually applied to solve this different version of the program?
#c177	this modification;the problem;a quadratic program;linear constraints;the optimization;algorithm;this different version;the program
#s178	Again, once we have obtained the weights and the bias, then we can have classified.
#c178	we;the weights;the bias;we
#s179	That's ready for classifying new objects.
#c179	new objects
#s180	So that's the basic idea of Sven.
#c180	the basic idea;Sven
#s181	So to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pros and cons, and the performance might also very different data sets for different problems.
#c181	the text categorisation methods;we;many methods;generative models;some more discriminative methods;no clear winner;each one;its pros;cons;the performance;different problems
#s182	Ann One reason is also becausw.
#c182	Ann One reason;becausw
#s183	The feature representation is very critical an so that these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here.
#c183	The feature representation;these methods;effective feature representation;we;domain knowledge;humans;important role
#s184	Although there are new machine learning methods like representation learning that can help with learning features.
#c184	new machine learning methods;features
#s185	An another common scene is that they might be.
#c185	An another common scene;they
#s186	Be performing similarly on the data set but with different mistakes and so their performance might be similar, but then the mistakes that make might be different, so that means it's useful to compare different methods for particular problem and then maybe combine multiple methods 'cause this can improve the robustness and they want to make the same mistakes so.
#c186	the data;different mistakes;their performance;the mistakes;it;different methods;particular problem;multiple methods;the robustness;they;the same mistakes
#s187	And symbol approaches that would combine different methods and tend to be more robust and can be useful in practice.
#c187	And symbol approaches;different methods;practice
#s188	Most techniques that we introduce the use supervised machine learning and which is a very general method.
#c188	Most techniques;we;the use;a very general method
#s189	So that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those.
#c189	these methods;any text categorization problem;we;humans;some training data;features;machine;an all these classifiers
#s190	Problems to solve the categorization problem.
#c190	Problems;the categorization problem
#s191	To allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data.
#c191	us;content;text;categories;the predictor;real world variables;text data
#s192	The computers of course here are trying to optimize the combinations of the features provided by human an.
#c192	The computers;course;the combinations;the features
#s193	As I say that there are many different ways of combining them and they also optimize different objects and functions.
#c193	I;many different ways;them;they;different objects;functions
#s194	But in order to achieve good performance, they all require effective features and also plenty of training data.
#c194	order;good performance;they;effective features;also plenty;training data
#s195	So as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better.
#c195	a general rule;you;the feature representation;more training data;you
#s196	So performance is often much more affected by the effectiveness of features and then by the choice of specific classifiers.
#c196	performance;the effectiveness;features;the choice;specific classifiers
#s197	So feature design tends to be more important than the choice of specific classifier.
#c197	feature design;the choice;specific classifier
#s198	So how do we design effective features?
#c198	we;effective features
#s199	Well, unfortunately this is very application specific, so there's no really much general thing to say here.
#c199	no really much general thing
#s200	But We can.
#c200	We
#s201	And do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us design features.
#c201	some analysis;the categorization problem;the what kind;features;us;categories;we;a lot;domain knowledge;us;design features
#s202	An another way to figure out effective features is to do error analysis on the categorisation results.
#c202	An another way;effective features;error analysis;the categorisation results
#s203	You could, for example, look at the which category tends to be confused with each other categories and you can use a confusion matrix to examine the errors systematically across categories, and then you can look into specific instances to see why the mistake has been made and what features can prevent the.
#c203	You;example;the which category;each other categories;you;a confusion matrix;the errors;categories;you;specific instances;the mistake;what features
#s204	This can allow you to obtain.
#c204	you
#s205	Insights for design new features.
#c205	Insights;design new features
#s206	So error analysis very important in general, and that's where you can get the insights about your specific problem.
#c206	So error analysis;you;the insights;your specific problem
#s207	And then finally we can leverage some machine learning techniques.
#c207	we;some machine;techniques
#s208	So for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you actually trainer for classifier, and sometimes training a classifier would also help you identify which features have high values.
#c208	example;feature selection;a technique;we;it;it;the most useful features;you;classifier;a classifier;you;which features;high values
#s209	And there are also other ways to ensure the sparsity of the model.
#c209	other ways;the sparsity;the model
#s210	Meaning to recognize the weights.
#c210	the weights
#s211	So for example, the SVM actually tries to minimize the weights on features, but you can further for some features to falsely use only a small number of features.
#c211	example;the SVM;the weights;features;you;some features;only a small number;features
#s212	There are also techniques for dimension reduction, and that's to reduce the high dimensional feature space into a lower dimensional space.
#c212	techniques;dimension reduction;the high dimensional feature space;a lower dimensional space
#s213	Typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models.
#c213	Typical biclustering;features;various ways;metrics factorization;such a job;the techniques;the topic models;we;topic models
#s214	LDA can actually help us reduce the dimension of features.
#c214	LDA;us;the dimension;features
#s215	Imagine the words are original feature representation, but the representation can be mapped to the topic space representation.
#c215	the words;original feature representation;the representation;the topic space representation
#s216	Let's say we have K topics, so a document cannot be represented as a vector of justice K values corresponding to the topics.
#c216	's;we;K topics;a document;a vector;justice K values;the topics
#s217	So we can let each topic define one dimension.
#c217	we;each topic;one dimension
#s218	So we have K dimensional space instead of the original high dimensional space corresponding to words.
#c218	we;K dimensional space;the original high dimensional space;words
#s219	And this is.
#s220	Often another way to learn factor features, especially, we could also use the categories to supervise learning of such low dimensional structures.
#c220	factor features;we;the categories;learning;such low dimensional structures
#s221	An so the original word features can be also combined with such such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful.
#c221	the original word features;such such latent dimension features;low dimensional space features;a multiresolution representation
#s222	Deep learning is a new technique that has been developed in machine learning.
#c222	Deep learning;a new technique;machine learning
#s223	It's particularly useful for learning representations, so different learning refers to deep neural network.
#c223	It;representations;so different learning;deep neural network
#s224	It's another kind of classifier where you can have intermediate features embedded in the model so that it's highly non linear classifier.
#c224	It;another kind;classifier;you;intermediate features;the model;it;highly non linear classifier
#s225	An some reason advance has allowed us to train such a complex network effectively.
#c225	An some reason advance;us;such a complex network
#s226	Ann is the technique has been shown to be quite effective for speech recognition, computer vision and recently it has been applied through text as well.
#c226	Ann;the technique;speech recognition;computer vision;it;text
#s227	It has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization.
#c227	It;some promise;one important advantage;this approach;relationship;the feature design;they;intermediate representations;compound features;effective representation;text localization
#s228	Although in Texas domain cause words are excellent representation of text content because these are.
#c228	Texas domain;words;excellent representation;text content
#s229	Humans invention for communication and they are generous sufficient for representing content for many tasks.
#c229	Humans;communication;they;content;many tasks
#s230	If there's a need for some new representation, people would have invented a new words and new World.
#c230	a need;some new representation;people;a new words;new World
#s231	So because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design.
#c231	this reason;the value;deep learning;text processing;computer vision;speech recognition;where there aren't corresponding wedding design
#s232	The words.
#c232	The words
#s233	As features.
#c233	As features
#s234	But deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words.
#c234	deep learning;effective features;complicated tasks;a sentiment analysis;it;bag;words
#s235	Regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor.
#c235	the training examples;it;a lot;training examples;it;human labor
#s236	But there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples.
#c236	some ways;one;some low quality training examples;a pseudo training examples
#s237	For example, if you take a reviews from the Internet, they might have overall ratings.
#c237	example;you;a reviews;the Internet;they;overall ratings
#s238	So to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then.
#c238	a sentiment categorizer;we;negative opinions;categorize reviews;these two categories
#s239	We could assume five star reviews are all positive training examples.
#c239	We;five star reviews;positive training examples
#s240	OnStar negative
#s241	but of course sometimes in five star reviews.
#c241	course;five star reviews
#s242	We also mention negative opinions so that rain example is not all of that high quality, but they can still be useful.
#c242	We;negative opinions;rain example;that high quality;they
#s243	Another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data.
#c243	Another idea;unable data;techniques;a semi supervised machine;techniques;you;label data;unlabeled data
#s244	So in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories.
#c244	our case;it;the mixture model;both text clustering;categorisation;you;a lot;unable text data;categorization;you;clustering;these text data;categories
#s245	And then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category.
#c245	these categories;the categories;the training data;we;which documents;which category
#s246	So you can in fact use the EM algorithm to actually combine both.
#c246	you;fact;the EM algorithm
#s247	That would allow you essentially to also pick up a useful words in the unlabeled data.
#c247	you;a useful words;the unlabeled data
#s248	You can think of this in another way.
#c248	You;another way
#s249	Basically, we can use, let's say a naive Bayes classifier to classify all the unlabeled text documents.
#c249	we;'s;a naive Bayes classifier;all the unlabeled text documents
#s250	And then we're going to assume the high confidence classification results, or actually reliable.
#c250	we;the high confidence classification results
#s251	Then you certainly have more training data.
#c251	you;more training data
#s252	The cause from the unlabeled data we some are labeled as category ones and more labeled as category two.
#c252	The cause;the unlabeled data;we;category ones;category
#s253	Although the label is not completely reliable.
#c253	the label
#s254	But then they can still be useful.
#c254	they
#s255	So let's assume they are actually training label examples
#c255	's;they;label examples
#s256	and then we combine them with the true training examples.
#c256	we;them;the true training examples
#s257	To improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that.
#c257	categorization method;this idea;the enable data and training data;we;other advanced machine;techniques;domain adaptation;transfer learning;we;some training examples;a related problem;a categorisation task
#s258	That involves data that follow very different distributions from what we are working on.
#c258	data;very different distributions;what;we
#s259	But basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data.
#c259	the two domains;we;the training domain;we;some signals;the related training data
#s260	So for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you.
#c260	example;training categorisation;news;you;an immediately effective classifier;topics;tweets;you;something;news;tweets;techniques;you
#s261	Do that effectively.
#s262	Here's a suggestion reading an where you can find more details about some of the methods that we have covered.
#c262	a suggestion;you;more details;the methods;we
410	804602d0-76c0-41f5-853b-82556a4ef6c6	187
#s1	This lecture is about the statistical language model.
#c1	This lecture;the statistical language model
#s2	In this lecture we're going to give an introduction to statistical language model.
#c2	this lecture;we;an introduction;statistical language model
#s3	This has to do with how do you model text data with probabilistic models so it's related to how we model query based on a document.
#c3	you;text data;probabilistic models;it;we;query;a document
#s4	We're going to talk about what is the language model and then we're going to talk about the simplest language model called a unigram language model, which also happens to be the most useful model for text retrieval.
#c4	We;what;the language model;we;the simplest language model;a unigram language model;the most useful model;text retrieval
#s5	And finally, we discussed possible uses of language model.
#c5	we;possible uses;language model
#s6	What is the language model?
#c6	What;the language model
#s7	It's just a probability distribution over word sequences.
#c7	It;just a probability distribution;word sequences
#s8	So here I show 1.
#c8	I
#s9	This model gives.
#c9	This model
#s10	The sequence today is Wednesday, a probability of 0.001.
#c10	The sequence;Wednesday;a probability
#s11	It gave today.
#c11	It
#s12	Wednesday is a very very small probability.
#c12	a very very small probability
#s13	Becauses amount for the medical.
#c13	Becauses
#s14	You can see the probability is given to these sentences or sequences of words can vary a lot depending on the model.
#c14	You;the probability;these sentences;sequences;words;a lot;the model
#s15	Therefore it's clearly context dependent.
#c15	it
#s16	In ordinary conversation, probably today is Wednesday is most popular among these sentences.
#c16	ordinary conversation;Wednesday;these sentences
#s17	But imagine in the context of discussing applied math, maybe the eigenvalues positive would have a higher probability.
#c17	the context;applied math;the eigenvalues;a higher probability
#s18	This means it can be used to represent the topic of the text.
#c18	it;the topic;the text
#s19	The Mortal Council be regarded as a probabilistic mechanism for generating text.
#c19	The Mortal Council;a probabilistic mechanism;text
#s20	And This is why it's also often called a generating model.
#c20	it;a generating model
#s21	So what does that mean?
#c21	what
#s22	We can imagine this is a mechanism.
#c22	We;a mechanism
#s23	That's visualized hands here as a stochastic system that can generate the sequences of words.
#c23	hands;a stochastic system;the sequences;words
#s24	So we can ask for a sequence and it's too simple sequence from the device if you want, and it might generate.
#c24	we;a sequence;it;too simple sequence;the device;you;it
#s25	For example, today is Wednesday.
#c25	example;Wednesday
#s26	But it could have generated any other sequences.
#c26	it;any other sequences
#s27	So for example there are many possibilities, right?
#c27	example;many possibilities
#s28	So this in this sense we can view our data as basically a sample observable from such a generating model.
#c28	this sense;we;our data;basically a sample;such a generating model
#s29	So why is such a model useful?
#c29	such a model
#s30	So many because it can quantify the uncertainties in natural language.
#c30	it;the uncertainties;natural language
#s31	Where do  unvertainties Come from it.
#c31	 unvertainties;it
#s32	One source is simply the ambiguity in natural language that we discussed earlier in Lab 2.
#c32	One source;the ambiguity;natural language;we;Lab
#s33	Another source is because we don't have complete understanding.
#c33	Another source;we;complete understanding
#s34	We lack all the knowledge to understand language.
#c34	We;all the knowledge;language
#s35	In that case there will be uncertainties as well, so let me show some examples of questions that we can answer with the language model that would have interesting application in different ways.
#c35	that case;uncertainties;me;some examples;questions;we;the language model;interesting application;different ways
#s36	Given that we see John and Fields.
#c36	we;John;Fields
#s37	How likely we see happy as opposed to habit as the next word in a sequence of words?
#c37	we;the next word;a sequence;words
#s38	Obviously this would be very useful for speech recognition, because happy and happy it would have similar acoustical sound acoustic signals.
#c38	speech recognition;it;similar acoustical sound acoustic signals
#s39	But if we look at the language model, will know that John feels happy would be far more likely than John feels habit.
#c39	we;the language model;John;John;habit
#s40	Another example, given that we observe baseball 3 times and game once in a news article, how likely is it about sports?
#c40	Another example;we;baseball;a news article;it;sports
#s41	This obviously is related to text categorization, an information retrieval.
#c41	text categorization;an information retrieval
#s42	Also, given that a user is interested in Sports News, how likely would the user used baseball in a query?
#c42	a user;Sports News;the user;baseball;a query
#s43	Now this is clearly related to the query likelihood that we discussed in the previous matching.
#c43	the query likelihood;we;the previous matching
#s44	So now let's look at the simplicity language model, called a unigram language model.
#c44	's;the simplicity language model;a unigram language model
#s45	In such a case.
#c45	such a case
#s46	We assume that we generate the text by generating each word independently.
#c46	We;we;the text;each word
#s47	So this means the probability of a sequence of words.
#c47	the probability;a sequence;words
#s48	Will be then the product of the probability of each world.
#c48	the product;the probability;each world
#s49	And normally they're not independent.
#c49	they
#s50	Right, so if you have seen a word like language that would make them far more likely to observe model than if you haven't seen the language.
#c50	you;a word;language;them;model;you;the language
#s51	So this assumption is not necessarily true, but we make this assumption to simplify the model.
#c51	this assumption;we;this assumption;the model
#s52	So now the model has precisely in parameters wherein is vocabulary size.
#c52	the model;parameters;vocabulary size
#s53	We have one probability for each word, and all these probabilities must sum to one.
#c53	We;one probability;each word;all these probabilities
#s54	So strictly speaking we actually have N-1 parameters.
#c54	we;N-1 parameters
#s55	As I said, text can be assumed to be assembled drawn from this world distribution.
#c55	I;text;this world distribution
#s56	So for example, now we can ask the device or the model to stochastic in general words for us instead of sequences.
#c56	example;we;the device;the model;general words;us;sequences
#s57	So instead of giving a whole sequence like today's Wednesday, it now gives us just one word and we can get all kinds of words, and we can assemble these words in a sequence.
#c57	a whole sequence;today's Wednesday;it;us;just one word;we;all kinds;words;we;these words;a sequence
#s58	So that would still allows little computer the probability of today's Wednesday as the product of the three probabilities.
#c58	little computer;the probability;today's Wednesday;the product;the three probabilities
#s59	As you can see, even though we have not asked the model to generate the sequence, it actually allows us to compute the probability for all the sequences.
#c59	you;we;the model;the sequence;it;us;the probability;all the sequences
#s60	But this model now only needs  N parameters to characterize.
#c60	this model;  N parameters
#s61	That means if we specify all the probabilities for all the words, then the models behavior is completely specified, whereas if we don't make this assumption we would have to specify probabilities for all kinds of combinations of words.
#c61	we;all the probabilities;all the words;the models behavior;we;this assumption;we;probabilities;all kinds;combinations;words
#s62	In sequences.
#c62	sequences
#s63	So by making this assumption, it makes it much easier to estimate these parameters, so let's see a specific example here.
#c63	this assumption;it;it;these parameters;'s;a specific example
#s64	Here I show two unigram language models with some probabilities and these are high probability words that are shown on top.
#c64	I;two unigram language models;some probabilities;high probability words;top
#s65	The first one clearly suggests a topic of text mining, because the high probability words are all related to this topic.
#c65	The first one;a topic;text mining;the high probability words;this topic
#s66	The second one is more related to health.
#c66	The second one;health
#s67	We can then ask the question, how likely will observe a particular text from each of these three models?
#c67	We;the question;a particular text;these three models
#s68	I suppose we sample words.
#c68	I;we;words
#s69	The former document.
#c69	The former document
#s70	Let's say we take the first distribution, which had a simple words.
#c70	's;we;the first distribution;a simple words
#s71	What words do you think it would be generated?
#c71	What words;you;it
#s72	Well, maybe text or maybe mining.
#c72	Well, maybe text;maybe mining
#s73	Maybe another word even food, which has a very small probability, might still be able to show up.
#c73	another word;even food;a very small probability
#s74	But in general, high probability words with likely show up more often.
#c74	But in general, high probability words
#s75	So we can imagine what gender the text that looks like a text mining.
#c75	we;the text;a text mining
#s76	In fact, there was a small probability you might be able to actually generate the actual text mining paper that would actually be meaningful, although the probability would be very very small.
#c76	fact;a small probability;you;the actual text mining paper;the probability
#s77	In the extreme case, you might imagine we might be able to generate the attacks paper text mining paper that would be accepted by a major conference.
#c77	the extreme case;you;we;the attacks paper text mining paper;a major conference
#s78	And in that case, the public in it would be even smaller.
#c78	that case;the public;it
#s79	But it's a non zero probability if we assume none of the words have non zero probability.
#c79	it;a non zero probability;we;none;the words;non zero probability
#s80	Similarly from the second topic, we can imagine we can generate the folder nutrition paper.
#c80	the second topic;we;we;the folder nutrition paper
#s81	That doesn't mean we cannot generate this paper from text mining.
#c81	we;this paper;text mining
#s82	Distribution.
#c82	Distribution
#s83	We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mine.
#c83	We;the probability;a paper;a major conference;text mine
#s84	So the point here is that given distribution.
#c84	the point;that given distribution
#s85	We can talk about the probability of observing a certain kind of text.
#c85	We;the probability;a certain kind;text
#s86	Some text will have higher probabilities than others.
#c86	Some text;higher probabilities;others
#s87	Now let's look at the problem in a different way.
#c87	's;the problem;a different way
#s88	Suppose we now have available of particular document.
#c88	we;particular document
#s89	In this case, maybe the abstract of a text reminding payroll.
#c89	this case;a text reminding payroll
#s90	And we see these world accounts here.
#c90	we;these world
#s91	The total number of words is 100.
#c91	The total number;words
#s92	Now the question will ask here is estimation question.
#c92	the question;estimation question
#s93	We can ask the question which model which word distribution has been used to generate this text.
#c93	We;the question;which model;which word distribution;this text
#s94	Assuming that the text that has been generated by sampling words from the distribution.
#c94	words;the distribution
#s95	So what would be your guess?
#c95	what;your guess
#s96	Have to decide what probability is.
#c96	what probability
#s97	Text mining etc would have.
#c97	Text mining
#s98	So pause the video for a second and try to think about your best guess.
#c98	the video;a second;your best guess
#s99	If you're like a lot of people, you would have guessed that my best guess is.
#c99	you;a lot;people;you;my best guess
#s100	text has a probability of 10 out of 100 because I've seen text 10 times an there are in total 100 words, so we simply not simply normalize these counts.
#c100	text;a probability;I;text;total 100 words;we;these counts
#s101	That's in fact the word justified, and your intuition is consistent with mathematical derivation, and this is called a maximum likelihood estimator.
#c101	fact;the word;your intuition;mathematical derivation;a maximum likelihood estimator
#s102	In this estimator we assume that the parameter settings are those that would give our observed data the maximum probability.
#c102	this estimator;we;the parameter settings;our observed data;the maximum probability
#s103	That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller.
#c103	we;these probabilities;the probability;the particular text data
#s104	So you can see this has a very simple formula.
#c104	you;a very simple formula
#s105	Basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length.
#c105	we;the count;a word;the document;the total number;words;the document;document length
#s106	Normalized frequency.
#c106	Normalized frequency
#s107	Or consequences of this is, of course we're going to assign zero probabilities to unseen words.
#c107	consequences;course;we;zero probabilities;unseen words
#s108	if we have not oveserve a word there will be no incentive to assign a non zero probability using this approach.
#c108	we;a word;no incentive;a non zero probability;this approach
#s109	Why 'cause that would take away probability mass for these ovbserved words?
#c109	probability mass;these ovbserved words
#s110	And that obviously wouldn't maximize the probability of this particular observer text data.
#c110	the probability;this particular observer text data
#s111	But one can still question whether this is our best estimate.
#c111	one;our best estimate
#s112	Well, the answer depends on what kind of model you want to find, right?
#c112	the answer;what kind;model;you
#s113	This is made.
#s114	It gives the best model based on this particular data.
#c114	It;the best model;this particular data
#s115	But if you're interested in a model that can explain the content of the four paper of this abstract, then you might have a second thought, right?
#c115	you;a model;the content;the four paper;this abstract;you;a second thought
#s116	So for one thing, there should be other words in the body of that article.
#c116	one thing;other words;the body;that article
#s117	So they should not have zero probabilities even though they are not observed in abstract.
#c117	they;zero probabilities;they
#s118	So we're going to cover this a little more later in discussing the query likelihood retrieval model model.
#c118	we;the query likelihood retrieval model model
#s119	So let's take a look at the some possible uses of this language.
#c119	's;a look;the some possible uses;this language
#s120	One use is simply to use it to represent the topics.
#c120	One use;it;the topics
#s121	So here I show some general English background text.
#c121	I;some general English background text
#s122	We can use this text to estimate the language model and the model might look like this.
#c122	We;this text;the language model;the model
#s123	So on the top will have those all common words like the is way etc
#c123	the top;those all common words;the is;way
#s124	and then we'll see some common words like these and then some very very rare words in the bottom.
#c124	we;some common words;then some very very rare words;the bottom
#s125	This is the background language model.
#c125	the background language model
#s126	It represents the frequency of words in English in general.
#c126	It;the frequency;words;English
#s127	Right, this is the background model.
#c127	the background model
#s128	Now let's look at the another text.
#c128	's;the another text
#s129	Maybe this time we'll look at the computer science research papers.
#c129	we;the computer science research papers
#s130	So we have a collection of computer science research papers we do estimation again.
#c130	we;a collection;computer science research papers;we;estimation
#s131	Again, we can just use the maximum microarrays better, where we simply normalize the frequencies.
#c131	we;the maximum microarrays;we;the frequencies
#s132	Now, in this case we will get the distribution that looks like this.
#c132	this case;we;the distribution
#s133	On the top.
#c133	the top
#s134	It looks similar because these words occur everywhere.
#c134	It;these words
#s135	They are very common, but as we go down we will see words that are more related to computer science, computer software, text, etc.
#c135	They;we;we;words;computer science;computer software;text
#s136	And so, although here we might also see these words, for example computer.
#c136	we;these words;example computer
#s137	But we can imagine the probability here is much smaller than the probability here, and we will see many other words here that would be more common in General English.
#c137	we;the probability;the probability;we;many other words;General English
#s138	So you can see this distribution characterizes the topic of the corresponding tents.
#c138	you;this distribution;the topic;the corresponding tents
#s139	We can look at the even the smaller text.
#c139	We;the even the smaller text
#s140	So in this case, let's look at the text mining paper.
#c140	this case;'s;the text mining paper
#s141	Now if we do the same, we have another distribution again.
#c141	we;we;another distribution
#s142	There can be expected to occur on the top, but soon we will see text mining Association clustering.
#c142	the top;we;text mining Association clustering
#s143	These words have.
#c143	These words
#s144	Relatively higher probabilities, in contrast in this distribution, will text has relatively small probability.
#c144	Relatively higher probabilities;contrast;this distribution;will text;relatively small probability
#s145	So this means again based on different attacks today or we can have a different model and model captures the topic.
#c145	different attacks;we;a different model;model;the topic
#s146	So we call this document language model and we call this collection language model.
#c146	we;this document language model;we;this collection language model
#s147	And later you will see how they are used in retrieval function.
#c147	you;they;retrieval function
#s148	But now let's look at the another use of this model.
#c148	's;the another use;this model
#s149	Can we statistically find what words are semantically related to computer?
#c149	we;what words;computer
#s150	Now how do we find the such words?
#c150	we;the such words
#s151	Well, our first thought is that let's take a look at the text that match computer so we can take a look at all the documents that contain the word computer.
#c151	our first thought;'s;a look;the text;computer;we;a look;all the documents;the word computer
#s152	Let's build a language model.
#c152	's;a language model
#s153	We can see what would we see there.
#c153	We;what;we
#s154	Not surprisingly, we see these common words on top.
#c154	we;these common words;top
#s155	As we always do so in this case, this language model gives us the conditional probability of seeing the world in the context of computer and these common words will naturally have high probabilities.
#c155	we;this case;this language model;us;the conditional probability;the world;the context;computer;these common words;high probabilities
#s156	But we also see computer itself and software will have relatively high probabilities.
#c156	we;computer;itself;software;relatively high probabilities
#s157	But if we just use this model, we cannot just say all these words are semantically related to computer.
#c157	we;this model;we;all these words;computer
#s158	So intuitively we would like to get rid of these.
#c158	we
#s159	Help.
#c159	Help
#s160	These common words.
#c160	These common words
#s161	How can we do that?
#c161	we
#s162	It turns out that it's possible to use, langage model to do that.
#c162	It;it;langage model
#s163	I suggested you don't think about that.
#c163	I;you
#s164	So how can we know what words are very common so that we want to kind of get rid of them?
#c164	we;what words;we;them
#s165	What model would tell us that? "
#c165	What model;us
#s166	" Maybe you can think about that.
#c166	you
#s167	So the background language model precisely tells us this.
#c167	the background language model;us
#s168	Information tells us what words are common in general.
#c168	Information;us;what words
#s169	So if we use this background model, we would know that these words are common words in general, so it's not surprising to observe them in the context of computer.
#c169	we;this background model;we;these words;common words;it;them;the context;computer
#s170	Where is the computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software.
#c170	the computer;a very small probability;it;we;computer;this probability;software
#s171	So then we can use these two models to somehow figure out the words that are related to the computer.
#c171	we;these two models;the words;the computer
#s172	For example, we can simply take the ratio of these two probabilities or normalize the topic language model by the probability of the world in the background language model.
#c172	example;we;the ratio;these two probabilities;the topic language model;the probability;the world;the background language model
#s173	So if we do that, we take the ratio, will see that, then on the top computer is ranked and then followed by software program.
#c173	we;we;the ratio;the top computer;software program
#s174	All these words are related to computer.
#c174	All these words;computer
#s175	Because they occur frequently in the context of computer, but not frequently in the whole collection.
#c175	they;the context;computer;the whole collection
#s176	Whereas these common words will not have a high probability.
#c176	these common words;a high probability
#s177	In fact they have ratio about one down there because they are not really related to computer.
#c177	fact;they;ratio;they;computer
#s178	By taking the sample of text that contains the computer, we don't really see more occurrences of them than in general.
#c178	the sample;text;the computer;we;more occurrences;them
#s179	So this shows that the even with these simple language models we can do some limited analysis of semantics.
#c179	these simple language models;we;some limited analysis;semantics
#s180	So in this lecture we talked about.
#c180	this lecture;we
#s181	Language model, which is basically probability distribution over text.
#c181	Language model;probability distribution;text
#s182	We talked about the simplest language model called unigram them model which is also just a word distribution.
#c182	We;the simplest language model;unigram;them;model;just a word distribution
#s183	We talked about the two uses of a language model one is represented topic in a document in the collection or in general the other is rediscovered water associations.
#c183	We;the two uses;a language model;topic;a document;the collection;water associations
#s184	In the next lecture, we're going to talk about how, then which model can be used to design retrieval function.
#c184	the next lecture;we;which model;retrieval function
#s185	Here are two additional readings.
#c185	two additional readings
#s186	The first is textbook on statistical natural language processing.
#c186	textbook;statistical natural language processing
#s187	The second is article that has a survey of statistical language models with a lot of pointers to research work.
#c187	article;a survey;statistical language models;a lot;pointers;work
410	813e04dd-8723-4326-9a07-ddadd23a8632	81
#s1	There are some interesting challenges in threshold learning in the filtering problem.
#c1	some interesting challenges;threshold learning;the filtering problem
#s2	So here I show the historical data that you can collect in a filtering system so you can see the scores and the status of relevance.
#c2	I;the historical data;you;a filtering system;you;the scores;the status;relevance
#s3	So the first one it has a score of 36.5
#c3	So the first one;it;a score
#s4	and it's relevant.
#c4	it
#s5	The second one is non-relevant and etc.
#c5	The second one
#s6	Of course we have a lot of documents for which we don't know the status because we have never delivered them to the user.
#c6	we;a lot;documents;we;we;them;the user
#s7	So as you can see here, we only see the judgments of documents delivered to the user, so this is not a random sample, so it's censored data.
#c7	you;we;the judgments;documents;the user;a random sample;it;censored data
#s8	It's kind of biased.
#c8	It
#s9	So that creates some difficulty for learning, and secondly there are in general very little labeled data,  and very few relevant data, so it's also challenging for machine learning approaches.
#c9	some difficulty;learning;general very little labeled data;relevant data;it;machine learning approaches
#s10	Typically they require more training data, and in the extreme case at the beginning we don't even have any label data as well.
#c10	they;more training data;the extreme case;the beginning;we;any label data
#s11	The system still has to make a decision, so that's a very difficult problem at the beginning.
#c11	The system;a decision;a very difficult problem;the beginning
#s12	Finally, there is also this issue of exploration versus exploitation tradeoff.
#c12	this issue;exploration;exploitation tradeoff
#s13	Now this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we haven't delivered.
#c13	we;the document space;the user;documents;we
#s14	So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the users' interests so well.
#c14	other words;we;the space;user interests;the user;some other documents;the users' interests
#s15	So how do we do that?
#c15	we
#s16	Well, we could lower the threshold a little bit and do just deliver some near misses to the user to see what the user would respond,  to see how the user would respond to this extra document.
#c16	we;the threshold;some near misses;the user;what;the user;the user;this extra document
#s17	And this is the trade-off because on the one hand you want to explore, but on the other hand you don't want to really explore too much 'cause then you would overdeliver non-relevant information.
#c17	the trade-off;the one hand;you;the other hand;you;you;non-relevant information
#s18	So exploitation means you would exploit what you learned about user.
#c18	exploitation;you;what;you;user
#s19	Let's say you know the user is interested in this particular topic so you don't want to deviate that much.
#c19	's;you;the user;this particular topic;you
#s20	But if you don't deviate at all then you don't explore it all.
#c20	you;you;it
#s21	That's also not good.
#s22	You might miss opportunity to learn another interest of the user.
#c22	You;opportunity;another interest;the user
#s23	So this is a dilemma.
#c23	a dilemma
#s24	And that's also a difficult problem to solve.
#c24	a difficult problem
#s25	Now how do we solve these problems?
#c25	we;these problems
#s26	In general,  I think one can use the empirical utility optimization strategy, and this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide.
#c26	I;one;the empirical utility optimization strategy;this strategy;the threshold;historical data;you;the previous slide
#s27	So you can just compute the utility on the training data for each candidate  score threshold.
#c27	you;the utility;the training data;each candidate;  score threshold
#s28	Pretend what if I cut at this point.
#c28	I;this point
#s29	What if I can cut at a different scoring threshold point what would happen, what's utility?
#c29	What;I;a different scoring threshold point;what;what;utility
#s30	Since these are training data, we can kind of compute the utility, right?
#c30	data;we;the utility
#s31	We know their relevance status or we assume that we know relevant status that's based on approximation of clickthroughs.
#c31	We;their relevance status;we;we;relevant status;approximation;clickthroughs
#s32	So then we can just choose the threshold that gives the maximum utility on the training data.
#c32	we;the threshold;the maximum utility;the training data
#s33	But this of course doesn't account for exploration that we just talked about.
#c33	course;exploration;we
#s34	And there is also the difficulty of bias training sample as we mentioned.
#c34	the difficulty;bias training sample;we
#s35	So in general we can only get upper bound for the true optimal threshold, because the threshold might be actually lower than this.
#c35	we;the true optimal threshold;the threshold
#s36	So it's possible that the discarded item might be actually interesting to the user.
#c36	it;the discarded item;the user
#s37	So how do we solve this problem where we generate and as I said, we can lower the threshold to explore a little bit, so here's one particular approach called better gamma threshold learning.
#c37	we;this problem;we;I;we;the threshold;a little bit;one particular approach;better gamma threshold learning
#s38	So the idea is following.
#c38	the idea
#s39	So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions.
#c39	I;a ranked list;all the training documents;we;they;their positions
#s40	And on the y-axis,  We show the utility.
#c40	the y-axis;We;the utility
#s41	Of course this function depends on how you specify the coefficients in the utility function, but we can then imagine.
#c41	this function;you;the coefficients;the utility function;we
#s42	that depending on the cut off position, we will have a utility that means.
#c42	the cut off position;we;a utility
#s43	Suppose I cut at this position and that would be the utility.
#c43	I;this position;the utility
#s44	So we can, for example identify some cutting cut off point.
#c44	we;example;some cutting;point
#s45	The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold.
#c45	The optimal point theta;the point;we;the maximum utility;we;this threshold
#s46	And there is also zero threshold zero utility threshold, and you can see at this cut off the utility is 0.
#c46	zero threshold;zero utility threshold;you;this cut;the utility
#s47	Now what does that mean?
#c47	what
#s48	That means if I lower the threshold a little bit and now I reach this threshold, the utility would be lower, but it's still positive it's still non negative at least.
#c48	I;the threshold;I;this threshold;the utility;it;it
#s49	So it's not as high as the optimal utility.
#c49	it;the optimal utility
#s50	But it gives us a safe point to explore the threshold.
#c50	it;us;a safe point;the threshold
#s51	As I just explained, it's desirable to explore the interest space, so it's desirable to lower the threshold based on your training data.
#c51	I;it;the interest space;it;the threshold;your training data
#s52	So that means in general we want to set the threshold somewhere in this range.
#c52	we;the threshold;this range
#s53	Let's say we can use alpha to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold.
#c53	's;we;alpha;the deviation;the optimal utility point;you;the formula;the threshold;just the interpolation;the zero utility threshold;the optimal utility threshold
#s54	Now the question is how should we set alpha?
#c54	the question;we;alpha
#s55	And when should we deviate more from the optimal utility point?
#c55	we;the optimal utility point
#s56	Well this can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point, but rather we're going to use other parameters to further define alpha, and this specifically is as follows.
#c56	multiple factors;one way;the problem;this threshold mechanism;the zero point;a safe point;we;the zero point;we;other parameters;alpha
#s57	So there will be a beta parameter to control the deviation from the optimal threshold, and this can be based on for example can be accounting for the overfitting to the training data let's say.
#c57	a beta parameter;the deviation;the optimal threshold;example;the overfitting;the training data;'s
#s58	And so this can be just adjustment factor.
#c58	just adjustment factor
#s59	But what's more interesting is this gamma parameter here and you can see in this formula,  gamma is controlling the influence of the number of examples in training dataset.
#c59	what;this gamma parameter;you;this formula;  gamma;the influence;the number;examples;training;dataset
#s60	So you can see it in this formula as N which denotes the number of training examples becomes bigger, then it would actually encourage less exploration.
#c60	you;it;this formula;the number;training examples;it;less exploration
#s61	In other words, when N is very small, it would try to explore more, and that just means if we have seen few examples we're not sure whether we have exhausted the space of interests.
#c61	other words;N;it;we;few examples;we;we;the space;interests
#s62	So we would explore.
#c62	we
#s63	But as we have seen many examples from the user, many data points, then we feel that we probably don't have to explore more.
#c63	we;many examples;the user;we;we
#s64	So this gives us a dynamic strategy for exploration, right?
#c64	us;a dynamic strategy;exploration
#s65	The more examples we have seen, the less explosion we're going to do, so the threshold would be closer to the optimal threshold.
#c65	The more examples;we;the less explosion;we;the threshold;the optimal threshold
#s66	So that's the basic idea of this approach.
#c66	the basic idea;this approach
#s67	Now this approach, it actually has been working well in some evaluation studies empirically effective.
#c67	it;some evaluation studies
#s68	And also can work on arbitrary utility with a proper lower bound.
#c68	arbitrary utility
#s69	And it explicitly addresses the exploration- exploitation tradeoff and it kind of uses the zero utility threshold point as a safeguard for exploration and exploitation tradeoff, we are never going to explore further than the zero utility point.
#c69	it;the exploration- exploitation tradeoff;it;the zero utility threshold point;a safeguard;exploration and exploitation tradeoff;we;the zero utility point
#s70	So if you take the analogy of gambling and you don't want to risk on losing money, so it's a safe strategy.
#c70	you;the analogy;gambling;you;money;it;a safe strategy
#s71	The conservative strategy for exploration.
#c71	The conservative strategy;exploration
#s72	And the problem is, of course this approach is purely heuristic.
#c72	the problem;course;this approach
#s73	And the zero utility lower bound is also often too conservative.
#c73	the zero utility
#s74	And there are of course more advanced machine learning approaches that have been proposed for solving these problems, and this is the active research area.
#c74	course;more advanced machine learning approaches;these problems;the active research area
#s75	So to summarize, there are two strategies for recommender systems or filtering systems.
#c75	two strategies;recommender systems;filtering systems
#s76	One is content based which is looking at the item similarity.
#c76	the item similarity
#s77	The other is collaborative filtering, which is looking at the user similarity.
#c77	collaborative filtering;the user similarity
#s78	In this lecture, we've covered the content based filtering approach in the next lecture we're going to talk about collaborative filtering.
#c78	this lecture;we;the content based filtering approach;the next lecture;we;collaborative filtering
#s79	  
#s80	In content-based filtering system, We generally have to solve several problems related to filtering decision and learning etc.
#c80	content-based filtering system;We;several problems;filtering decision
#s81	And such a system can actually be built based on a search engine system by adding a threshold mechanism, and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user.
#c81	such a system;a search engine system;a threshold mechanism;adaptive learning algorithm;the system;long-term feedback;the user
410	83629c6e-4221-4f2e-a108-ff2c58784242	147
#s1	This lecture is about how to do fast search by using inverted index.
#c1	This lecture;fast search;inverted index
#s2	In this lecture, we're going to continue the discussion of system implementation.
#c2	this lecture;we;the discussion;system implementation
#s3	In particular, we're going to talk about how to support fast search by using inverted index.
#c3	we;fast search;inverted index
#s4	So let's think about what a general scoring function might look like.
#c4	's;what;a general scoring function
#s5	Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form.
#c5	course;the vector space model;a special case;we;many other retrieval functions;the same form
#s6	So the form of this function is as follows.
#c6	the form;this function
#s7	We see this scoring function of document d and query q is defined as first a function of f(a).
#c7	We;this scoring function;document d;query q;a function;f(a
#s8	That's adjustment function that would consider two factors that are shown here at the end
#c8	adjustment function;two factors;the end
#s9	f sub d of (d) and f sub q of (q).
#c9	f sub d;(d;f sub;q;q
#s10	These are adjustment factors of document and query so they are at the level of a document and query.
#c10	adjustment factors;document;query;they;the level;a document;query
#s11	And then inside of this function we also see there's another function called h.
#c11	this function;we;another function
#s12	So this is the main part of the scoring function.
#c12	the main part;the scoring function
#s13	And these as I just said, are the scoring factors at the level of the whole document and query.
#c13	I;the scoring factors;the level;the whole document;query
#s14	For example document lengths.
#c14	example
#s15	And this aggregate functioning would then combine all these.
#c15	this aggregate functioning
#s16	Now inside this edge function there are functions that would compute the weights of the contribution of a matched query term t(i).
#c16	this edge function;functions;the weights;the contribution;a matched query term
#s17	So this is g.
#c17	g.
#s18	The function g gives us the weight of a match query term t(i) in document d.
#c18	The function;g;us;the weight;a match query term;document d.
#s19	And this h function would then aggregate all these weights, so it will for example, take a sum of all the matched query terms.
#c19	this h function;all these weights;it;example;a sum;all the matched query terms
#s20	But it can also be a product or could be another way of aggregating them.
#c20	it;a product;another way;them
#s21	And then finally, this adjustment function would then consider the document level or query level factors to further adjust the score.
#c21	this adjustment function;the document level or query level factors;the score
#s22	For example, documents normalization.
#c22	example
#s23	So this general form would cover many state of the art retrieval functions.
#c23	this general form;many state;the art retrieval functions
#s24	Let's look at how we can score documents with such a function using inverted index.
#c24	's;we;documents;such a function;inverted index
#s25	So here's a general algorithm that works as follows.
#c25	a general algorithm
#s26	First these query level and document level factors can be precomputed in the indexing time.
#c26	these query level and document level factors;the indexing time
#s27	Of course, for the query we have the computed at query time, but for document, for example, document lengths can be precomputed.
#c27	the query;we;query time;document;example;document lengths
#s28	And then we'll maintain a score accumulator for each document d to compute h.
#c28	we;a score accumulator;each document;h.
#s29	And h is the aggregation function of all the matched query terms.
#c29	h;the aggregation function;all the matched query terms
#s30	So how do we do that?
#c30	we
#s31	Well, for each query term we're going to fetch the inverted list from the inverted index.
#c31	each query term;we;the inverted list;the inverted index
#s32	This would give us all the documents that match this query term.
#c32	us;all the documents;this query term
#s33	And that includes d1, f1 through dn, fn.
#c33	d1;f1;dn
#s34	So each pair is document ID and the frequency of the term in the document.
#c34	each pair;document ID;the frequency;the term;the document
#s35	Then for each entry dj and fj are particular match of the term in this particular document dj.
#c35	each entry dj;fj;particular match;the term;this particular document dj
#s36	We're going to compute the function g. That would give us something like a TF if weights of this term.
#c36	We;the function g.;us;something;a TF;this term
#s37	So we'll compute the weighted contribution of matching this query term in this document.
#c37	we;the weighted contribution;this query term;this document
#s38	And then we're going to update the score accumulator for this document.
#c38	we;the score accumulator;this document
#s39	And this would allow us to add this to a accumulator that would incrementally compute the function h.
#c39	us;a accumulator;the function
#s40	So this is basically a general way to allow us to do computer all functions of this form by using inverted index.
#c40	a general way;us;all functions;this form;inverted index
#s41	Note that we don't have to touch any document that didn't match any query term.
#c41	we;any document;any query term
#s42	But this is why it's fast.
#c42	it
#s43	We only need to process the document that matched at least one query term.
#c43	We;the document;at least one query term
#s44	In the end, then, we're going to adjust the score to compute this function Fa, and then we can sort.
#c44	the end;we;the score;this function;we
#s45	So let's take a look at the specific example.
#c45	's;a look;the specific example
#s46	In this case, let's assume the scoring function is very simple while it just takes the sum of TF.
#c46	this case;'s;the scoring function;it;the sum;TF
#s47	The raw TF.
#c47	The raw TF
#s48	The count of a term in the document.
#c48	The count;a term;the document
#s49	Now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include other weights, like the transformation of TF or document length  normalization or IDF weighting.
#c49	this simplification;the algorithm;it;the computation;other weights;the transformation;TF;document length;  normalization;IDF weighting
#s50	So let's take a look at a specific example where the queries information security.
#c50	's;a look;a specific example
#s51	And I show some entries of the inverted index on the right side information occurred four documents and their frequencies.
#c51	I;some entries;the inverted index;the right side information
#s52	Also their security occurred in three documents.
#c52	their security;three documents
#s53	So let's see how the algorithm works.
#c53	's;the algorithm
#s54	So first we iterate over all the query terms.
#c54	we;all the query terms
#s55	An we fetch the first query them what is that?
#c55	we;the first query;them;what
#s56	That's information.
#c56	information
#s57	Imagine we have all these score accumulators to store the scores for these documents.
#c57	we;all these score accumulators;the scores;these documents
#s58	We can imagine there will be allocated but then they will only be allocated as needed.
#c58	We;they
#s59	So before we do any weighting of terms, we don't even need a score accumulator.
#c59	we;any weighting;terms;we;a score accumulator
#s60	But conceptually we have these score accumulators eventually allocated.
#c60	we;these score accumulators
#s61	Let's fetch the entries from the inverted list for information first.
#c61	's;the entries;the inverted list;information
#s62	That's the first one.
#s63	So these score accumulators obviously will be initialized at 0.
#c63	these score accumulators
#s64	So the first answer is d1 and 3. 3 is the occurrences of information in this document.
#c64	the first answer;d1;the occurrences;information;this document
#s65	Since our scoring function assumes that the score is just a sum of these raw counts, we just need to add 3 to the score accumulator to account for the increase of score due to matching this term information in document d1.
#c65	our scoring function;the score;just a sum;these raw counts;we;the score accumulator;the increase;score;this term information;document d1
#s66	And then we go to the next entry.
#c66	we;the next entry
#s67	That's d2 and 4
#c67	d2
#s68	and then we added 4 to the score accumulator of d2.
#c68	we;the score accumulator;d2
#s69	Of course, at this point that we will allocated the score accumulator as needed.
#c69	this point;we;the score accumulator
#s70	And so, at this point we allocated d1 and d2.
#c70	this point;we
#s71	The next is d3 and we add 1.
#c71	d3;we
#s72	We allocate another score cumulative for d3 and add 1 to it.
#c72	We;d3;it
#s73	And then finding the d4 gets a 5 because the term information occurred five times in this document.
#c73	the d4;the term information;this document
#s74	OK, so this completes the processing of all the entries in the inverted index for information.
#c74	the processing;all the entries;the inverted index;information
#s75	It processed all the contributions of matching information in these four documents.
#c75	It;all the contributions;matching information;these four documents
#s76	So now our algorithm will go to the next query term that security.
#c76	our algorithm;the next query term;that security
#s77	So we're going to fetch all the inverted index entries for security.
#c77	we;all the inverted index entries;security
#s78	So in this case there are three entries and we're going to go through each of them.
#c78	this case;three entries;we;them
#s79	The first is d2 and 3, and that means security occurs three times in d2.
#c79	d2;security;d2
#s80	And what do we do?
#c80	what;we
#s81	Well, we do exactly the same as what we did for information, so this time we're going to change the score accumulator d2, since it's already allocated.
#c81	we;what;we;information;we;the score accumulator d2;it
#s82	And what we do is to add 3 to the existing value which is 4.
#c82	what;we;the existing value
#s83	So we now get the 7 for d2.
#c83	we;d2
#s84	d2 score is increased because it matched both the information and security.
#c84	d2 score;it;both the information;security
#s85	Go to the next entry.
#c85	the next entry
#s86	That's d4 and 1 so we would update the score for d4 and again we add 1 to d4.
#c86	d4;we;the score;d4;we;d4
#s87	So d4 now goes from 5 to 6.
#c87	d4
#s88	Finally, we process d5 and 3.
#c88	we;d5
#s89	Since we have not yet allocated a score accumulator for d5.
#c89	we;a score accumulator;d5
#s90	At this point, we're going to allocate 1 for d5, and we're going to add 3 to it, So those scores on the last row are the final scores for these documents if our scoring function is just a simple sum of TF values.
#c90	this point;we;d5;we;it;those scores;the last row;the final scores;these documents;our scoring function;just a simple sum;TF values
#s91	Now, what if we actually would like to do length normalization?
#c91	we;length normalization
#s92	We can do the normalization at this point for each document.
#c92	We;the normalization;this point;each document
#s93	So to summarize this you can see we first processed the query term information.
#c93	you;we;the query term information
#s94	We processed all the entries in the inverted index for this term.
#c94	We;all the entries;the inverted index;this term
#s95	Then we processed the security.
#c95	we;the security
#s96	It's worth thinking about what should be the order of processing here.
#c96	It;what;the order;processing
#s97	when we consider query terms.
#c97	we;query terms
#s98	It might make difference, especially if we don't want to keep all the score accumulators.
#c98	It;difference;we;all the score accumulators
#s99	Let's say we only want to keep the most promising score accumulators.
#c99	's;we;the most promising score accumulators
#s100	What do you think it would be a good order to go through?
#c100	What;you;it;a good order
#s101	Would you go.
#c101	you
#s102	Would you process a common term first or would you process a rare term first?
#c102	you;a common term;you;a rare term
#s103	The answer is we should process the rare term first.
#c103	The answer;we;the rare term
#s104	A rare term would match fewer documents and then the score contribution would be higher because the idea of value will be higher.
#c104	A rare term;fewer documents;the score contribution;the idea;value
#s105	And then it allows us to touch the most promising documents first, so it helps pruning some non promising ones if we don't need to so many documents to be returned to the user.
#c105	it;us;the most promising documents;it;some non promising ones;we;so many documents;the user
#s106	Right, so those are all heuristics for further improving the accuracy here.
#c106	heuristics;the accuracy
#s107	You can also see how we can incorporate the IDF weighting so they can easily be incorporated when we process each query term.
#c107	You;we;the IDF weighting;they;we;each query term
#s108	When we fetch the inverted index, we can fetch the document frequency and then we can compute the IDF.
#c108	we;the inverted index;we;the document frequency;we;the IDF
#s109	Or maybe perhaps the IDF value has already been precomputed.
#c109	the IDF value
#s110	When we index the documents at that time, we already computed the IDF value that we can just fetch it.
#c110	we;the documents;that time;we;the IDF value;we;it
#s111	So all these can be done at this time, so that would mean when we process all the entries for information, these weights will be adjusted by the same IDF, which is IDF for information.
#c111	this time;we;all the entries;information;these weights;the same IDF;IDF;information
#s112	So this is the basic idea of using inverted index for faster search and it works well for all kinds of formulas that are of the general form.
#c112	the basic idea;inverted index;faster search;it;all kinds;formulas;the general form
#s113	This general form covers actually most state of the art retrieval functions.
#c113	This general form;most state;the art retrieval functions
#s114	So there are some tricks to further improve the efficiency.
#c114	some tricks;the efficiency
#s115	Some general techniques include the caching.
#c115	Some general techniques;the caching
#s116	This is just to store some results of popular queries so that next time when you see the same query, you simply return the stored results.
#c116	some results;popular queries;you;the same query;you;the stored results
#s117	Similarly, you can also store the list of inverted index in the memory for popular term and if the query terms are popular, likely you will soon need to fetch the inverted index for the same term again.
#c117	you;the list;inverted index;the memory;popular term;the query terms;you;the inverted index;the same term
#s118	So keeping them in the memory would help and these are general techniques for improving efficiency.
#c118	them;the memory;general techniques;efficiency
#s119	We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents.
#c119	We;only the most promising accumulators;a user;so many documents
#s120	We only need to return high quality subset of documents that likely are ranked on the top.
#c120	We;high quality subset;documents;the top
#s121	For that purpose, we can then prune the accumulators.
#c121	that purpose;we;the accumulators
#s122	We don't have to store all the accumulators.
#c122	We;all the accumulators
#s123	At some point, we just keep the highest value accumulators.
#c123	some point;we;the highest value accumulators
#s124	Another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and to scale up to the web scale, we need a special to have special techniques to do parallel processing and to distribute the storage of files on multiple machines.
#c124	Another technique;parallel processing;such a large data;the web data;the web scale;we;a special;special techniques;parallel processing;the storage;files;multiple machines
#s125	So here, as here's a list of some text retrieval tool kits, it's not a complete list.
#c125	a list;some text retrieval tool kits;it;a complete list
#s126	You can find more information at this URL on the bottom.
#c126	You;more information;this URL;the bottom
#s127	Here is the four here Lucene is one of the most popular toolkits that can support a lot of applications and it has very nice support for applications.
#c127	here Lucene;the most popular toolkits;a lot;applications;it;very nice support;applications
#s128	You can use it to build a search engine application very quickly.
#c128	You;it;a search engine application
#s129	The downside is that it's not that easy to extend it and algorithms implemented there also not the most advanced algorithms.
#c129	The downside;it;it;algorithms;not the most advanced algorithms
#s130	Lemur/Indri is another tool kit that does not have such a nice support for application as lucene, but it has many advanced search algorithms.
#c130	Lemur/Indri;another tool kit;such a nice support;application;it;many advanced search algorithms
#s131	And it's also easy to extend.
#c131	it
#s132	Terrier is yet another tool kit that also has good support for application capability and some advanced algorithms, so that's maybe in between Lemur or Lucene or maybe rather combining the strength of both, so that's also a useful tool kit.
#c132	another tool kit;good support;application capability;some advanced algorithms;Lemur;Lucene;the strength;a useful tool kit
#s133	MeTA is the tool kit that we will use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms.
#c133	MeTA;the tool kit;we;the programming assignment;a new tool kit;a combination;both text retrieval algorithms;text mining algorithms
#s134	And so topic of all those models are implemented there.
#c134	so topic;all those models
#s135	There are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms.
#c135	a number;text analysis algorithms;the toolkit;basic search algorithms
#s136	So to summarize, all the discussion about the system implementation, here are the major takeaway points.
#c136	all the discussion;the system implementation;the major takeaway points
#s137	Inverted index is the primary data structure for supporting a search engine.
#c137	Inverted index;the primary data structure;a search engine
#s138	That's the key to enable faster response to a user's query.
#c138	the key;faster response;a user's query
#s139	And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate so that we can save disk space and can speed up IO and processing of inverted index.
#c139	the basic idea;the data;we;we;compression;we;disk space;IO;processing;inverted index
#s140	In general we talked about how to construct the inverted index when the data can't fit into the memory, and then we talk about the fast search using inverted index.
#c140	we;the inverted index;the data;the memory;we;the fast search;inverted index
#s141	Basically to exploit the inverted index to accumulate the scores for documents matching or query term.
#c141	the inverted index;the scores;documents;query term
#s142	And we explore the Zipf's law to avoid attaching many documents that don't match any query term.
#c142	we;the Zipf's law;many documents;any query term
#s143	And this algorithm can for his support a wide range of ranking algorithms.
#c143	this algorithm;his support;ranking algorithms
#s144	So these basic techniques have great potential for further scaling up using distributed file system, parallel processing and caching.
#c144	these basic techniques;great potential;distributed file system;parallel processing;caching
#s145	Here are two additional readings that you can take a look if you have time and you're interested in learning more about this.
#c145	two additional readings;you;a look;you;time;you
#s146	The first one is a classic textbook about the efficiency of Inverted index and compression techniques and how to in general build efficient search engine in terms of the space, overhead and speed.
#c146	a classic textbook;the efficiency;Inverted index;compression techniques;efficient search engine;terms;the space;overhead;speed
#s147	The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.
#c147	The second one;a newer textbook;a nice discussion;search engines
410	83fb05d8-e122-4a84-b857-df197e4c5662	120
#s1	This lecture is about the query likelihood probabilistic retrieval model.
#c1	This lecture;the query likelihood probabilistic retrieval model
#s2	In this lecture we continue the discussion of probabilistic retrieval model.
#c2	this lecture;we;the discussion;probabilistic retrieval model
#s3	In particular, we're going to talk about the query likelihood retrieval function.
#c3	we;the query likelihood retrieval function
#s4	In the query likelihood retrieval model.
#c4	the query likelihood retrieval model
#s5	Our idea is to model how likely a user who likes a document would pose a particular query.
#c5	Our idea;a user;who;a document;a particular query
#s6	So in this case you can imagine if a user likes this particular document about the presidential campaign news.
#c6	this case;you;a user;this particular document;the presidential campaign news
#s7	Then we can assume the user would use this document as a basis to post a query to try to retrieve this document.
#c7	we;the user;this document;a basis;a query;this document
#s8	So we can imagine the user could use a process.
#c8	we;the user;a process
#s9	That works as follows, where we assume that the query is generated by sampling words from the document.
#c9	we;the query;words;the document
#s10	So for example, a user might pick a word like presidential from this document.
#c10	example;a user;a word;this document
#s11	And then use this as a query word.
#c11	a query word
#s12	And then the user would pick another "word like ""campaign"" and that will be" the second query word.
#c12	the user;another "word;""campaign;the second query word
#s13	Now this of course is assumption that we have made about how a user would pose a query.
#c13	course;assumption;we;a user;a query
#s14	Whether user actually followed this process.
#c14	user;this process
#s15	Maybe a different question, but this assumption has allowed us to formulate characterize this conditional probability.
#c15	Maybe a different question;this assumption;us;this conditional probability
#s16	And this allows us to also not rely on the big table that I showed you earlier to use empirical data to estimate this probability.
#c16	us;the big table;I;you;empirical data;this probability
#s17	And this is why we can use this idea to them.
#c17	we;this idea;them
#s18	Further derive retrieval function that we can implement with the program language.
#c18	Further derive retrieval function;we;the program language
#s19	So as you see, the assumption that we've made here is each query word is independently sampled and also each word is basically obtained from the document.
#c19	you;the assumption;we;each query word;each word;the document
#s20	So now let's see how this works exactly.
#c20	's
#s21	Well, since we are computing the query likelihood.
#c21	we;the query likelihood
#s22	Then the probability here is just the probability of this particular query, which is a sequence of words.
#c22	the probability;just the probability;this particular query;a sequence;words
#s23	And we make the assumption that each word is generated independently, so as a result, the probability of the query is just a product of the probability of each query word.
#c23	we;the assumption;each word;a result;the probability;the query;just a product;the probability;each query word
#s24	Now, how do we compute the probability of each query word Well based on the assumption that a word is picked from the document.
#c24	we;the probability;each query word;the assumption;a word;the document
#s25	That the user has in mind.
#c25	the user;mind
#s26	Now we know the probability of each word is just to the relative frequency of the word in the document.
#c26	we;the probability;each word;the relative frequency;the word;the document
#s27	So for example, the probability of presidential given the document.
#c27	example;the probability;the document
#s28	Would be just the count of presidential in the document divided by the total number of words in the document or document length.
#c28	just the count;the document;the total number;words;the document;document length
#s29	So with this these assumptions, we now have actually simple formula for retrieval, right?
#c29	we;actually simple formula;retrieval
#s30	We can use this to rank our documents.
#c30	We;our documents
#s31	So does this model work?
#c31	this model
#s32	Let's take a look.
#c32	's;a look
#s33	Here are some example documents that you have seen before.
#c33	some example documents;you
#s34	Suppose now the query is presidential campaign and we see the formula here on the top.
#c34	the query;presidential campaign;we;the formula;the top
#s35	So how do we score these documents?
#c35	we;these documents
#s36	It's very simple, right?
#c36	It
#s37	We just count how many times we have "seen ""presidential"" or how many times we" have seen campaign et cetera and within here for d4 and we have seen presidential twice that's two over the length of Document 4 multiplied by 1 over length of document 4 for probability of campaign.
#c37	We;we;we;et cetera;d4;we;the length;Document;length;document;probability;campaign
#s38	And similarly we can get probabilities for the other two documents.
#c38	we;probabilities;the other two documents
#s39	Now if you look at this, these numbers or these formulas for scoring all these documents.
#c39	you;these formulas;all these documents
#s40	It seems to make sense be cause if we assume D3 and D4 have about the same length than looks like we're going to rank D4 above D3, and which is above D2 as we would expect, looks like it did capture the TF heuristic.
#c40	It;sense;we;D3;D4;about the same length;we;D4;D3;D2;we;it
#s41	And so this seems to work well.
#s42	However.
#s43	If we try a different query like this one presidential campaign update.
#c43	we;a different query;this one presidential campaign update
#s44	Then we might see a problem.
#c44	we;a problem
#s45	What problem?
#c45	What problem
#s46	Well think about the update now.
#c46	the update
#s47	None of these documents has mentioned update.
#c47	None;these documents;update
#s48	So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining a word like update.
#c48	our assumption;a user;a word;a document;a query;a word;update
#s49	Would be what?
#c49	what
#s50	Would be 0, right?
#s51	So that caused a problem because we cause all these documents to have zero probability of generating this query.
#c51	a problem;we;all these documents;zero probability;this query
#s52	Now, while it's fine to have zero probability for D2 which is non relevant, it's not OK to have zero for D3 and D4, because now we no longer can distinguish them.
#c52	it;zero probability;D2;it;D3;D4;we;them
#s53	What's worse, we can't even distinguish them from D2, right?
#c53	What;we;them;D2
#s54	So that's obviously not desirable.
#s55	Now, whenever we've had such result.
#c55	we;such result
#s56	We should think about what has caused this problem.
#c56	We;what;this problem
#s57	So we have to examine what assumptions have been made.
#c57	we;assumptions
#s58	As we derive this ranking function.
#c58	we;this ranking function
#s59	Now, if you examine those assumptions carefully, you would realize what has caused this problem.
#c59	you;those assumptions;you;what;this problem
#s60	Right?
#s61	So take a moment to think about what do you think is the reason why update has zero probability.
#c61	a moment;what;you;the reason;update;zero probability
#s62	And how do we fix it?
#c62	we;it
#s63	Right, so if you think about this for a moment, you realize that that's because we have made assumption that every query word must be drawn from the document in the user's mind.
#c63	you;a moment;you;we;assumption;every query word;the document;the user's mind
#s64	So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document, so let's improve the model and the improvement here is to say that instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model.
#c64	order;we;the user;a word;the document;'s;the model;the improvement;a word;the document;'s;the user;a word;a document model
#s65	So I showed model here.
#c65	I;model
#s66	We assume that this document is generated using this unigram language model.
#c66	We;this document;this unigram language model
#s67	Now this model.
#c67	Now this model
#s68	Doesn't necessarily assign zero probability for update.
#c68	zero probability;update
#s69	In fact that we consume this model does not assign zero probability for any word.
#c69	fact;we;this model;zero probability;any word
#s70	Now if we think in this way, then the generation process is a little bit different.
#c70	we;this way;the generation process
#s71	Now the user has this model in mind.
#c71	the user;this model;mind
#s72	Instead of this particular document.
#c72	this particular document
#s73	Although the model has to be estimated based on the document.
#c73	the model;the document
#s74	So the user can again generate the query using a similar process, namely pick a word.
#c74	the user;the query;a similar process;a word
#s75	For example, presidential.
#c75	example
#s76	And another word, campaign.
#c76	And another word;campaign
#s77	Now the difference is that this time we can also pick a word like update even though update does not occur in the document to potentially generate the query word like update so that a query with update want to have zero probabilities.
#c77	the difference;we;a word;update;update;the document;the query word;update;a query;update;zero probabilities
#s78	So this will fix our problem, and it's also reasonable because we're now thinking of what the user is looking for in a more general way.
#c78	our problem;it;we;what;the user;a more general way
#s79	That is unigram language model instead of a fixed document.
#c79	unigram language model;a fixed document
#s80	So how do we compute this query  likelihood?
#c80	we;this query;  likelihood
#s81	If we make this assumption?
#c81	we;this assumption
#s82	Well, it involves 2 steps, right?
#c82	it;2 steps
#s83	The first is to compute this model.
#c83	this model
#s84	And we call it the document language model here.
#c84	we;it;the document language model
#s85	For example, I've shown two possible language models here is made based on two documents.
#c85	example;I;two possible language models;two documents
#s86	And then given a query and I get data mining algorithms.
#c86	a query;I;data mining algorithms
#s87	The second step would just compute the likelihood of this query and by making independent assumptions we could then have this probability as a product of the probability of each query word.
#c87	The second step;the likelihood;this query;independent assumptions;we;this probability;a product;the probability;each query word
#s88	But we do this for both documents and then we're going to score these two documents and then rank them.
#c88	we;both documents;we;these two documents;them
#s89	So that's the basic idea of this query, likelihood retrieval function.
#c89	the basic idea;this query;likelihood retrieval function
#s90	So more generally then, this ranking function would look like the following right here we assume that the query has N words.
#c90	this ranking function;we;the query;N words
#s91	W one through WN, and then the scoring function.
#c91	W;WN;then the scoring function
#s92	The ranking function is.
#c92	The ranking function
#s93	Probability that we observe this query given that the user is thinking of this document.
#c93	Probability;we;this query;the user;this document
#s94	And this is assumed to be product of probabilities of all individual words.
#c94	product;probabilities;all individual words
#s95	This is based on the independence assumption.
#c95	the independence assumption
#s96	Now we actually often score the document for this query by using log of the query likelihood as shown on the second line.
#c96	we;the document;this query;log;the query likelihood;the second line
#s97	Now we do this.
#c97	we
#s98	To avoid having a lot of small probabilities.
#c98	a lot;small probabilities
#s99	We multiply together and this could cause underflow and we might lose precision by transforming the value with a logarithm function.
#c99	We;underflow;we;precision;the value;a logarithm function
#s100	We maintain the order of these documents, yet we can avoid the underflow problem.
#c100	We;the order;these documents;we;the underflow problem
#s101	So if we take logarithm transformation, of course the product that would become a sum as shown on the second line here.
#c101	we;logarithm transformation;course;a sum;the second line
#s102	So it's a sum over all the query words inside the sum.
#c102	it;a sum;all the query words;the sum
#s103	The value is log of the probability of this word given by the document.
#c103	The value;log;the probability;this word;the document
#s104	And then we can further rewrite the sum into a different form.
#c104	we;the sum;a different form
#s105	So in the first sum here.
#c105	the first sum
#s106	In this sum, We have it all over the query words N query words.
#c106	this sum;We;it;the query words;N query words
#s107	And in this sum we have a sum over all the possible words, but we put a count here of each word in the query.
#c107	this sum;we;a sum;all the possible words;we;a count;each word;the query
#s108	Essentially we are only considering the words in the query because if a word is not in the query, the count would be 0.
#c108	we;the words;the query;a word;the query;the count
#s109	So we're still considering only these N words.
#c109	we;only these N words
#s110	But we are using a different form, as if we're going to take sum over all the words in the vocabulary.
#c110	we;a different form;we;sum;all the words;the vocabulary
#s111	And of course, a word might occur multiple times in the query.
#c111	course;a word;the query
#s112	That's why we have a count here.
#c112	we;a count
#s113	And then this part is log of the probability of the word given by the document language model.
#c113	this part;log;the probability;the word;the document language model
#s114	So you can see in this retrieval function we actually know the count of the word in the query.
#c114	you;this retrieval function;we;the count;the word;the query
#s115	So the only thing that we don't know is this document language model.
#c115	the only thing;we;this document language model
#s116	Therefore, we have converted the retrieval problem, include the problem of estimating this document language model.
#c116	we;the retrieval problem;the problem;this document language model
#s117	So that we can compute the probability of each query word given by this document.
#c117	we;the probability;each query word;this document
#s118	And different estimation methods here would lead to different ranking functions.
#c118	different estimation methods;different ranking functions
#s119	Now this is just like a different ways to place a document vector in the vector space would lead to a different  ranking function in the vector space model.
#c119	a different ways;a document vector;the vector space;a different  ranking function;the vector space model
#s120	Here different ways to estimate these document language model would lead to a different ranking function for query likelihood.
#c120	Here different ways;these document language model;a different ranking function;query likelihood
410	8717e27a-33fb-4d06-ae68-2e0d915b1568	128
#s1	This lecture is about the generative probabilistic models for text clustering.
#c1	This lecture;the generative probabilistic models;text clustering
#s2	In this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering
#c2	this lecture;we;text clustering;we;generative probabilistic models;a way;text clustering
#s3	So this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting.
#c3	the overall plan;text clustering;the previous lecture;we;what;text clustering;text clustering
#s4	In this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches.
#c4	this lecture;we;text clustering;you;this slide;two kinds;approaches
#s5	One is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches.
#c5	probabilistic models;the topic;this lecture;similarity based approaches
#s6	So to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models.
#c6	generative models;text clustering;it;the topic mining problem;topic models
#s7	Because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model.
#c7	the two problems;a slide;you;the lecture;topic model
#s8	Here we show that we have input of text collection C and number of topics K and vocabulary V, and we hope to generate as output two things.
#c8	we;we;input;text collection C;number;topics K;vocabulary V;we;output;two things
#s9	One is a set of topics denoted by Theta i's.
#c9	a set;topics;Theta;i
#s10	Each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic.
#c10	a word distribution;a pi ij;the probabilities;each document;each topic
#s11	So this is a topic coverage
#c11	a topic coverage
#s12	and it's also visualized here on this slide you can see that this is what we can get by using a topic model.
#c12	it;this slide;you;what;we;a topic model
#s13	Now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities.
#c13	a main difference;this and text clustering problem;a document;multiple topics;general document;more than one topic;non zero probabilities
#s14	In text clustering, however, we only allow a document to cover one topic.
#c14	text clustering;we;a document;one topic
#s15	If we assume one topic is a cluster.
#c15	we;one topic;a cluster
#s16	So.
#s17	That means if we change the topic definition just slightly by assuming that each document can only be generated by using precisely one topic.
#c17	we;the topic definition;each document;precisely one topic
#s18	Then we'll have a definition of the clustering problem.
#c18	we;a definition;the clustering problem
#s19	As shown here.
#s20	So here the output is changed so that we no longer have the detailed coverage distributions pi ij's, but instead will have cluster assignment decisions.
#c20	the output;we;the detailed coverage distributions pi ij;cluster assignment decisions
#s21	An CI and CI is decision for the document i. And c sub i is going to take a value from one through K to indicate one of the K clusters.
#c21	An CI;CI;decision;the document i.;c sub;i;a value;K;the K clusters
#s22	And basically tells us document Di is in which cluster.
#c22	us;document Di;which cluster
#s23	As illustrated here, we no longer have multiple topics covered in each document is precisely one topic, although which topic is still uncertain.
#c23	we;multiple topics;each document;precisely one topic;which topic
#s24	There is also a connection with the.
#c24	a connection
#s25	Problem of mining.
#c25	Problem;mining
#s26	One topic that we discussed earlier.
#c26	One topic;we
#s27	So here again it's a slide that you have seen before.
#c27	it;a slide;you
#s28	And here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic.
#c28	we;a topic model;word distribution;precisely one document;we;this document;precisely one topic
#s29	But we can also consider some variations of the problem.
#c29	we;some variations;the problem
#s30	For example, we can consider there are N documents, each covers different topic.
#c30	example;we;N documents;different topic
#s31	So that's N documents and topics.
#c31	N documents;topics
#s32	Of course, in this case these documents are independent and these topics also independent.
#c32	this case;these documents;these topics
#s33	But we can further allow these documents share topics and then.
#c33	we;these documents;topics
#s34	We can also assume that we are going to assume there are fewer topics.
#c34	We;we;fewer topics
#s35	The number of documents.
#c35	The number;documents
#s36	So this document must share some topics.
#c36	this document;some topics
#s37	And if we have N documents for share k topics, then will again have precisely the document clustering problem.
#c37	we;N documents;share k topics;precisely the document clustering problem
#s38	So because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering.
#c38	these connections;we;a probabilistic generating model;the problem;text clustering
#s39	So the question now is what generating model can be used to do clustering.
#c39	the question;what generating model;clustering
#s40	As in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model.
#c40	all cases;a generative model;we;the generative model;the output;we;we
#s41	So in this case it's a clustering structure.
#c41	this case;it;a clustering structure
#s42	The topics and each document that covers one topic, and we hope to embed such such preferences in a generative model.
#c42	The topics;each document;one topic;we;such such preferences;a generative model
#s43	But if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of K topics?
#c43	you;the main difference;this problem;the topic model;we;you;a main requirement;we;every document;precisely one topic;K topics
#s44	As in the topic model.
#c44	the topic model
#s45	So let's revisit the topic model again in more detail.
#c45	's;the topic model;more detail
#s46	So this is a detailed view of two component mixture model and when we have K components it looks similar.
#c46	a detailed view;two component mixture model;we;K components;it
#s47	So here we see that when we generate a document.
#c47	we;we;a document
#s48	We generated each word independently.
#c48	We;each word
#s49	And we generated each word First make a choice between these distributions with decided to use one of them with probability.
#c49	we;each word;a choice;these distributions;them;probability
#s50	So P of theta one is the probability of choosing the distribution on the top.
#c50	P;theta;one;the probability;the distribution;the top
#s51	Now we first make this decision regarding which distribution should be used to generate the world, and then we're going to use this distribution to sample word.
#c51	we;this decision;which distribution;the world;we;this distribution;word
#s52	Now.
#s53	Notice that in such a generative model.
#c53	such a generative model
#s54	The decision on which distribution to use for each word is independent, so "that means, for example, ""the"" here could" have been generated from the second distribution.
#c54	The decision;which distribution;each word;example;the second distribution
#s55	Theta two, whereas text is more likely generated from the first one on the top.
#c55	Theta;text;the top
#s56	That means the words in the document could have been generated in general from multiple distributions.
#c56	the words;the document;multiple distributions
#s57	Now this is not what we want to see for text clustering.
#c57	what;we;text clustering
#s58	For document clustering where we hope this document will be generated from precisely one topic.
#c58	document clustering;we;this document;precisely one topic
#s59	So now that means we need to modify the model, but how well, let's first think about why this model cannot be used for clustering, and I just say the reason is because.
#c59	we;the model;'s;this model;clustering;I;the reason
#s60	It has allowed multiple topics to contribute the words to the document.
#c60	It;multiple topics;the words;the document
#s61	And that causes confusion because we're not going to know which cluster this document is from an it's more importantly, it's violating our assumption about the partitioning of documents in the clusters.
#c61	confusion;we;which cluster;this document;it;it;our assumption;the partitioning;documents;the clusters
#s62	If we really have one topic to correspond to one cluster of documents, then we would have a document to be generated from precisely one topic.
#c62	we;one topic;one cluster;documents;we;a document;precisely one topic
#s63	That means all the words in the document must have been generated from precisely one distribution, and this is not true for such a topic model that we're seeing here, and that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate.
#c63	all the words;the document;precisely one distribution;such a topic model;we;clustering;it;only one distribution
#s64	All the words in one document.
#c64	All the words;one document
#s65	So if you realize this problem, then we can naturally design alternative mixture model for doing clustering.
#c65	you;this problem;we;alternative mixture model;clustering
#s66	So this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the K word distributions that we have.
#c66	what;you;we;a decision;document;the K word distributions;we
#s67	But this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the all the words in the document.
#c67	we;the decision;the topics;we;this distribution;the all the words;the document
#s68	And that means once we have made the choice of the distribution for in generating the first word.
#c68	we;the choice;the distribution;the first word
#s69	We're going to stay with this decision in generating all the other words in the document.
#c69	We;this decision;all the other words;the document
#s70	So in other words, we only make the choice once.
#c70	other words;we;the choice
#s71	for all.
#s72	Basically we make the decision once for this document and stay with this to generate all the words.
#c72	we;the decision;this document;all the words
#s73	Similarly, if I had chosen the second distribution, theta sub two here, you can see we will stay with this one and then generate the entire document
#c73	I;the second distribution;you;we;the entire document
#s74	D.
#c74	D.
#s75	Now, if you compare this picture with the previous one, you will see the desicion of.
#c75	you;this picture;the previous one;you;the desicion
#s76	Of using a particular distribution is made of just once for this document.
#c76	a particular distribution;this document
#s77	In the case of document clustering.
#c77	the case;document clustering
#s78	But in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models.
#c78	the case;topic model;we;as many decisions;the number;words;the document;each word;we;a potential different decision;the key difference;the two models
#s79	But this is obviously also a mixture model, so we can just group them together as one box to show that this is.
#c79	a mixture model;we;them;one box
#s80	Model that will give us a probability of a document.
#c80	Model;us;a probability;a document
#s81	Now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model.
#c81	this model;a different distribution;we;a mixture model
#s82	And of course, the main problem in document clustering is to infer.
#c82	course;the main problem;document clustering
#s83	Which distribution has been used to generator a document and that would allow us to recover the cluster identity over document So it would be useful to think about the difference from the topic model, as I have also mentioned multiple times.
#c83	Which distribution;a document;us;the cluster identity;document;it;the difference;the topic model;I;multiple times
#s84	There are many.
#s85	Two differences.
#c85	Two differences
#s86	One is the choice of.
#c86	the choice
#s87	Using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times.
#c87	a particular distribution;document clustering model;the topic model;it
#s88	Four different words.
#c88	Four different words
#s89	The second is that word distribution here is going to be used to generate all the words for a document.
#c89	that word distribution;all the words;a document
#s90	But in the case of topic modeling, one distribution doesn't have to generate with all the words in a document.
#c90	the case;topic modeling;one distribution;all the words;a document
#s91	Multiple distribution could have been used to generate the words in the document.
#c91	Multiple distribution;the words;the document
#s92	It's also think about the special case when one of the one of the probability of choosing a particular distribution is equal to 1.
#c92	It;the special case;the probability;a particular distribution
#s93	Now that just means we have no uncertainty now.
#c93	we;no uncertainty
#s94	We just stick with one particular distribution.
#c94	We;one particular distribution
#s95	Now in that case, clearly we will see this is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on one document.
#c95	that case;we;mixture model;no certainty;we;the distributions;a document;we;the case;one word distribution;one document
#s96	So that's the connection that we discussed earlier.
#c96	the connection;we
#s97	But now you can see more clearly.
#c97	you
#s98	So as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model.
#c98	more cases;a generative model;a problem;we;theta;the model
#s99	But once we design model, the next step is to write down the likelihood function.
#c99	we;model;the next step;the likelihood function
#s100	And after that we can do is to look at the how to estimate the parameters.
#c100	we;the parameters
#s101	so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different.
#c101	this case;what;the likelihood function;it;what;we;topic models;it
#s102	If you still recall what the likelihood function looks like in PLSA, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data.
#c102	you;what;the likelihood function;PLSA;you;the probability;a data point;mixture model;a sum;all the possibilities;the data
#s103	I in this case, so it's going to be some over these K topics because everyone can be used to generate the document and then inside the sum you can still recall what the formula looks like an it's going to be.
#c103	I;this case;it;these K topics;everyone;the document;the sum;you;what;the formula;it
#s104	A product of two probabilities and one is the probability of choosing a distribution.
#c104	A product;two probabilities;the probability;a distribution
#s105	The other is the probability of observing a particular data point from that distribution.
#c105	the probability;a particular data point;that distribution
#s106	So if you are map, this formula is kind of formula to our problem.
#c106	you;this formula;formula;our problem
#s107	Here you will see the probability of observing a document D is basically a sum, in this case over two different distributions.
#c107	you;the probability;a document;D;a sum;this case;two different distributions
#s108	Because we have a very simplified situation of just two clusters.
#c108	we;a very simplified situation;just two clusters
#s109	And so in this case you can see it's a sum of two cases.
#c109	this case;you;it;a sum;two cases
#s110	In each case it's indeed the probability of choosing the.
#c110	each case;it;the probability
#s111	Choosing the world distribution.
#c111	the world distribution
#s112	Is theta one or theta two right?
#c112	theta
#s113	And then it's this probability is multiplied by the probability of observing this document from this particular distribution.
#c113	it;this probability;the probability;this document;this particular distribution
#s114	And if you further expand this probability of observing the whole document, we see that it's product of observing each word X sub i. Here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document.
#c114	you;this probability;the whole document;we;it;product;each word;X sub i.;we;the assumption;each word;the probability;the whole document;just a product;the probability;each word;the document
#s115	So this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose I am also copying the probability of.
#c115	this form;the topic model;it;the difference;that purpose;I;the probability
#s116	topic model with two components here.
#c116	topic model;two components
#s117	So here you can see at the formula looks very similar or in many ways they are similar.
#c117	you;the formula;many ways;they
#s118	But there's also some difference.
#c118	some difference
#s119	And in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum.
#c119	the differences;the top;you;the mixture model;document clustering;we;a product;a sum
#s120	And that's corresponding to our assumption of 1st make a choice of choosing one distribution and then stay with this distribution to generate all the words.
#c120	our assumption;1st;a choice;one distribution;this distribution;all the words
#s121	And that's why we had the product inside the sum.
#c121	we;the product;the sum
#s122	The sum corresponds to the choice.
#c122	The sum;the choice
#s123	right.
#s124	Now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently.
#c124	the topic model;we;the sum;the product;we;each word
#s125	And that's why we have the product outside.
#c125	we;the product
#s126	But when we generate each each word, we have to make a decision regarding which distribution we use.
#c126	we;each each word;we;a decision;which distribution;we
#s127	So we have sum there for each word.
#c127	we;each word
#s128	But in general, ideas are all mixture models that we can estimate these models by using the EM algorithm as we will discuss more later.
#c128	ideas;mixture models;we;these models;the EM algorithm;we
410	885ba47a-e1d9-452f-a8c2-78533d27a5d0	7
#s1	this lecture is about the collaborative filtering in this lecture we're going to continue the discussion of recommender systems in particular we're going to look at the approach of collaborative filtering you have seen this slide before where we talked about the two strategies to answer the basic question where user you like item X in the previous lecture we looked at the item similarity that's compounded based filtering in this lecture we're going to look at the user similarity this is a different strategy called a collaborative filtering
#c1	this lecture;the collaborative filtering;this lecture;we;the discussion;recommender systems;we;the approach;collaborative filtering;you;this slide;we;the two strategies;the basic question;you;item X;the previous lecture;we;the item similarity;based filtering;this lecture;we;the user similarity;a different strategy;a collaborative filtering
#s2	so first what is collaborative filtering it is to make filtering decisions for individual user based on the judgments of other users and that is we say we were infer individuals interest or preferences from that of other similar users so the general idea is the following give him a user you were going to first find the similar users you want through U M and there were no predictor use preferences based on the preferences of these similar users you want through your M now the user similarity here can be judged based on their similarity in preferences on a common set of items now here you can see the exact content of item doesn't really matter we're going to look at the only the relation between the users and items so this means this approach is very general it can be applied to any items not just the text log kins so this approach will work well under the following assumptions first users with the same interest where have similar preferences second that users with similar preferences probably share the same interest so for example if the interest of the user is in information retrieval then we can infer the user probably favor signal our papers and so those quite interesting information retrieval researcher probably all failures eli up papers that's something that we make and with this assumption is true then it would help collaborative filtering work well we can also assume that if we see people favor cigar papers then we can infer their interest is probably information retrieval so in these simple examples this seems to make sense and in many cases such assumption actually does make sense so another something we have to make is that there are sufficient in a large number of user preference is available to us so for example if you see a lot of ratings of users for movies and those indicate their preferences on movies and if you have a lot of such data than collaborative filtering can be very effective if not there will be a problem and that's often called a cold start problem that means you don't have many preferences available so the system could not fully take advantage of collaborative filtering yet so let's look at the collaborative filtering problem in a more formal way and so this picture shows that we are in general considering a lot of users showing were showing em users here so you want through U M
#c2	what;collaborative filtering;it;filtering decisions;individual user;the judgments;other users;we;we;individuals;interest;preferences;other similar users;the general idea;the following;him;a user;you;the similar users;you;U M;no predictor use preferences;the preferences;these similar users;you;your M;the user similarity;their similarity;preferences;a common set;items;you;the exact content;item;we;the only the relation;the users;items;this approach;it;any items;this approach;the following assumptions;the same interest;similar preferences;users;similar preferences;the same interest;example;the interest;the user;information retrieval;we;the user;our papers;so those quite interesting information retrieval researcher;probably all failures;papers;something;we;this assumption;it;collaborative filtering work;we;we;people;cigar papers;we;their interest;information retrieval;these simple examples;sense;many cases;such assumption;sense;so another something;we;a large number;user preference;us;example;you;a lot;ratings;users;movies;their preferences;movies;you;a lot;such data;collaborative filtering;a problem;a cold start problem;you;many preferences;the system;advantage;collaborative filtering;'s;the collaborative filtering problem;a more formal way;this picture;we;a lot;users;em;users;you;U M
#s3	and we also considering a number of objects let's say N objects in order as oh one through oh N
#c3	we;a number;objects;'s;N;order
#s4	and then we will assume that the users will be able to charge those objects and the user could for example give ratings for those items for example those items could be movies could be products and then the users would give ratings one through five so what you see here is that we have assumed some ratings available for some combinations so some users have watched some movies with their rated those movies they obviously i won't be able to watch all the movies and some users may actually only watch a few movies
#c4	we;the users;those objects;the user;example;ratings;those items;example;those items;movies;products;the users;ratings;what;you;we;some ratings;some combinations;some users;some movies;their rated those movies;they;i;all the movies;some users;a few movies
#s5	so this is in general smalls metrics so many item many entries have unknown values and what's interesting here is we could potentially infer the value of element in this metrics based on other values and that's actually the central question in collaborative filtering and that is we assume there's unknown function here if that would map a pair of a user an object to rating and we have observed that some values of this function and we want to infer the value of this function for other pairs that with that don't have values available here so this is very similar to other machine learning problems where we have no values of the function on some training data set
#c5	general smalls;metrics;so many item;many entries;unknown values;what;we;the value;element;this metrics;other values;the central question;collaborative filtering;we;unknown function;a pair;a user;an object;rating;we;some values;this function;we;the value;this function;other pairs;values;problems;we;no values;the function;some training data
#s6	and we hope to predict the values of this function on some test there so this is a function approximation and how can we figure out the function based on the observed ratings
#c6	we;the values;this function;some test;a function approximation;we;the function;the observed ratings
#s7	so this is the setup now there are many approaches to solving this problem and in fact this is a very active research area or reason there are special conferences dedicated to the problem praxis is major conference devoted to the problem
#c7	the setup;many approaches;this problem;fact;a very active research area;reason;special conferences;the problem;praxis;major conference;the problem
410	8aa65515-aae3-4e5a-bb55-5709b1ea6368	11
#s1	this latter is about the query likely whole probabilistic retrieval model in this lecture with continue the discussion of probabilistic retrieval model in particular we're going to talk about the query like holder retrieval function in the query like a whole retrieval model our idea is the model how likely a user who likes a document with pose a particular query so in this case you can imagine if the user likes this particular document about the presidential campaign news then we can assume the user would use this token net basis to pose a query to try to retrieve the socket so we can't imagine the user could use a process that works as follows where we assume that the query is generated by sampling words from the document so for example a user might pick a war like presidential from this document and then use this as a query ward and then the user would pick another word like the campaign and that will be the second query ward now this of course is a something that we have made about how user would pose a query whether user actually follow this process maybe a different question but this assumption has allowed us to formally characterize this conditional probability and this allows us to also not rely on the big table that i showed you earlier to use empirical data to estimate this probability and this is why we can use this idea to them further deriver retrieval function that we can implement with the program language so as you see the assumption that we made here is each query water is indypendent assembled and also each word is basically obtained from the document so now let's see how this works exactly well since we are computing the query like hold then the probability here is just the probability of this particular query which is a sequence of words an we make the assumption that each word is generated independently so as a result the probability of the query is just a product of the probability of each query world now how do we compute the property in their view query word well based on the assumption that award is picked from the document that the user has in mind then we know the probability over to water is just to the relative frequency of the world in the document so for example the probability of presidential given the document would be just the count of presidential in the document divided by the total number of words in the document or documents so with these assumptions we now have actually simple formula for retrieval right we can use this to rank up document so this model walk let's take a look here are some examples documents that you have seen before suppose now the queries presidential campaign
#c1	this latter;the query;likely whole probabilistic retrieval model;this lecture;the discussion;probabilistic retrieval model;we;the query;holder retrieval function;the query;a whole retrieval model;our idea;the model;who;a document;a particular query;this case;you;the user;this particular document;the presidential campaign news;we;the user;this token net basis;a query;the socket;we;the user;a process;we;the query;words;the document;example;a user;a war;this document;a query ward;the user;another word;the campaign;the second query ward;course;a something;we;user;a query;user;this process;maybe a different question;this assumption;us;this conditional probability;us;the big table;i;you;empirical data;this probability;we;this idea;them;further deriver retrieval function;we;the program language;you;the assumption;we;each query water;each word;the document;'s;we;the query;hold;the probability;just the probability;this particular query;a sequence;words;we;the assumption;each word;a result;the probability;the query;just a product;the probability;each query world;we;the property;their view query word;the assumption;award;the document;the user;mind;we;the probability;water;the relative frequency;the world;the document;example;the probability;the document;just the count;the document;the total number;words;the document;documents;these assumptions;we;actually simple formula;retrieval right;we;document;this model walk;'s;a look;some examples documents;you
#s2	and we see the formula here on the top
#c2	we;the formula;the top
#s3	so how do we solve these documents
#c3	we;these documents
#s4	well it's very simple
#c4	it
#s5	right we just count how many times have seen presidential times web scene campaign etc and with a here forty four and we've seen president or twice so that's two over the length of document four multiplied by one over length of document four four probability of campaign and similar we can get probabilities for the other two documents now if you look at these numbers or these formulas for scoring all these documents it seems to make sense be cause if we assume T three and T four have about the same length then looks like we're going to rank D four above the three and which is above the two as we would expect looks like it did capture the TF heuristic and so this seems to work well however if we try a different query like this one presidential campaign update then we might see a problem what problem well think about the update now none of these documents has mentioned update
#c5	we;how many times;presidential times web scene campaign;we;president;the length;document;length;document;four four probability;campaign;we;probabilities;the other two documents;you;these numbers;these formulas;all these documents;it;sense;we;T;T;about the same length;we;D;we;looks;it;we;a different query;this one presidential campaign update;we;a problem;what problem;the update;none;these documents;update
#s6	so according to our assumption that a user would pick a water from a document to generate a query then the probability of obtaining a warden like update would be what would be zero
#c6	our assumption;a user;a water;a document;a query;the probability;a warden;update;what
#s7	right so that cause a problem becaus own cause all these documents to have zero probability of generating this query now while it's fine to have zero property and therefore D two which is not relevant it's not OK to have zero four D three and D four because now we no longer can distinguish them what's worse we can't even distinguish them from D two so that's obviously not desirable now when i move has such result we should think about what has caused this problem so we have to examine what assumptions have been made as we divide this ranking function now if you examine those assumptions carefully you would realize what has caused this problem hi so take a moment to think about it what do you think is the reason why update has zero probability and how do we fix it
#c7	a problem;becaus;all these documents;zero probability;this query;it;zero property;it;we;them;what;we;them;D;i;such result;we;what;this problem;we;what assumptions;we;this ranking function;you;those assumptions;you;what;this problem;a moment;it;what;you;the reason;update;zero probability;we;it
#s8	right
#s9	so if you think about this for moment you realize that that's be cause we have made assumption that every query will must be drawn from the document in users mine so in order to fix this we have to assume that the user could have drawn award not necessary from the document so let's improve the model of the improvement here is to say that well instead of drawing a word from the document let's imagine that the user would actually draw a word from a document model so i show model here here we assume that this document is generated using this unigram language model not this model doesn't necessarily assign zero probability for update in fact that we consume this model does not assign zero probability for any word now if we were thinking this way then the generation process is a little bit different now the user has this model in mind instead of this particular document although the model has to be estimated based on the document so the user can again generate the query using a similar process namely pick award for example presidential and another water campaign now the difference is that this time we can also pick up automatic update even though updated does not occur in the document to pretend you generate a query warden i update so that a query with updated will only have zero probabilities so this would fix our problem and it's also reasonable becaus we're now thinking of what the user is looking for in a more general way that is unigram language model instead of a fixed document so how do we compute this query like if we make this or some well it involves two steps right the first is to compute this model and we call it talking the language model for example drive assume two possible language models here is made based on two documents and then given a query i get data mining algorithms the second step will just compute the likelihood of this query and by making independence assumptions we could then have this probability as a product of the probability of each query word
#c9	you;moment;you;we;assumption;every query;the document;users mine;order;we;the user;award;the document;'s;the model;the improvement;a word;the document;'s;the user;a word;a document model;i;model;we;this document;this unigram language model;this model;zero probability;update;fact;we;this model;zero probability;any word;we;the generation process;the user;this model;mind;this particular document;the model;the document;the user;the query;a similar process;award;example;the difference;we;automatic update;the document;you;a query warden;i;a query;zero probabilities;our problem;it;reasonable becaus;we;what;the user;a more general way;unigram language model;a fixed document;we;this query;we;it;two steps;this model;we;it;the language model;example drive;two possible language models;two documents;a query;i;data mining algorithms;the second step;the likelihood;this query;independence assumptions;we;this probability;a product;the probability;each query word
#s10	right we do this for boats documents and then we're going to score these two documents and then rank them so that's the basic idea of this query like hold retrieval function so more generally then this ranking function would look like in the following we assume that the query has N words W one throw WN and then the scoring function ranking function is probability error that we observe this query given that the user is thinking of this document and this is assumed it would be product of probabilities of all individual words this is based on the independence assumption now we actually often score the document for this query by using log of the query mike code as you on the signal line now we do this to avoid having a lot of small probabilities be multiplied together and this could cause underflow and we might lose precision by transforming the value is the logarithm function we maintain the order of these documents yet we can avoid the flow problem so if we take logarithm transformation of course the product that would become a sum as you cigna lying here so it's a sum over all the query words inside of the sum value is log over the probability of this word given by the document and then we can further rewrite the sum into a different form so in the first some here i think this is something we have it over all the query words and query words and in this some we have a sum over all the possible words but we put a condom here of each word in the query essentially we are only considering the words in the query becaus if a word is not in the query account would be zero so we are still considering only these end words but we are using a different form as if we're going to take some over all the words in the vocabulary and of course award might occur multiple times in the query that's why we have account here
#c10	we;boats documents;we;these two documents;them;the basic idea;this query;retrieval function;this ranking function;the following;we;the query;N words;W;WN;then the scoring function ranking function;probability error;we;this query;the user;this document;it;product;probabilities;all individual words;the independence assumption;we;the document;this query;log;the query mike code;you;the signal line;we;a lot;small probabilities;underflow;we;precision;the value;the logarithm function;we;the order;these documents;we;the flow problem;we;logarithm transformation;course;the product;a sum;you;it;a sum;all the query words;the sum value;the probability;this word;the document;we;the sum;a different form;i;something;we;it;all the query words;query words;we;a sum;all the possible words;we;a condom;each word;the query;we;the words;the query becaus;a word;the query account;we;only these end words;we;a different form;we;all the words;the vocabulary;course;award;the query;we;account
#s11	and then this part is log of the probability of the world given by the document language model so you can see in this retrieval function we actually know the count of the world in the query so the only thing that we don't know is this document i am anymore therefore we have convert through the retrieval problem into the problem of estimating this talk in the language model so that we had computer the probability of each query award given by this document and different the estimation methods here would lead to a different ranking functions and this is just like a different ways to place document vector in the vector space with leader to a different ranking function in the vector space model here different noise estimate this document language model will need to do a different ranking function for query like trickle
#c11	this part;log;the probability;the world;the document language model;you;this retrieval function;we;the count;the world;the query;the only thing;we;this document;i;we;the retrieval problem;the problem;this talk;the language model;we;computer;the probability;each query award;this document;the estimation methods;a different ranking functions;a different ways;document vector;the vector space;leader;a different ranking function;the vector space model;different noise estimate;this document language model;a different ranking function;query;trickle
410	8b3827ae-3009-4a46-afc1-e875c14640d2	145
#s1	This lecture is about using a time series as context to potentially discover causal topics in text.
#c1	This lecture;a time series;context;causal topics;text
#s2	In this lecture we're going to continue discussing contextual text mining.
#c2	this lecture;we;contextual text mining
#s3	In particular, we're going to look at the time series as a context for analyzing text to potentially discover causal topics.
#c3	we;the time series;a context;text;causal topics
#s4	As usual, let's start with motivation.
#c4	's;motivation
#s5	In this case, we hope to use text mining,  to understand the time series.
#c5	this case;we;text mining;the time series
#s6	Here what you're seeing is Dow Jones industrial average and stock price curves and you see a sudden drop here.
#c6	what;you;Dow Jones industrial average and stock price curves;you;a sudden drop
#s7	Right, so one would be interested in knowing what might have caused the stock market crash.
#c7	one;what;the stock market crash
#s8	Well, if you know the background and you might be able to figure out if you look at the time stamp
#c8	you;the background;you;you;the time
#s9	or there are other data that can help us figure it out.
#c9	other data;us;it
#s10	But the question here is can we get some clues about this from the companion news stream and we have a lot of news data that are generated during that period.
#c10	the question;we;some clues;the companion news stream;we;a lot;news data;that period
#s11	So if you do that, we might actually discover the crash actually happened at the time of September 11 attack.
#c11	you;we;the crash;the time;September 11 attack
#s12	And that's the time when there is a sudden rise of the topic,  about the September 11 attack in news articles.
#c12	the time;a sudden rise;the topic;the September 11 attack;news articles
#s13	Here's another scenario where we want to analyze the presidential election.
#c13	another scenario;we;the presidential election
#s14	This is the time series data from a presidential prediction market.
#c14	the time series data;a presidential prediction market
#s15	For example, the Iowa electronic market would have stocks for each candidate, and if we believe one candidate will win, then you tend to buy the stock for that candi date causing the price of that candidate to increase.
#c15	example;the Iowa electronic market;stocks;each candidate;we;one candidate;you;the stock;that candi date;the price;that candidate
#s16	So that's a nice way to actually do survey of people's opinions about these candidates.
#c16	a nice way;survey;people's opinions;these candidates
#s17	Suppose you see a sudden drop of price for one candidate.
#c17	you;a sudden drop;price;one candidate
#s18	You might also want to know what might have caused the sudden drop.
#c18	You;what;the sudden drop
#s19	Or in social science study, you might be interested in knowing what mattered in this election, what issues really mattered to people.
#c19	social science study;you;what;this election;what issues;people
#s20	Now, again in this case, we can look at the companion news stream and ask the question.
#c20	this case;we;the companion news stream;the question
#s21	Are there any clues in the news stream that might provide insight about this.
#c21	any clues;the news stream;insight
#s22	So, for example, we might discover the mention of tax cut has been increasing since that point.
#c22	example;we;the mention;tax cut;that point
#s23	So maybe that's related to the drop of the price.
#c23	the drop;the price
#s24	So all these cases are special cases of a general problem of joint analysis of text and the time series data to discover causal topics.
#c24	all these cases;special cases;a general problem;joint analysis;text;the time series data;causal topics
#s25	The input in this case is a time series plus text data that are produced in the same time period- the companion text stream.
#c25	The input;this case;a time series;text data;the same time;the companion text stream
#s26	And this is to see this was different from the standard talking models where we have just the text collection.
#c26	the standard talking models;we;just the text collection
#s27	That's why we set time series here to  serve as context.
#c27	we;time series;context
#s28	Now the output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series.
#c28	the output;we;the topics;whose coverage;the text stream;strong correlations;the time series
#s29	For example, whenever the topic is mentioned, the price tends to go down, etc. "
#c29	example;the topic;the price
#s30	Now we call these topics ""causal" "topics""."
#c30	we;these topics;""causal" "topics
#s31	Of course, they're not, strictly speaking, causal topics or we are never going to be able to verify whether they are causal or there's a true causal relationship here.
#c31	they;causal topics;we;they;a true causal relationship
#s32	That's why we put causal in quotation marks.
#c32	we;quotation marks
#s33	But at least they are correlated topics that might potentially explain the cause, and humans can certainly analyze such topics to understand the issue better.
#c33	they;topics;the cause;humans;such topics;the issue
#s34	And the output so would contain topics just like in topic modeling.
#c34	the output;topics;topic modeling
#s35	But we hope these topics are not just regular topics.
#c35	we;these topics;regular topics
#s36	  With these topics, we certainly don't have to explain the data the best in text, but rather they have to explain the data in the text, meaning that they have to represent a meaningful topics in texts semantically coherent topics, but also more important they should be correlated with the external time series that is given as a context.
#c36	these topics;we;the data;text;they;the data;the text;they;a meaningful topics;texts semantically coherent topics;they;the external time series;a context
#s37	So to understand how we solve this problem, let's first just to solve the problem with the regular topic model.
#c37	we;this problem;'s;the problem;the regular topic model
#s38	For example, PLSA and we can apply this to text streams.
#c38	example;PLSA;we;streams
#s39	And, with some extension like a CPLSA or contextual PLSA, then we can discover these topics in the collection and also discover their coverage overtime.
#c39	some extension;a CPLSA or contextual PLSA;we;these topics;the collection;their coverage overtime
#s40	So one simple solution is to choose the topics from this set that have the strongest correlation with the external time series.
#c40	one simple solution;the topics;this set;the strongest correlation;the external time series
#s41	But this approach is not going to be very good.
#c41	this approach
#s42	Why, because we are restricted to the topics that were discovered by PSA or LDA?
#c42	we;the topics;PSA;LDA
#s43	And that means the choice of topics will be very limited
#c43	the choice;topics
#s44	and we know these models try to maximize light role of the text data, so those topics tend to be the major topics that explain the text data well, and they're not necessarily correlated with time series.
#c44	we;these models;light role;the text data;those topics;the major topics;the text data;they;time series
#s45	Even if we get the best one, the most correlated and the topics might still not be so interesting from causal perspective.
#c45	we;the topics;causal perspective
#s46	So here in this work site here, a better approach, it was proposed, and this approach is called Iterative causal topic model.
#c46	this work site;a better approach;it;this approach;Iterative causal topic model
#s47	The idea is to do a iterative adjustment of topics discovered by topic models using time series to induce a prior.
#c47	The idea;a iterative adjustment;topics;topic models;time series;a prior
#s48	So here's an illustration of how this works.
#c48	an illustration
#s49	How this works.
#s50	Take the text stream as input and apply regular topic modeling to generate a number of topics.
#c50	the text stream;input;regular topic;a number;topics
#s51	That said, four topics shown here.
#c51	four topics
#s52	And then we're going to use the external time series to assess which topic is more causally related or correlated with the external time series, so we can certainly rank them.
#c52	we;the external time series;which topic;the external time series;we;them
#s53	And we might figure out that topic one and topic four are more correlated and topic two and topic three are not.
#c53	we;that topic;topic
#s54	Now we could have stopped here and that would be just like the simple approaches that I talked about earlier, right, then we can get these topics and call them causal topics.
#c54	we;the simple approaches;I;we;these topics;them;causal topics
#s55	But as I also explained that these topics are likely very good because they are general topics that explained the whole text collection, they're not necessarily the best topics that are correlated with our time series.
#c55	I;these topics;they;general topics;the whole text collection;they;the best topics;our time series
#s56	So what we can do in this approach is to further zoom in the word level.
#c56	what;we;this approach;the word level
#s57	And we're going to look into each word in the top ranked word list for each topic.
#c57	we;each word;the top ranked word list;each topic
#s58	Let's say we take topic one as the target to examine.
#c58	's;we;topic;the target
#s59	We know topic one is correlated with the time series.
#c59	We;topic;the time series
#s60	Or this is the best that we could get from this set of topics so far.
#c60	we;this set;topics
#s61	And we're going to look at the words in this topic -  the Top words.
#c61	we;the words;this topic;the Top words
#s62	And if the topic is correlated with the time series, there must be some words that are highly correlated with the time series.
#c62	the topic;the time series;some words;the time series
#s63	So here, for example, we might discover W1 and W3 are positively correlated with time series.
#c63	example;we;W1;W3;time series
#s64	But W2 and W4 are negatively correlated.
#c64	W2;W4
#s65	So as a topic and it's not good to mix these words with different correlations, so we can then further separate these words, we're going to get all the red words that indicate positive correlation W1and W3, and we're going to also get another subtopic, if you  want, that represents a negatively correlated words W2 and W4.
#c65	a topic;it;these words;different correlations;we;these words;we;all the red words;positive correlation;W1and W3;we;another subtopic;you;a negatively correlated words W2;W4
#s66	Now these subtopics, all these variations of topics based on the correlation analysis, are topics that are still quite related to the original topic topic one, but they already deviating because of the use of time series information, to bias selection of words.
#c66	these subtopics;all these variations;topics;the correlation analysis;topics;the original topic topic;they;the use;time series information;selection;words
#s67	So they in some sense, well, we should expect so, they are in some sense more correlated with time series than the original topic one because the original topic one has mixed words here, we separate them.
#c67	they;some sense;we;they;some sense;time series;the original topic;the original topic;mixed words;we;them
#s68	So each of these two subtopics can be expected to be better correlated with time series.
#c68	these two subtopics;time series
#s69	However, they may not be so coherent semantically.
#c69	they;so coherent semantically
#s70	So the idea here is to go back to topic model by using these, each as a prior, to further guide the topic modeling, and that's to say we ask our topic models to now discover topics that are very similar to  each of these two subtopics, and this will cause a bias toward more correlated topics with the time series.
#c70	the idea;topic model;a prior;the topic;modeling;we;our topic models;topics;these two subtopics;a bias;more correlated topics;the time series
#s71	Of course, then we can apply topic models to get  another generation of topics, and that can be further ranked based on the time series to select the highly correlated topics.
#c71	we;topic models;another generation;topics;the time series;the highly correlated topics
#s72	Then we can further analyze the component words in the topic and then try to analyze word level correlation.
#c72	we;the component words;the topic;word level correlation
#s73	And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic model discovery.
#c73	the even more correlated subtopics;the process;the topic model discovery
#s74	So this whole process is just heuristic way of optimizing causality and coherence.
#c74	this whole process;just heuristic way;causality;coherence
#s75	That's our ultimate goal, right?
#c75	our ultimate goal
#s76	So here you see the pure topic models will be very good at maximizing topical coherence.
#c76	you;the pure topic models;topical coherence
#s77	The topicals will be all meaningful.
#c77	The topicals
#s78	If we only use causality test or correlation measure then we might get a set of words that are strongly correlated with time series, but they may not necessarily mean anything.
#c78	we;causality test;correlation measure;we;a set;words;time series;they;anything
#s79	They might not be semantically connected, so that will be at the other extreme on the top.
#c79	They;the other extreme;the top
#s80	Now the ideal is to get the causal topic that's scored high both in topical coherence, and also causal relation.
#c80	the ideal;the causal topic;topical coherence
#s81	And this approach can be regarded as an alternate way to maximize both dimensions.
#c81	this approach;an alternate way;both dimensions
#s82	So when we apply the topic models we are maximizing the coherence.
#c82	we;the topic models;we;the coherence
#s83	But when we decompose the topic model words into sets of words that are strongly very strongly correlated with time series, we select the most strongly correlated words with the time series we are pushing the model back to the causal dimension to make it better in causal scoring.
#c83	we;the topic model words;sets;words;time series;we;the most strongly correlated words;the time series;we;the model;the causal dimension;it;causal scoring
#s84	And then when we apply the selected words, as a prior to guide the topic modeling, we again go back to optimize the coherence because topic models will ensure the next generation of topics to be coherent and we can iterate, iterate, and optimize in this way as shown on this picture.
#c84	we;the selected words;a prior;the topic;modeling;we;the coherence;topic models;the next generation;topics;we;this way;this picture
#s85	So the only component that you haven't seen in such a framework is how to measure the causality because the rest is just topic model.
#c85	the only component;you;such a framework;the causality;the rest;just topic model
#s86	So let's have a little bit discussion of that.
#c86	's;a little bit discussion
#s87	So here we show that.
#c87	we
#s88	Let's say we have a topic about government response here and then with topic model, we can get the coverage of the topic overtime.
#c88	's;we;a topic;government response;topic model;we;the coverage;the topic overtime
#s89	So we have a time series Xt.
#c89	we;a time series;Xt
#s90	Now we also have our given a time series that represents external information.
#c90	we;our given a time series;external information
#s91	It's a non text time series,  Yt, is the stock prices.
#c91	It;a non text time series;  Yt;the stock prices
#s92	Now the question here is, does Xt cause Yt?
#c92	the question;Xt;Yt
#s93	Or in other words, we want to match the causality relation between the two.
#c93	other words;we;the causality relation
#s94	Or maybe just measure the correlation of the two?
#c94	the correlation
#s95	There are many measures that we can use in this framework.
#c95	many measures;we;this framework
#s96	For example, Pearson correlation is a commonly used measure and we can consider time lag here so that we can try to capture causal relation using somewhat past data, using the data in the past, to try to correlate that with the data on points on why that represents the future, for example, and by introducing such lag we can hopefully it captures on causal relation by even using correlation measures like Pearson correlation.
#c96	example;Pearson correlation;a commonly used measure;we;time;we;causal relation;somewhat past data;the data;the past;the data;points;the future;example;such lag;we;it;causal relation;correlation measures;Pearson correlation
#s97	But a commonly used measure for causality here is Granger causality test.
#c97	a commonly used measure;causality;Granger causality test
#s98	And the idea of this test is actually quite simple.
#c98	the idea;this test
#s99	Basically you're going to have auto regressive model to use the history information of Y to predict itself.
#c99	you;auto regressive model;the history information;Y;itself
#s100	And this is the best we could do without any other information.
#c100	we;any other information
#s101	So we're going to be able to use such a model.
#c101	we;such a model
#s102	And then we're going to add some history information of X into such a model to see if we can improve the prediction of Y. If we can.
#c102	we;some history information;X;such a model;we;the prediction;Y.;we
#s103	If we can do that with a statistically significant difference, then we just say X has some causal influence on Y or otherwise we wouldn't have caused the improvement of prediction of Y. If, on the other hand, the difference is insignificant, and that would mean X does not really have a causal relation with Y, and so that's the basic idea.
#c103	we;a statistically significant difference;we;X;some causal influence;Y;we;the improvement;prediction;the other hand;the difference;X;a causal relation;Y;the basic idea
#s104	Now we don't have time to explain this in detail, so you could read, but you would read this cited reference here to know more about this measure.
#c104	we;time;detail;you;you;this cited reference;this measure
#s105	It's very frequently used measure, it has many applications.
#c105	It;very frequently used measure;it;many applications
#s106	So next, let's look at some sample results generated by this approach.
#c106	's;some sample results;this approach
#s107	Here the data is New York Times and in the time period of June 2000 through December of 2011.
#c107	the data;New York Times;the time period;June;December
#s108	And here the time series we used is stock prices of two companies, American Airlines and Apple, and the goal here is to see if we inject some time series bias or time series context whether we can actually get topics that are biased towards these time series.
#c108	the time series;we;stock prices;two companies;American Airlines;Apple;the goal;we;some time series bias;time series;we;topics;these time series
#s109	Imagine if we don't use any input, we don't use any context, then the topics from New York Times discovered by PLSA would be just general topics that people talk about in news.
#c109	we;any input;we;any context;the topics;New York Times;PLSA;just general topics;people;news
#s110	Those major topics in the news event.
#c110	Those major topics;the news event
#s111	But here you see, these topics are indeed biased toward each time series.
#c111	you;these topics;each time series
#s112	In particular, if you look at the underlined words here in the American Airlines result and you see airlines, Airport, Air, United, trade, terrorism, etc.
#c112	you;the underlined words;the American Airlines;you;airlines;Airport;Air;United;trade;terrorism
#s113	So it clearly has topics that are more correlated with the external time series.
#c113	it;topics;the external time series
#s114	On the right side you see some of the topics are clearly related to Apple.
#c114	the right side;you;the topics;Apple
#s115	Right.
#s116	So you can see computer technology, software, Internet, com, web etc.
#c116	you;computer technology;software;Internet, com
#s117	So that just means the time series has effectively served as a context to bias the discovery of topics.
#c117	the time series;a context;the discovery;topics
#s118	From another perspective these result help us what people have talked about in each case, so in the not just the people what people have talked about, but what are some topics that might be correlated with their stock prices?
#c118	another perspective;these result;us;what;people;each case;the not just the people;what;people;what;some topics;their stock prices
#s119	And so these topics can serve as a starting point for people to further look into the issues and to find the true causal relations.
#c119	these topics;a starting point;people;the issues;the true causal relations
#s120	Here are some other results from analyzing presidential election Time series.
#c120	some other results;presidential election Time series
#s121	And the time series data here is from Iowa electronic market.
#c121	the time series data;Iowa electronic market
#s122	And that's a prediction market and the data is the same.
#c122	a prediction market;the data
#s123	New York Times from May 2000 to October 2000, for 2000 presidential campaign election.
#c123	New York Times;May;October;2000 presidential campaign election
#s124	Now what you here are the top 3 words insignificant topics from New York Times.
#c124	what;you;the top 3 words;insignificant topics;New York Times
#s125	And if you look at these topics and they are indeed quite related to the campaign.
#c125	you;these topics;they;the campaign
#s126	Actually, here the issues are very much related to the important issues of this presidential election.
#c126	the issues;the important issues;this presidential election
#s127	Now here I should mention that the text data has been filtered by using only the articles that mention these candidate names.
#c127	I;the text data;only the articles;these candidate names
#s128	At.
#s129	So it's a subset of these news articles, very different from the previous experiment.
#c129	it;a subset;these news articles;the previous experiment
#s130	But the results here clearly show that the approach we can uncover some important issues in that presidential election.
#c130	the results;we;some important issues;that presidential election
#s131	So tax cut, oil, energy, abortion and gun control are all known to be important issues in that presidential election, and that was supported by some literature in political science.
#c131	tax cut, oil, energy, abortion and gun control;important issues;that presidential election;some literature;political science
#s132	And also it was discussed in Wikipedia.
#c132	it;Wikipedia
#s133	So basically the results show that the approach can effectively discover possibly causal topics based on the time series data.
#c133	the results;the approach;causal topics;the time series data
#s134	So there are two suggested readings here.
#c134	two suggested readings
#s135	One is the paper about this iterative topic modeling with time series feedback, where you can find more details about how this approach works.
#c135	the paper;this iterative topic;time series feedback;you;more details;this approach
#s136	And the second one is reading about Granger causality test.
#c136	the second one;Granger causality test
#s137	So in the end, let's summarize the discussion of text based prediction.
#c137	the end;'s;the discussion;text based prediction
#s138	Now, Text based prediction is generally very useful for big data applications that involve text, because, you can help us infer new knowledge about the word and the knowledge can go beyond what's discussed in the text.
#c138	Text based prediction;big data applications;text;you;us;new knowledge;the word;the knowledge;what;the text
#s139	As a result, they can also support optimizig of our decision making, and this has widespread applications.
#c139	a result;they;optimizig;our decision making;widespread applications
#s140	Text data is often combined with non text data for prediction, because for this purpose, for the prediction purpose, we generally would like to combine non text data and text data together as much clues as possible for prediction.
#c140	Text data;non text data;prediction;this purpose;the prediction purpose;we;non text data;text data;as much clues;prediction
#s141	And so as a result, joined analysis of text and non text is very necessary.
#c141	a result;analysis;text;non text
#s142	And it's also very useful now when we analyze text data together with non text data we can see they can help each other.
#c142	it;we;text data;non text data;we;they
#s143	So non text data provide context for mining text data and we discussed a number of techniques for contextual text mining.
#c143	non text data;context;mining text data;we;a number;techniques;contextual text mining
#s144	And on the other hand, text data can also help interpret the patterns discovered from non text data and this is called a pattern annotation.
#c144	the other hand;text data;the patterns;non text data;a pattern annotation
#s145	And in general this is a very active research topic and there was there new papers being published and there are also many open challenges that have to be solved.
#c145	a very active research topic;new papers;many open challenges
410	8eaf2971-31ff-40b7-9fc6-b91c7637f916	94
#s1	This lecture is about a mixture of unigram language models.
#c1	This lecture;about a mixture;unigram language models
#s2	In this lecture we will continue discussing probabilistic topic models.
#c2	this lecture;we;probabilistic topic models
#s3	In particular, we're going to introduce a mixture of unigram language models.
#c3	we;a mixture;unigram language models
#s4	This is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document.
#c4	a slide;you;we;the background words;we;top;estimated language model;one document
#s5	So if you want to solve the problem.
#c5	you;the problem
#s6	It will be useful to think about why we end up having this problem.
#c6	It;we;this problem
#s7	Well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood.
#c7	these words;our data;we;a maximum likelihood estimate;the estimator;high probabilities;these words;order;the likelihood
#s8	So in order to get rid of them, that would mean we have to do something different here.
#c8	order;them;we;something
#s9	In particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution.
#c9	we;this distribution;all the words;the text data;we;these common words;this distribution
#s10	So one natural way to solve the problem is to think about using another distribution to account for just these common words.
#c10	one natural way;the problem;another distribution;just these common words
#s11	This way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words.
#c11	the two distributions;the text data;the other model;we;background topic model;the common words
#s12	This way our target is the topic theta here would be only generating the content words that characterize the content of the document.
#c12	our target;the topic theta;the content words;the content;the document
#s13	So how does this work?
#s14	It's just a small modification of the previous set up where we have just one distribution.
#c14	It;just a small modification;we;just one distribution
#s15	Since we now have two distributions, we have to decide which distribution to use when we generate the word, but each word will still be sampled from one of the two distributions, right?
#c15	we;two distributions;we;which distribution;we;the word;each word;the two distributions
#s16	So text data is still generating the same way.
#c16	text data;the same way
#s17	Namely, we're going to generate a one word at each time.
#c17	we;a one word;each time
#s18	An eventually we generated a lot of words.
#c18	we;a lot;words
#s19	When we generate the word, however, we're going to 1st decide which of the two distributions to use, and this is controlled by another probability: probability of theta sub D and probability of theta sub B here.
#c19	we;the word;we;the two distributions;another probability;probability;theta sub D;probability;theta sub B
#s20	So this is the probability of selecting the topic word distribution.
#c20	the probability;the topic word distribution
#s21	This is the probability of selecting the background word distribution denoted by Theta sub
#c21	the probability;the background word distribution;Theta sub
#s22	B. Now in this case I just give example where we can set both to .5.
#c22	this case;I;example;we
#s23	So if you can do basically flip a coin a fair coin to decide which one to use.
#c23	you;a coin;a fair coin;which one
#s24	But in general these probabilities don't have to be equal, so you might bias towards using one topic more than the other.
#c24	these probabilities;you;one topic
#s25	So now the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if.
#c25	the process;a word;a coin;these probabilities;each model
#s26	Let's say the coin shows up as head, which means we're going to use the topic word distribution.
#c26	's;the coin;head;we;the topic word distribution
#s27	Then we're going to use this word distribution to generate a word.
#c27	we;this word distribution;a word
#s28	Otherwise we might be going through this path.
#c28	we;this path
#s29	And we're going to use the background word distribution to generate the word.
#c29	we;the background word distribution;the word
#s30	So in such a case we have a model that has some uncertainty associated with the use of a word distribution.
#c30	such a case;we;a model;some uncertainty;the use;a word distribution
#s31	But we can still think of this as a model for generating text data and such a model is called a mixture model.
#c31	we;a model;text data;such a model;a mixture model
#s32	So now let's see.
#c32	's
#s33	In this case, what's the probability of observing the word w? "
#c33	this case;what;the probability;the word
#s34	Now here I showed some words like ""the""" "and ""text"", so as in all cases, once we" set up the model, we're interested in computing the likelihood function.
#c34	I;some words;""text;all cases;we;the model;we;the likelihood function
#s35	The basic question is, so what's the probability of observing a specific word here?
#c35	The basic question;what;the probability;a specific word
#s36	Now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases.
#c36	we;the word;the two distributions;we;2 cases
#s37	Therefore it's a sum over these two cases.
#c37	it;a sum;these two cases
#s38	The first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of Theta sub D, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model.
#c38	The first case;the topic word distribution;the word;such a case;the probability;the probability;Theta sub D;the probability;the model;the probability;the word;that model
#s39	Both events must happen in order to "observe ""the"".
#c39	Both events;order
#s40	" We first must have chosen the topic of  and then we also have to actually "have sampled the word ""the"" from the" distribution and similarly the second part accounts for a different way of generating the word from the background.
#c40	We;the topic;we;the word;the" distribution;similarly the second part;a different way;the word;the background
#s41	Now obviously the probability of text the same is all similar, right?
#c41	the probability;text
#s42	So we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution.
#c42	we;two ways;text;each case;a product;the probability;a particular word distribution;the probability;the word;that distribution
#s43	Now later you will see this is actually general form, so you might want to make sure that you have really understood this expression here.
#c43	you;general form;you;you;this expression
#s44	And you should convince yourself that this is indeed the probability of observing text.
#c44	you;yourself;the probability;observing text
#s45	So to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word.
#c45	what;we;the probability;a word;a mixture model;a sum;all different ways;the word
#s46	And in each case it's a product of the probability of selecting that component model.
#c46	each case;it;a product;the probability;that component model
#s47	multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later.
#c47	the probability;the data point;that component model;something;you;this occurring
#s48	So the basic idea of a mixture model is just to treated these two distributions together as one model.
#c48	the basic idea;a mixture model;these two distributions;one model
#s49	So I use the box to bring all these components together.
#c49	I;the box;all these components
#s50	So if you view this whole box as one model, it's just like any other generative model.
#c50	you;this whole box;one model;it;any other generative model
#s51	It would just give us the probability of a word.
#c51	It;us;the probability;a word
#s52	But the way that determines this probability is quite different from when we have just one distribution.
#c52	the way;this probability;we;just one distribution
#s53	And this is basically a more complicated mixture model.
#c53	a more complicated mixture model
#s54	Sorry, more complicated model than just one distribution, and it's called a mixture model.
#c54	Sorry, more complicated model;just one distribution;it;a mixture model
#s55	So as I just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function.
#c55	I;we;just a generative model;it;just the likelihood function
#s56	The illustration that you have seen before, which is dimmer now is just the illustration of this generation model.
#c56	The illustration;you;dimmer;just the illustration;this generation model
#s57	So mathematically, this model.
#c57	So mathematically, this model
#s58	This is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word.
#c58	nothing;the following generative model;the probability;word;a sum over 2 cases;the word
#s59	The form you're seeing now is more general form than.
#c59	The form;you;more general form
#s60	what you have seen in the calculation earlier.
#c60	what;you;the calculation
#s61	I just used a simple w to denote any word, but you can still see.
#c61	I;a simple w;any word;you
#s62	This is basically the first sum.
#c62	the first sum
#s63	Like And this sum is due to the fact that the word can be generating multiple ways.
#c63	this sum;the fact;the word;multiple ways
#s64	Two ways in this case.
#c64	Two ways;this case
#s65	At inside sum, each term is a product again of two terms.
#c65	inside sum;each term;a product;two terms
#s66	And the two terms are ,first, the probability of selecting a component like Theta sub D. Second, the probability of actually observing the word from this component model.
#c66	the two terms;the probability;a component;Theta sub D. Second;the probability;the word;this component model
#s67	And so this is a very general description of, in fact, all the mixture models.
#c67	a very general description;fact
#s68	And I just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models.
#c68	I;you;the basis;all kinds;topic models
#s69	So now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data?
#c69	we;the model;we;the likelihood function;we;the next question;we;the parameter;what;the parameters;the data
#s70	Well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover?
#c70	we;some observed text data;the model parameters;this mission;us;the interesting knowledge;the text;this case;what;we
#s71	Well, these are represented by our parameters, and we have two kinds of parameters.
#c71	our parameters;we;two kinds;parameters
#s72	One is the two word distributions.
#c72	the two word distributions
#s73	Those are two topics and the other is the coverage of each topic in each.
#c73	two topics;the coverage;each topic
#s74	The coverage of each topic and this is determined by probability of Theta sub D. and probability of Theta sub B. Note that they sum to one.
#c74	The coverage;each topic;probability;Theta sub D.;probability;Theta sub B. Note;they
#s75	Now what's interesting is also to think about the special cases, like when we set one of them to one.
#c75	what;the special cases;we;them
#s76	What would happen?
#c76	What
#s77	Well, the other would be 0, right?
#s78	And if you look at the likelihood function.
#c78	you;the likelihood function
#s79	It will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0.
#c79	It;the special case;just one distribution;you
#s80	So in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case.
#c80	this sense;the mixture model;the previous model;we;just one distribution;it;a special case
#s81	So to summarize, and we talked about the mixture of two unigram language models.
#c81	we;the mixture;two unigram language models
#s82	And the data we consider here is just still 1 document.
#c82	the data;we;just still 1 document
#s83	And the model is a mixture model with two components: two unigram language models.
#c83	the model;a mixture model;two components;two unigram language models
#s84	Specifically, Theta sub D which is intended to denote the topic of document D and Theta sub B which is representing a background topic that we can set to attract the common words.
#c84	Specifically, Theta sub D;the topic;document D;Theta;sub B;a background topic;we;the common words
#s85	Because common words would be assigned high probabilities in this model.
#c85	common words;high probabilities;this model
#s86	So the parameters can be collectively called a Lambda, which I show here again, and you can again think about the question about how many parameters are we talking about exactly.
#c86	the parameters;a Lambda;I;you;the question;how many parameters;we
#s87	This is usually good exercise to do because it allows you to see the model index and to have a complete understanding of what's going on in this model and we have mixing weights of course also.
#c87	good exercise;it;you;the model index;a complete understanding;what;this model;we;weights;course
#s88	So what is the likelihood function look like?
#c88	what;the likelihood function
#s89	It looks very similar to what we had before, so for the document, first, it's a product of all the words in the document exactly the same as before.
#c89	It;what;we;the document;it;a product;all the words;the document
#s90	The only difference is that inside here now it's a sum instead of just one, so you might recall before we just had this one.
#c90	The only difference;it;a sum;you;we
#s91	But now we had this sum because of the mixture model and because of the mixed model We also have to introduce the probability of choosing that particular component distribution.
#c91	we;this sum;the mixture model;the mixed model;We;the probability;that particular component distribution
#s92	And so this is just another way of writing it again by using a product over all the unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later.
#c92	just another way;it;a product;all the unique words;our vocabulary;a product;all the positions;the document;this form;we;different unique words;a convenient form;the maximum likelihood estimator
#s93	And the maximum likelihood estimator  is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds.
#c93	the maximum likelihood estimator;the parameters;this likelihood function;the constraints;course;two kinds
#s94	One is the word probabilities in each topic must sum to one, the other is the choice of each topic must sum to one.
#c94	the word probabilities;each topic;the choice;each topic
410	8ef16cab-abef-4a2d-adfd-25dc097fe2a1	62
#s1	This lecture is about the feedback in text retrieval.
#c1	This lecture;the feedback;text retrieval
#s2	So in this lecture we're going to continue the discussion of text retrieval methods.
#c2	this lecture;we;the discussion;text retrieval methods
#s3	In particular, we're going to talk about the feedback impacts retrieval.
#c3	we;the feedback impacts retrieval
#s4	This is a diagram that shows the retrieval process.
#c4	a diagram;the retrieval process
#s5	We can see the user with typing the query.
#c5	We;the user;the query
#s6	And then the query would be sent to a retrieval engine or search engine.
#c6	the query;a retrieval engine;search engine
#s7	And the engine will return results.
#c7	the engine;results
#s8	These results will be shown to the user.
#c8	These results;the user
#s9	After the user has seen these results.
#c9	the user;these results
#s10	The user can actually make judgments.
#c10	The user;judgments
#s11	So for example, the user had said this is good and this document is not very useful.
#c11	example;the user;this document
#s12	This is good again, etc.
#s13	Now this is called a relevant judgment or relevance feedback because we've got some feedback information from the user based on the judgments.
#c13	a relevant judgment;relevance feedback;we;some feedback information;the user;the judgments
#s14	This can be very useful to the system we learn.
#c14	the system;we
#s15	What exactly is interesting to the user.
#c15	What;the user
#s16	So the feedback module would then take this as input.
#c16	the feedback module;input
#s17	And also use the document collection to try to improve ranking.
#c17	the document collection
#s18	Typically it would involve updating the query so the system can now rank the results more accurately for the user.
#c18	it;the query;the system;the results;the user
#s19	So this is all relevance feedback.
#c19	all relevance feedback
#s20	The feedback is based on relevance judgments made by the users.
#c20	The feedback;relevance judgments;the users
#s21	Now these judgments are reliable, but the users generally don't want to make extra effort unless they have to.
#c21	these judgments;the users;extra effort;they
#s22	So the downside is that it involves some extra effort by the user.
#c22	the downside;it;some extra effort;the user
#s23	There was another form of feedback called pseudo relevance feedback or blind feedback, also called automatic feedback.
#c23	another form;feedback;pseudo relevance feedback;blind feedback;automatic feedback
#s24	In this case you can see.
#c24	this case;you
#s25	Once the user has got without all in fact we don't have to involve users so you can see there's no user involved here.
#c25	the user;fact;we;users;you;no user
#s26	And we simply assume that the top ranked documents to be relevant.
#c26	we;the top;documents
#s27	Let's say we can assume top ten as relevant.
#c27	's;we
#s28	And then we would then use these.
#c28	we
#s29	Assumed documents to learn and to improve the query.
#c29	documents;the query
#s30	Now you might wonder how could this help if we simply assume the top ranked documents to be relavant well.
#c30	you;we;the top;documents
#s31	You can imagine these top ranked documents are actually similar to relevant documents, even if they are not relevant.
#c31	You;these top ranked documents;relevant documents;they
#s32	They look like relevant documents, so it's possible to learn some related terms to the query from this set.
#c32	They;relevant documents;it;some related terms;the query;this set
#s33	In fact, there you may recall that we talked about using language model to analyze word association to learn "related words to the word ""computer""" right, and then what we did is we first use computer to retrieve all the documents that contain computer.
#c33	fact;you;we;language model;word association;related words;the word;""computer;what;we;we;computer;all the documents;computer
#s34	So imagine now the query here is a computer right?
#c34	the query;a computer
#s35	And then the results will be those documents that contain computer and what we can do then is to take the top N results.
#c35	the results;those documents;computer;what;we;the top N results
#s36	They can match computer very well and we're going to count the terms in this set.
#c36	They;computer;we;the terms;this set
#s37	And then we're going to then use the background language model to choose the terms that are frequent in this set, but not frequent In the whole collection.
#c37	we;the background language model;the terms;this set;the whole collection
#s38	So if we make a contrast between these two, what we can find these that would learn some related terms to the word computer?
#c38	we;a contrast;what;we;some related terms;the word computer
#s39	As we have seen before and these related words can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software.
#c39	we;these related words;the original query;the query;us;documents;computer;other words;a program;software
#s40	So this is the effective for improving the search result.
#c40	the search result
#s41	But of course, pseudo relavance feedback is completely unreliable.
#c41	course;relavance feedback
#s42	We have to arbitrary set a cut off, so there's also something in between called implicit feedback.
#c42	We;a cut;something;called implicit feedback
#s43	In this case, what we do is we do involve users, but we don't have to have asked users to make judgments instead, or even the observe how the user interacts with the search result.
#c43	this case;what;we;we;users;we;users;judgments;even the observe;the user;the search result
#s44	In this case, we're going to look at the clickthroughs so the user clicked on this one and the user viewed this one and the user skipped this one and the user view this one again.
#c44	this case;we;the clickthroughs;the user;this one;the user;this one;the user;this one;the user;this one
#s45	Now this also is a clue about whether a document is useful to the user.
#c45	a clue;a document;the user
#s46	And we can even assume that we're going to use only the snippet here in this document.
#c46	we;we;only the snippet;this document
#s47	The text that's actually seen by the user instead of the actual document of this entry.
#c47	The text;the user;the actual document;this entry
#s48	The link there, let's say in web search may be broken, but then it doesn't matter if the user tried to fetch this document, because of the display, the text.
#c48	The link;'s;web search;it;the user;this document;the display;the text
#s49	We can assume these display.
#c49	We;these display
#s50	The text is probably relevant, is interesting to user.
#c50	The text;user
#s51	So we can learn from such information and this is called implicit feedback.
#c51	we;such information;implicit feedback
#s52	And we can again use the information to update the query.
#c52	we;the information;the query
#s53	This is a very important technique used in modern search engines.
#c53	a very important technique;modern search engines
#s54	Will think about the Google and Bing and they can collect a lot of user activities while they're serving us, right?
#c54	the Google;Bing;they;a lot;user activities;they;us
#s55	So they would observe what documents we click on, what documents will skip, and this information is very valuable and they can use this to improve the search engine.
#c55	they;what documents;we;what documents;this information;they;the search engine
#s56	So to summarize, we talked about the three kinds of feedback here.
#c56	we;the three kinds;feedback
#s57	Relevance feedback, where the user makes explicit judgments.
#c57	Relevance feedback;the user;explicit judgments
#s58	It takes some user effort, but the judgment the information is reliable.
#c58	It;some user effort;the judgment;the information
#s59	We talk about pseudo feedback where we simply assume top-ranked documents to be relevant.
#c59	We;pseudo;feedback;we;top-ranked documents
#s60	We don't have to involve the user, therefore we could do that actually, before we return the results to the user.
#c60	We;the user;we;we;the results;the user
#s61	And the third is implicit feedback, where we use click clues.
#c61	implicit feedback;we;click clues
#s62	We don't we involved users, but the user doesn't have to make explicit effort to make judgment.
#c62	We;we;users;the user;explicit effort;judgment
410	90bca100-98a3-41f9-b699-80587ff61294	20
#s1	this lecture is about the text retrieval problem this picture shows our overall plan for lectures [test] in the last lecture we talked about the high level strategies for text to access we talked about push [test] versus pull search engines are the main tools for supporting the poor mode starting from this lecture we're going to talk about how search engines work in detail so first is about the text retrieval problem we're going to talk about the three things in this lecture first were define text your table second working to make comparison between text retrieval and the related task database retrieval finally we're going to talk about the document selection versus document ranking as two strategies for responding to a user 's query
#c1	this lecture;the text retrieval problem;this picture;our overall plan;lectures;the last lecture;we;the high level strategies;text;we;push;test;search engines;the main tools;the poor mode;this lecture;we;search engines;detail;the text retrieval problem;we;the three things;this lecture;define text;your table;comparison;text retrieval;the related task database retrieval;we;the document selection;document;two strategies;a user 's query
#s2	so what is texting retrieval it should be a task that's familiar to most of us becaus we're using web search engines all the time so text retrieval is basically a task where the system would respond to a user 's query with relevant documents basically to support the query as one way to implement the poor mode of information access so the scenario is the following you have a clashing off text documents these documents could be all the web pages on the web all the literature articles in digital library or maybe all the text files in your computer a user were typically give a query to the system to express' information need
#c2	what;retrieval;it;a task;us becaus;we;web search engines;text retrieval;a task;the system;a user 's query;relevant documents;the query;one way;the poor mode;information access;the scenario;you;a clashing off text documents;these documents;all the web pages;the web;all the literature articles;digital library;your computer;a user;a query;the system;information
#s3	and then the system would return relevant documents to users relevant documents refer to those documents that are useful to the user who typing in the query now this task is often called the information retrieval but literally information retrieval with broader include retrieval of other non textual information as well for example audio video
#c3	the system;relevant documents;users;relevant documents;those documents;the user;who;the query;this task;the information retrieval;retrieval;other non textual information;example;audio video
#s4	etc it's worth noting that text retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data so for example current images search engines actually it match the user 's query with the companion text data of image this problem is also called search problem and the technology is often called a search technology industry if you have taken cause in databases will be used for the pause the lecture at this point and think about the differences between text retrieval and database retrieval now these two tasks are similar in many ways but there are some important differences so spend a moment to think about the differences between the two think about the data and information manage it by a search engine versus those managed by a database system think about the difference between the queries that you typically specify for elevated system versus the queries that are typed it typed in by users on the search engine and then finally think about the answers what's the difference between the two
#c4	it;text retrieval;the core;information retrieval;the sense;other medias;video;the companion text data;example;current images search engines;it;the user 's query;the companion text data;image;this problem;search problem;the technology;a search technology industry;you;cause;databases;the pause;the lecture;this point;the differences;text retrieval and database retrieval;these two tasks;many ways;some important differences;a moment;the differences;the two think;the data;information;it;a search engine;a database system;the difference;the queries;you;elevated system;the queries;it;users;the search engine;the answers;what;the difference
#s5	OK
#s6	so if we think about the information or there are managed by the two systems will see that intex retrieval the data is unstructured is free tax but in databases they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages etc in unstructured text it's not obvious what are the names of people manage in the in the text becaus of this difference we also see that text information tends to be more ambiguous and we talked about that in the natural language processing lab too whereas in databases that data tended to have well defined the semantics there is also important difference in the queries and this is a positive due to the difference in the information or data so text queries tend to be ambiguous whereas in database search the queries are typically well defined think about the SQL query that would clear this specify what records to be returned so it has very well defined semantics keyword queries or natural language queries tend to be incomplete also in that it doesn't really folder specify what dawkins should be retrieved where is in the database search the SQL query can be regarded as a computer specification for what should be returned and be cause of these differences that ends would be also different in the case of text retrieval we're looking for random in the documents in the database search we are retrieving records or match records with the SQL query more precisely now in the case of text retrieval what should be the right answers to a query is not very well specified as we just discussed so it's unclear what should be the right answers to a query and this has very important consequences and that is text retrieval is an empirically defined problem so this is a problem be cause if it's empirically defined then we cannot mathematically prove one method is better than another method that also means we must rely on empirical evaluation involving users to know which method works better and that's why we have an lecture actually more than one lectures to cover the issue of evaluation be cause this is a very important topic for search engines without knowing how to evaluate algorithm a properly there's no way to tell whether we have got a better algorithm or whether one system is better than another so now let's look at the problem in a formal way so this slide shows a formal formulation of the text retrieval problem first we have our vocabulary set which is just a set of words in a language now here we're considering just one language but in reality on the web there might be multiple natural languages we have text there are in all kinds of languages but here for simplicity we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language although there is important difference the principles and methods a very similar next we have the query which is a sequence of words and so here you can see the query is defined as a sequence of words each Q sub i is word in the vocabulary a document is defined in the same way so it's also a sequence of words and here the supply J is also a word in the vocabulary now typically the documents are much longer than queries but there are also cases where the documents may be very short so you can think about what might be a example of that case i hope you can think of twitter search write tweets are very short but in general documents are longer than the queries now then we have a collection of documents and this collection can be very large so think about the web they could could be very large and then the goal of text retrieval is it'll find the set of relevant documents which we denote by R of Q becaus it depends on the query and this is in general a subset of all the documents in the collection unfortunately this set of relevant documents is generally unknown and user dependent in the sense that for the same query typing by different users they expect the relevant documents may be different the query given to us by the user is only a hint on which document should be in this set and indeed the user is generally unable to specify what exactly should be in this set especially in the case of web search where the collection is so large the user doesn't have complete knowledge about the whole collection so the best search system can do is to compute approximation of this relevant document set so we denoted by our prime of Q so formally we can see the task is to compute this our prime of Q approximation of the relevant documents so how can we do that now imagine if you are now asked to write a program to do this what would you do now think for a moment so these are the your input the query the documents
#c6	we;the information;the two systems;that intex retrieval;the data;free tax;databases;they;structured data;a clear defined schema;you;this column;the names;people;that column;ages;unstructured text;it;what;the names;people;the text becaus;this difference;we;text information;we;the natural language processing lab;databases;data;the semantics;important difference;the queries;the difference;the information;data;text queries;database;the queries;well defined think;the SQL query;this specify;what records;it;semantics keyword queries;natural language queries;it;what dawkins;the database;the SQL query;a computer specification;what;cause;these differences;the case;text retrieval;we;the documents;the database search;we;records;match records;the SQL query;the case;text retrieval;what;the right answers;a query;we;it;what;the right answers;a query;very important consequences;text retrieval;an empirically defined problem;a problem;it;we;one method;another method;we;empirical evaluation;users;which method;we;an lecture;actually more than one lectures;the issue;evaluation;a very important topic;search engines;algorithm;no way;we;a better algorithm;one system;'s;the problem;a formal way;this slide;a formal formulation;the text retrieval problem;we;our vocabulary set;just a set;words;a language;we;just one language;reality;the web;multiple natural languages;we;text;all kinds;languages;simplicity;we;one kind;language;the techniques;data;multiple languages;the techniques;documents;one language;important difference;the principles;methods;we;the query;a sequence;words;you;the query;a sequence;words;each Q sub;i;word;the vocabulary;a document;the same way;it;a sequence;words;J;a word;the vocabulary;the documents;queries;cases;the documents;you;what;a example;that case;i;you;twitter search write tweets;general documents;the queries;we;a collection;documents;this collection;the web;they;the goal;text retrieval;it;the set;relevant documents;we;R;Q becaus;it;the query;a subset;all the documents;the collection;this set;relevant documents;the sense;the same query;different users;they;the relevant documents;the query;us;the user;only a hint;document;this set;the user;what;this set;the case;web search;the collection;the user;complete knowledge;the whole collection;the best search system;approximation;this relevant document;we;our prime;Q;we;the task;our prime;Q approximation;the relevant documents;we;you;a program;what;you;a moment;the your input;the query;the documents
#s7	and then you have to compute the answers to this query which is a set of documents that would be useful to the user so how would you solve the problem out in general there are two strategies that we can use like the first strategies will document selection and that is we're going to have a binary classification function or binary classifier that's a function that would take a document and query as input and then give zero or one as output to indicate whether this document is relevant to the query or not so in this case you can see the document the the red when the document is set is defined as follows it basically all the documents that have a value of one by this function and so in this case you can see the system must decide if a document is relevant or not basically it has to say whether it's one or zero and this is called absolutely redness basically it needs to know exactly whether it's going to be useful to the user alternatively there's another strategy called document ranking now in this case the system is not going to make a call weather document is relevant or not
#c7	you;the answers;this query;a set;documents;the user;you;the problem;two strategies;we;the first strategies;selection;we;a binary classification function;binary classifier;a function;a document;query;output;this document;the query;this case;you;the document;the document;it;a value;this function;this case;you;the system;a document;it;it;it;it;the user;another strategy;document;this case;the system;a call weather document
#s8	but rather the system is going to use real value function F here that would simply give us a value that would indicate a which document is more likely relevant
#c8	the system;real value function;F;us;a value;a which document
#s9	so it's not going to make a call weather this document is relevant or not
#c9	it;a call weather;this document
#s10	but rather it would say which document is more likely relevant so this function then can be used to rank the documents
#c10	it;which document;this function;the documents
#s11	and then we're going to let the user decide where to stop when the user looks at the documents so we have a threshold see dark here to determine what documents should be in this approximation set
#c11	we;the user;the user;the documents;we;a threshold;what documents;this approximation
#s12	and we're going to assume that all the documents that are ranked above this threshold or in this set becaus in the fact these are the documents that we deliver to the user and theater is cut off terminal by the user
#c12	we;all the documents;this threshold;this set becaus;the fact;the documents;we;the user;theater;terminal;the user
#s13	so here we've got some collaboration from the user in some sense be cause we don't really make a cut off and then use a kind of helped the system make a cut off so in this case the system only needs to decide if one document is more likely relevant there another and that is it only needs to determine relative relevance as opposed to absolute the randomness now you can probably already sense that relevant relative relevance would be easier to determine their absolute relevance becaus in the first case we have to say exactly weather document is relevant or not and it turns out that ranking is indeed that generally preferred to document selection so let's look at this these through series in more detail so this picture shows how it works so on the left side we see these documents and we use the pluses to indicate the relevant documents so we can see that rule relevant documents here consists this set of true relevant documents consist of these classes these documents and with the document is selection function we're going to basically classify them into two groups relevant documents and non relevant ones of course the classifier will not be perfect so it will make mistakes so here we can see in the approximation of the relevant documents we have got some non relevant documents and similarly there is a relevant document let miss cat as wide as non relevant in the case of document ranking we can see the system seems they simply ranks all the documents in the descending order of the scores and then we're going to let the users stop wherever the user wants to stop so if a user wants to examine more documents then the user would go down the list to examine more and stop at the lower position but if the user only wants to read a few relevant documents the user might stop at the top position so in this case the user stops at D four so in effect we have delivered these four documents to our user so as i said ranking is generally preferred and one of the reasons is becaus the classifier in the case of documents that action is unlikely accurate whi becaus the only crew is usually the query but the query may not be accurate in the sense that it could be overly constrained for example you might expect the relevant documents to talk about all these topics you by using specific vocabulary and as a result you might match no random documents becaus in the collection know authors have discussed the topic using these vocabularies so in this case will see there is this problem of no relevant documents to return in the case of overly constrained query on the other hand if the query is under constrained for example if the query does not have sufficient that discriminate if words will find the relevant documents you may actually end up having over delivery and this is when you thought these words might be sufficient to help you find the relevant documents but it turns out that they are not sufficient and there are many distraction documents using similar words
#c13	we;some collaboration;the user;some sense;we;a cut;a kind;the system;a cut;this case;the system;one document;it;relative relevance;the randomness;you;relevant relative relevance;their absolute relevance becaus;the first case;we;exactly weather document;it;ranking;selection;'s;series;more detail;this picture;it;the left side;we;these documents;we;the pluses;the relevant documents;we;relevant documents;this set;true relevant documents;these classes;the document;selection function;we;them;two groups;relevant documents;non relevant ones;course;the classifier;it;mistakes;we;the approximation;the relevant documents;we;some non relevant documents;a relevant document;the case;document ranking;we;the system;they;all the documents;the descending order;the scores;we;the users;the user;a user;more documents;the user;the list;the lower position;the user;a few relevant documents;the user;the top position;this case;the user;D;effect;we;these four documents;our user;i;ranking;the reasons;the classifier;the case;documents;action;unlikely accurate whi becaus;the only crew;the query;the query;the sense;it;example;you;the relevant documents;all these topics;you;specific vocabulary;a result;you;no random documents becaus;the collection;authors;the topic;these vocabularies;this case;this problem;no relevant documents;the case;overly constrained query;the other hand;the query;example;the query;words;the relevant documents;you;delivery;you;these words;you;the relevant documents;it;they;many distraction documents;similar words
#s14	right so this is the case of over delivery unfortunately it's very hard to find the right position between these two extremes why be cause when the user is looking for the information in general the user does not have a good knowledge about the information to be found and in that case the user does not have a good knowledge about watt vocabularies will be used in those rare in documents
#c14	the case;delivery;it;the right position;these two extremes;the user;the information;the user;a good knowledge;the information;that case;the user;a good knowledge;watt vocabularies;documents
#s15	so it's very hard for user to pre specify the right level of of constraints even if the classifier is accurate we also still want to rank these relevant documents be cause they are generally not equally relevant relevance is often a matter of degree so we must prioritize these documents for a user to examine and this note that this prioritization is very important becaus a user cannot digest all the contents at once the user general would have to look at each document sequentially and therefore it would make sense to feed it users with the most relevant documents and that's what ranking is doing so for these reasons ranking is generally preferred now this preference also has a theoretical justification and this is given by the probability ranking principle in the end of this lecture there is a reference for this this principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions first the utility of the document into a user is independent of the utility of any other document second a user would be assumed that who browser results sequentially now it's easy to understand the why these two assumptions are needed in order to justify for the ranking strategy becaus if the documents are independent then we can evaluate the utility of each document separately and this would allow us to compute the score for each document independently
#c15	it;user;the right level;constraints;the classifier;we;these relevant documents;they;equally relevant relevance;a matter;degree;we;these documents;a user;this note;this prioritization;very important becaus;a user;all the contents;the user general;each document;it;sense;it;users;the most relevant documents;what ranking;these reasons;this preference;a theoretical justification;the probability ranking principle;the end;this lecture;a reference;this principle;a ranked list;documents;order;probability;a document;the query;the optimal strategy;the following two assumptions;the utility;the document;a user;the utility;any other document;a user;who;browser;it;these two assumptions;order;the ranking strategy becaus;the documents;we;the utility;each document;us;the score;each document
#s16	and then we're going to rank these documents based on those scores the second assumption is to say that the user would indeed a follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially then obviously the ordering would not be optimal so under these two assumptions we can theoretically justify the ranking strategy is in fact the best you could do now i've put one question here do these two assumptions hold now i suggest you to pause the lecture for a moment to think about these but can you think of some examples that would suggest these assumptions aren't necessarily true now if you think for a moment you may realize none of the assumptions is actually true for example in the case of independence assumption we might have identical documents that have similar content or exactly the same content if you look at each of them alone each is redmond
#c16	we;these documents;those scores;the second assumption;the user;the ranked list;the user;the ranked list;the documents;the ordering;these two assumptions;we;the ranking strategy;fact;you;i;one question;these two assumptions;i;you;the lecture;a moment;you;some examples;these assumptions;you;a moment;you;none;the assumptions;example;the case;independence assumption;we;identical documents;similar content;exactly the same content;you;them;redmond
#s17	but if the user has already seen one of them we assume it's generally not very useful for the user to see another similar or duplicated one so clearly the utility of the document is dependent on other documents that the user has seen in some other cases you might see a scenario where one document that may not be useful to the user
#c17	the user;them;we;it;the user;the utility;the document;other documents;the user;some other cases;you;a scenario;the user
#s18	but when three particular documents are put together they provide the answer to the user 's question so this is a collective redness and that also suggests that the value of a document might depend on other documents sequencer browsing generally would make sense if you have a ranked list there but even if you have a ramp list there was evidence showing that users don't always just go strictly sequentially through the entire list there sometimes would look at the bottom for example or skip some and if you think about it the more complicated interface that we could possible to use like two dimensional interface where you can put additional information on the screen then sequential browsing is a very restrictive assumption so the point here is that none of these assumptions is really true but nevertheless the probability ranking principle established some sort of the foundation for ranking as primary task for search engines and this has actually been the basis for a lot of research work information retrieval and many algorithms have been designed that based on this assumption despite that the assumptions of nessus ritual
#c18	three particular documents;they;the answer;the user 's question;a collective redness;the value;a document;other documents sequencer;sense;you;a ranked list;you;a ramp list;evidence;users;the entire list;the bottom;example;you;it;the more complicated interface;we;two dimensional interface;you;additional information;the screen;sequential browsing;a very restrictive assumption;the point;none;these assumptions;the probability ranking principle;some sort;the foundation;primary task;search engines;the basis;a lot;research work information retrieval;many algorithms;this assumption;the assumptions
#s19	and we can address this problem by doing post processing of a ranked list for example to remove redundancy so to summarize this lecture the main points that you can take away the following first text retrieval is an empirical defined problem and that means which algorithm is better must be judged by the users second document ranking is generally preferred and this will help users prioritize examination of search results and this is also the bypass the difficulty in determining absolute relevance becaus we can get some help from users in determining where to make the cut off it's more flexible so this further suggests that the main technical challenge in designing a search engine is redesign effective ranking function in other words we need to define what is the value of this function F on the query and document pair a whole are designed such a function is the main topic in the following lectures there are two suggest to the additional readings the first is the classical paper on probability ranking principle the second is a must read for anyone doing research information tribble
#c19	we;this problem;post processing;a ranked list;example;redundancy;this lecture;the main points;you;the following first text retrieval;an empirical defined problem;algorithm;the users;second document ranking;users;examination;search results;the bypass;the difficulty;absolute relevance becaus;we;some help;users;it;the main technical challenge;a search engine;effective ranking function;other words;we;what;the value;this function;F;the query;document pair;a whole;such a function;the main topic;the following lectures;the additional readings;the classical paper;probability ranking principle;anyone;research information
#s20	it's classical IR book which has excellent coverage of the main researcher results in early days up to the time when the book was written chapter six of this book has indexed discussion of the probability ranking principle and probabilistic retrieval models in general
#c20	it;classical IR book;excellent coverage;the main researcher results;early days;the time;the book;this book;discussion;the probability;ranking principle and probabilistic retrieval models
410	93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	75
#s1	This lecture is about the expectation maximization algorithm, also called the EM algorithm.
#c1	This lecture;the expectation maximization;algorithm;the EM algorithm
#s2	In this lecture, we're going to continue the discussion of probabilistic topic models.
#c2	this lecture;we;the discussion;probabilistic topic models
#s3	In particular, we're going to introduce the EM algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models.
#c3	we;the EM algorithm;a family;useful algorithms;the maximum likelihood estimate;mixture models
#s4	So this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here.
#c4	familiar scenario;a two component mixture model;the background words;one topic word distribution
#s5	So we are interested in computing this estimate.
#c5	we;this estimate
#s6	And we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all the other parameters are known.
#c6	we;these probability values;the probability;the observed document;we;all the other parameters
#s7	So the only thing unknown is this word probabilities are given by theta sub d update.
#c7	the only thing;this word;probabilities;d update
#s8	And in this lecture were going to look into how to compute this maximum likelihood  estimator.
#c8	this lecture;this maximum likelihood  estimator
#s9	Now let's start with the idea of separating the words in the text data into two groups.
#c9	's;the idea;the words;the text data;two groups
#s10	One group would be explained by the background model, the other group would be explained by the Unknown topic word distribution After all, this is the basic idea of mixture model.
#c10	One group;the background model;the other group;the Unknown topic word distribution;the basic idea;mixture model
#s11	But suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution.
#c11	we;which word;which distribution;example;these words;we;this background word distribution
#s12	On the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution.
#c12	the other hand;the other words;text;mining;clustering;the topic word distribution
#s13	If you can see the color, then these are shown in blue.
#c13	you;the color;blue
#s14	These blue words are then assumed to be from the topic word distribution.
#c14	These blue words;the topic word distribution
#s15	If we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right?
#c15	we;these words;the problem;the world distribution
#s16	If you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and normalize them.
#c16	you;a moment;you;we;all these words;this word distribution theta;d;them
#s17	So indeed this problem will be very easy to solve.
#c17	this problem
#s18	If we had known which words are from which distribution precisely.
#c18	we;which words;which distribution
#s19	And this is in fact Making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the Single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime
#c19	fact;this model;no longer mixture model;we;which distribution;which part;the data;we;the Single word distribution problem;this case;'s;these words;theta d pseudo document
#s20	and then all we need to do is just normalize these word counts for each word w sub I.
#c20	we;these word;each word;w sub;I.
#s21	And that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now.
#c21	it;the maximum likelihood estimate
#s22	This idea, however, doesn't work, because we in practice don't really know which word is from which distribution.
#c22	This idea;we;practice;which word;which distribution
#s23	But this gives us the idea of perhaps we can guess which word is from which.
#c23	us;the idea;we;which word
#s24	distribution.
#c24	distribution
#s25	Specifically, given all the parameters can we infer the distribution the word is from So let's assume that we actually know tentative probabilities for these words in theta sub D.
#c25	all the parameters;we;the distribution;the word;'s;we;tentative probabilities;these words;theta;sub D.
#s26	So now all the parameters are known for this mixture model.
#c26	all the parameters;this mixture model
#s27	And now let's consider word like  text.
#c27	's;word;  text
#s28	So the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub B?
#c28	the question;you;text;theta sub;d;theta sub B
#s29	So in other words, we want to infer which distribution has been used to generate this text.
#c29	other words;we;which distribution;this text
#s30	Now, this inference process is a typical Bayesian inference situation where we have some prior about These two distributions so can you see what is our prior here?
#c30	this inference process;a typical Bayesian inference situation;we;These two distributions;you;what
#s31	Well the prior here is the probability of each distribution, right?
#c31	the probability;each distribution
#s32	So the prior is given by these two probabilities.
#c32	the prior;these two probabilities
#s33	In this case the prior is Saying that each model is equally likely, but we can imagine perhaps a different prior possible.
#c33	this case;the prior;each model;we
#s34	So this is called prior because this is our guess of which distribution has been used to generate the world before we even observe the word.
#c34	our guess;which distribution;the world;we;the word
#s35	So that's why we call it prior.
#c35	we;it
#s36	if we don't observe the word or we don't know what word has been observed, our best guess is just say well.
#c36	we;the word;we;what word;our best guess
#s37	They are equally likely.
#c37	They
#s38	Alright, so it's just a flipping a coin.
#c38	it;just a flipping a coin
#s39	Now in Bayesian inference, we typically then would update our belief after we have observed evidence.
#c39	Bayesian inference;we;our belief;we;evidence
#s40	So what is evidence here?
#c40	what;evidence
#s41	While the evidence here is the word text.
#c41	the evidence;the word text
#s42	Now that we are interested in the word text, so text can be regarded as evidence.
#c42	we;the word text;text;evidence
#s43	And in the if we use Bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basically the probability of the word text from each distribution and we see that in both cases text is possible that even in the background it is still possible.
#c43	we;Bayes rule;what;we;the prior;the likelihood;you;the probability;the word text;each distribution;we;both cases;text;the background;it
#s44	It just has a very small probability.
#c44	It;a very small probability
#s45	So intuitively, what would be your guess?
#c45	what;your guess
#s46	So in this case Now, if you're like many others, you will guess text is probably from theta sub d is more likely from  theta sub d, why?
#c46	this case;you;many others;you;text;theta sub;d;theta;sub;d
#s47	And you will probably see that it's because.
#c47	you;it
#s48	Text has a much higher probability here.
#c48	Text;a much higher probability
#s49	By the theta sub d, than by the background model, which has a very small probability.
#c49	the theta;sub;d;the background model;a very small probability
#s50	And by this we are going to say text is more likely from theta sub d.
#c50	we;text;theta sub d.
#s51	So you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution.
#c51	you;our guess;which distribution;the text;how high the probability;the data;the text;each word distribution
#s52	We are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so.
#c52	We;the distribution;the word;higher probability;the likelihood
#s53	We're going to choose word that has a higher likelihood.
#c53	We;word;a higher likelihood
#s54	So in other words, we're going to compare these two probabilities.
#c54	other words;we;these two probabilities
#s55	Of the word given by each distributions.
#c55	the word;each distributions
#s56	But our guess must also be affected by the prior, so we also need to compare these two priors why?
#c56	our guess;the prior;we;these two priors
#s57	because imagine if we adjust these probabilities, we're going to say the probability of choosing a background model is almost 100%.
#c57	we;these probabilities;we;the probability;a background model;almost 100%
#s58	Now if we have that kind of strong prior, then that would affect your guess.
#c58	we;that kind;your guess
#s59	You might think well, wait a moment, maybe text could have been from the background as well, although the probability is very small here.
#c59	You;text;the background;the probability
#s60	The prior is very high.
#c60	The prior
#s61	So in the end we have to combine the two and the bayes formula provides provides us a solid and principled way of making these kind of guess to quantify that.
#c61	the end;we;the bayes formula;us;a solid and principled way;these kind
#s62	So more specifically, let's think about the probability that this word text has been generated.
#c62	's;the probability;this word text
#s63	In fact from theta sub D, The in order for texture to be generated from theta sub d two things must happen first.
#c63	fact;theta sub D;order;texture;theta sub;d;two things
#s64	The theta sub d must have been selected, so we have the selection probability here, and Secondly, we also have to actually have observed text from the distribution.
#c64	The theta sub;d;we;the selection probability;we;text;the distribution
#s65	So when we multiply the two together, we get the probability that text has in "fact has been generated from theta sub d  Similarly, for the background model an.
#c65	we;we;the probability;text;"fact;theta sub;the background model
#s66	The probability of generating text is another product of similar form.
#c66	The probability;generating text;another product;similar form
#s67	We also introduced a latent variable Z here to denote whether the word is from the background or the topic.
#c67	We;a latent variable Z;the word;the background;the topic
#s68	When Z is zero, it means it's from the topic theta sub d when it's one, it means it's from the background theta
#c68	Z;it;it;the topic theta;d;it;it;it;the background theta
#s69	sub b.
#c69	sub
#s70	So now we have the probability that text is generated from each.
#c70	we;the probability;text
#s71	Then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub b.
#c71	we;we;them;estimate;the probability;the word text;theta sub;d;theta sub b.
#s72	And equivalently, the probability that Z is equal to 0 given that the observed evidence is text.
#c72	Z;the observed evidence;text
#s73	So this is Application of Bayes rule.
#c73	Application;Bayes;rule
#s74	But this step is very crucial for understanding the EM algorithm.
#c74	this step;the EM algorithm
#s75	Because if we can do this, then we would be able to 1st initialize the parameter values Somewhat randomly, and then we're going to take a guess of these Z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply Bayes rule to infer which distribution is more likely to generate Each word and this prediction essentially helped us to separate words from the two distributions, although we can't separate them for sure, but we can separate them probabilistically as shown here.
#c75	we;we;the parameter values;we;a guess;these Z values;which distribution;which word;the initialized parameter values;us;a complete specification;the mixture model;us;Bayes rule;which distribution;Each word;this prediction;us;words;the two distributions;we;them;we;them
410	944e5a7f-6bd7-42ec-be49-afb0d307fb85	130
#s1	This lecture is about the feedback in the vector space model.
#c1	This lecture;the feedback;the vector space model
#s2	In this lecture we continue talking about feedback in text retrieval.
#c2	this lecture;we;feedback;text retrieval
#s3	Particularly, we're going to talk about feedback in the vector space model.
#c3	we;feedback;the vector space model
#s4	As we have discussed before, in the case of feedback.
#c4	we;the case;feedback
#s5	The task of a Text Retrieval system is to learn from examples to improve retrieval accuracy.
#c5	The task;a Text Retrieval system;examples;retrieval accuracy
#s6	We will have positive examples.
#c6	We;positive examples
#s7	Those are the documents that are assumed to be relevant or judgement be relevant.
#c7	the documents
#s8	Or the document that are viewed by users.
#c8	Or the document;users
#s9	We also have negative examples.
#c9	We;negative examples
#s10	Those are documents known in non-relevant, and they can also be the document that escaped by users.
#c10	documents;they;the document;users
#s11	The general method in the vector space model for feedback.
#c11	The general method;the vector space model;feedback
#s12	Is to modify our query vector and we want to place the query vector in a better position to make the accurate.
#c12	our query vector;we;the query vector;a better position
#s13	And what does that mean exactly?
#c13	what
#s14	If we think about the query vector, that would mean we have to do something to the vector elements.
#c14	we;the query vector;we;something;the vector elements
#s15	And in general, that would mean we might add new terms.
#c15	we;new terms
#s16	We might adjust weights of all terms or assign weights to new terms.
#c16	We;weights;all terms;weights;new terms
#s17	And as a result, in general the query will have more terms, so we often call this query expansion.
#c17	a result;the query;more terms;we;this query expansion
#s18	The most effective method in the vector space model for feedback is called Walker feedback, which was after he proposed at several decades ago.
#c18	The most effective method;the vector space model;feedback;Walker feedback;he
#s19	So the idea is quite simple.
#c19	the idea
#s20	We illustrate this idea by using a 2 dimensional display of all the documents in the collection and also the query vector.
#c20	We;this idea;a 2 dimensional display;all the documents;the collection;also the query vector
#s21	So now we can see the query vector is here in the center.
#c21	we;the query vector;the center
#s22	And these are all the documents.
#c22	all the documents
#s23	So when we use the query vector and use a similarity function to find the most similar documents, we are basically drawing a circle here and then.
#c23	we;the query vector;a similarity function;the most similar documents;we;a circle
#s24	These documents would be basically the top ranked of the documents.
#c24	These documents;the top;the documents
#s25	And these pluses are relevant documents.
#c25	these pluses;relevant documents
#s26	And these are relevant documents.
#c26	relevant documents
#s27	For example is relevant, etc.
#c27	example
#s28	And then these minuses are negative documents like this.
#c28	these minuses;negative documents
#s29	So our goal here is trying to move this query vector to some position to improve the retrieval accuracy.
#c29	our goal;this query vector;some position;the retrieval accuracy
#s30	By looking at this diagram.
#c30	this diagram
#s31	What do you think?
#c31	What;you
#s32	Where should we move the query of after so that we can improve the retrieval accuracy?
#c32	we;the query;we;the retrieval accuracy
#s33	Intuitively, where do you want to move the query vector?
#c33	you;the query vector
#s34	Now, if you want to think more, you can pause the video.
#c34	you;you;the video
#s35	Now, if you think about this picture, you can realize that in order to work well in this case you want to query about that to be as close to the positive vectors as possible.
#c35	you;this picture;you;order;this case;you;the positive vectors
#s36	That means ideally you want to place the query vector somewhere here, or you want to move the query vector closer to this point.
#c36	you;the query vector;you;the query vector;this point
#s37	Now, So what exactly is this point?
#c37	what;this point
#s38	Well, If you want these relevant documents to be ranked on the top, you want this to be in the center of all these relevant documents, right?
#c38	you;these relevant documents;the top;you;the center;all these relevant documents
#s39	Because then if you draw a circle around this one, you get all these relevant documents.
#c39	you;a circle;this one;you;all these relevant documents
#s40	So that means we can move the query vector towards the centroid of all the relevant locking vectors.
#c40	we;the query vector;the centroid;all the relevant locking vectors
#s41	And this is basically the idea of Rocchio you.
#c41	the idea;Rocchio;you
#s42	Of course you can consider the centroid of negative documents, and we want to move away from the negative documents.
#c42	you;the centroid;negative documents;we;the negative documents
#s43	Not geometrically, we're talking about the movie vector closer to some other bath and away from other vectors.
#c43	we;the movie vector;some other bath;other vectors
#s44	Algebraically, it just means we have this formula.
#c44	it;we;this formula
#s45	Here you can see this is original query vector.
#c45	you;original query vector
#s46	And, this average basically is the centroid vector of relevant documents.
#c46	this average;the centroid vector;relevant documents
#s47	When we take the average of these vectors, then we are computing the centroid of these vectors and similarly this is the average non relevant document vectors.
#c47	we;the average;these vectors;we;the centroid;these vectors;the average non relevant document vectors
#s48	So it's essentially of non relevant documents and we have these three parameters here of our alpha beta and gamma they're controlling the amount of movement.
#c48	it;non relevant documents;we;these three parameters;our alpha beta;gamma;they;the amount;movement
#s49	When we add these two vectors together, we are moving the query  closer to the centroid.
#c49	we;these two vectors;we;the query;the centroid
#s50	I said add them together when we subtract this part.
#c50	I;them;we;this part
#s51	We kind of move the query vector away from that Central.
#c51	We;the query vector;that Central
#s52	So this is the main idea of Rocchio feedback and after we have done this we will get a new query vector which can be used to store documents.
#c52	the main idea;Rocchio feedback;we;we;a new query vector;documents
#s53	This newer query vector will then reflect the move of this original query vector toward this relevant centroid vector and away from the non relevant centroid vector.
#c53	This newer query vector;the move;this original query vector;this relevant centroid vector;the non relevant centroid vector
#s54	OK, so let's take a look at the example.
#c54	's;a look;the example
#s55	This is the example that we have seen earlier, only that I give the display of the actual documents.
#c55	the example;we;I;the display;the actual documents
#s56	I only showed the vector representation of these documents.
#c56	I;the vector representation;these documents
#s57	We have 5 documents here.
#c57	We;5 documents
#s58	And we have.
#c58	we
#s59	Two relevant the documents here.
#c59	Two relevant the documents
#s60	Right?
#s61	They are displayed in red and these are the term vectors and I have just assumed some TF IDF weights.
#c61	They;red;the term vectors;I;some TF IDF weights
#s62	A  lot of terms.
#c62	A  lot;terms
#s63	We have zero weights of course and these are negative documents.
#c63	We;zero weights;course;negative documents
#s64	There are two here.
#s65	There was another one here.
#c65	another one
#s66	Now in this Rocchio method we first compute the centroid of each category I so let's see.
#c66	this Rocchio method;we;the centroid;each category;I;'s
#s67	Look at the centroid vectors of the positive documents.
#c67	the centroid vectors;the positive documents
#s68	What we simply just, so it's very easy to see.
#c68	What;we;it
#s69	We just add this with this one.
#c69	We;this one
#s70	The corresponding element, and that's down here and take the average
#c70	The corresponding element
#s71	and then we will know added the corresponding elements and then just take the average right?
#c71	we;the corresponding elements;the average right
#s72	So we do this for all these.
#c72	we
#s73	In the end what we have is this one.
#c73	the end;what;we;this one
#s74	This is the average vector of these two.
#c74	the average vector
#s75	So it's a centroid of this two.
#c75	it;a centroid
#s76	That's also look at the centroid of the negative documents.
#c76	the centroid;the negative documents
#s77	This is basically the same.
#s78	we are gonna take the average of three elements.
#c78	we;the average;three elements
#s79	And these are the corresponding elements in the three vectors, and so on so forth.
#c79	the corresponding elements;the three vectors
#s80	So in the end we have this one.
#c80	the end;we;this one
#s81	Now in the Rocchio Feedback Method, we're going to combine all these with the original query vector, which is this.
#c81	the Rocchio Feedback Method;we;the original query vector
#s82	So now, let's see how we combine them together.
#c82	's;we;them
#s83	That's basically this.
#s84	I saw we have a parameter alpha controlling the original period  weight, that's one, and then we have beta to control the influence of the positive centroid weight that's 1.5 that comes from here.
#c84	I;we;a parameter alpha;the original period;  weight;we;beta;the influence;the positive centroid weight
#s85	Alright, so this.
#s86	Goes here.
#s87	And we also have this negative weight here, controlled by Gamma here and this weight that has come.
#c87	we;this negative weight;Gamma;this weight
#s88	From of course the negative centroid here.
#c88	course;the negative centroid
#s89	And we do exactly the same for other terms.
#c89	we;other terms
#s90	Each is for one term.
#c90	one term
#s91	And this is our new of vector.
#c91	our new;vector
#s92	And we're going to use this new query vector.
#c92	we;this new query vector
#s93	This one to rent the documents.
#c93	This one;the documents
#s94	You can imagine what would happen right because of the movement that this one would match these red documents much better because we move this vector closer to them and it's going to penalize these black documents in these non relevant documents.
#c94	You;what;the movement;this one;these red documents;we;this vector;them;it;these black documents;these non relevant documents
#s95	So this is precisely what we want from feedback.
#c95	precisely what;we;feedback
#s96	Now, of course, if we apply this method in practice.
#c96	course;we;this method;practice
#s97	We will see one potential problem.
#c97	We;one potential problem
#s98	And that is the original query has only four terms that are.
#c98	the original query;only four terms
#s99	non-zero
#s100	But after we do query expansion, you can imagine it would have many terms that would have nonzero weights.
#c100	we;query expansion;you;it;many terms;nonzero weights
#s101	So the calculation would have to involve more terms.
#c101	the calculation;more terms
#s102	In practice, we often truncate this vector, and only retain the terms with highest weights.
#c102	practice;we;this vector;the terms;highest weights
#s103	So let's talk about how we use this method in practice.
#c103	's;we;this method;practice
#s104	I just mentioned that we often truncated adapter and see the only a small number of words that have highest weights in the centroid vector.
#c104	I;we;adapter;the only a small number;words;highest weights;the centroid vector
#s105	This is for efficiency concern.
#c105	efficiency concern
#s106	I also say that here that negative examples or non relevant examples tend not to be very useful, especially compared with positive examples.
#c106	I;negative examples;non relevant examples;positive examples
#s107	Now you can think about why.
#c107	you
#s108	One reason is because negative documents tend to distract the query in all directions.
#c108	One reason;negative documents;the query;all directions
#s109	So when you take the average it doesn't really tell you where exactly should be moving too.
#c109	you;the average;it;you
#s110	Where is positive documents tend to be clustered together and they will point you to a consistent direction.
#c110	positive documents;they;you;a consistent direction
#s111	So that also means sometimes we don't have to use those negative examples.
#c111	we;those negative examples
#s112	But note that in some cases in difficult queries where most top ranking results are negative, negative feedback factor is very useful.
#c112	some cases;difficult queries;most top ranking results;negative feedback factor
#s113	Another thing is to avoid overfitting.
#c113	Another thing
#s114	That means we have to keep relatively high weight on original query terms.
#c114	we;relatively high weight;original query terms
#s115	Why?
#s116	Because The sample that we see in feedback is a relatively small sample.
#c116	The sample;we;feedback;a relatively small sample
#s117	We don't want to overly trust the small sample.
#c117	We;the small sample
#s118	An the original query terms are still very important.
#c118	An the original query terms
#s119	Those terms of typing by the user and the user has decided that those terms are most important.
#c119	Those terms;the user;the user;those terms
#s120	So in order to prevent us from overfitting or drifting the topic with preventing topic drifting due to bias toward the feedback examples, we generally would have to keep a pretty high weight on the original terms.
#c120	order;us;the topic;topic;the feedback examples;we;a pretty high weight;the original terms
#s121	It's always safe to do that.
#c121	It
#s122	And this is especially true for pseudo relevance feedback.
#c122	pseudo relevance feedback
#s123	Now this method can be used for both relevance feedback and pseudo relevance feedback.
#c123	this method;both relevance feedback;pseudo;relevance feedback
#s124	In the case of pseudo feedback up, the parameter beta should be set to a smaller value because the relevant examples are assumed to be relevant.
#c124	the case;pseudo;feedback;the parameter beta;a smaller value;the relevant examples
#s125	They are not as reliable as the relevance feedback.
#c125	They;the relevance feedback
#s126	In the case of relevance feedback, we obviously could use a larger value, so those parameters they have to be set in imparallelly.
#c126	the case;relevance feedback;we;a larger value;those parameters;they
#s127	And the root Rocchio.
#c127	And the root Rocchio
#s128	method is usually robust.
#c128	method
#s129	and effective.
#s130	It's still very popular method of all feedback.
#c130	It;very popular method;all feedback
410	955ff55e-55e4-4de9-bd5b-34c4e2a32f4f	85
#s1	This lecture is about the web search.
#c1	This lecture;the web search
#s2	In this lecture we're going to talk about one of the most important applications of text retrieval: web search engines.
#c2	this lecture;we;the most important applications;text retrieval;web search engines
#s3	So let's first look at the some general challenges and opportunities in web search.
#c3	's;the some general challenges;opportunities;web search
#s4	Now, many information retrieval algorithms had been developed before the web was born, so when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about.
#c4	many information retrieval algorithms;the web;the web;it;the best opportunity;those algorithms;major application problem;everyone
#s5	So naturally there had to be some further extensions of the classical search algorithms.
#c5	some further extensions;the classical search algorithms
#s6	To address some new challenges encountered in web search.
#c6	some new challenges;web search
#s7	So here are some general challenges.
#c7	some general challenges
#s8	Firstly, there is a scalability challenge.
#c8	a scalability challenge
#s9	How to handle the size of the web an ensure completeness of coverage of all the information.
#c9	the size;the web;an ensure completeness;coverage;all the information
#s10	How to serve many users quickly and by answering all their queries?
#c10	many users;all their queries
#s11	So that's one major challenge before the web was born, the scale of search was relatively small.
#c11	one major challenge;the web;the scale;search
#s12	The second problem is that this low quality information and there are often spams.
#c12	The second problem;spams
#s13	The third challenge is dynamics of the web.
#c13	The third challenge;dynamics;the web
#s14	The new pages are constantly created and some pages may be updated very quickly, so it makes it harder to keep the index fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine.
#c14	The new pages;some pages;it;it;the index;the challenges;we;order;a high quality web search engine
#s15	On the other hand, there are also some interesting opportunities that we can leverage to improve search results.
#c15	the other hand;some interesting opportunities;we;search results
#s16	There are many additional heuristics.
#c16	many additional heuristics
#s17	For example, using links that we can leverage to improve scoring.
#c17	example;links;we;scoring
#s18	Now the algorithm that we talked about, such as a vector space model are general algorithms.
#c18	the algorithm;we;a vector space model;general algorithms
#s19	And there can be applied to any search applications, so that's the advantage.
#c19	any search applications;the advantage
#s20	On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search.
#c20	the other hand;they;advantage;special characteristics;pages;documents;the specific applications;web search
#s21	Web pages are linked with each other, so obviously the link information is something that we can also leverage.
#c21	Web pages;the link information;something;we
#s22	So because of these challenges and opportunities, there are new techniques that have been developed for web search or due to the need for web search.
#c22	these challenges;opportunities;new techniques;web search;the need;web search
#s23	One is parallel indexing and searching and this is to address the issue of scalability.
#c23	parallel indexing;the issue;scalability
#s24	In particular, Googles's imagine of MapReduce is very influential and has been very helpful in that Aspect.
#c24	Googles's imagine;MapReduce;that Aspect
#s25	Second, there are techniques that are developed for addressing the problem of spams.
#c25	techniques;the problem;spams
#s26	So spam detection we have to prevent those spam pages from being ranked high.
#c26	we;those spam pages
#s27	And there are also techniques to achieve robust ranking, and we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with a particular trick.
#c27	techniques;robust ranking;we;a lot;signals;pages;it;the search engine;a particular trick
#s28	And the third line of techniques is link analysis.
#c28	the third line;techniques;analysis
#s29	And these are techniques that can allow us to improve search results by leveraging extra information.
#c29	techniques;us;search results;extra information
#s30	And in general, in web search we're going to use multiple features for ranking, not just a link analysis, but also exploit in all kinds of clues, like the layout of web pages or anchor text that describes a link to another page.
#c30	web search;we;multiple features;ranking;not just a link analysis;all kinds;clues;the layout;web pages;anchor text;a link;another page
#s31	So here's a picture showing the basic search engine technology is.
#c31	a picture;the basic search engine technology
#s32	Basically this is the web on the left, and then user on the right side, and we're going to help this user to get the access for the web information and the first component is the crawler.
#c32	the web;the left;then user;the right side;we;this user;the access;the web information;the first component;the crawler
#s33	That would crawl pages.
#c33	pages
#s34	And then the second component is indexer that would take these pages and create a inverted index.
#c34	the second component;indexer;these pages;a inverted index
#s35	The third component that is a retriever that would use inverted index to answer users query by talking to the users browser and then the search results will be given to the user and then the browser would show those results and to allow the user to interact with the web so we are gonna talk about each of these components.
#c35	The third component;a retriever;inverted index;users;query;the users browser;the search results;the user;the browser;those results;the user;the web;we;these components
#s36	First we're going to talk about the crawler.
#c36	we;the crawler
#s37	Also called a spider or software robot that would do something like a crawling pages on the web.
#c37	a spider or software robot;something;a crawling pages;the web
#s38	To build a toy crawler is relatively easy 'cause you just need to start with a set of seed pages and then fetch pages from the web and pause these pages or and figure out the new links and then add them to the priority queue and then just explore those additional links.
#c38	a toy crawler;you;a set;seed pages;pages;the web;these pages;the new links;them;the priority queue;those additional links
#s39	But to build a real crawler actually is tricky and there are some complicated issues that you have to deal with.
#c39	a real crawler;some complicated issues;you
#s40	So for example, robustness.
#c40	example
#s41	What if the server doesn't respond?
#c41	What;the server
#s42	What if there's a trap that generates dynamically generated web pages that might attract your crawler to keep crawling the same site and to fetch dynamically generated pages?
#c42	What;a trap;dynamically generated web pages;your crawler;the same site;dynamically generated pages
#s43	The results with this issue of crawling courtesy and you don't want to overload one particular server with many crawling requests.
#c43	The results;this issue;courtesy;you;one particular server;many crawling requests
#s44	You have to respect the robot exclusion protocol.
#c44	You;the robot exclusion protocol
#s45	You also need to handle different types of files.
#c45	You;different types;files
#s46	There are images, PDF files, all kinds of formats on the web.
#c46	images;PDF files;all kinds;formats;the web
#s47	Ann, you have to also consider UI or extension, so sometimes those are CGI script an there are internal references etc and sometimes you have JavaScripts on the page that they also create the challenges and you ideally should also recognize redundant pages 'cause you don't have to duplicate the those pages.
#c47	you;UI;extension;CGI script;internal references;you;JavaScripts;the page;they;the challenges;you;redundant pages;you;the those pages
#s48	And finally, you may be interested in the discover hidden urls.
#c48	you;the discover hidden urls
#s49	Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path that you might be able to get some additional pages.
#c49	URLs;any page;you;the URL;a shorter path;you;some additional pages
#s50	So what are the major crawling strategies in general?
#c50	what;the major crawling strategies
#s51	Breadth first is most common becauses naturally balance balance is the server load.
#c51	Breadth;most common becauses;balance;the server load
#s52	You would not keep probing a particular server with many requests.
#c52	You;a particular server;many requests
#s53	Also, parallel crawling is very natural because this task is very easy to parallelize.
#c53	parallel crawling;this task
#s54	And there are some variations of the crawling task, and one interesting variation is called focused crawling.
#c54	some variations;the crawling task;one interesting variation;focused crawling
#s55	In this case, we're going to crawl just some pages about a particular topic.
#c55	this case;we;just some pages;a particular topic
#s56	For example, all pages about the automobiles.
#c56	example;the automobiles
#s57	And this is typically going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and gradually crawl more.
#c57	a query;you;the query;some results;a major search engine;you;those results
#s58	So one challenge in crawling is to find the new pages that people have created and people probably are creating new pages all the time.
#c58	one challenge;the new pages;people;people;new pages
#s59	And this is very challenging if the new pages have not been actually linked to any old page.
#c59	the new pages;any old page
#s60	If they are, then you can probably find them by re-crawling the old page.
#c60	they;you;them;the old page
#s61	So these are also some interesting challenges that have to be solved.
#c61	some interesting challenges
#s62	And finally, we might face the scenario of incremental crawling or repeated crawling, right?
#c62	we;the scenario;incremental crawling;repeated crawling
#s63	So your first, let's say if you want to build a web search engine and you're the first crawl a lot of data from the web.
#c63	's;you;a web search engine;you;the first crawl;data;the web
#s64	And then but then, once you have collected all the data and in the future we just need to crawl the updated pages.
#c64	you;all the data;the future;we;the updated pages
#s65	In general you don't have to re-crawl everything right?
#c65	you;everything
#s66	Or it's not necessary.
#c66	it
#s67	So in this case you your goal is to minimize the resource overhead by using minimum resources to just still crawl the updates to pages.
#c67	this case;you;your goal;the resource;minimum resources;the updates;pages
#s68	So this is actually very interesting research question here, and it's still open research question in that there aren't many standard algorithms established yet for doing this task.
#c68	very interesting research question;it;open research question;many standard algorithms;this task
#s69	But in general, you can imagine you can learn from the past experience.
#c69	you;you;the past experience
#s70	So the two major factors that you have to consider are first will this page be updated frequently and do I have to crawl this page again if the page is a static page that hasn't been changed for months, you probably don't have to re-crawl it everyday.
#c70	the two major factors;you;this page;I;this page;the page;a static page;months;you;it
#s71	Because it's unlikely that it will be changed frequently.
#c71	it;it
#s72	On the other hand, if it's a sports score page that gets updated very "frequently and you may need to  the maybe even multiple times on the same day and the other factor to consider is this page frequently accessed by users.
#c72	the other hand;it;a sports score page;you;the maybe even multiple times;the same day;the other factor;this page;users
#s73	If it is, then it means it's a high utility page
#c73	it;it;it;a high utility page
#s74	and then that's it's more important to ensure such a page to be fresh.
#c74	it;such a page
#s75	Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot, then it's probably not necessary to crawl that page, or at least it's not as not as urgent as to maintain the freshness of frequently accessed page by users.
#c75	another page;any users;a year;that page;it;that page;it;the freshness;frequently accessed page;users
#s76	So to summarize, web search is one of the most important applications of text retrieval, and there are some new challenges, particularly scalability, efficiency, quality information.
#c76	web search;the most important applications;text retrieval;some new challenges;particularly scalability;efficiency;quality information
#s77	There are also new opportunities, particularly rich link information and layout, etc.
#c77	new opportunities;particularly rich link information;layout
#s78	A crawler is an essential component of web search applications.
#c78	A crawler;an essential component;web search applications
#s79	And in general we can classify two scenarios.
#c79	we;two scenarios
#s80	One is initial crawling and here we want to have complete crowding.
#c80	initial crawling;we;complete crowding
#s81	Of the web.
#c81	the web
#s82	If you are doing a general search engine or focused crawling, if you want to just to target at a certain type of pages.
#c82	you;a general search engine;you;a certain type;pages
#s83	And then there is another scenario that's incremental updating of the  crawled data or incremental crawling.
#c83	another scenario;incremental updating;data
#s84	In this case you need to optimize the resource.
#c84	this case;you;the resource
#s85	Try to use minimum resource to get the needed fresh information.
#c85	minimum resource;the needed fresh information
410	95f92696-1963-4307-83c6-a8370ff03b30	61
#s1	This lecture is about the contextual text mining.
#c1	This lecture;the contextual text mining
#s2	Contextual text mining is related to multiple kinds of knowledge that we mine from text data.
#c2	Contextual text mining;multiple kinds;knowledge;we;text data
#s3	As I'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context.
#c3	I;topic mining;topics;context;a time;location;it;opinion mining;opinions
#s4	It's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem.
#c4	It;based prediction;it;us;non text data;text data;sophisticated predictors;the prediction problem
#s5	So more specifically, why are we interested in contextual text mining?
#c5	we;contextual text mining
#s6	Well that's, first, because text often has rich context information and this can include direct context such as meta data.
#c6	text;rich context information;direct context;meta data
#s7	And also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data.
#c7	And also indirect context;the direct context;the meta-data;time;location;authors;source;the text data
#s8	And they almost always available to us.
#c8	And they;us
#s9	Indirect text context refers to additional data related to the meta data.
#c9	Indirect text context;additional data;the meta data
#s10	So, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age.
#c10	example;authors;we;additional context;social network;the author;the author's age
#s11	And such information is not, in general, directly related to the text yet through the authors we can connect them.
#c11	such information;the text;the authors;we;them
#s12	There could be also other text data from the same source as this one, so the other context data can be connected with this text, as well.
#c12	other text data;the same source;this one;the other context data;this text
#s13	So in general, any related data can be regarded as context, so there could be remotely related to context.
#c13	any related data;context
#s14	context.
#c14	context
#s15	And so what's the use of, why is text context useful?
#c15	what;the use;text context
#s16	Well, context can be used to partition text data in many interesting ways.
#c16	context;text data;many interesting ways
#s17	It can almost allows partition text data in arbitrary ways as we need.
#c17	It;partition text data;arbitrary ways;we
#s18	And this is very important because this allows us to do interesting comparative analysis.
#c18	us;interesting comparative analysis
#s19	It also in general provides meaning to the discovery topics if we gonna associate the text with context.
#c19	It;meaning;the discovery topics;we;the text;context
#s20	So here's illustration of how context can be regarded as interesting ways of partitioning of text data.
#c20	illustration;context;interesting ways;partitioning;text data
#s21	So here I just show some research papers published in different years.
#c21	I;some research papers;different years
#s22	on different venues, different conference names here listed on the bottom, like SIGIR, ACL, etc.
#c22	different venues;the bottom;SIGIR;ACL
#s23	Now, such text data can be partitioning in many interesting ways because we have context.
#c23	such text data;many interesting ways;we;context
#s24	So the context here just includes time and the conference venues.
#c24	the context;time;the conference venues
#s25	And but perhaps we can include some other variables as well.
#c25	we;some other variables
#s26	But let's see how we can partition data in interesting ways.
#c26	's;we;data;interesting ways
#s27	First, we can treat each paper as a separate unit.
#c27	we;each paper;a separate unit
#s28	So in this case, a paper ID and each paper has its own context, it's independent.
#c28	this case;a paper ID;each paper;its own context;it
#s29	And.
#s30	But we can also treat all the papers written in 1998 as one group, and this is only possible because of the availability of time and we can partition data in this way.
#c30	we;all the papers;one group;the availability;time;we;data;this way
#s31	This would allow us to compare topics, for example in different years.
#c31	us;topics;example;different years
#s32	Similarly, we can partition the data based on the venues.
#c32	we;the data;the venues
#s33	We can get all the SIGIR papers and compare those papers with the rest or compare SIGIR papers with KDD papers with ACL papers.
#c33	We;all the SIGIR papers;those papers;the rest;SIGIR papers;KDD papers;ACL papers
#s34	We can also partition the data to obtain the papers written by authors in the US, and that of course uses additional context.
#c34	We;the data;the papers;authors;the US;course;additional context
#s35	of the authors and this would allow us to then compare such a subset with another set of papers written by authors in other countries.
#c35	the authors;us;such a subset;another set;papers;authors;other countries
#s36	Or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic.
#c36	we;a set;papers;the text mining;papers;another topic
#s37	topic.
#c37	topic
#s38	And note that these partitioning can be also intersect with each other to generate even more complicated partitions.
#c38	these partitioning;even more complicated partitions
#s39	And so in general, this enables discovery of knowledge associated with different context as needed.
#c39	discovery;knowledge;different context
#s40	And in particular, we can compare different contexts, and this often gives us a lot of useful knowledge.
#c40	we;different contexts;us;a lot;useful knowledge
#s41	For example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts.
#c41	example;topics;overtime;we;trends;topics;topics;different context;differences;the two contexts
#s42	So there are many interesting questions that require contextual text mining here, I list some very specific ones.
#c42	many interesting questions;contextual text mining;I;some very specific ones
#s43	For example, what topics have been gaining increasing attention recently in data mining research?
#c43	example;what;topics;increasing attention;data mining research
#s44	Now to answer this question, obviously we need to analyze text in the context of time.
#c44	this question;we;text;the context;time
#s45	So time is a context in this case.
#c45	time;a context;this case
#s46	Is there any difference in the responses of people in different regions to the event, to any event?
#c46	any difference;the responses;people;different regions;the event;any event
#s47	So this is a very broad analysis question, in this case, of course, location is the context.
#c47	a very broad analysis question;this case;course;location;the context
#s48	What are the common research interests of two researchers?
#c48	What;the common research interests;two researchers
#s49	In this case, authors can be the context.
#c49	this case;authors;the context
#s50	Is there any difference in the research topics published by authors in the USA and those outside?
#c50	any difference;the research topics;authors;the USA
#s51	Now, in this case, the context would include the authors and their affiliation and location.
#c51	this case;the context;the authors;their affiliation;location
#s52	So this goes beyond just the author himself or herself.
#c52	just the author;himself;herself
#s53	We need to look at the additional information connected to the author.
#c53	We;the additional information;the author
#s54	Is there any difference in the opinions about the topic expressed on one social network and another?
#c54	any difference;the opinions;the topic;one social network
#s55	In this case, the social network of authors and the topic can be the context.
#c55	this case;the social network;authors;the topic;the context
#s56	Are there topics in news data that are correlated with sudden changes in stock prices?
#c56	topics;news data;sudden changes;stock prices
#s57	In this case, we can use a time series such as stock prices as context.
#c57	this case;we;a time series;stock prices;context
#s58	What issues mattered in the 2012 presidential campaign or presidential election?
#c58	What issues;the 2012 presidential campaign;presidential election
#s59	Now in this case, time series again as context.
#c59	this case;context
#s60	df
#c60	df
#s61	So, as you can see, the list can go on and on, basically contextual text mining can have many applications.
#c61	you;the list;basically contextual text mining;many applications
410	99be14e5-691b-4843-8574-4e424e1d8c41	19
#s1	this letter is about the recommender systems so so far we have talked about a lot of aspects of search engines and we have talked about the problem of search and the ranking problem different methods for angee implementation of search engine and how to evaluate the search engine etc this was a positive cause we know that web search engines are by far the most important application itself text retrieval and they are the most useful tools to help people convert the big raw test data into small set of random the documents another reason why we spend the so many actors on search engines is becaus many techniques used in search engines actually also very useful for recommender systems which is the topic of this left him and so overall the two systems are actually well connected and there are many techniques that are shared by them so this is a slide that you have seen before when we talked about the two different modes of text for access poor and push an we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take the initiative to recommend the information to user order pushes a relevant information to the user and this often works well when the user has a relatively stable information data when the system has a good knowledge about the what the user wants so a recommender system is sometimes called a filtering system and its becaus recommending useful items to people is like a discarding or filtering out the users of course and so in this sense they are kind of similar and in all the cases the system must make a binary decision and usually there is a dynamic source of information items and you have some knowledge about the user 's interest and then the system would make a deliberate decision whether this item is interesting through the user and then if it is interesting than the system would recommend the article to the user so the basic of filtering question here is really weird this user like this item where you like item X and there are two ways to answer this question if you think about it i wanted to look at one items you likes
#c1	this letter;the recommender systems;we;a lot;aspects;search engines;we;the problem;search;the ranking problem different methods;angee implementation;search engine;the search engine;a positive cause;we;web search engines;the most important application;itself;text retrieval;they;the most useful tools;people;the big raw test data;small set;the documents;we;the so many actors;search engines;becaus;many techniques;search engines;recommender systems;the topic;him;the two systems;many techniques;them;a slide;you;we;the two different modes;text;access;we;recommender systems;the main systems;users;the push mode;the systems;the initiative;the information;user order;a relevant information;the user;the user;a relatively stable information data;the system;a good knowledge;what;the user;a recommender system;a filtering system;its becaus;useful items;people;a discarding;the users;course;this sense;they;all the cases;the system;a binary decision;a dynamic source;information items;you;some knowledge;the user 's interest;the system;a deliberate decision;this item;the user;it;the system;the article;the user;the basic;filtering question;this item;you;item X;two ways;this question;you;it;i;one items;you
#s2	and then we can see if X is actually like those items the other is to look at the whole like sex and we can see if this user looks like one of those users or like most of those users and these strategies can be combined if we follow the first strategy it had to look at item similarity in the case of recommending text objects then we are talking about the content based filtering or content based recommendation if we look at the second strategy then it will compare users and in this case we are exploiting user similarity and the technique is often called collaborative filtering
#c2	we;X;those items;the whole;sex;we;this user;those users;those users;these strategies;we;the first strategy;it;item similarity;the case;recommending text objects;we;the content based filtering;recommendation;we;the second strategy;it;users;this case;we;user similarity;the technique;collaborative filtering
#s3	so let's first look at the content based filtering system this is what the system would look like inside the system there will be a binary classifier that would have some knowledge about the user 's interest
#c3	's;the content based filtering system;what;the system;the system;a binary classifier;some knowledge;the user 's interest
#s4	and it's called a user interest profile it maintains this profile to keep track of the users interest and then there was a utility function to guide the user to make decisions and i explained that utility function in a moment it helps system decide where to set the threshold and then the accepted documents will be those that have passed the threshold according to the classifier there should be also in initialization module that would take a users input maybe from a users specified keywords or choosing category etc and this will be to feed the system with the initial user profile there is also typically a learning module that will learn from users feedback overtime now note that in this case typical to use this information either is stable so the system would have a lot of opportunities to observe the users if the user has taken recommended item has viewed that and this is the signal to indicate that the redmond item may be relevant if we use discarded relevant and so such feedback can be a long-term feedback and you can ask for a long time and the system can clock collect a lot of information about the user 's interest and this can then be used to improve the classified now what's criterion for evaluating such a system how do we know this filtering system actually performs well now in this case we cannot use the ranking evaluation measures like a map because we can afford a waiting for a lot of documents and then rank the documents to make the decision for the user and so the system must make a decision in real time in general to decide whether the item is above the threshold or not so in other words we are trying to decide absolutely relevance so in this case one commonly used strategies you use a utility function to evaluate the system so here i show linear utility function that's defined as for example three multiplied by the number of good items that you delivered minus two multiplied by the number of data items that you deliver so in other words we could kind of just treat this as almost in a gambling game if you delete you deliver one good item let's say you win three dollars you gain three down three dollars but if you believe are a bad awhile and you will lose two dollars and this utility function basically kind of meshes how much money you get by doing this kind of game and so it's clear that if you want to maximize this utility function you strategy should be to deliver as many good articles as possible and minimize the delivery of better articles that's obvious now interesting question here is how should we set these coefficients now i just show the three and negative two as the post local efficient but one can ask the question are they reasonable
#c4	it;a user interest profile;it;this profile;track;the users interest;a utility function;the user;decisions;i;that utility function;a moment;it;system;the threshold;the accepted documents;the threshold;the classifier;initialization module;a users;input;a users;keywords;category;the system;the initial user profile;a learning module;users;feedback overtime;this case;this information;the system;a lot;opportunities;the users;the user;recommended item;the signal;the redmond item;we;such feedback;a long-term feedback;you;a long time;the system;a lot;information;the user 's interest;what;criterion;such a system;we;this filtering system;this case;we;the ranking evaluation measures;a map;we;a waiting;a lot;documents;the documents;the decision;the user;the system;a decision;real time;the item;the threshold;other words;we;absolutely relevance;this case;one commonly used strategies;you;a utility function;the system;i;linear utility function;example;the number;good items;you;the number;data items;you;other words;we;a gambling game;you;you;one good item;'s;you;three dollars;you;three down three dollars;you;you;two dollars;this utility function;how much money;you;this kind;game;it;you;this utility function;as many good articles;the delivery;better articles;obvious now interesting question;we;these coefficients;i;one;the question;they
#s5	so what do you think do you think that that's a reasonable choice what about the other choices also for example we can have ten and minus one or one minus ten what's the difference what do you think how would this utility function affected systems threshold decision but you can think of these two extreme cases ten minus one verses one minus ten which one do you think it would encourage the system to over the lid of which you are with encourages system to be conservative if you think about that they will see that when we get a big award for delivering a good document you incur only a small penalty for delivering a better one intuitively you would be encouraged to deliver more right and you can try to deliver more in hope of getting a good one delivered
#c5	what;you;you;a reasonable choice;what;the other choices;example;we;what;the difference;what;you;this utility function;systems threshold decision;you;these two extreme cases;which one;you;it;the system;the lid;you;encourages system;you;they;we;a big award;a good document;you;only a small penalty;you;you;hope;a good one
#s6	and then we'll get a big award so on the one hand if you choose one minus ten you don't really get such a big price if you deliver deliver good document on the on hannah you will have a big los if you deliver better while you can imagine that the system would be very reluctant that you deliver a lot of documents it has to be absolutely sure that it's not non rather than the one so this utility function has to be designed based on specific application the three basic problems in content based filtering the forum first it has to make a filtering decision so it has to be a binary decision maker a binary classifier given a text text document and a profile description of the user it has to say yes or no whether this document should be delivered or not so that's a decision module
#c6	we;a big award;the one hand;you;you;such a big price;you;good document;hannah;you;a big los;you;you;the system;you;a lot;documents;it;it;this utility function;specific application;the three basic problems;the forum;it;a filtering decision;it;a binary decision maker;a binary classifier;a text text document;a profile description;the user;it;this document;a decision module
#s7	and they should be an initialization module as you have seen earlier and this is to get the system started and we have to initialize the system based on only very limited text description or very few examples from the user and the third component is a learning module which you have to be able to learn from limited at relevance judgments becaus we county learn from the user about their preferences on the delivery documents if we don't deliver document of the user would never know would never be able to know whether the user likes it or not we can accumulate a lot of documents from the entire history and all these modules would have to be optimized to maximize the utility so how can we be over such a system and there are many different approaches here we're going to talk about how to extend retrieval system a search engine for information filtering again here's why we've spent a lot of time to talk about the search engines becaus
#c7	they;an initialization module;you;the system;we;the system;only very limited text description;very few examples;the user;the third component;a learning module;you;relevance judgments becaus;we;the user;their preferences;the delivery documents;we;document;the user;the user;it;we;a lot;documents;the entire history;all these modules;the utility;we;such a system;many different approaches;we;retrieval system;a search engine;we;a lot;time;the search engines becaus
#s8	it's actually not very hard to extend the search engine for information filtering so here's the basic idea for extending a retrieval system for information filtering first we can reuse a lot of retrieval techniques to do scoring so we know how to score documents against queries etc measure similarity between profile text description and document
#c8	it;the search engine;information filtering;the basic idea;a retrieval system;information filtering;we;a lot;retrieval techniques;scoring;we;documents;queries;etc measure similarity;profile text description;document
#s9	and then we can use a score threshold for the filter indecision we do retrieval
#c9	we;a score threshold;the filter indecision;we;retrieval
#s10	and then we kind of find the scores of documents and then apply a threshold to say to see whether document is passing the threshold or not if it's passing the threshold are going to say it's relevant and we're going to deliver it to the user an another component the width added is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring and we know rock hill can be using for scoring improvement
#c10	we;the scores;documents;a threshold;document;the threshold;it;the threshold;it;we;it;the user;an another component;the width;course;the history;we;the traditional feedback techniques;scoring;we;rock hill;scoring improvement
#s11	hi ann
#s12	but we have to develop a new approaches to learn how to set the threshold
#c12	we;a new approaches;the threshold
#s13	and we need to set it to initially
#c13	we;it
#s14	and then we have to learn how to update the threshold overtime
#c14	we;the threshold overtime
#s15	so here's what the system might look like if we just generalize the vector space model for filtering problems so you can see the document of actor could be fed into a scoring module which is already exists in search engine that implements in fact space model and the profile will be treated as a query essentially and then the profile back that can be matched with the document vector to generate the score and then this score would be fed into a threshold in module that would say yes or no
#c15	what;the system;we;the vector space model;filtering problems;you;the document;actor;a scoring module;search engine;fact space model;the profile;a query;the document vector;the score;this score;a threshold;module
#s16	and then the evaluation would be based on utility for the filtering results if it says
#c16	the evaluation;utility;the filtering results;it
#s17	yes
#s18	and then the document would be sent into the user
#c18	the document;the user
#s19	and then the user could give some feedback and the feedback information would have been would be used to both adjust to the threshold and to adjust to the vector representation so the vector learning is essentially the same as query modification or feedback in the case of search the threshold are learning is new component and that we need to talk a little bit more about
#c19	the user;some feedback;the feedback information;the threshold;the vector representation;the vector learning;query modification;feedback;the case;search;the threshold;new component;we
410	9a443634-7f2e-4d3a-9ccd-0f1b6604c939	133
#s1	In this lecture, we continue discussing paradigmatic relation discovery.
#c1	this lecture;we;paradigmatic relation discovery
#s2	Earlier, we introduced a method called expected overlap of words in context.
#c2	we;a method;expected overlap;words;context
#s3	In this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the DOT product.
#c3	this method;we;each context;a word;vector;the probability;word;the context;we;the similarity;the DOT product
#s4	Which can be interpreted as the probability that to randomly pick the words from the two contexts  are identical, we also discuss the two problems of this method.
#c4	the probability;the words;the two contexts;we;the two problems;this method
#s5	The first is that it favors matching one frequent term very well over matching more distinct terms.
#c5	it;one frequent term;more distinct terms
#s6	It put too much emphasis on matching one term very well.
#c6	It;too much emphasis;one term
#s7	The second is that it treats every word equally.
#c7	it;every word
#s8	Even a common word like 'the' would contribute equally as content word like 'eats'.
#c8	Even a common word;content word
#s9	So now we are going to talk about how to solve these problems.
#c9	we;these problems
#s10	Most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector.
#c10	we;some retrieval heuristics;text retrieval;these heuristics;these problems;these problems;text retrieval;we;a query vector;document vector
#s11	So to address the first problem, we can use a sub linear transformation of term frequency.
#c11	the first problem;we;a sub linear transformation;term frequency
#s12	That is, we don't have to use the raw frequency count of term to represent the context.
#c12	we;the raw frequency count;term;the context
#s13	We can transform it into some form that wouldn't emphasize so much on the raw frequency.
#c13	We;it;some form;the raw frequency
#s14	To address the second problem, we can put more weight on rare terms.
#c14	the second problem;we;more weight;rare terms
#s15	That is, we can reward matching a rare word and this heuristic is called IDF term weighting in text retrieval.
#c15	we;a rare word;this heuristic;IDF term;text retrieval
#s16	IDF stands for inverse document frequency.
#c16	IDF;inverse document frequency
#s17	So now we're going to talk about the two heuristics in more detail.
#c17	we;the two heuristics;more detail
#s18	First, let's talk about the TF transformation.
#c18	's;the TF transformation
#s19	That is, to convert the raw count of word in the document into some weight that reflects our belief about how important this word in the document.
#c19	the raw count;word;the document;some weight;our belief;how important this word;the document
#s20	And so that will be denoted by TF of W&D as shown in the Y axis.
#c20	TF;W&D;the Y axis
#s21	Now in general there are many ways to map that, and let's first look at the simple way of mapping.
#c21	many ways;'s;the simple way;mapping
#s22	In this case, we're going to say, any non zero counts will be mapped to one.
#c22	this case;we;any non zero counts
#s23	And then zero count will be mapped to 0.
#c23	zero count
#s24	So with this mapping, all the frequencies will be mapped to only two values, zero or one, and the mapping function is shown here as a flat line here.
#c24	this mapping;all the frequencies;only two values;the mapping function;a flat line
#s25	Now this is naive because it ignored the frequency of words.
#c25	it;the frequency;words
#s26	However, this actually has the advantage of emphasizing matching all the words in the context, so it does not allow a frequent word to dominate the matching.
#c26	the advantage;all the words;the context;it;a frequent word;the matching
#s27	Now the approach that we have taken earlier in the expected overlap account approach is a linear transformation.
#c27	the approach;we;the expected overlap account approach;a linear transformation
#s28	We basically take Y as the same as X.
#c28	We;Y;X.
#s29	So we use the raw count as representation.
#c29	we;the raw count;representation
#s30	And that created the problem that we just talked about.
#c30	the problem;we
#s31	Namely it answers too much on just matching one frequent term.
#c31	it;one frequent term
#s32	Matching one frequent term can contribute a lot.
#c32	one frequent term;a lot
#s33	So we can have a lot of other interesting transformations in between the two extremes.
#c33	we;a lot;other interesting transformations;the two extremes
#s34	And they generally form a sub linear transformation.
#c34	they;a sub linear transformation
#s35	So for example, one possibility is to take logarithm of the raw count, and this will give us curve that looks like this, right?
#c35	example;one possibility;logarithm;the raw count;us;curve
#s36	That you're seeing here.
#c36	you
#s37	In this case, you can see the high frequency counts, the high counts are penalized a little bit right?
#c37	this case;you;the high frequency counts;the high counts
#s38	So the curve is a sub linear curve, an it brings down the weight of really those really high counts.
#c38	the curve;a sub linear curve;it;the weight;really those really high counts
#s39	And this is what we want, because it prevents that kind of terms from dominating the scoring function.
#c39	what;we;it;that kind;terms;the scoring function
#s40	Now there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that.
#c40	another interesting transformation;a BM25 transformation;retrieval;this transformation;we;a form
#s41	Looks like this.
#s42	I saw it's (K + 1)
#c42	I;it;+
#s43	* X /( X + K) where K is a parameter.
#c43	* X;+;K;a parameter
#s44	X is the count, the raw count of word.
#c44	X;the count;the raw count;word
#s45	Now the transformation is very interesting in that it can actually kind of go from one extreme to the other extreme by varying K.
#c45	the transformation;it;one extreme;the other extreme;K.
#s46	And it also is interesting that it has upper bound K +1 in this case.
#c46	it;it;K;this case
#s47	So this puts a very strict constraint on high frequency terms, because their weight would never exceed K+1.
#c47	a very strict constraint;high frequency terms;their weight;K+1
#s48	As we vary K, if we can simulate the two extremes.
#c48	we;K;we;the two extremes
#s49	So one case is set to zero.
#c49	one case
#s50	We roughly have the 01 vector.
#c50	We;the 01 vector
#s51	Whereas when we set the key to a very large value, it would behave more like the linear transformation.
#c51	we;the key;a very large value;it;the linear transformation
#s52	So this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up.
#c52	this transformation function;the most effective transformation function;text retrieval;it;sense;our problem
#s53	So we just talk about how to solve the problem of over emphasizing a frequently frequent term.
#c53	we;the problem;a frequently frequent term
#s54	Now let's look at the second problem, and that is how we can penalize popular terms.
#c54	's;the second problem;we;popular terms
#s55	Matching 'the' is not surprising because 'the' occurs everywhere, but matching 'eats' will account a lot.
#s56	So how can we address that problem?
#c56	we;that problem
#s57	In this case we can use the  IDF weighting that's commonly used in retrieval.
#c57	this case;we;the  IDF weighting;retrieval
#s58	IDF stands for inverse document frequency.
#c58	IDF;inverse document frequency
#s59	Document frequency means the count of the total number of documents that contain a particular word.
#c59	Document frequency;the count;the total number;documents;a particular word
#s60	So here we show that the IDF measure is defined as a logarithm function of the number of documents that match the term, or document frequency.
#c60	we;the IDF measure;a logarithm function;the number;documents;the term;document frequency
#s61	So K is the number of documents containing word or document frequency and M here is the total number of documents in the collection.
#c61	K;the number;documents;word;document frequency;M;the total number;documents;the collection
#s62	The IDF function is giving a higher value for a lower K, meaning that it rewards a rare term.
#c62	The IDF function;a higher value;a lower K;it;a rare term
#s63	And the maximum value is log of M + 1.
#c63	the maximum value;log;M
#s64	That's when the word occurs just once in the context.
#c64	the word;the context
#s65	So that's a very rare term, the rarest term in the whole collection.
#c65	a very rare term;the rarest term;the whole collection
#s66	The lowest value you can see here is when K reaches its maximum, which would be M. That would be a very low value close to 0 in fact.
#c66	The lowest value;you;K;its maximum;a very low value;fact
#s67	Right so this.
#s68	This of course measure is used in search where we naturally have a collection.
#c68	course;search;we;a collection
#s69	In our case, what will be our collection?
#c69	our case;what;our collection
#s70	We can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low IDF.
#c70	We;the context;we;all the words;our collection;a word;the collection;a low IDF
#s71	Because depending on the data set, we can construct the context vectors in different ways, but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected context documents.
#c71	the data;we;the context vectors;different ways;the end;a term;the original data set;it;the collected context documents
#s72	So how can we add these heuristics to improve our.....
#c72	we;these heuristics;our
#s73	Our similarity function.
#c73	Our similarity function
#s74	Here's one way, and there are many other ways that are possible.
#c74	one way;many other ways
#s75	But this is a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining.
#c75	a reasonable way;we;the BM25 retrieval model;paradigmatic relation mining
#s76	So here we define in this case we define the document vector.
#c76	we;this case;we;the document vector
#s77	As containing elements representing normalized BM 25 values.
#c77	elements;normalized BM 25 values
#s78	So in this normalization function we see we take sum over some of all the words and then we normalize the weight of each word by the sum of the weights of  all the words.
#c78	this normalization function;we;we;sum;all the words;we;the weight;each word;the sum;the weights;all the words
#s79	This is to again ensure all the x(i) will sum to one in this vector.
#c79	all the x(i;this vector
#s80	So this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one.
#c80	what;we;this vector;something;distribution;the exercise;sum
#s81	Now the weight of BM25 for each word is defined here.
#c81	the weight;BM25;each word
#s82	And if you compare this with our old definition where we just have a normalized count.
#c82	you;our old definition;we;a normalized count
#s83	On this one, right?
#c83	this one
#s84	So we only have this one and the document length or the total count of words in that context document.
#c84	we;the document length;the total count;words;that context document
#s85	And that's what we had before.
#c85	what;we
#s86	But now with the BM 25 transformation, we introduced something else.
#c86	the BM 25 transformation;we;something
#s87	First, of course, this extra occurrence of this count is just to achieve the sub linear normalization.
#c87	course;this extra occurrence;this count;the sub linear normalization
#s88	But we also see we introduce the parameter K here.
#c88	we;we;the parameter K
#s89	And this parameter is generally non negetive number, although zero is also possible.
#c89	this parameter;non negetive number
#s90	This controls the upper bound and the kinds controls can choose To what extent is simulates the linear transformation.
#c90	the kinds;controls;what extent;the linear transformation
#s91	And so this is 1 parameter.
#c91	1 parameter
#s92	But we also see there is another parameter here b and this will be within zero and one.
#c92	we;another parameter;b
#s93	And this is a parameter to control  length normalization.
#c93	a parameter;  length normalization
#s94	And, and in this case the normalizing formula has average document length here.
#c94	this case;the normalizing formula;average document length
#s95	And this is the computed by taking the average of the lengths of all the documents in the collection.
#c95	the average;the lengths;all the documents;the collection
#s96	In this case, all the lengths of all the context documents that we are considering.
#c96	this case;all the context documents;we
#s97	So this average documents will be a constant for any given collection, so it actually is only affecting the effect of the parameter B here.
#c97	this average documents;a constant;any given collection;it;the effect;the parameter B
#s98	Because this is a constant.
#s99	But I kept it here because it's constant that's useful in retrieval, where it would give us a stabilized interpretation of parameter b.
#c99	I;it;it;retrieval;it;us;a stabilized interpretation;parameter;b.
#s100	But for our purpose this will be a constant, so it would only be affecting the length formalization together with parameter B. Now with this definition, then we have a new way to define our document vectors and we can compute the vector D2 in the same way.
#c100	our purpose;it;the length formalization;parameter B. Now;this definition;we;a new way;our document vectors;we;the vector D2;the same way
#s101	The difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms.
#c101	The difference;the high frequency terms;a somewhat lower weights;the influence;these high frequency terms
#s102	Now the IDF can be added here in the scoring function.
#c102	the IDF;the scoring function
#s103	That means we'll introduce weight for matching each term.
#c103	we;weight;each term
#s104	So you may recall this sum indicates all the possible words that can be a overlap between the two contexts.
#c104	you;this sum;all the possible words;a overlap;the two contexts
#s105	And the Xi and Yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word.
#c105	the Xi and Yi probabilities;the word;both contexts;it;a match;this word
#s106	Now IDF would give us the importance of matching this word.
#c106	IDF;us;the importance;this word
#s107	A common word will be worth less than rare word, so we emphasize more on matching rare words now.
#c107	A common word;rare word;we;rare words
#s108	So with this modification, then the new function will likely address those two problems.
#c108	this modification;the new function;those two problems
#s109	Now interestingly we can also use this approach to discover syntagmatic relations.
#c109	we;this approach;syntagmatic relations
#s110	In general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context.
#c110	we;a term vector;the sorry;context;the term vector;we;some terms;higher weights;other terms;lower weights;we;weights;these terms;we;these weights;the words;the candidate word;the context
#s111	So let's take a look at the term vector in more detail here.
#c111	's;a look;the term vector;more detail
#s112	And we have each Xi, defined as a normalized weight of BM 25.
#c112	we;each Xi;a normalized weight;BM
#s113	Now this weight alone only reflects how frequently the word occurs in the context.
#c113	this weight;the word;the context
#s114	But we can't just say any frequent term in the context that would be correlated with the candidate word.
#c114	we;any frequent term;the context;the candidate word
#s115	Because many common words like 'the' will occur frequently in all the context.
#c115	many common words;all the context
#s116	But if we apply IDF weighting as you see here, we can then we weight these terms based on IDF That means the words that are common, like 'the' will get penalized.
#c116	we;IDF weighting;you;we;we;these terms;IDF;the words
#s117	So now the highest weighted terms will not be those common terms because they have lower IDFs.
#c117	the highest weighted terms;those common terms;they;lower IDFs
#s118	Instead, those terms would be the terms that are frequent in the context, but not frequently in the collection.
#c118	those terms;the terms;the context;the collection
#s119	So those are clearly the words that tend to occur in the context of the candidate word, for example, cat.
#c119	the words;the context;the candidate word;example;cat
#s120	So for this reason, the highly weighted terms in this IDF weighted vector can also be assumed to be candidate for Syntagmatic relations.
#c120	this reason;the highly weighted terms;this IDF;vector;candidate;Syntagmatic relations
#s121	Now of course, this is only a bi-product of our approach for discovering paradigmatic relations.
#c121	course;only a bi;-;product;our approach;paradigmatic relations
#s122	And in the next lecture, we're going to talk more about how to discover Syntagmatic relations.
#c122	the next lecture;we;Syntagmatic relations
#s123	But it clearly shows the relation between discovering the two relations.
#c123	it;the relation;the two relations
#s124	And indeed they can be discussed, discovered in a joint manner by leveraging such associations.
#c124	they;a joint manner;such associations
#s125	So to summarize, the main idea for discovering  paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words.
#c125	the main idea;  paradigmatic relations;the context;a candidate word;a pseudo document;a bag;words
#s126	And then compute the similarity of the corresponding context documents of two candidate words.
#c126	the similarity;the corresponding context documents;two candidate words
#s127	An then we can take the highly similar word pairs and treat them as having paradigmatic relations.
#c127	we;the highly similar word pairs;them;paradigmatic relations
#s128	These are the words that share similar context.
#c128	the words;similar context
#s129	And there are many different ways to implement this general idea
#c129	many different ways;this general idea
#s130	and we just talk about some of the approaches.
#c130	we;the approaches
#s131	And more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations.
#c131	we;text retrieval models;us;effective similarity function;the paradigmatic relations
#s132	More specifically, we have used the  BM25 and IDF weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques.
#c132	we;the  BM25 and IDF weighting;paradigmatic relation;these approaches;the state;the art;text retrieval techniques
#s133	Finally, Syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.
#c133	Syntagmatic relations;a bi;-;product;we;paradigmatic relations
410	9b2615a8-42c5-485d-a616-fb8dba74a888	117
#s1	This lecture is about the basic measures for evaluation of text retrieval systems.
#c1	This lecture;the basic measures;evaluation;text retrieval systems
#s2	In this lecture we're going to discuss how we design basic measures.
#c2	this lecture;we;we;basic measures
#s3	To quantitatively compared to retrieval systems.
#c3	retrieval systems
#s4	This is a slide that you have seen earlier in the lecture where we talked about.
#c4	a slide;you;the lecture;we
#s5	Cranfield evaluation methodology.
#c5	Cranfield evaluation methodology
#s6	We can have a test collection that consists of queries, documents and relevance judgments.
#c6	We;a test collection;queries;documents;relevance judgments
#s7	We can then run two systems on these datasets to quantitatively evaluate their performance.
#c7	We;two systems;these datasets;their performance
#s8	And we raised the question about which set of results is better.
#c8	we;the question;which set;results
#s9	Is system a better or system B better?
#c9	system
#s10	So let's now talk about how to actually quantify their performance.
#c10	's;their performance
#s11	Suppose we have a total of 10 relevant documents in the collection for this query.
#c11	we;a total;10 relevant documents;the collection;this query
#s12	Now the relevance judgments shown on the right.
#c12	Now the relevance judgments;the right
#s13	Did not include all the ten, obviously and we have only seen three relevant documents there, but we can imagine there are other relevant documents in judge before this query.
#c13	we;three relevant documents;we;other relevant documents;judge;this query
#s14	So now intuitively we thought that system A is better because it did not have much noise, and in particular we have seen that among the three results, two of them are relevant.
#c14	we;system A;it;much noise;we;the three results;them
#s15	But in system B. We have 5 results and only three of them are relevant.
#c15	system;We;5 results;them
#s16	So intuitively it looks like system A is more accurate and this intuition can be captured by a measure called precision where we simply compute: to what extent, all the retrieval results are relevant if you have 100% precision, that would mean all the retrieval documents are relevant.
#c16	it;system A;this intuition;a measure;precision;we;what extent;all the retrieval results;you;100% precision;all the retrieval documents
#s17	So in this case the system A has a precision of two out of three, system B has 3 / 5.
#c17	this case;A;a precision;system B
#s18	And this shows that system A is better by precision.
#c18	that system;precision
#s19	But we also talked about System B might be preferred by some other users who like to retrieve as many relevant documents as possible.
#c19	we;System B;some other users;who;as many relevant documents
#s20	So in that case will have to compare the number of relevant documents, then retrieve and there is another measure called recall.
#c20	that case;the number;relevant documents;another measure;recall
#s21	This measures the completeness of coverage of relevant documents in your retrieval.
#c21	the completeness;coverage;relevant documents;your retrieval
#s22	Result, so we just assume that there are 10 relevant documents in the collection.
#c22	we;10 relevant documents;the collection
#s23	An here we've got two of them in system A, so the record is 2 out of 10 Whereas System B has got three.
#c23	we;them;system A;the record;2 out of 10 Whereas System B
#s24	So it's a 3 out of 10.
#c24	it
#s25	Now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines, and they are very important because they also widely used in.
#c25	we;recall system B;these two measures;the very basic measures;search engines;they;they
#s26	Many other task evaluation problems, for example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported for all kinds of tasks.
#c26	Many other task evaluation problems;example;you;the applications;machine learning;you;precision recall numbers;all kinds;tasks
#s27	OK, so now let's define these two measures more precisely, and these measures are to evaluate a set of retrieval documents.
#c27	's;these two measures;these measures;a set;retrieval documents
#s28	So that means we are considering that approximation of the set of relevant documents.
#c28	we;that approximation;the set;relevant documents
#s29	We can distinguish it into 4 cases depending on the situation of the document.
#c29	We;it;4 cases;the situation;the document
#s30	A document that can be retrieved or not retrieved, right?
#c30	A document
#s31	Because we're talking about the set of results.
#c31	we;the set;results
#s32	A document can be also relevant or non relevant depending on whether the user thinks this is useful document.
#c32	A document;the user;useful document
#s33	So we can now have counts of documents in each of the four categories.
#c33	we;counts;documents;the four categories
#s34	We can have A to represent the number of documents that are retrieved and relevant.
#c34	We;the number;documents
#s35	B for documents that are not retrieved but relevant, etc.
#c35	B;documents
#s36	Now with this table, then we could define precision as the.
#c36	this table;we;precision
#s37	Ratio of.
#c37	Ratio
#s38	The relevant retrieved documents A to the total number of retrieval documents, so this is just A divided by the sum of A&C. sum of this column.
#c38	The relevant retrieved documents;the total number;retrieval documents;the sum;A&C. sum;this column
#s39	Similarly, recall is defined by dividing A by the sum of A&B, so that's again the divide A by the sum of the rule instead of the column.
#c39	the sum;A&B;the divide;A;the sum;the rule;the column
#s40	Right, so we can see precision and recall is all focused on looking at A.
#c40	we;precision;recall;A.
#s41	That's the number of retrieval relevant documents.
#c41	the number;retrieval relevant documents
#s42	But we're going to use different denominators.
#c42	we;different denominators
#s43	OK, So what would be an ideal result?
#c43	what;an ideal result
#s44	You can easily see in the ideal case we have precision and recall, or to be 1.0 that means we have got 1% of all the relevant documents in our results and all the results that we return are relevant.
#c44	You;the ideal case;we;precision;we;1%;all the relevant documents;our results;all the results;we
#s45	At least there's no single nonrelevant document in return.
#c45	no single nonrelevant document;return
#s46	In reality, however, high record tends to be associated with low precision.
#c46	reality;high record;low precision
#s47	And you can imagine why that's the case as you go down the list to try to get as many relevant documents as possible, you tend to encounter a lot of non relevant documents, so the precision would go down.
#c47	you;the case;you;the list;as many relevant documents;you;a lot;non relevant documents;the precision
#s48	Note that this set can also be defined by a cut off in the ranked list.
#c48	this set;a cut;the ranked list
#s49	That's why although these two measures are defined for a set of retrieval documents, they are actually very useful for evaluating a ranked list.
#c49	these two measures;a set;retrieval documents;they;a ranked list
#s50	There are fundamental measures in text retrieval and many other tasks.
#c50	fundamental measures;text retrieval;many other tasks
#s51	We often are interested in the precision at 10 documents for web search.
#c51	We;the precision;10 documents;web search
#s52	This means we look at the how many documents among the top 10 results are actually relevant.
#c52	we;the how many documents;the top 10 results
#s53	Now this is a very meaningful measure because it tells us how many relevant documents the User can expect to see on the first page of search results where they typically show 10 results.
#c53	a very meaningful measure;it;us;how many relevant documents;the User;the first page;search results;they;10 results
#s54	So precision and recall are the basic measures and we need to use them to further evaluate search engine.
#c54	precision;recall;the basic measures;we;them;search engine
#s55	But there are the building blocks really.
#c55	the building blocks
#s56	We just said that there tends to be a tradeoff between precision and recall, so naturally it would be interesting to combine them.
#c56	We;a tradeoff;precision;it;them
#s57	And here's one measure that's often used called F measure.
#c57	one measure;F measure
#s58	And, it's a harmonic mean of precision and recall is defined on this slide.
#c58	it;a harmonic mean;precision;recall;this slide
#s59	So you can see.
#c59	you
#s60	It first.
#c60	It
#s61	Compute the.
#s62	Inverse of R and P here and then it would interpret the two by using the coefficients.
#c62	Inverse;R;P;it;the coefficients
#s63	Depending on a parameter beta.
#c63	a parameter beta
#s64	And after some transformation you can easily see it would be of this form.
#c64	some transformation;you;it;this form
#s65	And in any case, this is just a combination of precision and recall.
#c65	any case;just a combination;precision
#s66	Beta is a parameter that's often set to one.
#c66	Beta;a parameter
#s67	It can control the emphasis on precision or recall when we set Beta to one, we end up having a special case of F measure, often called F1.
#c67	It;the emphasis;precision;we;Beta;we;a special case;F measure;F1
#s68	This is a popular measure that's often used little combine precision and recall, and formula looks very simple.
#c68	a popular measure;little combine precision;formula
#s69	It's just this here.
#c69	It
#s70	Now it's easy to see that if you have a larger precision or larger recall than F measure would be high.
#c70	it;you;a larger precision;larger recall;F measure
#s71	But what's interesting is that.
#c71	what
#s72	The tradeoff between precision and recall is captured in the interesting way in F1.
#c72	The tradeoff;precision;recall;the interesting way;F1
#s73	So in order to understand that, we can first look at the natural question, why not just combine them using a simple arithmetic mean as shown here?
#c73	order;we;the natural question;them;a simple arithmetic
#s74	That would be likely the most natural way of combining them.
#c74	the most natural way;them
#s75	So what do you think?
#c75	what;you
#s76	If you want to think more, you can pause the video.
#c76	you;you;the video
#s77	So why is this not as good as F1?
#c77	F1
#s78	Or what's the problem with this?
#c78	what;the problem
#s79	Now.
#s80	If you think about the arithmetic mean, you can see this is the sum of multiple terms.
#c80	you;you;the sum;multiple terms
#s81	In this case is a sum of precision and recall.
#c81	this case;a sum;precision
#s82	In the case of a sum, the total value tends to be dominated by the large values.
#c82	the case;a sum;the total value;the large values
#s83	That means if you have a very high P or very high R, then you really don't care about the weather.
#c83	you;a very high P;R;you;the weather
#s84	The other value is low, so the whole sum would be high.
#c84	The other value;the whole sum
#s85	Now this is not desirable because one can easily have a perfect recall.
#c85	one;a perfect recall
#s86	We can have perfect recall easily.
#c86	We;perfect recall
#s87	Can you imagine how?
#c87	you
#s88	It's probably very easy to imagine that we simply retrieve all the document in the collection.
#c88	It;we;all the document;the collection
#s89	Then we have a perfect recall.
#c89	we;a perfect recall
#s90	And this will give us point of five as the average.
#c90	us;point;the average
#s91	But such results are clearly not very useful for users, even though the average using this formula would be relatively high.
#c91	such results;users;the average;this formula
#s92	But in contrast, you can see F1 would reward the case where precision and recall are roughly similar, so it would penalize a case where you have extremely high value for one of them.
#c92	contrast;you;F1;the case;precision;recall;it;a case;you;extremely high value;them
#s93	So this means F1 encodes different.
#c93	F1 encodes
#s94	 tradeoffs between them.
#c94	 tradeoffs;them
#s95	For this example, shows actually a very important methodology here.
#c95	this example;a very important methodology
#s96	When you try to solve a problem.
#c96	you;a problem
#s97	You might naturally think of 1 solution, let's say in this case it's this arithmetic mean.
#c97	You;1 solution;'s;this case;it
#s98	But it's important not to settle on this solution.
#c98	it;this solution
#s99	It's important to think whether you have other ways to combine them.
#c99	It;you;other ways;them
#s100	And once you think about multiple variants, it's important to analyze their difference.
#c100	you;multiple variants;it;their difference
#s101	And then think about which one makes more sense in this case.
#c101	one;more sense;this case
#s102	If you think more carefully, you will feel that F1 probably makes more sense than the simple arithmetic mean, although in other cases there may be different results, but in this case the arithmetic mean seems not reasonable.
#c102	you;you;F1;more sense;the simple arithmetic;other cases;different results;this case
#s103	But if you don't pay attention to these subtle differences, you might just take a easy way to combine them and then go ahead with it.
#c103	you;attention;these subtle differences;you;a easy way;them;it
#s104	And there later you will find that measure.
#c104	you;that measure
#s105	It doesn't seem to work well.
#c105	It
#s106	so at this methodology is actually very important in general in solving problem and try to think about the best solution.
#c106	this methodology;problem;the best solution
#s107	Try to understand the problem very well and then know why you need this measure and why you need to combine precision and recall and then use that to guide you in finding a good way to solve the problem.
#c107	the problem;you;this measure;you;precision;you;a good way;the problem
#s108	To summarize, we talked about precision.
#c108	we;precision
#s109	Which addresses the question.
#c109	the question
#s110	Are the retrieval results all relevant.
#s111	We also talked about the recall.
#c111	We;the recall
#s112	Which addresses the question have all the relevant documents being retrieved.
#c112	the question;all the relevant documents
#s113	These two are the two basic measures in text retrieval evaluation.
#c113	the two basic measures;text retrieval evaluation
#s114	They are useful for many other tasks as well.
#c114	They;many other tasks
#s115	We talked about F measure as a way to combine precision and recall.
#c115	We;F measure;a way;precision
#s116	We also talked about the tradeoff between precision and recall and.
#c116	We;the tradeoff;precision
#s117	This turns out to depend on the users search tasks and will discuss this point more in the later lecture.
#c117	the users search tasks;this point;the later lecture
410	9e5a0c5f-ff4a-42a6-b37b-f43628632860	99
#s1	This lecture is about the syntagmatic relation discovery and mutual information.
#c1	This lecture;the syntagmatic relation discovery;mutual information
#s2	In this lecture, we're going to continue discussing syntagmatic relation discovery.
#c2	this lecture;we;syntagmatic relation discovery
#s3	In particular, we're going to talk about another concept, the information theory, called mutual information.
#c3	we;another concept;the information theory;mutual information
#s4	And how it can be used to discover syntagmatic relations?
#c4	it;syntagmatic relations
#s5	Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus.
#c5	we;a problem;conditional entropy;the conditional entropy;different pairs;words;it;strong syntagmatic relations;corpus
#s6	So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make.
#c6	we;mutual information;another concept;information theory;us;some sense;the conditional entropy
#s7	a more comparable across different pairs.
#c7	different pairs
#s8	In particular, mutual information, denoted by I(X;Y), measures the entropy reduction of X obtained from knowing Y.
#c8	particular, mutual information;I(X;Y;the entropy reduction;X
#s9	More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y.
#c9	More specifically the question;we;how much reduction;the entropy;X;we;Y.
#s10	So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y.
#c10	it;the difference;the original entropy;X;the conditional entropy;X;Y.
#s11	And you might see here you can see here.
#c11	you;you
#s12	It can also be defined as a reduction of entropy of Y, because of knowing X. Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal.
#c12	It;a reduction;entropy;Y;X. Normally;H(Y|X
#s13	 
#s14	But interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by I here and this function has some interesting properties.
#c14	the reduction;entropy;them;this quantity;mutual information;I;this function;some interesting properties
#s15	First, it's also non negative.
#c15	it
#s16	This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy.
#c16	the original entropy;the possibly reduced conditional entropy
#s17	In other words, the conditional entropy would never exceed the original entropy.
#c17	other words;the conditional entropy;the original entropy
#s18	Knowing some information can always help us potentially, but won't hurt us in predicting X. The second property is that it's symmetric while conditional entropy is not symmetrical.
#c18	some information;us;us;X.;The second property;it;conditional entropy
#s19	Mutual information is.
#c19	Mutual information
#s20	The third property is that it reaches its minimum zero if and only if the two random variables are completely independent.
#c20	The third property;it;the two random variables
#s21	That means knowing one of them doesn't tell us anything about the other.
#c21	them;us;anything
#s22	And this last property can be verified by simply looking at the equation above.
#c22	this last property;the equation
#s23	And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X.
#c23	it;the conditional entropy;X;Y;original entropy;X.
#s24	So that means knowing why did not help at all, and that's when X&Y are completely independent.
#c24	X&Y
#s25	Now when we fix X to rank different
#c25	we;X
#s26	Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed.
#c26	conditional entropy;the same order;mutual information;the function;H;X;X
#s27	So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y.
#c27	mutual information;the conditional entropy;X;Y.
#s28	But the mutual information allows us to compare different pairs of X&Y, so that's why mutual information is more general and in general more useful.
#c28	the mutual information;us;different pairs;X&Y;mutual information
#s29	So let's examine them intuition of using mutual information for syntagmatic relation mining.
#c29	's;them;intuition;mutual information;syntagmatic relation mining
#s30	Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur?
#c30	the question;we;syntactic relation mining;eats;what other words
#s31	So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats.
#c31	this question;a mutual information question;higher mutual information;eats
#s32	So we're going to compute the mutual information between eats and other words.
#c32	we;the mutual information;eats;other words
#s33	And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related.
#c33	we;it;the same intuition;conditional entropy;we;words;high mutual information;words
#s34	We have lower mutual information, so this I give some example here.
#c34	We;lower mutual information;I;some example
#s35	The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the.
#c35	The mutual information;eats;meats;meats;major information;The mutual information;eats
#s36	Because knowing the doesn't really help us predict eats.
#c36	us;eats
#s37	Similarly knowing eats doesn't help us predicting the as well.
#c37	Similarly knowing eats;us
#s38	And you also can easily see that the mutual information between  a word and itself is the largest which is equal to the mutual info.
#c38	you;the mutual information;a word;itself;the mutual info
#s39	The entropy of this word.
#c39	The entropy;this word
#s40	So because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero.
#c40	this case;the reduction;us;the conditional entropy
#s41	Therefore the mutual information reaches its maximum.
#c41	the mutual information;its maximum
#s42	It's going to be larger than or equal to the mutual information between eats and another word.
#c42	It;the mutual information;eats;another word
#s43	In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself.
#c43	other words;any other word;mutual information;eats;that word;you;any mutual information;the mutual information;eats;itself
#s44	So now let's think about how to compute the mutual information.
#c44	's;the mutual information
#s45	Now, in order to do that, we often.
#c45	order;, we
#s46	use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance.
#c46	a different form;mutual information;we;the mutual information;the form;this slide;we;a formula;what;KL-divergences;callback labeler divergance
#s47	This is another term in information theory that measures the divergance between two distributions.
#c47	another term;information theory;the divergance;two distributions
#s48	Now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions.
#c48	you;the formula;it;sum;many combinations;different values;the two random variables;the sum;we;a comparison;2 joint distributions
#s49	The numerator has the joint actual observed.
#c49	The numerator
#s50	Join the distribution of the two random variables.
#c50	the distribution;the two random variables
#s51	The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables.
#c51	The bottom part;the denominator;the expected joint distribution;the two random variables
#s52	If there were independent.
#s53	Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities.
#c53	two random variables;they;distribution;the product;the two probabilities
#s54	So this comparison would tell us whether the two variables are indeed independent if there indeed independent, then we would expect that the two are the same.
#c54	this comparison;us;the two variables;we
#s55	But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association.
#c55	the numerator;the denominator;the two variables;the association
#s56	The sum is simply to take into consideration of all the combinations of the values of these two random variables.
#c56	The sum;consideration;all the combinations;the values;these two random variables
#s57	In our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here.
#c57	our case;each random variable;the two values;we;four combinations
#s58	So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption.
#c58	we;this form;mutual information;it;the mutual information;the diversions;the actual joint distribution;the expected distribution;the independence assumption
#s59	The larger this divergence is, the higher the mutual information would be.
#c59	The larger this divergence;the mutual information
#s60	So now let's further look at the what are exactly the probabilities involved in this formula of mutual information.
#c60	's;what;the probabilities;this formula;mutual information
#s61	And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word.
#c61	I;all the probabilities;it;you;we;first 2 probabilities;the presence;absence;each word
#s62	So for W1, we have two probabilities shown here.
#c62	W1;we;two probabilities
#s63	They should sum to 1 because a word can either be present or absent in the segment.
#c63	They;a word;the segment
#s64	And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well.
#c64	the second word;we;two probabilities;presence;absence;this word;this sums
#s65	And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words.
#c65	we;a lot;joint probabilities;the scenarios;Co;-;occurrences;the two words
#s66	And they are shown here.
#c66	they
#s67	Right, so this sums to 1 because the two words can only have these four possible scenarios.
#c67	the two words;these four possible scenarios
#s68	Either they both occur.
#c68	they
#s69	So in that case both variables will have a value of one or one of them occurs.
#c69	that case;both variables;a value;them
#s70	There are two scenarios.
#c70	two scenarios
#s71	In these two cases, one of the random variables will be equal to 1 and the other would be 0.
#c71	these two cases;the random variables
#s72	And finally we have the scenario when none of them occurs.
#c72	we;the scenario;none;them
#s73	So this is when the two variables taking a value of 0.
#c73	the two variables;a value
#s74	And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information.
#c74	they;the probabilities;the calculation;mutual information
#s75	here.
#s76	Once we know how to calculate these probabilities, we can easily calculate the mutual information.
#c76	we;these probabilities;we;the mutual information
#s77	It's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the marginal probabilities of these words sum to one, and we also have seen this constraint that says the two words can only have these four different scenarios of Co occurrences, but we also have some additional constraints listed in the bottom.
#c77	It;some relations;constraints;these probabilities;we;them;the previous slide;you;the marginal probabilities;these words;we;this constraint;the two words;these four different scenarios;Co occurrences;we;some additional constraints;the bottom
#s78	And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed.
#c78	example;this one;we;the probabilities;we;the two words;the probabilities;the word;the first word;the second word;we;exactly the probability;the first word
#s79	In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed.
#c79	other words;the word;the first word;only two scenarios;weather second word
#s80	So this probability captures the first scenario when the signal word actually is also observed.
#c80	this probability;the first scenario;the signal word
#s81	And this captures the second scenario when the seond word is not observed, so we only see the first word.
#c81	the second scenario;the seond word;we;the first word
#s82	And it's easy to see the other equations also follow the same reasoning.
#c82	it;the other equations;the same reasoning
#s83	Now these equations allow us to compute some probabilities based on other probabilities.
#c83	these equations;us;some probabilities;other probabilities
#s84	And this can simplify the computation.
#c84	the computation
#s85	So more specifically, and if we know the probability that a word is present, and in this case right?
#c85	we;the probability;a word;this case
#s86	So if we know this.
#c86	we
#s87	And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right?
#c87	we;the presence;the probability;presence;the second word;we;their absence probability
#s88	It's very easy to use this equation to do that.
#c88	It;this equation
#s89	An so we this will take care of the computation of these probabilities of presence or absence of each word.
#c89	so we;care;the computation;these probabilities;presence;absence;each word
#s90	Now let's look at their joint distribution, right?
#c90	's;their joint distribution
#s91	Let's assume that we also have available probability that they occur together.
#c91	's;we;available probability;they
#s92	Now it's easy to see that we can actually compute the all the rest of these probabilities based on these.
#c92	it;we;the all the rest;these probabilities
#s93	Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes.
#c93	example;this equation;we;the probability;the first word;the second word;we;these probabilities;the boxes
#s94	And similarly, using this equation we can compute the probability that we observe only the second word.
#c94	this equation;we;the probability;we;only the second word
#s95	And then finally we.
#c95	And then finally we
#s96	This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right?
#c96	This probability;this equation
#s97	So this can be easier to calculate.
#s98	Right, so now this can be calculated.
#s99	So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, namely the presence of each word and the Co occurrence of both words in a segment.
#c99	this slide;we;these three probabilities;the boxes;namely the presence;each word;the Co occurrence;both words;a segment
410	9faafef6-a1cb-4e0d-a06d-e134eaec84d9	19
#s1	in this lecture we're going to talk about how to instantiate vectors based model so that we can get very specific ranking function so this is to continue the discussion of the vectors based model which is one particular approach to design (test) ranking function and we're going to talk about how we use the general framework of the vectors based model as a guidance to instantiate the framework to derive a specifical ranking function and we're going to cover the simplest instantiation of the framework so as we discussed in the previous lecture the vectors based model is really a framework it didn't didn't say as we discussed in the previous lecture vector space model is really a framework it doesn't say many things so for example here it shows that it did not say how we should define the dimension it also did not say how we place a document a vector in this space he did not say how we place a query vector in this space and finally it did not say how we should measure the similarity between the query vector and the document vector so you can imagine in order to implement this model we have to say specifically how we compute these vectors what is exactly X I and what is exactly why i this would determine where we place a document about that where we place a query about them and of course we also need to say exactly what should be the similarity function so if we can provide a definition of the concepts that would define the dimensions and these excise or why eyes and then the weights of terms for query and document then we will be able to play stocking the raptors and query back in this where they find the space and then if we also specify similarity function then we'll have well defined the ranking function so let's see how we can do that and think about the simplest instantiation actually i would suggest you to pause the lecture at this point spend a couple of minutes it will think about it suppose you i ask the two implemented this idea you've come up with the idea of vector space model but you still haven't figured out how to compute these vectors exactly how to define the similarity function what would you do
#c1	this lecture;we;vectors;model;we;very specific ranking function;the discussion;the vectors;model;one particular approach;design;(test;ranking function;we;we;the general framework;the vectors;model;a guidance;the framework;a specifical ranking function;we;the simplest instantiation;the framework;we;the previous lecture;the vectors;model;a framework;it;we;the previous lecture vector space model;a framework;it;many things;example;it;it;we;the dimension;it;we;a document;a vector;this space;he;we;a query vector;this space;it;we;the similarity;the query vector;the document vector;you;order;this model;we;we;these vectors;what;X I;what;i;we;a document;we;a query;them;course;we;exactly what;the similarity function;we;a definition;the concepts;the dimensions;these excise;why eyes;then the weights;terms;query;document;we;the raptors;query;they;the space;we;similarity function;we;the ranking function;'s;we;the simplest instantiation;i;you;the lecture;this point;a couple;minutes;it;it;you;i;this idea;you;the idea;vector space model;you;these vectors;the similarity function;what;you
#s2	so i think for a couple of minutes and then proceed so let's think about some simple ways of instantiating this vector space model firstly how do we define dimension where the obvious choice is to use each word in our vocabulary the diviner dimension and who we sure that there are N words in our vocabulary therefore there RN dimensions each word defines one dimension and this is basically the bag of words with temptation now let's look at how we place vectors in this space again here the simplest strategy is to use a bit vector to represent both the query and a document and that means each element X
#c2	i;a couple;minutes;'s;some simple ways;this vector space model;we;dimension;the obvious choice;each word;our vocabulary;the diviner dimension;who;we;N words;our vocabulary;RN dimensions;each word;one dimension;the bag;words;temptation;'s;we;vectors;this space;the simplest strategy;a bit vector;both the query;a document;each element
#s3	I N Y I would be taking a value of either zero or one when it's one it means the corresponding world is present in the document or in query what's zero is going to mean that it's absolute so you can imagine if the user types in a few words in the query then the query back that we only have a few ones many many zeros the docking the vector general we have more ones of course but it will also have many zeros since the vocabulary is generally very large many words don't really occur in any document many words we only occasionally occur in the document a lot of words will be absent in a particular document so now we have placed the documents and the query in the vector space let's look at how we measure the similarity so a commonly used the similarity measure here is dot product the thought product of two vectors is simply defined as the some of the products of the corresponding elements of the two vectors
#c3	N Y;I;a value;it;it;the corresponding world;the document;query;what;it;you;a few words;the query;then the query;we;a few ones;many many zeros;the docking;the vector general;we;more ones;course;it;many zeros;the vocabulary;very large many words;any document;many words;we;the document;a lot;words;a particular document;we;the documents;the query;the vector space;'s;we;the similarity;the similarity measure;dot product;the thought product;two vectors;the products;the corresponding elements;the two vectors
#s4	so here we see that it's the product of X one and why one so here and then X two multiplied by Y two and then finally eggs an multiplied by Y N
#c4	we;it;the product;Y;Y N
#s5	and then we take a sum so that's the thought product now we can represent this in a more general way using a some here
#c5	we;a sum;the thought product;we;a more general way
#s6	so this is only one of the many different ways of measuring the similarity so now we see that we have defined the the dimensions
#c6	the many different ways;the similarity;we;we;the the dimensions
#s7	we have defined vectors an we have also define the similarity function so now we find that they have the simplest the vector space model which is based on the bit vector recommendation dot product similarity and bag of words recommendation and the formula looks like this
#c7	we;vectors;we;the similarity function;we;they;the vector space model;the bit vector recommendation dot product similarity;bag;words;recommendation;the formula
#s8	so this is our formula and that's actually a particular retrieval function a ranking function right now we can finally implemented this function using a programming language and then rank documents for query now at this point you should again pause the lecture to think about how we can interpret this school so we have gone through the process of modeling the retrieval problem using a vector space model
#c8	our formula;a particular retrieval function;a ranking function;we;this function;a programming language;documents;query;this point;you;the lecture;we;this school;we;the process;the retrieval problem;a vector space model
#s9	and then we make assumptions about how we place about this in the vector space and how we define the similarity so in the end that we've got a specifically retrieval functioning she won't hear now the next step is to think about the weather this retrieval function actually makes sense i can we expect that this function to after perform well when we use it to ranger documents for users queries so it's worth thinking about what is this value that will calculate so in the end we get a number but what is this number mean is it meaningful so spend a couple of minutes to think about that
#c9	we;assumptions;we;the vector space;we;the similarity;the end;we;a specifically retrieval functioning;she;the next step;the weather;this retrieval function;sense;i;we;this function;we;it;documents;users;it;what;this value;the end;we;a number;what;this number;it;a couple;minutes
#s10	and of course the general question here is do you believe this is good ranking function what they'd actually work well so again think about how to interpret this value is it actually meaningful there's the mean something it's related to how well the document match the query so in order to assess whether this simplest vectors with model actually works well let's look at the example
#c10	course;the general question;you;good ranking function;what;they;this value;it;the mean something;it;the document;the query;order;this simplest vectors;model;'s;the example
#s11	so here i show some sample documents anna sample query the query is news about the presidential campaign and we have five documents here they cover different terms in the query and if you look at the these documents for a moment you may realize that some documents are probably relevant and some others are probably non relevant now if i ask you to rank these documents how would you rent them this is basically our ID or ranking when humans can examine the documents and then try to rank them not so think for a moment and take a look at this slide and perhaps by pausing the lap gym
#c11	i;some sample documents anna sample query;the query;news;the presidential campaign;we;five documents;they;different terms;the query;you;the these documents;a moment;you;some documents;some others;i;you;these documents;you;them;our ID;ranking;humans;the documents;them;a moment;a look;this slide;the lap gym
#s12	so i think most of you would agree that T four and T three are probably better than others becaus they really cover the query well they match news press value and the campaign so it looks like these through documents are probably better than the others they should be ranked that on top an the other three D two D one and D five really non relevant so we can also say D four and D three are relevant documents and D one D two and D five are non relevant so now let's see if our simplicity vectors based model could do the same or could do something closer so let's first think about how we actually use this model to school documents right here i show two documents D one and D three
#c12	i;you;T;T;others;they;the query;they;news press value;the campaign;it;documents;the others;they;top;we;relevant documents;D one D;D;'s;our simplicity vectors;based model;something;'s;we;this model;school documents;i;two documents;D;D
#s13	and we have the query also here in the vectors with model of course we want to first compute the vectors for these documents and the query now i show the vocabulary here as well so these are the end dimensions that will be thinking about
#c13	we;the query;the vectors;model;course;we;the vectors;these documents;the query;i;the vocabulary;the end dimensions
#s14	so what do you think is vector repetition for the query note that we are assuming that we only use zero and one to indicate whether a term is absent or present in the query or in the document so these are zero one bit vectors
#c14	what;you;vector repetition;the query note;we;we;a term;the query;the document;zero one bit vectors
#s15	so what do you think is query raptor well the query has four words here so for these forwards there will be a one and for the rest will be zero now what about documents it's the same
#c15	what;you;query;the query;four words;these forwards;the rest;documents;it
#s16	so T one has two words news and about so there are two ones here and the rest of zeros similarly so uh now that we have the two vectors let's computer the similarity and we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements right so these two will be former they forming product and these two general another product and these two which end with yet another product after and so on so forth now you can easy to see if we do that we actually don't have to care about these zeros becaus if whenever we have a zero the product will be zero so when we take a sum over all these pairs then the zero entries will be gone as long as you have one zero then the product will be zero so in fact we're just counting how many pairs of one and one in this case we have seen two
#c16	T one;two words news;two ones;the rest;zeros;we;the two vectors;'s;the similarity;we;dot product;you;we;dot product;we;the corresponding elements;they;product;yet another product;you;we;we;these zeros becaus;we;the product;we;a sum;all these pairs;the zero entries;you;the product;fact;we;how many pairs;this case;we
#s17	so the result would be too so what does that mean well that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document becaus if a document if the term is matched in the document then there will be two ones if it's not then there will be zero on the document aside similarly if the document has a term but the term is not in the query there will be a zero in the query back then so those don't count so as a result this scoring function basically meshes how many unique query terms are matched in a document this is how we interpreted this score now we can also take a look at the D three in this case you can see the result is three becaus these three match to three distinct query was news presidential campaign where is eva only match the two now in this case it seems reasonable to rank the three on top of E one and this simplest of X is based model indeed it does that so that looks pretty good however if we examine this model in detail we likely will find some problems
#c17	the result;what;this number;the value;this scoring function;the count;how many unique query terms;the document becaus;the term;the document;two ones;it;the document;the document;a term;the term;the query;the query;a result;this scoring function;how many unique query terms;a document;we;this score;we;a look;the D;this case;you;the result;three becaus;these three match;three distinct query;news presidential campaign;eva;this case;it;top;E;this simplest;X;model;it;we;this model;detail;we;some problems
#s18	so here i'm going to show all the scores for these five documents and you can easily verify they are correct becaus we basically counting the number of unique query terms match the in each document note that this measure after that makes sense right it basically means if the document in matches more unique query terms then the document are we assuming to be more relevant and that seems to make sense the only problem is here we can note set there are three documents D two D three and D four
#c18	i;all the scores;these five documents;you;they;correct becaus;we;the number;unique query terms;each document note;this measure;sense;it;the document;matches;the document;we;sense;the only problem;we;three documents;D
#s19	and they tide with a three as a school so that's a problem becaus if you look at them carefully is seems that D four should be ranked above the three becaus is re only imagine the presidential once the default messaging data multiple times in the case of the three presidential could be an extender managing but the four is clearly about presidential campaign another problem is that D two and D three also have the same score but if you look at the three words that are matched in the case of the two it matched the news about an campaign but in the case of the three it match the news presidential and campaign so intuitively these reads better becaus matching presidential is more important than matching about even though about and presidential are both in the query so intuitively would like T three it will rank the above D two but this model doesn't do that so that means this model is there not good enough we have to solve these problems to summarize in this lecture we talked about how to instantiate a vector space model we may need to do three things one is too define the dimension the second is to decide how to place documents as vectors in the vector space and to also plays a query in the baptist space as a vector and third is to define the similarity between two vectors particularly the query about the end document map we also talk about various simple way to instantiate a vector space model indeed that probably the simplest of vectors based model that we can arrive in this case we use each word with the final dimension we use zero one bit vector to represent a document or query in this case we basically only care about what the presence or absence we ignore the frequency and we use the thought product as similarity function and with such in situation and we showed that the scoring function is basically to score a document based on the number of distinct query words matching the document we also show that such a simple vector space model still doesn't work well and we need to improve it and this is a topic that we're going to cover in the next election
#c19	they;a school;a problem becaus;you;them;D;the three becaus;the default messaging data;the case;an extender;presidential campaign;another problem;the same score;you;the three words;the case;it;the news;an campaign;the case;it;the news presidential;campaign;better becaus matching presidential;the query;it;D;this model;this model;we;these problems;this lecture;we;a vector space model;we;three things;one;the dimension;the second;documents;vectors;the vector space;a query;the baptist space;a vector;the similarity;two vectors;particularly the query;the end document map;we;various simple way;a vector space model;vectors;model;we;this case;we;each word;the final dimension;we;zero one bit vector;a document;query;this case;we;what;we;the frequency;we;the thought product;similarity function;situation;we;the scoring function;a document;the number;distinct query words;the document;we;such a simple vector space model;we;it;a topic;we;the next election
410	a09e9c43-a8d0-4347-873b-041c21e3b725	24
#s1	this lecture is about how we can evaluate a ranked list in this lecture we will continue the discussion of evaluation in particular we're going to look at how we can evaluate the ranked list of results in the previous lecture we talked about precision and recall these are the two basically measures for quantitatively measuring the performance of search result but as we talked about ranking before we frame with the texture retrieval problem as a ranking problem so we also need to evaluate the quality of a ranked list how can we use precision and recall to evaluate a ranked list well naturally we have to look at the precision and recall at different cut offs becaus in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing if we assume the user sequentially flowers is the list of results the user would stop at some point and that point would determine the set and then that's the most important cut off that will have to consider when we compute the brazilian recall without knowing where exactly the user would stop then we have to consider all the positions where the user could stop so let's look at these positions look at this slide and then let's look at the weather for the user stops at the first document what's the precision and recall at this point what do you think well it's easy to see that this document is relevant so the position is one out of one we have gotten one document and that's random what about the recall well note that we assume that there are ten relevant documents for this query in the collection
#c1	this lecture;we;a ranked list;this lecture;we;the discussion;evaluation;we;we;the ranked list;results;the previous lecture;we;precision;the two basically measures;the performance;search result;we;ranking;we;the texture retrieval problem;a ranking problem;we;the quality;a ranked list;we;precision;a ranked list;we;the precision;different cut offs becaus;the end;the approximation;relevant documents;a ranked list;the user;we;the user;sequentially flowers;the list;results;the user;some point;that point;the set;the most important cut;we;the user;we;all the positions;the user;'s;these positions;this slide;'s;the weather;the user;the first document;what;the precision;this point;what;you;it;this document;the position;we;one document;the recall well note;we;ten relevant documents;this query;the collection
#s2	so it's one out of ten what if the user stops at the second position top two well the procedure is the same one hundred percent two out of two and the records two out of ten what if the user stops at the third position well this is interesting becaus in this case we have not got any additional random in the document so the recall it doesn't change but the precision is lower because we've gotten our eleanor
#c2	it;the user;the second position;top;the procedure;the same one hundred percent;the records;the user;the third position;interesting becaus;this case;we;any additional random;the document;the recall;it;the precision;we;our eleanor
#s3	so what exactly the position well it's two out of three right and recall is the same throughout of ten so when would we see another point where the recall would be different if you look down the list
#c3	what exactly the position;it;two out of three right;we;another point;the recall;you;the list
#s4	well it won't happen and here we have seen another reading the document in this case
#c4	it;we;the document;this case
#s5	D five at that point the recoil is increased to three out of ten and the precision is the three out of five so you can see if we keep doing this we can also get to T eight and then we will have a procedure of four out of eight because there are eight document san four of them are relevant and the recall is a four out of ten now when can we get a recall of five out of ten well in this list we don't have it so we have to go down on this we don't know where it is but as a convenience we often assume that the position is zero at all the precision is zero at all the other levels of recall that are beyond the search results so of course this is a pessimistic assumption the actual precision would be higher but we make this assumption in order to have easy way to compute another metric called average precision that will discuss later now i should also say now here you see we make these assumptions that are clearly not accurate but this is usually OK for the purpose of comparing to text richard methods and this is for the relative comparison so it's OK if the actual measure or actual actual number deviates a little bit of from that rule number as well as the deviation is not biased toward any particular retrieval method and we are OK we can still accurate detail which method works better and this is an important point to keep in mind when you compare different algorithms the keys to avoid any biased toward each method and as long as you can avoid that it's OK you do transformation of these measures in anyway so you can preserve the order
#c5	D;that point;the recoil;the precision;you;we;we;T;we;a procedure;eight document;them;the recall;we;a recall;five out of ten well;this list;we;it;we;we;it;a convenience;we;the position;all the precision;all the other levels;recall;the search results;course;a pessimistic assumption;the actual precision;we;this assumption;order;easy way;another metric called average precision;i;you;we;these assumptions;the purpose;richard methods;the relative comparison;it;the actual measure;a little bit;that rule number;the deviation;any particular retrieval method;we;we;still accurate detail;which method;an important point;mind;you;different algorithms;the keys;each method;you;it;you;transformation;these measures;you;the order
#s6	OK so we just talk about we can get a lot of precision recall numbers at different positions so now you can imagine we can plot the curve and this just shows on the X axis we show the recalls and on the Y access we show the position so the precision levels are marked as point one point two point three and one point zero
#c6	we;we;a lot;precision recall numbers;different positions;you;we;the curve;the X axis;we;the recalls;the Y access;we;the position;the precision levels;point one point;one point
#s7	so this is different levels of recall and the Y axis also has different amounts that's for precision so we plot these precision recall numbers that we have got as points on this picture now we can further link these points performer curve as you see we assume the all the other precision that high level recalls is zero and that's why they are done here i thought the hours europe the actual curve problem will be something like this
#c7	different levels;recall;the Y axis;different amounts;precision;we;these precision recall numbers;we;points;this picture;we;these points performer curve;you;we;the all the other precision;high level;they;i;the hours;europe;the actual curve problem;something
#s8	but as we just discuss it doesn't matter that much for comparing two methods 'cause this would be and there is made for all the methods
#c8	we;it;two methods;all the methods
#s9	OK so now that we have this precision recall curve how can we compare it to read the list
#c9	we;this precision recall curve;we;it;the list
#s10	right so that means we have to compare to PR curves and here is still two cases where system a is shown in red system be showing blue with crosses right so which one is better i hope you can see more system A is clearly better why be cause for the same level of recall and see same level of recall here and you can see the precision point buy system is better than system be so there's no question india you can imagine what is the curve look like for idea of searching system
#c10	we;PR curves;two cases;red system;crosses;one;i;you;more system;cause;the same level;recall;same level;recall;you;the precision point buy system;system;no question;india;you;what;the curve;idea;searching system
#s11	well it has to have perfect the position at all the recall points so it has to be this line that would be the ideal system in general the higher the curve is the better the problem is that we might see a case like this this actually happens often like the two curves cross each other now in this case which one is better what do you think now this is a real problem that you actually might face suppose you build a search engine an you have older algorithm that's shown here in blue or system be and you have come up with a new idea and you test it and the results are shown in red curve eight now your question is is your new method better than the old method or more practically do you have to replace the algorithm that you already using your in your search engine with another new algorithm so we use system method A to replace method would be this is going to be a real decision that you have to make if you make the replacement the search engine would behave like a system may here whereas if you don't do that it will be like a system be so what do you do now if you want to spend more time to think about this pause the video and it's after a very useful to think about that as i said it's a real decision that you have to make if you are building your own search engine or if you're working for a company that cares about the search now if you have thought about this woman you might realize that well in this case it's hard to say there was some users might like a system made some users might like like system be so what's the difference here well the difference is just that in the low level of recording this region system bees better that's higher precision but in high recall reading system is better now so that also means it depends on whether the user cares about the high recall or lower recall but high position and you can imagine someone is just going to check out what's happening today and you want to find some random in the news well which one is better what do you think in this case clearly system bees better because the user is unlikely examining a lot of results the user doesn't care about high recall on the other hand if you think about a case where a user is doing it's a little too so you starting problem you want to find whether your idea has been started before in that case you emphasize high recall so you want to see as many reading the documents as possible therefore you might have favored system eh so that means which ones better actually depends on users an more precisely users task
#c11	it;the position;all the recall points;it;this line;the ideal system;the curve;the problem;we;a case;the two curves;this case;one;what;you;a real problem;you;you;a search engine;an you;older algorithm;system;you;a new idea;you;it;the results;red curve;eight now your question;your new method;the old method;you;the algorithm;you;your;your search engine;another new algorithm;we;system method A;method;a real decision;you;you;the replacement;the search engine;a system;you;it;a system;what;you;you;more time;this pause;the video;it;i;it;a real decision;you;you;your own search engine;you;a company;the search;you;this woman;you;this case;it;some users;a system;some users;system;what;the difference;the difference;the low level;this region system bees;higher precision;high recall reading system;it;the user;the high recall;lower recall;high position;you;someone;what;you;some random;the news;one;what;you;this case;clearly system bees;the user;a lot;results;the user;high recall;the other hand;you;a case;a user;it;you;problem;you;your idea;that case;you;high recall;you;the documents;you;system;which ones;users;an more precisely users task
#s12	so this means you may not necessarily be able to come up with one number that would accurately depict the performance you have to look at the overall picture yet as i said when you have a practical decision to make whether you replace the algorithm with another then you may have to actually come up with a single number to quantify each method or when we compare many different methods in research ideally we have one number to compare them with so that we can easily make a lot of comparisons so for all these reasons it's desirable to have one single number to measure that so how do we do that and that needs a number to summarize arrange so here again it's the precision recall curve and one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve
#c12	you;one number;the performance;you;the overall picture;i;you;a practical decision;you;the algorithm;you;a single number;each method;we;many different methods;research;we;one number;them;we;a lot;comparisons;all these reasons;it;one single number;we;a number;it;the precision recall curve;one way;this whole ranked list;this whole curve;the area;the curve
#s13	right so this is one way to measure that there are other ways to measure that
#c13	one way;other ways
#s14	but it just turns out that this particular way of measuring has been very popular and has been used since a long time ago for text retrieval evaluation and this is basically computed in this way
#c14	it;this particular way;measuring;text retrieval evaluation;this way
#s15	and it's called average position basically we're going to take a look at every different recall point and then look at the precision so we know this is one precision this is another with different recall now this we don't count this one becaus the record level is the same and we're going to then look at this number and that's the precision at a different recall level etc
#c15	it;average position;we;a look;every different recall point;the precision;we;one precision;different recall;we;this one becaus;the record level;we;this number;the precision;a different recall level
#s16	so we have all these you know added up these are the positions that had the different points corresponding to retrieving the first irrelevant document the second
#c16	we;you;the positions;the different points;the first irrelevant document
#s17	and then the third the force etc now we miss them any random documents so in all of those cases we just assume that they have zero precisions and then finally we take the average so with divided by ten and which is a total number of relevant document in the collection note that here we are not dividing the sum by four which is a number of retrieve the relevant documents now imagine if i divide by four what would happen now think about this for a moment it's a common mistake that people sometimes overlook so if we divide this by four it's actually not very good in fact that you are favoring a system that would retrieve very few random documents as in that case the denominator would be very small so this would be not a good measure so not that this is ten the total number of random documents and this will basically compute the area and needs the curve and this is the standard method used for evaluating a ranked list note that it actually combines recall and precision but first we have precision numbers here but second that we also consider recalled becaus if you miss the minute there would be many O scale so it combines precision and recall and furthermore you can see this measure is sensitive to a small change of a position of the relevant document that let's say if i move this relevant document up a little bit now it would increase this mean at this average precision whereas if i move any relevant document down let's say i move this random not gonna let down then it would decrease the average precision so this is very good be cause
#c17	the force;we;them;any random documents;those cases;we;they;zero precisions;we;the average;a total number;relevant document;the collection note;we;the sum;a number;retrieve;the relevant documents;i;what;a moment;it;a common mistake;people;we;it;fact;you;a system;very few random documents;that case;the denominator;a good measure;the total number;random documents;the area;the curve;the standard method;a ranked list note;it;recall;precision;we;precision numbers;we;becaus;you;the minute;many O scale;it;precision;recall;you;this measure;a small change;a position;the relevant document;'s;i;this relevant document;it;this average precision;i;any relevant document;'s;i;it;the average precision;cause
#s18	it's a very sensitive to the ranking of every relevant document it can tell small differences between two ranked lists and that's what we want sometimes one algorithm only works a slightly better than another
#c18	it;the ranking;every relevant document;it;small differences;two ranked lists;what;we
#s19	and we want to see this difference in contrast if we look at the precision at the ten documents to look at this this whole set
#c19	we;this difference;contrast;we;the precision;the ten documents
#s20	well what's the procedure whether you think well it's easy to see that four out of ten right so that precision is very meaningful because it tells us what user would see so that's pretty useful
#c20	what;the procedure;you;it;that precision;it;us;what user
#s21	right
#s22	so it's a meaningful measure from users perspective
#c22	it;a meaningful measure;users
#s23	but if we use this measure to compare systems it wouldn't be good be cause it wouldn't be sensitive to wear these fall rather than documents are ranked if i move them around the precision at ten is there the same right
#c23	we;this measure;systems;it;it;these fall;documents;i;them;the precision;the same right
#s24	so this is not a good measure for comparing different algorithms in contrast the average position is much better measure it can tell the difference of different difference in ranked lists in subtle ways
#c24	a good measure;different algorithms;contrast;the average position;much better measure;it;the difference;different difference;ranked lists;subtle ways
410	a43eb3f2-84ee-45a2-b9ec-1f97d6de3c6e	8
#s1	so to summarize our discussion of recommended systems in some sense the filtering task or recommended tasker is easy and some other senses the task is actually difficulty so it's easy becaus the users expectations though in this case the system takes initiative to push the information to the user so the user doesn't really make any effort so any recommendation is better than nothing right
#c1	our discussion;recommended systems;some sense;the filtering task;tasker;some other senses;the task;difficulty;it;easy becaus;this case;the system;initiative;the information;the user;the user;any effort;any recommendation;nothing
#s2	so well unless you recommend all the noise items or useless documents if you can recommend some useful information users general with appreciated so that's in that sense that's easy however filtering is actually much harder task and then retrieval becaus you have to make a binary decision and you can afford a waiting for a lot of items and then you're going to see whether one item is better than others you have to make a decision when you see this item will think about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user wait for few days well even if you can make accurate recommendation of the most relevant news the utilities will be significantly decreased another reason why it's hard it's be cause of data sparseness if you think of this as a learning problem in collaborative filtering for example it's purely based on learning from the past ratings so if you don't have many ratings there's not much you can do and yeah just mentioned this cold start problem this is actually a very serious serious problem but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem you can use for example more user information to assess their similarity instead of using the preferences of these users on these items that may be additional information available about the user etc
#c2	you;all the noise items;useless documents;you;some useful information users;that sense;filtering;much harder task;you;a binary decision;you;a waiting;a lot;items;you;one item;others;you;a decision;you;this item;the news filtering;you;the news;you;the news;a user;few days;you;accurate recommendation;the most relevant news;the utilities;it;it;cause;data sparseness;you;a learning problem;collaborative filtering;example;it;the past ratings;you;many ratings;you;this cold start problem;a very serious serious problem;course;strategies;the problem;different strategies;you;the problem;you;example;more user information;their similarity;the preferences;these users;these items;additional information;the user
#s3	and we also talk about the two strategies for filtering task one is content based where we look at item similarity the others collaborative filtering where we look at the user similarity and the obviously can be combined in a practical system you can imagine the general would have to be combined so that will give us a hybrid strategy for filtering and we also could recall that we talked about push this is a poor as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are serving users in the pool mode obviously the tool should be combined and they can be combined to have a system that we can support user with multiple mode information access so in the future we could anticipate is such a system to be more useful to the user
#c3	we;the two strategies;filtering task;one;content;we;item similarity;the others;collaborative filtering;we;the user similarity;a practical system;you;the general;us;a hybrid strategy;filtering;we;we;push;two strategies;access;the text data;recommended system;users;the push mode;search engines;users;the pool mode;the tool;they;a system;we;user;multiple mode information access;the future;we;such a system;the user
#s4	and i saw this is the active research area so there are a lot of new algorithm the being proposed all the time in particular those new algorithms tend to use a lot of context information now the context here could be the context of the user
#c4	i;the active research area;a lot;new algorithm;those new algorithms;a lot;context information;the context;the context;the user
#s5	and then it could be also context of documents or items the items are not isolated
#c5	it;context;documents;items;the items
#s6	and they're connected in many ways the users might form social network are as well
#c6	they;many ways;the users;social network
#s7	so there's a rich context there that we can leverage in order to really solve the problem well
#c7	a rich context;we;order;the problem
#s8	and then that's active research area where also machine learning algorithms that have been applied there are some additional readings in the handbook called recommended systems and has a clashing of a lot of good articles that can give you an overview of a number of specific approaches to recommended systems
#c8	active research area;algorithms;some additional readings;the handbook;recommended systems;a clashing;a lot;good articles;you;an overview;a number;specific approaches;recommended systems
410	a893068a-73ee-4771-8c52-43d8a6133363	29
#s1	So, here are some specific examples of what we can't do today, and part of speech tagging is not easy to do one hundred percent correctly.
#c1	some specific examples;what;we;part;speech tagging;one hundred percent
#s2	"
#s3	So in the example: ""He turned off the" "highway"" versus ""
#c3	the example;He;the" "highway
#s4	He turned off the fan"" "and the two ""off""'s actually have" somewhat different syntactic categories.
#c4	He;the fan;" somewhat different syntactic categories
#s5	And also it's very difficult to get complete parsing correct.
#c5	it
#s6	Again, "the example ""A man saw a boy with a" "telescope"" can actually be very" difficult to parse depending on the context.
#c6	the example;A man;a boy;a" "telescope;the context
#s7	Precise deep semantic analysis is also very hard.
#c7	Precise deep semantic analysis
#s8	For example, to define the meaning of """own"" precisely is very difficult in a" "sentence like ""John owns a restaurant""."
#c8	example;the meaning;a" "sentence;John;a restaurant
#s9	So the state of the art can be summarized as follows.
#c9	the state;the art
#s10	Robust and general NLP tends to be shallow, while deep understanding does not scale up.
#c10	Robust and general NLP;deep understanding
#s11	For this reason, in this course, the techniques that we cover are, in general, shallow techniques for analyzing text data, to mine text data.
#c11	this reason;this course;the techniques;we;shallow techniques;text data;mine text data
#s12	And they are generally based on statistical analysis, so they are robust, and general, and, And they are in the category of shallow analysis, so such techniques have the advantage of being able to be applied to any text data in any natural language about any topic.
#c12	they;statistical analysis;they;they;the category;shallow analysis;such techniques;the advantage;any text data;any natural language;any topic
#s13	But the downside is that they don't give us a deeper understanding of text.
#c13	the downside;they;us;a deeper understanding;text
#s14	For that we have to rely on deeper natural language analysis techniques.
#c14	we;deeper natural language analysis techniques
#s15	That typically would require human effort to annotate a lot of examples of analysis that we'd like to do, and then computers can use machine learning techniques to learn from these training examples to do the task.
#c15	human effort;a lot;examples;analysis;we;computers;techniques;these training examples;the task
#s16	So in practical applications we generally combine the two kinds of techniques with the general statistical and methods as backbone as the basis, since it can be applied to any text data and on top of that we're going to use humans to annotate more data and to use supervised machine learning to do some tasks as well as we can, especially for those important tasks.
#c16	practical applications;we;the two kinds;techniques;methods;backbone;the basis;it;any text data;top;we;humans;more data;supervised machine;some tasks;we;those important tasks
#s17	So to bring humans in, into the loop to analyze, fix, to analyze text data more precisely.
#c17	humans;the loop;text data
#s18	But this course will cover the general statistical approaches that generally don't require much human effort.
#c18	this course;the general statistical approaches;much human effort
#s19	So they are practically more useful than some of the deeper analysis techniques that require a lot of human effort to annotate text data.
#c19	they;the deeper analysis techniques;a lot;human effort;text data
#s20	So to summarize this lecture, the main points to take away are, first NLP is the foundation for text mining.
#c20	this lecture;the main points;first NLP;the foundation;text mining
#s21	So obviously the better we can understand the text data, the better we can do text mining.
#c21	we;the text data;we;text mining
#s22	Computers today are far from being able to understand the natural language.
#c22	Computers;the natural language
#s23	Deeper NLP requires common sense knowledge and inferences, thus only working for very limited domains.
#c23	Deeper NLP;common sense knowledge;inferences;very limited domains
#s24	Not feasible for large scale text mining.
#c24	large scale text mining
#s25	Shallow NLP based on statistical methods can be done in large scale.
#c25	Shallow NLP;statistical methods;large scale
#s26	And is the main topic of this course and they are generally applicable to a lot of applications.
#c26	the main topic;this course;they;a lot;applications
#s27	They are in some sense also more useful techniques.
#c27	They;some sense;also more useful techniques
#s28	In practice, we use statistical NLP as the basis.
#c28	practice;we;statistical NLP;the basis
#s29	And we have humans to help as needed in various ways.
#c29	we;humans;various ways
410	a93e9263-8950-4d65-a5db-a2114d5774d9	168
#s1	This lecture is about document Length normalization.
#c1	This lecture;document Length normalization
#s2	In the vector space model.
#c2	the vector space model
#s3	In this lecture, we're going to continue the discussion of the vector space model.
#c3	this lecture;we;the discussion;the vector space model
#s4	In particular, we're going to discuss the issue of document length normalization.
#c4	we;the issue;document length normalization
#s5	So far in the lectures about the vector space model, we have used various signals from the document to assess the matching of the document, with the query.
#c5	the lectures;the vector space model;we;various signals;the document;the matching;the document;the query
#s6	In particular, we have considered the term frequency.
#c6	we;the term frequency
#s7	The count of the term in the document.
#c7	The count;the term;the document
#s8	We have also considered its global statistics.
#c8	We;its global statistics
#s9	Such as IDF inverse document frequency.
#c9	IDF inverse document frequency
#s10	But we have not considered document length.
#c10	we;document length
#s11	So here, I show 2 example documents.
#c11	I;2 example documents
#s12	D4 is much shorter with only 100 words.
#c12	D4;only 100 words
#s13	D6, on the other hand, has 5000 words.
#c13	D6;the other hand;5000 words
#s14	If you look at the matching of these query words, we see that in D6 there are more matchings of the query words.
#c14	you;the matching;these query words;we;D6;more matchings;the query words
#s15	But one might reason that D6 May have matched these query words  
#c15	one;D6;these query words
#s16	In a scattered manner.
#c16	a scattered manner
#s17	So maybe the topic of D6 is not really about the topic of the query.
#c17	the topic;D6;the topic;the query
#s18	So the discussion of campaign at the beginning of the document may have nothing to do with the mention of presidential at the end.
#c18	the discussion;campaign;the beginning;the document;nothing;the mention;the end
#s19	In general, if you think about long documents, they would have a higher chance to match any query in fact.
#c19	you;long documents;they;a higher chance;any query;fact
#s20	If you generate a long document randomly by simply sampling words.
#c20	you;a long document;words
#s21	From a distribution of words, Then eventually you probably will match any query.
#c21	a distribution;words;you;any query
#s22	So in this sense we should penalize long documents because they just naturally have better chances for matching any query.
#c22	this sense;we;long documents;they;better chances;any query
#s23	And this is the idea of document length normalization.
#c23	the idea;document length normalization
#s24	We also need to be careful in avoiding to over penalize long documents.
#c24	We;penalize long documents
#s25	On the one hand, we want to penalize a long document, but on the other hand we also don't want to over penalize them.
#c25	the one hand;we;a long document;the other hand;we;them
#s26	And the reason is because a document may be long because of different reasons.
#c26	the reason;a document;different reasons
#s27	In one case, the document may be long because it uses more words.
#c27	one case;the document;it;more words
#s28	So for example.
#c28	example
#s29	Think about the full text article of a research paper.
#c29	the full text article;a research paper
#s30	It would use more words than the corresponding abstract.
#c30	It;more words;the corresponding abstract
#s31	So this is the case where we probably should penalize the matching of.
#c31	the case;we;the matching
#s32	Long documents such as full paper.
#c32	Long documents;full paper
#s33	When we compare the matching of words in such a long document with matching of the words in a short abstract.
#c33	we;the matching;words;such a long document;matching;the words;a short abstract
#s34	Then long papers generally have higher chance of matching query words.
#c34	long papers;higher chance;matching query words
#s35	Therefore we should penalize them.
#c35	we;them
#s36	However, there is another case when the document is long and that is when the document simply has more content.
#c36	another case;the document;the document;more content
#s37	Now consider another case of a long document where we simply concatenated a lot of abstracts of different papers.
#c37	another case;a long document;we;a lot;abstracts;different papers
#s38	In such a case, obviously we don't want to over penalize such a long document.
#c38	such a case;we;such a long document
#s39	Indeed, we probably don't want to penalize such a document because it's long.
#c39	we;such a document;it
#s40	So that's why we need to be careful about.
#c40	we
#s41	Using the right degree of penalization.
#c41	the right degree;penalization
#s42	A method that has been working well based on research results is called  pivotal length normalization, and in this case the idea is to use the average document length as a pivot as a reference point.
#c42	A method;research results;  pivotal length normalization;this case;the idea;the average document length;a pivot;a reference point
#s43	That means we'll assume that for the average length documents, the score is about right, so the normalizer would be 1.
#c43	we;the average length documents;the score;the normalizer
#s44	But if a document is longer than the average document length, then there will be some penalization, whereas if it's a shorter, then there's even some reward.
#c44	a document;the average document length;some penalization;it;even some reward
#s45	So this is illustrated using this slide.
#c45	this slide
#s46	On the axis, X-axis, you can see the length of document.
#c46	the axis;you;the length;document
#s47	On the Y axis we show the normalizer in this case, the pivoted length normalization formula for the normalizer is.
#c47	the Y axis;we;the normalizer;this case;the pivoted length normalization formula;the normalizer
#s48	is seem to be Interpolation of 1 and the normalized document length controlled by a parameter b here.
#c48	Interpolation;the normalized document length;a parameter b
#s49	So you can see here, when we first divide the length of the document by the average document length, This not only gives us some sense about how this document is compared with the average document length, but also gives us a. Benefits of not worrying about the unit of.
#c49	you;we;the length;the document;the average document length;us;some sense;this document;the average document length;us;a. Benefits;the unit
#s50	Length, we can measure the length by words or by characters.
#c50	we;the length;words;characters
#s51	Anyway, this Normalizer has an interesting property.
#c51	this Normalizer;an interesting property
#s52	First we see that if we set the parameter B to 0 then the value would be one, so there's no  normalization at all.
#c52	we;we;the parameter B;the value;no  normalization
#s53	So b in this sense controls the length normalization.
#c53	this sense;the length normalization
#s54	Whereas if we set b to a nonzero  value then the normalizer would look like this so the value would be higher for documents that are longer than the average document length.
#c54	we;b;a nonzero;  value;the normalizer;the value;documents;the average document length
#s55	Whereas the value of the normalizer would be smaller for shorter documents.
#c55	the value;the normalizer;shorter documents
#s56	So in this sense we see there is a panelization for long documents.
#c56	this sense;we;a panelization;long documents
#s57	And there is a reward for short documents.
#c57	a reward;short documents
#s58	The degree of penalization is controlled by b because if we set B to a larger value than the normalizer would look like this, there's even more penalization for long documents and more reward for the short documents.
#c58	The degree;penalization;b;we;B;a larger value;the normalizer;even more penalization;long documents;more reward;the short documents
#s59	By adjusting b which varies from zero to one, we can control the degree of Length normalization.
#c59	b;we;the degree;Length normalization
#s60	So if we plug in this length normalization factor into the vector space model ranking functions that we have already examined.
#c60	we;this length normalization factor;the vector space model ranking functions;we
#s61	Then we will end up having the following formulas.
#c61	we;the following formulas
#s62	And these are in fact the state of art vector space model formulas.
#c62	fact;the state;art vector space model formulas
#s63	So let's look at this.
#c63	's
#s64	Take a look at each of them.
#c64	a look;them
#s65	The first one is called a pivoted length Normalization vector space model.
#c65	a pivoted length Normalization vector space model
#s66	And a reference in the end has details about derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed.
#c66	a reference;the end;details;derivation;this model;we;it;the TF IDF weighting model;we
#s67	The idea of component should be very familiar now to you.
#c67	The idea;component;you
#s68	There is also a query term frequency component.
#c68	a query term frequency component
#s69	Here.
#s70	And then in the middle, there is the normalized TF.
#c70	the middle;the normalized TF
#s71	And in this case we see we used a double logarithm.
#c71	this case;we;we;a double logarithm
#s72	As we discussed before, and this is to achieve a sub linear transformation.
#c72	we;a sub linear transformation
#s73	But we also put a document length Normalizer in the bottom.
#c73	we;a document length;Normalizer;the bottom
#s74	right, so this would cause penalization for long document, because the larger the denominator is then the smaller TF weighting is.
#c74	penalization;long document;the larger the denominator;the smaller TF weighting
#s75	And this is of course controlled by the parameter b here.
#c75	course;the parameter b
#s76	And you can see again if b is set to  zero and there is no length normalization.
#c76	you;no length normalization
#s77	OK, so this is one of the two most effective vector space model formulas.
#c77	the two most effective vector space model formulas
#s78	The next one, called BM25 or okapi.
#c78	BM25;okapi
#s79	Is.
#s80	Also similar in that it also has a IDF of component here.
#c80	it;a IDF;component
#s81	And a query TF component here.
#c81	And a query TF component
#s82	But in the middle, the normalization is a little bit different.
#c82	the middle;the normalization
#s83	As we explained there is this.
#c83	we
#s84	okapi TF transformation here.
#c84	TF transformation
#s85	That does sublinear transformation with the upper bound.
#c85	transformation
#s86	In this case, we have put the length normalization.
#c86	this case;we;the length normalization
#s87	Factor here we are adjusting K, but it achieves a similar factor, just because We put a normalizer in the denominator therefore.
#c87	we;K;it;a similar factor;We;a normalizer;the denominator
#s88	Again, if a document is longer than the term weight of this model So you can see after we have gone through all the analysis that we talked about.
#c88	a document;the term weight;this model;you;we;all the analysis;we
#s89	And we have.
#c89	we
#s90	In the end, reached basically the state of the art retrieval functions.
#c90	the end;basically the state;the art retrieval functions
#s91	So.
#s92	So far we have talked about the mainly how to place the document vector in the vector space.
#c92	we;the document vector;the vector space
#s93	And this has played an important role in determining the effectiveness of the retrieval function.
#c93	an important role;the effectiveness;the retrieval function
#s94	But there are also other dimensions where we did not really examine in detail.
#c94	other dimensions;we;detail
#s95	For example, can we further improve the instantiation of the dimension of the vector space model?
#c95	example;we;the instantiation;the dimension;the vector space model
#s96	Now We've just assumed that the bag of words representation, so each dimension is the word.
#c96	We;the bag;words;each dimension;the word
#s97	But obviously we can consider many other choices.
#c97	we;many other choices
#s98	For example, stemmed words.
#c98	example;words
#s99	Those are the words that are have been transformed into the same root form.
#c99	the words;the same root form
#s100	So that the computation and computing will all become the same and they can be matched.
#c100	the computation;computing;they
#s101	We can do stop word removal.
#c101	We;word removal
#s102	This is to remove Some very common words that don't carry "any content like ""the"", ""a"" or ""of"".
#c102	Some very common words;any content
#s103	" We can use phrases to define dimensions.
#c103	We;phrases;dimensions
#s104	We can even use latent semantic analysis to find some clusters of words that represent a latent concept as one dimension.
#c104	We;latent semantic analysis;some clusters;words;a latent concept;one dimension
#s105	We can also use smaller units, like a character N-grams.
#c105	We;smaller units;a character;N-grams
#s106	Those are sequences of N characters for dimensions.
#c106	sequences;N characters;dimensions
#s107	However, in practice, people have found that the bag of words representation with phrases is still the most effective one, and it's also efficient.
#c107	practice;people;the bag;words;representation;phrases;it
#s108	So this is still so far the most popular dimension instantiation method.
#c108	the most popular dimension instantiation method
#s109	And it's used in all the major search engines.
#c109	it;all the major search engines
#s110	I should also mention that sometimes we need to do language-specific and domain-specific tokenization, and this is actually very important as we might have variations of terms.
#c110	I;we;language-specific and domain-specific tokenization;we;variations;terms
#s111	That might prevent us from matching them with each other, even though they mean the same thing in some languages like Chinese There is also the challenge in segmenting Text to obtain word boundaries because it's just a sequence of characters.
#c111	us;them;they;the same thing;some languages;Chinese;the challenge;segmenting Text;word boundaries;it;just a sequence;characters
#s112	a word might correspond to 1 character or two characters or even 3 characters.
#c112	a word;1 character;two characters;even 3 characters
#s113	So it's Easier in English when we have a space to separate the words, but in some other languages we may need to do some natural language processing to figure out where all the boundaries for words.
#c113	it;English;we;a space;the words;some other languages;we;some natural language processing;words
#s114	There is also a possibility to improve the similarity function, and so far we have used the dot product, but one can imagine there are other measures.
#c114	a possibility;the similarity function;we;the dot product;one;other measures
#s115	For example, we can measure the cosine of the angle between two vectors, or we can use Euclidean distance measure.
#c115	example;we;the cosine;the angle;two vectors;we;Euclidean distance measure
#s116	And these are all possible.
#s117	But dot product seems still the best and one reason is because it's very general.
#c117	dot product;the best and one reason;it
#s118	In fact, it's sufficiently general.
#c118	fact;it
#s119	If you consider the possibilities of doing weighting in different ways.
#c119	you;the possibilities;weighting;different ways
#s120	So for example, cosine measure can be regarded as the dot product of two normalized vectors.
#c120	example;cosine measure;the dot product;two normalized vectors
#s121	That means we first normalize each vector
#c121	we;each vector
#s122	and then we take the dot product that would be equivalent to the cosine measure.
#c122	we;the dot product;the cosine measure
#s123	I just mentioned that the BM 25 seems to be one of the most effective formulas.
#c123	I;the BM;the most effective formulas
#s124	But there has been also further development in improving BM.
#c124	further development;BM
#s125	25, Although none of these works have changed the BM25 fundamentally.
#c125	none;these works;the BM25
#s126	So in one line work people have derived BM25F.
#c126	one line work;people;BM25F.
#s127	Here F stands for Field and this is to use BM25 for documents with the structures.
#c127	F;Field;BM25;documents;the structures
#s128	So for example you might consider Title Field, the abstract or body or the research article or even anchor text.
#c128	example;you;Title Field;the abstract;body;the research article;even anchor text
#s129	On the web pages, those are the text fields that describe links to other pages, and these can all be combined with appropriate weights of different fields to help improve scoring for a document.
#c129	the web pages;the text fields;links;other pages;appropriate weights;different fields;scoring;a document
#s130	When we use BM25 for such a document.
#c130	we;BM25;such a document
#s131	And the obvious choice is to apply the BM25 for each field and then combine the scores.
#c131	the obvious choice;the BM25;each field;the scores
#s132	Basically the idea of BM25F is to first combine the frequency counts of terms in all the fields.
#c132	the idea;BM25F;the frequency counts;terms;all the fields
#s133	And then apply BM 25.
#c133	BM
#s134	Now this has advantage of avoiding over counting the first occurrence of the term.
#c134	advantage;the first occurrence;the term
#s135	Remember, in the sub linear transformation of TF, the first occurrence is very important and it contributes a large weight and if we do that for all the fields than the same term might have gained a lot of advantage in every field.
#c135	the sub linear transformation;TF;the first occurrence;it;a large weight;we;all the fields;the same term;a lot;advantage;every field
#s136	But when we combine these word frequencies together, we just do the transformation.
#c136	we;these word frequencies;we;the transformation
#s137	A one time at that time, then the extra occurrences will not be counted as fresh first occurrences.
#c137	that time;the extra occurrences;fresh first occurrences
#s138	And this method has been working very well for scoring structure documents.
#c138	this method;scoring structure documents
#s139	The other line of extension is called a PM 25 plus.
#c139	The other line;extension;a PM
#s140	In this line, researchers have addressed the problem of over penalization of long documents by BM 25.
#c140	this line;researchers;the problem;penalization;long documents;BM
#s141	So to address this problem, the fix is actually quite simple.
#c141	this problem;the fix
#s142	We can simply add a small constant to the TF normalization formula, But what's interesting is that we can analytically prove that by doing such a small modification.
#c142	We;a small constant;the TF normalization formula;what;we;such a small modification
#s143	We will fix the problem of over penalization of long documents by the original BM25 so the new formula, called BM25+.
#c143	We;the problem;penalization;long documents;the original BM25;the new formula;BM25+
#s144	Is empirically an analytically shown to be better than BM25.
#c144	BM25
#s145	So to summarize, all what we have said about the vector space model.
#c145	what;we;the vector space model
#s146	Here are the major takeaway points.
#c146	the major takeaway points
#s147	First, in such a model we use the similarity notion relevance, assuming that the relevance of a document with respect to a query is.
#c147	such a model;we;the similarity notion relevance;the relevance;a document;respect;a query
#s148	Basically proportional to the similarity between the query and document, so naturally that implies that the query an document must be represented in the same way, and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general.
#c148	the similarity;the query;document;the query;an document;the same way;this case;we;them;vectors;high dimensional vector space;the dimensions;words;concepts;terms
#s149	And we generally need to use a lot of heuristics to design the ranking function.
#c149	we;a lot;heuristics;the ranking function
#s150	We use.
#c150	We
#s151	Some examples to show the need for several heuristics, including TF weighting and transformation.
#c151	Some examples;the need;several heuristics;TF weighting;transformation
#s152	and IDF weighting and document length normalization.
#c152	and IDF weighting;document length normalization
#s153	These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of text.
#c153	These major heuristics;the most important heuristics;such a general ranking function;all kinds;text
#s154	And finally, BM25 and pivoted normalization seems to be the most effective formulas out of the vector space model.
#c154	BM25 and pivoted normalization;the most effective formulas;the vector space model
#s155	Now I have to say that I've put BM 25 in the category of vector space model.
#c155	I;I;BM;the category;vector space model
#s156	But in fact the BM 25 has been derived using probabilistic modeling.
#c156	fact;the BM;probabilistic modeling
#s157	So the reason why I've put it in the vector space model is.
#c157	the reason;I;it;the vector space model
#s158	First, the ranking function actually has a nice interpretation in the vector space model, we can easily see it looks very much like a vector space model with a special weighting function.
#c158	the ranking function;a nice interpretation;the vector space model;we;it;a vector space model;a special weighting function
#s159	The second reason is because the original BM25 has somewhat different form of IDF.
#c159	The second reason;the original BM25;somewhat different form;IDF
#s160	And that form of IDF actually doesn't really work so well as the standard IDF.
#c160	that form;IDF;the standard IDF
#s161	That you have seen here, so as effective retrieval function BM25 should probably use a heuristic modification of the IDF to make it even more look like a vector space model.
#c161	you;effective retrieval function;BM25;a heuristic modification;the IDF;it;a vector space model
#s162	There are some additional readings.
#c162	some additional readings
#s163	The first is a paper about the pivoted length normalization.
#c163	a paper;the pivoted length normalization
#s164	It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derived length normalization formula.
#c164	It;an excellent example;empirical data analysis;the need;length normalization;length normalization formula
#s165	The second is the original paper where BM25 was proposed.
#c165	the original paper;BM25
#s166	The 3rd paper has a thorough discussion of BM25 and its extensions.
#c166	The 3rd paper;a thorough discussion;BM25;its extensions
#s167	Particularly BM25F.
#c167	Particularly BM25F.
#s168	And finally, the last paper has a discussion of improving BM25 to correct the over penalization of long documents.
#c168	the last paper;a discussion;BM25;the over penalization;long documents
410	a9456b11-885e-4481-b745-9706b5fd893e	126
#s1	So let's plug in these smoothing methods into the ranking function to see what we will get.
#c1	's;these smoothing methods;the ranking function;what;we
#s2	So, this is a general smoothing... sorry, general ranking function for smoothing with collection language model.
#c2	a general smoothing... sorry, general ranking function;collection language model
#s3	You have seen this before.
#c3	You
#s4	And now we have a very specific smoothing method, the JM smoothing method.
#c4	we;a very specific smoothing method;the JM;method
#s5	So now let's see what what's the value for alpha sub D here.
#c5	's;what;what;the value;alpha sub D
#s6	And, what's the value for P sub seen here?
#c6	what;the value;P sub
#s7	So we may need to decide this in order to figure out the exact form of the ranking function, and we also need to figure out, of course, Alpha.
#c7	we;order;the exact form;the ranking function;we;course
#s8	So let's see, well, this ratio.
#c8	's
#s9	Is basically this right?
#s10	So Here this is the probability of seeing word on the top.
#c10	the probability;word;the top
#s11	And this is the probability of unseen word, or in other words, lambda is basically the alpha here.
#c11	the probability;unseen word;other words;lambda;the alpha
#s12	So it's easy to see that this can be rewritten as this.
#c12	it
#s13	Very simple.
#s14	So we can plug this into here.
#c14	we
#s15	And then here, what's the value for alpha?
#c15	what;the value;alpha
#s16	What do you think?
#c16	What;you
#s17	It will be just Lambda, right?
#c17	It;just Lambda
#s18	And, what would happen if we plug in this value here?
#c18	what;we;this value
#s19	If this is lambda, what can we say about this?
#c19	lambda;what;we
#s20	Does it depend on the document?
#c20	it;the document
#s21	No, so it can be ignored.
#c21	it
#s22	Right?
#s23	So we end up having this ranking function shown here.
#c23	we;this ranking function
#s24	And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms.
#c24	this case;you;precisely a vector space model;this part;the sum;all the matched query terms
#s25	This is the element of the query vector what do you think is the element of the document vector?
#c25	the element;the query vector;what;you;the element;the document vector
#s26	It's this, so that's our document.
#c26	It;our document
#s27	vector element.
#c27	vector element
#s28	And let's further examine what's inside this logarithm.
#c28	's;what;this logarithm
#s29	So one plus this, so it's going to be a non-negative  log of this.
#c29	it;a non-negative  log
#s30	It's going to be at least one, right?
#c30	It
#s31	And this is a parameter.
#c31	a parameter
#s32	So Lambda is parameter and let's look and this is a TF.
#c32	Lambda;parameter;'s;a TF
#s33	Now we see very clearly this TF weighting here.
#c33	we
#s34	And.
#s35	The larger the count is, the higher the weight will be.
#c35	the count;the weight
#s36	We also see IDF weighting which is given by this.
#c36	We;IDF weighting
#s37	And with our document length normalization here.
#c37	our document length normalization
#s38	So all these heuristics are captured in this formula.
#c38	all these heuristics;this formula
#s39	What's interesting that we kind of have got this weighting function automatically by making various assumptions, whereas in the vector space model we had to go through those heuristic design in order to get this.
#c39	What;we;this weighting function;various assumptions;the vector space model;we;those heuristic design;order
#s40	And in this case note that there is a specific form and we can see whether this form actually makes sense.
#c40	this case;a specific form;we;this form;sense
#s41	So what do you think Is the denominator here?
#c41	what;you;the denominator
#s42	This is the length of document, total number of words multiplied by the probability of the word given by the collection.
#c42	the length;document;words;the probability;the word;the collection
#s43	So this actually can be interpreted as expected count of the word.
#c43	expected count;the word
#s44	If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document.
#c44	we;a word;the collection language model;we;the number;words;the document
#s45	If you do that, the expected count of a word W would be precisely given by this denominator.
#c45	you;the expected count;a word;W;this denominator
#s46	So this ratio basically is comparing the actual count here.
#c46	this ratio;the actual count
#s47	The actual count of the word in the document with the expected count given by this product.
#c47	The actual count;the word;the document;the expected count;this product
#s48	If the word is in fact the following, the distribution in the collection this.
#c48	the word;fact;the following;the distribution;the collection
#s49	And if this counter is larger than the expected count, this part, this ratio would be larger than one.
#c49	this counter;the expected count;this ratio
#s50	So that's actually a very interesting interpretation, right?
#c50	a very interesting interpretation
#s51	It's very natural.
#c51	It
#s52	And intuitively it makes a lot of sense.
#c52	it;a lot;sense
#s53	And this is one advantage of using this kind of probabilistic reasoning.
#c53	one advantage;this kind;probabilistic reasoning
#s54	Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here.
#c54	we;explicit assumptions;we;we;a logarithm;we;these probabilities
#s55	And we also have a formula that intuitively makes a lot of sense.
#c55	we;a formula;a lot;sense
#s56	And does TF-IDF weighting and document length normalization.
#c56	TF-IDF weighting;document length normalization
#s57	Let's look at the Dirichlet Prior Smoothing.
#c57	's;the Dirichlet Prior Smoothing
#s58	It's very similar to the case of JM smoothing.
#c58	It;the case
#s59	In this case, the smoothing parameter is Mu and that's different from lambda that we saw before, but the format looks very similar.
#c59	this case;the smoothing parameter;Mu;lambda;we;the format
#s60	The form of the function looks very similar.
#c60	The form;the function
#s61	So we still have linear interpolation here.
#c61	we;linear interpolation
#s62	And when we compute this ratio, while we defined that is that the ratio is equal to this.
#c62	we;this ratio;we;the ratio
#s63	But what's interesting here is that we are doing another comparison here now.
#c63	what;we;another comparison
#s64	We're comparing the actual count with the expected count of the word if we sample Mu words according to the collection of the probability.
#c64	We;the actual count;the expected count;the word;we;Mu words;the collection;the probability
#s65	So note that it's interesting we don't even see document length here.
#c65	it;we;document length
#s66	Unlike in the JM smoothing.
#s67	So this of course should be plugged into this part.
#c67	course;this part
#s68	So you might wonder, where  is the document length?
#c68	you;the document length
#s69	Interestingly, the document length is here.
#c69	the document length
#s70	In alpha sub d so this would be  plugged into this part.
#c70	alpha sub;d;this part
#s71	As a result, what we get is the following function here, and this is again a sum over all the matched query words.
#c71	a result;what;we;the following function;a sum;all the matched query words
#s72	And we again see the query term frequency here.
#c72	we;the query term frequency
#s73	And you can interpret this as the element of a document vector.
#c73	you;the element;a document vector
#s74	But this is no longer a simple dot product, right?
#c74	a simple dot product
#s75	Because we have this part.
#c75	we;this part
#s76	And note that n is the length of the query.
#c76	n;the length;the query
#s77	So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document.
#c77	we;this function;we;a sum;all the query words;some adjustment;the score;the document
#s78	But it's still It's still clear that it does document length normalization because this lens is in the denominator, so a longer document will have a lower weight here.
#c78	it;It;it;length normalization;this lens;the denominator;a longer document;a lower weight
#s79	And we can also see it has TF here and then IDF.
#c79	we;it;TF
#s80	Only that this time the form of the formula is different from the previous one in JM smoothing.
#c80	the form;the formula;the previous one
#s81	But intuitively, is still implements TF IDF weighting and document length normalization.
#c81	TF IDF weighting;document length normalization
#s82	Again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made.
#c82	the form;the function;the probabilistic reasoning;assumptions;we
#s83	Now there are also disadvantages of this approach, and that is there's no guarantee that such a form of the formula would actually work well.
#c83	disadvantages;this approach;no guarantee;such a form;the formula
#s84	So if you look back at this retrieval function.
#c84	you;this retrieval function
#s85	Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation.
#c85	it;TF IDF weighting;the length normalization;example;it;we;sub-linear transformation
#s86	But Fortunately we can see here.
#c86	we
#s87	There is a logarithm function here, so we do have also the here, right?
#c87	a logarithm function;we
#s88	So we do have the sub-linear transformation, but we did not intentionally do that.
#c88	we;the sub-linear transformation;we
#s89	That means there's no guarantee that will end up in this in this way.
#c89	no guarantee;this way
#s90	Suppose we don't have logarithm, then there's no sub linear transformation.
#c90	we;logarithm;no sub linear transformation
#s91	As we discussed before, perhaps the formula is not going to work so well.
#c91	we;the formula
#s92	So that's example of the gap between formal model like this and the relevance that we have to model, which is really a subjective machine.
#c92	example;the gap;formal model;the relevance;we;a subjective machine
#s93	That is tight to users.
#c93	users
#s94	So it doesn't mean we cannot fix this.
#c94	it;we
#s95	For example, imagine if we did not have this logarithm, right?
#c95	example;we;this logarithm
#s96	So we can heuristically add one, or we can even add a double logarithm, but then it would mean that the function is no longer probabilistic model.
#c96	we;we;a double logarithm;it;the function;probabilistic model
#s97	So the consequence of the modification is no longer as predictable as what we have been doing now.
#c97	the consequence;the modification;what;we
#s98	So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25.
#c98	example;BM;challenge;probabilistic model;a better model;BM25
#s99	In particular, how do we use query likelihood to derive a model that would work consistently better than BM25?
#c99	we;query likelihood;a model;BM25
#s100	Currently we still cannot do that.
#c100	we
#s101	It's still an interesting open question.
#c101	It;an interesting open question
#s102	So to summarize this part we've talked about the two smoothing methods.
#c102	this part;we;the two smoothing methods
#s103	Jelinek-Mercer, which is doing fixed coefficient linear interpolation.
#c103	Jelinek-Mercer;fixed coefficient linear interpolation
#s104	Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents.
#c104	pseudocounts;every word;adaptive interpolation;the coefficient;shorter documents
#s105	In both cases we can see by using these smoothing methods we would be able to reach a retrieval function, whether assumptions are clearly articulated, so they're less heuristic.
#c105	both cases;we;these smoothing methods;we;a retrieval function;assumptions;they
#s106	Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization.
#c106	Experiment results;these retrieval functions;they;BM;length normalization
#s107	So this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design.
#c107	a major advantage;probabilistic model;we;a lot;heuristic design
#s108	Yet in the end, we naturally implemented TF IDF weighting and document length normalization.
#c108	the end;we;TF IDF weighting;document length normalization
#s109	Each of these functions also has precisely one smoothing parameter.
#c109	these functions;precisely one smoothing parameter
#s110	In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters.
#c110	this case;course;we;the smoothing parameter;methods;these parameters
#s111	So overall this shows by using probabilistic model we follow very different strategy than the vector space model.
#c111	probabilistic model;we;very different strategy;the vector space model
#s112	Yet in the end we end up with some retrieval functions that look very similar to a vector space model with some advantages in having assumptions clearly stated.
#c112	the end;we;some retrieval functions;a vector space model;some advantages;assumptions
#s113	And then the form dictated by probabilistic model.
#c113	And then the form;probabilistic model
#s114	Now, this also concludes our discussion of the query likelihood problems model.
#c114	our discussion;the query likelihood problems
#s115	And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture.
#c115	's;what assumptions;we;order;the functions;we;this lecture
#s116	We basically have made four assumptions that I listed here.
#c116	We;four assumptions;I
#s117	The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made.
#c117	The first assumption;the relevance;the query likelihood;the second assumption;we
#s118	It is a query words are generated independently that allows us to decompose the probability of the whole query.
#c118	It;a query words;us;the probability;the whole query
#s119	Into a product of probabilities of all the words in the query.
#c119	a product;probabilities;all the words;the query
#s120	And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made  one of these two assumptions about the smoothing.
#c120	the third assumption;we;a word;the document;we;its probability;its probability;the collection;the smoothing;the collection language model;we;these two assumptions;the smoothing
#s121	So we either use JM smoothing or the Dirichlet smoothing.
#c121	we;the Dirichlet smoothing
#s122	If we make these four assumptions, then we have no choice but to take the form of the retrieval function that we have seen earlier.
#c122	we;these four assumptions;we;no choice;the form;the retrieval function;we
#s123	Fortunately, the function has a nice property in that implements TF IDF weighting and documents length normalization
#c123	the function;a nice property;TF IDF weighting;documents;length normalization
#s124	And, these functions also work very well, so in that sense these functions are less heuristic compared with a vector space model.
#c124	these functions;that sense;these functions;a vector space model
#s125	And there are many extensions.
#c125	many extensions
#s126	This basic model and you can find the discussion of them in the reference at the end of this Lecture.
#c126	This basic model;you;the discussion;them;the reference;the end;this Lecture
410	a97a9d5e-48b7-4f4e-9754-4c5b30a31424	91
#s1	This lecture is about text based prediction.
#c1	This lecture;text based prediction
#s2	" In this lecture we're going to start talking about mining a different kind of knowledge as you, you can see here on this slide.
#c2	this lecture;we;a different kind;knowledge;you;you;this slide
#s3	here on this slide.
#c3	this slide
#s4	Namely, we're going to use text data to infer values of some other variables in the real world.
#c4	we;text data;values;some other variables;the real world
#s5	That may not be directly related to the text, or only remotely related to text data.
#c5	the text;text data
#s6	So this is very different from content analysis or topic mining where we directly characterize the content of text.
#c6	content analysis;topic mining;we;the content;text
#s7	" It's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective content  which reflects what we know about the opinion holder.
#c7	It;opinion mining or sentiment analysis;the content;we;the subjective content;what;we;the opinion holder
#s8	But this only provides limited view of what we can predict.
#c8	limited view;what;we
#s9	In this lecture and the following lectures, we're going to talk more about how we can predict more information about the world.
#c9	this lecture;the following lectures;we;we;more information;the world
#s10	How can we get sophisticated patterns of text together with other kinds of data?
#c10	we;sophisticated patterns;text;other kinds; data
#s11	"
#s12	It would be useful to first take a look at the big picture of prediction in data mining in general and I call this data mining loop. "
#c12	It;a look;the big picture;prediction;data mining;I;this data mining loop
#s13	So the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the real world in the form of data. "
#c13	the picture;you;multiple sensors;human sensors;what;we;the real world;the form;data
#s14	And of course the data are in the form of non text data and text data.
#c14	course;the data;the form;non text data;text data
#s15	And our goal is to see if we can predict some values of important real world variables that matter to us.
#c15	our goal;we;some values;important real world variables;us
#s16	For example, someone's health condition or the weather, or etc.
#c16	example;the weather
#s17	So these variables would be important because we might want to act on that.
#c17	these variables;we
#s18	We might want to make decisions based on that.
#c18	We;decisions
#s19	So how can we get from the data to these predicted values?
#c19	we;the data;these predicted values
#s20	Well, in general we first have to do data mining and analysis of the data.
#c20	we;data mining;analysis;the data
#s21	Because we in general should treat all the data that we collected.
#c21	we;all the data;we
#s22	in such a prediction problem set up, we are very much interested in joint mining of non text and text data.
#c22	such a prediction problem;we;joint mining;non text and text data
#s23	We should mine all the data together.
#c23	We;all the data
#s24	And then through the analysis, we generally can generate the multiple predictors of this interesting variable to us, and we call these features.
#c24	the analysis;we;the multiple predictors;this interesting variable;us;we;these features
#s25	And these features can then be put into a predictive model to actually predict the value of any interesting variable.
#c25	these features;a predictive model;the value;any interesting variable
#s26	So this then allows us to change the world and so this basically is the general process for making a prediction based on data, including text data.
#c26	us;the world;the general process;a prediction;data;text data
#s27	Now it's important to emphasize that human actually plays very important role in this process.
#c27	it;human;very important role;this process
#s28	Especially because of the involvement of text data.
#c28	the involvement;text data
#s29	And so human first would be involved in the mining of the data.
#c29	the mining;the data
#s30	It will control the generation of these features.
#c30	It;the generation;these features
#s31	And also help us understand the text data because text data are created to be consumed by humans.
#c31	us;the text data;text data;humans
#s32	Humans are the best in consuming or interpreting text data.
#c32	Humans;text data
#s33	But when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. " Sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text data in all applications.
#c33	course;text data;machines;we;text data mining;machines;patterns;a lot;data;humans;general human;an important role;text data;all applications
#s34	Next human also must be involved in predictive model building and adjusting or testing.
#c34	Next human;predictive model building;testing
#s35	So in particular we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model, and then next, of course, when we have predicted values for the variables, then humans would be involved in taking actions to change the world or make decisions based on these predictive values.
#c35	we;a lot;domain knowledge;the problem;prediction;we;this predictive model;course;we;values;the variables;humans;actions;the world;decisions;these predictive values
#s36	And finally, it's interesting that human could be also involved in controlling the sensors.
#c36	it;human;the sensors
#s37	And, this is so that we can adjust the sensors to collect the most useful data for prediction.
#c37	we;the sensors;the most useful data;prediction
#s38	So that's why I called this data mining loop because as we perturb the sensors to collect the new data and more useful data then we will obtain more data for prediction.
#c38	I;this data mining loop;we;the sensors;the new data;more useful data;we;more data;prediction
#s39	This data generally will help us improve the prediction accuracy and in this loop are humans will recognize what additional data needs to be collected and machines would of course help humans identify what data should be collected next.
#c39	This data;us;the prediction accuracy;this loop;humans;what;additional data;machines;humans;what data
#s40	In general, we want to collect data that are most useful for learning.
#c40	we;data
#s41	And this there is actually a subarea in machine learning called active learning that has to do with this.
#c41	a subarea;machine learning;active learning
#s42	How do you identify data points?
#c42	you;data points
#s43	That would be most helpful for machine learning programs if you can label them, right.
#c43	machine learning programs;you;them
#s44	So in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the world and then observe what happens.
#c44	you;a loop;data acquisition;data analysis;data mining;prediction;values;actions;the world;what
#s45	And then you can then decide what additional data.
#c45	you;what additional data
#s46	Have to be collected by adjusting the sensor.
#c46	the sensor
#s47	Or from the prediction errors you can also know what additional data we need to acquire in order to improve the accuracy of prediction.
#c47	the prediction errors;you;what additional data;we;order;the accuracy;prediction
#s48	And this big picture is actually very general and it's reflecting a lot of important applications of big data.
#c48	this big picture;it;a lot;important applications;big data
#s49	So it's useful to keep that in mind while we're looking at some text mining techniques. "
#c49	it;mind;we;some text mining techniques
#s50	So from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions.
#c50	text mining perspective;we;text based prediction;course;text;predictions
#s51	And this is most useful for prediction about human behavior or human preferences or opinions.
#c51	prediction;human behavior;human preferences;opinions
#s52	But in general text data will be put together with non text data.
#c52	general text data;non text data
#s53	So the interesting questions here would be first how can we design effective predictors?
#c53	the interesting questions;we;effective predictors
#s54	And how do we generate such effective predictors from text?
#c54	we;such effective predictors;text
#s55	This question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data.
#c55	This question;some extent;some previous lectures;we;what kind;features;we;text data
#s56	It has also been addressed to some extent by talking about the other knowledge that we can mine from text.
#c56	It;some extent;the other knowledge;we;text
#s57	So for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model.
#c57	example;topic mining;the patterns;topic based indicators;predictors;a predictive model
#s58	So topics can be intermediate representation of text.
#c58	topics;intermediate representation;text
#s59	That would allow us to design high level features or predictors that are useful for prediction of some other variable.
#c59	us;high level features;predictors;prediction;some other variable
#s60	It maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors.
#c60	It;it;original text data;it;a much better representation;the problem;it;more effective predictors
#s61	And similarly, sentiment analysis can lead to such predictors as well.
#c61	sentiment analysis;such predictors
#s62	So those are the data mining or text mining algorithms can be used to generate the predictors.
#c62	the data mining or text mining algorithms;the predictors
#s63	The other question is how can we join mine text and non text data together?
#c63	The other question;we;mine text;non text data
#s64	Now this is a question that we have not addressed yet.
#c64	a question;we
#s65	So in this lecture and the following lectures we're going to address this problem because this is where we can generate the much more enriched features for prediction and allows us to review a lot of interesting knowledge about the world.
#c65	this lecture;the following lectures;we;this problem;we;the much more enriched features;prediction;us;a lot;interesting knowledge;the world
#s66	These patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction.
#c66	These patterns;text;non text data;themselves;prediction;they;many other predictors;they;the accuracy;prediction
#s67	Basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis.
#c67	you;text based prediction character;a unified framework;many text mining and analysis techniques;topic mining;content;any content mining techniques;sentiment analysis
#s68	The goal here is mainly to infer values of real world variables.
#c68	The goal;values;real world variables
#s69	But in order to achieve the goal, we can do some other preparations and these are sub tasks.
#c69	order;the goal;we;some other preparations;sub tasks
#s70	So one sub task could be mine, mine the content of text data like topic mining.
#c70	one sub task;mine;the content;text data;topic mining
#s71	And the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis.
#c71	mine knowledge;the observer;analysis;opinion analysis
#s72	And both can help provide predictors for the prediction problem.
#c72	predictors;the prediction problem
#s73	And of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis.
#c73	course;we;non text data;the predictive model;non text data;context;text analysis;the topic mining;the opinion analysis
#s74	And such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text.
#c74	such improvement;more effective predictors;our problems;it;the space;patterns;opinions;topics;we;text
#s75	As we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives.
#c75	we;the join analysis;text;non text data;2 perspectives
#s76	In one perspective, we can see non text data can help text mining.
#c76	one perspective;we;non text data;text mining
#s77	Because non text data can provide a context for mining text data.
#c77	non text data;a context;mining text data
#s78	Provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining.
#c78	a way;partition text data;different ways;a number;techniques;contextual text mining
#s79	And that's to mine text in the context defined by non text data.
#c79	mine text;the context;non text data
#s80	And you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures.
#c80	you;this reference;a large body;work;this direction;we;them;the next lectures
#s81	Now the other perspective is text data can help non text data mining as well.
#c81	the other perspective;text data;non text data mining
#s82	And this is because text data can help interpret patterns discovered from non text data.
#c82	text data;patterns;non text data
#s83	This helps discover some frequent patterns from non text data.
#c83	some frequent patterns;non text data
#s84	Now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur.
#c84	we;the text data;instances;the pattern;text data;instances;the pattern
#s85	And this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content " is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns.
#c85	us;two sets;text data;we;what;the difference;this difference;text data;text content;that difference;some meaning;this pattern;we;non text data;such patterns
#s86	And this technique is called pattern annotation.
#c86	this technique;pattern annotation
#s87	And, you can see this reference listed here for more detail.
#c87	you;this reference;more detail
#s88	So here are the reference that I just mentioned.
#c88	the reference;I
#s89	The first is reference for pattern annotation.
#c89	reference;pattern annotation
#s90	" The second is a Qiaozhu Mei dissertation on contextual text mining.
#c90	a Qiaozhu Mei dissertation;contextual text mining
#s91	It contains a large body of work on contextual text mining techniques. "
#c91	It;a large body;work;contextual text mining techniques
410	aac4e33c-97bb-46c8-a108-3e3e3322a85c	86
#s1	This lecture is about the methods for text categorization.
#c1	This lecture;the methods;text categorization
#s2	So in this lecture were going to discuss how to do text categorization.
#c2	this lecture;text categorization
#s3	1st.
#c3	1st
#s4	There are many methods for text categorization In such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem.
#c4	many methods;text categorization;such a method;the idea;the category;some rules;we;the domain knowledge;the categorization problem
#s5	So, for example, if you want to do topical categorisation for news articles, you can say if the news article mentions word like game and Sports three times that we're going to say it's about sports.
#c5	example;you;topical categorisation;news articles;you;the news article;word;game;Sports;we;it;sports
#s6	Things like that and this would allow us to deterministically decide which category A document should be put into.
#c6	Things;us;which category A document
#s7	Now such a strategy would work well if the following conditions hold.
#c7	such a strategy;the following conditions
#s8	First, the categories must be very well defined, and this allows the person to clearly decide the category based on some clear rules.
#c8	the categories;the person;the category;some clear rules
#s9	Secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever.
#c9	the categories;surface features;text;superficial features;keywords;punctuations
#s10	You can easily identify text data.
#c10	You;text data
#s11	For example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category.
#c11	example;some special vocabulary;a particular category;we;such a vocabulary;pattern;such a vocabulary;this category
#s12	Now we also should have sufficient knowledge.
#c12	we;sufficient knowledge
#s13	For designing these rules and so if that's the case, then such a method can be effective, and so it does have a provisions in some domains and sometimes.
#c13	these rules;the case;such a method;it;a provisions;some domains
#s14	However in general there are several problems with this approach.
#c14	several problems;this approach
#s15	First, of course it's labor intensive.
#c15	course;it
#s16	It requires a lot of manual work.
#c16	It;a lot;manual work
#s17	Obviously we can't do this for all kinds of categorization problems.
#c17	we;all kinds;categorization problems
#s18	We have to do it From scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well.
#c18	We;it;scratch;a different problem;becauses different rules;it
#s19	 
#s20	Secondly, it cannot handle uncertainties in rules.
#c20	it;uncertainties;rules
#s21	Often the rules aren't 100% reliable take for example, and looking at the occurrences of words in text and try to decide the topic.
#c21	the rules;example;the occurrences;words;text;the topic
#s22	It's actually very hard to have 1% correct the rule.
#c22	It;1%;the rule
#s23	So for example, you can say if it has games, sports, basketball, then for sure it's about sports.
#c23	example;you;it;games;sports;basketball;it;sports
#s24	But one can also imagine some text articles that mention these keywords.
#c24	one;some text articles;these keywords
#s25	But that may not be exactly about the sports, or only marginally touching sports.
#c25	the sports;sports
#s26	The main topic could be another topic, different topic then sports.
#c26	The main topic;another topic;different topic;sports
#s27	So that's one disadvantage of this approach, and then finally the rules may be inconsistent and this would need to concern about robustness more specifically, and sometimes the results of categorization may be different depending on which rule to be applied.
#c27	one disadvantage;this approach;the rules;robustness;the results;categorization;which rule
#s28	So in that case then you will face uncertainty and you will also have to decide the order of applying the rules or combination of results that are contradictory.
#c28	that case;you;uncertainty;you;the order;the rules;combination;results
#s29	So all these.
#s30	Problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning.
#c30	Problems;this approach;it;the both problems;machine learning
#s31	So these machine learning methods are more automatic, but I still put automatic in quotation marks cause they're not really completely automatic because it still require manual work.
#c31	these machine learning methods;I;quotation marks;they;it;manual work
#s32	More specifically, we have to use human experts to help in two ways.
#c32	we;human experts;two ways
#s33	First, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories.
#c33	the human experts;datasets;category labels;the computer;documents;which categories
#s34	And this is called a training data.
#c34	a training data
#s35	And then Secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category.
#c35	the human experts;a set;features;each text object;a clue;the category
#s36	So we need to provide some basic features for the computers to look into.
#c36	we;some basic features;the computers
#s37	And in the case of text, natural choice would be the words.
#c37	the case;text;natural choice;the words
#s38	So using each word as a feature is a very common choice to start with.
#c38	each word;a feature;a very common choice
#s39	But of course there are other sophisticated features like phrases or even policy feature tags or even syntactic structures.
#c39	course;other sophisticated features;phrases;even policy feature tags;even syntactic structures
#s40	So once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data.
#c40	human experts;we;soft rules;categorization;the training data
#s41	So soft rules just means we're going to still decide which category should be assigned to the document.
#c41	soft rules;we;which category;the document
#s42	But it's not going to be used using a rule that is deterministic, so we might use something similar to saying that if it matches game sports many times, it's likely to be a sports.
#c42	it;a rule;we;something;it;game sports;it;a sports
#s43	But we're not going to say exactly for sure, but instead we're going to use probabilities or weights so that we can combine multiple evidences, so the learning process basically is going to figure out which features are most useful for separating different categories.
#c43	we;we;probabilities;weights;we;multiple evidences;the learning process;which features;different categories
#s44	And it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important.
#c44	it;features;errors;categorisation;the training data;you
#s45	It's the basis for learning.
#c45	It;the basis
#s46	And then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object.
#c46	the train classifier;a new text object;the most likely category;the prediction;what;a human;this text object
#s47	If the human would to make a judgement.
#c47	the human;a judgement
#s48	So when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning.
#c48	we;text categorization;we;the problem;the general setting;supervised learning
#s49	So the setup is.
#c49	the setup
#s50	To learn a classifier to map a value of X into a map of Y.
#c50	a classifier;a value;X;a map;Y.
#s51	So here X is all the text objects.
#c51	X;all the text objects
#s52	And Y is all the categories a set of categories, so the classifier would take any value in X as input and we generate the value in Y as output, and we hope the output Y would be the right category for X, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of Input and output for function and then the computer is going to figure out how the function behaves like based on these examples and then try to be able to compute the values for future access that we have not seen.
#c52	Y;all the categories;a set;categories;the classifier;any value;X;input;we;the value;Y;output;we;the output;Y;the right category;X;course;the training data;the general goal;all the machine;problems;supervised learning problems;you;some examples;Input;output;function;the computer;the function;these examples;the values;future access;we
#s53	So in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans.
#c53	all methods;features;text objects;different categories;these features;they;humans
#s54	And they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data.
#c54	they;multiple features;a weighted matter;weights;the errors;the training data
#s55	So ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data.
#c55	the learning processes optimization problem;the objective function;tide;the errors;the training data
#s56	Different methods tend to vary in their ways of measuring the errors on the training data.
#c56	Different methods;their ways;the errors;the training data
#s57	They might optimize a different object function, which is often also called a loss function or cost function.
#c57	They;a different object function;a loss function;cost function
#s58	They also tend to vary in their ways of combining the features, so linear combination for example is simple is often used.
#c58	They;their ways;the features;so linear combination;example
#s59	But they're not as powerful as non linear combination, but nonlinear models might be more complex for training.
#c59	they;non linear combination;nonlinear models;training
#s60	So there are tradeoffs as well, but that would lead to different variations of.
#c60	tradeoffs;different variations
#s61	Many variations of these learning methods.
#c61	Many variations;these learning methods
#s62	So in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers.
#c62	we;the two kinds;classifiers;a high level;classifiers
#s63	The other is called discriminative classifiers.
#c63	discriminative classifiers
#s64	The generative classifiers try to learn what the data looks like in each category.
#c64	The generative classifiers;what;the data;each category
#s65	So it attempts to model the join the distribution of the data and the label
#c65	it;the join;the distribution;the data;the label
#s66	X&Y.
#c66	X&Y.
#s67	And, this can then be factored out to a product of Y. The distribution of labels and join the probability of sorry the conditional probability of X given Y
#c67	a product;Y.;The distribution;labels;the probability;the conditional probability;X;Y
#s68	so it's Y.
#c68	it;Y.
#s69	So we first model distribution of labels and then we model how the data is generated given a particular label here.
#c69	So we first model distribution;labels;we;the data;a particular label
#s70	And once we can estimate these models, then we can compute this conditional probability of label given data based on.
#c70	we;these models;we;this conditional probability;label;data
#s71	The probability of data given label.
#c71	The probability;data;label
#s72	And the label distribution here by using the base rule.
#c72	And the label distribution;the base rule
#s73	Now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely.
#c73	the most important thing;this conditional probability;the label;which label
#s74	So in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors.
#c74	such approaches;the objective function;we;the data;it;the training errors
#s75	But if we can model the data in each category accurately, then we can also classify accurately.
#c75	we;the data;each category;we
#s76	One example is naive Bayes classifier.
#c76	One example;naive Bayes classifier
#s77	In this case.
#c77	this case
#s78	The other kind of approaches are called discriminative classifiers.
#c78	The other kind;approaches;discriminative classifiers
#s79	These classifiers try to learn what features separate categories, so they directly tackle the problem of categorisation or separation of classes.
#c79	These classifiers;what;separate categories;they;the problem;categorisation;separation;classes
#s80	So sorry for the problem.
#c80	the problem
#s81	So these discriminative classifiers attempted to model the.
#c81	these discriminative classifiers
#s82	Conditional.
#s83	Probability of the label given the data point directly.
#c83	Probability;the label;the data point
#s84	So the objective function tends to directly measure the errors of categorisation on the training data.
#c84	the objective function;the errors;categorisation;the training data
#s85	Some examples include the logistical regression support vector machines and the K nearest neighbors.
#c85	Some examples;vector machines;the K nearest neighbors
#s86	We will cover some of these classifiers in detail in the next few lectures.
#c86	We;these classifiers;detail;the next few lectures
410	ab2240b1-d836-4dbc-bcca-0a99472355d6	82
#s1	In this lecture we're going to talk about text access.
#c1	this lecture;we;text access
#s2	In the previous lecture we talk about natural language content analysis.
#c2	the previous lecture;we;natural language content analysis
#s3	We explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner.
#c3	We;the state;the art natural language processing techniques;a lot;unrestricted text data;a robust manner
#s4	As a result, bag of words representation remains very popular in applications like search engines.
#c4	a result;words;applications;search engines
#s5	In this lecture we're going to talk about some high level strategies.
#c5	this lecture;we;some high level strategies
#s6	To help users get access to the text data.
#c6	users;access;the text data
#s7	This is also important step to convert raw big text data into small relevant data that are actually needed in a specific application.
#c7	important step;raw big text data;small relevant data;a specific application
#s8	So the main question we will address here is how can a text information system help users get access to the relevant text data we're going to cover two complementary strategies, push versus pull.
#c8	the main question;we;a text information system;users;access;the relevant text data;we;two complementary strategies;pull
#s9	And then we're going to talk about the two ways to implement the pull mode: querying versus browsing.
#c9	we;the two ways;the pull mode;browsing
#s10	So first push versus pull.
#c10	So first push;pull
#s11	These are two different ways to connect users with the right information at the right time.
#c11	two different ways;users;the right information;the right time
#s12	The difference is.
#c12	The difference
#s13	Which takes the initiative.
#c13	the initiative
#s14	Which party takes the initiative?
#c14	Which party;the initiative
#s15	In the pull mode, the users would take the initiative.
#c15	the pull mode;the users;the initiative
#s16	To start the information access process.
#c16	the information access process
#s17	And in this case, a user typically would use a search engine to fulfill the goal.
#c17	this case;a user;a search engine;the goal
#s18	For example, the user may type in the query and then browse results to find the relevant information.
#c18	example;the user;the query;results;the relevant information
#s19	So this is usually appropriate for satisfying a users Ad hoc information need.
#c19	a users;Ad hoc information
#s20	An ad hoc information need is temporary information need, for example.
#c20	ad hoc information;temporary information;example
#s21	You want to buy a product so you suddenly have a need to read reviews about related products.
#c21	You;a product;you;a need;reviews;related products
#s22	But after you have collected information and have purchased your product.
#c22	you;information;your product
#s23	You generally no longer need such information, so it's a temporary information need.
#c23	You;such information;it;a temporary information need
#s24	In such a case, it's very hard for a system will predict your need and it's more appropriate for the users to take the initiative, and that's why search engines are very useful today because many people have many ad hoc information needs all the time.
#c24	such a case;it;a system;your need;it;the users;the initiative;search engines;many people;many ad hoc information;all the time
#s25	So as we're speaking Google is probably processing many queries from us and those are all or mostly all ad hoc information needs.
#c25	we;Google;many queries;us;hoc information
#s26	So this is a pull mode in contrast, in the push mode, the system will take the initiative to push the information to the user, or to recommend that information to the user.
#c26	a pull mode;contrast;the push mode;the system;the initiative;the information;the user;that information;the user
#s27	So in this case this is usually supported by a recommender system.
#c27	this case;a recommender system
#s28	Now this would be appropriate if the user has a stable information need.
#c28	the user;a stable information need
#s29	For example, you may have a research interest in some topic and that interest tends to stay for awhile, so it's relatively stable.
#c29	example;you;a research interest;some topic;interest;it
#s30	Your hobby is another example of a stable information need.
#c30	Your hobby;another example;a stable information need
#s31	In such a case, the system can interact with you and can learn your interest and then can monitor the information stream.
#c31	such a case;the system;you;your interest;the information stream
#s32	If it is, the system has seen any relevant items to your interest the system could then take the initiative to recommend information to you.
#c32	it;the system;any relevant items;your interest;the system;the initiative;information;you
#s33	So for example, a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you.
#c33	example;a news filter;news recommender system;the news stream;interesting news;you;the news articles;you
#s34	This mode of information access maybe also appropriate when the system has good knowledge about the users need and this happens in the search context.
#c34	This mode;information access;the system;good knowledge;the users;the search context
#s35	So for example, when you search for information on the web, a search engine might infer you might be also interested in some related information.
#c35	example;you;information;the web;a search engine;you;some related information
#s36	And they would recommend the information to you, so that should remind you,
#c36	they;the information;you;you
#s37	for example advertisement placed on search page.
#c37	example;advertisement;search page
#s38	So this is about the two high level strategies or two modes of text access.
#c38	the two high level strategies;two modes;text access
#s39	Now let's look at the pull mode in more detail.
#c39	's;the pull mode;more detail
#s40	In the pull mode, we can further distinguish in two ways to help users querying versus browsing.
#c40	the pull mode;we;two ways;users
#s41	In querying the user will just enter a query.
#c41	the user;a query
#s42	Typical keyword query and the search engine system would return relevant documents to users.
#c42	Typical keyword query;the search engine system;relevant documents;users
#s43	And this works when the user knows what exactly are the keywords to be used.
#c43	the user;what;the keywords
#s44	So if you know exactly what you're looking for, you tend to know the right keywords, and then querying would work very well
#c44	you;exactly what;you;you;the right keywords
#s45	and we do that all the time.
#c45	we
#s46	But we also know that sometimes it doesn't work so well, when you don't know the right keywords to use in the query or you want to browse information in some topic area.
#c46	we;it;you;the right keywords;the query;you;information;some topic area
#s47	In this case browsing would be more useful.
#c47	this case
#s48	So in this case.
#c48	this case
#s49	In the case of browsing, the users would simply navigate into the relevant information by following the paths supported.
#c49	the case;browsing;the users;the relevant information;the paths
#s50	By the structures documents.
#c50	the structures;documents
#s51	So the system would maintain some kind of structures and then the user could follow these structures to navigate.
#c51	the system;some kind;structures;the user;these structures
#s52	So this really works well when the user wants to explore the information space.
#c52	the user;the information space
#s53	Or the user doesn't know what are the key words to use in the query.
#c53	the user;what;the key words;the query
#s54	Or simply because the user finds it inconvenient to type in a query.
#c54	the user;it;a query
#s55	So even if the user knows what query to type in, if the user is using a cell phone.
#c55	the user;what query;the user;a cell phone
#s56	To search for information there, it's still hard to enter the query in such a case.
#c56	information;it;the query;such a case
#s57	Again, browsing tends to be more convenient.
#s58	The relationship between browsing and the query is best understood by making an analogy to sight seeing.
#c58	The relationship;the query;an analogy
#s59	Imagine if you are touring the city now.
#c59	you;the city
#s60	If you know the exact address of the attraction then taking a taxi, there is perhaps the fastest way you can go directly to the site, but if you don't know the exact address you may need to walk around, or you can take a taxi to a nearby place and then walk around.
#c60	you;the exact address;the attraction;a taxi;the fastest way;you;the site;you;the exact address;you;you;a taxi;a nearby place
#s61	It turns out that we do exactly the same in the information space.
#c61	It;we;the information space
#s62	If you know exactly what you're looking for, then you can use the right keywords in your query to find the information directly.
#c62	you;exactly what;you;you;the right keywords;your query;the information
#s63	That's usually the fastest way to do find information.
#c63	the fastest way;information
#s64	But what if you don't know the exact keywords to use?
#c64	you;the exact keywords
#s65	Your query probably won't work, so you'll land on some related pages, and then you need to also walk around in the information space, meaning by following the links or by browsing.
#c65	Your query;you;some related pages;you;the information space;the links
#s66	You can then finally get into the relevant page.
#c66	You;the relevant page
#s67	If you want to learn about the topic again you will likely do a lot of browsing.
#c67	you;the topic;you;a lot;browsing
#s68	So just like you are looking around in some area and you want to see some interesting attractions in a related- in the same region.
#c68	you;some area;you;some interesting attractions;a related-;the same region
#s69	So this is analogy also tells us that today.
#c69	analogy;us
#s70	We have very good spot for query but we don't really have good support for browsing.
#c70	We;very good spot;query;we;good support
#s71	And this is because.
#s72	In order to browse effectively, we need a map to guide us.
#c72	order;we;a map;us
#s73	Just like you need a map of Chicago to tour the city of Chicago, you need a topic map to tour the information space.
#c73	you;a map;Chicago;the city;Chicago;you;a topic map;the information space
#s74	So how to construct such a topic map is in fact a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications.
#c74	such a topic map;fact;a very interesting research question;us;more interesting browsing experience;the web;other applications
#s75	So to summarize this lecture we've talked about the two high level strategies for text access, push and pull.
#c75	this lecture;we;the two high level strategies;text access;push;pull
#s76	Push tends to be supported by recommender systems and pull tends to be supported by a search engine.
#c76	Push;recommender systems;tends;a search engine
#s77	Of course in a sophisticated intelligent information system we should combine the two.
#c77	a sophisticated intelligent information system;we
#s78	In the pull mode we can further distinguish querying and browsing again, we generally want to combine the two ways to help users so that you can support both querying and browsing.
#c78	the pull mode;we;we;the two ways;users;you
#s79	If you want to know more about the relationship between pull and push.
#c79	you;the relationship;pull;push
#s80	You can read this article.
#c80	You;this article
#s81	This gives excellent discussion of the relationship between information filtering and information retrieval.
#c81	excellent discussion;the relationship;information filtering and information retrieval
#s82	Here, information filtering is similar to information recommendation or the push mode of information access.
#c82	information filtering;information recommendation;the push mode;information access
410	aca8d826-412b-4134-8d2c-87537fdc4a76	73
#s1	So looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data.
#c1	the text mining problem;we;the problem;general data mining;we;text data
#s2	And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world.
#c2	we;text mining algorithms;us;text data;actionable knowledge;we;(the) real world
#s3	Especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual.
#c3	decision making;whatever tasks;text data;many real world problems;data mining;we;other kinds;data
#s4	So a more general picture would be to include non text data as well.
#c4	a more general picture;non text data
#s5	And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining.
#c5	this reason;we;joint mining;text;non text data;this course;we;text mining
#s6	But we can also touch how to join the analysis of both text data and non-text data.
#c6	we;the analysis;both text data;non-text data
#s7	With this problem definition we can now look at the landscape of the topics in text mining analytics.
#c7	this problem definition;we;the landscape;the topics;text mining analytics
#s8	Now this slide shows the process of generating text data in more detail.
#c8	this slide;the process;text data;more detail
#s9	Most specifically, human sensor or human observer would look at the world from some perspective.
#c9	human sensor;human observer;the world;some perspective
#s10	Different people would be looking at the world from different angles and they will pay attention to different things.
#c10	Different people;the world;different angles;they;attention;different things
#s11	The same person at a different time might also pay attention to different aspects of the observed world.
#c11	The same person;a different time;attention;different aspects;the observed world
#s12	And so the human sensor would perceive the world from some perspective.
#c12	the human sensor;the world;some perspective
#s13	And that human...
#c13	And that human
#s14	The sensor would then form a view of the world and that can be called the observed world.
#c14	The sensor;a view;the world;the observed world
#s15	Of course this would be different from the real world because of the perspective that the person has taken.
#c15	the real world;the perspective;the person
#s16	This can often be biased also.
#s17	Now the observable world can be represented as for example entity relation graphs or more in a more general way, using knowledge representation language.
#c17	the observable world;example;entity relation graphs;a more general way;knowledge representation language
#s18	But in general, this is basically what a person has in mind about the world, and we don't really know what exactly it looks like, of course.
#c18	what;a person;mind;the world;we;what;it;course
#s19	But then the human would express what the person has observed using a natural language such as English, and the result is text data.
#c19	the human;what;the person;a natural language;English;the result;text data
#s20	Of course, the person could have used a different language to express what he or she has observed.
#c20	the person;a different language;what;he;she
#s21	In that case, we might have text data of mixed languages for different languages.
#c21	that case;we;text data;mixed languages;different languages
#s22	So the main goal of text mining is actually to revert this process of generating test data.
#c22	the main goal;text mining;this process;test data
#s23	And we hope to be able to uncover some aspect in this process.
#c23	we;some aspect;this process
#s24	And so specifically we can think about the mining, for example, knowledge about the language.
#c24	we;the mining;example;the language
#s25	And that means by looking at text data in English, we may be able to discover something about English...
#c25	text data;English;we;something;English
#s26	Some usage of English...
#c26	Some usage;English
#s27	Some patterns of English.
#c27	Some patterns;English
#s28	So this is 1 type of mining problems where the result is some knowledge about language which may be useful in various ways.
#c28	1 type;mining problems;the result;some knowledge;language;various ways
#s29	If you look at the picture, we can also "then mine knowledge about the ""Observed" "World"".
#c29	you;the picture;we;the ""Observed" "World
#s30	"
#s31	As so, this has much to do with mining the content of text data.
#c31	the content;text data
#s32	We're going to look at the what the text data are about and then try to get the essence of it.
#c32	We;what;the text data;the essence;it
#s33	Or extracting high quality information about a particular aspect of the world that we're interested in.
#c33	high quality information;a particular aspect;the world;we
#s34	For example, everything that has been said about a particular person or particular entity, and this can be regarded as mining content to describe the observed world in the user's mind, in the person's mind.
#c34	example;everything;a particular person;particular entity;mining content;the observed world;the user's mind;the person's mind
#s35	If you look further then you can also imagine we can mine knowledge about this observer himself or herself.
#c35	you;you;we;knowledge;this observer;himself;herself
#s36	So this has also to do with using text data to infer some properties of this person.
#c36	text data;some properties;this person
#s37	And these properties could include the mood of the person or sentiment of the person.
#c37	these properties;the mood;the person;sentiment;the person
#s38	And note that we distinguish the observed the world from the person because text data can describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer.
#c38	we;the observed the world;the person;text data;what;the person;an objective way;the description;sentiment;you;the text data;some factual descriptions;the world;some subjective comments;it;text mining;mine knowledge;the observer
#s39	Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right?
#c39	you;the picture;the left side;this picture;you;we;something;the real world
#s40	So indeed we can do text mining to infer other real world variables, and this is often called predictive analytics.
#c40	we;text mining;other real world variables;predictive analytics
#s41	And we want to predict the value of certain interesting variables.
#c41	we;the value;certain interesting variables
#s42	So this picture basically covered multiple types of knowledge that we can mine from text in general.
#c42	this picture;multiple types;knowledge;we;text
#s43	When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction.
#c43	we;other real world variables;we;the results;mining text data;intermediate results;the prediction
#s44	For example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world.
#c44	example;we;the content;text data;we;some summary;content;that summary;us;the variables;the real world
#s45	Now of course, this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important.
#c45	course;the original text data;I;the processing;text data;some features;the prediction
#s46	And that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction.
#c46	we;the results;some other mining tasks;the content;text data and mining knowledge;the observer;prediction
#s47	In fact, when we have a non-text data, we could also use the non-text data to help prediction.
#c47	fact;we;a non-text data;we;the non-text data;prediction
#s48	And of course, it depends on the problem.
#c48	course;it;the problem
#s49	In general, non-text data can be very important for such prediction tasks.
#c49	general, non-text data;such prediction tasks
#s50	For example, if you want to predict the stocks.
#c50	example;you;the stocks
#s51	Stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables.
#c51	Stock prices;changes;stock prices;discussion;the news articles;social media;an example;text data;some other real world variables
#s52	Now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction.
#c52	this case;the historical stock price data;this prediction;example;non-text data;the prediction;we;both kinds;data;the prediction
#s53	Now non-text data can be also useful for analyzing text by supplying context.
#c53	non-text data;text;context
#s54	When we look at the text data alone will be mostly looking at the content and opinions expressed in text.
#c54	we;the text data;the content;opinions;text
#s55	But text data generally have also context associated.
#c55	text data
#s56	For example, the time, the location, of that associated with the text data and these are useful context information.
#c56	example;the location;the text data;useful context information
#s57	And the context can provide interesting angles for analyzing text data.
#c57	the context;interesting angles;text data
#s58	For example, we might partition text data into different time periods because of the availability of time.
#c58	example;we;text data;different time periods;the availability;time
#s59	Now we can analyze text data in each time period and then make a comparison.
#c59	we;text data;each time period;a comparison
#s60	Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios.
#c60	we;text data;locations;any metadata;interesting comparison scenarios
#s61	So in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data.
#c61	this sense;non-text data;interesting angles;perspectives;text analysis;us;context sensitive analysis;content;the language usage;the opinions;the observer;the authors;text data
#s62	We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics.
#c62	We;the sentiment;different context;fairly general landscape;the topics;text mining;analytics
#s63	In this course we're going to selectively cover some of those topics.
#c63	this course;we;those topics
#s64	We actually hope to cover most of these general topics.
#c64	We;these general topics
#s65	First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining.
#c65	we;natural language processing;text data;we;text;text mining
#s66	Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language.
#c66	we;word associations;text data;word associations;a form;useful lexical knowledge;a language
#s67	Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content.
#c67	we;the topic mining;analysis;only one way;content;text;it;a very useful way;content
#s68	It's also one of the most useful techniques in text mining.
#c68	It;the most useful techniques;text mining
#s69	And then we're going to talk about opinion mining and sentiment analysis.
#c69	we;opinion mining and sentiment analysis
#s70	So this can be regarded as one example of mining knowledge about the observer.
#c70	one example;mining knowledge;the observer
#s71	And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data.
#c71	we;a text based prediction problems;we;some real world variable;text data
#s72	So this slide also serves as a road map for this course.
#c72	this slide;a road map;this course
#s73	And will use this as outline for the topics that will cover in the rest of this course.
#c73	outline;the topics;the rest;this course
410	acd44eef-f129-496a-adca-4e46d40b0a44	123
#s1	In this lecture we are going to talk about how to improve the instantiation of the vector space model.
#c1	this lecture;we;the instantiation;the vector space model
#s2	This is the continued discussion of the vector space model.
#c2	the continued discussion;the vector space model
#s3	We're going to focus on how to improve the instantiation of this model.
#c3	We;the instantiation;this model
#s4	In the previous lecture, you have seen that with simple same instantiations of the vector space model.
#c4	the previous lecture;you;simple same instantiations;the vector space model
#s5	We can come up with a simple scoring function that would give us basically a count of how many unique query terms of matching the document.
#c5	We;a simple scoring function;us;a count;how many unique query terms;the document
#s6	We also have seen that this function has a problem.
#c6	We;this function;a problem
#s7	As shown on this slide, in particular, if you look at these three documents, they will all get the same score because they matched 3 unique query words.
#c7	this slide;you;these three documents;they;the same score;they;3 unique query words
#s8	But intuitively we would like D4 to be ranked above D3 and D2 is really non relevant.
#c8	we;D4;D3;D2
#s9	So the problem here is that this function could not capture.
#c9	the problem;this function
#s10	At the following heuristics: First, we would like to give more credit to D4 because it matched the presidential more times than these three; Second, Intuitively, matching presidential should be more important than matching about because about this very common word that occurs everywhere.
#c10	the following heuristics;we;more credit;D4;it;the presidential;Second;this very common word
#s11	It doesn't really carry that much content.
#c11	It;that much content
#s12	So in this natural, let's see how we can improve the model to solve these two problems.
#c12	's;we;the model;these two problems
#s13	It's worth thinking at this point about the """why do we have these two problems?"
#c13	It;this point;we;these two problems
#s14	"" If we look back at assumptions we have made while instantiating the vector space model, we will realize that the problem is really coming from some of the assumptions.
#c14	we;assumptions;we;the vector space model;we;the problem;the assumptions
#s15	In particular, it has to do with how we place the vectors in the vector space.
#c15	it;we;the vectors;the vector space
#s16	So then, naturally, in order to fix these problems, we have to revisit those assumptions.
#c16	order;these problems;we;those assumptions
#s17	Perhaps we will have to use different ways to instantiate the vector space model.
#c17	we;different ways;the vector space model
#s18	In particular, we have to place the vectors in a different way.
#c18	we;the vectors;a different way
#s19	So let's see how can improve this.
#c19	's
#s20	One natural thought is, in order to consider multiple times of the term in the document, we should consider the term frequency instead of just absence or presence.
#c20	One natural thought;order;multiple times;the term;the document;we;the term frequency;just absence;presence
#s21	In order to consider the difference between a document where aquarium occurred multiple times and one where the query term occurs just once, we have to consider the term frequency:
#c21	order;the difference;a document;aquarium;the query term;we;the term frequency
#s22	The count of a term in the document.
#c22	The count;a term;the document
#s23	In the simplest model, we only model the presence and absence of the time.
#c23	the simplest model;we;the presence;absence;the time
#s24	We ignore the actual number of times that term occurs in the document.
#c24	We;the actual number;times;that term;the document
#s25	So let's add this back so we can do then represent a document by a vector with term frequency as elements.
#c25	's;we;a document;a vector;term frequency;elements
#s26	So that is to say, now the elements of both queries vector and document vector will not be 0 or 1s, but instead they will be the counts of word in the query or the document.
#c26	the elements;both queries vector and document vector;1s;they;the counts;word;the query;the document
#s27	So this would bring in additional information about the document.
#c27	additional information;the document
#s28	So this can be seen as a more accurate representation of our documents.
#c28	a more accurate representation;our documents
#s29	So now let's see what the formula would look like if we change this representation.
#c29	's;what;the formula;we;this representation
#s30	So as you see on this slide we still use DOT product, and so the formula looks very similar in the form.
#c30	you;this slide;we;DOT product;the formula;the form
#s31	In fact it looks identical, but inside the sum of course Xi and Yi are now different, and all the counts of word I in the query and in the document.
#c31	fact;it;the sum;course;Xi;Yi;all the counts;word;I;the query;the document
#s32	Now at this point I also suggest you to pause the lecture for a moment and just think about how we can interpret the score of this new function.
#c32	this point;I;you;the lecture;a moment;we;the score;this new function
#s33	It's doing something very similar to what the simplest VSM is doing.
#c33	It;something;what;the simplest VSM
#s34	But because of the change of the vector, now the new score has a different interpretation.
#c34	the change;the vector;the new score;a different interpretation
#s35	Can you see the difference?
#c35	you;the difference
#s36	And it has to do with the consideration of multiple occurrences of the same term in a document.
#c36	it;the consideration;multiple occurrences;the same term;a document
#s37	More important, they would like to know whether this would fix the problems of the simplest of vector space model.
#c37	they;the problems;vector space model
#s38	So let's look at this example again.
#c38	's;this example
#s39	So suppose we change the vector representation into term frequency vectors.
#c39	we;the vector representation;term frequency vectors
#s40	Now let's look at these three documents again.
#c40	's;these three documents
#s41	The query vector is the same because all these words occur exactly once in the query, so the vector is 001 vector.
#c41	The query vector;all these words;the query;the vector;001 vector
#s42	And in fact, D2 is also essentially representing the same way, because none of these words has been repeated many times as a result of the score is also the same, still 3.
#c42	fact;D2;the same way;none;these words;a result;the score
#s43	The same goes for D3.
#c43	D3
#s44	And we still have a 3.
#c44	we
#s45	But D4 would be different.
#c45	D4
#s46	Because now presidential occured twice here.
#s47	So the element for presidential in the document factor would be 2 instead of 1.
#c47	the element;the document factor
#s48	As a result, now the score for D4 is high.
#c48	a result;the score;D4
#s49	It's 4 now.
#c49	It
#s50	So this means, by using term frequency we can now rank D4 above D2 and D3 as we hope to.
#c50	term frequency;we;D4;D2;D3;we
#s51	So this solved the problem with D4.
#c51	the problem;D4
#s52	But we can also see that, D2 and D3 are still treated in the same way.
#c52	we;D2;D3;the same way
#s53	They still have identical scores, so it did not fix the problem here.
#c53	They;identical scores;it;the problem
#s54	So how can we fix this problem?
#c54	we;this problem
#s55	Intuitively, we would like to give more credit for matching presidential than matching about, but how can we solve the problem in a general way?
#c55	we;more credit;we;the problem;a general way
#s56	Is there any way to determine which word should be treated more importantly, and which word can be basically ignored about is such a word at which does not really carry that much content?
#c56	any way;which word;which word;such a word;that much content
#s57	We can essentially ignore that we sometimes call such a word stop word.
#c57	We;we;such a word;word
#s58	Those are generally very frequently occur everywhere matching it doesn't really mean anything, but computationally.
#c58	it;anything
#s59	How can we capture that?
#c59	we
#s60	So again, I encourage you to think a little bit about this.
#c60	I;you
#s61	Can you come up with any statistical approaches to somehow distinguish presidential from about?
#c61	you;any statistical approaches
#s62	If you think about it for a moment, you will realize that what differences that words like about occurs everywhere.
#c62	you;it;a moment;you;what differences;words
#s63	So if you count the occurrence of the word in the whole collection, then we would see that about has much higher frequency than presidential which tends to occur only in some documents.
#c63	you;the occurrence;the word;the whole collection;we;much higher frequency;some documents
#s64	So this idea suggest that we could somehow use global statistics of terms or some other information to try to down weight the element that for about in the vector representation of D2.
#c64	this idea;we;global statistics;terms;some other information;weight;the element;the vector representation;D2
#s65	At the same time, we hope to somehow increase the weight of presidential in the vector of these three.
#c65	the same time;we;the weight;the vector
#s66	If we can do that, then we can expect the D2 will get the overall score to be less than 3.
#c66	we;we;the D2;the overall score
#s67	While D3 will get the score above 3, then we would be able to rank this lead on top of D2.
#c67	D3;the score;we;this lead;top;D2
#s68	So how can we do this systematically?
#c68	we
#s69	Again, we can rely on some statistical counts, and in this case the particular idea is called the inverse document frequency.
#c69	we;some statistical counts;this case;the particular idea;the inverse document frequency
#s70	We have seen document frequency as one signal used in the modern retrieval functions.
#c70	We;document frequency;one signal;the modern retrieval functions
#s71	We discussed this in previous lecture, so here's a specific way of using it.
#c71	We;previous lecture;a specific way;it
#s72	Document frequency is the count of documents that contain a particular term.
#c72	Document frequency;the count;documents;a particular term
#s73	Here we said inverse document frequency because we actually want to reward a word that doesn't occur in many documents.
#c73	we;inverse document frequency;we;a word;many documents
#s74	And so the way to incorporate this into our vector representation is then to modify the frequency count.
#c74	the way;our vector representation;the frequency count
#s75	By multiplying it by the idea of the corresponding word as shown here.
#c75	it;the idea;the corresponding word
#s76	If we can do that, then we can penalize common words which generally have a low IDF and reward rare words which will have high IDF.
#c76	we;we;common words;a low IDF;rare words;high IDF
#s77	So more specifically, the IDF can be defined as a logarithm of (M + 1) / K, where M is the total number of documents in the collection, K is the DF or document frequency, the total number of documents containing the word
#c77	the IDF;a logarithm;M;+;/ K;M;the total number;documents;the collection;K;the DF;document frequency;documents;the word
#s78	W. Now if we plot this function by varying k, then you will see the curve would look like this.
#c78	we;this function;k;you;the curve
#s79	In general, you can see it would give a higher value for a low DF word, a rare word.
#c79	you;it;a higher value;a low DF word;a rare word
#s80	You can also see the maximum value of this function is log of M + 1.
#c80	You;the maximum value;this function;log;M
#s81	Will be interesting for you to think about what's minimum value for this function?
#c81	you;what;minimum value;this function
#s82	This could be interesting exercise.
#c82	interesting exercise
#s83	Now a specific function may not be as important as the heuristic to simply penalize popular terms.
#c83	a specific function;the heuristic;popular terms
#s84	But it turns out that this particular function form has also worked very well.
#c84	it;this particular function form
#s85	Now, whether there is a better form of function here is still open research question, but it's also clear that if we use a linear panelization like what's shown here with this line, then it may not be as reasonable as the standard IDF.
#c85	a better form;function;open research question;it;we;a linear panelization;what;this line;it;the standard IDF
#s86	In particular, you can see the difference.
#c86	you;the difference
#s87	In the standard IDF, and we somehow have a turning point here.
#c87	the standard IDF;we;a turning point
#s88	After this point they were gonna say these terms are essentially not very useful.
#c88	this point;they;these terms
#s89	They can be essentially ignored and this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important, and it's basically a common term.
#c89	They;sense;the term;'s;a term;more than 50%;the documents;the term;it;a common term
#s90	It's not very important to match this word, so with the standard idea you can see it's basically assuming that they all have lower weights.
#c90	It;this word;the standard idea;you;it;they;lower weights
#s91	There's no difference.
#c91	no difference
#s92	But if you look at the linear panelization at this point that there is some difference.
#c92	you;the linear panelization;this point;some difference
#s93	So intuitively we want to focus more on the discrimination of Low DF words rather than these common words.
#c93	we;the discrimination;Low DF words;these common words
#s94	Of course, which one works better still has to be validated by using the empirical created dataset and we have to use users to judge which results are better.
#c94	one;the empirical created dataset;we;users;which results
#s95	So now let's see how this can solve problem 2.
#c95	's;problem
#s96	Alright, so now let's look at the two documents again.
#c96	's;the two documents
#s97	Now, without the IDF weighting before, we just have term frequency vectors, but with IDF weighting, we now can adjust the TF weight by multiplying with the IDF value.
#c97	the IDF weighting;we;term frequency vectors;IDF weighting;we;the TF weight;the IDF value
#s98	For example, here we can see is adjustment, and in particular for about there is adjustment by using the IDF  value of about which is smaller than the IDF value of presidential.
#c98	example;we;adjustment;adjustment;the IDF  value;the IDF value
#s99	So if you look at these, the IDF will distinguishing these two words as a result of adjustment here would be larger, would make this weight larger.
#c99	you;the IDF;these two words;a result;adjustment;this weight
#s100	So if we score with these new vectors, then what would happen is that of course they share the same weights for news and campaign.
#c100	we;these new vectors;what;course;they;the same weights;news;campaign
#s101	But the matching of about and presidential with discriminate them.
#c101	But the matching;them
#s102	So now as a result of IDF weighting, we will have D3 to be ranked above D2 becauses it matched rare word whereas D2 matched common word.
#c102	a result;IDF weighting;we;D3;D2 becauses;it;rare word;D2;common word
#s103	So this shows that the IDF weighting can solve problem 2.
#c103	the IDF weighting;problem
#s104	So how effective is this model in general?
#c104	this model
#s105	When we use TF IDF weighting, let's look at the obvious documents that we have seen before nicely.
#c105	we;TF IDF weighting;'s;the obvious documents;we
#s106	These are the new scores of the new documents, but how effective is this new weighting method and new scoring function?
#c106	the new scores;the new documents;this new weighting method;new scoring function
#s107	So now let's see overall how effective is this new ranking function with TF IDF weighting?
#c107	's;this new ranking function;TF IDF weighting
#s108	Here we show all the five documents that we have seen before, and these are their scores.
#c108	we;all the five documents;we;their scores
#s109	Now we can see the scores for the first 4 documents here seem to be quite reasonable.
#c109	we;the scores;the first 4 documents
#s110	They are as we expected.
#c110	They;we
#s111	However, we also see a new problem.
#c111	we;a new problem
#s112	Because Now D5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score.
#c112	a very high score;our simplest vector space model;a very high score
#s113	In fact, it has the highest score here.
#c113	fact;it;the highest score
#s114	So this creates a new problem.
#c114	a new problem
#s115	This is actually a common phenomenon in designing retrieval functions.
#c115	a common phenomenon;retrieval functions
#s116	Basically, when you try to fix one problem, you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function.
#c116	you;one problem;you;other problems;it;effective ranking function;what;the best ranking function
#s117	Is there open research questions researchers are still working on that?
#c117	open research questions;researchers
#s118	But in the next few lectures, we'll are also gonna talk about some additional ideas to further improve this model and try to fix this problem.
#c118	the next few lectures;we;some additional ideas;this model;this problem
#s119	So to summarize this lecture we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TF IDF weighting.
#c119	this lecture;we;the vector space model;we;the instantiation;the vector space model;TF IDF weighting
#s120	So the improvement mostly is on the placement of the vector where we give higher weight to a term that occured many times in the document but infrequently in the whole collection.
#c120	the improvement;the placement;the vector;we;higher weight;a term;the document;the whole collection
#s121	And we have seen that this improvement model indeed works better than the simplest vector space model.
#c121	we;this improvement model;the simplest vector space model
#s122	But it also still has some problems.
#c122	it;some problems
#s123	In the next lecture, we're going to look at the how to address these additional problems.
#c123	the next lecture;we;these additional problems
410	ad71da51-7567-4149-9eef-0139aad79369	22
#s1	so now let's take a look at the specific method that's based on regression now this is one of the many different methods and in fact it's one of the simplest methods i choose this to explain the idea because it's simple so in this approach we simply assume that the relevance of the document with respect to the query is related to a linear combination of all the features here i used X I two in note the feature so X I of Q M D is a feature and we can have as many features as we would like and we assume that these features can be combined in the linear manner each video is controlled by a parameter here and this parizer parameter that's a waiting parameter a larger value with me the feature would have highway tan with contribute more through the scoring function the specific form of the function actually also involves a transformation of the probability of relevance
#c1	's;a look;the specific method;regression;the many different methods;fact;it;the simplest methods;i;the idea;it;this approach;we;the relevance;the document;respect;the query;a linear combination;all the features;i;I;note;the feature;I;Q M D;a feature;we;as many features;we;we;these features;the linear manner;each video;a parameter;this parizer parameter;a waiting parameter;a larger value;me;the feature;highway tan;the scoring function;the specific form;the function;a transformation;the probability;relevance
#s2	so this is a probability of relevance we know that the probability of relevance is within the range from zero to one and we could have just assumed the scoring function is related to this linear combination so we can do linear regression
#c2	a probability;relevance;we;the probability;relevance;the range;we;the scoring function;this linear combination;we;linear regression
#s3	but then the value of this linear combination could easy to go beyond one so this transformation here would map zero two one range through the whole range of real values you can you can verify it by yourself so this allows us meant to connect with the probability of relevance which is between zero and one to a linear combination of arbitrary features and if we rewrite this into a probability function will get the next one so on this equation that we will have the probability of relevance an on the right hand side it will have this form now this form is clearly non active and it still involves the linear combination of features
#c3	the value;this linear combination;this transformation;the whole range;real values;you;you;it;yourself;us;the probability;relevance;a linear combination;arbitrary features;we;a probability function;this equation;we;the probability;relevance;the right hand side;it;this form;this form;it;the linear combination;features
#s4	and it's also clear that is if this value is this is actually negative of the linear combination in the question above if this value here if this value is large than it would mean this value is small and therefore this probability is whole probability would be large and that's what we expect basically it would mean if this combination gives us a high value then the documents more likely relevant so this is our hypothesis again this is not necessarily the best of hypothesis
#c4	it;this value;the linear combination;the question;this value;it;this value;this probability;whole probability;what;we;it;this combination;us;a high value;the documents;our hypothesis;hypothesis
#s5	but this is a simple way to connect these features with the probability of relevance so now we have this combination function the next task is to see how to estimate the parameters so that the function character will be applied without knowing the beta values it's harder to apply this function so let's see how we can estimate data values let's take a look at simple example in this example we have three features once BM twenty five score of the document and query one is the page rank score of the document which might or might not depend on the query we might have a topic sensitive page brand that would depend on query otherwise the general page rank doesn't really depend on query
#c5	a simple way;these features;the probability;relevance;we;the next task;the parameters;the function character;the beta values;it;this function;'s;we;data values;'s;a look;simple example;this example;we;three features;BM;twenty five score;the document;query;the page rank score;the document;the query;we;a topic sensitive page brand;query;the general page rank;query
#s6	and then we have P M twenty five score on the anchor text of the document these are then the feature values for a particular top document query pair an in this case the document is D one and the judgment says that is relevant here's another training instance and with these feature values but in this case it's not relevant
#c6	we;P M twenty five score;the anchor text;the document;the feature values;a particular top document query pair;this case;the document;D;the judgment;another training instance;these feature values;this case;it
#s7	OK
#s8	this is over simplified case where we just have two instances
#c8	simplified case;we;two instances
#s9	but it's sufficient illustrated the point so what we can do is we use the maximum micro estimator to actually estimate the parameters basically we're going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here can we predict the the relevance
#c9	it;the point;what;we;we;the maximum micro estimator;the parameters;we;the relevance status;the document;the feature values;we;these feature values;we;the the relevance
#s10	yeah
#s11	and of course the prediction will be using this function that you see here
#c11	course;the prediction;this function;you
#s12	and we hypothesize that the probability of relevance is related to features in this way so we are going to see for what values of beta we can predict the relevance
#c12	we;the probability;relevance;features;this way;we;what values;beta;we;the relevance
#s13	well what do we mean well what do we mean by predicting the relevance
#c13	what;we;what;we;the relevance
#s14	well well we just mean in the first case for DVR this expression here right here should give a high values in fact that we hope this to give a value closely one why becaus this is relevant document on the other hand in the second case for D two we hope this value will be small becaus it's nonrandom the document so now let's see how this can be mathematical expressed this is similar to expressing the probability of document only that we're not talking about the probability of words but talking about probability of relevance one or zero so what's the probability of this document the relevant if it has these feature values well this is just this expression right we just need to plug in the excise so that's what we will get it's exactly like a what we have seen about only that we replaced these excise with now specific values i saw for example this point seven goes to here and this point eleven close to here and these are different feature values then we combine them in this particular way the better values are still unknown but this gives us the probability that this document is relevant if we assume such a model
#c14	we;the first case;DVR;this expression;a high values;fact;we;a value;relevant document;the other hand;the second case;D;we;this value;small becaus;it;nonrandom;the document;'s;the probability;document;we;the probability;words;probability;relevance;what;the probability;this document;it;these feature values;just this expression;we;the excise;what;we;it;what;we;we;these excise;now specific values;i;example;this point;different feature values;we;them;this particular way;the better values;us;the probability;this document;we;such a model
#s15	OK we want to maximize this probability since this is a random in the document what do we do for the second document well we want to compute the probability that the predictions is not relevant so this would mean we have to compute one minus this expression since this expression is actually the probability of relevance so to compute the non relevance from relevance we just one minus the probability of relevance
#c15	we;this probability;a random;the document;what;we;the second document;we;the probability;the predictions;we;this expression;this expression;the probability;relevance;the non relevance;relevance;we;relevance
#s16	OK so this whole expression then just is our probability of predicting these two relevance values one is one here one is zero and this quick question is our probability of observing a one here an offer zero here of course this probability depends on the beta values
#c16	this whole expression;our probability;these two relevance values;this quick question;our probability;a one;an offer;course;this probability;the beta values
#s17	right
#s18	so then our goal is to adjust the beta values to make this whole thing which is maximum make the as large as possible so that means we're going to compute this the beta is just the the parameter values that would maximize this whole like hold expression and what that means is if you look at the function is working to choose battles to make this as large as possible and make this also as large as possible which is equivalent to say make this the other as small as possible
#c18	our goal;the beta values;this whole thing;we;the beta;just the the parameter values;expression;what;you;the function;battles
#s19	and this is precisely what we want
#c19	precisely what;we
#s20	so once we do the training now we will know the beta values
#c20	we;the training;we;the beta values
#s21	so then this function would be well defined once better values are known propose this and this would be completed this best file so for any new query an new documents we can simply compute the features for that pair
#c21	this function;once better values;this best file;any new query;we;the features;that pair
#s22	and then we just use this formula to generate the ranking scope and this scoring function can be used to rank documents for a particular query so that's the basic idea of learning to rank
#c22	we;this formula;the ranking scope;this scoring function;documents;a particular query;the basic idea
410	ae943406-56a1-4b3b-9515-637ea27e438a	114
#s1	This lecture is a summary of this whole course.
#c1	This lecture;a summary;this whole course
#s2	First, let's revisit the topics that we covered in this course.
#c2	's;the topics;we;this course
#s3	In the beginning, we talked about the natural language processing and how it can enrich text representation.
#c3	the beginning;we;the natural language processing;it;text representation
#s4	We then talked about how to mine knowledge about the language, natural language used to express what's observed in the world of text data.
#c4	We;knowledge;the language;natural language;what;the world;text data
#s5	And in particular, we talked about how to mine word associations.
#c5	we;word associations
#s6	We then talked about how to analyze topic syntax, how to discover topics, and analyze them.
#c6	We;topic syntax;topics;them
#s7	This can be regarded as knowledge about the oberved of the world, and then we talked about how to mine knowledge about the observer and particularly talk about how to mine opinions and do sentiment analysis.
#c7	knowledge;the oberved;the world;we;knowledge;the observer;mine opinions;analysis
#s8	And finally, we talked about the text based prediction, which has to do with predicting values of other real world variables based on text data.
#c8	we;the text based prediction;values;other real world variables;text data
#s9	And in discussing this, we also discussed the rule of non-text data which can contribute to additional predictors for the prediction problem and also it can provide a context for analyzing text data.
#c9	we;the rule;non-text data;additional predictors;the prediction problem;it;a context;text data
#s10	And in particular, we talked about how to use context to analyze topics.
#c10	we;context;topics
#s11	So, here are the key high-level takeaway messages from this course.
#c11	the key high-level takeaway messages;this course
#s12	I'm gonna go over each of these major topics and point out what are the key takeaway messages that you should remember.
#c12	I;these major topics;what;you
#s13	First in NLP and text representation, you should realize that NLP is always very important for any text applications because it enriches text representation the more NLP, better text representation we can have and this further enables more accurate knowledge discovery, to discover deeper knowledge buried in text.
#c13	NLP;text representation;you;NLP;any text applications;it;text representation;the more NLP;better text representation;we;more accurate knowledge discovery;deeper knowledge;text
#s14	However, the current state of the art of natural language processing is still not robust enough, and so as a result, the robust text mining technologies today tend to be based on word representation and tend to rely a lot on statistical analysis, as we have discussed in this course.
#c14	the current state;the art;natural language processing;a result;the robust text mining technologies;word representation;a lot;statistical analysis;we;this course
#s15	You may recall we mostly used word based representations, and we've relied a lot on statistical techniques,  on statistical learning techniques particularly.
#c15	You;we;word based representations;we;a lot;statistical techniques;statistical learning techniques
#s16	In Word Association Mining and analysis,  the important points are first are we introduced the two concepts for two basic plan complementary relations of words, paradigmatic and syntagmatic relations.
#c16	Word Association Mining;analysis;the important points;we;the two concepts;two basic plan complementary relations;words;paradigmatic and syntagmatic relations
#s17	These are actually very general relations between elements in any sequences.
#c17	very general relations;elements;any sequences
#s18	If you take it as meaning elements that occur in similar context in the sequence and elements that tend to co-occur with each other and these relations might also be meaningful for other sequences of data.
#c18	you;it;meaning;elements;similar context;the sequence;elements;these relations;other sequences;data
#s19	We also talked a lot about the text similarity when we discuss how to discover paradigmatically relations, we compare their context of words, discover words that share similar contexts.
#c19	We;the text similarity;we;relations;we;their context;words;words;similar contexts
#s20	At that point that we talked about, representing text data with a vector space model, and we talked about some retrieval techniques such as BM25 for measuring similarity of text and for assigning weights to terms, TF-IDF weighting, etc.
#c20	that point;we;text data;a vector space model;we;some retrieval techniques;BM25;similarity;text;weights;terms;TF-IDF weighting
#s21	And this part is well connected to text retrieval.
#c21	this part;text retrieval
#s22	There are other techniques that can be relevant here also.
#c22	other techniques
#s23	The next point that is about the co- occurrence analysis of text
#c23	The next point;the co- occurrence analysis;text
#s24	and we introduced some information theory concepts such as entropy, conditional entropy and mutual information.
#c24	we;some information theory concepts;entropy;conditional entropy;mutual information
#s25	These are not only very useful for measuring the co-occurrences of words, and they're also very useful for analyzing other kind of data, and they're useful for example for feature selection in text categorization as well.
#c25	the co;-;occurrences;words;they;other kind;data;they;example;feature selection;text categorization
#s26	So this is another important concept to know.
#c26	another important concept
#s27	And then we talked about the topic mining and analysis and that's where we introduced the probabilistic topic model.
#c27	we;the topic mining;analysis;we;the probabilistic topic model
#s28	We spend a lot of time to explain the basic topic model PLSA in detail.
#c28	We;a lot;time;the basic topic model PLSA;detail
#s29	And this is also the basis for understanding LDA, which is a theoretically more appealing model.
#c29	the basis;LDA;a theoretically more appealing model
#s30	But we did not have enough time to really go in depth in introducing LDA.
#c30	we;enough time;depth;LDA
#s31	But in practice, PLSA seems as effective as LDA, and it's simpler to implement.
#c31	practice;PLSA;LDA;it
#s32	It's also more efficient.
#c32	It
#s33	In this part we also introduce some general concepts that would be useful to know.
#c33	this part;we;some general concepts
#s34	One is generating model and this is a general method for modeling text data and modeling other kinds of data as well.
#c34	model;a general method;text data;other kinds;data
#s35	And we talked about the maximum likelihood estimator and the EM algorithm for solving the problem of computing maximum likelihood estimator.
#c35	we;the maximum likelihood estimator;the EM algorithm;the problem;maximum likelihood estimator
#s36	So these are all general techniques that tends to be very useful in other scenarios as well.
#c36	general techniques;other scenarios
#s37	Then we talked about the text clustering and text categorization.
#c37	we;the text clustering;text categorization
#s38	Those are two important building blocks in any text mining application systems.
#c38	two important building blocks;any text mining application systems
#s39	In text clustering we talked about the, how we can solve the problem by using a slightly different than mixture model than the probabilistic topic model.
#c39	text clustering;we;we;the problem;mixture model;the probabilistic topic model
#s40	And we then also briefly reviewed some similarity based approaches to text clustering.
#c40	we;some similarity based approaches;text clustering
#s41	In categorization, we also talked about the two kinds of approaches.
#c41	categorization;we;the two kinds;approaches
#s42	One is generative classifiers, they rely on base rule to infer the conditional probability of a category given text data.
#c42	generative classifiers;they;base rule;the conditional probability;a category;text data
#s43	In particular, we introduced Naive Bayes in detail.
#c43	we;Naive Bayes;detail
#s44	This is a practical, useful technique for a lot of text categorization tasks.
#c44	a practical, useful technique;a lot;text categorization tasks
#s45	We also briefly introduce some discriminative classifiers, particularly logistical regression K nearest neighbor and SVN.
#c45	We;some discriminative classifiers;particularly logistical regression K nearest neighbor;SVN
#s46	There also very important that they are very popular and they're very useful for text categorization as well.
#c46	they;they;text categorization
#s47	In both parts we also discussed how to evaluate the results, and evaluation is quite important because if the measures that you use don't really reflect the utility of the method, then it would give you misleading results.
#c47	both parts;we;the results;evaluation;the measures;you;the utility;the method;it;you;misleading results
#s48	So it's very important to get evaluation right
#c48	it;evaluation
#s49	and we can talk the about evaluation of categorisation in detail with a lot of specific measures.
#c49	we;the about evaluation;categorisation;detail;a lot;specific measures
#s50	Then we talked about the sentiment analysis and opinion mining, and that's where we introduced sentiment classification problem.
#c50	we;the sentiment analysis;opinion mining;we;sentiment classification problem
#s51	And although it's a special case of text categorization, but we talked about how to extend or improve the text categorisation method by using more sophisticated features that would be needed for sentiment analysis.
#c51	it;a special case;text categorization;we;the text categorisation method;more sophisticated features;sentiment analysis
#s52	We did the review of some commonly used the complex features for text analysis and then we also talked about how to capture the order of these categories in sentiment classification, and in particular we introduced the ordinal logistical regression.
#c52	We;the review;the complex features;text analysis;we;the order;these categories;sentiment classification;we;the ordinal logistical regression
#s53	Then we also talk about latent aspect rating analysis.
#c53	we;latent aspect rating analysis
#s54	This is unsupervised way of using a generated model to understand the review data in more detail.
#c54	unsupervised way;a generated model;the review data;more detail
#s55	In particular, it allows us to understand the decomposed ratings of reviewer on different aspects of the topic.
#c55	it;us;the decomposed ratings;reviewer;different aspects;the topic
#s56	So given text reviews with overall ratings, the method would allow us to infer the ratings on different aspects.
#c56	text reviews;overall ratings;the method;us;the ratings;different aspects
#s57	And it also allows us to infer the reviewers latent weights on these aspects on which aspects are more important to the reviewer, can be reviewed as well, and this enables a lot of interesting applications.
#c57	it;us;the reviewers;weights;these aspects;aspects;the reviewer;a lot;interesting applications
#s58	Finally, in the discussion of text based prediction, we mainly talked about the joint mining of text and non text data as they are both very important for prediction.
#c58	the discussion;text based prediction;we;the joint mining;text;non text data;they;prediction
#s59	And we particularly talked about how text data can help non text data and vice versa.
#c59	we;text data;text data
#s60	In the case of using non text data to help the text data analysis, we talked about the contextual text mining.
#c60	the case;non text data;the text data analysis;we;the contextual text mining
#s61	We introduce the contextual PLSA as a generalization or generalized model of PLSA to allow us to incorporate context variables such as time and location and this is a general way to allow us to review a lot of interesting topical patterns in text data.
#c61	We;the contextual PLSA;a generalization;generalized model;PLSA;us;context variables;time;location;a general way;us;a lot;interesting topical patterns;text data
#s62	We also introduced the net PLSA.
#c62	We;the net PLSA
#s63	In this case we use social network or network in general of text data to help analyzing topics.
#c63	this case;we;social network;network;text data;topics
#s64	And finally we talked about how time series data can be used as context to mine, potentially causal topics in text data.
#c64	we;series data;context;mine;potentially causal topics;text data
#s65	Now in the other way of using text to help, to help interpreting patterns discovered from non text data, we did not really discuss anything in detail but just provide the reference, but I should stress that that's actually very important direction to know about if you want to build a practical text mining systems, because understanding and interpreting patterns is quite important.
#c65	the other way;text;patterns;non text data;we;anything;detail;the reference;I;very important direction;you;a practical text mining systems;understanding and interpreting patterns
#s66	So this is a summary of the key takeaway messages.
#c66	a summary;the key takeaway messages
#s67	And, I hope these would be very useful to you for building any text mining applications or doing further study of these algorithms and this should provide a good basis for you to read Frontier Research papers to know about more advanced algorithms or to invent new algorithms yourself.
#c67	I;you;any text mining applications;further study;these algorithms;a good basis;you;Frontier Research papers;more advanced algorithms;new algorithms
#s68	So to know more about this topic, I would suggest you to look into other areas in more depth.
#c68	this topic;I;you;other areas;more depth
#s69	And during this short period of time of this course, we could only touch the basic concepts, basic principles of text mining and we emphasize the coverage of practical, useful algorithms, and this is at the cost of covering some more advanced algorithms only briefly, or in many cases we omitted the discussion of a lot of advanced algorithms.
#c69	this short period;time;this course;we;the basic concepts;basic principles;text mining;we;the coverage;practical, useful algorithms;the cost;some more advanced algorithms;many cases;we;the discussion;a lot;advanced algorithms
#s70	So to learn more about this subject, you should definitely and learn more about the natural language processing, because this is the foundation for all text based applications.
#c70	this subject;you;the natural language processing;the foundation;all text based applications
#s71	The more NLP you can do, the better representation of texts that you can get and then the deeper knowledge you can discover.
#c71	The more NLP;you;texts;you;you
#s72	So this is very important.
#s73	The second area that you should look into is statistical machine learning and these techniques are now the backbone techniques for not just text analysis applications, but also for NLP.
#c73	The second area;you;statistical machine learning;these techniques;the backbone techniques;not just text analysis applications;NLP
#s74	A lot of NLP techniques are nowadays actually based on supervised machine learning.
#c74	A lot;NLP techniques;supervised machine learning
#s75	So they are very important  because they are key to also understanding some advanced NLP techniques and naturally they would provide more tools for doing text analysis in general.
#c75	they;they;some advanced NLP techniques;they;more tools;text analysis
#s76	Now, a particularly interesting area called Deep Learning has attracted a lot of attention recently.
#c76	a particularly interesting area;Deep Learning;a lot;attention
#s77	It has also shown promise in many application areas, especially in speech and vision, and it has been applied to text data as well.
#c77	It;promise;many application areas;speech;vision;it;text data
#s78	So, for example, recently there has been work on using deep learning to do sentiment analysis to achieve better accuracy, and so that's one example of advanced techniques that we weren't able to cover.
#c78	example;work;deep learning;analysis;better accuracy;one example;advanced techniques;we
#s79	But that's also very important.
#s80	And the other area that has emerged in statistical learning is the word embedding technique where they can learn vector representation of words and then these vector representations would allow you to compute the similarity of words.
#c80	the other area;statistical learning;the word;technique;they;vector representation;words;these vector representations;you;the similarity;words
#s81	As you can see, this provides directly a way to discover potentially paradigmatically relations of words and results that people have got so far are very impressive.
#c81	you;a way;relations;words;results;people
#s82	That's another promising technique that we did not have time to touch.
#c82	another promising technique;we;time
#s83	But of course, whether these new techniques would lead to practical use for techniques that work much better than the current technologies, is the open question that has to be examined.
#c83	course;these new techniques;practical use;techniques;the current technologies;the open question
#s84	And no serious evaluation has been done yet in, for example, examining the practical value of word embedding other than word similarity based evaluation.
#c84	no serious evaluation;example;the practical value;word;word similarity;based evaluation
#s85	But nevertheless, these are advanced techniques that surely will make impact in text mining in the future.
#c85	advanced techniques;impact;text mining;the future
#s86	So it's very important to know more about these.
#c86	it
#s87	Statistical learning is also key to predictive modeling, which is very crucial for many big data applications.
#c87	Statistical learning;predictive modeling;many big data applications
#s88	We did not talk about that predictive modeling component, but this is mostly about the regression or categorization techniques, and this is another reason why statistical learning is important.
#c88	We;that predictive modeling component;the regression;categorization techniques;another reason;statistical learning
#s89	We also suggested you to learn more about data mining, and that's simply because general data mining algorithms can always be applied to text data which can be regarded as a special case of general data.
#c89	We;you;data mining;general data mining algorithms;text data;a special case;general data
#s90	So there are many applications of data mining techniques in particular, for example, a pattern discovery would be very useful to generate a interesting features for text analysis.
#c90	many applications;data mining techniques;example;a pattern discovery;a interesting features;text analysis
#s91	Recently, information network mining techniques can also be used to analyze text information network.
#c91	information network mining techniques;text information network
#s92	So these are all good to know in order to develop effective text analysis techniques.
#c92	order;effective text analysis techniques
#s93	And finally, we also recommend you to learn more about the text retrieval information retrieval or search engines.
#c93	we;you;the text retrieval information retrieval;search engines
#s94	And this is especially important if you're interested in building practical text data application systems.
#c94	you;practical text data application systems
#s95	And a search engine would be essential system component in any text based applications, and that's because text data are created for humans, to us to consume.
#c95	a search engine;essential system component;any text based applications;text data;humans;us
#s96	So humans are at the best position to understand the text data.
#c96	humans;the best position;the text data
#s97	It's important to have human in the loop in a big text data applications.
#c97	It;human;the loop;a big text data applications
#s98	So it can in particular help text mining systems in two ways.
#c98	it;two ways
#s99	One is to effectively reduce the data size from a large collection to a small collection with the most relevant text data that only matter for the particular application.
#c99	the data size;a large collection;a small collection;the most relevant text data;the particular application
#s100	So the other is to provide a way to annotate it, to explain patterns, and this has to do with knowledge provenance.
#c100	a way;it;patterns;knowledge provenance
#s101	Once we discover some knowledge, we have to figure out whether the discovery is really reliable and so we need to go back to the original text that are verified and that's when the search engine is very important.
#c101	we;some knowledge;we;the discovery;we;the original text;the search engine
#s102	Moreover, some techniques and information retrieval, for example BM 25, vector space and language models, also very useful for text data mining.
#c102	Moreover, some techniques;information retrieval;example;vector space and language models;text data mining
#s103	We only mention some of them, but if you know more about the text retrieval, you'll see that there are many techniques that are useful.
#c103	We;them;you;the text retrieval;you;many techniques
#s104	Another technique that's useful is indexing technique that enables quick response of search engine to users query and such techniques can be very useful for building efficient text mining systems as well.
#c104	Another technique;indexing technique;quick response;search engine;users query;such techniques;efficient text mining systems
#s105	So finally I want to remind you of this big picture for harnessing big text data that I showed you at the very beginning of the semester.
#c105	I;you;this big picture;big text data;I;you;the very beginning;the semester
#s106	So in general, to build a big text data application system, we need two kinds of techniques, text retrieval and text mining.
#c106	a big text data application system;we;two kinds;techniques;text retrieval;text mining
#s107	And text retrieval as I explained, is to help convert the big text data into a small amount of most relevant data for a particular problem, and can also help providing knowledge prominence, help interpreting patterns later.
#c107	text retrieval;I;the big text data;a small amount;most relevant data;a particular problem;knowledge prominence;patterns
#s108	Text mining has to do with further analyzing the relevant data to discover the actionable knowledge that can be directly useful for decision making or many other tasks.
#c108	Text mining;the relevant data;the actionable knowledge;decision making;many other tasks
#s109	So this course covered text mining, and there's a companion course called text retrieval and search engines that covers text retrieval.
#c109	this course;text mining;a companion course;text retrieval and search engines;text retrieval
#s110	If you haven't taken that course, it would be useful for you to take it, especially if you are interested in building a text application system and taking both courses would give you a complete set of practical skills for building such a system.
#c110	you;it;you;it;you;a text application system;both courses;you;a complete set;practical skills;such a system
#s111	So in very end I just would like to thank you for taking this course.
#c111	very end;I;you;this course
#s112	I hope you have learned useful knowledge and skills in text mining and analytics.
#c112	I;you;useful knowledge;skills;text mining;analytics
#s113	As you see from our discussions, there are a lot of application opportunities for this kind of techniques, and there are also a lot of open challenges, so I hope you can use what you have learned to build a lot of useful applications to benefit the society and to also join the research community to discover new techniques for text mining and analytics.
#c113	you;our discussions;a lot;application opportunities;this kind;techniques;a lot;open challenges;I;you;what;you;a lot;useful applications;the society;the research community;new techniques;text mining;analytics
#s114	Thank you.
#c114	you
410	b1854d1c-3199-4c42-ab7d-f219f70259a3	123
#s1	This lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis.
#c1	This lecture;the latent aspect rating analysis;opinion mining;sentiment;analysis
#s2	In this lecture, we're going to continue discussing opinion mining and sentiment analysis.
#c2	this lecture;we;opinion mining and sentiment analysis
#s3	In particular, we're going to introduce.
#c3	we
#s4	Late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings.
#c4	the aspect;rating analysis;us;detailed analysis;reviews;overall ratings
#s5	First, motivation.
#c5	First, motivation
#s6	Here are two reviews that you often see on the Internet about the Hotel and You see some overall ratings.
#c6	two reviews;you;the Internet;the Hotel;You;some overall ratings
#s7	In this case, both reviewers have given five stars.
#c7	this case;both reviewers;five stars
#s8	And of course there are also reviews that are in text.
#c8	course;reviews;text
#s9	Now, if you just look at these reviews, it's not very clear whether a hotel is good for its location or for its service, and it's also unclear why are.
#c9	you;these reviews;it;a hotel;its location;its service;it
#s10	If you are like this hotel.
#c10	you;this hotel
#s11	So what we want to do is to decompose this overall rating.
#c11	what;we;this overall rating
#s12	Into ratings on different aspects such as value, rooms, location and service.
#c12	ratings;different aspects;value;rooms;location;service
#s13	So if we can decompose overrating two ratings on these different aspects.
#c13	we;two ratings;these different aspects
#s14	Then we can obtain more detailed understanding of the reviewers opinions about the hotel.
#c14	we;more detailed understanding;the reviewers opinions;the hotel
#s15	And this would also allow us to rank hotels along different dimensions, such as valuable rooms, but in general such detailed understanding would reveal more information about the users, preferences, reviews, preferences and also we can understand better how reviewers view this hotel from different perspectives.
#c15	us;hotels;different dimensions;valuable rooms;such detailed understanding;more information;the users;preferences;reviews;preferences;we;reviewers;this hotel;different perspectives
#s16	Now, not only do we want to.
#c16	we
#s17	Infer this aspect ratings.
#c17	this aspect ratings
#s18	We also want to infer the aspect of weights, so some reviewers may care more about values as opposed to service, and that would be a case like what's shown on the left for the weight distribution where you can see a lot of weight is placed on value.
#c18	We;the aspect;weights;some reviewers;values;service;a case;what;the left;the weight distribution;you;a lot;weight;value
#s19	But others might care more about service and therefore they might place more weight on service then value.
#c19	others;service;they;more weight;service;then value
#s20	Now, the reason why this is also important that is be cause do you think about a five star on value?
#c20	the reason;you;a five star;value
#s21	It might still be very expensive if the reviewer cares a lot about service, right?
#c21	It;the reviewer;service
#s22	For this kind of service, this price is good, so the reviewer might give it a five star.
#c22	this kind;service;this price;the reviewer;it;a five star
#s23	But if reviewer really cares about the value of the hotel, then the five star most likely would mean really cheaper prices.
#c23	reviewer;the value;the hotel;the five star;really cheaper prices
#s24	So in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights.
#c24	order;the ratings;different aspects;we;these aspect weights
#s25	When they are combined together, we can have a more detailed understanding of the opinion.
#c25	they;we;a more detailed understanding;the opinion
#s26	So the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output.
#c26	the task;these reviews;their overall ratings;input;the both the aspect ratings;aspect ratings;the aspect;weights;output
#s27	And this is a problem called latent aspect rating analysis.
#c27	a problem;latent aspect rating analysis
#s28	So the task in general is given a set of review articles about the topic with overall ratings.
#c28	the task;a set;review articles;the topic;overall ratings
#s29	An we hope to generate the three things.
#c29	we;the three things
#s30	One is the major aspects comment on in the reviews.
#c30	the major aspects comment;the reviews
#s31	The second is the ratings on each aspect, such as value and room or service.
#c31	the ratings;each aspect;value;room;service
#s32	And 3rd is the relative weights placed on different aspects by the reviewers, and this task has a lot of applications.
#c32	3rd;the relative weights;different aspects;the reviewers;this task;a lot;applications
#s33	If we can do this and we would enable a lot of applications, I just listed some here and later.
#c33	we;we;a lot;applications;I
#s34	I will show you some results.
#c34	I;you;some results
#s35	And for example, we can do opinion based and the ranking.
#c35	example;we;opinion;the ranking
#s36	We can generate a aspect level opinion summary.
#c36	We;a aspect level opinion summary
#s37	We can also analyze reviewers preferences, compare them or compare their preferences on different hotels.
#c37	We;reviewers preferences;them;their preferences;different hotels
#s38	And we can do personalized recommendation of products.
#c38	we;recommendation;products
#s39	So of course the question is how can we solve this problem?
#c39	course;the question;we;this problem
#s40	Now, as in other cases of these advanced topics, we won't have time to really cover the technique in detail, but I'm going to give a press basic introduction to the technique developed for this problem.
#c40	other cases;these advanced topics;we;time;the technique;detail;I;a press basic introduction;the technique;this problem
#s41	So first we're going to talk about how to solve the problem in two stages.
#c41	we;the problem;two stages
#s42	Later, we're going to also mention that we can do this in the unified model.
#c42	we;we;the unified model
#s43	Now take this review with the overall reading as input.
#c43	this review;the overall reading;input
#s44	What we want to do is first we're going to segment the aspects.
#c44	What;we;we;the aspects
#s45	So we're going to figure out what words are talking about location in what words are talking about, the room conditioning, etc.
#c45	we;what words;location;what words;the room conditioning
#s46	So with this we would be able to obtain aspect segments.
#c46	we;aspect segments
#s47	In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C supply of WND.
#c47	we;the counts;all the words;each segment;C supply;WND
#s48	This can be done by using seed words like location and room.
#c48	seed words;location;room
#s49	Or price to retrieve the relevant the segments and then from those segments we can further mine correlated words.
#c49	Or price;the segments;those segments;we;correlated words
#s50	With these seed words and that would allow us to segment the text into segments.
#c50	these seed words;us;the text;segments
#s51	Discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation,
#c51	different aspects;course;we;we;topic models;the segmentation
#s52	But anyway, that's the first stage where we would obtain the counts of words in each segment.
#c52	the first stage;we;the counts;words;each segment
#s53	In the segmentation stage, which is called latent rating regression, we're going to use these words and their frequencies in different aspects to predict the overall rating, and this prediction happens in two stages.
#c53	the segmentation stage;latent rating regression;we;these words;their frequencies;different aspects;the overall rating;this prediction;two stages
#s54	In the first stage, we're going to use the sentiment weights of these words in each aspect to predict the aspect rating.
#c54	the first stage;we;the sentiment weights;these words;each aspect;the aspect rating
#s55	So, for example, if in the discussion of location using a word like amazing mentioned many times and it has a high weight.
#c55	example;the discussion;location;a word;it;a high weight
#s56	For example, here is 3.9.
#c56	example
#s57	Then it would increase the aspect rating for location.
#c57	it;the aspect rating;location
#s58	But another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating.
#c58	But another word;a negative weight;it;it;the rating
#s59	So the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words.
#c59	the aspect rating;a weighted combination;these word frequencies;the weights;the sentiment;the words
#s60	Now of course these sentiment weights might be different for different aspects.
#c60	course;these sentiment weights;different aspects
#s61	So we have for each aspect a set of sentiment weights.
#c61	we;each aspect;a set;sentiment weights
#s62	As shown here, and that's denoted by beta sub I and W.
#c62	beta sub;I;W.
#s63	In the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings.
#c63	the second stage;a second step;we;the overall rating;these aspect ratings
#s64	So we're going to assume we have aspect weights in order by of R sub of D.
#c64	we;we;weights;order;R sub;D.
#s65	And this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the.
#c65	a weighted average;the aspect ratings;our supply
#s66	And we can assume the overall rating is simply a weighted average of this aspect ratings.
#c66	we;the overall rating;a weighted average;this aspect ratings
#s67	So this setup allows us to predict the overall rating based on the observed word frequencies.
#c67	this setup;us;the overall rating;the observed word frequencies
#s68	So on the left side you will see all these observed information, the arts, the and the count.
#c68	the left side;you;all these observed information;the arts;the count
#s69	But on the right side you see all the information that we're interested in is actually latent.
#c69	the right side;you;all the information;we
#s70	So we hope to discover them.
#c70	we;them
#s71	Now this is a typical case of generating model where we would embed the interesting variables in the generating model.
#c71	a typical case;generating model;we;the interesting variables;the generating model
#s72	And then we're going to set up a generation probability for the overall rating given the observed words.
#c72	we;a generation probability;the overall rating;the observed words
#s73	And then of course, then we can adjust these parameter values including betas, rs, alpha i.
#c73	course;we;these parameter values;betas;rs, alpha i.
#s74	In order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document.
#c74	order;the probability;the data;this case;the observed rating;the document
#s75	And so we have seen such cases before in, for example, PLSA, where we predict the text data.
#c75	we;such cases;example;we;the text data
#s76	But here we predicting the rating and the parameters of course are also very different.
#c76	we;the rating;the parameters;course
#s77	But if you can see if we can uncover these parameters, that would be nice because also R of D is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub ID is precisely the aspect weights that we hope to get.
#c77	you;we;these parameters;R;D;the aspect ratings;we;decomposer ratings;different aspects;our sub ID;the aspect;we
#s78	As a bi product that will also get the beta vector and these are the aspects of specifica sentiment, weights of words, so more formally.
#c78	a bi product;the beta vector;the aspects;specifica sentiment;weights;words
#s79	They thought we are modeling.
#c79	They;we
#s80	Here is a set of review documents with overall ratings.
#c80	a set;review documents;overall ratings
#s81	And each review documents denoted by AT and overall rating is denoted by R sub D and these pre segmented into K as their segments and we're going to use C sub W and D and to denote the count of world W in aspect segment I.
#c81	each review documents;AT;overall rating;R sub D;these pre;K;their segments;we;C sub W;D;the count;world W;aspect segment
#s82	Of course it's zero if the world doesn't occur in the segment.
#c82	it;the world;the segment
#s83	Now the model is going to predict the rating based on the.
#c83	the model;the rating
#s84	So we are interested in the conditional probability of R sub T given D.
#c84	we;the conditional probability;R sub T;D.
#s85	And this model is set up as follows.
#c85	this model
#s86	So all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings.
#c86	a normal distribution;a mean;denotes;the average;the aspect ratings
#s87	R sub of D as shown here is normal distribution has a variance of or square.
#c87	R sub;D;normal distribution;a variance
#s88	Now of course, this is just what our assumption in the actual reading is not necessary generating this way.
#c88	course;just what;our assumption;the actual reading
#s89	But as always when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities.
#c89	we;this assumption;we;a formal way;the problem;us;interesting quantities
#s90	In this case, the aspect ratings and aspect of weights.
#c90	this case;aspect;weights
#s91	Now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just sentiment wait.
#c91	the aspect rating;you;the second line;sum;these weights;the weight;sentiment
#s92	So.
#s93	As I said, the overall rating is assumed to be a weighted average of aspect ratings.
#c93	I;the overall rating;a weighted average;aspect ratings
#s94	Now this alpha Values of a alpha sub of D together by our vector that depends on D is the document specific weights and we can assume this factor itself is drawn from another multivariate Gaussian distribution with mean denoted by a mule vector and covariance matrix Sigma,
#c94	this alpha Values;a alpha sub;D;our vector;D;the document specific weights;we;this factor;itself;another multivariate Gaussian distribution;mean;a mule vector;covariance matrix;Sigma
#s95	yeah.
#s96	Now, so this means when we generate our overall rating, we're going to first draw.
#c96	we;our overall rating;we
#s97	A set of other values from this multivariate Gaussian prior distribution and once we get these alpha values were going to use, then the weighted average of aspect ratings as the mean here to use the normal distribution.
#c97	A set;other values;this multivariate;Gaussian prior distribution;we;these alpha values;aspect ratings;the mean;the normal distribution
#s98	And to generate the overall rating.
#c98	the overall rating
#s99	Now the aspect rating as I just said is the sum of the sentiment weights of words in their spectrum.
#c99	the aspect rating;I;the sum;the sentiment weights;words;their spectrum
#s100	Note that here the sentiment weights are specifically to aspects, so beta is indexed by I.
#c100	the sentiment weights;aspects;beta;I.
#s101	And As for aspect.
#c101	aspect
#s102	And that gives us way to model different segment of award.
#c102	us;way;different segment;award
#s103	This is neither because of the same word might have positive sentiment for once back, but negative sentiment for another aspect.
#c103	the same word;positive sentiment;negative sentiment;another aspect
#s104	It's also useful to then see.
#c104	It
#s105	What premise we have here, but I just said that the beta sub I W gives us a aspect specific sentiment of W.
#c105	What premise;we;I;the beta sub;I W;us;a aspect specific sentiment;W.
#s106	So obviously that's one of the important parameters, but in general we can see we have these parameters.
#c106	the important parameters;we;we;these parameters
#s107	The beta values that Delta and then the mu and Sigma.
#c107	The beta values
#s108	So next question is, how can we estimate these parameters and so we collectively denote all the parameters by Lambda here.
#c108	next question;we;these parameters;we;all the parameters;Lambda
#s109	Now we can, as usual, use The maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed.
#c109	we;The maximum likelihood;us;the settings;this premise;the maximizer
#s110	Observer ratings condition on their respective reviews.
#c110	Observer ratings condition;their respective reviews
#s111	And of course, this would then give us all the useful variables that will interest in computing.
#c111	course;us;all the useful variables;computing
#s112	So now more specifically, we can now once we estimate the parameters, we can easily compute the abstract rating for aspect I or sub I of D
#c112	we;we;the parameters;we;the abstract rating;aspect;I;I;D
#s113	and that's simply to take all the words that occurred in the segment I and then take their accounts and then multiply that by the sentiment weight of each word and take a sum.
#c113	all the words;the segment;I;their accounts;the sentiment weight;each word;a sum
#s114	So of course this counter would be 04 words that are not occurring in the aspect I, and that's why we can take some over all the words in the vocabulary.
#c114	course;this counter;04 words;the aspect;I;we;all the words;the vocabulary
#s115	Now, what about the aspect weights?
#c115	the aspect weights
#s116	Alpha sub I of D?
#c116	I;D
#s117	It's not part of our parameter, right?
#c117	It;part;our parameter
#s118	So we have to use Bayesian inference to compute it.
#c118	we;Bayesian inference;it
#s119	And in this case we can use the maximum a posteriori.
#c119	this case;we;the maximum;a posteriori
#s120	2 computer this alpha value.
#c120	2 computer;this alpha value
#s121	Basically we're going to maximize the product of the prior of our according to our assumed market valued Gaussian distribution and the likelihood in this case likely is the probability of generating this observed overall rating given this particular Alpha value and some other parameters.
#c121	we;the product;the prior;our;our assumed market;Gaussian distribution;the likelihood;this case;the probability;this observed overall rating;this particular Alpha value;some other parameters
#s122	As you see here.
#c122	you
#s123	So for more details about this model, you can read this paper cited here.
#c123	more details;this model;you;this paper
410	b355e801-bbf0-41a7-bdcd-75906c167014	106
#s1	This lecture is about the how to evaluate the text retrieval system when we have multiple levels of judgments.
#c1	This lecture;the text retrieval system;we;multiple levels;judgments
#s2	In this lecture we will continue the discussion of evaluation.
#c2	this lecture;we;the discussion;evaluation
#s3	We're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments.
#c3	We;the text retrieval system;we;multiple level;judgments
#s4	So, so far we have talked the about binary judgments.
#c4	we;binary judgments
#s5	That means a document is judged as being relevant or non relevant.
#c5	a document
#s6	But earlier we also talk about the relevance as a matter of degree, so we often can distinguishing very high relative documents.
#c6	we;the relevance;a matter;degree;we;very high relative documents
#s7	Those are very useful documents from your moderately relevant documents.
#c7	very useful documents;your moderately relevant documents
#s8	They are ok,  they are useful perhaps.
#c8	They;they
#s9	And further from non relevant documents, those are not useful.
#c9	non relevant documents
#s10	So imagine you can have ratings for these pages.
#c10	you;ratings;these pages
#s11	Then you would have multiple levels of ratings.
#c11	you;multiple levels;ratings
#s12	For example here I show example of three levels, 3 for relevant sorry 3 for very relevant, two for marginally relevant and one for non relevant.
#c12	example;I;example;three levels
#s13	Now how do we evaluate search engine system using these judgments?
#c13	we;search engine system;these judgments
#s14	Obviously the map doesn't work.
#c14	the map
#s15	Average precision doesn't work.
#c15	Average precision
#s16	Precision and recall doesn't work because they rely on binary judgments.
#c16	Precision;recall;they;binary judgments
#s17	So let's look at some top ranked results when using these judgments, right?
#c17	's;some top;these judgments
#s18	Imagine the user would be mostly care about the top 10 results here.
#c18	the user;the top 10 results
#s19	Right?
#s20	And we marked the reading levels or relevance levels for these documents as shown here, 32113, etc. "
#c20	we;the reading levels;relevance levels;these documents
#s21	And we call these ""Gain""."
#c21	we
#s22	"And the reason why we call it ""Gain""" is because the measure that we're introducing is called nDCG(Normalized Discounted Cumulative Gain).
#c22	the reason;we;it;the measure;we;nDCG(Normalized Discounted Cumulative Gain
#s23	So this gain basically can measure how much gain of relevant information the user can obtain by looking at each document.
#c23	this gain;how much gain;relevant information;the user;each document
#s24	Alright, so looking at the first document that the user can gain three points.
#c24	the first document;the user;three points
#s25	Looking at the non random document, the user would only gain one point.
#c25	the non random document;the user;one point
#s26	By looking at the moderately relevant or marginal relevant documents, the user would get two points.
#c26	the moderately relevant or marginal relevant documents;the user;two points
#s27	Etc.
#s28	So this gain intuitively matches the utility of a document from a user's perspective.
#c28	this gain;the utility;a document;a user's perspective
#s29	Of course, if we assume the user stops at the 10 documents and we're looking at the cut off at 10, we can look at the total game of the user.
#c29	we;the user;the 10 documents;we;the cut;we;the total game;the user
#s30	And what's that?
#c30	what
#s31	Well, that's simply the sum of these
#c31	the sum
#s32	and we call it a cumulative gain.
#c32	we;it;a cumulative gain
#s33	So if the user stops at the position one where there's just three, if the user looks at the another document, that's  3 + 2.
#c33	the user;the position;the user;the another document
#s34	If the user looks at the more documents, then the cumulative gain is more.
#c34	the user;the more documents;the cumulative gain
#s35	Of course, this is at the cost of spending more time to examine the list.
#c35	the cost;more time;the list
#s36	So cumulative gain gives us some idea about the how much total gain the user would have if the user examines all these documents.
#c36	cumulative gain;us;some idea;the how much total gain;the user;the user;all these documents
#s37	Now in nDCG we also have another letter here, D discounted.
#c37	nDCG;we;another letter;D
#s38	Cumulative gain.
#c38	Cumulative gain
#s39	So why do we want to do discounting?
#c39	we
#s40	Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents.
#c40	you;this cumulative gain;one deficiency;it;the rank position;these documents
#s41	So, for example, looking at the this sum here.
#c41	example;the this sum
#s42	And we only know there is one highly relevant document one marginally relevant document, two non relevant documents.
#c42	we;one highly relevant document;one marginally relevant document;two non relevant documents
#s43	We don't really care where they are ranked.
#c43	We;they
#s44	Ideally we want these two to be ranked on the top and which is the case here.
#c44	we;the top;the case
#s45	But how can we capture that intuition?
#c45	we;that intuition
#s46	Well, we have to say this is 3 here.
#c46	we
#s47	is not as good as this three on the top.
#c47	the top
#s48	And that means the contribution of the gain from different positions has to be weighted by their position, and this is the idea of discounting, basically.
#c48	the contribution;the gain;different positions;their position;the idea
#s49	So we're going to say well, the first one doesn't need to be discounted, because the user can be assumed to always see this document, but the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it.
#c49	we;the first one;the user;this document;the second one;this one;a small possibility;the user;it
#s50	So we divide this gain by the weight based on the position, so log of 2.
#c50	we;this gain;the weight;the position
#s51	Two is the rank position of this document.
#c51	the rank position;this document
#s52	And when we go to the third position, we discount even more because the normalizes log of three and so on, so forth.
#c52	we;the third position;we;the normalizes
#s53	So when we take a such a sum than a lower rank document will not contribute contribute that much as a highly ranked document.
#c53	we;a such a sum;a lower rank document;a highly ranked document
#s54	So that means if you for example switch the position of this, let's say this position and this one, and then you would get more discount if you put.
#c54	you;example;the position;'s;this position;this one;you;more discount;you
#s55	For example, very relevant document here, as opposed to here.
#c55	example
#s56	Imagine if you put three here, then it would have to be discounted, so it's not as good as if we would put the three here.
#c56	you;it;it;we
#s57	So this is the idea of discounting.
#c57	the idea
#s58	OK, so now at this point that we have got that discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments.
#c58	this point;we;the utility;this ranked list;multiple levels;judgments
#s59	So are we happy with this?
#c59	we
#s60	Well, We can use this rank systems.
#c60	We;this rank systems
#s61	Now, We still need to do a little bit more in order to make this measure comfortable across different topics.
#c61	We;order;this measure;different topics
#s62	And this is the last step.
#c62	the last step
#s63	And, By the way, here we just showed the DCG at the ten right?
#c63	the way;we;the DCG;the ten right
#s64	So this is the total sum of DCG.
#c64	the total sum;DCG
#s65	Overall these 10 documents.
#c65	Overall these 10 documents
#s66	So the last step is called the N normalization and if we do that then we will get a normalized DCG.
#c66	the last step;the N normalization;we;we;a normalized DCG
#s67	So how do we do that?
#c67	we
#s68	Well the idea here is we're going to normalize DCG by the ideal DCG at the same cut off.
#c68	the idea;we;DCG;the ideal DCG;the same cut
#s69	What is the ideal DCG?
#c69	What;the ideal DCG
#s70	This is the DCG of ideal ranking.
#c70	the DCG;ideal ranking
#s71	So imagine if we have 9 documents in the whole collection.
#c71	we;9 documents;the whole collection
#s72	Rated 3 here.
#s73	And that means in total we have 9 documents rated 3.
#c73	total;we;9 documents
#s74	Then our ideal rank, the Lister would have put all these nine documents on the very top.
#c74	our ideal rank;the Lister;all these nine documents;the very top
#s75	So all these would have to be 3
#s76	and then this will be followed by a two here because that's the best we could do after we have run out of threes.
#c76	we;we;threes
#s77	But all these positions would be threes.
#c77	all these positions;threes
#s78	Right?
#s79	So this will be an ideal ranked list.
#c79	an ideal ranked list
#s80	And then we can compute the DCG for this ideal ranked list.
#c80	we;the DCG;this ideal ranked list
#s81	So this would be given by this formula that you see here, and so this ideal DCG would then be used as the normalizer DCG..., here.
#c81	this formula;you;this ideal DCG;the normalizer DCG
#s82	And this ideal DCG will be used as a normalizer.
#c82	this ideal DCG;a normalizer
#s83	So you can imagine now normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic.
#c83	you;normalization;the actual DCG;the best DCG;you;this topic
#s84	Now, why do we want to do this?
#c84	we
#s85	Well, by doing this will map the DCG values into a range of zero through one, so the best value or the highest value for every query would be one.
#c85	the DCG values;a range;the best value;the highest value;every query
#s86	That's when your ranked list is in fact the ideal list.
#c86	your ranked list;fact;the ideal list
#s87	But otherwise, in general you will be lower than one.
#c87	you
#s88	Now, what if we don't do that?
#c88	we
#s89	Well, you can see this transformation or this normalization doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG.
#c89	you;this transformation;this normalization;the relative comparison;systems;just one topic;this ideal DCG;all the systems;the ranking;systems;only DCG;you;them;the normalized DCG
#s90	The difference however is when we have multiple topics.
#c90	The difference;we;multiple topics
#s91	because if we don't do normalization, different topics will have different scales of DCG.
#c91	we;normalization;different topics;different scales;DCG
#s92	For a topic like this one we have 9 highly relevant documents.
#c92	a topic;this one;we;9 highly relevant documents
#s93	The DCG can get really high, but imagine in another case, There are only two very relevant documents in total, in the whole collection.
#c93	The DCG;another case;only two very relevant documents;total;the whole collection
#s94	Then the highest DCG that any system could achieve for such a topic will not be very high.
#c94	the highest DCG;any system;such a topic
#s95	So again, we face the problem of different scales of DCG values, and we take an average.
#c95	we;the problem;different scales;DCG values;we;an average
#s96	We don't want the average to be dominated by those high values.
#c96	We;the average;those high values
#s97	Those are again easy queries, so by doing the normalization we can avoid the avoid the problem making all the queries contribute equally to the average.
#c97	easy queries;the normalization;we;the avoid;the problem;all the queries;the average
#s98	So this is the idea of nDCG.
#c98	the idea;nDCG
#s99	It's useful for measuring ranked list based on multiple level relevance judgments.
#c99	It;ranked list;multiple level relevance judgments
#s100	So more in the more general way, this is basically a measure that can be applied to any rank the task with multiple level of judgments.
#c100	the more general way;a measure;any rank;multiple level;judgments
#s101	And The scale of the judgments can be multiple.
#c101	The scale;the judgments
#s102	Can be more than binary, not only more than binary.
#s103	They can be multiple levels like a 1 through 5 or even more depending on your application.
#c103	They;multiple levels;your application
#s104	And the main idea of this measure I just to summarize is to measure the total utility of the top K documents.
#c104	the main idea;this measure;I;the total utility;the top K documents
#s105	So you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower ranked document.
#c105	you;a cut;you;the total utility;it;the contribution;a lower ranked document
#s106	And finally it will do normalization to ensure comparability across queries.
#c106	it;normalization;comparability;queries
410	b36805e6-d5d6-4d4c-a58c-7f1a5f6c233c	20
#s1	this letter is about the statistical language model in this lecture we're going we're going to give an introduction to statistical language model just has to do with how do you model text data with problems models so it's related to how we model query based on a document we're going to talk about what is the language model and then we're going to talk about the simplest language model called unigram language model which you also happens to be the most useful model for text retrieval and finally we discussed possible uses of language model what is the language model
#c1	this letter;the statistical language model;this lecture;we;we;an introduction;statistical language model;you;text data;problems models;it;we;query;a document;we;what;the language model;we;the simplest language model;unigram language model;you;the most useful model;text retrieval;we;possible uses;language model;what;the language model
#s2	well it's just the probability distribution over water sequences so here i show one this model gives the sequence today is wednesday a probability of zero point zero zero one it gave today wednesday is a very very small probability be'cause its amankila medical you can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model therefore it's clear the context dependent in ordinary conversation probably today's wednesday is most popular among these sentences but imagine in the context of discussing apply the math maybe the eigenvalue is positive would have a higher probability this means it can be used to represent the topic of the text the model can also be regarded as a probabilistic mechanism for generating text and this is why it's also often called a generating model
#c2	it;just the probability distribution;water sequences;i;this model;the sequence;wednesday;a probability;zero point;zero zero one;it;a very very small probability;you;the probabilities;these sentences;sequences;words;a lot;the model;it;the context;ordinary conversation;today's wednesday;these sentences;the context;the math;the eigenvalue;a higher probability;it;the topic;the text;the model;a probabilistic mechanism;text;it;a generating model
#s3	so what does that mean we can imagine this is a mechanism that's visualizer hands here as a stock ask the system that can generate the sequences of words so we can ask for a sequence
#c3	what;we;a mechanism;visualizer hands;a stock;the system;the sequences;words;we;a sequence
#s4	and it's too simple sequence from the device if you want
#c4	it;too simple sequence;the device;you
#s5	and they might generate for example today is wednesday by the could have generated any other sequences so for example there are many possibilities so this in this sense we have you our data as basically a sample observer from such a generating model
#c5	they;example;wednesday;any other sequences;example;many possibilities;this sense;we;you;our data;basically a sample observer;such a generating model
#s6	so why is such a model useful
#c6	such a model
#s7	well so many becaus it can quantify the uncertainties in natural language where do i insert in this come from well it one source is simply the ambiguity in natural language that we discussed earlier in the rapture another source is because we don't have complete understanding we lack or the knowledge to understand language in that case they will be answered in this as well so let me show some examples of questions that we can answer with the language model that would have interesting application in different ways given that wizzy john and feels how likely will see happy as a possible habit as the next war in a sequence of words obviously this would be very useful for speech recognition because happy and happy with having similar acoustical sound acoustic signals but if we look at the language model will know that john feels happy would be far more likely than john feels habit another example given that we observe baseball three times and game once in a news article how likely is it about the sports this obviously is related to text the categorisation and information retrieval also given that the user is interested in sports news how likely would the user use baseball in a query now this is a career related to the query likelihood that we discussed in the previous raptor so let's look at the simplicity language model called unigram language model in such a case we assume that we generate the text by generating each were independent so this means the probability of a sequence of words will be then the product of the probability of each world and normally they are not independent
#c7	it;the uncertainties;natural language;i;it;one source;the ambiguity;natural language;we;the rapture;another source;we;complete understanding;we;the knowledge;language;that case;they;me;some examples;questions;we;the language model;interesting application;different ways;that wizzy john;a possible habit;the next war;a sequence;words;speech recognition;similar acoustical sound acoustic signals;we;the language model;john;john;habit;we;baseball;a news article;it;the sports;the categorisation and information retrieval;the user;sports news;the user;baseball;a query;a career;the query likelihood;we;the previous raptor;'s;the simplicity language model;unigram language model;such a case;we;we;the text;the probability;a sequence;words;the product;the probability;each world;they
#s8	right
#s9	so if you have seen a warden like a language that would make them far more likely to observe model then if you haven't seen language so this is something is not necessarily true but we make this assumption into simplified model so now the model has precisely N parameters where N is vocabulary size we have one probability for each word and all these probabilities muscle some lap so strictly speaking we actually have minus one primers as i say that text can then be assumed to be assembled drawn from this word distribution so for example now we can ask the device or the model to stochastically general the words for us instead of sequences so instead of giving a whole sequence mega today's wednesday it now gives us just one word and we can get all kinds of words and we can assemble these words in a sequence so that would still allow the computer the probability of today's wednesday as the product of the three probabilities as you can see even though we have not asked the model the generator the sequences it actually allows us to compute the probability for all the sequences but this model now only needs in parameters to characterize that means if we specify all the probabilities for all the words then the models behavior is completely specified whereas if we don't make this assumption what would have to specify probabilities for all kinds of combinations of words in sequences so by making this assumption it makes it much easier to estimate these parameters so let's see a specific example here he also to unigram language models with some probabilities and these are high probability words that are shown on top the first one clearly suggests a topic of attacks reminding because the high probability awards are all related to this topic the second one is more related to health now we can they ask the question how likely will observe a particular text from each of these two models i suppose we assemble words to form the document let's say we take the first distribution which might assemble words what words do you think it would be generated well maybe text or maybe mining maybe another war even for which is a very small probability might disturb you able to show up but in general high probability was will likely show up more often so we can imagine what gender the texture that looks like a text mining in fact up with a small probability you might be able to actually generate the actual text mining paper that would actually meaningful although the probability would be very very small in the extreme case you might imagine we might be able to generate a text paper text mind never that would be accepted by major conference and in that case the probability would be even smaller but it's a non zero public anything if we assume none of the words have non zero probability similarly from the signal topic we can imagine we can generate the folding using paper that doesn't mean we cannot generate this paper from text mining distribution we can but the probability would be very very small maybe smaller than even generating a paper that can be accepted by a major conference on text mine so the point here is that the key point distribution we can talk about the probability of observing a certain kind of text some text that we have higher probabilities now those now let's look at the problem in a different way suppose we now have available a particular a document in this case maybe the abstract of the text mining paper and we see these word counts here the total number of words is one hundred now the question will ask here is estimation question we can ask the question which model which water distribution has been used it we generated this text assuming that the text that has been generated by assembling words from the distribution so what would be your guest let have to decide what probabilities text mining etc would have suppose the video for a second and try to think about your best gas if you like a lot of people you would have guessed that well my god best guesses in text it has a probability of ten out of one hundred because i've seen text ten times an there are in total one hundred words
#c9	you;a warden;a language;them;model;you;language;something;we;this assumption;simplified model;the model;precisely N parameters;N;vocabulary size;we;one probability;each word;all these probabilities;some lap;we;minus one primers;i;text;this word distribution;example;we;the device;the model;the words;us;sequences;a whole sequence;today's wednesday;it;us;just one word;we;all kinds;words;we;these words;a sequence;the computer;today's wednesday;the product;the three probabilities;you;we;the model;the generator;the sequences;it;us;the probability;all the sequences;this model;parameters;we;all the probabilities;all the words;the models behavior;we;this assumption;what;probabilities;all kinds;combinations;words;sequences;this assumption;it;it;these parameters;'s;a specific example;he;language models;some probabilities;high probability words;top;the first one;a topic;attacks;the high probability awards;this topic;the second one;health;we;they;the question;a particular text;these two models;i;we;words;the document;'s;we;the first distribution;words;what words;you;it;text;maybe another war;a very small probability;you;general high probability;we;the texture;a text mining;fact;a small probability;you;the actual text mining paper;the probability;the extreme case;you;we;a text paper text mind;major conference;that case;the probability;it;a non zero public anything;we;none;the words;non zero probability;the signal topic;we;we;the folding;paper;we;this paper;text mining distribution;we;the probability;a paper;a major conference;text mine;the point;the key point distribution;we;the probability;a certain kind;text;some text;we;higher probabilities;'s;the problem;a different way;we;a particular a document;this case;the text mining paper;we;these word;the total number;words;the question;estimation question;we;the question;which model;which water distribution;it;we;this text;the text;words;the distribution;what;your guest;what probabilities;the video;a second;your best gas;you;a lot;people;you;text;it;a probability;i;text;total one hundred words
#s10	so we simply not simply normalizing these counts and that's in fact they were justified and your intuition is consistent with mathematical derivation and this is called a maximum likelihood basement in this estimator we assume that the parameter settings are those that would give our observe the data the maximum probability that means if we change these probabilities then the probability of observing the particular task data would be somewhat a smaller so you can see this has a very simple formula basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length normalized frequency or consequences of this is of course we're going to assign zero probabilities to unseen words if we're having the observer ward there will be no incentive to assign a non zero probability using this approach why be cause that would take away probability mass for these observ the words and that obviously wouldn't maximize the probability of this particular observer text there
#c10	we;these counts;fact;they;your intuition;mathematical derivation;a maximum likelihood basement;this estimator;we;the parameter settings;our observe;the data;the maximum probability;we;these probabilities;the probability;the particular task data;you;a very simple formula;we;the count;a word;the document;the total number;words;the document;document length;normalized frequency;consequences;course;we;zero probabilities;unseen words;we;the observer ward;no incentive;a non zero probability;this approach;probability mass;these observ;the words;the probability;this particular observer text
#s11	but once you question whether this is our best estimate well then that depends on what kind of model you want to find right this is made it gives the best model based on this particular data but if you interest rema model that can explain the content of the four paper of this abstract then you might have a second all right so for one thing they surely be other words in the body of the article so they should not have zero probabilities even though they're not observing the abstract and we're going to cover this a little later in discussing the query like retrieval model
#c11	you;our best estimate;what kind;model;you;it;the best model;this particular data;you;the content;the four paper;this abstract;you;one thing;they;other words;the body;the article;they;zero probabilities;they;the abstract;we;the query;retrieval model
#s12	so let's take up a look at the some possible uses of this language models one uses simply to use it to represent the topics so here i show some general english background text we can use this text to estimate a language model and the model might look like this i saw on the top we have those all common words like the is way etc
#c12	's;a look;the some possible uses;this language models;it;the topics;i;some general english background text;we;this text;a language model;the model;i;the top;we;those all common words;the is;way
#s13	and then we'll see some common words like these and then some very very rare words in the bottom this is the background language model it represents the frequency of words in english in general this is the background model not let's look at another text maybe this time we look at the computer science research papers so we have a collection of computer science research papers we do as matching again we can just use the maximum micro arrays matter where we simply normalize the frequencies now in this case will get the distribution that looks like this on the top it looks similar because these words occur everywhere they're very common
#c13	we;some common words;then some very very rare words;the bottom;the background language model;it;the frequency;words;english;the background model;'s;another text;we;the computer science research papers;we;a collection;computer science research papers;we;we;the maximum micro arrays matter;we;the frequencies;this case;the distribution;the top;it;these words;they
#s14	but as we go down will see words that are more related to computer science computer software attacks etc so although here we might also see these words for example computer
#c14	we;words;computer science computer software attacks;we;these words;example computer
#s15	but we can imagine the probability here is much smaller than the probably in there here and we will see many other words here that would be more common in general image so you can see this distribution characterize the topic of the corresponding tents we can look at the even a smaller text so in this case let's look at the text mining paper now if we do the same we have another distribution again there can be expected to occur on the top assume we will see text mining association clustering these words have relatively high probabilities in contrast in this distribution will text has relatively small probability so this means again based on different the text that we can have a different model and model captures the topic so we call this document the damage model and we call this collection language model and later you will see how they're used in retrieval function but now let's look at that another use of this model can we statistically find the what wars are semantically related to computer now how do we find such words well office thought is that let's take a look at the text that match computer so we can take a look at all the documents that contain the word computer let's build a language model we can see what words we see there well not surprisingly we see these common was on top as we always do so in this case this language model gives us a conditional probability overseeing award in the context of computer and these common words were naturally have higher probabilities
#c15	we;the probability;we;many other words;general image;you;this distribution;the topic;the corresponding tents;we;the even a smaller text;this case;'s;the text mining paper;we;we;another distribution;the top assume;we;text mining association;these words;relatively high probabilities;contrast;this distribution;relatively small probability;different the text;we;a different model;model;the topic;we;this document;the damage model;we;this collection language model;you;they;retrieval function;'s;another use;this model;we;what wars;computer;we;such words;'s;a look;the text;computer;we;a look;all the documents;the word computer;'s;a language model;we;what words;we;we;top;we;this case;this language model;us;a conditional probability overseeing award;the context;computer;higher probabilities
#s16	but we also see computer itself and software we have relatively higher probabilities but if we just use this model we cannot just say all these words are semantically related to computer so intuitively would like to get rid of these yep these common was how can we do that it turns out that it's possible to use language model will do that i suggested to think about that
#c16	we;computer;itself;software;we;relatively higher probabilities;we;this model;we;all these words;computer;we;it;it;language model;i
#s17	so how can we know what words are very comments that we want to kind of get rid of them what model would tell us that well maybe you can think about that so the background language model precisely tells us this information that tells us what words are common in general so if we use this background model we would know that these words are common words in general so it's not surprising observe them in the context of computer where is computer has a very small probability in general
#c17	we;what words;very comments;we;them;what model;us;you;the background language model;us;this information;us;what words;we;this background model;we;these words;common words;it;them;the context;computer;computer;a very small probability
#s18	so it's very surprising that we have seen computer with this probability and the same is true for software
#c18	it;we;computer;this probability;software
#s19	so then we can use this to models to somehow figure out the words that are related to computer for example we can simply take the ratio of these who probabilities or normalize the topic language model by the probability of the world in the background language model
#c19	we;models;the words;computer;example;we;the ratio;who;the topic language model;the probability;the world;the background language model
#s20	so if we do that we take the ratio will see that then on the top of computer is ranked and then followed by software program all these words are related to computer be cause they're can very frequently in the context of computer but not frequently in the whole collection whereas these common words will not have a high probability in fact they have ratio about of one down there because they are not really related to computer by taking the sample of text that contains the computer we don't really see more occurrences of them then in general so this shows that even with this simple language models we can do some limited analysis of semantics so in this lecture we talked about then with model which is basically a probability distribution over text we talked about the simplest of empty model called unigram them model which is also just a word distribution we talked about the two uses of a language model one is will represent the topic in a document in the collection or in general the other is rediscovered associations in the next lecture we're going to talk about how them with model can be used to design retrieval function here are two additional readings the first is textbook on statistical natural language processing the second is article that has a survey of statistical language models with a lot of pointers to research work
#c20	we;we;the ratio;the top;computer;software program;all these words;computer;they;the context;computer;the whole collection;these common words;a high probability;fact;they;ratio;they;computer;the sample;text;the computer;we;more occurrences;them;this simple language models;we;some limited analysis;semantics;this lecture;we;model;a probability distribution;text;we;empty model;unigram;them;model;just a word distribution;we;the two uses;a language model;one;the topic;a document;the collection;rediscovered associations;the next lecture;we;them;model;retrieval function;two additional readings;textbook;statistical natural language processing;article;a survey;statistical language models;a lot;pointers;work
410	b599210f-ecad-4734-895d-83a3cc025112	77
#s1	This lecture is about link analysis for web search.
#c1	This lecture;link analysis;web search
#s2	In this lecture we're going to talk about web search.
#c2	this lecture;we;web search
#s3	And particularly focusing on how to do link analysis and use the results to improve search.
#c3	analysis;the results;search
#s4	The main topic of this lecture is to look at the ranking algorithms for web search.
#c4	The main topic;this lecture;the ranking algorithms;web search
#s5	In the previous lecture we talked about how to create index now that we have got index.
#c5	the previous lecture;we;index;we;index
#s6	We want to see how we can improve ranking of pages.
#c6	We;we;ranking;pages
#s7	Our standard IR models can be also applied here.
#c7	Our standard IR models
#s8	In fact they are important building blocks for improvement for supporting web search, but they aren't sufficient and mainly for the following reasons.
#c8	fact;they;important building blocks;improvement;web search;they;the following reasons
#s9	First, on the web we tend to have very different information needs.
#c9	the web;we
#s10	For example, people might search for a web page or entry page and this is different from the traditional library search where people are primarily interested in collecting literature information.
#c10	example;people;a web page;entry page;the traditional library search;people;literature information
#s11	So this kind of query is often called navigational queries.
#c11	this kind;query;navigational queries
#s12	The purpose is to navigate into a particular target page.
#c12	The purpose;a particular target page
#s13	So for such queries we might benefit from using link information.
#c13	such queries;we;link information
#s14	Secondly, documents have additional information and on the web web pages are well format.
#c14	documents;additional information;the web web pages;well format
#s15	There are a lot of other clues such as the layout, title or link information.
#c15	a lot;other clues;the layout;title;information
#s16	Again, so this has provided the opportunity to use extra context information.
#c16	the opportunity;extra context information
#s17	Of the document to improve scoring and finally information quality varies a lot, so that means we have to consider many factors to improve the ranking algorithm.
#c17	the document;scoring;information quality;we;many factors;the ranking algorithm
#s18	This would give us a more robust way to rank the pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page.
#c18	us;a more robust way;the pages;it;any spammer;the one signal;the ranking;a page
#s19	So as a result, people have made a number of major extensions to the ranking algorithms.
#c19	a result;people;a number;major extensions;the ranking algorithms
#s20	One line is to exploit links to improve scoring.
#c20	One line;links;scoring
#s21	And that's the main topic of this lecture.
#c21	the main topic;this lecture
#s22	People have also proposed algorithms to exploit the large scale implicit feedback information in the form of click throughs, and that's of course in the category of feedback techniques.
#c22	People;algorithms;the large scale implicit feedback information;the form;click throughs;course;the category;feedback techniques
#s23	And machine learning is often used there.
#c23	machine learning
#s24	In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features.
#c24	web search;the ranking algorithms;algorithms;all kinds;features
#s25	Many of them are based on the standard visual models such as BM25 that we talked about.
#c25	them;the standard visual models;BM25;we
#s26	Or query likelihood to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring.
#c26	query;different parts;documents;additional features;content matching;link;information;they;additional scoring
#s27	Signals.
#c27	Signals
#s28	So let's look at links in more detail on the web.
#c28	's;links;more detail;the web
#s29	So this is a snapshot of some part of the web and say so we can see there are many links that link the different pages together, and in this case you can also look at the center here.
#c29	a snapshot;some part;the web;we;many links;the different pages;this case;you;the center
#s30	There is a description of a link that's pointing to the document on the right side.
#c30	a description;a link;the document;the right side
#s31	Now this description text is called anchor text.
#c31	this description text;anchor text
#s32	Now if you think about the this text, it's actually quite a useful because it provides some extra description of that page being pointed to.
#c32	you;the this text;it;it;some extra description;that page
#s33	So for example, if someone wants to bookmark Amazon.com, front page the person might say.
#c33	example;someone;Amazon.com;front page;the person
#s34	The biggest online bookstore and then with the link to Amazon.
#c34	The biggest online bookstore;the link;Amazon
#s35	Right, so the description here actually is very similar to what the user will type in the query box when they are looking for such a page, and that's why it's very useful for ranking pages.
#c35	the description;what;the user;the query box;they;such a page;it;ranking pages
#s36	Suppose someone types in query like online bookstore or fixed online bookstore.
#c36	someone;query;online bookstore;fixed online bookstore
#s37	The query would match this anchor text.
#c37	The query;this anchor text
#s38	In the page.
#c38	the page
#s39	Here and then, this actually provides evidence for matching the page that's being pointed to.
#c39	evidence;the page
#s40	That is the Amazon entry page.
#c40	the Amazon entry page
#s41	So if you match the anchor text that describes a link to a page, actually that provides good evidence for the relevance of the page being pointed to, so anchor text is very useful.
#c41	you;the anchor text;a link;a page;good evidence;the relevance;the page;anchor text
#s42	If you look at the bottom part of this picture you can also see there are some patterns of links, and these links might indicate the utility of a document.
#c42	you;the bottom part;this picture;you;some patterns;links;these links;the utility;a document
#s43	So for example on the right side you can see this page has received, many in links.
#c43	example;the right side;you;this page;links
#s44	That means many other pages are pointing to this page and this shows that this page is quite useful.
#c44	many other pages;this page;this page
#s45	On the left side you can see, this is another page that points to many other pages, so this is a directory page that would allow you to actually see a lot of other pages.
#c45	the left side;you;another page;many other pages;a directory page;you;a lot;other pages
#s46	So we can call the first case authority page and the second case hub page.
#c46	we;the first case authority page;the second case hub page
#s47	This means the link information can help in two ways.
#c47	the link information;two ways
#s48	One is to provide extra text for matching and the other is to provide some additional scores for the web pages to characterize how likely a page is a hub, how likely a page is authority.
#c48	extra text;matching;some additional scores;the web pages;a page;a hub;a page;authority
#s49	So people then of course proposed ideas to leverage these in this link information.
#c49	people;course;ideas;this link information
#s50	Now Google's Pagerank, which was the main technique that they used in early days, is a good example and that is an algorithm to capture page popularity, basically to score authority.
#c50	Google's Pagerank;the main technique;they;early days;a good example;an algorithm;page popularity;authority
#s51	So the intuition's here are links are just like a citations in the literature.
#c51	the intuition;links;a citations;the literature
#s52	Think about one page pointing to another page.
#c52	another page
#s53	This is very similar to one paper citing another paper.
#c53	one paper;another paper
#s54	So of course, then if a page is cited often, then we can assume this page to be more useful, in general.
#c54	course;a page;we;this page
#s55	So that's a very good intuition.
#c55	a very good intuition
#s56	Now Pagerank is essentially to take advantage of this intuition to implement it with the principled approach.
#c56	Pagerank;advantage;this intuition;it;the principled approach
#s57	Intuitively, it's essentially doing citation counting or in link counting.
#c57	it;citation counting;link counting
#s58	It just improves this simple idea in two ways.
#c58	It;this simple idea;two ways
#s59	One is it would consider indirect citations.
#c59	it;indirect citations
#s60	So that means you don't just look at the how many in links you have.
#c60	you;links;you
#s61	You also look at the what are those pages that are pointing to you.
#c61	You;what;those pages;you
#s62	If those pages themselves have a lot of in links, well that means a lot.
#c62	those pages;themselves;a lot;links;a lot
#s63	In some sense you will get some credit from them.
#c63	some sense;you;some credit;them
#s64	But, if those pages that are pointing to you are not being pointed to by other pages, they themselves don't have many in links then, well, you don't get that much credit.
#c64	those pages;you;other pages;they;themselves;links;you;that much credit
#s65	So that's the idea of getting indirected citation.
#c65	the idea;indirected citation
#s66	Alright you can also understand this idea by looking at again the research papers.
#c66	you;this idea;the research papers
#s67	If you're cited by let's say 10 papers and those 10 papers are just workshop papers and or some papers that are not very influential, right?
#c67	you;'s;10 papers;those 10 papers;workshop papers;some papers
#s68	So although you get 10 in links and that's not as good as if you were cited by 10 papers that themselves have attracted a lot of other citations.
#c68	you;links;you;10 papers;themselves;a lot;other citations
#s69	So this is.
#s70	A case where we would like to consider indirect links and Pagerank does that.
#c70	A case;we;indirect links;Pagerank
#s71	The other idea is it's going to smooth the citations or assume that basically every page is having a non zero pseudo citation count.
#c71	The other idea;it;the citations;every page;a non zero pseudo citation count
#s72	Essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone.
#c72	we;many virtual links;all the pages;you;pseudo citations;everyone
#s73	The reason why they want to do that is this would allow them to solve the problem elegantly with.
#c73	The reason;they;them;the problem
#s74	Linear algebra technique.
#c74	Linear algebra technique
#s75	So.
#s76	I think maybe the best way to understand the page rank is through.
#c76	I;the best way;the page rank
#s77	Think of this as do computer the probability of random surfer, visiting every web page.
#c77	the probability;random surfer;every web page
410	b6f9dd1a-1d38-48b8-8942-d107c9d4d2b7	29
#s1	this latter is about the web indexing in this lecture we will continue talking about the web search and we're going to talk about how to create web scale index so once we cross the web we've got a lot of web pages the next step is to use the indexer to create the inverted index in general we can use the standard information retrieval techniques for creating the index and that is what we talk about in previous lecture but there are new challenges that we have to solve for web scale in dancing and the two main challenges our scalability and efficiency the index would be so large that it cannot actually fit into any single machine or single disk so we have to store the data on multiple machines also because the data is so large it's beneficial to process that data in parallel so that we can produce the index quickly how to address these challenges google has made a number of innovations one is the google fire system that's a general distributed file system that can help the program 's manage files stored on a cluster of machines the second is map reduce this is a general software framework for supporting parallel computation paducah is the most well known open source implementation of map reduce now used in many applications
#c1	the web indexing;this lecture;we;the web search;we;web scale index;we;the web;we;a lot;web pages;the next step;the indexer;the inverted index;we;the standard information retrieval techniques;the index;what;we;previous lecture;new challenges;we;web scale;dancing;the two main challenges;our scalability;efficiency;the index;it;any single machine;single disk;we;the data;multiple machines;the data;it;that data;parallel;we;the index;these challenges;google;a number;innovations;the google fire system;a general distributed file system;the program 's manage files;a cluster;machines;map;a general software framework;parallel computation paducah;the most well known open source implementation;map;many applications
#s2	so this is the architecture of the google file system it uses very simple centralized management that mechanism to manage it all the specific locations of files so the maintains the filename space and look up a table to know where exactly each files installed the application client that would then talk to this chi ever semester and that obtains specifica locations of the files that they want the process and once the GF 's kind obtained the the specific information about the files then the application climbed can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network
#c2	the architecture;the google file system;it;very simple centralized management;that mechanism;it;all the specific locations;files;the filename space;a table;each files;the application client;this chi;specifica locations;the files;they;the process;the GF 's kind;the the specific information;the files;the application;the specific servers;the data;you;other nodes;the network
#s3	so when this file system stores the fire zone machines they system also would create a fixed sizes of chunks so that data files are separately in too many chunks each chunk is sixty four megabytes
#c3	this file system;the fire zone machines;they;a fixed sizes;chunks;data files;too many chunks;each chunk;sixty four megabytes
#s4	so it's pretty big
#c4	it
#s5	and that's a property for large data processing these chunks all replicated to ensure reliability
#c5	a property;large data;these chunks;reliability
#s6	so this is something that problem doesn't have to worry about
#c6	something;that problem
#s7	and it's all taken care of by this fire system so from the application perspective the programmer would see this as if it's a normal file the program doesn't have to know exact rates are stored and can just invoke high level operate this to process the file ann another feature is that the data transfer is directed between application and chunk servers so it's efficient in this sense on top of the google file system and over also propose map reduce as a general framework for parallel programming now this is very useful to support a task like a building inverted index i saw this framework is hiding a lot of low level features from the program as a result the programmer can make minimum effort to create application that can be run large cluster in parallel and so some of the low level details hitting in the framework including the specific network communications or load balancing or where the tasks are executed all these details are hidden from the programmer there is also a nice visual which is the building for the tolerance if one server is broken let's say the service down and then some tasks may not be finished then the map reduce mechanism would know that the task has not been down so it will automatically dispatch the taskbar on other servers that can do the job and
#c7	it;care;this fire system;the application perspective;the programmer;it;a normal file;the program;exact rates;high level;the file;another feature;the data transfer;application;chunk servers;it;this sense;top;the google file system;map;a general framework;parallel programming;a task;a building inverted index;i;this framework;a lot;low level features;the program;a result;the programmer;minimum effort;application;large cluster;parallel;the low level details;the framework;the specific network communications;the tasks;all these details;the programmer;the building;the tolerance;one server;'s;the service;some tasks;the map;mechanism;the task;it;the taskbar;other servers;the job
#s8	therefore again the program it doesn't have to worry about that
#c8	therefore again the program;it
#s9	so here's how mapreduce works the input they're not would be separately into a number of key value pairs now what exactly is in the value will depend on the data
#c9	mapreduce;they;a number;key value;what;the value;the data
#s10	and it's actually a fairly general framework to allow you to just partition the data into different parts
#c10	it;a fairly general framework;you;the data;different parts
#s11	and she probably can be there processed in parallel each key value pair would be and send it to a map function the problem with the right map function of course and then the map function with the process this key value pair and with generate the a number of other key value pairs of course the new key is usually different from the old key that given through the map as in fault and these key value pairs are the output of the mac function and all the outputs of all the map functions would be then collected
#c11	she;parallel;each key value pair;it;a map function;the problem;the right map function;course;then the map function;the process;this key value pair;the a number;other key value pairs;course;the new key;the old key;the map;fault;these key value pairs;the output;the mac function;all the outputs;all the map functions
#s12	and then there would be for the sort based on the key and the result is that all the values that are associated with the same key would be land grouped together so now we've got a pair of a key and a set of values that are attached to this P
#c12	the sort;the key;the result;all the values;the same key;land;we;a pair;a key;a set;values;this P
#s13	so this would then be sent to a reduce function now of course each reduce function will handle a different each different key
#c13	a reduce function;course;each reduce function;a different each different key
#s14	so we will send this output values to multiple reduce functions each handling unique key a reduce function with them process the input which is a key and a set of values to produce another set of key values as the output so these output values will be then collected together to form the final output
#c14	we;this output values;multiple reduce functions;unique key;a reduce function;them;the input;a key;a set;values;another set;key values;the output;these output values;the final output
#s15	so this is the general framework of map reduce now the programmer only needs right the map function and the reduce function everything else is actually taken care of by the mapreduce framework so you can see the program really only needs to do minimum work and with such a framework of the input data can be partitioned into multiple parts each is processed in parallel first by map and then in the process after we reach the reduce stage then multiple reduce functions can also further process the different keys and their source their values in parallel so it achieves some it achieves the purpose of parallel processing of large data set so let's take a look at a simple example
#c15	the general framework;map;the programmer;right the map function;the reduce function;everything;care;the mapreduce framework;you;the program;minimum work;such a framework;the input data;multiple parts;parallel;map;the process;we;the reduce stage;multiple reduce functions;the different keys;their source;their values;parallel;it;it;the purpose;parallel processing;large data;'s;a look;a simple example
#s16	and that's what accounting are the input is is files containing words and the articles that we want to generate this is a number of occurrences of each word
#c16	the input;files;words;the articles;we;a number;occurrences;each word
#s17	so it's the word account we know this kind of accounting would be useful to for example assess the popularity over word in a large collection and this is useful for achieving a factor of IDF weighting all search so how can we solve this problem well one natural thought is that well this this task can be down imperial by simply counting different parts of the fire imperial and then in the end we just combine all the counts and that's precisely the idea of what we can do with mapreduce we can parallelize on lines in this input file so more specifically we can assume the input to each map function is key value pair that represents the line number and the stream on that line so the first line for example has key of one
#c17	it;the word account;we;this kind;accounting;example;the popularity;word;a large collection;a factor;IDF;all search;we;this problem;one natural thought;this task;different parts;the end;we;all the counts;precisely the idea;what;we;mapreduce;we;lines;this input file;we;the input;each map function;key value pair;the line number;the stream;that line;the first line;example;key
#s18	and the value is hello word by word and just the four words on that line so this key value pair will be sent to a map function the map function would then just count the words in this line and in this case of course there are only four words each water gets account of one
#c18	the value;hello word;word;just the four words;that line;this key value pair;a map function;the map function;the words;this line;this case;course;only four words;each water;account
#s19	and these are the output that you see here on this slide from this map function so the map function is really very simple if you look at what the pseudo code looks like on the right side you see it simply needs to iterate over all the words in this line and then just cover collect function which means it would then send the word and the counter to the collector the collector would then try to sort all these key value pairs from different functions
#c19	the output;you;this slide;this map function;the map function;you;what;the pseudo code;the right side;you;it;all the words;this line;collect function;it;the word;the counter;the collector;the collector;different functions
#s20	so the function is very simple and the programmer specifies is this function as a way to process each part of the data of course the second line will be handled by a different map function which would produce a similar output OK now the output of from the map functions will be then send it to a collector and the clap that will do the internal grouping or sorting so at this stage you can see we have collected much board pairs each pair is award
#c20	the function;the programmer specifies;this function;a way;each part;the data;course;the second line;a different map function;a similar output;the output;the map functions;it;a collector;the clap;the internal grouping;this stage;you;we;much board pairs;each pair;award
#s21	and it's count in a lie so once we see all these pairs then we can sort them based on the key which is the world so we will collect all the counts of award like a buy here together and similar will do that for other words like hadoop hello etc so each word now is attached to a number of values a number of accounts and these counts represented the occurrences of this word in different lines so now we have got a new pair of kiana set of values and this pair will then be fitting to reduce function so reduce functional would have to finish the job of counting the total occurrences of this world now it has already got all these partial counts so it needs to do is similar to add them up so they're reduced function show here is very simple as well you have counter and then iterate over all the words that you see in this array and then you just accumulated account and then finally you output the key and the total account and that's precisely what we want as the output of this whole program so you can see this is already very similar to building a inverted index and if you think about it at the output here is indexer bio world
#c21	it;count;a lie;we;all these pairs;we;them;the key;the world;we;all the counts;award;a buy;other words;hadoop;each word;a number;values;accounts;these counts;the occurrences;this word;different lines;we;a new pair;kiana;values;this pair;function;functional;the job;the total occurrences;this world;it;all these partial counts;it;them;they;reduced function show;you;counter;all the words;you;this array;you;account;you;the key;the total account;precisely what;we;the output;this whole program;you;a inverted index;you;it;the output;indexer;world
#s22	and we have already got the dictionary basically we have got to the counts
#c22	we;we;the counts
#s23	but what's missing is the document i DS and the specific frequency counts words in those documents so we can modify this is slightly to actually build a inverted index in parallel so here's one way to do that so in this case we can assume the input for map function is a pair of a key which denotes the document ID and value denoting the stream for that document
#c23	what;the document;i;words;those documents;we;a inverted index;parallel;one way;this case;we;the input;map function;a pair;a key;the document ID;the stream;that document
#s24	so it's all the words in that document and so the map function will do something very similar to what we have seen in the water company example simply groups all the counts of this word in this document together and it would that generate a set of key value pairs each key is award and the value is the count of this word in this document plus the document ID now you can easily see why we need to add document ID here course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later now similarly another document of the tool can be processed in the same way so in the end again there is a sorting mechanism that with a group them together
#c24	it;all the words;that document;the map function;something;what;we;the water company example;all the counts;this word;this document;it;a set;key value;each key;award;the value;the count;this word;this document;the document;ID;you;we;document ID;course;the inverted index;we;this information;the map function;track;it;the reduce function;similarly another document;the tool;the same way;the end;a sorting mechanism;a group;them
#s25	and then we will have just a key like a java associated with all the documents that matches this key or all the documents where java occured and the account i saw the counts of java in those documents and this will be collected together and this will be so fed into the reduce function so now you can see the reduce function has already got info that looks like a inverted index entry
#c25	we;just a key;all the documents;this key;all the documents;the account;i;the counts;those documents;the reduce function;you;the reduce function;info;a inverted index entry
#s26	right
#s27	so it's just the word and all the documents that contain the word and the frequencies of the world in those documents
#c27	it;just the word;all the documents;the word;the frequencies;the world;those documents
#s28	so all it needs to do is simply to concatenate them into a continuous chunk of data and this can be then written into a fire system so basically the reduce function is going to do very minimum work and so this is pseudocode for inverted index construction here we see two functions procedure map and procedure reduce an a programmer would specify these two functions to program on top of map reduce and you can see basically they're doing what i just described in the case of map it's going to count the occurrences of word using associative array and will output all the counts together with the document ID here i saw this is the reduce function on the other hand simply concatenates or the input that it has been given and then put them together as one single entry for this key
#c28	it;them;a continuous chunk;data;a fire system;the reduce function;very minimum work;inverted index construction;we;two functions procedure map;procedure;an a programmer;these two functions;top;map;reduce;you;they;what;i;the case;map;it;the occurrences;word;associative array;all the counts;the document ID;i;the reduce function;the other hand;the input;it;them;one single entry;this key
#s29	so this is very simple map reduce function yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines the program doesn't have to take care of the details so this is how we can do parallel index construction for web search so to summarize web scaling that scene requires some new techniques that go beyond the standard traditional indexing techniques mainly we have to store the index on multiple machines and this is usually done by using file system like a google file system that distributed file system an secondly it requires creating the index in parallel because it's so large it takes a long time to create an index for all the documents so if we can do it in parallel it'll be much faster and this is done by using the mapreduce framework note that oppose the GFS an mapreduce frameworks are very general so they can also support many other applications
#c29	very simple map;function;it;us;the inverted index;very large scale;the data;different machines;the program;care;the details;we;index construction;web search;web scaling;that scene;some new techniques;the standard traditional indexing techniques;we;the index;multiple machines;file system;a google file system;file system;it;the index;parallel;it;it;a long time;an index;all the documents;we;it;parallel;it;the mapreduce framework note;the GFS;an mapreduce frameworks;they;many other applications
410	b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	100
#s1	This lecture is about the topic mining and analysis.
#c1	This lecture;the topic mining;analysis
#s2	We are going to talk about using a term as topic.
#c2	We;a term;topic
#s3	This is a slide that you have seen in the earlier lecture where we defined the task of top mining and analysis.
#c3	a slide;you;the earlier lecture;we;the task;top mining;analysis
#s4	We also raised the question how do we exactly define the topic theta?
#c4	We;the question;we;the topic theta
#s5	So in this lecture we are going to offer one way to define it, and that's our initial idea.
#c5	this lecture;we;one way;it;our initial idea
#s6	Our idea here is to define a topic simply as a term.
#c6	Our idea;a topic;a term
#s7	A term can be a word or a phrase.
#c7	A term;a word;a phrase
#s8	And in general, we can use these terms to describe topics, so our first thought is just to define a topic as one term.
#c8	we;these terms;topics;our first thought;a topic;one term
#s9	For example, we might have terms like sports, travel or science as you see here.
#c9	example;we;terms;sports;travel;science;you
#s10	Now if we define a topic in this way, we can analyze the coverage of such topics in each document.
#c10	we;a topic;this way;we;the coverage;such topics;each document
#s11	Here, for example, we might want to discover to what extent document 1 covers sports and we found that 30% of the content of document 1 is about sports.
#c11	example;we;what extent;document;sports;we;30%;the content;document;sports
#s12	And 12% is about the travel etc.
#c12	12%
#s13	We might also discover Document 2 does not cover sports at all, so the coverage is zero, etc.
#c13	We;Document;sports;the coverage
#s14	So now of course, as we discussed.
#c14	course;we
#s15	In the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage.
#c15	the task definition;topic mining;analysis;we;two tasks;one;the topics;the 2nd;the coverage
#s16	So let's first think about how we can discover topics if we represent each topic by a term.
#c16	's;we;topics;we;each topic;a term
#s17	So that means we need to mine K topical terms from a collection.
#c17	we;K topical terms;a collection
#s18	Now there are of course many different ways of doing that and.
#c18	course;many different ways
#s19	We're going to talk about a natural way of doing that, which is also likely effective.
#c19	We;a natural way
#s20	So first we're going to parse the text data in the collection to obtain candidate terms.
#c20	we;the text data;the collection;candidate terms
#s21	Here, candidate terms can be words or phrases.
#c21	candidate terms;words;phrases
#s22	Let's say the simplest solution is to just take each word as a term.
#c22	's;the simplest solution;each word;a term
#s23	These words then become candidate topics.
#c23	These words;candidate topics
#s24	Then we're going to design a scoring function to measure how good each term is as a topic.
#c24	we;a scoring function;each term;a topic
#s25	So how can we design such a function?
#c25	we;such a function
#s26	Well, there are many things that we can consider.
#c26	many things;we
#s27	For example, we can use pure statistics to design such as scoring function.
#c27	example;we;pure statistics;scoring function
#s28	Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection.
#c28	we;representative terms;terms;a lot;content;the collection
#s29	So that would mean we want to favor a frequent term.
#c29	we;a frequent term
#s30	However, if we simply use the frequency to design the scoring function, then the highest scored terms would be "general terms or functional terms, like ""the"", ""a""" etc.
#c30	we;the frequency;the scoring function;the highest scored terms;general terms;functional terms
#s31	Those terms are very frequent in English.
#c31	Those terms;English
#s32	So we also want to avoid having such words on the top, so we want to penalize such words, but in general would like the favor terms that are fairly frequently but not so frequent.
#c32	we;such words;the top;we;such words;the favor terms
#s33	So a particular approach could be based on, TF-IDF weighting from retrieval.
#c33	a particular approach;TF-IDF weighting;retrieval
#s34	And TF stands for term frequency  IDF stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations.
#c34	TF;term frequency;  IDF;inverse document frequency;we;these ideas;the lectures;the discovery;word associations
#s35	So these are statistical methods, meaning that the function is defined mostly based on statistics.
#c35	statistical methods;the function;statistics
#s36	So the scoring function would be very general.
#c36	the scoring function
#s37	It can be applied to any language and any text.
#c37	It;any language;any text
#s38	But when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics.
#c38	we;such an approach;a particular problem;we;some domain specific heuristics
#s39	For example, in news we might favor title words.
#c39	example;news;we;title words
#s40	Actually, in general, we might want to favor title words becauses the authors tend to use the title to describe the topic of an article.
#c40	we;title words;becauses;the authors;the title;the topic;an article
#s41	If we're dealing with tweets, we could also favor hashtags which are invented to denote topics.
#c41	we;tweets;we;hashtags;denote topics
#s42	So naturally hashtags can be good candidates for representing topics.
#c42	hashtags;good candidates;topics
#s43	Anyway, after we have designed the scoring function, then we can discover the K topical terms by simply picking K terms with the highest scores.
#c43	we;the scoring function;we;the K topical terms;K terms;the highest scores
#s44	Now of course we might encounter a situation where the highest scored terms are all very similar.
#c44	course;we;a situation;the highest scored terms
#s45	They are semantically similar or closely related or even synonyms.
#c45	They;synonyms
#s46	So that's not desirable, so we also want to have coverage over all the content in the collection.
#c46	we;coverage;all the content;the collection
#s47	So we would like to remove redundancy and one way to do that is to do a greedy algorithm, which is sometimes called maximal marginal relevance ranking.
#c47	we;redundancy;one way;a greedy algorithm;maximal marginal relevance ranking
#s48	Basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the K topical terms.
#c48	the idea;the list;our scoring function;an gradually take terms;the K topical terms
#s49	The first term of course will be picked when we pick the next term.
#c49	The first term;course;we;the next term
#s50	We're going to look at the what terms have already been picked and try to avoid picking a term that's too similar.
#c50	We;what terms;a term
#s51	similar.
#s52	So while we are considering the ranking of term in the list, we're also consider in the redundancy of the candidate term with respect to the terms that we already picked.
#c52	we;the ranking;term;the list;we;the redundancy;the candidate term;respect;the terms;we
#s53	With some thresholding then we can get balance of redundancy removal and also high score over term.
#c53	some thresholding;we;balance;redundancy removal;also high score;term
#s54	OK so after this then we will get K topical terms and those can be regarded as the topics that we discovered from the collection.
#c54	we;K topical terms;the topics;we;the collection
#s55	Next let's think about how we can "compute the topic coverage
#c55	's;we;the topic coverage
#s56	i j
#c56	i
#s57	So looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document How should we figure out the coverage of each topic in the document?
#c57	this picture;we;sports;travel;science;these topics;you;a document;we;the coverage;each topic;the document
#s58	One approach can be to simply count occurrences of these terms.
#c58	One approach;occurrences;these terms
#s59	So for example, sports might have occurred four times in this document, and travel occurred twice, etc, and then we can just normalize.
#c59	example;sports;this document;travel;we
#s60	these counts as our estimate of the coverage probability for each topic.
#c60	these counts;our estimate;the coverage probability;each topic
#s61	So in general the formula would be to collect the counts of all the terms that represented the topics and then simply normalize them so that.
#c61	the formula;the counts;all the terms;the topics;them
#s62	The coverage of each topic in the document would add to one.
#c62	The coverage;each topic;the document
#s63	This forms a distribution over the topics for the document to characterize coverage of different topics in the document.
#c63	a distribution;the topics;the document;coverage;different topics;the document
#s64	Now, as always when we think about the idea for solving problem, we have to ask the question, how good is this one?
#c64	we;the idea;problem;we;the question;this one
#s65	Or is this the best way of solving the problem?
#c65	the best way;the problem
#s66	So now let's examine this approach.
#c66	's;this approach
#s67	In general, we have to do some empirical evaluation by using actual datasets and to see how well it works.
#c67	we;some empirical evaluation;actual datasets;it
#s68	In this case, let's take a look at a simple example.
#c68	this case;'s;a look;a simple example
#s69	Here we have the text document that is about the NBA basketball game.
#c69	we;the text document;the NBA basketball game
#s70	So in terms of the content, it's about the sports.
#c70	terms;the content;it;the sports
#s71	But if we simply count these words that represent our topics, and we will find that the word sports actually did not occur in the article, even though the content is about the sports.
#c71	we;these words;our topics;we;the word sports;the article;the content;the sports
#s72	So the count of sports is zero.
#c72	the count;sports
#s73	That means the coverage of sports will be  "estimated Now of course.
#c73	the coverage;sports;course
#s74	The term science also did not occur in the document, and it's estimated also zero.
#c74	The term science;the document;it
#s75	That's OK, but sports certainly is not OK.
#c75	sports
#s76	'cause we know the content is about sports.
#c76	we;the content;sports
#s77	So this estimate has problem.
#c77	this estimate;problem
#s78	What's worse, term travel actually occurred in the document, so when we estimate the coverage of the topic travel, we have gotten a non-zero count, so it's estimated coverage would be non zero.
#c78	What;term travel;the document;we;the coverage;the topic travel;we;a non-zero count;it;estimated coverage
#s79	So this obviously is also not desirable.
#s80	So this simple example illustrates some problems of this approach.
#c80	this simple example;some problems;this approach
#s81	First, when we count what words belong to the topic, we also need to consider related words.
#c81	we;what;words;the topic;we;related words
#s82	We can't simply just count the topic.
#c82	We;the topic
#s83	word sports.
#c83	word sports
#s84	In this case, it did not occur at all, but there are many related words like  basketball, game, etc.
#c84	this case;it;many related words;  basketball;game
#s85	So we need to count related words.
#c85	we;related words
#s86	Also.
#s87	The second problem is that a word like star can be actually ambiguous.
#c87	The second problem;a word;star
#s88	So here it probably means a basketball star
#c88	it;a basketball star
#s89	But we can imagine it might also mean a star on the Sky.
#c89	we;it;a star;the Sky
#s90	So in that case the star might actually suggest perhaps a topic of science.
#c90	that case;the star;a topic;science
#s91	So we need to deal with that as well.
#c91	we
#s92	Finally, the main restriction of this approach is that we have only one term to describe this topic So it cannot really describe complicated topics.
#c92	the main restriction;this approach;we;only one term;this topic;it;complicated topics
#s93	For example a very specialized topic would be hard to describe by using just a word or one phrase, we need to use more words, so this example illustrates some general problems with this approach of treating a term as topic.
#c93	example;a very specialized topic;just a word;one phrase;we;more words;this example;some general problems;this approach;a term;topic
#s94	First, it lacks expressive power, meaning that it can only represent the symbol general topics.
#c94	it;expressive power;it;the symbol general topics
#s95	But it cannot represent the complicated topics that might require more words to describe.
#c95	it;the complicated topics;more words
#s96	Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term.
#c96	it;vocabulary coverage;the topic;itself;one term
#s97	It does not suggest what other terms are related to the topic, even if we're talking about the sports, there are many terms that are related, so it does not allow us to easily count related terms toward contributing to coverage of this topic.
#c97	It;what;other terms;the topic;we;the sports;many terms;it;us;related terms;coverage;this topic
#s98	Finally, there's this problem of word sense ambiguation, a topical term or related term can be ambiguous.
#c98	this problem;word sense ambiguation;a topical term;related term
#s99	For example, basketball star versus star in the Sky.
#c99	example;star;the Sky
#s100	So in the next lecture we're going to talk about how to solve the problem with probabilistic modeling of the topic.
#c100	the next lecture;we;the problem;probabilistic modeling;the topic
410	b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	98
#s1	This lecture is about the paradigmatic relation discovery.
#c1	This lecture;the paradigmatic relation discovery
#s2	In this lecture we're going to talk about how to discover a particular kind of word Association called paradigmatic relations.
#c2	this lecture;we;a particular kind;word;Association;paradigmatic relations
#s3	By definition, 2 words are paradigmatically related if they share similar contexts.
#c3	definition;2 words;they;similar contexts
#s4	Namely, they occur in similar positions in text.
#c4	they;similar positions;text
#s5	So naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts.
#c5	our idea;such relation;the context;each word;the similarity;those contexts
#s6	So here's an example of context of word Cat.
#c6	an example;context;word Cat
#s7	Here I have taken the word cat out of the context.
#c7	I;the word;cat;the context
#s8	And you can see we are seeing some remaining words in the sentences that contain cat.
#c8	you;we;some remaining words;the sentences;cat
#s9	Now we can do the same thing for another word like a dog.
#c9	we;the same thing;another word;a dog
#s10	So in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog.
#c10	we;such a context;the similarity;the context;cat;the context;a word;dog
#s11	So now the question is, how can we formally represent the context and then define the similarity function?
#c11	the question;we;the context;the similarity function
#s12	So first we note that the context actually contains a lot of words.
#c12	we;the context;a lot;words
#s13	So they can be regarded as a pseudo document.
#c13	they;a pseudo document
#s14	An imaginary document.
#c14	An imaginary document
#s15	But there are also different ways of looking at the context.
#c15	different ways;the context
#s16	For example, we can look at the word that occurs before the word cat.
#c16	example;we;the word;the word cat
#s17	We can call.
#c17	We
#s18	We can call this context left1 context.
#c18	We;this context left1 context
#s19	So in this case you will see words like my, his or big, a, the, etc.
#c19	this case;you;words;my
#s20	These are the words that can occur to the left of the world cat.
#c20	the words;the left;the world cat
#s21	So we say my cat, his cat big cat.
#c21	we;my cat;his cat big cat
#s22	a cat etc.
#c22	a cat
#s23	Similarly, we can also collect the words that occur right after the word cat.
#c23	we;the words;the word;cat
#s24	We can call this context right1.
#c24	We;this context right1
#s25	And here we see words eats, ate, is, has, etc.
#c25	we;words
#s26	Or more generally, we can look at the all the words in the window of text around the word cat.
#c26	we;the all the words;the window;text;the word cat
#s27	Here let's say we can take a window of eight words around the world cat.
#c27	's;we;a window;eight words;the world cat
#s28	We call this context Window8
#c28	We;this context
#s29	Now of course, you can see all the words from left or from right, and so we have a bag of words in general to represent the context.
#c29	course;you;all the words;right;we;a bag;words;the context
#s30	Now, such a word based representation would actually give us interesting way to define the perspective of measuring the similarity. "
#c30	such a word based representation;us;interesting way;the perspective;the similarity
#s31	  similarity of left1, then we'll see words that share just the words in the left context and we kind of ignore the other words that are also in the general context.
#c31	left1;we;words;just the words;the left context;we;the other words;the general context
#s32	So that gives us one perspective to measure the similarity.
#c32	us;one perspective;the similarity
#s33	And similarly, if we only use the right1 context will capture the similarity from another perspective.
#c33	we;the right1 context;the similarity;another perspective
#s34	Using both left1 and right1, ofcourse would allow us to capture the similarity with even more strict criteria.
#c34	both left1;right1;us;the similarity;even more strict criteria
#s35	So in general, context may contain adjacent words like eats and my that you see here or non-adjacent words like Saturday, Tuesday or some other words in the context.
#c35	context;adjacent words;eats;my;you;non-adjacent words;Saturday;the context
#s36	And this flexibility also allows us to measure the similarity similarity in some other different ways.
#c36	this flexibility;us;the similarity similarity;some other different ways
#s37	Sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics.
#c37	we;similarity;general content;us;loosely related paradigmatic relations;you;only the words;the left;the right;the world;you;words;their syntactical categories;semantics
#s38	So the general idea of discovering paradigmatic relations is to compute the similarity of context of two words.
#c38	the general idea;paradigmatic relations;the similarity;context;two words
#s39	So here for example, we can measure the similarity of cat and dog based on the similarity of their contexts.
#c39	example;we;the similarity;cat;dog;the similarity;their contexts
#s40	In general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts.
#c40	we;all kinds;views;the context;the similarity function;general combination;similarities;different contexts
#s41	And of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context.
#c41	course;we;weights;these different similarities;us;particular kind;context;again here that main idea;paradigmatically related words;the similarity;their context
#s42	So next, let's see how we exactly compute these similarity functions.
#c42	's;we;these similarity functions
#s43	Now to answer this question it's useful to think of bag of words representation as vectors in the vector space model.
#c43	this question;it;bag;words;vectors;the vector space model
#s44	Now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search.
#c44	you;who;information retrieval;text retrieval techniques;vector space model;documents;queries;search
#s45	But here we also find it convenient to model the context of a word for paradigmatically relation discovery.
#c45	we;it;the context;a word;paradigmatically relation discovery
#s46	So the idea of this approach is to view each word in our vocabulary as defining one dimension in high dimensional space so we have N words in total in the vocabulary.
#c46	the idea;this approach;each word;our vocabulary;one dimension;high dimensional space;we;N words;total;the vocabulary
#s47	Then we have N dimensions as illustrated here.
#c47	we;N dimensions
#s48	And on the bottom you can see frequency vector representing a context.
#c48	the bottom;you;frequency vector;a context
#s49	And here we see when eats occured five times in this context, ate occurred three times etc.
#c49	we;this context
#s50	So this vector can then be placed in this vector space model.
#c50	this vector;this vector space model
#s51	So in general, we can represent a pseudo document or context of cat as one vector.
#c51	we;a pseudo document;context;cat;one vector
#s52	d1.
#c52	d1
#s53	An another word dog might give us a different context, so d2.
#c53	An another word dog;us;a different context;d2
#s54	And then we can measure the similarity of these two vectors.
#c54	we;the similarity;these two vectors
#s55	So by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity.
#c55	context;the vector space model;we;the problem;paradigmatic relations discovery;the problem;the vectors;their similarity
#s56	So the two questions that we have to address is first how to compute each vector, that is, how to compute the xi or yi?
#c56	the two questions;we;each vector;the xi;yi
#s57	And the other question is, how do you compute the similarity?
#c57	the other question;you;the similarity
#s58	Now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval.
#c58	many approaches;the problem;them;information retrieval
#s59	And they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here.
#c59	they;a query vector;a document vector;we;the ideas;the similarity;context documents;our purpose
#s60	So let's first look at the one possible approach, where we try to measure the similarity of context based on the expected overlap of words and we call this EOWC.
#c60	's;the one possible approach;we;the similarity;context;the expected overlap;words;we;this EOWC
#s61	So the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word.
#c61	the idea;a context;award vector;each word;a weight;the probability;a randomly picked word;this document vector;this word
#s62	So in other words.
#c62	other words
#s63	xi is defined as the normalized count of word wi in the context.
#c63	xi;the normalized count;word;wi;the context
#s64	And this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word.
#c64	a probability;you;this word;d1;you;the word
#s65	Now of course these xi's will sum to 1 because they are normalized frequencies.
#c65	course;these xi;they;frequencies
#s66	And this means the vector is actually probability distribution over words.
#c66	the vector;probability distribution;words
#s67	So, the vector d2 can be also computed in the same way.
#c67	the vector d2;the same way
#s68	And this would give us then two probability distributions representing two contexts.
#c68	us;two probability distributions;two contexts
#s69	So that addresses the problem how to compute the vectors?
#c69	the problem;the vectors
#s70	Next, let's see how we can define similarity in this approach.
#c70	's;we;similarity;this approach
#s71	Well, here we simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors.
#c71	we;the similarity;a dot product;two vectors;the sum;the products;all the corresponding elements;the two vectors
#s72	Now it's interesting to see that this similarity function actually has a nice interpretation.
#c72	it;this similarity function;a nice interpretation
#s73	And there is this dot product  infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical?
#c73	this dot product;  infact;us;the probability;words;the two contexts;we;a word;one context;another word;another context;we;the question;they
#s74	If the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical.
#c74	the two contexts;we;we;the two words;the two contexts
#s75	If they are very different then the chance of seeing identical words being picked from the two contexts would be small.
#c75	they;the chance;identical words;the two contexts
#s76	So this intuitively makes sense for measuring similarity of contexts.
#c76	sense;similarity;contexts
#s77	Now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.
#c77	you;a look;the exact formulas;the probability;two randomly picked words
#s78	So if you just stay at the formula  to check what's inside this sum then you will see, basically in each case it gives us the probability that we'll see overlap on a particular word, wi and where xi gives us the probability that will pick this particular word from d1 and  yi gives us the probability of picking this word from d2 and when we pick the same word from the two contexts then we have identical  pick.
#c78	you;the formula;what;this sum;you;each case;it;us;the probability;we;overlap;a particular word;wi;xi;us;the probability;this particular word;d1;  yi;us;the probability;this word;d2;we;the same word;the two contexts;we;identical  pick
#s79	Alright, so that's one possible approach.
#c79	one possible approach
#s80	EOWC expected overlap of words in context.
#c80	EOWC;overlap;words;context
#s81	Now, as always, we would like to assess whether this approach it would work well.
#c81	we;whether this approach;it
#s82	Now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations.
#c82	course;we;the approach;real data;it;us;really semantically related words;us;a paradigmatic relations
#s83	But analytically, we can also analyze this formula little bit.
#c83	we;this formula
#s84	So first, as I said, it does make sense right?
#c84	I;it;sense
#s85	because this formula will give a higher score if there is more overlap between the two contexts.
#c85	this formula;a higher score;more overlap;the two contexts
#s86	So that's exactly what we want.
#c86	exactly what;we
#s87	But if you analyze the formula more carefully, then you also see there might be some potential problems.
#c87	you;the formula;you;some potential problems
#s88	And specifically there are two potential problems.
#c88	two potential problems
#s89	First it might favor matching one frequent term very well over matching more distinct terms, and that is because in the dot product, if one element has a high value and this element is shared by both context and it contributes a lot to the overall sum.
#c89	it;one frequent term;more distinct terms;the dot product;one element;a high value;this element;both context;it;a lot;the overall sum
#s90	And it might indeed make the score higher than in another case where the two vectors actually have a lot of overlap in different terms, but each term has a relatively low frequency.
#c90	it;the score;another case;the two vectors;a lot;overlap;different terms;each term;a relatively low frequency
#s91	So this may not be desirable.
#s92	Of course, this might be desirable in some other cases, but in our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context.
#c92	some other cases;our case;we;a case;we;more different terms;the context;we;more confidence;the two words;similar context
#s93	If you only rely on one term and that's a little bit questionable.
#c93	you;one term
#s94	It may not be robust.
#c94	It
#s95	The second problem is that it treats every word equally, so  if you match a word like the, and match was, it would be the same as matching on the word like eats.
#c95	The second problem;it;every word;you;a word;match;it;the word;eats
#s96	But intuitively we know matching the isn't really surprising because the occurs everywhere, so matching the is not as such  a strong evidence as matching a word like eats  which doesn't occur frequently.
#c96	we;a strong evidence;a word;eats
#s97	So this is another problem of this approach.
#c97	another problem;this approach
#s98	In the next lecture, we're going to talk about how to address these problems.
#c98	the next lecture;we;these problems
410	c1e55739-56e6-4547-9bec-86797f401ea8	20
#s1	There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure.
#c1	many more advanced learning algorithms;then the regression based approaches;they;the optimizer retrieval measure
#s2	like MAP or nDCG.
#c2	MAP;nDCG
#s3	Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents.
#c3	the optimization objective function;we;the previous slide;retrieval measure;the prediction;we;the ranking;those documents
#s4	One can imagine that why our prediction may not be too bad.
#c4	One;our prediction
#s5	Let's say both are around .5, so it's kinda of in the middle of zero and one for the two documents.
#c5	's;it;the middle;the two documents
#s6	But the ranking can be wrong, so we might have got a larger value for D2 and than D1.
#c6	the ranking;we;a larger value;D2;D1
#s7	So that won't be good from retrieval perspective, even though by likelihood function is not bad.
#c7	retrieval perspective;likelihood function
#s8	In contrast, we might have another case where we predicted values all around .9, let's say and by the objective functioning the error will be larger, but if we can get the order of the two documents correct, that's actually a better result.
#c8	contrast;we;another case;we;values;'s;the error;we;the order;the two documents;a better result
#s9	So these new, more advanced approaches will try to correct that problem.
#c9	these new, more advanced approaches;that problem
#s10	Of course, then the challenge is that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end to know more about the these approaches.
#c10	the challenge;the optimization problem;researchers;many solutions;the problem;you;more reference;the end;the these approaches
#s11	Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem.
#c11	approaches;they;many other ranking problems;not just the retrieval problem
#s12	So here I list some, for example recommender systems, computational advertising or summarization and there are many others that you can probably encounter in your applications.
#c12	I;example;recommender systems;computational advertising;summarization;many others;you;your applications
#s13	To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results.
#c13	this lecture;we;multiple features;ranking results
#s14	Actually the use of machine learning in information retrieval has started since many decades ago.
#c14	the use;machine learning;information retrieval
#s15	So, for example, the Rocchio feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems, and first, it's mostly driven by the availability of a lot of training data in the form of click throughs.
#c15	example;the Rocchio feedback;we;machine learning approach;relevance feedback;the most recent use;machine learning;some changes;the environment;applications;retrieval systems;it;the availability;a lot;training data;the form;click throughs
#s16	Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this.
#c16	Such data;the data;a lot;useful knowledge;the relevance and machine learning methods;leverage
#s17	Secondly, it's also driven by the need for combining many features and this is not only just because there are more features available on the web that can be naturally used to improve scoring, it's also because by combining them we can improve the robustness of ranking, so this is desired for combating spams.
#c17	it;the need;many features;more features;the web;scoring;it;them;we;the robustness;ranking;spams
#s18	Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing.
#c18	Modern search engines;some kind;machine learning techniques;many features;ranking;a major feature;these commercial engines;Google;Bing
#s19	The topic of learning to rank is still active research topic in the community and so you can expect to see new results being developed in the next few years, perhaps.
#c19	The topic;rank;active research topic;the community;you;new results;the next few years
#s20	Here are some additional readings that can give you more information about how learning to rank works and also some advanced methods.
#c20	some additional readings;you;more information;works;also some advanced methods
410	c2e926e1-eba7-4512-9b8b-594be572e4df	12
#s1	this letter is about the link analysis for web search in this lecture we're going to talk about web search and particularly focusing on how to do link analysis and use the results to improve search the main topic of this lecture is to look at the ranking algorithms for web search in the previous after we talked about the how to create index now that we have got the index we want to see how we can improve ranking of pages the web
#c1	this letter;the link analysis;web search;this lecture;we;web search;analysis;the results;the main topic;this lecture;the ranking algorithms;web search;we;index;we;the index;we;we;ranking;pages;the web
#s2	i'll stand the IR models can be also applied here in fact that they are important the building blocks for improvement for supporting web search but they aren't sufficient an mainly for the following reasons first on the web we tend to have very different information needs for example people might search for a web page or entry page and this is different from the traditional library search people are primarily interested in collecting literature information so this kind of queries often called navigation operates the purposes or navigate into a particular target page so for such queries we might benefit from using linking permission secondly documents have additional information and on the web web pages are well format there are lots of other crews such as the layout of the title or link information again so this has provide the opportunity to use extra context information of a document to improve the scoring and finally information quality varies a lot that means we have to consider many factors to improve the ranking algorithm this would give us a more robust way through rank the pages making it harder for N S member to just manipulate the one signal to improve the ranking of page so as a result people have made a number of major extensions to the ranking algorithms one line is to explore links to improve scoring and that's the main topic of this active people have also proposed algorithms to exploit the largest scale implicit affair back information in the form of click throats that's of course in the category of feedback techniques and machine learning is often used there in general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features many of them are based on the standard visual models such as BM twenty five that we talked about query likely hold to score different parts of documents or to provide additional features based on content matching but link information is also very useful so they provide additional scoring signals so let's look at links in more detail on the web
#c2	i;the IR models;fact;they;the building;improvement;web search;they;the following reasons;the web;we;very different information;example;people;a web page;entry page;the traditional library search people;literature information;this kind;queries;navigation;the purposes;a particular target page;such queries;we;permission;documents;additional information;the web web pages;well format;lots;other crews;the layout;the title;information;the opportunity;extra context information;a document;the scoring;information quality;we;many factors;the ranking algorithm;us;a more robust way;rank;the pages;it;N S member;the one signal;the ranking;page;a result;people;a number;major extensions;the ranking algorithms;one line;links;scoring;the main topic;this active people;algorithms;the largest scale implicit affair back information;the form;click throats;course;the category;feedback techniques;machine learning;web search;the ranking algorithms;algorithms;all kinds;features;them;the standard visual models;BM;we;query;different parts;documents;additional features;content matching;information;they;additional scoring signals;'s;links;more detail;the web
#s3	so this is snapshot of some part of the web so we can see there are many links link different pages together an in this case you can also look at the centre here there is a description of a link that's pointing to the document on the right side now this description text it's called anchor text if you think about this text it's actually quite useful becaus it provides some extra description of that page being pointed too so for example if someone wants to bookmark amazon dot com front page the person might say the biggest online bookstore and then with a link to amazon right so the description here actually is very similar to what the user would typing in the query box when they are looking for such a page and that's why it's very useful for ranking pages suppose someone types in the query like online bookstore or fixed online bookstore the query would match this anchor text in the page here
#c3	snapshot;some part;the web;we;many links;different pages;this case;you;the centre;a description;a link;the document;the right side;this description;it;anchor text;you;this text;it;quite useful becaus;it;some extra description;that page;example;someone;amazon dot com front page;the person;the biggest online bookstore;a link;amazon;the description;what;the user;the query box;they;such a page;it;ranking pages;someone types;the query;online bookstore;fixed online bookstore;the query;this anchor text;the page
#s4	and then this actually provides evidence for matching the page that's being pointed to that is the amazon entry page so if you match the anchor text that describes a link to a page actually that provides good evidence for the relevance of the page being pointed to it so any attacks are very useful and if you look at the bottom part of this picture you can also see there are some patterns of links and these links might indicate the utility of a document so for example on the right side you can see this page has received a major in english alarm is many other pages are pointing to this page and this shows that this page is quite useful on the left side you can see this is another page that points to many other pages
#c4	evidence;the page;the amazon entry page;you;the anchor text;a link;a page;good evidence;the relevance;the page;it;any attacks;you;the bottom part;this picture;you;some patterns;links;these links;the utility;a document;example;the right side;you;this page;a major;english alarm;many other pages;this page;this page;the left side;you;another page;many other pages
#s5	so this is the directory page that would allow you to actually see a lot of other pages so we can call the first case authority page and the second is a hub page this means the link information can help in two ways one is to provide extra fast for matching and the other is to provide some additional scores for the web pages to characterize how likely are pages how likely pages authoritie so people then of course an proposed ideas to leverage it is this link information google 's page rank which was the main technique that they used in early days is a good example and that that is the algorithm to capture page popularity basically to score authoritie so the intuitions here are links i just like citations in the literature think about the one page pointing to another page this is very similar to one paper citing another paper
#c5	the directory page;you;a lot;other pages;we;the first case authority page;a hub page;the link information;two ways;one;some additional scores;the web pages;pages;pages;people;course;it;this link information;google 's page rank;the main technique;they;early days;a good example;the algorithm;page popularity;authoritie;the intuitions;links;i;citations;the literature;the one page;another page;one paper;another paper
#s6	so of course then if a PP cited off and then we consume this page to be more useful in general so that's a very good intuition now page rank is essentially to take advantage of this intuition to implement it with principle approach intuitively it's essentially doing citation counting or in link counting it just improves the simple idea in two ways one is it will consider in direct citations so that means you don't just look at how many in links you have you also look at what are those pages that are pointing to you if those pages themselves have a lot of england 's
#c6	course;a PP;we;this page;a very good intuition;page rank;advantage;this intuition;it;principle approach;it;citation counting;link;it;the simple idea;two ways;one;it;direct citations;you;links;you;you;what;those pages;you;those pages;themselves;a lot;england
#s7	well
#s8	that means alot in some sense you will get some credit from them but if those pages that are pointing to you are not being pointed to by other pages they themselves don't have many index then well you don't get that much credit so that's the idea of the directive citation i saw you can also understand this idea by looking at again the research papers if you're cited by
#c8	alot;some sense;you;some credit;them;those pages;you;other pages;they;themselves;many index;you;that much credit;the idea;the directive citation;i;you;this idea;the research papers;you
#s9	and it's a ten papers and those ten papers are just workshop papers and that or some papers that are not very influential
#c9	it;a ten papers;those ten papers;workshop papers
#s10	right
#s11	so although you got ten A links and that's not as good as if you are side by ten papers that themselves have attracted a lot of other citations so in this is a case where we would like to consider in direct links an pager anger does that young idea it's going to small citations or assume that basically every page is having a non zero zero citation count essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone the reason why they want to do that is this would allow them to solve the problem elegantly with linear algebra technique
#c11	you;ten A links;you;ten papers;themselves;a lot;other citations;a case;we;direct links;an pager anger;it;small citations;every page;a non zero zero citation count;we;many virtual links;all the pages;you;pseudo citations;everyone;the reason;they;them;the problem;linear algebra technique
#s12	so i think maybe the best way to understand the page rank is through think of this as to compute the probability of random server visiting every web page
#c12	i;the best way;the page rank;think;the probability;random server;every web page
410	c6213520-fc62-43c3-80cd-1e9b91389463	57
#s1	This lecture is a summary of this course.
#c1	This lecture;a summary;this course
#s2	This map shows the major topics we have covered in this course.
#c2	This map;the major topics;we;this course
#s3	And here are some key high level takeaway messages.
#c3	some key high level takeaway messages
#s4	First we talked about the natural language content analysis.
#c4	we;the natural language content analysis
#s5	Here the main takeaway message is natural language processing is the foundation for text retrieval.
#c5	the main takeaway message;natural language processing;the foundation;text retrieval
#s6	But current NLP isn't robust enough, so the bag of words representation is generally the main method used in modern search engines.
#c6	current NLP;the bag;words;representation;the main method;modern search engines
#s7	And it's often sufficient for the most of the search tasks, but obviously for more complex such tasks than we need a deeper natural language processing techniques.
#c7	it;the search tasks;more complex such tasks;we;a deeper natural language processing techniques
#s8	And we then talked about the high level strategies for text access and we talked about the push vs pull.
#c8	we;the high level strategies;text access;we;the push;pull
#s9	In pull, we talked about the querying versus browsing.
#c9	pull;we;the querying;browsing
#s10	In general, in future search engines we should integrate all these techniques to provide multiple information access.
#c10	future search engines;we;all these techniques;multiple information access
#s11	And then we talked about a number of issues related to search engines we talked about the search problem, and we framed that as a ranking problem.
#c11	we;a number;issues;search engines;we;the search problem;we;a ranking problem
#s12	And we talked about a number of retrieval methods.
#c12	we;a number;retrieval methods
#s13	We started with the overview of vector space model and the probabilistic model, and then we talked about the vector Space Model in depth.
#c13	We;the overview;vector space model;the probabilistic model;we;the vector Space Model;depth
#s14	and we also later talked about the language modeling approaches, and that's a probabilistic model and here The main takeaway messages is that modern retrieval functions tend to look similar, and they generally use various heuristics.
#c14	we;the language modeling approaches;a probabilistic model;The main takeaway messages;modern retrieval functions;they;various heuristics
#s15	Most important ones are TF-IDF weighting, document length normalization, and TF is often transformed through a sublinear Transformation function.
#c15	Most important ones;TF-IDF weighting;document length normalization;TF;a sublinear Transformation function
#s16	And then we talked about how to implement a retrieval system.
#c16	we;a retrieval system
#s17	And here the main techniques that we talked about are how to construct the inverted index so that we can prepare the system to answer query quickly.
#c17	the main techniques;we;the inverted index;we;the system;query
#s18	And we talked about how to perform search by using the inverted index.
#c18	we;search;the inverted index
#s19	And we then talked about how to evaluate the text retrieval system.
#c19	we;the text retrieval system
#s20	Mainly introduced the Cranefield evaluation methodology.
#c20	the Cranefield evaluation methodology
#s21	This is a very important evaluation methodology that can be applied to many tasks.
#c21	a very important evaluation methodology;many tasks
#s22	We talked about the major evaluation measures, so the most important measures for search engine--map(mean average precision),  nDCG(normalized, discounted cumulative gain), an also precision and recall are the two basic measures.
#c22	We;the major evaluation measures;the most important measures;search engine;map(mean average precision;discounted cumulative gain;an also precision;recall;the two basic measures
#s23	And we then talked about the feedback techniques and we talked about the Rocchio in the vector space model and the mixture model in the language modeling approach.
#c23	we;the feedback techniques;we;the Rocchio;the vector space model;the mixture model;the language modeling approach
#s24	Feedback is a very important technique, especially considering the opportunity of learning from a lot of clickthroughs on the web.
#c24	Feedback;a very important technique;the opportunity;a lot;clickthroughs;the web
#s25	We then talked about Web search, and here we talked about how to use parallel indexing to solve the scalability issue.
#c25	We;Web search;we;parallel indexing;the scalability issue
#s26	In indexing we introduce the map reduce
#c26	indexing;we;the map
#s27	and then we talked about how to use linking information on the web to improve search.
#c27	we;information;the web;search
#s28	We talked about page rank and HITS as the major algorithms to analyze links on the web.
#c28	We;page rank;HITS;the major algorithms;links;the web
#s29	We then talked about learning to rank.
#c29	We
#s30	This is the use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach, but we can also improve the robustness of the ranking function so that it's not easy to spam the search engine with just.
#c30	the use;multiple features;scoring;not only the effectiveness;this approach;we;the robustness;the ranking function;it;the search engine
#s31	Some features to promote a page.
#c31	a page
#s32	And finally, we talked about the future of web search.
#c32	we;the future;web search
#s33	We talked about some major directions that we might see in the future in improving the current generation of search engines.
#c33	We;some major directions;we;the future;the current generation;search engines
#s34	And then finally we talked about the recommender systems and these are systems to implement the push mode
#c34	we;the recommender systems;systems;the push mode
#s35	and we talked about the two approaches.
#c35	we;the two approaches
#s36	One is content based, one is collaborative filtering and they can be combined together.
#c36	content;one;collaborative filtering;they
#s37	Now.
#s38	An obvious missing piece in this picture is the user, you can see, so user interface is also an important component in any search engine, even though the current searching interface is relatively simple, they actually have been a lot of studies of user interface related to visualization for example.
#c38	An obvious missing piece;this picture;the user;you;user interface;an important component;any search engine;the current searching interface;they;a lot;studies;user interface;visualization;example
#s39	And this is a topic that you can learn more by reading this book.
#c39	a topic;you;this book
#s40	It's an excellent book about all kinds of studies of search interface.
#c40	It;an excellent book;all kinds;studies;search interface
#s41	If you want to know more about the topics that we talked about, you can also read some additional readings that are listed here in this short course.
#c41	you;the topics;we;you;some additional readings;this short course
#s42	We only managed to cover some basic topics in text retrieval and search engines.
#c42	We;some basic topics;text retrieval and search engines
#s43	And these resources provide additional information about the more advanced topics, and they gave a more thorough treatment of some of the topics that we talked about and a main source is synthesis digital library.
#c43	these resources;additional information;the more advanced topics;they;a more thorough treatment;the topics;we;a main source;synthesis digital library
#s44	Where you can see a lot of short textbook or textbooks or long tutorials, they tend to provide a lot of information to explain a topic and there are multiple serieses that are related to this course, and one is the information concepts retrieval and services and another is Human language technology and yet another is artificial intelligence and machine learning.
#c44	you;a lot;short textbook;textbooks;long tutorials;they;a lot;information;a topic;multiple serieses;this course;the information concepts retrieval;services;Human language technology;artificial intelligence;machine learning
#s45	There were also some major journals and conferences listed here that tend to have a lot of research papers related to the topic of this course, and finally, For more information about resources, including readings and toolkits, etc.
#c45	some major journals;conferences;a lot;research papers;the topic;this course;more information;resources;readings;toolkits
#s46	You can check out this URL.
#c46	You;this URL
#s47	So if you have not taken the text mining course in this data mining specialization series, then naturally the next step is to take that course as this picture shows to mine big text data we generally need two kinds of techniques, one is text retrieval, which is covered in this course and these techniques would help us convert the raw big text data into small relevant text data which are actually needed in the specific application.
#c47	you;the text mining course;this data mining specialization series;the next step;that course;this picture;big text data;we;two kinds;techniques;text retrieval;this course;these techniques;us;the raw big text data;small relevant text data;the specific application
#s48	Human plays an important role in mining any text data becausw text data is written for humans to consume, so involving humans in the process of data mining is very important.
#c48	Human;an important role;any text data;becausw text data;humans;humans;the process;data mining
#s49	And in this course we have covered various strategies to help users get access to the most relevant data.
#c49	this course;we;various strategies;users;access;the most relevant data
#s50	These techniques are also essential in any text mining system to help provide provenance, stand to help users interpret in the patterns that user would find through text data mining.
#c50	These techniques;any text mining system;provenance;users;the patterns;user;text data mining
#s51	So in general the user would have to go back to the original data to better understand the patterns.
#c51	the user;the original data;the patterns
#s52	So the text mining course, or rather text mining analytics course will be dealing with what to do once the user has found the information.
#c52	the text mining course;rather text mining analytics course;what;the user;the information
#s53	So this is the second step in this picture where we would convert the text data into actionable knowledge.
#c53	the second step;this picture;we;the text data;actionable knowledge
#s54	And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge buried in text and such knowledge can then be used in application system to help decision making or to help user finish a task.
#c54	users;the found information;the patterns;knowledge;text;such knowledge;application system;decision making;user;a task
#s55	So if you have not taken that course, the natural step in the natural next step would be to take that course.
#c55	you;that course;the natural step;the natural next step;that course
#s56	Thank you for taking this course.
#c56	you;this course
#s57	I hope you have found this course to be useful to you and I look forward to interacting with you at a future opportunity.
#c57	I;you;this course;you;I;you;a future opportunity
410	c78efb34-5da4-4f3b-aefa-9fa2f2f5337d	18
#s1	in this lecture we are going to talk about how to improve the instantiation of the vector space model this is a continual discussion of the vector space model we're going to focus on how to improve the instantiation of this model in the previous lecture you have seen that with simple since initiations over the vector space model we can come up with a simple scoring function that will give us basically account of how many unique query terms are matching the document we also have seen that this function has a problem as shown on this slide in particular if you look at these three documents they will all get the same score becaus they matched three unique query words but intuitively would like A T four to be ranked above the three and D two is really non relevant so the problem here is that this function couldn't capture at the following heuristics first we would like to give more credit to two D four becaus it matched the presidential more times than the three second intuitively matching presidential should be more important than matching about because about is a very common word that occurs everywhere it doesn't really carry that much content so in this lecture let's see how we can improve the model to solve these two problems it's worth thinking at this point about why do we have this role problems if we look back at assumptions we have made while instantiating the vector space model we will realize that the problem is really coming from some of the assumptions in particular it has to do with how we place the vectors in the practice space so then naturally in order to fix these problems we have to revisit those assumptions perhaps we will have to use different ways to instantiate the vectors based model in particular we have to place the vectors in the different way so let's see how can you prove this while natural thought is in order to consider multiple times of the term in the document we should consider the term frequency instead of just absence or presence in order to consider the difference between a document where a queer at home occured in multiple times and one where the query term occur just once we have to consider the term frequency the calendar of a term in the document in the simplest model with only model the presence and absence of the time we ignored the actual number of times that a term occurs in the document so let's add this back so we can do then represent document by a vector was term frequency as elements so that is to say now the elements of both the query about the and the document about the will not be zero or once but instead there will be the counts of award in the query or the document
#c1	this lecture;we;the instantiation;the vector space model;a continual discussion;the vector space model;we;the instantiation;this model;the previous lecture;you;initiations;the vector space model;we;a simple scoring function;us;basically account;how many unique query terms;the document;we;this function;a problem;this slide;you;these three documents;they;the same score becaus;they;three unique query words;A T;the problem;this function;the following heuristics;we;more credit;two D four becaus;it;the presidential;a very common word;it;that much content;this lecture;'s;we;the model;these two problems;it;this point;we;this role problems;we;assumptions;we;the vector space model;we;the problem;the assumptions;it;we;the vectors;the practice space;order;these problems;we;those assumptions;we;different ways;the vectors;model;we;the vectors;the different way;'s;you;natural thought;order;multiple times;the term;the document;we;the term frequency;just absence;presence;order;the difference;a document;a queer;home;multiple times;the query term;we;the term frequency;the calendar;a term;the document;the simplest model;only model;the presence;absence;the time;we;the actual number;times;a term;the document;'s;we;document;a vector;term frequency;elements;the elements;both the query;the document;the counts;award;the query;the document
#s2	so this was bringing additional information about the document so this can be seen as more accurate representation of our documents so now let's see what the formula would look like if we change this representation so as you see on this slide we still use dot product and so the formula looks very similar in the form in fact it looks identical but inside the sum of course zion why are now different then out of the counts of words i in the query and in the document now at this point i also suggest you to pause the lecture for moment and just think about how we can interpret the score of this new function it's doing something very similar to what the simplest at V S M is doing but be cause of the change of the vector now the new score has a different interpretation can you see the difference
#c2	additional information;the document;more accurate representation;our documents;'s;what;the formula;we;this representation;you;this slide;we;dot product;the formula;the form;fact;it;the sum;course;the counts;words;i;the query;the document;this point;i;you;the lecture;moment;we;the score;this new function;it;something;what;V S M;cause;the change;the vector;the new score;a different interpretation;you;the difference
#s3	and it has to do with the consideration of multiple occurrences of the same term in a document more important they would like to know whether this would fix the problems of the simplest of access based model
#c3	it;the consideration;multiple occurrences;the same term;a document;they;the problems;access based model
#s4	so let's look at this example again so supposedly changed the vector appendage into term frequency vectors now let's look at these three documents again the query vector is the same becaus all these words occur exactly once in the query so the vector is zero zero one vector and in fact D two is also essentially representing the same way
#c4	's;this example;the vector appendage;term frequency vectors;'s;these three documents;the query vector;the same becaus;all these words;the query;the vector;zero zero one vector;fact
#s5	be cause none of these words has been repeated many times as a result of the score is also same steel three the same issue forty three
#c5	none;these words;a result;the score;same steel;the same issue
#s6	and we still have a three but before would be different this is now presidential occured twice here so the element of for presidential in the talk with the vector would be two instead of one as a result now the score forty four is higher
#c6	we;the element;the talk;the vector;a result;the score
#s7	it's a fall now so this means by using term frequency we can now rank D four about D two and D three as we hope to so this solve the problem with default but we can also see that T two and T three R steer treated in the same way this they have identical scores
#c7	it;a fall;term frequency;we;D;D;we;the problem;default;we;T;T three R steer;the same way;they;identical scores
#s8	so it did not fix the problem here so how can we fix this problem intuitively we would like to give more credit for matching presidential then matching about but how can we solve the problem in a general way is there any way to determine which words should be treated more importantly and which word can be basically ignored about is such a word at which does not really care that much content we can essentially ignore that we sometimes call such a word stop word those are generally very frequently occur everywhere match it doesn't really mean anything but computationally how can we capture that so again i encourage you to think a little bit about this can you come up with any statistical approaches to somehow distinguish impressed angel from about if you think about it for a moment you realize that what differences that award like about occurs everywhere so if you count the currents of the world in the whole collection then we would see that about as much higher frequency then presidential which tends to occur only in some documents
#c8	it;the problem;we;this problem;we;more credit;we;the problem;a general way;any way;which words;which word;such a word;that much content;we;we;such a word;word;it;anything;we;i;you;you;any statistical approaches;impressed angel;you;it;a moment;you;what differences;you;the currents;the world;the whole collection;we;as much higher frequency;some documents
#s9	so this idea suggests that we could somehow use the global statistics of terms or some other information to try to down weight the element for about in the vector representation of D two at the same time we hope to somehow increase the weight of presidential in the vector of these three if we can do that then we can expect that D tool will get the overall score to be less sensory Y
#c9	this idea;we;the global statistics;terms;some other information;weight;the element;the vector representation;D;the same time;we;the weight;the vector;we;we;D tool;the overall score;less sensory Y
#s10	A D three will get the school about three then we will be able to rank these on top of D two
#c10	the school;we;top;D
#s11	so how can we do this systematically again we can rely on some statistical counts and in this case the particular idea is called the inverse document frequency we have seen document frequency as one signal used in the modding retrieval functions we discussed this in previous vector
#c11	we;we;some statistical counts;this case;the particular idea;the inverse document frequency;we;document frequency;one signal;the modding retrieval functions;we;previous vector
#s12	so here's a specific way of using it document frequency is the calendar of documents that contain a particular term here we say inverse document frequency becaus we actually want to reward award that doesn't occur in many document and so the way to incorporate this into our vector repetition is to modify the frequency count by multiplying it by the IDF of the corresponding word assume here if we can do that then we can't analyze common words which generally have a lower idea and reward rail words which we have a high IDF so more specifically the IDF can be defined as a logarithm of N plus one divided by K wear em is the total number of documents in the collection K is the TF or document frequency the total number of documents containing the word
#c12	a specific way;it;document frequency;the calendar;documents;a particular term;we;inverse document frequency becaus;we;award;many document;so the way;our vector repetition;the frequency count;it;the IDF;the corresponding word assume;we;we;common words;a lower idea;rail words;we;a high IDF;the IDF;a logarithm;N;K wear;em;the total number;documents;the collection K;the TF or document frequency;the total number;documents;the word
#s13	W now if we plot this function by varying K then you want to see the curve would look like this in general you can see it would give higher value for low D F word a rare work you can also see the maximum value of this function is log of N plus one will be interesting for you to think about what's minimum value for this function this could be interesting exercise now this specific function may not be as important as the heuristic to simply penalize popular times but it turns out that this particular function form has also worked very well now whether there is a better form of function here is still open research question but it's also clear that if we use a linear panelization like what's shown here with this line then it may not be as reasonable as the standard IDF in particular you can see the difference in the standard IDF
#c13	we;this function;K;you;the curve;you;it;higher value;low D F word;a rare work;you;the maximum value;this function;log;N;you;what;minimum value;this function;interesting exercise;this specific function;the heuristic;popular times;it;this particular function form;a better form;function;open research question;it;we;a linear panelization;what;this line;it;the standard IDF;you;the difference;the standard IDF
#s14	and we somehow have turning point here after this point that we're going to say these terms are essentially not very useful there can be essentially ignored
#c14	we;point;this point;we;these terms
#s15	and this makes sense when the term occurs so frequently and let's say a term occurs in more than fifty percent of the documents then the term is unlikely very important and it's basically a common term it's not very important match this water so with the standard idea you can see it's basically assuming that they all have low weights there's no difference but if you look at the linear penalization at this point there is there some difference so intuitively would want to focus more on the discrimination of low D F words rather than these common words well of course which one works better still has to be validated by using the empirically created data set and we have to use users to judge which results are better so now let's see how this can solve problem too so now let's look at the two documents again now without the idea of waiting before we just have term frequency vectors but with the idea of waiting we now can adjust the T F weight by multiplying with the idea of value for example here we can see is adjustment and in particular for about there is adjustment by using the idea of value of about which is smaller than the IDF value of presidential
#c15	sense;the term;'s;a term;more than fifty percent;the documents;the term;it;a common term;it;very important match;this water;the standard idea;you;it;they;low weights;no difference;you;the linear penalization;this point;some difference;the discrimination;low D F words;these common words;course;the empirically created data;we;users;which results;'s;problem;'s;the two documents;the idea;we;term frequency vectors;the idea;we;the T F weight;the idea;value;example;we;adjustment;adjustment;the idea;value;the IDF value
#s16	so if you look at this the idea where distinguish these two words as a result of adjustment here would be larger would make this weight larger so if we score with these new vectors then what would happen is that of course they share the same weights for news then the campaign but the matching of about an presidential will discriminate them so now as a result of IDF weighting we will have the three to be ranked above D two becaus it match the real world whereas D two match the common word so this shows that the idea of waiting can solve problem too so how effective is this model in general when we use TF IDF weighting well let's look at the all these documents that we have seen before these are the newest scores of the new documents but how effective is this new weighting method an new scoring function so now let's see overall how effective is this new ranking function with TF IDF weighting here we show all the five documents that we have seen before and these are their scores now we can see the scores for the first four documents here seem to be quite reasonable they are as we expected however we also see a new problem becaus now TI five here which did not have a very high score with our simplest vectors based model now actually has a very high score in fact it has the highest scope here so this creates a new problem this is actually a common phenomenon in designing retrieval functions basically when you try to fix one problem you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function is there open research question researchers are still working on that but in the next few lectures of urban doors or talk about some additional ideas to further improve this model and try to fix this problem so to summarize this lecture we've talked about how to improve the vectors based model
#c16	you;the idea;these two words;a result;adjustment;this weight;we;these new vectors;what;course;they;the same weights;news;the campaign;the matching;about an presidential;them;a result;IDF weighting;we;D;two becaus;it;the real world;D;the common word;the idea;problem;this model;we;TF IDF weighting;'s;the all these documents;we;the newest scores;the new documents;this new weighting method;an new scoring function;'s;this new ranking function;TF IDF weighting;we;all the five documents;we;their scores;we;the scores;the first four documents;they;we;we;a new problem becaus;a very high score;our simplest vectors;model;a very high score;fact;it;the highest scope;a new problem;a common phenomenon;retrieval functions;you;one problem;you;other problems;it;effective ranking function;what;the best ranking function;open research question researchers;the next few lectures;urban doors;some additional ideas;this model;this problem;this lecture;we;the vectors;model
#s17	an we have gotten improve the instantiation of the vectors based model based on TF IDF weighting so the improvement most of that is on the placement of the vector where we give higher weight to a term that occur them many times in a document but infrequently in the whole collection and we have seen that this improvement model indeed works better than the simplest vectors based model
#c17	we;the instantiation;the vectors;model;TF IDF weighting;so the improvement;the placement;the vector;we;higher weight;a term;them;a document;the whole collection;we;this improvement model;the simplest vectors
#s18	but it also still has some problems in the next structure we're going to look at the how do address these additional problems
#c18	it;some problems;the next structure;we;these additional problems
410	c91df6aa-48b6-4ceb-a12e-f31953378688	19
#s1	this lecture is about how to evaluate the text retrieval system when we have multiple levels of judgments in this lecture we will continue the discussion of evaluation we're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments so so far we have talked about binary judgments that means a document is judged as being random in the owner adam but earlier we also talked about relevance as a matter of degree
#c1	this lecture;the text retrieval system;we;multiple levels;judgments;this lecture;we;the discussion;evaluation;we;the text retrieval system;we;multiple level;judgments;we;binary judgments;a document;the owner;adam;we;relevance;a matter;degree
#s2	so we often can't distinguish it very highly relevant documents those are very useful documents from your moderator random in the documents they're OK they're used for perhaps and further from nonrandom documents those are not useful so imagine you can have ratings for these pages then you would have multiple levels of ratings for example here i show example of three levels through evil relevant sorry three four very relevant to for marginally relevant than one for non relevant now how do we evaluate the search engine system using these judgments obviously the map doesn't work average precision doesn't work precision and recall it doesn't involve becaus they rely on binary judgments
#c2	we;it;very highly relevant documents;very useful documents;your moderator;the documents;they;they;nonrandom documents;you;ratings;these pages;you;multiple levels;ratings;example;i;example;three levels;we;the search engine system;these judgments;the map;average precision;precision;it;becaus;they;binary judgments
#s3	so let's look at the some top ranked results when using these judgments right imagine the user would be most really care about the top ten results here right
#c3	's;the some top;results;these judgments;the user;the top ten results
#s4	and we marked the rating levels or relevance levels for these documents as assume here three two one one three etc
#c4	we;the rating levels;relevance levels;these documents;assume
#s5	and we call these game and the reason why we call it again is be cause the measure that we're introducing is called endd normalized discounted cumulative gain so this game basically can measure how much gain of relevant information a user can obtain by looking at each document right so looking at the first document that the user can gain three points looking at the non random document user with only game one point by looking at the moderate are relevant or marginal relevant document the user would get two points it cetera so this gain intuitive the mesh is the utility of a document from the users perspective of course if you assume the user stops at the ten documents
#c5	we;these game;the reason;we;it;the measure;we;endd;discounted cumulative gain;this game;how much gain;relevant information;a user;each document;the first document;the user;three points;the non random document user;only game;relevant or marginal relevant document;the user;two points;it;this gain;the mesh;the utility;a document;the users perspective;course;you;the user;the ten documents
#s6	and we're looking at the color of the ten we can look at the total game of the user and what's that
#c6	we;the color;the ten;we;the total game;the user;what
#s7	well that's simply the sum of this
#c7	the sum
#s8	and we called the cumulative day so if the user stops at the position one that's just for three if the user looks at another document that's a three plus two if the user looks at more documents then the cumulative gain is more of course this is at the cost of spending more time to examine the list so cumulative gain gives us some idea about how much too low gain the user would have if the user examines all these documents now in a TCG we also have another letter here T discounted cumulative gain so why do we want to do discounting well if you look at this cumulative gain there is one deficiency which is did not consider the rank position of these documents so for example looking at some here and we only know there is one highly random document one marginally raring document to non relevant documents we don't really care where they are ranked ideally we want these two to be ranked on the top and which is the case here but how can we capture that intuition well we have to say well this is three here is not as good as this three on the top and that means the contribution of the gain from different positions has to be weighted by their position and this is the idea of discounting basically so we're going to say well the first one doesn't need to be discounted because the user can be assumed that you always see this document but the second while this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it so we divide this cane by the weight based on the position so log of two two is the rank position of this document and when we go to the third position we discount even more because the normalizes log of three and so on and so forth so when we take a such a sum then a lower rear end document will not contribute contribute that much as highly ranked document so that means if you for example switch the position of this let's say this position and this one
#c8	we;the cumulative day;the user;the position;one;the user;another document;the user;more documents;the cumulative gain;course;the cost;more time;the list;so cumulative gain;us;some idea;how much too low gain;the user;the user;all these documents;a TCG;we;another letter;T;cumulative gain;we;you;this cumulative gain;one deficiency;the rank position;these documents;example;we;one highly random document;one marginally raring document;relevant documents;we;they;we;the top;the case;we;that intuition;we;the top;the contribution;the gain;different positions;their position;the idea;we;the first one;the user;you;this document;this one;a small possibility;the user;it;we;this cane;the weight;the position;so log;the rank position;this document;we;the third position;we;the normalizes;we;a such a sum;a lower rear end document;that much as highly ranked document;you;example;the position;'s;this position
#s9	and then you would get more discount if you put for example very relevant document here as opposed to tool here imagine if you put the three here then it would have to be discounted so it's not as good as if we put the three here
#c9	you;more discount;you;example;very relevant document;tool;you;it;it;we
#s10	so this is the idea of discounting OK so now at this point that we have got the discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments so are we happy with this where we can use this rank systems now we still need to do a little more in order to make this measure comfortable across different topics and this is the last step ann by the way here we just show the TCG at ten
#c10	the idea;this point;we;the discounted cumulative gain;the utility;this ranked list;multiple levels;judgments;we;we;this rank systems;we;order;this measure;different topics;the last step;the way;we;the TCG
#s11	right
#s12	so this is the total sum of this EG of all these ten documents so the last step is called N normalization an if we do that then we will get a normalized disease and so how do we do that well the idea here is within the normalized DCG by the ID or TCG at the same cut of what is the ideal DCG well this is any of ID or ranking so imagine
#c12	the total sum;this EG;all these ten documents;the last step;N normalization;we;we;a normalized disease;we;the idea;the normalized DCG;the ID;TCG;the same cut;what;the ideal DCG;ID
#s13	if we have nine documents in the whole collection rated three here and that means in kudo we have nine documents rated three then our ideal rank the listener would have put all these nine documents on the very top so all these would have to be three
#c13	we;nine documents;the whole collection;kudo;we;nine documents;three then our ideal rank;the listener;all these nine documents;the very top
#s14	and then this would be followed by a two here because that's the best we could do after we have run out of trees but all these positions would be series
#c14	we;we;trees;all these positions;series
#s15	right so this would be ideal ranked list
#c15	ideal ranked list
#s16	and then we can compute the TCG for this ideal rank list so this would be given by this formula that you see here
#c16	we;the TCG;this ideal rank list;this formula;you
#s17	and so this idea this would then be used as the normalizer TCG so here and this idea dizzy will be used as a normalizing so you can imagine now normalization essentially it will compare the actual dizzy with the best DVD you can possible to get for this topic now why do we want to do this where by doing this we're map the DCG values into a range of zero through one so the best value or the highest value for every query would be one that's when you're ranked list is in fact the idea list but otherwise in general you will be lower than one now what if we don't do that well you can see this transformation or this normalization doesn't really affect the relative comparison of systems for just one topic becausr this ideal DCG is the same for all the systems
#c17	the normalizer TCG;you;normalization;it;the actual dizzy;the best DVD;you;this topic;we;we;the DCG values;a range;the best value;the highest value;every query;you;list;fact;the idea list;you;we;you;this transformation;this normalization;the relative comparison;systems;just one topic;this ideal DCG;all the systems
#s18	so the ranking of systems based on only DG would be exactly the same as if you rank them based on the normalized DCG the difference however is when we have much more topics becaus if we don't do normalization different topics will have different scales of this EG for a topical like this one we have nine highly relevant documents the DCG can get really high but imagine another case there are only two very relevant documents in total in the whole collection then the heist TCG that any system could achieve for such a topical will not be very high
#c18	the ranking;systems;only DG;you;them;the normalized DCG;the difference;we;much more topics becaus;we;normalization;different topics;different scales;this EG;this one;we;nine highly relevant documents;the DCG;another case;only two very relevant documents;total;the whole collection;the heist;TCG;any system
#s19	so again we face the problem of different scales of DCG values and we take an average it we don't want the average to be dominated by those high values those are again easy queries so by doing the normalization we can avoid the avoided the problem making all the queries contribute equal to the average so this is the idea of NDC G it's useful for measuring ranked list based on multiple level relevance judgments so more in a more general way this is basically a measure that can be applied to any rank the task with multiple level of judgment ann the scale of the judgments can be multiple can be more than binary not only more than binary there can be multiple levels and like one zero five or even more depending on your application and the main idea of this measure i just to summarize is to measure the total utility of the top K documents so you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower rank the document and then finally it would do normalization to ensure comparability across queries
#c19	we;the problem;different scales;DCG values;we;an average;it;we;the average;those high values;easy queries;the normalization;we;the avoided the problem;all the queries;the average;the idea;NDC G;it;ranked list;multiple level relevance judgments;a more general way;a measure;any rank;multiple level;judgment ann;the judgments;multiple levels;your application;the main idea;this measure;i;the total utility;the top K documents;you;a cut;you;the total utility;it;the contribution;a lower rank;the document;it;normalization;comparability;queries
410	c9288ad9-146c-43c5-a84c-95df47b082d0	76
#s1	In this lecture we continue the discussion of vector space model.
#c1	this lecture;we;the discussion;vector space model
#s2	In particular, we're going to talk about the TF transformation.
#c2	we;the TF transformation
#s3	In the previous lecture, we have derived a TF IDF weighting formula using the vector space model.
#c3	the previous lecture;we;a TF IDF weighting formula;the vector space model
#s4	And we have shown that this model actually works pretty well for these examples, as shown on this slide except for D5, which has received very high score.
#c4	we;this model;these examples;this slide;D5;very high score
#s5	Indeed, it has received the highest score among all these documents, but this document is intuitively non relevant, so this is not desirable.
#c5	it;the highest score;all these documents;this document
#s6	In this lecture we're gonna talk about how we can use TF transformation to solve this problem.
#c6	this lecture;we;we;TF transformation;this problem
#s7	Before we discuss the details, let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high score.
#c7	we;the details;'s;a look;the formula;this simple TF IDF;ranking function;this document;such a high score
#s8	So this is the formula and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.
#c8	the formula;you;the formula;you;it;a sum;all the matched query terms
#s9	And inside the sum, each matching query term has a particular weight and this way it is TF IDF weighting.
#c9	the sum;each matching query term;a particular weight;it;TF IDF weighting
#s10	So it has an idea of component where we see two variables.
#c10	it;an idea;component;we;two variables
#s11	One is the total number of documents in the collection.
#c11	the total number;documents;the collection
#s12	And that is M. The other is the document frequency.
#c12	the document frequency
#s13	This is the number of documents that contain this word W. The other variables involved in the formula included the count of the query, term Term.
#c13	the number;documents;this word;The other variables;the formula;the count;the query;term Term
#s14	W in the query and the count of the word in the document.
#c14	W;the query;the count;the word;the document
#s15	If you look at this document again, now it's not hard to realize that the reason why it hasn't received the highest score is be cause it has a very high count of campaign.
#c15	you;this document;it;the reason;it;the highest score;it;a very high count;campaign
#s16	So the count of campaign in this document is a four which is much higher than the other documents and has contributed to the high score of this document.
#c16	the count;campaign;this document;the other documents;the high score;this document
#s17	So intuitively, in order to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document.
#c17	order;the score;this document;we;the contribution;the matching;this term;the document
#s18	And if you think about the matching of terms in a document carefully, you actually would realize.
#c18	you;the matching;terms;a document;you
#s19	We probably shouldn't reward multiple occurrences.
#c19	We;multiple occurrences
#s20	So generously.
#s21	By that I mean the first occurrence of term says a lot about the matching of this term because it goes from zero count to a count of 1 and that increase means a lot.
#c21	I;the first occurrence;term;a lot;the matching;this term;it;zero count;a count;that increase;a lot
#s22	Once we see a word in the document, it's very likely that the document is talking about this word.
#c22	we;a word;the document;it;the document;this word
#s23	If we see a extra occurrence on top of the first occurrence, that is to go from one to two.
#c23	we;a extra occurrence;top;the first occurrence
#s24	Then we also can say that was the 2nd occurrence.
#c24	we;the 2nd occurrence
#s25	Kind of confirmed that it's not a accidental matching of the word.
#c25	it;a accidental matching;the word
#s26	Now we are more sure that this document is talking about this word.
#c26	we;this document;this word
#s27	But imagine we have seen, let's say, 50 times of the word in the document.
#c27	we;'s;the word;the document
#s28	Then adding one extra occurrence is not good to tell us more about the evidence 'cause we already sure that this document is about this word.
#c28	one extra occurrence;us;the evidence;we;this document;this word
#s29	So if you think in this way, it seems that we should restrict the contribution of high count of term.
#c29	you;this way;it;we;the contribution;high count;term
#s30	And that is the idea of TF transformation.
#c30	the idea;TF transformation
#s31	So this transformation function is going to turn the raw count of word into a term frequency, wait for the word in the document.
#c31	this transformation function;the raw count;word;a term frequency;the word;the document
#s32	So here I show in X axis the raw count.
#c32	I;X axis;the raw count
#s33	In Y axis I showed term frequency weight.
#c33	Y axis;I;term frequency weight
#s34	So in the previous ranking functions we actually have implicitly used some kind of transformation.
#c34	the previous ranking functions;we;some kind;transformation
#s35	For example, in the 01 bit vector representation, we actually use researcher transformation function as shown here.
#c35	example;the 01 bit vector representation;we;researcher transformation function
#s36	Basically, if the count is 0, then it has zero weight, otherwise it would have a weight of 1.
#c36	the count;it;zero weight;it;a weight
#s37	It's a flat.
#c37	It
#s38	Now, what about using term count as TF wait, that's the linear function, right?
#c38	term count;TF;the linear function
#s39	So it has just exactly the same way as the count.
#c39	it;just exactly the same way;the count
#s40	We have just seen that this is not desirable.
#c40	We
#s41	So what we want is something like this.
#c41	what;we;something
#s42	So for example, with the logarithm function, we can have a sub linear transformation that looks like this and this would control the influence of really high weight because it's going to lower its inference, yet it will retain the inference of small counts.
#c42	example;the logarithm function;we;a sub linear transformation;the influence;really high weight;it;its inference;it;the inference;small counts
#s43	Or we might want to even bend the curve more by applying logarithm twice.
#c43	we;the curve;logarithm
#s44	Now people have tried all these methods and they are indeed working better than the linear form of the transformation.
#c44	people;all these methods;they;the linear form;the transformation
#s45	But so far what works the best seems to be this special transformation called  BM25 transformation.
#c45	what;this special transformation;BM25 transformation
#s46	BM stands for best matching.
#c46	BM;best matching
#s47	Now in this transformation you can see there's a parameter K here.
#c47	this transformation;you;a parameter K
#s48	And this K controls the upper bound of this function.
#c48	this K;this function
#s49	It's easy to see this function has upper bound.
#c49	It;this function
#s50	Because if you look at the x / x + K where K is non negative number then the numerator will never be able to exceed the denominator, right?
#c50	you;the x / x;+;K;K;negative number;the numerator;the denominator
#s51	So it's upper bounded by K
#c51	it;K
#s52	+ 1.
#s53	This is also difference between this transformation function and the logarithm transformation which doesn't have upper bound.
#c53	difference;this transformation function;the logarithm transformation
#s54	Furthermore, 1 interesting property of this function is that as we vary K, we can actually simulate a different transformation functions, including the two extremes that I've shown here.
#c54	1 interesting property;this function;we;K;we;a different transformation functions;the two extremes;I
#s55	That is a 01 bit transformation and the linear transformation.
#c55	a 01 bit transformation;the linear transformation
#s56	So for example, if we set K to 0 now you can see, the function value would be 1.
#c56	example;we;K;you;the function value
#s57	So we precisely recover the 01 bit transformation.
#c57	we;the 01 bit transformation
#s58	If you set K to a very large number, on the other hand is going to look more like the linear transformation function.
#c58	you;K;a very large number;the other hand;the linear transformation function
#s59	So in this sense, this transformation is very flexible.
#c59	this sense;this transformation
#s60	It allows us to control the shape of the transformation.
#c60	It;us;the shape;the transformation
#s61	It also has a nice block of the upper bound.
#c61	It;a nice block
#s62	And this upper bound is useful to control the influence of a particular time.
#c62	the influence;a particular time
#s63	And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time.
#c63	we;a spammer;the count;one term;all queries
#s64	In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to computer score.
#c64	other words;all terms;we;the weights;computer score
#s65	As I said, this transformation function has worked well so far.
#c65	I;this transformation function
#s66	So to summarize this lecture.
#c66	this lecture
#s67	The main point is that we need to do some linear TV TF transformation, and this is needed to capture the intuition of diminishing return from higher term counts.
#c67	The main point;we;some linear TV TF transformation;the intuition;return;higher term counts
#s68	It's also to avoid dominance by one single term over all others.
#c68	It;dominance;one single term;all others
#s69	This BM25  transformation that we talked about is very interesting.
#c69	This BM25  transformation;we
#s70	It's so far one of the best performing TF transformation formulas.
#c70	It;the best performing TF transformation formulas
#s71	It has upper bound and social robust and effective.
#c71	It
#s72	If you plug this function into our TF IDF weighting vector space model.
#c72	you;this function;our TF IDF;vector space model
#s73	then we would end up having the following ranking function which has a BM 25 TF component.
#c73	we;the following ranking function;a BM 25 TF component
#s74	Now this is already very close to 2.
#s75	a state of the art ranking function called BM 25.
#c75	a state;the art ranking function;BM
#s76	And will discuss how we can further improve this formula in the next lecture.
#c76	we;this formula;the next lecture
410	cc9d1885-767f-4969-b091-62523657bef7	25
#s1	this lecture is about the feedback in the vector space model in this lecture we continue talking about the feedback in text retrieval particularly we're going to talk about feedback in the vector space model as we have discussed before in the case of feedback task of a text retrieval system is rhel learn from examples to improve retrieval accuracy we will have positive examples those are the documents that are assumed little bit relevant or judgment be relevant all of the document that are viewed by users we also have negative examples those are documents known vinaigrette and they can also be the document atascii why users the general method in the vectors based model for feedback is to modify our query about them and we want to place the query about the in a better position to make it accurate and what does that mean exactly well if we think about the query vector i would mean we have to do something to the vector elements and in general that would mean we might add new terms and we might adjust weights of all terms or assign weights to new terms and as a result in general the query we have more terms so we often call this query expansion the most effective method in the vector space model or feel bad news called rock your feedback which was after it proposed several decades ago so the idea is a quite simple we illustrate this idea by using a two dimensional display of all the documents in the collection and also the query vector
#c1	this lecture;the feedback;the vector space model;this lecture;we;the feedback;text retrieval;we;feedback;the vector space model;we;the case;feedback task;a text retrieval system;rhel;examples;retrieval accuracy;we;positive examples;the documents;judgment;the document;users;we;negative examples;documents known vinaigrette;they;the document;why users;the general method;the vectors;model;feedback;our query;them;we;the query;a better position;it;what;we;the query vector;i;we;something;the vector elements;we;new terms;we;weights;all terms;weights;new terms;a result;the query;we;more terms;we;this query expansion;the most effective method;the vector space model;bad news;rock;your feedback;it;the idea;we;this idea;a two dimensional display;all the documents;the collection;also the query vector
#s2	so now we can see the query vector is here in the center and these are all the documents so when you sequera bakhtaran user similarity function will find the most similar documents we are basically join our so called here
#c2	we;the query vector;the center;all the documents;you;bakhtaran user similarity function;the most similar documents;we
#s3	and then these documents would be basically the top rank the documents and this process are relevant documents and these are very LA kings for example this relevant etc
#c3	these documents;the top rank;the documents;this process;relevant documents;very LA kings;example
#s4	and then these minus is our negative documents like this so our goal here is trying to move this query vector to sum position to improve the ritual accuracy by looking at this diagram what do you think where should we move the query vector so that we can improve the retrieval accuracy intuitively where do you want to move the query graph two if you want to think more you can post a video if you think about this picture you can realize that in order to work well in this case you want the query about that we as close to the positive factors as possible that means ideally you want to place the query back that somewhere here or you want to move the query vector closer to this point now so what exactly this point
#c4	these minus;our negative documents;our goal;this query vector;position;the ritual accuracy;this diagram;what;you;we;the query vector;we;the retrieval accuracy;you;the query graph;you;you;a video;you;this picture;you;order;this case;you;the query;the positive factors;you;the query;you;the query vector;this point
#s5	well if you want these relevant documents to rank the on the top you want this to be in the center of all these relevant documents right because then if you draw a circle around this one you get all these relevant documents so that means we can move the query vector toward the center of all the relevant document vectors and this is basically the idea of rock hill of course you can consider the centroid of negative documents and we want to move away from the negative log not joe match
#c5	you;these relevant documents;the top;you;the center;all these relevant documents;you;a circle;this one;you;all these relevant documents;we;the query vector;the center;all the relevant document vectors;the idea;rock hill;course;you;the centroid;negative documents;we;the negative log;not joe
#s6	but they were talking about the moving closer to some other back then away from other vectors algebraically it just means we have this formula here you can see this is original query vector anne this average basically is the centroid vector of relevant documents when we take the average of these vectors there were computing the centroid of these vectors a similarly this is the average of that now relevant locking in the factors
#c6	they;other vectors;it;we;this formula;you;original query vector anne;this average;the centroid vector;relevant documents;we;the average;these vectors;the centroid;these vectors;the average;the factors
#s7	so it's essentially of now remember documents
#c7	it;documents
#s8	and we have these three parameters here awful beta and gamma there controlling the amount of movement when we add these two actors together we're moving the query that the closer to the central later
#c8	we;these three parameters;awful beta;gamma;the amount;movement;we;these two actors;we;the query
#s9	i said well add them together when we subtract this part what kind of move the query vector away from that century so this is the main idea of rock hill feedback and after we have done this we work at a new query vector which can be used to score documents this new re nuclear about the will then reflect the move of this original query vector toward this relevant centroid vector and away from the noun relevant essentially about OK
#c9	i;them;we;this part;the query vector;that century;the main idea;rock hill feedback;we;we;a new query vector;documents;this new re;nuclear;the move;this original query vector;this relevant centroid vector;the noun
#s10	so let's take a look at the example this is the example that we have seen earlier only
#c10	's;a look;the example;the example;we
#s11	that i in the display of the actual documents i only showed the vector representation of these documents we have five documents here and we have two random in the documents here right they are displayed in red and these are the term vectors and i have just assumed some TF IDF weights a lot of terms we have zero weights of course and these are negative documents there are two here there is another one here now in this rock hill method we first compute the center of each category i saw let's see look at the center of the vector of the positive documents we simply just so it's very easy to see we just add this with this one the corresponding element
#c11	i;the display;the actual documents;i;the vector representation;these documents;we;five documents;we;the documents;they;red;the term vectors;i;some TF IDF;a lot;terms;we;zero weights;course;negative documents;this rock hill method;we;the center;each category;i;'s;the center;the vector;the positive documents;we;it;we;this one;the corresponding element
#s12	and then that's down here and take the average
#s13	and then we win the added the corresponding elements and then just take the average we do this for all these in the end that what we have is this one this is the average vector of these tool
#c13	we;the added the corresponding elements;the average;we;the end;what;we;this one;the average vector;these tool
#s14	so it's a centroid of these tool that's also look at the central area of the negative documents this is basically the same we're going to take the average of three elements and these are the corresponding elements in the three vectors and so on so forth so in the end we have this one now in the rock hill feedback method we're going to combine all these with the original query about that which is this so now let's see how we combine them together
#c14	it;a centroid;these tool;the central area;the negative documents;we;the average;three elements;the corresponding elements;the three vectors;the end;we;this one;the rock hill feedback method;we;the original query;'s;we;them
#s15	well that's basically this
#s16	so we have a parameter are for controlling the original query term
#c16	we;a parameter;the original query term
#s17	wait that's one
#s18	and then we've baydar to control the inference of the positive centroid vector way that's one point five that comes from here
#c18	we;the inference;the positive centroid vector way
#s19	right so this girls kia
#c19	right so this girls
#s20	and we also have this negative wait here controller by gamma here in this world has come from cause the negative essentially here and we do exactly the same for other terms each is for one term
#c20	we;controller;gamma;this world;we;other terms;one term
#s21	and this is our new vector
#c21	our new vector
#s22	and we're going to use this new query vector this one to rank documents you can imagine what would happen right becaus of the movement that this one with match these reader documents much better becaus we move that this vector closer to then and it's going to penalize these black documents these non rather the documents
#c22	we;this new query vector;this one;documents;you;what;right becaus;the movement;this one;match;much better becaus;we;it;these black documents;the documents
#s23	so this is precisely what we want a front feedback now of course if we applied this method in practice we will see one potential problem and that is the original query has only four terms that are non zero but after we do query expansion you can imagine and we have made it comes that would have nonzero weights so the calculation would have to involve more terms in practice we often truncate this vector an only retain the terms with heist awaits so let's talk about how we use this method in practice i just mentioned that we often truncated that can see the only a small number of words that have highest weights in the central factor this is for efficiency answered i also say that here that a negative examples or non relevant examples can not be very useful especially compared with positive examples now you can think about why one reason is because negative documents tend to distract the query in all directions so when i take the average it doesn't really tell you where exactly should be moving whereas positive documents tend to be clustered together
#c23	precisely what;we;a front feedback;course;we;this method;practice;we;one potential problem;the original query;only four terms;we;query expansion;you;we;it;nonzero weights;the calculation;more terms;practice;we;this vector;the terms;heist awaits;'s;we;this method;practice;i;we;the only a small number;words;highest weights;the central factor;efficiency;i;a negative examples;non relevant examples;positive examples;you;one reason;negative documents;the query;all directions;i;the average;it;you;positive documents
#s24	and they were point you to consistently direction so that also means that sometimes we don't have to use those negative examples but note that in some cases in difficult queries where most programming results are negative negative feedback factor is very useful another thing is will avoid overfitting that means we have to keep relatively high weight on original query terms why be cause the sample that we see in feedback is a relatively small sample we don't want to over the trust of this more simple an the original query terms are still very important those terms of hyping by the user and the user has decided that those terms are most important so in knowledge to prevent us from over fitting or drifting topics with prevent talk with drifting you do the bias toward the feedback examples with general would have to keep up with a higher weight on the original terms so it is safe to do that
#c24	they;you;we;those negative examples;some cases;difficult queries;most programming results;negative negative feedback factor;another thing;we;relatively high weight;original query terms;the sample;we;feedback;a relatively small sample;we;the trust;an the original query terms;those terms;the user;the user;those terms;knowledge;us;fitting or drifting topics;prevent talk;you;the bias;the feedback examples;a higher weight;the original terms;it
#s25	and this is especially true for pseudo relevance feedback now this method can be used for both relevance feedback and pseudorandomness feedback in the case of pseudo feedback up the parameter beta should be set to a smaller value that cause the relevant examples are assumed to be random as reliable as he relevance feedback in the case of relevance feedback we love this could use a larger value so those parameters they have to be set an paracle and the root rock your method is usually robust and effective it's still very popular method the force feedback
#c25	pseudo relevance feedback;this method;both relevance feedback;pseudorandomness feedback;the case;pseudo;feedback;the parameter beta;a smaller value;the relevant examples;he;feedback;the case;relevance feedback;we;a larger value;those parameters;they;an paracle;the root rock;your method;it;very popular method
410	ce32ec54-c313-4bab-8698-300a558c5790	15
#s1	this lecture is about the future of web search in this lecture we're going to talk about some possible future trends of web search and intending the information retrieval systems in general in order to further improve the accuracy of search engine it's important to consider special cases of information need so one particular trend could be to have more and more specialized than a customized search engines and they can be called a vertical search engines these vertical search engines can be expected to be more effective than the current general search engines becaus they could assume that the users are special group of users that might have a common implementing need and then the search engine can be customized search users and be cause of the customization it's also possible to do plus summarization so the search can be personalized he calls we have a better understanding of the users be cause of the restricting the domain we also have some advantage in handling the documents be cause we can have better understanding of documents for example particular words may not be ambiguous in such a domain so we can bypass the problem of ambiguity another trend that we can't expect the Z is the search engine will be able to learn all the time it's like life time learning or lifelong learning and this is of course very attractive because that means the search anywhere
#c1	this lecture;the future;web search;this lecture;we;some possible future trends;web search;the information retrieval systems;order;the accuracy;search engine;it;special cases;information;one particular trend;a customized search engines;they;a vertical search engines;these vertical search engines;the current general search engines becaus;they;the users;special group;users;a common implementing need;the search engine;search users;cause;the customization;it;the search;he;we;a better understanding;the users;cause;the restricting the domain;we;some advantage;the documents;we;better understanding;documents;example;particular words;such a domain;we;the problem;ambiguity;another trend;we;the Z;the search engine;all the time;it;course;the search
#s2	self improve itself as more people are using it search anywhere become better and better and this is already happening because the search engines that can learn from the implicit feedback and more users use it and the quality of the search results for the popular queries that are typing by many users will likely become better so this is another feature of that we would see the third of trained might be the integration of multi models of information access so search navigation and recommendation or filtering might be combined to form a full fledged information management system and in the beginning of this cause we talked about push versus port these are different modes of information access but these modes can be combined and similarly in the poor mode querying and browsing could also be combined them and in fact that we're doing that basically today with the current search engines we are querying sometimes browsing clicking on links sometimes we've got some information recommended although most of the cases information recommended becaus of advertising but in the future you can imagine similarly integrate the system with multimode for information access and that would be convenient for people another trend is that we might see systems that try to go beyond the surgical support user tasks after all the reason why people want to search is to solve a problem or to make a decision or perform a task for example consumers might search for opinions about products in order to purchase a product and choose a good product to buy so in this case it would be beneficial to support the whole workflow of purchasing our product or choosing a product in this area after the current search engines already provide good support for example you can sometimes look at the reviews and then if you want to buy it and you can just click on the button the code or shopping side of the direct will get it down but it does not provide good task support for many other tasks for example for researchers you might want to find the relevant literature or site to the literature and then there's no support for finishing a task such as writing a paper so in general i think there are many opportunities to innovate and so in the following a few slides
#c2	self;itself;more people;it;search;the search engines;the implicit feedback;more users;it;the quality;the search results;the popular queries;many users;another feature;we;the integration;multi models;information access;search navigation;recommendation;filtering;a full fledged information management system;the beginning;we;push;port;different modes;information access;these modes;the poor mode;them;fact;we;the current search engines;we;clicking;links;we;some information;the cases;becaus;advertising;the future;you;the system;multimode;information access;people;another trend;we;systems;the surgical support user tasks;all the reason;people;a problem;a decision;a task;example;consumers;opinions;products;order;a product;a good product;this case;it;the whole workflow;our product;a product;this area;the current search engines;good support;example;you;the reviews;you;it;you;the button;the code or shopping side;it;it;good task support;many other tasks;example;researchers;you;the relevant literature;site;the literature;no support;a task;a paper;i;many opportunities;a few slides
#s3	i'll be talking a little bit more about some specific ideas or thoughts that hopefully can help you imagine new application possibilities some of them might be already relevant to what you are currently working on in general you can think about any intelligent system especially in technical information system as these specified by these three nodes and social connect these is really into a triangle that will be able to specify information system i call this data user service triangle so basically the three questions you ask would be a whole are you serving and what kind of data are you are managing and what kind of service you provide right there this would that help us basically specify your system and there are many different ways will connect them depending on how you connect them you have a different kind of system so they may give you some example on the top you can see different kinds of users on the left side you can see different types of data or information on the bottom you can see different service functions now imagine we can connect all these in different ways so for example if then connect everyone with web pages and the support that search and browsing what do you get well that's web search what if we connect the UI UC employees with organizacion documents or enterprise documents to support the search end drowsy
#c3	i;some specific ideas;thoughts;you;new application possibilities;them;what;you;you;any intelligent system;technical information system;these three nodes;a triangle;information system;i;this data user service triangle;the three questions;you;a whole;you;what kind;data;you;what kind;service;you;us;your system;many different ways;them;you;them;you;a different kind;system;they;you;some example;the top;you;different kinds;users;the left side;you;different types;data;information;the bottom;you;different service functions;we;different ways;example;everyone;web pages;the support;what;you;web search;we;the UI UC employees;organizacion documents;enterprise documents;the search end drowsy
#s4	well that's enterprise search if you connect the scientists with literature information to provide all kinds of service including search browsing or alert of new randomly documents or mining analyzing research trends or provide the task of support or decision support for example they might be able to provide support for automatically generating related work section for research paper and this would be closer to task support
#c4	enterprise search;you;the scientists;literature information;all kinds;service;search browsing;alert;new randomly documents;research trends;the task;support;decision support;example;they;support;related work section;research paper;task support
#s5	so then we can imagine this would be a literature assistant if we connect the online shoppers with block articles or product reviews then we can help these people to improve shopping experience so we can provide for example there are mining capabilities to analyze the reviews too compare products coming out sentiment of callouts anne to provide a test this folder with decision support to help them choose what product to buy or we can connect customer service people with emails from the customers an we can imagine a system that can provide analysis of these emails for finding the major complaints of the customers we can imagine the system could provide task support by automatically generating response to customer email maybe intending generally attach also promotion message if a property it can detect that's a positive message not a complaint
#c5	we;a literature assistant;we;the online shoppers;block articles;product reviews;we;these people;shopping experience;we;example;mining capabilities;the reviews;products;sentiment;callouts;anne;a test;decision support;them;what product;we;customer service people;emails;the customers;we;a system;analysis;these emails;the major complaints;the customers;we;the system;task support;response;customer email;message;a property;it;a positive message;a complaint
#s6	and then you might to take this opportunity to attach some promotion information or else if it's a complaint that you might be able to automatically generate this some generic response first tell the customer that he or she can expect the detail response later etc all these are trying to help people to improve the productivity so this shows that the activity days are really a lot it's just only restricted by our imagination so this picture shows the trend of the technology
#c6	you;this opportunity;some promotion information;it;a complaint;you;some generic response;the customer;he;she;the detail response;people;the productivity;the activity days;a lot;it;our imagination;this picture;the trend;the technology
#s7	and also it characterizes invented information system in three angles you can see in the center there is a triangle that connects keyword queries to search and bag of words representation that means the current the search engines basically provides search support to do this and mostly model users based on keyword queries and it sees the data through a bag of words
#c7	it;invented information system;three angles;you;the center;a triangle;keyword;bag;words;representation;the search engines;search support;users;keyword queries;it;the data;a bag;words
#s8	representation
#c8	representation
#s9	so it's a very simple approximation of the actual information in the documents
#c9	it;a very simple approximation;the actual information;the documents
#s10	but that's what the current system does it connects these three nodes in such a simple way or it only provides basically search function and this and that really understanding the user and then doesn't really understand that matching from mission documents now i show the some trends to push each in herb toward more advanced functions so think about the user node here so we can go beyond the keyword queries look at the user search history and then further model the user completely to understand the user 's task environment task need context other information
#c10	what;the current system;it;these three nodes;such a simple way;it;search function;the user;mission documents;i;the some trends;herb;more advanced functions;the user node;we;the keyword queries;the user search history;the user;the user 's task environment task;context other information
#s11	so this is pushing for personal ization and complete the user model
#c11	personal ization;the user model
#s12	and this is a major direction in research in in order to build intelligent information systems on the document aside we can also see we can go beyond bag of words retransition to have entity relation representation this means will recognize peoples names or relations locations etc
#c12	a major direction;research;order;intelligent information systems;the document;we;we;bag;words;retransition;entity relation representation;peoples;names;relations locations
#s13	and this is already feasible with today's natural language processing technique and google 's reason the initiative on the knowledge graph if you have heard of it is a good step toward this direction and once we can get to that level representation in robust manner at larger scale it can enable the search engine to provide a much better service in the future we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent so this cost for large scale semantic analysis and perhaps this is more visible for vertical search engines it's easier to make progress in a particular domain now the service side we see that we need to go beyond the search of support information access in general
#c13	today's natural language processing technique;google 's reason;the initiative;the knowledge graph;you;it;a good step;this direction;we;that level representation;robust manner;larger scale;it;the search engine;a much better service;the future;we;knowledge representation;we;perhaps inference rules;the search engine;large scale semantic analysis;vertical search engines;it;progress;a particular domain;we;we;the search;support information access
#s14	so search is only one way to get access to information recommender systems and push in port lights with different ways to get access to relevant information but going beyond access we also need to help people digest the information once the information is found and this step has to do with analysis of information or data mining where to find the patterns or convert the text information in your knowledge that can be used in the application more actionable knowledge that can be used for decision making and furthermore the knowledge it would be used to help user to improve productivity in finishing a task for example a decisionmaking task so this is a friend
#c14	search;only one way;access;information recommender systems;port lights;different ways;access;relevant information;access;we;people;the information;the information;this step;analysis;information;data mining;the patterns;the text information;your knowledge;the application;more actionable knowledge;decision making;furthermore the knowledge;it;user;productivity;a task;example;a decisionmaking task;a friend
#s15	and so basically in this time edging we anticipate in the future in vanity information systems will provide intelligent an interactive tasks support now i should also emphasize interactive here be cause it's important to optimize the combined the intelligence of the users and the system so we can get some help from users in some natural way an we don't have to assume the system has to do everything when the human user and the machine can collaborate in the impending way inefficient way then combine the intelligence will be high and in general we can minimize the users overall effort in solving problem so this is the big picture of future intelligent information systems and this hopefully can provide us some insights about how to make further innovations on top of what we can do today
#c15	this time;we;the future;vanity information systems;i;it;the combined the intelligence;the users;the system;we;some help;users;some natural way;we;the system;everything;the human user;the machine;the impending way inefficient way;the intelligence;we;the users;overall effort;problem;the big picture;future intelligent information systems;us;some insights;further innovations;top;what;we
410	cec5fcba-eefa-4d09-8c3c-624c7a621e9b	7
#s1	this lecture is about the web search in this lecture we're going to talk about one of the most important applications of text retrieval web search engines so let's first look at the some general challenges an AC unit is in web search now many information retrieval algorithms had been developed before the web was born so when the web was born it created the best opportunity to apply those algorithms to measure application problem that everyone would care about so naturally there had to be some further extensions of the classical search algorithms to address some new challenges encountered in web search so here are some general challenges first of this cabinet the challenging how to handle the size of the weapon in your complete list of coverage of all the information how to solve many users quickly and by answering all their queries so that's one major challenge you an before the web was won't the scale of search was relatively small the second problem is that this low quality information and there are often slams the third challenges dynamics of the the new pages are constantly created and some pages may be updated very quickly so it makes it harder to keep the index of fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine on the other hand there also some interesting opportunities that we can leverage to improve the search results there are many additional heuristics for example using links that we can leverage to improve scoring now the algorithm that we talked about the such as the vector space model our general algorithms and they can be applied to any search applications so that's the advantage on the other hand they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search web pages are linked with each other so obviously the link information is something that we can also leverage so the cause of these challenges then opportunities there are new techniques that have been divided for web search or due to the need of a web search one is parallel indexing and searching and this is what has the issue of scalability in particular googles imagine of map reduce is very inferential and has been very helpful in that aspect second and there are techniques that not developed for addressing the problem of spans so spam detection we have to prevent those spam pages from being ranked high and there are also techniques to achieve robust ranking and we're going to use a lot of signals to rank pages so that it's not easy to spend the search engine with a particular trick and the third line of techniques is link analysis and these are techniques that can allow us to improve search results by leveraging extra information and in general in web search if we're going to use multiple features for ranking not just link analysis but also exploiting all kind of clothes like layout of web pages or anchor text that describes a link to another page so here's a picture showing the basic search engine technologies basically this is the web on the left and then use it on the right side and we're going to help this user hookah taxes for the web information and the first component is a crawler that would crawl pages and then the second component that is index that that would take these pages it will create a inverted index the third component there is a retriever that with the user inverted index to answer users query by talking to the users browser and then search results will be given to the user and then the browser will show those results and to allow the user to interact with the web
#c1	this lecture;the web search;this lecture;we;the most important applications;text retrieval web search engines;'s;the some general challenges;an AC unit;web search;many information retrieval algorithms;the web;the web;it;the best opportunity;those algorithms;application problem;everyone;some further extensions;the classical search algorithms;some new challenges;web search;some general challenges;this cabinet;the challenging;the size;the weapon;your complete list;coverage;all the information;many users;all their queries;one major challenge;you;the web;the scale;search;the second problem;this low quality information;the third challenges;dynamics;the the new pages;some pages;it;it;the index;the challenges;we;order;a high quality web search engine;the other hand;some interesting opportunities;we;the search results;many additional heuristics;example;links;we;scoring;the algorithm;we;the vector space model;our general algorithms;they;any search applications;the advantage;the other hand;they;advantage;special characteristics;pages;documents;the specific applications;web search web pages;the link information;something;we;the cause;these challenges;new techniques;web search;the need;a web search;one;parallel indexing;what;the issue;scalability;particular googles;map reduce;that aspect;techniques;the problem;spans;so spam detection;we;those spam pages;techniques;robust ranking;we;a lot;signals;pages;it;the search engine;a particular trick;the third line;techniques;analysis;techniques;us;search results;extra information;web search;we;multiple features;analysis;all kind;clothes;layout;web pages;anchor text;a link;another page;a picture;the basic search engine technologies;the web;the left;it;the right side;we;this user;hookah taxes;the web information;the first component;a crawler;pages;index;these pages;it;a inverted index;the third component;a retriever;the user inverted index;users;query;the users browser;search results;the user;the browser;those results;the user;the web
#s2	so we're going to talk about each of these components first we couldn't talk about the crawler also called a spider or software robot that would do something like a crawling pages on the web to build a toy roller is relatively easy 'cause you just need to start with a set of see the pages and then fetch pages from the web and pause these pages or freak out the new links and then add them to the priority queue and then just explore those additional links but to build real crawler actually is a tricky
#c2	we;these components;we;the crawler;a spider or software robot;something;a crawling pages;the web;a toy roller;you;a set;the pages;pages;the web;these pages;the new links;them;the priority queue;those additional links;real crawler
#s3	and there are some complicated issues that you have to deal with so for example robustness what if the server doesn't respond what if there is a trap that generates dynamically generated web pages that might attract your crawler to keep crawling the same site then to fetch dynamically generated pages the resource with this issue of crawling curtis
#c3	some complicated issues;you;example;the server;a trap;dynamically generated web pages;your crawler;the same site;dynamically generated pages;the resource;this issue;curtis
#s4	and you don't want to overload the one particular server with many crawling requests an you have to respect the robot exclusion protocol you also need to handle different types of virus there were images PDF files all kinds of formats on the web and you have to also consider UI or extension so sometimes those are CGI script an there are internal references etc and sometimes you have javascript on the page that they also create the challenges and you ideally should also recognize redundant pages 'cause you don't have to duplicate the those pages and finally you may be interesting to discover hidden urls those are urls that may not be linked to any page
#c4	you;the one particular server;requests;an you;the robot exclusion protocol;you;different types;virus;images;all kinds;formats;the web;you;UI;extension;CGI script;internal references;you;javascript;the page;they;the challenges;you;redundant pages;you;the those pages;you;hidden urls;urls;any page
#s5	but if you truncate the URL to shorter path that you might be able to get some additional pages so one of the major crowding strategies in general breakfast first is most common becauses natural advantages balances the server load you would not keep approving a particular server with requests also parallel crawling is very natural because this task is very easy to parallelize and there are some variations of the crowding task and one interesting variations called folks to crawling in this case we're going to crawl just some pages about a particular topic for example all pages about the automobiles and this is typically a going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and and gradually chrome or so one challenger in crawling ISRO find the new pages that people have created and people probably are creating new pages all the time and this is very challenging if the new pages have not been actually linked to any old page if they are then you can probably find them by recording the old page so these are also some interesting challenges that have to be solved and finally we might face the scenario of incremental crawling to repeated crawling right let's say if you want to build web search engine and you're the first across a lot of data from the web and then
#c5	you;the URL;shorter path;you;some additional pages;the major crowding strategies;general breakfast;most common becauses natural advantages;the server load;you;a particular server;requests;crawling;this task;some variations;the crowding task;one interesting variations;folks;this case;we;just some pages;a particular topic;example;all pages;the automobiles;a query;you;the query;some results;a major search engine;you;those results;ISRO;the new pages;people;people;new pages;the new pages;any old page;they;you;them;the old page;some interesting challenges;we;the scenario;'s;you;web search engine;you;a lot;data;the web
#s6	but then once you have collected all the data and in the future we just need to crawl the update pages in general you don't have to recall everything right
#c6	you;all the data;the future;we;the update pages;you;everything
#s7	or it's necessary so in this case you will go is to minimize the resource overhead by using minimum resources to just still closed the updates the pages so this is after a very interesting research question here and opa research question in that there aren't many standard algorithms established yet for doing this this task but in general you can imagine you can learn from the past experience so the two major factors that you have to consider our first will this page be updated frequently and do i have to crawl this page again if the page is a static page that hasn't been changed for months is you probably don't have to record the everyday right because it's unlikely that it will be changed frequently on the other hand if it's sports score page that gets updated very frequently and you may need to re croydon maybe even multiple times on the same day and the other factor to consider is this page frequently accessed by users if it if it is that it means it's a high utility page and then that's it's more important to ensure such a page to be fresh compare with another page that has never been fetched by any users for a year then even though that page has been changed a lot then it's probably not necessary to crawl that page or at least it's not as urgent as to maintain the freshness of frequently access page by users so to summarize web search is one of the most important applications of text retrieval and there are some new challenges particularly scalability if you should say quality information there also new opportunities particularly rich linked information and layout etc a crawler is an essential component of web search applications an in general we can classify two scenarios one is initial crawling and here we want to have complete crawling of the web if you are doing a general search ending or focusing crawling if you want to just to target at a certain type of pages and then there is another scenario that's incremental updating of the crowd data or incremental crawling in this case you need to optimize resource try to use minimum resource forget fresh information
#c7	it;this case;you;the resource;minimum resources;the updates;the pages;a very interesting research question;opa research question;many standard algorithms;this task;you;you;the past experience;the two major factors;you;this page;i;this page;the page;a static page;months;you;it;it;the other hand;it;sports score page;you;maybe even multiple times;the same day;the other factor;this page;users;it;it;it;it;a high utility page;it;such a page;fresh compare;another page;any users;a year;that page;it;that page;it;the freshness;frequently access page;users;web search;the most important applications;text retrieval;some new challenges;you;quality information;new opportunities;information;layout;a crawler;an essential component;web search applications;we;two scenarios;initial crawling;we;complete crawling;the web;you;a general search;you;a certain type;pages;another scenario;incremental updating;the crowd data;this case;you;resource;minimum resource;fresh information
410	d1d123ff-d463-4af3-bd67-df56205a7e5c	85
#s1	This lecture is about the vector space retrieval model we're going to give a introduction to its basic idea.
#c1	This lecture;the vector space retrieval model;we;a introduction;its basic idea
#s2	In the last lecture we talked about the different ways of designing a retrieval model.
#c2	the last lecture;we;the different ways;a retrieval model
#s3	Which would give us a different a ranking function.
#c3	us;a different a ranking function
#s4	In this lecture we're going to talk about this specific way of designing a ranking function called vector space retrieval model.
#c4	this lecture;we;this specific way;a ranking function;vector space retrieval model
#s5	And we're going to give a brief introduction to the basic idea.
#c5	we;a brief introduction;the basic idea
#s6	Vector space model is a special case of similarity based models, as we discussed before, which means we assume relevance is roughly similarity between the document and the query.
#c6	Vector space model;a special case;similarity based models;we;we;relevance;roughly similarity;the document;the query
#s7	Now, whether this assumption is true is actually a question.
#c7	this assumption;a question
#s8	But in order to solve a search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the programming language.
#c8	order;a search problem;we;the vague notion;relevance;a more precise definition;the programming language
#s9	So in this process will have to make a number of assumptions.
#c9	this process;a number;assumptions
#s10	This is the first assumption that we make here.
#c10	the first assumption;we
#s11	Basically, we assume that if a document is more similar to a query than another document then the first document will be assumed to be more relevant than the second one, and this is the basis for ranking documents in this approach.
#c11	we;a document;a query;another document;the first document;the second one;the basis;ranking documents;this approach
#s12	Again, it's questionable whether this is really the best definition for relevance.
#c12	it;the best definition;relevance
#s13	As we will see later, there are other ways to model relevance.
#c13	we;other ways;relevance
#s14	The basic idea of vector space retrieval model is actually very easy to understand.
#c14	The basic idea;vector space retrieval model
#s15	Imagine a high dimensional space.
#c15	a high dimensional space
#s16	Where each dimension corresponds to a term.
#c16	each dimension;a term
#s17	So here I show a 3 dimensional space.
#c17	I;a 3 dimensional space
#s18	With three words, programming, library and presidential.
#c18	three words;programming;library
#s19	So each term here defines one dimension.
#c19	each term;one dimension
#s20	Now we can consider vectors in this 3 dimensional space.
#c20	we;vectors;this 3 dimensional space
#s21	And we're going to assume that all our documents and the query will be placed in this vector space.
#c21	we;all our documents;the query;this vector space
#s22	So for example, one document might be represented by this vector, D1.
#c22	example;one document;this vector;D1
#s23	Now this means this document probably covers library, and presidential, but it doesn't really talk about programming.
#c23	this document;library;it;programming
#s24	Alright, what does this mean terms of representation of document?
#c24	what;terms;representation;document
#s25	That just means we're going to look at our document from the perspective of this vector.
#c25	we;our document;the perspective;this vector
#s26	We're going to ignore everything else.
#c26	We;everything
#s27	Basically, what we see here is only the vector representation of the document.
#c27	what;we;only the vector representation;the document
#s28	Of course, the document has other information.
#c28	the document;other information
#s29	For example, the orders of words are simply ignored, and that's because we assume that the bag of words with representation.
#c29	example;the orders;words;we;words;representation
#s30	So with this representation you can already see D1 seems to suggest a topic about presidential library.
#c30	this representation;you;D1;a topic;presidential library
#s31	Now this is different from another document which might be represented as a different vector D2 here.
#c31	another document;a different vector D2
#s32	So in this case, the document covers programming and library, but does not talk about the presidential.
#c32	this case;the document;programming;library;the presidential
#s33	So what does this remind you?
#c33	what;you
#s34	You can probably guess the topic is likely about programming language, and the library is software library.
#c34	You;the topic;programming language;the library;software library
#s35	So this shows that by using this vector space representation we can actually capture the differences between topics of documents.
#c35	this vector space representation;we;the differences;topics;documents
#s36	Now you can also imagine there are other vectors, for example D3 is pointing to that direction.
#c36	you;other vectors;example;D3;that direction
#s37	That might be about present in your program.
#c37	your program
#s38	And in fact that we can place all the documents in this vector space.
#c38	fact;we;all the documents;this vector space
#s39	And they will be pointing to all kinds of directions.
#c39	they;all kinds;directions
#s40	And similarly, we're going to place our query also in this space as another vector.
#c40	we;our query;this space;another vector
#s41	And then we're going to measure the similarity between the query vector and every document vector.
#c41	we;the similarity;the query vector;every document vector
#s42	So in this case, for example, we can easily see D2 seems to be the closest to this query vector, and therefore D2 will be ranked above others.
#c42	this case;example;we;D2;this query vector;D2;others
#s43	So this is basically the main idea of the vector space model.
#c43	the main idea;the vector space model
#s44	So to be more precise.
#s45	To be more precise.
#s46	Vector space model is a framework.
#c46	Vector space model;a framework
#s47	In this framework, we make the following assumptions.
#c47	this framework;we;the following assumptions
#s48	First, we represent a document and query via term vector.
#c48	we;a document;query;term vector
#s49	So here are term can be any basic concept, for example a word or a phrase.
#c49	term;any basic concept;example
#s50	Or even N-gram of characters.
#c50	Or even N-gram;characters
#s51	Those are just sequence of characters Inside the word.
#c51	sequence;characters;the word
#s52	Each term is assumed to define one dimension.
#c52	Each term;one dimension
#s53	Therefore, N terms in our vocabulary would define an N dimensional space.
#c53	N terms;our vocabulary;an N dimensional space
#s54	A query vector would consist of a number of elements.
#c54	A query vector;a number;elements
#s55	Corresponding to the weights on different terms.
#c55	the weights;different terms
#s56	Each document vector is also similar.
#c56	Each document vector
#s57	It has a number of elements and each value of each element is indicating that weight of the corresponding term.
#c57	It;a number;elements;each value;each element;that weight;the corresponding term
#s58	Here you can see we assume there are N dimensions.
#c58	you;we;N dimensions
#s59	Therefore there are N elements.
#c59	N elements
#s60	Each corresponding to the weight on a particular term.
#c60	the weight;a particular term
#s61	So the relevance in this case would be assumed to be the similarity between the two vectors.
#c61	the relevance;this case;the similarity;the two vectors
#s62	Therefore, our ranking function is also defined as the similarity between the query vector and document vector.
#c62	our ranking function;the similarity;the query vector;document vector
#s63	Now, if I ask you to write a program to implement this approach in the search engine, you would realize that this is far from clear, right?
#c63	I;you;a program;this approach;the search engine;you
#s64	We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this.
#c64	We;a lot;things;detail;it;the program
#s65	That's why I said this is a framework.
#c65	I;a framework
#s66	And this has to be refined in order to actually suggest a particular ranking function that you can implement on your computer.
#c66	order;a particular ranking function;you;your computer
#s67	So what does this framework not say?
#c67	what;this framework
#s68	It actually hasn't set up many things that would be required in order to implement this function.
#c68	It;many things;order;this function
#s69	First, it did not say how we should define or select the basic concepts exactly.
#c69	it;we;the basic concepts
#s70	We clearly assume the concepts are orthogonal, otherwise there will be redundancy.
#c70	We;the concepts;redundancy
#s71	For example, if two synonyms are somehow distinguished as two different concepts, then there would be defining two different dimensions and that would clearly cause redundancy here, or over emphasizing of matching this concept.
#c71	example;two synonyms;two different concepts;two different dimensions;redundancy;this concept
#s72	Because it would be as if you match the two dimensions when you actually match one semantic concept.
#c72	it;you;the two dimensions;you;one semantic concept
#s73	Secondly, it did not say how exactly should place documents and query in this space.
#c73	it;documents;query;this space
#s74	Basically I showed you some examples of query and document vectors, but where exactly should the vector for a particular document point to?
#c74	I;you;some examples;query;document vectors;where exactly should the vector;a particular document point
#s75	So this is equivalent to how to define the term weights.
#c75	the term weights
#s76	How do you compute those element values in those vectors?
#c76	you;those element values;those vectors
#s77	Now this is a very important question because term weight in the query vector indicates the importance of term.
#c77	a very important question;term weight;the query vector;the importance;term
#s78	So depending on how you assign the weights, you might prefer some terms to be matched over others.
#c78	you;the weights;you;some terms;others
#s79	Similarly to term weight in the document is also very meaningful.
#c79	term weight;the document
#s80	It indicates how well the term characterizes the document.
#c80	It;the term;the document
#s81	If you got it wrong, then you clearly don't represent this document accurately.
#c81	you;it;you;this document
#s82	Finally, how to define the similarity measure is also not given.
#c82	the similarity measure
#s83	So these questions must be addressed before we can have a operational function that we can actually implement using a program language.
#c83	these questions;we;a operational function;we;a program language
#s84	So how do we solve these problems?
#c84	we;these problems
#s85	Is the main topic of the next lecture.
#c85	the main topic;the next lecture
410	d7c569a0-95fd-4876-9cbf-594a163cb8d4	100
#s1	This lecture is about the evaluation of text retrieval systems.
#c1	This lecture;the evaluation;text retrieval systems
#s2	In the previous lectures we have talked about a number of text retrieval methods, different kinds of ranking functions.
#c2	the previous lectures;we;a number;text retrieval methods;different kinds;ranking functions
#s3	But how do we know which one works the best?
#c3	we;one
#s4	In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods.
#c4	order;this question;we;them;we;these retrieval methods
#s5	So this is the main topic of this lecture.
#c5	the main topic;this lecture
#s6	First, let's think about the why do we have to do evaluation?
#c6	's;we;evaluation
#s7	I already give one reason and that is we have to use evaluation to figure out which retrieval method works better.
#c7	I;one reason;we;evaluation;which retrieval method
#s8	Now this is very important for advancing our knowledge, otherwise we wouldn't know whether a new idea works better than old idea.
#c8	our knowledge;we;a new idea;old idea
#s9	In the beginning of this course, we talked about the problem of text retrieval.
#c9	the beginning;this course;we;the problem;text retrieval
#s10	We compared it with database retrieval.
#c10	We;it;database retrieval
#s11	There we mentioned that text retrieval is empirically defined problem.
#c11	we;text retrieval
#s12	So evaluation must rely on users.
#c12	evaluation;users
#s13	Which system works better would have to be judged by our users
#c13	Which system;our users
#s14	So this becomes a very challenging problem.
#c14	a very challenging problem
#s15	Because.
#s16	How can we get users involved in the evaluation?
#c16	we;users;the evaluation
#s17	How can we do a fair comparison of different methods?
#c17	we;a fair comparison;different methods
#s18	So just go back to the reasons for evaluation.
#c18	the reasons;evaluation
#s19	I listed two reasons here.
#c19	I;two reasons
#s20	The second reason is basically what I just said, but there is also another reason which is to assess the actual utility of text retrieval system.
#c20	The second reason;what;I;another reason;the actual utility;text retrieval system
#s21	Now imagine you're building your own search engine applications.
#c21	you;your own search engine applications
#s22	It would be interested in knowing how well your search engine works for your users.
#c22	It;your search engine;your users
#s23	So in this case, matches must reflect the utility to the actual users in a real application.
#c23	this case;matches;the utility;the actual users;a real application
#s24	And typically this has to be done by using user studies and using the real search engine.
#c24	user studies;the real search engine
#s25	In the second case, or for the second reason.
#c25	the second case;the second reason
#s26	The measures actually only to be correlated with the utility to actual users.
#c26	The measures;the utility;actual users
#s27	Thus they don't have to accurately reflect the exact utility to users.
#c27	they;the exact utility;users
#s28	So the measure only needs to be good enough to tell which method works better.
#c28	the measure;which method
#s29	And this is usually done through a test collection, and this is the main idea that we'll be talking about in this course.
#c29	a test collection;the main idea;we;this course
#s30	This has been very important for comparing different algorithms and for improving search engine system in general.
#c30	different algorithms;search engine system
#s31	So next we talk about what to measure right?
#c31	we;what
#s32	There are many aspects of a search engine that we can measure we can evaluate.
#c32	many aspects;a search engine;we;we
#s33	And here I listed the three major aspects.
#c33	I;the three major aspects
#s34	One is effectiveness or accuracy.
#c34	effectiveness;accuracy
#s35	How accurate the other search results.
#c35	How accurate the other search results
#s36	In this case, we're measuring systems capability of ranking relevant documents on top of non random ones.
#c36	this case;we;systems capability;relevant documents;top;non random ones
#s37	The second is efficiency.
#c37	efficiency
#s38	How quickly can a user get some results?
#c38	a user;some results
#s39	How much computing resources are needed to answer query?
#c39	How much computing resources;query
#s40	So in this case we need to measure the space and time overhead of the system.
#c40	this case;we;the space;time;the system
#s41	The third aspect is usability.
#c41	The third aspect;usability
#s42	Basically, the question is how useful is a system for real user tasks.
#c42	the question;a system;real user tasks
#s43	Here, obviously interfaces and many other things are also important, and we typically would have to do user studies.
#c43	obviously interfaces;many other things;we;user studies
#s44	Now in this course we are going to talk mostly about effectiveness and accuracy measures because the efficiency and usability dimensions are not really unique to search engines and so.
#c44	this course;we;effectiveness and accuracy measures;the efficiency and usability dimensions;search engines
#s45	They are needed for evaluating any other software systems, and there is also good coverage of such materials in other courses.
#c45	They;any other software systems;good coverage;such materials;other courses
#s46	But how to evaluate a search engines quality or accuracy is something unique to text retrieval, and we're going to talk a lot about this.
#c46	a search engines quality;accuracy;something;text retrieval;we;a lot
#s47	The main idea that people have proposed for using a test set to evaluate text retrieval algorithm is called the Cranfield evaluation methodology.
#c47	The main idea;people;a test;text retrieval algorithm;the Cranfield evaluation methodology
#s48	This one actually was developed a long time ago, developed in 1960s.
#c48	This one
#s49	It's a methodology for laboratory test.
#c49	It;a methodology;laboratory test
#s50	Of system components, it's actually methodology that has been very useful not just for search engine evaluation, but also for evaluating virtually all kinds of empirical tasks.
#c50	system components;it;methodology;search engine evaluation;virtually all kinds;empirical tasks
#s51	And for example, in natural language processing or in other fields where the problem is empirically defined, we typically would need to use such a methodology.
#c51	example;natural language processing;other fields;the problem;we;such a methodology
#s52	And today with the Big Data Challenge with use of machine learning everywhere, this methodology has been very popular, but it was first developed for search engine application in 1960s.
#c52	the Big Data Challenge;use;machine learning;this methodology;it;search engine application
#s53	So the basic idea of this approach is to build a reusable test collections and define measures.
#c53	the basic idea;this approach;a reusable test collections;measures
#s54	One such a test collection is build.
#c54	One such a test collection
#s55	It can be used again and again to test the different algorithms, and we're going to define measures that would allow you to quantify the performance of a system or an algorithm.
#c55	It;the different algorithms;we;measures;you;the performance;a system;an algorithm
#s56	So how exactly would this work?
#s57	We're going to have a sample collection of documents and this is just to simulate the real document collection in search application.
#c57	We;a sample collection;documents;the real document collection;search application
#s58	We can also have a sample set of queries or topics.
#c58	We;a sample set;queries;topics
#s59	This is to simulate users queries.
#c59	users
#s60	Then we'll have to have relevance judgments.
#c60	we;relevance judgments
#s61	These are judgments of which documents should be returned for which queries.
#c61	judgments;documents
#s62	Ideally they have to be made by users who formulated the queries, 'cause those are the people that know exactly what documents would be useful, and then finally we have to have measures to quantify how well systems result  matches the ideal ranked list that would be constructed based on users relevance judgments.
#c62	they;users;who;the queries;the people;exactly what documents;we;measures;systems;the ideal ranked list;users relevance judgments
#s63	So this methodology is very useful for starting retrieval algorithms because the tested connection can be reused many times and it would also provide a fair comparison for all the methods.
#c63	this methodology;retrieval algorithms;the tested connection;it;a fair comparison;all the methods
#s64	We have the same criteria, same data set to be used to compare different algorithms.
#c64	We;the same criteria;different algorithms
#s65	This allows us to compare a new algorithm with an older algorithm that was developed many years ago by using the same standard.
#c65	us;a new algorithm;an older algorithm;the same standard
#s66	So this is an illustration of how this works.
#c66	an illustration
#s67	So as I said, we need the queries that are shown here.
#c67	I;we;the queries
#s68	We have Q1Q2, etc.
#c68	We;Q1Q2
#s69	We also need the documents that's called a document collection and on the right side you see we need relevance judgments.
#c69	We;the documents;a document collection;the right side;you;we;relevance judgments
#s70	These are basically.
#s71	The binary judgments of documents with respect to a query.
#c71	The binary judgments;documents;respect;a query
#s72	So, for example d1 is judged as being relevant to Q1, D2 is judged as being relevant as well.
#c72	example;d1;Q1;D2
#s73	And d3 is judged as non relevant.
#c73	d3
#s74	The two, Q1, etc.
#c74	The two, Q1
#s75	These would be created by users.
#c75	users
#s76	But once we have these and then we basically have a text collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results.
#c76	we;we;a text collection;you;two systems;you;them;you;each system;these queries;documents;each system;results
#s77	Let's say if the query is Q1 and then we would have results.
#c77	's;the query;Q1;we;results
#s78	Here I show R sub A as results from system A.
#c78	I;R sub;results;system A.
#s79	So this is remember we talked about.
#c79	we
#s80	Task of computing approximation of the relevant document set R sub A  is system
#c80	Task;computing approximation;the relevant document;R sub;system
#s81	A's approximation here.
#c81	A's approximation
#s82	And R sub B is system B's approximation of relevant documents.
#c82	R sub B;system B's approximation;relevant documents
#s83	Now let's take a look at these results.
#c83	's;a look;these results
#s84	So which is better now?
#s85	Imagine for a user, which one would you like?
#c85	a user;which one;you
#s86	Now let's take a look at the both  results.
#c86	's;a look;the both  results
#s87	And there are some differences, and there are some documents that are returned by both systems.
#c87	some differences;some documents;both systems
#s88	But if you look at the results, you would feel that well, maybe A is better in the sense that we don't have many non relevant documents and among the three documents returned, two of them are relevant, so that's good, it's precise.
#c88	you;the results;you;A;the sense;we;many non relevant documents;the three documents;them;it
#s89	On the other hand, one can also say, maybe B is better because we've got more relevant documents.
#c89	the other hand;one;B;we;more relevant documents
#s90	We've got 3 instead of two.
#c90	We
#s91	So which one is better and how do we quantify this?
#c91	one;we
#s92	Obviously this question highly depends on the users task and it depends on users as well.
#c92	this question;the users task;it;users
#s93	You might be able to imagine for some users may be system A is better.
#c93	You;some users;system A
#s94	If the user is not interested in getting all the relevant document.
#c94	the user;all the relevant document
#s95	But in this case, the user doesn't have to read many and the user would see most of the relevant documents.
#c95	this case;the user;the user;the relevant documents
#s96	On the one hand,  one can also imagine the user might need to have as many relevant documents as possible.
#c96	the one hand;one;the user;as many relevant documents
#s97	For example, if you are doing a literature survey, you might be in the segment category and you might find that system B is better.
#c97	example;you;a literature survey;you;the segment category;you;system B
#s98	So in that case we will have to also define measures to quantify them.
#c98	that case;we;measures;them
#s99	And we might need to define multiple measures.
#c99	we;multiple measures
#s100	Because users have different perspectives of looking at the results.
#c100	users;different perspectives;the results
410	d857a66b-1018-4ffb-821a-9d8acc6f5012	102
#s1	So now let's talk about the extension of PLSA to derive LDA and to motivate that we need to talk about some deficiency of PLSA.
#c1	's;the extension;PLSA;LDA;we;some deficiency;PLSA
#s2	First, it's not really generating model because we cannot compute the probability of a new document.
#c2	it;model;we;the probability;a new document
#s3	You can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data.
#c3	You;the pies;the document;the pis;the document;we;the training data
#s4	So we cannot compute the pis for future document.
#c4	we;the pis;future document
#s5	And there was some heuristic.
#s6	A work around though.
#c6	A work
#s7	And Secondly, it has many parameters
#c7	it;many parameters
#s8	and I've asked you to compute how many parameters exactly there are in PLSA and "you will see there are many  That means the model is very complex and that also means there are many "local  overfitting and that means it's very hard to also find a good local maximum.
#c8	I;you;how many parameters;PLSA;you;the model;many "local  overfitting;it;a good local maximum
#s9	And that really represents global maximum.
#c9	global maximum
#s10	And in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model.
#c10	terms;future data;we;it;the training data;the complexity;the model
#s11	The model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data.
#c11	The model;precisely what;the training data;it;us;the model;other data
#s12	This, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have.
#c12	necessary problem;text mining;we;the training documents;we
#s13	We are not always interested in modeling future data, but in other cases or if we care about generality, we would worry about this over fitting.
#c13	We;future data;other cases;we;generality;we;fitting
#s14	So LDA is proposed to improve that and it basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters.
#c14	LDA;it;PLSA;a Dirichlet;the model parameters
#s15	Dirichlet is just a special distribution that we can use to specify prior.
#c15	Dirichlet;just a special distribution;we
#s16	So in this sense, LDA is just a Bayesian version of PLSA and the parameters are now much more regularized.
#c16	this sense;LDA;just a Bayesian version;PLSA;the parameters
#s17	You will see there are many fewer parameters.
#c17	You;many fewer parameters
#s18	And you can achieve the same goal as PLSA for text mining.
#c18	you;the same goal;PLSA;text mining
#s19	It means it can compute the topic coverage and topic word distributions as in PLSA.
#c19	It;it;the topic coverage;topic word distributions;PLSA
#s20	However, there is no free launch while the parameters for PLSA  is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model.
#c20	no free launch;the parameters;fewer parameters;order;the topic coverage;word distributions;we;the problem;influence;these variables;they;the parameters;the model
#s21	So the inference part.
#c21	So the inference part
#s22	Again, face the local Maxima problem.
#c22	the local Maxima problem
#s23	So essentially they are doing something very similar, but theoretically LDA is more elegant way of looking at the topic modeling problem.
#c23	they;something;LDA;more elegant way;the topic modeling problem
#s24	So let's see how we can generalize PLSA to LDA or extend the PLSA to have LDA now a full treatment of LDA is beyond the scope of this course
#c24	's;we;PLSA;LDA;the PLSA;LDA;a full treatment;LDA;the scope;this course
#s25	and we just don't have time to go in depth in talking about that.
#c25	we;time;depth
#s26	But here I just want to give you a brief idea about what's the extension and what it enables.
#c26	I;you;a brief idea;what;the extension;what;it
#s27	So this is a picture of LDA.
#c27	a picture;LDA
#s28	Now I remove the background model just for simplicity.
#c28	I;the background model;simplicity
#s29	Now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors.
#c29	this model;all these parameters;we;these word distributions;theta;i
#s30	So these word distributions.
#c30	So these word distributions
#s31	So here and the other set of parameters are pis and we present as a vector also.
#c31	the other set;parameters;pis;we;a vector
#s32	And this is for convenience to introduce LDA
#c32	convenience;LDA
#s33	and we have one vector for each document.
#c33	we;one vector;each document
#s34	And in this case in theta we have one vector for each topic.
#c34	this case;theta;we;one vector;each topic
#s35	Now that the difference between LDA and PLSA is that in LDA we're going to not allow them to free the change.
#c35	the difference;LDA;PLSA;LDA;we;them;the change
#s36	Instead, we're going to force them to be drawn from another distribution.
#c36	we;them;another distribution
#s37	So more specifically they will be drawn from 2 Dirichlet distributions respectively.
#c37	they;2 Dirichlet distributions
#s38	"
#s39	The  vectors, so it gives us a probability for a particular choice of a vector.
#c39	The  vectors;it;us;a probability;a particular choice;a vector
#s40	Take for example pis, right?
#c40	example;pis
#s41	So this Dirichlet distribution tell us which vector of pis is more likely, and this distribution itself is controlled by another vector of parameters of alpha's.
#c41	this Dirichlet distribution;us;which vector;pis;this distribution;itself;another vector;parameters;alpha
#s42	"Depending on  characterize the distribution in different ways and with force certain choices of pi's.
#c42	the distribution;different ways;force;certain choices;pi
#s43	To be more likely than others.
#c43	others
#s44	For example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by Alpha.
#c44	example;you;a choice;relatively uniform distribution;all the topics;you;skewed coverage;topics;Alpha
#s45	And similar here.
#s46	The topic word distributions are drawn from another Dirichlet distribution with beta parameters and note that here Alpha has K parameters corresponding to our inference on the k values of pis for a document, whereas here beta has N values corresponding to controlling the N words in our vocabulary.
#c46	The topic word distributions;another Dirichlet distribution;beta parameters;Alpha;K parameters;our inference;the k values;pis;a document;beta;values;the N words;our vocabulary
#s47	Now, once we impose these price than the generation process will be different an we all start with drawing pi's from this Dirichlet distribution and this pi will tell us these probabilities.
#c47	we;these price;the generation process;we;pi;this Dirichlet distribution;this pi;us;these probabilities
#s48	And then we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model.
#c48	we;the pi;which topic;course;the PLSA model
#s49	A similar here we're not going to have these distributions free.
#c49	we;these distributions
#s50	Instead we can do draw one from the Dirichlet distribution, and then from this, then we're going to further sample a word and the rest is very similar to the PLSA.
#c50	we;the Dirichlet distribution;we;a word;the rest;the PLSA
#s51	The likelihood function now is more complicated for LDA, but there's a close connection between the likelihood function of LDA and PLSA, so I'm going to illustrate the difference here.
#c51	The likelihood function;LDA;a close connection;the likelihood function;LDA;PLSA;I;the difference
#s52	So in the top you see PLSA.
#c52	the top;you;PLSA
#s53	Likelihood function that you have already seen before it's copied from previous slide only that I dropped the background for simplicity.
#c53	Likelihood function;you;it;previous slide;I;the background;simplicity
#s54	So in the LDA formulas you see very similar things.
#c54	the LDA formulas;you;very similar things
#s55	First you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions.
#c55	you;the first equation;the probability;a word;multiple word distributions
#s56	And this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multiplied by the probability of observing the world from that topic.
#c56	this formula;a sum;all the possibilities;the word;the sum;a product;the probability;a topic;the probability;the world;that topic
#s57	So this is a very important formula as I have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of LDA or PLSA and they all rely on this.
#c57	a very important formula;I;the core assumption;all the topic models;you;other topic models;extensions;LDA;PLSA;they
#s58	So it's very important to understand this.
#c58	it
#s59	And this gives us the probability of getting a word from a mixture model.
#c59	us;the probability;a word;a mixture model
#s60	Now next in the probability of a document we see there is a PLSA component in the LDA formula.
#c60	the probability;a document;we;a PLSA component;the LDA formula
#s61	But the LDA formula would add some integral here, and that's to explain to account for the fact that the pis are not fixed, so they are drawn from Dirichlet distribution.
#c61	the LDA formula;the fact;the pis;they;Dirichlet distribution
#s62	And that's shown here.
#s63	That's why we have to take the integral to consider all the possible pi's that we could possibly draw from this "Dirichlet And similarly, in the likelihood for the whole collection, we also see further components added.
#c63	we;all the possible pi;we;this "Dirichlet;the likelihood;the whole collection;we;further components
#s64	Another integral here.
#s65	Right, so basically in the LDA we just added these integrals to account for the uncertainties
#c65	the LDA;we;these integrals;the uncertainties
#s66	and we added of "course the  govern the choice of these parameters, pi's and theta's.
#c66	we;"course;the  govern;the choice;these parameters;pi;theta
#s67	So this is a likelihood function for LDA.
#c67	a likelihood function;LDA
#s68	Now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for LDA.
#c68	's;'s;parameter;an inference;the parameters;exactly the same approach maximum likelihood estimator;LDA
#s69	Now you might think about how many parameters are there in LDA versus PLSA.
#c69	you;how many parameters;LDA;PLSA
#s70	You will see there are fewer parameters in LDA because in this case the only parameters are alphas and betas.
#c70	You;fewer parameters;LDA;this case;the only parameters;alphas;betas
#s71	So we can use the maximum likelihood estimated to compute that.
#c71	we;the maximum likelihood
#s72	Of course it's more complicated because the form of likelihood functions more complicated.
#c72	it;the form;likelihood functions
#s73	But what's also important is not set.
#c73	what
#s74	Now.
#s75	These parameters that we are interested in, namely the topics and the coverage, are no longer parameters in LDA.
#c75	These parameters;we;namely the topics;the coverage;parameters;LDA
#s76	In this case we have to use Bayesian inference or posterior inference to compute them based on the parameters Alpha and beta.
#c76	this case;we;Bayesian inference;posterior inference;them;the parameters;Alpha;beta
#s77	Unfortunately, this computation is intractable, so we generally have to resort to approximate.
#c77	this computation;we
#s78	Influence.
#c78	Influence
#s79	And there are many methods are available for and then.
#c79	many methods
#s80	So you will see them when you use different toolkits for LDA, or you read the papers about that these different extensions of LDA.
#c80	you;them;you;different toolkits;LDA;you;the papers;these different extensions;LDA
#s81	Now here we of course can't give in depth introduction to, but just know that they are computed based on Bayesian inference with.
#c81	we;course;depth;introduction;they;Bayesian inference
#s82	By using the parameters of alphas and beta.
#c82	the parameters;alphas;beta
#s83	But algorithmically, actually in the end, in some algorithm at least, it's very similar to PLSA an, especially when we use algorithm called collapsed Gibbs sampling.
#c83	the end;some algorithm;it;PLSA;we;algorithm
#s84	Then the algorithm looks very similar to the EM algorithm.
#c84	the algorithm;the EM algorithm
#s85	So in the end they're doing something very similar.
#c85	the end;they;something
#s86	So to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications.
#c86	our discussion;probabilistic topic models;these models;a general principal way;mining;topics;texts;many applications
#s87	The best basis test setup is to take tax data as input, and we're going to output the key topics.
#c87	The best basis test setup;tax data;input;we;the key topics
#s88	Each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document.
#c88	Each topic;a word distribution;we;to also output proportions;these topics;each document
#s89	And PLSA is the basic topic model, and in fact the most basic topic Model.
#c89	PLSA;the basic topic model;fact;Model
#s90	And this is also often adequate for most applications.
#c90	most applications
#s91	That's why we spend a lot of time to explain PLSA in detail.
#c91	we;a lot;time;PLSA;detail
#s92	Now LDA improves over PLSA by imposing priors.
#c92	LDA;PLSA;priors
#s93	This has led to theoretically more appealing models.
#c93	theoretically more appealing models
#s94	However, in practice, LDA and PLSA intended to give similar performance, so in practice, PLSA, an LDA, would work equally well for most tasks.
#c94	practice;LDA;PLSA;similar performance;practice;PLSA;an LDA;most tasks
#s95	Here are some suggested readings if you want to know more about the topic.
#c95	some suggested readings;you;the topic
#s96	First is a nice review of probabilistic topic models.
#c96	a nice review;probabilistic topic models
#s97	The 2nd paper has a discussion about how to automatically label a topic model.
#c97	The 2nd paper;a discussion;a topic model
#s98	Now I've shown some distributions and they intuitively suggest the topic, but what exactly is the topic?
#c98	I;some distributions;they;the topic;what;the topic
#s99	Can we use phrases to label the topic to make it more easy to understand?
#c99	we;phrases;the topic;it
#s100	And this paper is about the techniques for doing that.
#c100	this paper;the techniques
#s101	The third one is empirical comparison of LDA and PLSA for various tasks.
#c101	The third one;empirical comparison;LDA;PLSA;various tasks
#s102	The conclusion is that they tend to perform similarly.
#c102	The conclusion;they
410	da74c929-efc1-4b65-9635-684c7ebcab3f	64
#s1	This lecture is about evaluation of text cluster.
#c1	This lecture;evaluation;text cluster
#s2	So far we have talked about multiple ways of doing text clustering but how do we know which method works the best?
#c2	we;multiple ways;text clustering;we;which method
#s3	So this has to do with evaluation.
#c3	evaluation
#s4	Now to talk about evaluation, one must go to go back to the clustering bias that we introduced at the beginning.
#c4	evaluation;one;the clustering bias;we;the beginning
#s5	Because two objects can be similar depending on how you look at them, we must clearly specify the perspective of similarity.
#c5	two objects;you;them;we;the perspective;similarity
#s6	Without that, the problem of clustering is not well defined.
#c6	the problem;clustering
#s7	So this perspective is also very important for evaluation.
#c7	this perspective;evaluation
#s8	If you look at this slide and you can see we have two different ways to cluster these shapes.
#c8	you;this slide;you;we;two different ways;these shapes
#s9	An if you ask a question, which one is the best or which one is better you actually see there's no way to answer this question without knowing whether we'd like to cluster based on shapes or cluster based on sizes.
#c9	you;a question;one;which one;you;no way;this question;we;shapes;cluster;sizes
#s10	And that's precisely why the perspective or clustering bias is crucial for evaluation.
#c10	the perspective or clustering bias;evaluation
#s11	In general, we can evaluate text clusters in two ways.
#c11	we;text clusters;two ways
#s12	One is direct evaluation and the other is indirect evaluation.
#c12	direct evaluation;indirect evaluation
#s13	So in directl valuation, we want to answer the following question: How close are the system generated clusters to the ideal clusters that are generated by humans?
#c13	directl valuation;we;the following question;the system;clusters;the ideal clusters;humans
#s14	So the closeness here can be assessed assessed from multiple perspectives and that would help us characterize the quality of clustering results in multiple angles.
#c14	the closeness;multiple perspectives;us;the quality;clustering results;multiple angles
#s15	And this is sometimes desirable.
#s16	Now.
#s17	We also want to quantify the closeness because this would allow us to easily compare different methods based on their performance figures.
#c17	We;the closeness;us;different methods;their performance figures
#s18	And finally, you can see in this case we essentially inject the clustering bias by using humans.
#c18	you;this case;we;the clustering bias;humans
#s19	Basically, humans would bring the needed or desired clustering bias.
#c19	humans;clustering bias
#s20	Now how do we do that exactly?
#c20	we
#s21	The general procedure would look like this.
#c21	The general procedure
#s22	Given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result.
#c22	the test set;a lot;text objects;we;humans;who;the ideal clustering result
#s23	That is, we're going to ask humans to partition the objects to create the gold standard.
#c23	we;humans;the objects;the gold standard
#s24	And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results.
#c24	they;their judgments;the need;a particular application;what;they;the best clustering results
#s25	And this would be then used to compare with the system generated clusters from the same test set.
#c25	the system;clusters;the same test
#s26	And ideally we want the system results to be the same as human generated results, but in general they are not going to be the same, so we would like to then qualify the similarity between the system generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results.
#c26	we;the system;human generated results;they;we;the similarity;the system;clusters;the gold standard clusters;this similarity;multiple perspectives;us;various measures;a cluster clustering result;the commonly used measures;purity;a cluster;similar objects;the same cluster;the gold standard;mutual information;a commonly used measure;the identity;the cluster;object;the system-generated results
#s27	How well can you predict the cluster of the object in the gold standard or vice versa.
#c27	you;the cluster;the object;the gold standard
#s28	Mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose.
#c28	Mutual information;the correlation;these cluster labels;normalized mutual information;the similarity;this evaluation purpose
#s29	F measure is another possible measure.
#c29	F measure;another possible measure
#s30	Now again, a thorough discussion of this evaluation, of these evaluations, would be beyond the scope of this course.
#c30	a thorough discussion;this evaluation;these evaluations;the scope;this course
#s31	I've suggested some reading in the end that you can take a look to know more about that.
#c31	I;some reading;the end;you;a look
#s32	So here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications.
#c32	I;some high level ideas;you;evaluation;your applications
#s33	The 2nd way to evaluate text clusters is to do indirect evaluation.
#c33	The 2nd way;text clusters;indirect evaluation
#s34	So in this case the question to answer is how useful are the clustering results for the intended applications?
#c34	this case;the question;the clustering results;the intended applications
#s35	Now this of course is application specific question, so usefulness is is going to depend on specific applications.
#c35	course;application specific question;usefulness;specific applications
#s36	In this case, the clustering bias is imposed by the intended application as well.
#c36	this case;the clustering bias;the intended application
#s37	So what counts as the best clustering result would be dependent on the application.
#c37	what;the best clustering result;the application
#s38	Procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system.
#c38	we;the test;text objects;the intended application;the performance;the system
#s39	In this case what we care about is the contribution of clustering to some application.
#c39	this case;what;we;the contribution;clustering;some application
#s40	So we often have a baseline system to compare with.
#c40	we;a baseline system
#s41	This could be the current system for doing something and then you hope to add clustering to improve it or the baseline system could be using a different clustering method and you then what you are trying to experiment with and you hope to have a better idea for clustering.
#c41	the current system;something;you;clustering;it;the baseline system;a different clustering method;you;what;you;you;a better idea;clustering
#s42	So in case you have a baseline system to work with and then you can add a clustering algorithm to the baseline system to produce a clustering system.
#c42	case;you;a baseline system;you;a clustering algorithm;the baseline system;a clustering system
#s43	And then we're going to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application.
#c43	we;the performance;your clustering system;the baseline system;terms;the performance measure;that particular application
#s44	So in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application.
#c44	this case;we;it;indirect evaluation;clusters;no explicit assessment;the quality;clusters;its;the contribution;clusters;a particular application
#s45	So to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content.
#c45	text clustering;it;a very useful unsupervised general text mining technique;an overall picture;the text content
#s46	This is often needed to explore text data.
#c46	text data
#s47	And this is often the first step when you deal with a lot of text data.
#c47	the first step;you;a lot;text data
#s48	The second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful.
#c48	The second application;second kind;application;interesting clustering structures;text data;these structures
#s49	There are many approaches that can be used for text clustering and we discussed them: Model based approaches and similarity based approaches.
#c49	many approaches;text clustering;we;them;Model based approaches;similarity based approaches
#s50	In general, strong clusters tend to show up no matter what method is used.
#c50	strong clusters;what method
#s51	Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias.
#c51	Also the effectiveness;a method;the desired clustering bias;the right generative model;the model design;clustering;the right similarity function;bias
#s52	Deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data to guide us to select the best number of clusters.
#c52	the optimal number;clusters;very difficult problem;all the classroom methods;it;unsupervised algorithm;no training data;us;the best number;clusters
#s53	Now sometimes you may see some methods that can automatically determine the number of clusters.
#c53	you;some methods;the number;clusters
#s54	But in general, that has some implied application of clustering bias there, and that's just not specified.
#c54	some implied application;clustering bias
#s55	Without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what?
#c55	a clustering bias;it;the optimal number;cluster;what
#s56	So this is important to keep in mind.
#c56	mind
#s57	And I should also say sometimes we can use application to determine the number of clusters.
#c57	I;we;application;the number;clusters
#s58	For example, if you are clustering search results, then obviously you don't want to generate 100 clusters, right?
#c58	example;you;search results;you;100 clusters
#s59	So the number can be dictated by the interface design.
#c59	the number;the interface design
#s60	In other situations, we might be able to use the fitness of data to assess whether we've got a good number of clusters to explain our data well and to do that, you can vary the number of clusters and watch how well you can fit the data.
#c60	other situations;we;the fitness;data;we;a good number;clusters;our data;you;the number;clusters;you;the data
#s61	If it's in general, when you add more components to mixture model, you should fit the data better, because you can always set the probability of using the new component at 0, so you can't in general fit the data worse than before, but as the question is, as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters.
#c61	it;you;more components;mixture model;you;the data;you;the probability;the new component;you;the data;the question;you;more components;you;the fitness;the data;the right number;clusters
#s62	And finally, evaluation of clustering results and can be done both directly and indirectly.
#c62	And finally, evaluation;clustering results
#s63	And we also would like to do both in order to get good sense about how our method works.
#c63	we;order;good sense;our method
#s64	So here's some suggested reading, and this is particularly useful to better understand the how the measures are calculated and clustering in general.
#c64	some suggested reading;the measures
410	db1d54dd-bb05-46c0-995b-5f7d5243e3c4	70
#s1	This lecture is about the text representation.
#c1	This lecture;the text representation
#s2	In this lecture we're going to discuss text representation.
#c2	this lecture;we;text representation
#s3	And discuss how natural language processing can allow us to represent text in many different ways.
#c3	how natural language processing;us;text;many different ways
#s4	Let's take a look at this example sentence again.
#c4	's;a look;this example sentence
#s5	We can represent this sentence in many different ways.
#c5	We;this sentence;many different ways
#s6	1st.
#c6	1st
#s7	We can always represent such a sentence as a string of characters.
#c7	We;such a sentence;a string;characters
#s8	This is true for all the languages when we store them in the computer.
#c8	all the languages;we;them;the computer
#s9	When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data.
#c9	we;a natural language sentence;a string;characters;we;the most general way;text;we;this approach;any text data
#s10	But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining.
#c10	such a representation;us;semantic analysis;many applications;text mining
#s11	The reason is because we're not even recognizing words.
#c11	The reason;we;words
#s12	So as a string we're going to keep all the spaces and these ASCII symbols.
#c12	a string;we;all the spaces;these ASCII symbols
#s13	We can perhaps count how...
#c13	We
#s14	what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics.
#c14	what;the most frequent character;English text;the correlation;those characters;we;semantics
#s15	Yet this is the most general way of representing text, because we can use this to represent any natural language text.
#c15	the most general way;text;we;any natural language text
#s16	If we try to do a little bit more natural language processing by doing word segmentation.
#c16	we;a little bit more natural language processing;word segmentation
#s17	Then we can obtain a representation of the same text, but in the form of a sequence of words.
#c17	we;a representation;the same text;the form;a sequence;words
#s18	So here we see that we can identify words like: a, dog, is, chasing, etc.
#c18	we;we;words;a, dog;chasing
#s19	Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful.
#c19	this level;representation;we;a lot;things;words;the basic units;human communication;natural language;they
#s20	By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc.
#c20	words;we;example;what;the most frequent words;this document;the whole collection
#s21	And these words can be used to form topics.
#c21	these words;topics
#s22	When we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis.
#c22	we;related words;some words;some words;we;analysis
#s23	So representing text data as a sequence of words opens up a lot of interesting analysis possibilities.
#c23	text data;a sequence;words;a lot;interesting analysis possibilities
#s24	However, this level of representation is slightly less general than string of characters, because in some languages such as Chinese, it's actually not that easy to identify all the word boundaries, because in such a language you see text as a sequence of characters with no space in between.
#c24	this level;representation;string;characters;some languages;Chinese;it;all the word boundaries;such a language;you;text;a sequence;characters;no space
#s25	So you have to rely on some special techniques to identify words.
#c25	you;some special techniques;words
#s26	In such a language, of course, then we might make mistakes in segmenting words.
#c26	such a language;course;we;mistakes;segmenting words
#s27	So the sequence of words representation is not as robust as string of characters.
#c27	the sequence;words;representation;string;characters
#s28	But in English it's very easy to obtain this level of representation, so we can do that all the time.
#c28	English;it;this level;representation;we
#s29	Now if we go further to do natural language processing, we can add a part of speech tags.
#c29	we;natural language processing;we;a part;speech tags
#s30	Now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc.
#c30	we;we;example;the most frequent nouns;what kind;nouns;what kind;verbs
#s31	So this opens up a little bit more interesting opportunities for further analysis.
#c31	a little bit more interesting opportunities;further analysis
#s32	Note that I use the plus sign here, because by representing text as a sequence of part of speech tags.
#c32	I;the plus sign;text;a sequence;part;speech tags
#s33	We don't necessarily replace the original word sequence representation Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags.
#c33	We;the original word sequence representation;we;an additional way;text data;the data;both a sequence;words;a sequence;part;speech tags
#s34	This enriches the representation of text data and thus, also,  enables a more interesting analysis.
#c34	the representation;text data;a more interesting analysis
#s35	If we go further then we'll be parsing the sentence to obtain a syntactic structure.
#c35	we;we;the sentence;a syntactic structure
#s36	Now this of course further open up more interesting analysis of, for example, the writing styles, or correcting grammar mistakes.
#c36	course;more interesting analysis;example;grammar mistakes
#s37	If we could go further for semantic analysis, then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location.
#c37	we;semantic analysis;we;dog;animal;we;boy;a person;a location
#s38	And we can further analyze their relations, for example, dog is chasing the boy and the boy is on the playground.
#c38	we;their relations;example;dog;the boy;the boy;the playground
#s39	Now this is to add more entities and relations through entity-relation recognition.
#c39	more entities;relations;entity-relation recognition
#s40	At this level, then we can do even more interesting things.
#c40	this level;we;even more interesting things
#s41	For example, now we can count easily the most frequent person that's mentioned in this whole collection of news articles, or whenever you mention this person, you also tend to see mention of another person, etc.
#c41	example;we;the most frequent person;this whole collection;news articles;you;this person;you;mention;another person
#s42	So this is very useful representation an it's also related to the Knowledge Graph that some of you may have heard of.
#c42	very useful representation;it;the Knowledge Graph;you
#s43	That Google is doing as a more semantic way of representing text data.
#c43	Google;a more semantic way;text data
#s44	However, it's also less robust than sequence of words or even syntactic analysis, because it's not always easy to identify all the entities with the right types, and we might make mistakes, and relations are even harder to find and we might make mistakes.
#c44	it;sequence;words;even syntactic analysis;it;all the entities;the right types;we;mistakes;relations;we;mistakes
#s45	So this makes this level of representation less robust, yet it's very useful.
#c45	this level;representation;it
#s46	Now if we move further to logical representation then we can have predicates and even inference rules.
#c46	we;logical representation;we;predicates;even inference rules
#s47	And with inference rules we can infer interesting, derived facts from the text.
#c47	inference rules;we;interesting, derived facts;the text
#s48	So that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes, and we can't do that all the time for all kinds of sentences.
#c48	this level;representation;it;we;mistakes;we;all kinds;sentences
#s49	And finally, speech acts with added yet another level of representation of the intent of saying this sentence.
#c49	speech;added yet another level;representation;the intent;this sentence
#s50	So in this case it might be a request.
#c50	this case;it;a request
#s51	So knowing that would allow us to analyze more, even more interesting things about the observer order.
#c51	us;more, even more interesting things;the observer order
#s52	Author of this sentence, what's the intention of saying that?
#c52	this sentence;what;the intention
#s53	What scenarios, what kind of actions will be made?
#c53	what kind;actions
#s54	So this is...
#s55	Another level of analysis that would be very interesting.
#c55	Another level;analysis
#s56	So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used.
#c56	this picture;we;we;more sophisticated natural language processing techniques
#s57	And unfortunately, such techniques would require more human effort.
#c57	such techniques;more human effort
#s58	And they are less accurate.
#c58	they
#s59	That means there are mistakes.
#c59	mistakes
#s60	So if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors.
#c60	we;text data;the levels;, deeper analysis;language;we;the errors
#s61	So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words.
#c61	it;such deep analysis;shallow analysis;example;words
#s62	On the right side you see the arrow points down, to indicate that as we go down with our representation of text, it's closer to knowledge representation in our mind, and need for solving a lot of problems.
#c62	the right side;you;the arrow points;we;our representation;text;it;knowledge representation;our mind;a lot;problems
#s63	Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge.
#c63	we;text;the level;knowledge;we;the knowledge
#s64	That's the purpose of text mining.
#c64	the purpose;text mining
#s65	So there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust.
#c65	a trade;deeper analysis;errors;us;direct knowledge;text;shallow analysis
#s66	But wouldn't actually give us the necessary deeper representation of knowledge.
#c66	us;the necessary deeper representation;knowledge
#s67	I should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role.
#c67	I;text data;humans;humans;a result;a text data analysis text mining;humans;a very important role
#s68	They are always in the loop.
#c68	They;the loop
#s69	Meaning that we should optimize the collaboration of humans and computers.
#c69	we;the collaboration;humans;computers
#s70	So in that sense, it's OK that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively.
#c70	that sense;it;computers;completely accurate representation;text data;patterns;text data;humans;humans;the computers;more accurate analysis;more data;features;the machine;programs;them
410	dc61d7ef-1929-4c5a-95ac-e2c8d8e8c610	22
#s1	this lecture is about the feedback in the language modeling approach in this lecture we will continue the discussion of feedback in text retrieval in particular we're going to talk about the feedback in language modeling approaches so we derive the query likelihood ranking function by making various assumptions as a base for retrieval function that formula all those formulas work that
#c1	this lecture;the feedback;the language modeling approach;this lecture;we;the discussion;feedback;text retrieval;we;the feedback;language modeling approaches;we;the query likelihood ranking function;various assumptions;a base;retrieval function;all those formulas
#s2	well but if we think about the feedback information it's a little bit of all quarter to use query like hold too perform feedback because a lot of times the feedback information is additional information about the query
#c2	we;the feedback information;it;a little bit;all quarter;query;perform feedback;a lot;times;the feedback information;additional information;the query
#s3	but we assume that the query is generated by assembling words from language model in the query like hold method it's kind of a natural to assemble words that form feedback documents as a result then researchers proposed a way to generalize query like hold function and it's called quebec labor diverges retrieval model and this model is actually going to make the query likely hold retrieval function much closer to vertice based model yet this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query like holder as a special case and in this case then feedback can be achieved the throw simple query model estimation more updating this is very similar to rock hill which updates query vector so let's see what is this care divergent switcher model so on the top what you see is query like hold retrieval function right this one and that care divergens or also called cross entropy which is more model is basically to generalize the frequency part here into a lambda model
#c3	we;the query;words;language model;the query;hold method;it;kind of a natural;words;feedback documents;a result;researchers;a way;query;hold function;it;retrieval model;this model;the query;retrieval function;vertice based model;this form;the language model;a generalization;query;hold;the sense;it;query;holder;a special case;this case;feedback;the throw simple query model estimation;rock hill;query vector;'s;what;this care divergent switcher model;the top;what;you;query;retrieval function;that care divergens;cross entropy;more model;the frequency part;a lambda model
#s4	so basically it's the difference given by the probabilistic model here to characterize what the user is looking for versus the count of query words there and this difference allows us to plug in various different ways to estimate this so this can be estimated in many different ways including using feedback information now this is called a care divergent becaus this can be interpreted as measuring the KL divergent SOV two distributions one is the query model in order by this distribution one is the document that language model here an SMS it with clashing language model of course and we're not going to talk about the detail of that and going to find it in some references it's also called cross entropy be cause in the fact we can ignore some terms in the care divergent function and we will end up having actually cross entropy and that both are terms in information theory but anyway for our purpose here you can just receive the two formulas look almost identical except that here we have a probability of award given by a query language model all this and here the sum is over all the words that are in the document and also with the number zero probability for the query model
#c4	it;the difference;the probabilistic model;what;the user;the count;query words;this difference;us;various different ways;many different ways;feedback information;a care divergent becaus;the KL divergent SOV;two distributions;the query model;order;this distribution;the document;an SMS;it;language model;course;we;the detail;it;some references;it;cross entropy;cause;the fact;we;some terms;the care divergent function;we;entropy;terms;information theory;our purpose;you;the two formulas;we;a probability;award;a query language model;the sum;all the words;the document;the number zero probability;the query model
#s5	so it's kind of again a generalization of some over the match query words now you can also easy to see we can recover the query like code which will something by simplest setting this query model to the relative frequency of words in the query this is very easy to see once you plug this into here you can eliminate this query length that's a constant and then you'll get exactly like that so you can see the equipments and that's also why this K I virgins model can be regarded as a generalization of query like hold becaus we can cover query like rolled as a special case but it would also allow us to do much more than that
#c5	it;a generalization;the match query words;you;we;the query;code;something;this query model;the relative frequency;words;the query;you;you;this query length;you;you;the equipments;this K;I;model;a generalization;query;becaus;we;query;a special case;it;us
#s6	so this is how we can use the care divergent 's model to feedback the picture shows that we first estimate document language model then we estimate the query named model and we compute the KL divergent is often denoted by A D here but this basically means this was exactly i convect this based model cause we come through the vector for the document are computer another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions and then we got the results and we can find some feedback documents let's assume they are most inactive sorry mostly positive documents although we could also consider both kinds of documents so what we could do is like in rock you'll ever know compute another language model code feedback language model here again this is going to be another vector just like a computing century about the in rock hill and then this model can be combined with the original query model using a linear interpolation and this would then give us a update model just like again in rock hill so here we can see the parameter alpha can control the amount of feedback if it's set to zero then you say here there's no feedback after set to one we got full feedback if we ignore the original query and this is generated not desirable so this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are important so of course the main question here is how do you compute this data F this is the big question here and once you can do that the rest is easy
#c6	we;the care divergent 's model;the picture;we;document language model;we;the query;model;we;the KL divergent;A D;i;we;the vector;the document;another vector;the query;we;the distance;these vectors;special forms;their probability distributions;we;the results;we;some feedback documents;'s;they;most inactive sorry mostly positive documents;we;both kinds;documents;what;we;rock;you;another language model code feedback language model;another vector;a computing century;rock hill;this model;the original query model;a linear interpolation;us;a update model;rock hill;we;the parameter alpha;the amount;feedback;it;you;no feedback;we;full feedback;we;the original query;you;you;a lot;relevant documents;the query terms;course;the main question;you;this data;the big question;you;the rest
#s7	so here will talk about one of the approaches and there are many approaches of course this approach is based on generated model
#c7	the approaches;many approaches;course;this approach;generated model
#s8	and i'm going to show you how it works this is to use a generator mixture ball so this picture shows that we have this model here the feedback model that we want to estimate and the basis is the feedback of documents let's say we are observing the positive documents these are the click the documents by users or random documents judging by users or simply top ranked document that we assumed to be relevant now imagine how we can compute ascentia weight for these documents by using language model one approach is simply to assume these documents our generator from this language model as we did before what we could do is do just normalize the water frequency here
#c8	i;you;it;a generator mixture ball;this picture;we;this model;the feedback model;we;the basis;the feedback;documents;'s;we;the positive documents;the click;the documents;users;random documents;users;ranked document;we;we;ascentia weight;these documents;language model;one approach;these documents;our generator;this language model;we;what;we;the water frequency
#s9	and then we get this water distribution now the question is whether this distribution is good for feedback or you can imagine the top ranking the words would be well what do you think well those words will be common was right as we always see in a language model the top rated words are actually common words like the etc
#c9	we;this water distribution;the question;this distribution;feedback;you;the top;the words;what;you;those words;we;a language model;the top rated words;common words
#s10	so it's not very good for feedback because we would be adding a lot of such words to our query when we interpreted this with original query model
#c10	it;feedback;we;a lot;such words;our query;we;original query model
#s11	so this is not good so we need to do something in particular we are trying to get rid of those common words and we have seen actually one way to do that by using background language model in the case of learning associations with words words that are related to the water computer we could do that and that would be another way to do this
#c11	we;something;we;those common words;we;one way;background language model;the case;associations;words words;the water computer;we;another way
#s12	but here we are going to talk about another approach which is more principle approach in this case we're going to say well you say that there are common words here in this these documents that should not belong to this topic model
#c12	we;another approach;more principle approach;this case;we;you;common words;these documents;this topic model
#s13	right so now what we can do is to assume that well those was are generated from background language model so they were generated those words like the example and if we use maximum likely resume to note that if all the words here must be generated from this model then this model is forced to assign high probabilities to award like that because it occurs so frequently here note that in order to reduce its probability in this model we have to have another model which is this one to help explain the word the here and in this case it's not a property to use the background language model to achieve this goal becaus this model would assign high probabilities to these common words so in this approach then we assume this machine that would generate these words with work as follows we have a source controller here imagine we flip a coin here to decide what distribution to use was probability of lemma the coin shows up as head and we're going to use the background language model and we can do then simple word from that more with probability of one minus them now we do decide to use the unknown topic model here that we would like to estimate and we're going to then generate a reward if we make this assumption and this whole thing will be just one model and we call this mixture model becaus there are two distributions that are mixed together and we actually don't know when each distribution is used so again think of this whole thing as one model and we just go ask for words and it will still give us a war in a random manner right and of course which word would show up with depend on both this distribution and that is reaching in the teaching would also depend on this lambda becaus if you say lambda is very high and it's going to always use the background distribution you'll get different words and then if you say well i'm not very small we're going to use this right so all these are parameters in this model and then if you think in this way basically we can do exactly the same as what we did before we're going to use maximum likelihood estimator to adjust this model to estimate the parameters basically we're going to adjust well this parameter so that we can pass to explain all the data the difference now is that we are not asking this model alone to explain this but rather we're going to ask this whole model makes you more also explain the data becaus there has got some help from the background model it doesn't have to assign high probabilities towards mega the as a result it would then assign higher probabilities two other words that are common here but not having high probability here so those will be common here
#c13	what;we;background language model;they;those words;the example;we;maximum likely resume;all the words;this model;this model;high probabilities;it;order;its probability;this model;we;another model;this one;the word;this case;it;a property;the background language model;this goal becaus;this model;high probabilities;these common words;this approach;we;this machine;these words;work;we;a source controller;we;a coin;what distribution;probability;lemma;head;we;the background language model;we;simple word;probability;them;we;the unknown topic model;we;we;a reward;we;this assumption;this whole thing;just one model;we;this mixture model becaus;two distributions;we;each distribution;this whole thing;one model;we;words;it;us;a war;a random manner;course;which word;depend;both this distribution;the teaching;this lambda becaus;you;lambda;it;the background distribution;you;different words;you;i;we;this right;parameters;this model;you;this way;we;what;we;we;maximum likelihood estimator;this model;the parameters;we;this parameter;we;all the data;the difference;we;this model;we;this whole model;you;the data becaus;some help;the background model;it;high probabilities;mega;a result;it;higher probabilities;two other words;high probability
#s14	i and if they are common they would have to have a high probability this according to maximum like are estimated anne if they are rare here so if they are rare here then you don't get much help from this background model as a result this topic model must assign high probability days so the high probability words according to the topic model would be those that are common here but rare in the background
#c14	i;they;they;a high probability;maximum;anne;they;they;you;much help;this background model;a result;this topic model;high probability days;the high probability words;the topic model;the background
#s15	OK so this is basically a little bit like a idea of waiting here but this would allow us to achieve the effect of removing these top awards that are meaningless in the feedback so mathematically what we have is to compute the like hold again local like hold of the feedback documents and and note that we also have another parameter lambda here
#c15	a idea;us;the effect;these top awards;the feedback;what;we;the like hold;hold;the feedback documents;we;another parameter lambda
#s16	but we assume that the lender denotes the noise in the feedback document so we are going to let's say set this to a parameter let's say fifty percent of the words are noise or nine example noise and this can be assumed it would be fixed if we assume this fixed then we only have these probabilities as parameters just like in the simplest unigram language model we have N parameters an is the number of words and then the likelihood of function would look like this it's very similar to the likelihood function dog likely hold function free see before except that inside of the logarithm there's a some here and this some isba cause we consider two distributions and which ones used would depend on lambert
#c16	we;the lender;the noise;the feedback document;we;'s;a parameter;'s;fifty percent;the words;noise;nine example noise;it;we;we;these probabilities;parameters;the simplest unigram language model;we;N parameters;the number;words;the likelihood;function;it;the likelihood function dog;function;the logarithm;this some isba;we;two distributions;which ones;lambert
#s17	and that's why we have this form but mathematically this is the uh function with theater as unknown variables
#c17	we;this form;the uh function;theater;unknown variables
#s18	so this is just a function all the other values are known except for this guy so we can then choose this probability distribution to maximize this log likely code the same idea as the maximum like horace made it as a mathematical problem we just we just have to solve this optimization problem we send your word try all the theater values and until we find one that gives this whole thing the maximum probability so it's well defined math problem once we have done that we obtain this F that can be the interpreter with original query model to feedback so here are some examples of the feedback model learned from a web document collection and we do pseudo feedback are we just use the top ten documents and we use this mixture model so the queries airport the security what we do is we first retrieve ten documents from the web database and this is of course a pseudo feedback
#c18	just a function;all the other values;this guy;we;this probability distribution;this log;the same idea;horace;it;a mathematical problem;we;we;this optimization problem;we;your word;all the theater values;we;this whole thing;the maximum probability;it;well defined math problem;we;we;this F;the interpreter;original query model;some examples;the feedback model;a web document collection;we;feedback;we;the top ten documents;we;this mixture model;the queries;the security;what;we;we;ten documents;the web database;course;a pseudo feedback
#s19	and then we're going to fit that mixture model to this ten welcome the set
#c19	we;that mixture model;this ten welcome;the set
#s20	and these are the words learned using this approach this is the probability of award given by the feedback model in both cases so in both cases you can see the highest probability was include a very relevant words to the query so airport security for example these query words still show up as high probabilities in each case naturally becaus they occur frequently that operated documents
#c20	the words;this approach;the probability;award;the feedback model;both cases;both cases;you;the highest probability;a very relevant words;the query;airport security;example;these query words;high probabilities;each case;they
#s21	but we also see beverage alcohol bomb terrorists etc so these are relevant to this topic and they if combined with the original query you can help us met more accurately documents and also they can help us bring up documents that only imagine the some of these other words and maybe for example just the airport and then form for example so this is how sudo works issues that this model really works and picks up some related words to the query what's also interesting is that if you look at the two tables here and you come here then and you see in this case when lembar is set to a small value and we still see some common words here and that means when we don't use the background model often remember lemler confuses the probability of using the background model to generate the text if we don't rely much on background model we still have to use this topic model to account for the common words whereas if we set lambda to a very high value we will use the background model very often to explain these words then there's no burden on explaining those common words in the feedback documents by the topic model
#c21	we;beverage alcohol bomb terrorists;this topic;they;the original query;you;us;documents;they;us;documents;these other words;example;example;issues;this model;some related words;the query;what;you;the two tables;you;you;this case;lembar;a small value;we;some common words;we;the background model;the probability;the background model;the text;we;background model;we;this topic model;the common words;we;lambda;a very high value;we;the background model;these words;no burden;those common words;the feedback documents;the topic model
#s22	so as a result of the topic model here is very discriminant if it contains all the relevant without common words so this can be added to the original query to achieve feedback so to summarize in this lecture will talk about the feedback in language model approach in general feedback is to learn from examples these examples can be assumed examples can be sued examples like assume top ten document that are assumed to be random there could be based on user interactions like a feedback based on clicks rules or implicit feedback we talked about the three major feedback scenarios random is feedback pseudo feedback and in principle feedback we talked about how to use rock you to do figure back in vector space model and how to use query model as machine for feedback in language model and we briefly talk about the mixture model the basic idea there are many other methods for example the relevance model is a very effective model for estimating query model so you can read more about these methods in the references are listed at the end of this lecture so there are two additional readings here the first one is book that has a systematic review and discussion of language models of all information retrieval and signal one is important research paper that's about relevance based on damage models and it's a very effective way of computing query model
#c22	a result;the topic model;it;common words;the original query;feedback;this lecture;the feedback;language model approach;general feedback;examples;these examples;examples;examples;top ten document;user interactions;a feedback;clicks rules;implicit feedback;we;the three major feedback scenarios;feedback pseudo feedback;principle feedback;we;rock;you;vector space model;query model;machine;feedback;language model;we;the mixture model;many other methods;example;the relevance model;a very effective model;query model;you;these methods;the references;the end;this lecture;two additional readings;the first one;book;a systematic review;discussion;language models;all information retrieval;important research paper;relevance;damage models;it;a very effective way;computing query model
410	dccc8a84-66da-47ce-ab88-28e8acf192b9	116
#s1	This lecture is a continued discussion of generative probabilistic models for text clustering.
#c1	This lecture;a continued discussion;generative probabilistic models;text clustering
#s2	In this lecture we're going to finish the discussion of generative probabilistic models for text clustering.
#c2	this lecture;we;the discussion;generative probabilistic models;text clustering
#s3	So this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters.
#c3	a slide;you;we;we;the mixture model;text;what;the likelihood function;we;the maximum liklihood estimate;the parameters
#s4	In this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator.
#c4	this lecture;we;we;the maximum likelihood estimator
#s5	Now, as in most cases, the EM algorithm can be used to solve this problem for mixture models.
#c5	most cases;the EM algorithm;this problem;mixture models
#s6	So here's the detail of this EM algorithm for document clustering.
#c6	the detail;this EM algorithm;document clustering
#s7	Now, if you have understood how EML works for topic models, PLSA and I think here it will be very similar and you just need to adapt a little bit to  this new mixture model.
#c7	you;EML;topic models;PLSA;I;it;you;a little bit;this new mixture model
#s8	So as you may recall, EM algorithm starts with initialization of all the parameters.
#c8	you;EM algorithm;initialization;all the parameters
#s9	So this is the same as what happened before for topic models.
#c9	what;topic models
#s10	And then we're going to repeat until their likelihood converges.
#c10	we;their likelihood converges
#s11	An in each step will do E step and M step.
#c11	each step;E step;M step
#s12	in M step.
#c12	M step
#s13	we're going to infer which distribution has been used to generate each document.
#c13	we;which distribution;each document
#s14	And so we have to introduce a hidden variable ZD for each document and this value variable could take a value from the range of one through K representing K different distributions.
#c14	we;a hidden variable ZD;each document;this value variable;a value;the range;K representing K different distributions
#s15	And more specifically, basically we're going to apply Bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution.
#c15	we;Bayes rule;which distribution;this document;the posterior probability;the distribution
#s16	Given the document.
#c16	the document
#s17	An we know it's proportional to the probability of selecting this distribution P of Theta I and the probability of generating this whole document from that distribution, which is a product of all the probabilities of words for this document, as you see here.
#c17	we;it;the probability;this distribution P;Theta;I;the probability;this whole document;that distribution;a product;all the probabilities;words;this document;you
#s18	Now, as in all cases, it's useful to kind of remember the normalizer or the OR the constraint on this probability.
#c18	all cases;it;the normalizer;the OR;the constraint;this probability
#s19	So in this case we know the constraint on this probability in the E step is that all the probabilities of Z equals I must sum to one 'cause the document must have been generated from precise or one of these K topics.
#c19	this case;we;the constraint;this probability;the E step;all the probabilities;Z;I;the document;these K topics
#s20	So the probability of the generator from each of them should sum to one.
#c20	the probability;the generator;them
#s21	And if this constraint then you can easy to compute this distribution as long as.
#c21	you;this distribution
#s22	what it is proportional to, right?
#c22	what;it
#s23	So once you compute this product that you see here, then you simply normalize this these probabilities to make them some to 1 what over all the topics.
#c23	you;this product;you;you;these probabilities;them;all the topics
#s24	So that's E step after E Step we would know  which distribution is more likely to have generated this document D, which is unlikely.
#c24	E step;E Step;we;which distribution;this document D
#s25	And then in the M step, we're going to relist, made all the parameters based on the, infer the Z values, or in further knowledge about which district has been used to generate which document.
#c25	the M step;we;all the parameters;the Z values;further knowledge;which district;which document
#s26	So there estimation involves two kinds of parameters.
#c26	estimation;two kinds;parameters
#s27	One is P of Theta and this is the probability of selecting a particular distribution.
#c27	P;Theta;the probability;a particular distribution
#s28	Before we observe anything, we don't have any knowledge about which cluster is more likely, but after we have observed these documents, then we can collect the evidence.
#c28	we;anything;we;any knowledge;cluster;we;these documents;we;the evidence
#s29	To infer which cluster is more likely, and so this is proportional to the sum of the probability of Z sub DJ is equal to I.
#c29	which cluster;the sum;the probability;Z sub DJ;I.
#s30	And so this gives us all the evidence about using topic.
#c30	us;all the evidence;topic
#s31	I said I to generate a document and we put them together and again we normalize them into probabilities.
#c31	I;I;a document;we;them;we;them;probabilities
#s32	And then so this is for P of Theta sub
#c32	P;Theta;sub
#s33	I.
#c33	I.
#s34	Now the other kind of parameters are the probabilities of words in each distribution, each cluster, and this is very similar to the case of PLSA.
#c34	the other kind;parameters;the probabilities;words;each distribution;each cluster;the case;PLSA
#s35	And here we just pulled the counts of words that are in documents that are inverted to have been generated from a particular topic Theta I here   
#c35	we;the counts;words;documents;a particular topic;I
#s36	and this would allow us to then estimate how many words have actually been generated from Theta I.
#c36	us;how many words;Theta I.
#s37	And then we normalize again.
#c37	we
#s38	These counts into probabilities so that the probabilities on all the words  some to one.
#c38	probabilities;the probabilities;all the words
#s39	Note that it's very important to understand these constraints as they are precisely the normalizers in all these formulas, and it's also important to know that distribution is over what?
#c39	it;these constraints;they;the normalizers;all these formulas;it;distribution;what
#s40	For example, the probability of Theta is overall the key topics and that's why these K probabilities sum to 1.
#c40	example;the probability;Theta;the key topics;these K probabilities
#s41	well, whereas the probability of a word given Theta is a probability distribution over all the words.
#c41	the probability;a word;Theta;a probability distribution;all the words
#s42	So there are many probabilities and they have to send one.
#c42	many probabilities;they
#s43	So now let's take a look like this.
#c43	's;a look
#s44	Take a look at the simple example of two clusters.
#c44	a look;the simple example;two clusters
#s45	I have two clusters.
#c45	I;two clusters
#s46	I've shown some initializer values for the two distributions.
#c46	I;some initializer values;the two distributions
#s47	And let's assume we randomly initialized to probabilities of selecting each cluster as .5.
#c47	's;we;probabilities;each cluster
#s48	So equally likely.
#s49	And then let's consider one document that you have seen here.
#c49	's;one document;you
#s50	There are two words, sorry, two occurrences of text and two occurrences of mining.
#c50	two words;two occurrences;text;two occurrences;mining
#s51	So there are four words together.
#c51	four words
#s52	Medical and health did not occur in this document, so this first thing about the hidden variables.
#c52	health;this document;the hidden variables
#s53	Now for each document we must use a hidden variable and before in PLSA we used 1 hidden variable for each word.
#c53	each document;we;a hidden variable;PLSA;we;1 hidden variable;each word
#s54	Because that's the output from what mixture model.
#c54	the output;what mixture model
#s55	So in our case the output from a mixture model or the observation from mixture model is a document not a word.
#c55	our case;the output;a mixture model;the observation;mixture model;a document;not a word
#s56	So now we have 1 hidden variable attached to the document.
#c56	we;1 hidden variable;the document
#s57	That hidden variable must tell us which distribution has been used to generate the document, so it's going to take two values, one and two to indicate the two topics.
#c57	That hidden variable;us;which distribution;the document;it;two values;the two topics
#s58	So now how do we infer which distribution has been used to generate the D?
#c58	we;which distribution;the D
#s59	It's to use Bayes rule so it looks like this in order for the first topic is setup, want to generate the document.
#c59	It;Bayes rule;it;order;the first topic;setup;the document
#s60	Two things must happen.
#c60	Two things
#s61	First theater subway must have been selected, so it's given by P of 01 second.
#c61	First theater subway;it;P;01 second
#s62	It must have also been generating the four words in the document, namely two occurrences of text and two occurrences of mining.
#c62	It;the four words;the document;namely two occurrences;text;two occurrences;mining
#s63	That's why you see the numerator has the product of the probability of selecting Theta one and the probability of generating the document from Theta 1.
#c63	you;the numerator;the product;the probability;the probability;the document;Theta
#s64	So the denominator is just the sum of two possibilities of generating this document, and you can plug in the numerical values to verify.
#c64	the denominator;just the sum;two possibilities;this document;you;the numerical values
#s65	Indeed in this case the document is more likely to be generated from Theta 1,  much more likely than from than Theta 2.
#c65	this case;the document;Theta;Theta
#s66	So once we have this problem that we can easily compute the probability of Z = 2 given this document, how we're going to use the constraint?
#c66	we;this problem;we;the probability;Z;=;this document;we;the constraint
#s67	Right now it's going to be 1 - 100 /101 1,000,000 one.
#c67	it
#s68	So now it's important to note that in such a computation there is a potential problem of underflow, and that is because if you look at the numerator, the original numerator and denominator it involves the computation of a product of many small probabilities.
#c68	it;such a computation;a potential problem;underflow;you;the numerator;the original numerator;denominator;it;the computation;a product;many small probabilities
#s69	Imagine if a document has many words and it's going to be a very small value here, as it can cause the problem of underflow.
#c69	a document;many words;it;a very small value;it;the problem;underflow
#s70	So to solve the problem.
#c70	the problem
#s71	We can use a normalized.
#c71	We
#s72	So here you see that we take average of all these two solutions to compute another average district called Theater Bar.
#c72	you;we;all these two solutions;another average district;Theater Bar
#s73	And this does the average distribution will be comperable to each of these distributions.
#c73	the average distribution;these distributions
#s74	In terms of the quantities, the magnitude.
#c74	terms;the quantities
#s75	So we can then divide the numerator and the denominator both by this normalizer.
#c75	we;the numerator;the denominator;this normalizer
#s76	So basically this normalizes the probability of generating this document by using this average word distribution.
#c76	the probability;this document;this average word distribution
#s77	So you can see the normalizer here.
#c77	you;the normalizer
#s78	And since we have used exact the same normalizer for the numerator and denominator, the whole value of this expression is not changed.
#c78	we;the same normalizer;the numerator;denominator;the whole value;this expression
#s79	But by doing this normalization you can see we can make the numerators and denominators more manageable in that the overall value is not going to be very small for each, and thus we can avoid underflow problem.
#c79	this normalization;you;we;the numerators;denominators;the overall value;we;underflow problem
#s80	In some other times we sometimes also use logarithm of the product to convert this into a sum of log of probabilities.
#c80	some other times;we;logarithm;the product;a sum;log;probabilities
#s81	This can help preserve precision as well, but in this case we cannot use logarithms to solve the problem because there's sum in the denominator, But this kind of normalizes can be effective for solving this problem, so it's a technique that's sometimes useful in other situations as well.
#c81	precision;this case;we;logarithms;the problem;sum;the denominator;this kind;normalizes;this problem;it;a technique;other situations
#s82	Now let's look at the M step.
#c82	's;the M step
#s83	So from the E step we can see our estimate of which distribution is more likely to have generated a document, and you can see D1 is more likely from the first topic.
#c83	the E step;we;our estimate;which distribution;a document;you;D1;the first topic
#s84	Where is D2 is more like from the second topic, etc.
#c84	D2;the second topic
#s85	Now let's think about what we need to compute in the M step.
#c85	's;what;we;the M step
#s86	Basically we need to re estimate all the parameters.
#c86	we;all the parameters
#s87	Let's first look at the P of Theta 1 and P of Theta 2.
#c87	's;the P;Theta;P;Theta
#s88	How do we estimate that?
#c88	we
#s89	Intuitively, you can just pull together the Z probability Z probabilities from E Steps, right?
#c89	you;the Z probability Z probabilities;E Steps
#s90	So if all these documents say they're more likely from silouan, then we intuitively would give a high probability to see that one right?
#c90	all these documents;they;silouan;we;a high probability
#s91	So in this case, so we can just take the average of these probabilities that you see here, and we obtain the .6 for Theta 1 so Theta 1 is more likely  than Theta 2.
#c91	this case;we;the average;these probabilities;you;we;Theta;Theta;Theta
#s92	So you can see the probability of Theta 2 would be naturally .4.
#c92	you;the probability;Theta
#s93	What about these world probabilities?
#c93	these world probabilities
#s94	What we do the same?
#c94	What;we
#s95	And intuition is the same, so we're going to see in order to estimate the probabilities of words in Theta one, we're going to look at which documents have been generated from Scylla and we're going to pull together the words in those documents and normalize them.
#c95	intuition;we;order;the probabilities;words;Theta;we;documents;Scylla;we;the words;those documents;them
#s96	So this is basically what I just said.
#c96	what;I
#s97	Most specifically, we're going to for example.
#c97	we;example
#s98	Use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account.
#c98	all the counts;text;these documents;the probability;tax;we;their raw counts;total account
#s99	Instead, we can do that.
#c99	we
#s100	Discount them by the probabilities that each document is likely be generated from Theta 1.
#c100	them;the probabilities;each document;Theta
#s101	So this gives us some fractional counts, and then these Council would be then normalized in order to get the probability.
#c101	us;some fractional counts;these Council;order;the probability
#s102	Now how do we normalize them?
#c102	we;them
#s103	These probabilities of these words must sum to one.
#c103	These probabilities;these words
#s104	So to summarize, our discussion of generating models for clustering.
#c104	our discussion;generating models;clustering
#s105	We showed that a slight variation of Top Model can be used for clustering documents and this also shows the power of generating models in general by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data.
#c105	We;a slight variation;Top Model;documents;the power;generating models;the generation assumption;the model;we;different goals;we;different patterns;text data
#s106	So in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model.
#c106	this case;each class;unigram language model;word distribution;topic model
#s107	So here you can see the word distribution actually generates a term cluster as a byproduct.
#c107	you;the word distribution;a term cluster;a byproduct
#s108	A document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models.
#c108	A document;a unigram language model;all the words;the document;this single language model;again topic model;we;the words;the document;multiple unigram language models
#s109	And then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster.
#c109	the estimated model pamateter;both a topic capitalization;each cluster;the probabilistic assignment;each document;a cluster
#s110	And this probabilistic assignment that sometimes is useful for some applications.
#c110	And this probabilistic assignment;some applications
#s111	But if we want to achieve a harder clusters mainly to partition documents into disjoint clusters.
#c111	we;a harder clusters;documents;disjoint clusters
#s112	Then we can just force the document into the cluster corresponding to the water distribution.
#c112	we;the document;the cluster;the water distribution
#s113	That's most likely to have generated the document.
#c113	the document
#s114	We've also shown that the EM algorithm can be used with computer.
#c114	We;the EM algorithm;computer
#s115	The maximum lag or is made up an.
#c115	The maximum lag
#s116	In this case we need to use a special normalization technique to avoid underflow.
#c116	this case;we;a special normalization technique
410	dda800a9-7b08-467f-b670-ebed5d4a639b	32
#s1	This lecture is about learning to rank.
#c1	This lecture
#s2	In this lecture we are going to continue talking about web search.
#c2	this lecture;we;web search
#s3	In particular, we're going to talk about the using machine learning to combine different features to improve the ranking function.
#c3	we;different features;the ranking function
#s4	So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results.
#c4	the question;we;this lecture;we;many features;a single ranking function;search results
#s5	In the previous lectures we have talked about a number of ways to rank documents.
#c5	the previous lectures;we;a number;ways;documents
#s6	We have talked about some retrieval models, like BM25 or query like code.
#c6	We;some retrieval models;BM25;query;code
#s7	They can generate the content based scores for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking.
#c7	They;the content based scores;documents;a query;we;the link based approaches;page rank;additional scores;us
#s8	Now the question now is how can we combine all these features and potential many other features to do ranking and this will be very useful for ranking web pages.
#c8	the question;we;all these features;potential many other features;ranking;ranking web pages
#s9	Not only just to improve accuracy, but also to improve the robustness of the ranking function so that it's not easy for a spammer to just perturb a one or a few features to promote the page.
#c9	accuracy;the robustness;the ranking function;it;a spammer;a one or a few features;the page
#s10	So the general idea of learning to rank is to use machine learning to combine these features to optimize the weights, different features to generate the optimal ranking function.
#c10	the general idea;these features;the weights;different features;the optimal ranking function
#s11	So we will assume that the given a query document appear Q and D. We can define a number of features.
#c11	we;a query document;Q;D.;We;a number;features
#s12	And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25, or query likelihood or pivot length normalization PL2 etc.
#c12	these features;content based features;a score;the document;respect;the query;a retrieval function;BM25;query likelihood
#s13	It can also be linked based score like page rank score.
#c13	It;based score;page rank score
#s14	It can be also application of retrieval models to the anchor text of the page, those are the text descriptions of links that point to this page.
#c14	It;application;retrieval models;the anchor text;the page;the text descriptions;links;this page
#s15	So these can all be clues about whether this document is relevant or not.
#c15	clues;this document
#s16	We can even include a feature such as whether the URL has a tilde '~', because this might be indicator of homepage or engine page.
#c16	We;a feature;the URL;a tilde;indicator;homepage or engine page
#s17	So all these features can then be combined together to generate the ranking function.
#c17	all these features;the ranking function
#s18	The question is of course, how can we combine them?
#c18	The question;course;we;them
#s19	In this approach we simply hypothesize that the probability that this document is relevant to this query is function of all these features.
#c19	this approach;we;the probability;this document;this query;function;all these features
#s20	So we can hypothesize that the probability of relevance is related to these features through a particular form of the function that has some parameters.
#c20	we;the probability;relevance;these features;a particular form;the function;some parameters
#s21	These parameters can control the influence of different features on the final relevance.
#c21	These parameters;the influence;different features;the final relevance
#s22	This is of course just a assumption whether this assumption really makes sense is the big question and they have to empirically evaluate the function.
#c22	course;just a assumption;this assumption;sense;the big question;they;the function
#s23	But by hypothesising that relevance is related to these features in a particular way, we can then combine these features to generate the potentially more powerful ranking function and more robust ranking function.
#c23	hypothesising;that relevance;these features;a particular way;we;these features;the potentially more powerful ranking function;more robust ranking function
#s24	Naturally, the next question is how do we estimate those parameters and how do we know which features should have a higher weight than which features should have lower weight, so this is the task of training or learning, right?
#c24	the next question;we;those parameters;we;which features;a higher weight;features;lower weight;the task;training;learning
#s25	So in this approach, what we will do is to use some training data.
#c25	this approach;what;we;some training data
#s26	Those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be ranked high for which queries.
#c26	the data;users;we;the relevance judgments;we;which documents
#s27	And this information can be based on real judgments by users, or this can also be approximated by just using click through information where we can assume the clicked documents are better than the skiped documents or clicked documents are relevant and skiped documents are non relevant.
#c27	this information;real judgments;users;information;we;the clicked documents;the skiped documents;clicked documents;documents
#s28	So in general, we would fit such a hypothesize the ranking function to the training data, meaning that we will try to optimize its retrieval accuracy on the training data, we can adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as MAP or nDCG.
#c28	we;such a hypothesize;the ranking function;the training data;we;its retrieval accuracy;the training data;we;these parameters;we;the performance;the function;the training data;terms;some measures;MAP;nDCG
#s29	So the training data would look like a table of tuples.
#c29	the training data;a table;tuples
#s30	Each tuple has three elements.
#c30	Each tuple;three elements
#s31	The query, the document and judgment.
#c31	The query;the document;judgment
#s32	So it looks very much like our relevance judgments that we talked about in evaluation of retrieval systems.
#c32	it;our relevance judgments;we;evaluation;retrieval systems
410	de325d45-7bf3-451f-99c7-8949673e2f5e	16
#s1	this letter is about to some practical issues that you would have to address in evaluation of text retrieval systems in this lecture we will continue the discussion of evaluating will cover some practical issues that you have to solve in actual evaluation of text retrieval systems so in order to create the tester clashing we have to create a set of queries a set of documents and set of relevance judgments it turns out that each is actually challenging to create first the documents and queries must be representative they must represent the real queries anril documents that the users handle and we also have to use many queries and many documents in order to avoid a bias to conclusions for the matching of relevant documents with the queries we also need to ensure that there exists a lot of relevant documents for each query if a query has only one let's stay relevant document in the collection then you know it's not very informative to compare different methods using such a query 'cause there's not much room for us to see difference so ideally there should be more relevant documents in the collection
#c1	this letter;some practical issues;you;evaluation;text retrieval systems;this lecture;we;the discussion;evaluating;some practical issues;you;actual evaluation;text retrieval systems;order;the tester;we;a set;queries;a set;documents;relevance judgments;it;the documents;queries;they;the real queries;anril documents;the users;we;many queries;many documents;order;a bias;conclusions;the matching;relevant documents;the queries;we;a lot;relevant documents;each query;a query;'s;relevant document;the collection;you;it;different methods;such a query;much room;us;difference;more relevant documents;the collection
#s2	but yet the query is also we should represent the real queries that we care about in terms of relevance judgments the challenge is to ensure complete judgments of all the documents for all the queries yet minimizing human fault be cause we have to use the human labor to label these documents it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries especially considering a giant data set micro the web
#c2	the query;we;the real queries;we;terms;relevance judgments;the challenge;complete judgments;all the documents;all the queries;human fault;we;the human labor;these documents;it;a result;it;all the documents;all the queries;a giant data;micro the web
#s3	so this is actually a major challenge it's a very difficult challenge for measures it's also challenging because we want to measure that would accurately reflect the perceived utility of users we have to consider carefully what the users care about and that design measures to measure that if you measure is not measuring the right thing then your conclusion would be misled so it's very important so we're going to talk about a couple of issues here one is the statistical significance test and this also is the reason why we have to use a lot of queries and the question here is how sure can you be that observed difference it doesn't simply result from the particular queries you choose
#c3	a major challenge;it;a very difficult challenge;measures;it;we;the perceived utility;users;we;what;the users;you;the right thing;your conclusion;it;we;about a couple;issues;the statistical significance test;the reason;we;a lot;queries;the question;you;that observed difference;it;the particular queries;you
#s4	so here are some sample results of average position for system and system be in two different experiments and you can see in the bottom we have mean average precision
#c4	some sample results;average position;system;system;two different experiments;you;the bottom;we;average precision
#s5	so the mean if you look at the meaning average precision the mean average precisions all exactly the same in both experiments so you can see this is point two this is point four four system V and again here it's also pointed to an point the fall so they are identical yet if you look at these exact average positions for different queries if you look at these numbers in detail you will realize that the in one case you would feel that you can trust the conclusion here given by the average in another case in the other case you will feel that well i'm not sure so why don't you take a look at all these numbers for a moment pause the video
#c5	you;the meaning average precision;the mean average precisions;both experiments;you;point;point four four system V;it;an point;they;you;these exact average positions;different queries;you;these numbers;detail;you;one case;you;you;the conclusion;the average;another case;the other case;you;i;you;a look;all these numbers;a moment;the video
#s6	so if you look at the average the mean average precision we can easily say that well system B is better right so it's after all it's point forward
#c6	you;the average;the mean average precision;we;system B;it;it;point
#s7	and then this is twice as much as point two so that's a better performance
#c7	a better performance
#s8	but if you look at these two experiments i look at the detail results you will see that would be more confident that to say that in the case one in experiment one in this case be cause these numbers seem to be consistently better for system be whereas in experiment two we're not sure becaus looking at some results like this after a system a is better and this is another case system a is better
#c8	you;these two experiments;i;the detail results;you;the case;experiment;this case;these numbers;system;experiment;we;becaus;some results;a system;another case system
#s9	but yet if we look at the only average system bees better so what do you think you know how reliable is our conclusion if we only look at the average now in this case intuitively we feel experiment one is more reliable but how can we quantitatively answer this question and this is why we need to do statistical significance test so the idea of statistical significance test is basically to assess the variance across these different queries if there is a a big variance that means the the results could fluctuate a lot according to different queries then we should believe that unless you have used a lot of queries the results might change if we will use another set of queries
#c9	we;the only average system bees;you;you;our conclusion;we;the average;this case;we;experiment;we;this question;we;statistical significance test;the idea;statistical significance test;the variance;these different queries;a a big variance;the results;a lot;different queries;we;you;a lot;queries;the results;we;another set;queries
#s10	right
#s11	so this is not so if you have C high variance then it's not very reliable so let's look at these results again in the second case so here we show two different ways to compare them one is assigned test where we just look at the sign if system B is better than system we have a plus sign when system is better we have a minus sign etc it is in this case if you see this well there are seven cases we have to have four cases where system B is better but three cases system is better you intuitively this is almost like a random results so if you just take a random sample flip seven coins and if you use plus to denote the head and minus your dinner detail and that could easy to be the results of just randomly flipping these seven coins so the fact that average is larger doesn't tell us anything
#c11	you;C high variance;it;'s;these results;the second case;we;two different ways;them;test;we;the sign;system B;system;we;a plus sign;system;we;a minus sign;it;this case;you;seven cases;we;four cases;system B;three cases system;you;a random results;you;a random sample;seven coins;you;the head;your dinner detail;the results;these seven coins;the fact;us;anything
#s12	and we can reliability conclude that and this can be quantitatively measured by P value and that basically means the probability that this result is in fact from random fluctuation in this case probabilities one it means it surely is random fluctuation now in wilcoxon test nonparametric test and we would be not only looking at the signs will be also looking at the magnitude of the difference but we can do a similar conclusion where you say well it's very likely to be from random so to illustrate this let's think about such a distribution and this is called allow distribution we assume that the mean is zero here let's say we start with something that there's no difference between the two systems but we assume that that be cause of random fluctuations depending on the queries or we might observe a difference so the actual difference might be on the left side here or on the right side here so this curve kind of shows the probability that we will actually observed values that are deviating from zero here now so if we look at this picture then we see that if a differences observed here then the chance is very high that this is in fact a random observation right we can define a reading of likely observation becaus of random fluctuation and this is ninety five percent of all the outcomes and in this info then the observed values may still be from random fluctuation but if you observe a value in this region or a difference on this side then the difference is unlikely from random fluctuation so there's a very small probability that you will observe such a difference just becaus of random fact eration so in that case we can then conclude the difference must be real so system B is indeed better so this is the idea of statistical significance test the takeaway message here is that you have to use many queries to avoid jumping into a conclusion as in this case to say system B is better there are many different ways of doing this statistical signaling task
#c12	we;P value;the probability;this result;fact;random fluctuation;this case;one;it;it;random fluctuation;wilcoxon test nonparametric test;we;the signs;the magnitude;the difference;we;a similar conclusion;you;it;'s;such a distribution;allow distribution;we;the mean;'s;we;something;that there's no difference;the two systems;we;cause;random fluctuations;the queries;we;a difference;the actual difference;the left side;the right side;this curve;the probability;we;values;we;this picture;we;a differences;the chance;fact;we;a reading;likely observation becaus;random fluctuation;ninety five percent;all the outcomes;this info;the observed values;random fluctuation;you;a value;this region;a difference;this side;the difference;random fluctuation;a very small probability;you;such a difference;just becaus;random fact eration;that case;we;the difference;system B;the idea;the takeaway message;you;many queries;a conclusion;this case;system B;many different ways;this statistical signaling task
#s13	so let's talk about the other problem of making judgments and as we said earlier it's very hard to judge all the documents completely unless it's a very small data set so the question is if we can afford a judging all the documents in the collection which is subset should we judge and the solution here is pulling and this is a strategy that has been used in many cases to solve this problem so the idea of pulling is the following we would first choose a diverse set of ranking methods these are text retrieval systems and we hope these methods that can help us nominate likely relevant documents so the goal is to figure out the relevant documents we want to make judgments on relevant logins because those are the mostly useful documents from a user 's perspective
#c13	's;the other problem;judgments;we;it;all the documents;it;a very small data;the question;we;a judging;all the documents;the collection;subset;we;the solution;a strategy;many cases;this problem;the idea;we;a diverse set;ranking methods;text retrieval systems;we;these methods;us;likely relevant documents;the goal;the relevant documents;we;judgments;relevant logins;the mostly useful documents;a user 's perspective
#s14	so then we're going to have each to return top K documents the cake can vary from systems right
#c14	we;top K documents;systems
#s15	but the point is to have asked them to suggest the most likely random in the documents
#c15	the point;them;the documents
#s16	and then we simply combine all these top K sets to form a poor of documents for humor assessors to judge so imagine you have many systems each will return K documents will take the top K documents and we formed the union now of course there are many documents that are duplicated 'cause mini systems might have retrieved the same random in the documents so there will be some duplicate documents there are also unique document that only returned by one system and so the idea of having diverse set of ranking methods is to ensure the PO is broad and can include as many possible relevant documents as possible and then the users would hugh masses would make completely judgments on this data set this poor and there are other unjudged documents are usually just assumed to be non relevant now for the poor is large enough this some machines are OK but if the poor is not very large in this actually has to be we considered and we might use other strategies to deal with them and there are indeed other methods to handle such cases and such a strategy is generally OK for comparing systems that contribute to the poor that means if you participate in contributing to the poor then it's unlikely that it will penalize your system because that operating with documents have all been judging however this is problematical for evaluating a new system that may have not contributed to the pool in this case a new system might be penalized because it might have nominated some relevant documents that have not been judged so those documents might be assumed to be non relevant that's unfair so to summarize the whole part of text retrieval evaluation it's extremely important be cause the problem is the empirical define the problem if we don't rely on users there's no way to tell whether method works better if we have in the property experiment design we might miss guide our research or applications and we might just do our own conclusions and we have seen this in some of our discussion so make sure you get it right for your research or application the main methodology is cream filled the virus and methodology and this is near the main paradigm used in all kinds of empirical evaluation task is not just a search engine marriage map an end DCG are the two main measures that should definitely know about and they are a property for comparing ranking algorithms you will see them often in research papers proceeding at the ten documents is easier to interpret from the users perspective so that's also often useful what's not covered is some other evaluation strategy like AB test where the system would mix to the results of two methods randomly and then will show the mixer results to users of course the users don't see which resulted from which method the users would charge those results or click on those documents in in a search engine application in this case then the search engine can keep track of the click the document and see if one method has contributed more through the click the documents if the user tends to click on one the results from one method then it suggests that that method may may be better so this is the leverage is the real users of a search engine to do evaluation it's called AB test and it's a strategy that's often used by modding search engines the commercial search engines another way to evaluate IR or text retrieval is user studies and we haven't covered that i've put some references here that you can look at if you want to know more about that so there are three additional readings here these are three many books about evaluation and they're all excellent in covering a broader review of information retrieval evaluation and discovered some of the things that we discussed but they also have a lot of others to offer
#c16	we;all these top K sets;documents;humor assessors;you;many systems;K documents;the top K documents;we;the union;course;many documents;mini systems;the same random;the documents;some duplicate documents;unique document;one system;the idea;diverse set;ranking methods;the PO;as many possible relevant documents;the users;masses;completely judgments;this data;other unjudged documents;some machines;we;we;other strategies;them;other methods;such cases;such a strategy;systems;you;it;it;your system;documents;a new system;the pool;this case;a new system;it;some relevant documents;those documents;the whole part;text retrieval evaluation;it;the problem;the empirical define;the problem;we;users;no way;method;we;the property experiment design;we;our research;applications;we;our own conclusions;we;our discussion;you;it;your research;the main methodology;cream;the virus;methodology;the main paradigm;all kinds;empirical evaluation task;just a search engine marriage map;DCG;the two main measures;they;a property;ranking algorithms;you;them;the ten documents;the users perspective;what;some other evaluation strategy;AB test;the system;the results;two methods;the mixer results;users;course;the users;which method;the users;those results;those documents;a search engine application;this case;the search engine;track;the click;the document;one method;the click;the documents;the user;the results;one method;it;that method;the leverage;the real users;a search engine;evaluation;it;AB test;it;a strategy;search engines;the commercial search engines;IR or text retrieval;user studies;we;i;some references;you;you;three additional readings;three many books;evaluation;they;a broader review;information retrieval evaluation;the things;we;they;a lot;others
410	de9f09fc-7cf7-40cf-9073-816e9e1a5300	265
#s1	This lecture is about the text retrieval problem.
#c1	This lecture;the text retrieval problem
#s2	This picture shows our overall plan for lectures.
#c2	This picture;our overall plan;lectures
#s3	In the last lecture we talked about the high level strategies for text access.
#c3	the last lecture;we;the high level strategies;text access
#s4	We talked about push versus pull.
#c4	We;push;pull
#s5	Search engines are the main tools for supporting the pull mode.
#c5	Search engines;the main tools;the pull mode
#s6	Starting from this lecture, we're going to talk about how the search engines work in detail.
#c6	this lecture;we;the search engines;detail
#s7	So first it's about the text retrieval problem.
#c7	it;the text retrieval problem
#s8	We are going to talk about three things in this lecture.
#c8	We;about three things;this lecture
#s9	First, we'll define text retrieval.
#c9	we;text retrieval
#s10	Second, we're going to make a comparison between text retrieval and the related task, database retrieval.
#c10	we;a comparison;text retrieval;the related task;database retrieval
#s11	Finally, we're going to talk about the document selecting versus document ranking as two strategies for responding to a users query.
#c11	we;the document;document;two strategies;a users query
#s12	So what is text retrieval?
#c12	what;text retrieval
#s13	It should be a task that's familiar to most of us because we're using web search engines all the time.
#c13	It;a task;us;we;web search engines
#s14	So text retrieval is basically a task where the system would respond to a user's query with relevant documents.
#c14	text retrieval;a task;the system;a user's query;relevant documents
#s15	Basically, to support the query.
#c15	the query
#s16	As one way to implement the pull mode of information access.
#c16	one way;the pull mode;information access
#s17	So the scenario is the following you have a collection of text documents.
#c17	the scenario;you;a collection;text documents
#s18	These documents could be all the web pages on the web.
#c18	These documents;all the web pages;the web
#s19	Or all the literature articles in the digital library.
#c19	Or all the literature articles;the digital library
#s20	Or maybe all the text files in your computer.
#c20	Or maybe all the text files;your computer
#s21	A user will typically give a query to the system to express information need and then the system would return relevant documents to users.
#c21	A user;a query;the system;information need;the system;relevant documents;users
#s22	Relevant documents refer to those documents that are useful to the user who is in typing the query.
#c22	Relevant documents;those documents;the user;who;the query
#s23	Now this task is often called information retrieval.
#c23	this task;information retrieval
#s24	But literally, information retrieval would broadly include retrieval of other non textual information as well.
#c24	information retrieval;retrieval;other non textual information
#s25	For example audio, video etc.
#c25	example
#s26	It's worth noting that text retrieval is at the core of information retrieval, in the sense that other medias, such as video, can be retrieved by exploiting the companion text data.
#c26	It;text retrieval;the core;information retrieval;the sense;other medias;video;the companion text data
#s27	So for example.
#c27	example
#s28	Current image search engines.
#c28	Current image search engines
#s29	Actually match the users query with the companion text data of the image.
#c29	the users query;the companion text data;the image
#s30	This problem is also called the search problem.
#c30	This problem;the search problem
#s31	And the technology is often called search technology in industry.
#c31	the technology;search technology;industry
#s32	If you have to take a course in databases.
#c32	you;a course;databases
#s33	It will be useful to pause the lecture at this point.
#c33	It;the lecture;this point
#s34	And think about.
#s35	The differences between text retrieval and database retrieval.
#c35	The differences;text retrieval and database retrieval
#s36	Are these two tasks are similar in many ways?
#c36	these two tasks;many ways
#s37	But there are some important differences.
#c37	some important differences
#s38	So spend a moment to think about the differences between the two.
#c38	a moment;the differences
#s39	Think about the data and information managed by search engine versus those that are managed by a database system.
#c39	the data;information;search engine;a database system
#s40	Think about the difference between the queries that you typically specify for a database system.
#c40	the difference;the queries;you;a database system
#s41	Versus the queries that are typed in by users on the search engine.
#c41	the queries;users;the search engine
#s42	And then finally, think about the answers.
#c42	the answers
#s43	What's the difference between the two?
#c43	What;the difference
#s44	OK, so if we think about the information or data managed by the two systems, we will see that.
#c44	we;the information;data;the two systems;we
#s45	In text retrieval, the data is unstructured free text, but in databases.
#c45	text retrieval;the data;unstructured free text;databases
#s46	They are structured data where there is a clear definer schema to tell you.
#c46	They;structured data;a clear definer schema;you
#s47	This column is the names of people in that column is ages, etc.
#c47	This column;the names;people;that column;ages
#s48	In unstructured text it's not obvious what are the names of people mentioned in the text.
#c48	unstructured text;it;what;the names;people;the text
#s49	Because of this difference, we can also see that text information tends to be more ambiguous, and we talked about that in the natural language processing lecture.
#c49	this difference;we;text information;we;the natural language processing lecture
#s50	Whereas in databases that data tended to have well defined the semantics.
#c50	databases;data;the semantics
#s51	There is also important difference in the queries.
#c51	important difference;the queries
#s52	And this is partly due to the difference in the information.
#c52	the difference;the information
#s53	Or data.
#c53	Or data
#s54	So text queries tend to be ambiguous.
#c54	text queries
#s55	Where as in database search.
#c55	database search
#s56	The queries are typically well defined.
#c56	The queries
#s57	Think about the SQL query that would clearly specify what records to be returned so it has very well defined semantics.
#c57	the SQL query;what records;it
#s58	Keyword queries or natural language queries tend to be incomplete also.
#c58	Keyword;natural language queries
#s59	In that it doesn't really fully specify what document should be retrieved.
#c59	it;what document
#s60	Where as in the database search.
#c60	the database search
#s61	The SQL query can be regarded as a computer specification for what should be returned.
#c61	The SQL query;a computer specification;what
#s62	And because of these differences, the answers will be also different.
#c62	these differences;the answers
#s63	In the case of texy retrieval, we're looking for relevant documents.
#c63	the case;texy retrieval;we;relevant documents
#s64	In the database search we are retrieving records or match records with.
#c64	the database search;we;records;match records
#s65	The SQL query more precisely.
#c65	The SQL query
#s66	Now, in the case of tax retrieval, what should be the right answers to a query is not very well specified as we just discussed.
#c66	the case;tax retrieval;what;the right answers;a query;we
#s67	So it's unclear what should be the right answers to a query, and this has very important consequences and that is text retrieval is an empirically defined problem.
#c67	it;what;the right answers;a query;very important consequences;text retrieval;an empirically defined problem
#s68	So this is.
#s69	A problem because.
#c69	A problem
#s70	If it's empirically defined.
#c70	it
#s71	Then we cannot mathematically prove one method is better than another method.
#c71	we;one method;another method
#s72	That also means we must rely on empirical evaluation involving users.
#c72	we;empirical evaluation;users
#s73	To know which method works better.
#c73	which method
#s74	And that's why we have a lecture, actually more than one lectures to cover the issue of evaluation.
#c74	we;a lecture;actually more than one lectures;the issue;evaluation
#s75	Because this is a very important topic for search engines.
#c75	a very important topic;search engines
#s76	Without knowing how to evaluate the algorithm appropriately, there's no way to tell whether we have got a better algorithm or whether one system is better than another.
#c76	the algorithm;no way;we;a better algorithm;one system
#s77	So now let's look at the problem in a formal way.
#c77	's;the problem;a formal way
#s78	So this slide shows a formal formulation of the text retrieval problem.
#c78	this slide;a formal formulation;the text retrieval problem
#s79	First, we have our vocabulary set.
#c79	we;our vocabulary set
#s80	Which is just a set of words in a language.
#c80	just a set;words;a language
#s81	Now here.
#s82	We are considering just one language, but in reality on the web there might be multiple natural languages.
#c82	We;just one language;reality;the web;multiple natural languages
#s83	We have text data in all kinds of languages.
#c83	We;text data;all kinds;languages
#s84	But here for simplicity, we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language.
#c84	simplicity;we;one kind;language;the techniques;data;multiple languages;the techniques;documents;one language
#s85	Although there is important difference, the principles and methods are.
#c85	important difference;the principles;methods
#s86	very similar.
#s87	Next we have the query, which is a sequence of words.
#c87	we;the query;a sequence;words
#s88	And so here.
#s89	You can see.
#c89	You
#s90	The. Query.
#c90	The. Query
#s91	Is defined as a sequence of words.
#c91	a sequence;words
#s92	Each q sub I is a word in the vocabulary.
#c92	I;a word;the vocabulary
#s93	A document is defined in the same way, so it's also a sequence of words and here d sub ij.
#c93	A document;the same way;it;a sequence;words;sub ij
#s94	is also a word in the vocabulary.
#c94	a word;the vocabulary
#s95	Now typically the documents are much longer than queries.
#c95	the documents;queries
#s96	But there are also cases where.
#c96	cases
#s97	The documents may be very short.
#c97	The documents
#s98	So you can think about what might be an example of that case.
#c98	you;what;an example;that case
#s99	I hope you can think of Twitter search right tweets are very short.
#c99	I;you;Twitter search right tweets
#s100	But in general, documents are longer than the queries.
#c100	documents;the queries
#s101	Now then we have a collection of documents.
#c101	we;a collection;documents
#s102	And this collection can be very large, so think about the web.
#c102	this collection;the web
#s103	It could could be very large.
#c103	It
#s104	And then the goal of text retrieval is to find the set of relevant documents which we denoted by R of Q because it depends on the query and this is in general a subset of all the documents in the collection.
#c104	the goal;text retrieval;the set;relevant documents;we;R;Q;it;the query;a subset;all the documents;the collection
#s105	Unfortunately, this set of relevant documents is generally unknown.
#c105	this set;relevant documents
#s106	And user dependent in the sense that for the same query typed in by different users.
#c106	And user;the sense;the same query;different users
#s107	The expected relevant documents may be different.
#c107	The expected relevant documents
#s108	The query given to us by the user is only a hint on which document should be in this set.
#c108	The query;us;the user;only a hint;document;this set
#s109	And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search.
#c109	the user;what;this set;the case;web search
#s110	Where collection is so large the user doesn't have complete knowledge about the whole collection.
#c110	collection;the user;complete knowledge;the whole collection
#s111	So the best a search system can do.
#c111	a search system
#s112	Is to compute an approximation of this relevant document.
#c112	an approximation;this relevant document
#s113	set so we denote by r prime of Q.
#c113	we;r prime;Q.
#s114	So formally, we can see the task is to compute this r prime of Q, an approximation of the relevant documents.
#c114	we;the task;this r prime;Q;an approximation;the relevant documents
#s115	So how can we do that?
#c115	we
#s116	Imagine if you are now asked to write a program to do this.
#c116	you;a program
#s117	What would you do now?
#c117	What;you
#s118	Think for a moment.
#c118	a moment
#s119	Right, so these are your input.
#c119	your input
#s120	the query the documents.
#c120	the query;the documents
#s121	And then you are to compute the answers to this query.
#c121	you;the answers;this query
#s122	Which is a set of documents that would be useful to the user.
#c122	a set;documents;the user
#s123	So how would you solve the problem?
#c123	you;the problem
#s124	Now, in general.
#s125	There are two strategies that we can use.
#c125	two strategies;we
#s126	The first strategies will do document selection and that is we're going to have a binary classification function or binary classifier.
#c126	The first strategies;document selection;we;a binary classification function;binary classifier
#s127	That's a function that would take a document and query as input.
#c127	a function;a document;query
#s128	And then give a zero or one as output to indicate whether this document is relevant to the query or not.
#c128	output;this document;the query
#s129	So in this case you can see the document.
#c129	this case;you;the document
#s130	The relevant document in the set is defined as follows, it basically.
#c130	The relevant document;the set
#s131	All the documents that.
#c131	All the documents
#s132	Have a value of 1 by this function.
#c132	a value;this function
#s133	And so in this case you can see the system must decide if a document is relevant or not.
#c133	this case;you;the system;a document
#s134	Basically it has to say whether it's one or zero.
#c134	it;it
#s135	And this is called absolute relevance.
#c135	absolute relevance
#s136	Basically it needs to know exactly whether it's going to be useful to the user.
#c136	it;it;the user
#s137	Alternatively, there's another strategy called document ranking.
#c137	another strategy;document ranking
#s138	Now, in this case the system is not going to make a call whether a document is relevant or not, but rather the system is going to use the real value function F here.
#c138	this case;the system;a call;a document;the system;the real value function;F
#s139	That would simply give us a value that would indicate which document is more likely relevant.
#c139	us;a value;which document
#s140	So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant.
#c140	it;a call;this document;it;which document
#s141	So this function then can be used to rank the documents.
#c141	this function;the documents
#s142	And then we're going to let the user decide where to stop when the user looks at the documents.
#c142	we;the user;the user;the documents
#s143	So we have a threshold.
#c143	we;a threshold
#s144	See down here.
#s145	To determine what documents should be in this approximation, set.
#c145	what documents;this approximation
#s146	And we can assume that all the documents that are ranked above this threshold are in this set.
#c146	we;all the documents;this threshold;this set
#s147	Be cause in effect, these are the documents that we deliver to the user.
#c147	cause;effect;the documents;we;the user
#s148	And theta is a cut off determined by the user.
#c148	theta;a cut;the user
#s149	So here we've got some collaboration from the user in some sense, because we don't really make a cut off and the user kind of helped the system make a cut off.
#c149	we;some collaboration;the user;some sense;we;a cut;the user;the system;a cut
#s150	So in this case the system only needs to decide if one document is more likely relevant than another, and that is it only needs to determine relative relevance.
#c150	this case;the system;one document;it;relative relevance
#s151	As opposed to absolute relevance.
#c151	absolute relevance
#s152	Now you can probably already sense that.
#c152	you
#s153	Relevant relative relevance would be easier to determine their absolute relevance because in the first case, we have to say exactly whether a document is relevant or not.
#c153	Relevant relative relevance;their absolute relevance;the first case;we;a document
#s154	And it turns out that ranking is indeed generally preferred to document selection.
#c154	it;ranking;selection
#s155	So let's look at this.
#c155	's
#s156	These two strategies in more detail.
#c156	These two strategies;more detail
#s157	So this picture shows how it works.
#c157	this picture;it
#s158	So on the left side we see these documents and we use the pluses.
#c158	the left side;we;these documents;we;the pluses
#s159	To indicate the relevant documents so we can see the true relevant documents here.
#c159	the relevant documents;we;the true relevant documents
#s160	This is.
#s161	This set of two random documents consist of these pluses these documents.
#c161	This set;two random documents;these pluses
#s162	And with the document selection.
#c162	the document selection
#s163	Functioning we are going to do basically classify them into two groups, relevant documents and non relevant ones.
#c163	Functioning;we;them;two groups;relevant documents;non relevant ones
#s164	Of course the classifier will not be perfect so it will make mistakes.
#c164	the classifier;it;mistakes
#s165	So here we can see in the approximation of the relevant documents we have got some non relevant documents.
#c165	we;the approximation;the relevant documents;we;some non relevant documents
#s166	And similarly there is a relevant document that's miss classified as non relevant.
#c166	a relevant document;miss
#s167	In the case of document ranking, we can see the system seems like simply ranks all the documents in the descending order of the scores.
#c167	the case;document ranking;we;the system;all the documents;the descending order;the scores
#s168	And then we're going to let the users stop wherever the user wants to stop.
#c168	we;the users;the user
#s169	So if a user wants to examine more documents, then the user would go down the list to examine more and stop at the lower position.
#c169	a user;more documents;the user;the list;the lower position
#s170	But if the user only wants to read a few relevant documents, the user might stop at the top position.
#c170	the user;a few relevant documents;the user;the top position
#s171	So in this case the user stops at d4  so in effect we have delivered these four documents.
#c171	this case;the user;effect;we;these four documents
#s172	To our user.
#c172	our user
#s173	So as I said, ranking is generally preferred.
#c173	I
#s174	An one of the reasons is because the classifier.
#c174	the reasons;the classifier
#s175	In the case of document selection is unlikely accurate.
#c175	the case;document selection
#s176	Why?
#s177	Because the only clue is usually the query, but query may not be accurate in the sense that it could be overly constrained.
#c177	the only clue;the query;query;the sense;it
#s178	For example, you might expect the relevant documents to talk about all these.
#c178	example;you;the relevant documents
#s179	Topics you by using specific vocabulary and as a result.
#c179	Topics;you;specific vocabulary;a result
#s180	You might.
#c180	You
#s181	Match no random documents because in the collection no others have discussed the topic using these vocabularies.
#c181	no random documents;the collection;no others;the topic;these vocabularies
#s182	So in this case you will see there is this problem of.
#c182	this case;you;this problem
#s183	No relevant documents to return in the case of overly constraint query.
#c183	No relevant documents;the case;overly constraint query
#s184	On the other hand, if the query is underconstrained, for example.
#c184	the other hand;the query;example
#s185	If the query does not have sufficient discriminating words to find the relevant documents, you may actually end up having over delivery.
#c185	the query;sufficient discriminating words;the relevant documents;you;delivery
#s186	And this is when you thought these words might be sufficient to help you find the relevant documents.
#c186	you;these words;you;the relevant documents
#s187	But it turns out that they are not sufficient, and there are many distraction documents using similar words.
#c187	it;they;many distraction documents;similar words
#s188	Right so.
#s189	This is the case of over delivery.
#c189	the case;delivery
#s190	Unfortunately, it's very hard to find the right position between these two extremes.
#c190	it;the right position;these two extremes
#s191	Why cause when the user is looking for the information?
#c191	the user;the information
#s192	In general, the user does not have a good knowledge about the information to be found.
#c192	the user;a good knowledge;the information
#s193	And in that case the user does not have a good knowledge about what.
#c193	that case;the user;a good knowledge;what
#s194	Vocabularies will be used in those random documents.
#c194	Vocabularies;those random documents
#s195	So it's very hard for user to pre specify the right level of constraints.
#c195	it;user;the right level;constraints
#s196	Even if the classifier is accurate.
#c196	the classifier
#s197	We also still want to rank these red documents because.
#c197	We;these red documents
#s198	They are generally not equally relevant.
#c198	They
#s199	Relevance is often a matter of degree.
#c199	Relevance;a matter;degree
#s200	So we must prioritize.
#c200	we
#s201	These documents for a user to examine.
#c201	These documents;a user
#s202	And this note that this prioritization is very important.
#c202	And this note;this prioritization
#s203	Because a user cannot digest all the contents at once, the user general would have to look at each document sequentially.
#c203	a user;all the contents;the user general;each document
#s204	And therefore it would make sense to feed the users with the most relevant documents.
#c204	it;sense;the users;the most relevant documents
#s205	And that's what ranking is doing.
#c205	what ranking
#s206	So For these reasons, ranking is generally preferred now.
#c206	these reasons
#s207	This preference also has a theoretical justification, and this is given by the probability ranking principle.
#c207	This preference;a theoretical justification;the probability ranking principle
#s208	In the end of this lecture there is reference for this.
#c208	the end;this lecture;reference
#s209	This principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions.
#c209	This principle;a ranked list;documents;order;probability;a document;the query;the optimal strategy;the following two assumptions
#s210	First, the utility of the document to a user is independent of the utility of any other document.
#c210	the utility;the document;a user;the utility;any other document
#s211	2nd, a user would be assumed to browse the results sequentially.
#c211	a user;the results
#s212	Now it's easy to understand why these two assumptions are needed in order to justify for the ranking strategy.
#c212	it;these two assumptions;order;the ranking strategy
#s213	because if the documents are independent then we can evaluate the utility of each document separately.
#c213	the documents;we;the utility;each document
#s214	And this would allow us to compute the score for each document independently, and then we're going to rank these documents based on those scores.
#c214	us;the score;each document;we;these documents;those scores
#s215	The second assumption is to say that the user would indeed follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially, then obviously the ordering would not be optimal.
#c215	The second assumption;the user;the ranked list;the user;the ranked list;the documents;the ordering
#s216	So under these two assumptions.
#c216	these two assumptions
#s217	We can theoretically justify the rankings strategy is in fact the best you could do.
#c217	We;the rankings strategy;fact;you
#s218	Now I've put one question here.
#c218	I;one question
#s219	Do these two assumptions hold?
#c219	these two assumptions
#s220	Now I suggest you to pause the lecture for a moment to think about this.
#c220	I;you;the lecture;a moment
#s221	Now, can you think of some examples?
#c221	you;some examples
#s222	That would suggest.
#s223	These Assumptions aren't necessarily true.
#c223	These Assumptions
#s224	Now, if you think for a moment you may realize.
#c224	you;a moment;you
#s225	None of the assumptions is actually true.
#c225	None;the assumptions
#s226	For example, in the case of independence assumption.
#c226	example;the case;independence assumption
#s227	We might have identical documents that have similar content or exactly the same content.
#c227	We;identical documents;similar content;exactly the same content
#s228	If you look at each of them alone.
#c228	you;them
#s229	Each is relevant.
#s230	But if the user has already seen one of them.
#c230	the user;them
#s231	We assume it's generally not very useful for the user to see another similar or duplicated one.
#c231	We;it;the user
#s232	So clearly the utility of document is dependent on other documents that user has seen.
#c232	the utility;document;other documents;user
#s233	In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together, they provide answer to the users question.
#c233	some other cases;you;a scenario;the user;three particular documents;they;answer
#s234	So this is a collective relevance and that also suggests that the value of the document might depend on other documents.
#c234	a collective relevance;the value;the document;other documents
#s235	Sequential browse in general would make sense if you have a ranked list there.
#c235	Sequential browse;sense;you;a ranked list
#s236	But even if you have a ranked list.
#c236	you;a ranked list
#s237	There was evidence showing that users don't always just go strictly sequentially through the entire list.
#c237	evidence;users;the entire list
#s238	There sometimes would look at the bottom, for example, or skip some.
#c238	the bottom;example
#s239	And if you think about the more complicated interface that we could possibly use, like 2 dimensional interface where you can put the additional information on the screen, then seek when you're browsing is a very restrictive assumption.
#c239	you;the more complicated interface;we;2 dimensional interface;you;the additional information;the screen;you;a very restrictive assumption
#s240	So the point here is that.
#c240	the point
#s241	None of these assumptions is really true.
#c241	None;these assumptions
#s242	But nevertheless.
#s243	The probability ranking principle established some solid foundation for ranking as a primary task for search engines.
#c243	The probability ranking principle;some solid foundation;ranking;a primary task;search engines
#s244	This has actually been the basis for a lot of research work in information retrieval, and many algorithms have been designed based on this assumption Despite that.
#c244	the basis;a lot;research work;information retrieval;many algorithms;this assumption
#s245	The assumptions aren't necessarilly true and we can address this problem by doing post processing of a ranked list, for example to remove redundancy.
#c245	The assumptions;we;this problem;post processing;a ranked list;example;redundancy
#s246	So to summarize this lecture, the main points.
#c246	this lecture
#s247	That you can take away are the following.
#c247	you;the following
#s248	First text retrieval is an empirical defined problem.
#c248	First text retrieval;an empirical defined problem
#s249	And that means which algorithm is better must be judged by the users.
#c249	algorithm;the users
#s250	Second, document ranking is generally preferred.
#c250	document ranking
#s251	And this will help users prioritize examination of search results.
#c251	users;examination;search results
#s252	And this is also to bypass the difficulty in determining absolute relevance.
#c252	the difficulty;absolute relevance
#s253	Because we can get some help from users in determining where to make the cut off.
#c253	we;some help;users;the cut
#s254	It's more flexible.
#c254	It
#s255	So this further suggests that the main technical challenge in designing search engine is redesigned effective ranking function.
#c255	the main technical challenge;designing search engine;effective ranking function
#s256	In other words, we need to define what is the value of this function F on.
#c256	other words;we;what;the value;this function;F
#s257	The query and document pair.
#c257	The query;document pair
#s258	The whole design.
#c258	The whole design
#s259	Such a function is the main topic in the following lectures.
#c259	Such a function;the main topic;the following lectures
#s260	There are two suggested additional readings.
#c260	two suggested additional readings
#s261	The first is the classic paper on probability ranking principle.
#c261	the classic paper;probability ranking principle
#s262	The second is a must read for anyone doing research information travel.
#c262	anyone;research information travel
#s263	It's a classic IR book.
#c263	It;a classic IR book
#s264	Which has excellent coverage of the main research results in early days.
#c264	excellent coverage;the main research results;early days
#s265	Up to the time when the book was written, Chapter 6 of this book has in depth discussion of the probability ranking principle and probabilistic retrieval models in general.
#c265	the time;the book;Chapter;this book;depth discussion;the probability;ranking principle and probabilistic retrieval models
410	e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	87
#s1	This lecture is a continued discussion of probabilistic topic models.
#c1	This lecture;a continued discussion;probabilistic topic models
#s2	In this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document.
#c2	this lecture;we;probabilistic models;we;a very simple case;we;one topic;one document
#s3	So in this simple setup we are interested in analyzing one document and trying to discover just one topic.
#c3	this simple setup;we;one document;just one topic
#s4	So this is the simplest case of topic modeling.
#c4	the simplest case;topic modeling
#s5	The input now no longer has K, which is the number of topics because we know there is only one topic.
#c5	The input;K;the number;topics;we;only one topic
#s6	And the collection has only one document also.
#c6	the collection;only one document
#s7	In the output we also no longer have coverage because we assumed that the document covers this topic 100%.
#c7	the output;we;coverage;we;the document;this topic
#s8	So the main goal is just to discover the word probabilities for this single topic, as shown here.
#c8	the main goal;the word probabilities;this single topic
#s9	As always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the data or data representation.
#c9	we;a generative model;such a problem;we;what kind;data;we;what perspective;we;the data or data representation
#s10	And then we're going to design a specific model for the generation of the data from our perspective.
#c10	we;a specific model;the generation;the data;our perspective
#s11	Where our perspective just means we want to take a particular angle of looking at the data so that the model would have the right parameters for discovering the knowledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model.
#c11	our perspective;we;a particular angle;the data;the model;the right parameters;the knowledge;we;we;the likelihood function;the library function;a data point;this model
#s12	And the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm.
#c12	the likelihood function;some parameters;the function;we;those parameters;example;the likelihood;maximum likelihood estimator;these estimated parameters;the output;the mining algorithm
#s13	Which means we'll take the estimated parameters as a knowledge that we discover from the text.
#c13	we;the estimated parameters;a knowledge;we;the text
#s14	So let's look at these steps for this very simple case.
#c14	's;these steps;this very simple case
#s15	Later, we'll look at this procedure for some more complicated cases.
#c15	we;this procedure;some more complicated cases
#s16	So our data in this case is just the document which is a sequence of words.
#c16	our data;this case;just the document;a sequence;words
#s17	Each word here is denoted by X sub
#c17	Each word;X sub
#s18	I.
#c18	I.
#s19	Our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal.
#c19	Our model;a unigram language model;a word distribution;we;a topic;our goal
#s20	So we will have as many parameters as many words in our vocabulary, in this case M.
#c20	we;as many parameters;many words;our vocabulary;this case
#s21	And for convenience we're going to use theta sub I to denote the probability of word W sub I.
#c21	convenience;we;theta sub;I;the probability;word;W sub;I.
#s22	And obviously these thetas of i's would sum to one.
#c22	these thetas;i
#s23	Now, what does the likelihood function look like?
#c23	what;the likelihood function
#s24	This is just the probability of generating this whole document given such a model.
#c24	just the probability;this whole document;such a model
#s25	Because we assume the independence in generating each word, so the probability of the word the document would be just a product of the probability of each word.
#c25	we;the independence;each word;the probability;the word;the document;just a product;the probability;each word
#s26	And since some word might have repeated occurrences, so we can also rewrite this product in a different form.
#c26	some word;occurrences;we;this product;a different form
#s27	So in this line we have rewritten the formula into a product over all the unique words in the vocabulary, W sub one through the W sub M.
#c27	this line;we;the formula;a product;all the unique words;the vocabulary;the W sub;M.
#s28	Now this is different from the previous line where the product is over different positions of words in the document.
#c28	the previous line;the product;different positions;words;the document
#s29	Now when we do this transformation, we then would need to introduce a count function here.
#c29	we;this transformation;we;a count function
#s30	This denotes the count of word one in document.
#c30	the count;word;document
#s31	And similarly, this is the count of words of M in the document.
#c31	the count;words;M;the document
#s32	Because these words might have repeated occurrences.
#c32	these words;occurrences
#s33	You can also see if a word did not occur in the document, it would have a zero count and therefore that corresponding term will disappear.
#c33	You;a word;the document;it;a zero count;corresponding term
#s34	So this is a very useful form of writing down the likelihood function that we will often use later.
#c34	a very useful form;the likelihood function;we
#s35	So I want you to pay attention to this.
#c35	I;you;attention
#s36	Just get familiar with this notation.
#c36	this notation
#s37	It's just to change the product over all the different words in the vocabulary.
#c37	It;the product;all the different words;the vocabulary
#s38	So in the end, of course we'll use theta sub I to express this likelihood function and it would look like this.
#c38	the end;course;we;theta sub;I;this likelihood function;it
#s39	Next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function.
#c39	we;the theta values;probabilities;these words;this likelihood function
#s40	So now let's take a look at the maximum likelihood estimate problem more closely.
#c40	's;a look;the maximum likelihood estimate problem
#s41	This line is copied from the previous slide.
#c41	This line;the previous slide
#s42	It's just our likelihood function.
#c42	It;our likelihood function
#s43	So our goal is to maximize this likelihood function.
#c43	our goal;this likelihood function
#s44	We will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, because after the logarithm transformation, our function will become a sum instead of a product.
#c44	We;it;the log likelihood;the original likelihood;mathematical convenience;the logarithm transformation;our function;a sum;a product
#s45	And we also have constraints over these probabilities.
#c45	we;constraints;these probabilities
#s46	The sum makes it easier to take derivative, which is often needed for finding the optimal solution of this function.
#c46	The sum;it;derivative;the optimal solution;this function
#s47	So please take a look at this sum again here and this is a form of function that you often see later also in more general topic models.
#c47	a look;this sum;a form;function;you;more general topic models
#s48	So it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document.
#c48	it;a sum;all the words;the vocabulary;the sum;a count;words;the document
#s49	And this is multiplied by the logarithm of the probability.
#c49	the logarithm;the probability
#s50	So let's see how we can solve this problem.
#c50	's;we;this problem
#s51	Now at this point the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem.
#c51	this point;the problem;a mathematical problem;we;the optimal solution;a constrained maximization problem
#s52	The objective function is the likelihood function, and the constraint is that all these probabilities must sum to one.
#c52	The objective function;the likelihood function;the constraint;all these probabilities
#s53	So one way to solve the problem is to use Lagrange multiplier approach.
#c53	one way;the problem;Lagrange multiplier approach
#s54	Now this content is beyond the scope of this course.
#c54	this content;the scope;this course
#s55	But since Lagrange multiplier is very useful approach, I also would like to just give a brief introduction to this for those of you who are interested.
#c55	Lagrange;very useful approach;I;a brief introduction;you;who
#s56	So in this approach we will construct a Lagrange function here.
#c56	this approach;we;a Lagrange function
#s57	And this function would combine our objective function with another term that encodes our constraints.
#c57	this function;our objective function;another term;our constraints
#s58	And we introduce Lagrange multiplier here, Lambda.
#c58	we;Lagrange
#s59	So it's additional parameter.
#c59	it;additional parameter
#s60	Now the idea of this approach is to just turn the constrained optimization into, in some sense, unconstrained optimizing problem.
#c60	the idea;this approach;the constrained optimization;some sense;problem
#s61	So now we're just interested in optimizing this Lagrange function.
#c61	we;this Lagrange function
#s62	As you may recall from calculus, an optimal point would be achieved when the derivative is set to 0.
#c62	you;calculus;an optimal point;the derivative
#s63	This is a necessary condition.
#c63	a necessary condition
#s64	It's not sufficient though, so.
#c64	It
#s65	If we do that, you will see the partial derivative with respect to theta i here is equal to this.
#c65	we;you;the partial derivative;respect;theta;i
#s66	And this part comes from the derivative of the logarithm function.
#c66	this part;the derivative;the logarithm function
#s67	And this Lambda is simply taken from here.
#c67	this Lambda
#s68	And when we set it to zero, we can easily see theta sub i is related to Lambda in this way.
#c68	we;it;we;theta sub;i;Lambda;this way
#s69	Since we know all the theta I's must sum to one, we can plug this into this constraint here, and this will allow us to solve for Lambda.
#c69	we;all the theta;I;we;this constraint;us;Lambda
#s70	And this is just negative sum of all the counts and this further allows us to then solve optimization problem.
#c70	just negative sum;all the counts;us;optimization problem
#s71	Eventually to find the optimal setting for Theta Sub I.
#c71	the optimal setting;Theta Sub I.
#s72	And if you look at this formula, it turns out that it's actually very intuitive because this is just the normalized count of these words by the document length, which is also a sum of all the counts of words in the document.
#c72	you;this formula;it;it;just the normalized count;these words;the document length;a sum;all the counts;words;the document
#s73	So after all this math, after all, we have just obtained something that's very intuitive, and this will be just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here.
#c73	all this math;we;something;just our intuition;we;the theta;as much probability mass;all the observed words
#s74	And you might also notice that this is the general result of maximum likelihood estimator.
#c74	you;the general result;maximum likelihood estimator
#s75	In general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later.
#c75	the estimate;count;it;the counts;a particular way;you
#s76	So this is basically an analytical solution to our optimization problem.
#c76	an analytical solution;our optimization problem
#s77	In general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula.
#c77	the likelihood function;we;the optimization problem;a closed form formula
#s78	Instead, we have to use some numerical algorithms, and we're going to see such cases later also.
#c78	we;some numerical algorithms;we;such cases
#s79	So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document D here, let's imagine this document is a text mining paper.
#c79	you;what;we;we;such a maximum likelihood estimator;one topic;a single document D;'s;this document;a text mining paper
#s80	Now what you might see is something that looks like this.
#c80	what;you;something
#s81	On the top you will see the high probability words tend to be those very common words, often functional words in English, and this will be followed by some content words that really characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be externally mentioned in the document.
#c81	the top;you;the high probability words;those very common words;often functional words;English;some content words;the topic;text;the end;you;various more probabilities;words;the topic;they;the document
#s82	As a topic representation, you will see this is not ideal, right?
#c82	a topic representation;you
#s83	The because of the high probability words are functional words they are not really characterizing the topic.
#c83	the high probability words;functional words;they;the topic
#s84	So one question is how can we get rid of such common words?
#c84	one question;we;such common words
#s85	"
#s86	Now this is a topic of the next lecture.
#c86	a topic;the next lecture
#s87	We're going to talk about how to use probabilistic models to somehow get rid of these common words.
#c87	We;probabilistic models;these common words
410	ec28a239-f43c-4fb5-bda0-920395666f51	47
#s1	this letter is about inverted index construction in this lecture we will continue the discussion of the system implementation in particular we're going to discuss how to construct the inverted index the construction of the inverted index is actually very easy if the data set is very small it's very easy to construct dictionary and then install the postings in a fire the problem is that when our data is not able to fit to the memory then we have to use some special method which will deal with it an unfortunately in most retrieval applications that are set would be large and they generate cannot be loaded into memory at once and there are many approaches to solving the problem and sorting based method is quite common it works in four steps assume here first you collect the local term ID document ID and frequency tables basically you will look counterterms in a small set of documents and then once you crack those counts you have sought those counts based on terms so that you build a local a partial inverted index and these are called rounds and then you write them into temporary file on the disk and then you merge in step three you would do pairwise merging of these runs until you eventually merge all the runs we generate a single inverted index so this is an illustration of this method on the left you see some documents and on the right we have shown a complex him and a document ID lacks these lessons are too map a stream based repetitions of document IDS or terms into integer representations or adam that back from integers to the screen rotation and the reason why we are interested in using integers represent these i DS is becaus integers are often easier to handle for example integers can be used as index for array and there are also easy to compress so this is one reason why we tended to map these strings into integers so that so that we don't have to carry these strings around
#c1	this letter;inverted index construction;this lecture;we;the discussion;the system implementation;we;the inverted index;the construction;the inverted index;the data;it;dictionary;the postings;a fire;the problem;our data;the memory;we;some special method;it;most retrieval applications;they;memory;many approaches;the problem;based method;it;four steps;you;the local term;ID document ID;frequency tables;you;counterterms;a small set;documents;you;those counts;you;those counts;terms;you;a local a partial inverted index;rounds;you;them;temporary file;the disk;you;step;you;pairwise merging;these runs;you;all the runs;we;a single inverted index;an illustration;this method;the left;you;some documents;the right;we;a complex him;a document;ID;these lessons;a stream based repetitions;document IDS;terms;integer representations;adam;integers;the screen rotation;we;integers;these i DS;becaus integers;example;integers;index;array;one reason;we;these strings;integers;we;these strings
#s2	so how does this approach work well it's very simple look at the scan these documents sequentially and then pause the document an account of the frequencies of terms an in this stage we generally sort the frequencies by document IDS becaus we process each document sequentially so we first encounter all the terms in the first document therefore the document i DS one 's in this case and so and this would be followed by document IDS tool tools an their natural resource in this order just becaus we process the data in the sequential order at some point we will run out of memory
#c2	this approach;it;very simple look;the scan;these documents;the document;an account;the frequencies;terms;this stage;we;the frequencies;document IDS becaus;we;each document;we;all the terms;the first document;i;DS one;this case;document IDS tool tools;an their natural resource;this order;we;the data;the sequential order;some point;we;memory
#s3	and then we would have to write them into the disk before we do that we're going to sort them just to use whatever memory we have we can sort them
#c3	we;them;the disk;we;we;them;whatever memory;we;we;them
#s4	and then this time we're going to sort based on term items noted out here we're using this term i DS as a peter thought so all the entries that share the same term would be grouped together in this case we can see all the or the IDS of document that match tom one with the groups together
#c4	we;term items;we;this term;i;all the entries;the same term;this case;we;document;the groups
#s5	and we're going to write this into the disk as a temporary file and that would allow them to use the memory the plus as an extra batch of documents and we're going to do that for all the documents
#c5	we;the disk;a temporary file;them;the memory;an extra batch;documents;we;all the documents
#s6	so we're going to write a lot of temporary files into the disk
#c6	we;a lot;temporary files;the disk
#s7	and then the next stages blue merge assault basically window merge them and then sort them eventually we will get single inverted index whether their entries are sorted based on term IDS and on the top we can see these are the order entries for the documents that match term ID one so this is basically how we can do the construction of inverted index even though the data cannot be or loaded into the memory now we mentioned earlier that the cost of postings are very large it's desirable to compress them so let's now talk a little bit about how we compress inverted index well the idea of compression in general is the leverage skewer distributions of values and we generally have to use variable length encoding instead of the fixed lens encoding as we use in by default in a programming language like C plus plus and so how can we leverage the skewed distributions of values to compress these values well in general would use fewer bits to encode those frequent awards at the cost of using longer bits to encode those rail values so in our case let's think about how we can compress the TF term frequency if you can picture what the inverted index would look like muc in postings there are a lot of confidence is those are the frequencies of terms in all those documents now we if you think about what kind of values are most frequently there your program will be able to guess that a small numbers tend to occur far more frequently than large numbers why well think about the distribution of words and this isn't due to the zipf 's law and many words occur just rarely so we see a lot of small numbers therefore we can use a few orbits for the small but highly frequent integers and at the cost of using more bits for large integers this is a trade off of course if the values are distributed in uniform then this one will save us any space but be cause we tend to see many small values they are very frequent we can save on average even though sometimes when we see a large number we have to use a lot of bits what about the document i DS that we also saw in postings
#c7	the next stages;blue merge assault;basically window;them;them;we;single inverted index;their entries;term IDS;the top;we;the order entries;the documents;term ID;we;the construction;inverted index;the data;the memory;we;the cost;postings;it;them;'s;we;inverted index;the idea;compression;the leverage skewer distributions;values;we;variable length encoding;the fixed lens encoding;we;default;a programming language;C;we;the skewed distributions;values;these values;fewer bits;those frequent awards;the cost;longer bits;those rail values;our case;'s;we;the TF term frequency;you;what;the inverted index;muc;postings;a lot;confidence;the frequencies;terms;all those documents;you;what kind;values;your program;a small numbers;large numbers;the distribution;words;the zipf 's law;many words;we;a lot;small numbers;we;a few orbits;the small but highly frequent integers;the cost;more bits;large integers;a trade;course;the values;uniform;this one;us;any space;we;many small values;they;we;we;a large number;we;a lot;bits;the document;i;we;postings
#s8	well they are not distributed in the skilled away
#c8	they
#s9	right
#s10	so how can we deal with that where it turns out that really use a trick called the gap and that is restore the difference of these term IDS and we can imagine if a term has matched the many documents then there will be a long list of document IDS
#c10	we;it;a trick;the gap;the difference;these term;IDS;we;a term;the many documents;a long list;document IDS
#s11	so when we take the gap i'm going to take the difference between adjacent document IDS those gaps will be small so again see a lot of small numbers whereas if atoma cutting only a few documents then the gap would be large the large numbers will not be frequent
#c11	we;the gap;i;the difference;adjacent document;IDS;those gaps;a lot;small numbers;atoma;only a few documents;the gap;the large numbers
#s12	so this creates some security distribution that would allow us to compress this values and this is also possible becaus in order to uncover or uncompressed these document IDS we have the sequential process that data because we store the difference an in order to recover the exactly document ID we have to first recover the previous document ID
#c12	some security distribution;us;this values;possible becaus;order;these document;IDS;we;the sequential process;we;the difference;order;the exactly document ID;we;the previous document;ID
#s13	and then we can add the difference to the previous document ID to restore the current document ID now this was possible becaus we only need to have sequential access to those document IDS once we look up a term we fetch all the document i DS that match the term then we sequentially process so it's very natural that's why this trick actually works and there are many different methods for encoding so binary cold is a common user called in just programming language where we use basically fixed lens encoding unary called gamma gold and yellow gold are awful spinedace and there are many other possible in this so let's look at some of them in more detail binary coding is really equal lansing coding that's a property for randomly distributed values the unary coding is variable lansing calling method in this case integer that's at least one would be encoded as X minus one one bit followed by zero so for example three would be encoded as two ones followed by zero whereas five will be encoded as four ones followed by zero etc so now you can imagine how many bits do we have to use for large number like one hundred so how many bits do have to use exactly four number like
#c13	we;the difference;the previous document ID;the current document;ID;possible becaus;we;sequential access;those document;IDS;we;a term;we;all the document;i;the term;we;it;this trick;many different methods;so binary cold;a common user;just programming language;we;basically fixed lens;gamma gold;yellow gold;awful spinedace;'s;them;more detail;binary coding;really equal lansing coding;a property;randomly distributed values;the unary coding;variable lansing calling method;this case;integer;at least one;X;example;two ones;four ones;you;how many bits;we;large number;how many bits;exactly four number
#s14	one hundred well
#c14	one hundred well
#s15	exactly we have to use one hundred bits right so it's the same number of bits as the value of this number
#c15	we;one hundred bits;it;the same number;bits;the value;this number
#s16	so this is very inefficient if you will likely see some large numbers imagine if you occasionally see a number like one thousand you have to use one thousand bits so this only works well if you are absolutely sure that there would be no large numbers mostly very very often you see very small numbers how do you decode it this code since these are variable length encoding methods and you can't just count how many bits
#c16	you;some large numbers;you;a number;you;one thousand bits;you;no large numbers;you;very small numbers;you;it;variable length encoding methods;you
#s17	and then they just stop why are you going to say eight bits or thirty two bit then you will start another code they are variable length
#c17	they;you;eight bits;you;another code;they;variable length
#s18	so you have to rely on some mechanism in this case for unary you can see it's very easy to see the boundary now you can easily see zero with signal the end of encoding
#c18	you;some mechanism;this case;you;it;the boundary;you;signal;the end;encoding
#s19	so you just count how many ones you have seen and here you hit zero you know you have finished one number you will start another number now we just saw that the universe coding is too aggressive in rewarding small numbers if you occasionally you can see very big number it will be a disaster so what about some other less aggressive method well demarco ding is one of them an in this method will can do use unary coding for transform form of the value
#c19	you;how many ones;you;you;you;you;one number;you;another number;we;the universe coding;rewarding small numbers;you;you;very big number;it;a disaster;some other less aggressive method;demarco ding;them;this method;unary coding;transform form;the value
#s20	so it's one plus the flow of log of X
#c20	it;the flow;log;X
#s21	so the magnitude of this value is much lower than the original X
#c21	the magnitude;this value;the original X
#s22	so that's why we can't afford using unary code for that
#c22	we;unary code
#s23	so and so first we have the unary code for coding this log of X and this will be followed by a uniform code or binary code
#c23	we;the unary code;this log;X;a uniform code;binary code
#s24	and this is basically the same uniform code and binary code of the same and we're going to use this code to code the remaining part of the value of X
#c24	the same uniform code;binary code;we;this code;the remaining part;the value;X
#s25	and this is basically precisely X minus one two to the floor of log of X
#c25	precisely X;the floor;log;X
#s26	so the unary called basically called the flow of log of X well add one there i can hear but the remaining part will be using uniform code to actually code difference between the X and this two to the log of X
#c26	the flow;log;X;i;the remaining part;uniform code;difference;the X;the log;X
#s27	and it's easy to show that for this this value this difference we only need to use up to this many bits and the floor of log of X bits an this is easy to understand if the difference is too large then we would have a higher flow of log of X
#c27	it;this value;we;this many bits;the floor;log;the difference;we;a higher flow;log;X
#s28	so here are some examples for example three is encoded as one oh one the first two digits are the unary coder
#c28	some examples;example;one oh one;the first two digits;the unary coder
#s29	so this is before value two like one zero in codes two enumerate coding
#c29	value;codes;two enumerate
#s30	and so that means log of X the flow of log of X is one because we want actually use unary coded to encode one plus the flow of log of X since this is two then we know that the flow of log of X is actually one
#c30	log;X;the flow;log;X;we;the flow;log;X;we;the flow;log;X
#s31	so but three is still larger than to do the one so the difference is one and that one is encoded here at the end so that's why we have one oh one four three of similarly at five the encoded as one one zero followed by zero one an in this case is a unary code encodes
#c31	the one;the difference;that one;the end;we;this case;a unary code encodes
#s32	three
#s33	and so this is the unary code one one zero and so the floor of log of X is two
#c33	the unary code;the floor;log;X
#s34	and that means we're going to compute the difference between five and two to the two
#c34	we;the difference
#s35	and that's one
#s36	and so we now have again one at the end
#c36	we;the end
#s37	but this time we're going to use two bits becaus with this level flow of log of X we could have more numbers five six seven they would all share the same prefix here one one zero so in order to differentiate them we have to use two bits in the end to differentiate them so you can imagine six would be one zero here in the end instead of zero one after one one zero it's also true that the form of a gamma code is always and the first odd number of bits and in the center there is a zero that's the end of the unary code and before that all on the left side of this zero there would be all once an on the right side of this zero it's binary coding or uniform code so how can you decode such a code
#c37	we;two bits becaus;this level flow;log;X;we;more numbers;they;the same prefix;order;them;we;two bits;the end;them;you;the end;it;the form;a gamma code;bits;the center;the end;the unary code;the left side;the right side;it;binary coding;uniform code;you;such a code
#s38	well you again first do you know recording right once you hit zero you know you have got the unary code and this also would tell you how many bits you have to read further to decode the uniform code
#c38	you;you;you;you;you;the unary code;you;how many bits;you;the uniform code
#s39	so this is how you can decode it there michael there is also terrible code that's basically the same as a gamecode except that you replace the unary prefix with the gamma code so that's even less conservative than comma code in terms of rewarding small integers
#c39	you;it;terrible code;a gamecode;you;the unary prefix;the gamma code;comma code;terms;rewarding small integers
#s40	so that means it's OK if you occasionally see a large number it's OK with the error code it's also fine with gamecode
#c40	it;you;a large number;it;the error code;it;gamecode
#s41	it's really a big loss for unary code and they are all operating of course at different degrees of favoring short faring small integers and that also means there would be a property for asserting distribution but none of them is perfect for all distributions and which method works the best would have to depend on the actual distribution in your data set for inverted index compression people have found that gamma coding seems to work well
#c41	it;a big loss;unary code;they;course;different degrees;short faring small integers;a property;distribution;none;them;all distributions;which method;the actual distribution;your data;inverted index compression;people;gamma coding
#s42	so how do i uncompressed invert index
#c42	i
#s43	and we just talked about this first day would decode those encode integers
#c43	we;this first day;those encode integers
#s44	an we just i think discussed how it is called the union recording and camera coding
#c44	an we;i;it;the union recording;camera coding
#s45	so i won't repeat what about the document i DS that might be compressed using the gap
#c45	i;the document;i;the gap
#s46	well we're going to do sequential decoding so suppose the encoded idealist is X one X two X three etc we first the codex one obtain the first document id id one then with the decode X two which is actually the difference between the second ID and the first one so we have to add the decoder value of X two two ID one to recover the value of the the ID at this second position so this is all well you can see the advantage of converting document IDS into integers and that allows us to do this kind of compression
#c46	we;the encoded idealist;X;we;the codex;one;the first document;i;d i;the decode X;the difference;the second ID;we;the decoder value;X two two ID one;the value;the the ID;this second position;you;the advantage;document IDS;integers;us;this kind;compression
#s47	and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position
#c47	we;we;all the documents;we;the document ID;the previous position;you;the document ID;the next position
410	eca11e7a-dae2-480b-9c0e-e1ac7a91dd57	99
#s1	This lecture is the overview of text retrieval methods.
#c1	This lecture;the overview;text retrieval methods
#s2	In the previous lecture we introduced the problem of text retrieval.
#c2	the previous lecture;we;the problem;text retrieval
#s3	We explained that the main problem is to design a ranking function to rank documents for a query in this lecture we will give a overview of different ways of designing this ranking function.
#c3	We;the main problem;a ranking function;documents;a query;this lecture;we;a overview;different ways;this ranking function
#s4	So the problem is the following.
#c4	the problem;the following
#s5	We have a query that has a sequence of words and a document that's also a sequence of words and we hope to define a function F. That can compute a score based on the query and document.
#c5	We;a query;a sequence;words;a document;a sequence;words;we;a function;a score;the query;document
#s6	So the main challenge here is to design a good ranking function that can rank all the relevant documents on top of all the non relevant ones.
#c6	the main challenge;a good ranking function;all the relevant documents;top;all the non relevant ones
#s7	Now clearly this means our function must be able to measure the likelihood that a document d, is relevant to a query Q. That also means we have to have some way to define relevance.
#c7	our function;the likelihood;a document d;a query;we;some way;relevance
#s8	In particular, in order to implement the program to do that, we have to have a computational definition of relevance.
#c8	order;the program;we;a computational definition;relevance
#s9	And we achieve this goal by designing a retrieval model which gives us a formalization of relevance.
#c9	we;this goal;a retrieval model;us;a formalization;relevance
#s10	Now, over many decades, researchers have designed many different kinds of retrieval models.
#c10	many decades;researchers;many different kinds;retrieval models
#s11	And they fall into different categories.
#c11	they;different categories
#s12	1st.
#c12	1st
#s13	One thing many of the models are based on the similarity idea.
#c13	One thing;the models;the similarity idea
#s14	Basically, we assume that if a document is more similar to the query, then another document is then we will say the first document is more relevant than the second one.
#c14	we;a document;the query;another document;we;the first document
#s15	So in this case, the ranking function is defined as the similarity between the query and the document.
#c15	this case;the ranking function;the similarity;the query;the document
#s16	One well known example in this case is vectors space model, which we will cover more in detail later in the lecture.
#c16	One well known example;this case;vectors space model;we;detail;the lecture
#s17	The second kind of models are called probabilistic models.
#c17	The second kind;models;probabilistic models
#s18	In this family of models we follow a very different strategy, where we assume that.
#c18	this family;models;we;a very different strategy;we
#s19	Queries and documents are all observations from random variables.
#c19	Queries;documents;observations;random variables
#s20	And we assume there is a Binary Random variable called R here.
#c20	we;a Binary Random variable;R
#s21	To indicate whether a document is relevant to a query.
#c21	a document;a query
#s22	We then define the score of document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query.
#c22	We;the score;document;respect;a query;the probability;this random variable R;a particular document;query
#s23	There are different cases of such a general idea.
#c23	different cases;such a general idea
#s24	One is classic probabilistic model.
#c24	classic probabilistic model
#s25	Another is language model, yet another is divergent from randomness model.
#c25	language model;randomness model
#s26	In the later lecture we will talk more about one case which is language model.
#c26	the later lecture;we;more about one case;language model
#s27	The third kind of models are based on probabilistic influence, so here the idea is to associate uncertainity to inference rules, and we can then quantify the probability that we can show that the query follows from the document.
#c27	The third kind;models;probabilistic influence;the idea;uncertainity;rules;we;the probability;we;the query;the document
#s28	Finally, there is also a family of models that are using axiomatic thinking.
#c28	a family;models;axiomatic thinking
#s29	Here the idea is to define a set of constraints that we hope.
#c29	the idea;a set;constraints;we
#s30	A good retrieval function.
#c30	A good retrieval function
#s31	To satisfy.
#s32	So in this case, the problem is to seek A good ranking function that can satisfy all the desired constraints.
#c32	this case;the problem;A good ranking function;all the desired constraints
#s33	Interestingly, although these different models are based on different thinking.
#c33	these different models;different thinking
#s34	In the end.
#c34	the end
#s35	The retrieval function.
#c35	The retrieval function
#s36	Tends to be very similar.
#s37	And these functions tend to also involve similar variables.
#c37	these functions;similar variables
#s38	So now let's take a look at the common form of a state of the retrieval model.
#c38	's;a look;the common form;a state;the retrieval model
#s39	And to examine some of the common ideas used in all these models.
#c39	the common ideas;all these models
#s40	First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture.
#c40	these models;the assumption;bag;words;text;we;the natural language processing lecture
#s41	Bag of words representation remains the main representation.
#c41	words;the main representation
#s42	Used in all the search engines.
#c42	all the search engines
#s43	So with this assumption, the score of a query, presidential campaign news.
#c43	this assumption;a query
#s44	With respect to a document d here would be based on scores computed based on each individual word.
#c44	respect;a document;d;scores;each individual word
#s45	And that means the score would depend on the score of each word.
#c45	the score;the score;each word
#s46	Such as presidential campaign and news.
#c46	presidential campaign;news
#s47	Here we can see.
#c47	we
#s48	There are three different components, each corresponding to how well the document matches each of the query words.
#c48	three different components;the document;the query words
#s49	Inside these functions.
#c49	these functions
#s50	We see a number of heuristics used.
#c50	We;a number;heuristics
#s51	So for example, one factor that affects the.
#c51	example
#s52	Function G Here is how many times does the world presidential occur in the document.
#c52	Function G;how many times;the world;the document
#s53	This is called a term frequency or TF.
#c53	a term frequency;TF
#s54	We might also denote as c of presidential and d. In general, if.
#c54	We;c;presidential;d.
#s55	The word occurs more frequently in the document then the value of this function would be larger.
#c55	The word;the document;the value;this function
#s56	Another factor is how long is the document.
#c56	Another factor;the document
#s57	And This is to use the document length for scoring.
#c57	the document length;scoring
#s58	In general.
#s59	If a term occurs in a long document that many times.
#c59	a term;a long document
#s60	It's not as significant as.
#c60	It
#s61	If it occurred the same number of times in a short document, because in a long document.
#c61	it;the same number;times;a short document;a long document
#s62	Any term is expected to occur more frequently.
#c62	Any term
#s63	Finally, there is this factor called the document frequency.
#c63	this factor;the document frequency
#s64	That is, we also want to look at the how often presidential occurs in the entire collection.
#c64	we;the how often presidential occurs;the entire collection
#s65	And we call this document frequency or DF of presidential.
#c65	we;this document frequency;DF;presidential
#s66	And in some other models we might also.
#c66	some other models;we
#s67	Use a probability.
#c67	a probability
#s68	To characterize this information.
#c68	this information
#s69	So here is show the probability of presidential in the collection.
#c69	the probability;the collection
#s70	So all these are trying to characterize the popularity of the term in the collection in general, matching a rare term in the collection.
#c70	the popularity;the term;the collection;a rare term;the collection
#s71	Is contributing more to the overall score than matching a common term.
#c71	the overall score;a common term
#s72	So this captures some of the main ideas used in pretty much all the state of art retrieval models.
#c72	the main ideas;all the state;art retrieval models
#s73	So the natural question is which model works the best?
#c73	the natural question;model
#s74	Now it turns out that many models work equally well, so here are a list of the four major models that are generally regarded as a state of the art retrieval models.
#c74	it;many models;a list;the four major models;a state;the art retrieval models
#s75	Pivoted length normalization.
#c75	Pivoted length normalization
#s76	BM25, query likelihood, PL2.
#c76	BM25;query likelihood
#s77	When optimized, these models tend to perform similarly.
#c77	these models
#s78	And this was discussed in detail in this reference at the end of this lecture.
#c78	detail;this reference;the end;this lecture
#s79	Among all these BM25 is probably the most popular.
#c79	all these BM25
#s80	It's most likely that this has been used in virtually all the search engines and you will also often see this method discussed in research papers.
#c80	It;virtually all the search engines;you;this method;research papers
#s81	And we'll talk more about this method later in.
#c81	we;this method
#s82	Some other letures.
#c82	Some other letures
#s83	So to summarize.
#s84	The main points made in this lecture are first the design of a good ranking function.
#c84	The main points;this lecture;the design;a good ranking function
#s85	Pre requires a computational definition of relevance and we achieve this goal by designing appropriate retrieval model.
#c85	Pre;a computational definition;relevance;we;this goal;appropriate retrieval model
#s86	Second, many models are equally effective, but we don't have a single winner yet.
#c86	many models;we;a single winner
#s87	Researchers are still actively working on this problem.
#c87	Researchers;this problem
#s88	Trying to find a truly optimal retrieval model.
#c88	a truly optimal retrieval model
#s89	Finally, the state of art ranking functions tend to rely on the following ideas.
#c89	the state;art ranking functions;the following ideas
#s90	First bag of words representation.
#c90	First bag;words
#s91	2nd.
#c91	2nd
#s92	TF and document frequency of words, such information is used in.
#c92	words;such information
#s93	the weighting function to determine the overall contribution of matching a word.
#c93	the weighting function;the overall contribution;a word
#s94	And document lengths.
#c94	And document lengths
#s95	These are often combined in interesting ways and we'll discuss how exactly they are combined to rank documents in the lectures later.
#c95	interesting ways;we;they;documents;the lectures
#s96	There are two suggested the additional readings.
#c96	the additional readings
#s97	If you have time.
#c97	you;time
#s98	The first is a paper where you can find a detailed discussion and comparison of multiple state of the art models.
#c98	a paper;you;a detailed discussion;comparison;multiple state;the art models
#s99	The second is a book with a chapter that gives a broad review of different retrieval models.
#c99	a book;a chapter;a broad review;different retrieval models
410	ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	104
#s1	This lecture is a continued discussion of latent aspect rating analysis.
#c1	This lecture;a continued discussion;latent aspect rating analysis
#s2	Earlier we talked about how to solve the problem of Lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights.
#c2	we;the problem;Lara;two stages;we;segmentation;different aspects;we;a little regression model;the aspect ratings;the weights
#s3	Now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model.
#c3	it;a unified generative model;this problem;we;we;the generation;text;we;the generation;text;a natural solution;topic model
#s4	So given an entity, we can assume there are aspects that are described by word distributions.
#c4	an entity;we;aspects;word distributions
#s5	Topics and then we can use a topic model to model the generation of the review text.
#c5	Topics;we;a topic model;the generation;the review text
#s6	Our assumed the words in the review text are drawn from these distributions.
#c6	Our;the words;the review text;these distributions
#s7	In the same way as we assumed for a generative model like PSA.
#c7	the same way;we;a generative model;PSA
#s8	And then we can then plug in the latent regression model to use the text to further predict the Overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating.
#c8	we;the latent regression model;the text;the Overall rating;we;the aspect rating;them;aspect weights;the overall rating
#s9	So this would give us a unified generative model where we model both the generation of text and the overall rating condition on text.
#c9	us;a unified generative model;we;both the generation;text;the overall rating condition;text
#s10	So we don't have time to discuss this model in detail, as in many other cases in this part of the course where we discuss the cutting edge topics.
#c10	we;time;this model;detail;many other cases;this part;the course;we;the cutting edge topics
#s11	But there is a reference site here where you can find more details.
#c11	a reference site;you;more details
#s12	So now I'm going to show you some simple results that you can get by using this kind of generative models.
#c12	I;you;some simple results;you;this kind;generative models
#s13	First it's about rating decomposition.
#c13	it;rating decomposition
#s14	So here what you see are the decomposed ratings for three hotels that have the same overall rating.
#c14	what;you;the decomposed ratings;three hotels;the same overall rating
#s15	So if you just look at the overall rating you don't.
#c15	you;the overall rating;you
#s16	You can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some.
#c16	You;much difference;these hotels;these ratings;aspect ratings;we;some hotels;higher ratings
#s17	Dimensions like value, but others might score better in other dimensions like location and so this can reveal detailed opinions at the aspect level.
#c17	Dimensions;value;others;other dimensions;location;detailed opinions;the aspect level
#s18	Now here, the ground truth is shown in the plans, so this also allows you to see whether the prediction is accurate.
#c18	the ground truth;the plans;you;the prediction
#s19	It's not always accurate, but it's mostly still reflecting some of the trends.
#c19	It;it;the trends
#s20	The 2nd result is to compare different reviewers on the same hotel so the table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same.
#c20	The 2nd result;different reviewers;the same hotel;the table;the decompose ratings;two reviewers;same hotel;their high level overall ratings
#s21	So if you just look at the overall ratings, you don't really get that much information about the difference between the two reviews.
#c21	you;the overall ratings;you;that much information;the difference;the two reviews
#s22	But after you decompose the ratings you can see clearly they have high scores on different dimensions.
#c22	you;the ratings;you;they;high scores;different dimensions
#s23	So this shows that the model can reveal differences in.
#c23	the model;differences
#s24	Opinions of different reviewers and such a detailed understanding can help us understand better about reviews and also better about their feedback on the hotel.
#c24	Opinions;different reviewers;such a detailed understanding;us;reviews;their feedback;the hotel
#s25	This is something very interesting because this is in some sense some byproduct in our problem formulation.
#c25	something;some sense;some byproduct;our problem formulation
#s26	We did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects.
#c26	We;the design;the generative model;this component;sentiment waits;words;different aspects
#s27	And you can see the highly weighted words versus the negatively lower weighted words here for each of the four dimensions.
#c27	you;the highly weighted words;the negatively lower weighted words;the four dimensions
#s28	Value, rooms, location and cleanliness.
#c28	Value;rooms;location;cleanliness
#s29	I added the top words, cleared it, makes sense, and the bottom words also makes sense.
#c29	I;the top words;it;sense;the bottom words;sense
#s30	So this shows that with this apology, we can also learn sentiment information directly from the data.
#c30	this apology;we;sentiment information;the data
#s31	Now this kind of laxing is very useful becausw in general a word like long, let's say, may have different the sentiment polarities for different context.
#c31	this kind;laxing;very useful becausw;a word;'s;different the sentiment polarities;different context
#s32	So if I say the battery life of this laptop is long, then that's positive.
#c32	I;the battery life;this laptop
#s33	But if I say the rebooting time for the laptop is long, that's bad, right?
#c33	I;the rebooting time;the laptop
#s34	So even for reviews about the same product laptop, the word long Is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about the hotels in social media like tweets.
#c34	reviews;the same product laptop;the word;it;this kind;lexicon;we;this kind;generative models;a word;a particular aspect;fact;such a lexicon;other reviews;hotels;tag comments;the hotels;social media;tweets
#s35	And, what's also interesting that since this is an almost computer and supervised, assuming that the reviews with overall ratings are available, and then this can allow us to learn from potentially a large amount of data on the Internet to reach sentiment lexicon.
#c35	what;an almost computer;the reviews;overall ratings;us;potentially a large amount;data;the Internet;sentiment lexicon
#s36	And here are some results to validate the preference weights.
#c36	some results;the preference weights
#s37	Remember, the model can infer whether a reviewer cares more about service or the price.
#c37	the model;a reviewer;service;the price
#s38	Now, how do we know whether the inferred weights are correct and this poses a very difficult challenge for evaluation.
#c38	we;the inferred weights;a very difficult challenge;evaluation
#s39	Now here we show some interesting way of evaluating result.
#c39	we;some interesting way;result
#s40	what you here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviews.
#c40	what;you;the prices;hotels;different cities;the prices;hotels;different groups;reviews
#s41	The top ten other reviewers with the highest inferred value to other aspect ratio.
#c41	The top ten other reviewers;the highest inferred value;other aspect ratio
#s42	So for example, value versus location value versus room etc.
#c42	example;location value
#s43	But the top ten are the reviewers that have the highest ratios by this measure.
#c43	the reviewers;the highest ratios;this measure
#s44	And that means these reviewers tend to put a lot of weight on value as compared with other dimensions.
#c44	these reviewers;a lot;weight;value;other dimensions
#s45	That means they really emphasize on value.
#c45	they;value
#s46	The bottom ten, on the other hand, are the reviews that have the lowest ratio.
#c46	the other hand;the reviews;the lowest ratio
#s47	What does that mean?
#c47	What
#s48	Well, that means these reviewers have put higher weights on other aspects than value, so those are people that care about the another dimension and they didn't care so much about the value in some sense by this, less compared with the top ten group.
#c48	these reviewers;higher weights;other aspects;value;people;the another dimension;they;the value;some sense;the top ten group
#s49	Now these ratios are computer based on the inferred weights from the model.
#c49	these ratios;computer;the inferred weights;the model
#s50	So now you can see the average prices of hotels are favored by toptenreviews are indeed and much cheaper than those that are favored by the bottom 10.
#c50	you;the average prices;hotels;toptenreviews
#s51	And this provides some Indirect way of validating the infer wait.
#c51	some Indirect way;the infer
#s52	It just means the weights are not random and they are actually meaningful here and in comparison with the average price in these three cities, you can actually the top ten tends to have below average price, whereas the bottom time where they care a lot about other things like service or room condition tend to have hotels that have higher prices than average.
#c52	It;the weights;they;comparison;the average price;these three cities;you;average price;the bottom time;they;other things;service;room condition;hotels;higher prices
#s53	So with these results we can build a lot of interesting applications.
#c53	these results;we;a lot;interesting applications
#s54	For example, direct application would be the generator rated aspect, the summary.
#c54	example;direct application;the generator;aspect;the summary
#s55	An because of the decomposition, we can now generate the summaries for each aspect.
#c55	the decomposition;we;the summaries;each aspect
#s56	The positive sentence is negative sentences about each aspect.
#c56	The positive sentence;negative sentences;each aspect
#s57	It's more informative than original review that has just overall rating and review test.
#c57	It;original review;just overall rating and review test
#s58	Here are also mother results about the aspects discovered from reviews with low ratings.
#c58	mother results;the aspects;reviews;low ratings
#s59	These are MP3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects.
#c59	MP3 three reviews;an these results;the model;some interesting aspects;low overall ratings;those high overall ratings;they;the different aspects
#s60	Or they comment more on different aspects.
#c60	they;different aspects
#s61	So that can help us discover, for example, consumers trained in appreciating different features of product.
#c61	us;example;consumers;different features;product
#s62	For example, one might have discovered the trend that people tend to like large screens of cell phones or lightweight of laptop etc.
#c62	example;one;the trend;people;large screens;cell phones;lightweight
#s63	And such knowledge can be useful for manufacturers to design their next generation of products.
#c63	such knowledge;manufacturers;their next generation;products
#s64	Here are some interesting results on analyzing users rating behavior.
#c64	some interesting results;users rating behavior
#s65	So what you see is average weights on different dimensions by different groups of reviewers.
#c65	what;you;average weights;different dimensions;different groups;reviewers
#s66	And on the left side you see the weights of reviews like the expensive hotels they give.
#c66	the left side;you;the weights;reviews;the expensive hotels;they
#s67	The whole expensive hotels five stars and you can see their average weights tend to be more focused on service and that suggests that people might be expensive hotels because of good service.
#c67	The whole expensive hotels;five stars;you;their average weights;service;people;expensive hotels;good service
#s68	And that's not surprising as also another way to validate the inferred weights.
#c68	also another way;the inferred weights
#s69	But if you look at the right side where look at the column of five stars, these are the reviewers that like the cheaper hotels and they give cheaper hotels, five stars as we expected, and they put more weight on value and that's why they like the cheaper hotels.
#c69	you;the right side;the column;five stars;the reviewers;the cheaper hotels;they;cheaper hotels;we;they;more weight;value;they;the cheaper hotels
#s70	But if you look at the when they didn't like expensive hotels or cheaper hotels and you seal it tended to have more weights on the condition of the room cleanliness.
#c70	you;they;expensive hotels;cheaper hotels;you;it;more weights;the condition;the room cleanliness
#s71	So this shows that by using this model we can infer some information that's very hard to obtain, even if you read all the reviews.
#c71	this model;we;some information;you;all the reviews
#s72	Even if you read all the reviews, it's very hard to infer such preferences or such emphasis.
#c72	you;all the reviews;it;such preferences;such emphasis
#s73	So this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful.
#c73	a case;text mining algorithms;what;humans;interesting patterns;the data;course
#s74	You can compare different hotels, compare the opinions from different consumer groups in different locations, and of course the model is general.
#c74	You;different hotels;the opinions;different consumer groups;different locations;course;the model
#s75	It can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications.
#c75	It;any reviews;overall ratings;very useful technique;a lot;text mining applications
#s76	Finally, there is also some result on applying this model for personalized ranking or recommendation of entities.
#c76	some result;this model;personalized ranking;recommendation;entities
#s77	So because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about.
#c77	we;the reviewers;weights;different dimensions;we;a user;what;you
#s78	So, for example, if a query here that shows 90% of the way it should be on value and 10% on others.
#c78	example;90%;the way;it;value;others
#s79	So that just means I don't care about other aspects, I just care about getting a cheap hotel.
#c79	I;other aspects;I;a cheap hotel
#s80	My emphasis is on the value dimension.
#c80	My emphasis;the value dimension
#s81	Now what we can do is such a query is that we can use reviewers that we believe have a similar preference to recommend the hotels for you.
#c81	what;we;such a query;we;reviewers;we;a similar preference;the hotels;you
#s82	How can we know that we can infer the weights of those reviewers on different aspects?
#c82	we;we;the weights;those reviewers;different aspects
#s83	We can find the reviewers whose weights or more precise whose inferred weights or similar to yours and then use those reviews to recommend the hotels for you.
#c83	We;the reviewers;whose weights;whose inferred weights;yours;those reviews;the hotels;you
#s84	And this is what we call a personalized or rather query specific recommendation.
#c84	what;we;a personalized or rather query specific recommendation
#s85	The non personalized recommendation results are shown on the top.
#c85	The non personalized recommendation results;the top
#s86	An you can see the top results generally have much higher price than the low Group, and that's because when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels.
#c86	you;the top results;much higher price;the low Group;reviewers;the value;this query;they;favor;low price hotels
#s87	So this is yet another application of this technique.
#c87	another application;this technique
#s88	And shows that by doing text mining we can understand the users better.
#c88	text mining;we;the users
#s89	And once we can end users better, we can serve these users better.
#c89	we;users;we;these users
#s90	So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications.
#c90	our discussion;opinion mining;a very important topic;a lot;applications
#s91	And as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation.
#c91	a task sentiment analysis;just text categorization;standard techniques;we;enriched feature representation
#s92	And we also need to consider the order of those categories and we talk about the ordinal regression.
#c92	we;the order;those categories;we;the ordinal regression
#s93	For solving this problem.
#c93	this problem
#s94	We have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of words in the model.
#c94	We;generative models;mining latent user preferences;the generating model;the rating regression;we;some interesting preference information;weights;words;the model
#s95	As a result, we can learn those useful information when fitting the model to the data.
#c95	a result;we;those useful information;the model;the data
#s96	Most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear
#c96	Most approaches;product reviews;the cause;such a context;the opinion holder;an opinion target
#s97	and they are easy to analyze
#c97	they
#s98	and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all.
#c98	course;a lot;practical applications;opinion mining;news;social media;review data;the opinion holders;opinion targets
#s99	implicit and so that calls for natural language processing techniques to uncover them accurately.
#c99	natural language processing techniques;them
#s100	So here are some suggested readings, the first 2.
#c100	some suggested readings
#s101	are small books that are excellent reviews of this topic where you can find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem.
#c101	small books;excellent reviews;this topic;you;a lot;discussion;the other variations;the problem and techniques proposal;the problem
#s102	The next two papers are about the generative models for letting the aspect rating analysis.
#c102	The next two papers;the generative models;the aspect
#s103	The first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model.
#c103	the problem;two stages;the second one;the unified model;topic model;the regression model
#s104	To solve the problem using a unified model.
#c104	the problem;a unified model
410	f1951cf2-4293-450b-8578-4d74c72f9862	140
#s1	This lecture is about opinion mining and sentiment analysis covering its motivation.
#c1	This lecture;opinion mining and sentiment analysis;its motivation
#s2	In this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data.
#c2	this lecture;we;a different kind;knowledge;namely knowledge;the observer;humans;text data
#s3	In particular, we're going to talk about the opinion mining and sentiment analysis.
#c3	we;the opinion mining and sentiment analysis
#s4	As we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors.
#c4	we;text data;the data;humans;subjective sensors
#s5	In contrast, we have other devices such as video recorder that can report what's happening in the real world objectively to generate the video data, for example.
#c5	contrast;we;other devices;video recorder;what;the real world;the video data;example
#s6	Now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans.
#c6	the main difference;text data;other data;video data;it;rich and rich opinions;the content;it;humans
#s7	Now this is actually unique advantage of text data, as compared with other data because it offers us a great opportunity to understand the observers.
#c7	actually unique advantage;text data;other data;it;us;a great opportunity;the observers
#s8	We can mine the text data to understand the opinions, understand the people's preferences, how people think about something.
#c8	We;the text data;the opinions;the people's preferences;people;something
#s9	So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data.
#c9	this lecture;the following lectures;we;mine and analyze opinions;a lot;text data
#s10	So let's start with the concept of opinion that it's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something.
#c10	's;the concept;opinion;it;opinion;we;opinion;a subjective statement;what;a person;something
#s11	Now I highlighted a quite a few words here, and that's because it was thinking a little more about these words and that would help us better understand what's in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining.
#c11	I;a quite a few words;it;these words;us;what;the opinion;us;opinion;the problem;opinion mining
#s12	So let's first look at the keyword subjective here.
#c12	's;the keyword
#s13	Now this is in contrast with objective statement or factual statement.
#c13	contrast;objective statement;factual statement
#s14	Those statements can be proved right or wrong.
#c14	Those statements
#s15	And this is a key differentiating factor from opinion, which tends to be not easy to prove wrong or right because it reflects what a person thinks about something.
#c15	a key differentiating factor;opinion;it;what;a person;something
#s16	So in contrast, objective statement can usually be proved wrong or correct.
#c16	contrast;objective statement
#s17	For example, you might say this computer has a screen and a battery.
#c17	example;you;this computer;a screen;a battery
#s18	Now that's something you can check.
#c18	something;you
#s19	It's either having a battery or not.
#c19	It;a battery
#s20	But in contrast, if you think about the sentence such as this laptop has the best battery.
#c20	contrast;you;the sentence;this laptop;the best battery
#s21	Or this laptop has a nice screen, now these statements are more subjective and it's very hard to prove whether it's wrong or correct.
#c21	this laptop;a nice screen;these statements;it;it
#s22	So opinion is subjective statement.
#c22	opinion;subjective statement
#s23	And next, let's look at the keyword person here and that indicates this opinion Holder 'cause when we talk about opinion, it's about the opinion held by someone and then we notice that there is something here.
#c23	's;the keyword person;this opinion Holder;we;opinion;it;the opinion;someone;we;something
#s24	So that's the target of the opinion.
#c24	the target;the opinion
#s25	The opinions is expressed on this something.
#c25	The opinions;this something
#s26	And now, of course, believes or thinks implies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context.
#c26	course;the opinion;the culture;background;context;person;the different context
#s27	People from different background may also think in different ways.
#c27	People;different background;different ways
#s28	So this analysis shows that there are multiple elements that we need to include in order to characterize an opinion.
#c28	this analysis;multiple elements;we;order;an opinion
#s29	So what's the basic opinion representation like, well,  it should include at least three measurements, right?
#c29	what;the basic opinion representation;it;at least three measurements
#s30	First it has to specify what's the opinion Holder.
#c30	it;what;the opinion Holder
#s31	So whose opinion is this, second must also specify the target.
#c31	whose opinion;the target
#s32	What's this opinion about?
#c32	What;this opinion
#s33	And 3rd, of course we want opinion content
#c33	course;we;opinion content
#s34	and So what exactly is the opinion?
#c34	what;the opinion
#s35	If you can identify this, we get a basic understanding of an opinion and can already be useful.
#c35	you;we;a basic understanding;an opinion
#s36	Sometimes if you want to understand further, we want to enrich the opinion representation.
#c36	you;we;the opinion representation
#s37	And that means we also want to understand, for example, the context of the opinion and what situation was opinion expressed.
#c37	we;example;the context;the opinion;what situation;opinion
#s38	For example, in what time was it expressed?
#c38	example;what time;it
#s39	We also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative?
#c39	We;opinion sentiment;what;the opinion;us;the opinion holder's feeling;example;this opinion
#s40	Or perhaps the opinion holder was happy or sad.
#c40	the opinion holder
#s41	And so such understanding obviously goes beyond just extracting the opinion content and needs some analysis.
#c41	such understanding;the opinion content;some analysis
#s42	So let's take a simple example of a product review.
#c42	's;a simple example;a product review
#s43	In this case, this actually explicit opinion Holder and explicit target, so it's It's obviously what's opinion Holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed.
#c43	this case;explicit target;it;It;what;opinion Holder;just a reviewer;it;what;the opinion target;the product
#s44	For example iPhone 6.
#c44	example
#s45	When the review was posted, usually you can extract such information easily.
#c45	the review;you;such information
#s46	Now the content of course is the review text that's in general also easy to obtain.
#c46	the content;course;the review text
#s47	So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation.
#c47	you;product reviews;terms;a basic opinion representation
#s48	But of course, if you want to get more information, we might want to know the context.
#c48	course;you;more information;we;the context
#s49	For example, the review was written in 2015.
#c49	example;the review
#s50	Or we want to know that the sentiment of this review is positive, and so this additional understanding of course adds value to mining the opinions.
#c50	we;the sentiment;this review;this additional understanding;course;value;the opinions
#s51	Now you can see in this case the task is relatively easy, and that's because.
#c51	you;this case;the task
#s52	The opinion Holder and opinion target that have already been identified.
#c52	The opinion Holder;opinion target
#s53	Now let's take a look at the sentence in the news.
#c53	's;a look;the sentence;the news
#s54	In this case, we have implicit holder and implicit target.
#c54	this case;we;implicit holder;implicit target
#s55	And the task is in general harder so we can identify opinion holder here and that's governor of Connecticut.
#c55	the task;we;opinion holder;governor;Connecticut
#s56	We can also identify the target.
#c56	We;the target
#s57	So one target is Hurricane Sandy.
#c57	one target;Hurricane Sandy
#s58	But there is also another target, which is a hurricane of 1938.
#c58	another target;a hurricane
#s59	So what's the opinion?
#c59	what;the opinion
#s60	Well, this negative sentiment here that's indicated by words like a bad and worst.
#c60	Well, this negative sentiment;words
#s61	And we can also then identify the context.
#c61	we;the context
#s62	New England in this case.
#c62	New England;this case
#s63	Now unlike in the product review, all these elements must be extracted by using natural language processing techniques.
#c63	the product review;all these elements;natural language processing techniques
#s64	So the task is much harder and we need a deeper natural language processing.
#c64	the task;we;a deeper natural language processing
#s65	And these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened.
#c65	these examples;a lot;work;product reviews;what
#s66	Analyzing sentiment in news is still quite difficult.
#c66	sentiment;news
#s67	It's more difficult than the analysis of opinions in product reviews.
#c67	It;the analysis;opinions;product reviews
#s68	Now there are also some other interesting variations.
#c68	some other interesting variations
#s69	In fact, here we're going to examine the variations of opinions more systematically.
#c69	fact;we;the variations;opinions
#s70	First, lets think about the opinion Holder.
#c70	lets;the opinion Holder
#s71	Now the Holder could be an individual or could be a group of people and sometimes opinion was from a committee or from a whole country of people.
#c71	the Holder;an individual;a group;people;opinion;a committee;a whole country;people
#s72	Opinion Target can also vary a lot.
#c72	Opinion Target
#s73	It can be about 1 entity, a particular person, a particular product, a particular policy, etc.
#c73	It;about 1 entity;a particular person;a particular product;a particular policy
#s74	But it could be about a group of products.
#c74	it;a group;products
#s75	Could be about the product from a company in general.
#c75	the product;a company
#s76	Could also be very specific about one attribute, attribute of Entity for example.
#c76	one attribute;attribute;Entity;example
#s77	It's just about the battery of iPhone.
#c77	It;the battery;iPhone
#s78	It could be about someone else opinion and one person might comment on another persons opinion etc.
#c78	It;someone;opinion;one person;another persons opinion
#s79	So you can see there is a lot of variation here that will cause the problem to vary a lot.
#c79	you;a lot;variation;the problem
#s80	Now opinion content, of course, can also vary a lot on the surface.
#c80	opinion content;course;a lot;the surface
#s81	You can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article.
#c81	You;one sentence opinion;one phrase opinion;you;longer text;the opinion;a whole article
#s82	And Furthermore, we can identify the variation in the sentiment or emotion dimension.
#c82	we;the variation;the sentiment;emotion dimension
#s83	That's about the feeling of the opinion Holder.
#c83	the feeling;the opinion Holder
#s84	So we can distinguish positive versus negative or neutral or happy versus sad, etc.
#c84	we
#s85	Finally, the opinion context can also vary.
#c85	the opinion context
#s86	We can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed.
#c86	We;simple context;different time;different locations;complex text;some background topic
#s87	So when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion.
#c87	opinion;the particular discourse context;it;different ways;it;another context;the context;the entire discourse context;opinion
#s88	From computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective.
#c88	computational perspective;we;what;opinions;text data;it;we;different kinds;opinions;text data;computation perspective
#s89	First, the Observer might make a comment about the opinion target in the observed world.
#c89	the Observer;a comment;the opinion target;the observed world
#s90	So in this case we have the author's opinion.
#c90	this case;we;the author's opinion
#s91	For example, I don't like this phone at all, and that's opinion of this author.
#c91	example;I;this phone;opinion;this author
#s92	In contrast, the text might also report opinions about others so the person could also make observation about another person's opinion and report this opinion.
#c92	contrast;the text;opinions;others;the person;observation;another person's opinion;this opinion
#s93	So for example, I believe he loves the painting and that opinion is really about, is really expressed by another person.
#c93	example;I;he;the painting;that opinion;another person
#s94	Here, so it doesn't mean this author loves that painting.
#c94	it;this author;that painting
#s95	So clearly the two kinds of opinions need to be analyzed in different ways and sometimes in product reviews you can see, although mostly the opinions are from this reviewer.
#c95	the two kinds;opinions;different ways;product reviews;you;the opinions;this reviewer
#s96	Sometimes a reviewer might mention opinions of his friend or her friend, right?
#c96	a reviewer;opinions;his friend;her friend
#s97	And another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion.
#c97	another complication;indirect opinions;inferred opinions;inferences;what;the text;opinion
#s98	For example, one statement might be this phone ran out of battery in just one hour.
#c98	example;one statement;this phone;battery;just one hour
#s99	Now this is in a way, a factual statement, 'cause you know it's either true or false, right?
#c99	a way;you;it
#s100	You can even verify that.
#c100	You
#s101	But from this statement one can also infer some negative opinions about the quality of the battery of this phone or the feeling of the opinion holder about the battery.
#c101	this statement;one;some negative opinions;the quality;the battery;this phone;the feeling;the opinion holder;the battery
#s102	In the opinion, Holder clearly wish the battery to last longer.
#c102	the opinion;Holder;the battery
#s103	So these are interesting variations that we need to pay attention to when we extract opinions.
#c103	interesting variations;we;attention;we;opinions
#s104	Also, for this reason about the indirect opinions.
#c104	this reason;the indirect opinions
#s105	It's often also very useful to extract it or whatever the person had said about the product, and sometimes factual sentences like this are also very useful.
#c105	It;it;the person;the product;sometimes factual sentences
#s106	So from practical viewpoint, sometimes we don't necessarily extract the subjective sentences.
#c106	practical viewpoint;we;the subjective sentences
#s107	Instead, would you just get all the sentences that are about opinions that are useful for understanding the person or understanding the product that we are commenting on.
#c107	you;all the sentences;opinions;the person;the product;we
#s108	So the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations.
#c108	the task;opinion mining;text data;input;a set;opinion representations
#s109	In each representation we should identify opinion Holder, target content and context.
#c109	each representation;we;opinion Holder;target content;context
#s110	Ideally we can also infer opinion sentiment from the content and context to better understand the opinion.
#c110	we;opinion sentiment;the content;the opinion
#s111	Now often some elements of the representation are already known.
#c111	some elements;the representation
#s112	I just gave a good example, in the case of product reviews where the opinion Holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks.
#c112	I;a good example;the case;product reviews;the opinion Holder;opinion target;the simplest opinion mining tasks
#s113	Now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques.
#c113	it;other tasks;the cases;you;applications;opinion mining techniques
#s114	So now that we have talked about what is opinion mining
#c114	we;what;opinion mining
#s115	and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful.
#c115	we;the task;'s;opinion mining;it
#s116	So here I identify three major reasons, 3 broad reasons.
#c116	I;three major reasons;3 broad reasons
#s117	The first is it can help decision support.
#c117	it;decision support
#s118	I can help us optimize our decisions.
#c118	I;us;our decisions
#s119	We often look at the other peoples opinions and look at the reader reviews in order to make a decision like buying, buying a product, or using the service.
#c119	We;the other peoples opinions;the reader reviews;order;a decision;a product;the service
#s120	We also Would be interested in others opinions.
#c120	We;others opinions
#s121	When we decide whom to vote, for example.
#c121	we;whom;example
#s122	And policymakers may also want to know peoples opinions when designing a new policy.
#c122	policymakers;peoples opinions;a new policy
#s123	So that's one general kind of applications.
#c123	one general kind;applications
#s124	And it's very broad, of course.
#c124	it;course
#s125	The second application is to understand people, and this is also very important.
#c125	The second application;people
#s126	For example, it can help understand peoples preferences.
#c126	example;it;peoples preferences
#s127	So and this could help us better serve people.
#c127	us;people
#s128	For example, we can optimize the product search engine, optimize recommender system if we know what people are interested in, what people think about products.
#c128	example;we;the product search engine;recommender system;we;what;people;what;people;products
#s129	It can also help her with advertising, of course, and we can have targeted advertising if we know what kind of people tend to know to like what kind of product.
#c129	It;her;advertising;course;we;advertising;we;what kind;people;what kind;product
#s130	Now the third kind of applications can be called voluntary survey.
#c130	the third kind;applications;voluntary survey
#s131	Now this is mostly to support research that used to be done by doing surveys, doing manual surveys, questioning answering.
#c131	research;surveys;manual surveys
#s132	People need to fill in forms to answer some questions.
#c132	People;forms;some questions
#s133	Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans to kind of assess the general opinion.
#c133	humans;sensors;we;opinions;a lot;humans;the general opinion
#s134	Now this is would be very useful for business intelligence, where product manufacturers want to know, where their products have advantages over others.
#c134	business intelligence;product manufacturers;their products;advantages;others
#s135	What are the winning features of their product or winning features of competitive products?
#c135	What;the winning features;their product;features;competitive products
#s136	Market research has to do with understanding consumers opinions and this is clearly very useful, directed for that.
#c136	Market research;consumers opinions
#s137	Data Driven social science research can benefit from this because they can do text mining to understand the people's opinions.
#c137	Data Driven social science research;they;text mining;the people's opinions
#s138	And if we can aggregate a lot of opinions from social media from a lot of public information, then you can actually do some study of some questions.
#c138	we;a lot;opinions;social media;a lot;public information;you;some study;some questions
#s139	For example, we can study the behavior of people on social media or in social networks, and these can be regarded as voluntary survey, but done by those people.
#c139	example;we;the behavior;people;social media;social networks;voluntary survey;those people
#s140	And in general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of prediction.
#c140	we;a lot;advantage;any prediction task;we;the text data;extra data;any problem;we;text based prediction techniques;you;prediction;the accuracy;prediction
410	f47410d8-2bc1-44d5-a37d-cbf2d2c917f8	122
#s1	This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems.
#c1	This lecture;some practical issues;you;evaluation;text retrieval systems
#s2	In this lecture we will continue the discussion of evaluation we'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems.
#c2	this lecture;we;the discussion;evaluation;we;some practical issues;you;actual evaluation;text retrieval systems
#s3	So, In order to create the test collection, we have to create a set of queries, a set of documents and a set of relevance judgments.
#c3	order;the test collection;we;a set;queries;a set;documents;a set;relevance judgments
#s4	It turns out that each is actually challenging to create.
#c4	It
#s5	First, the documents and queries must be representative.
#c5	the documents;queries
#s6	They must represent the real queries, and real documents that the users handle, and we also have to use many queries and many documents in order to avoid biased conclusions.
#c6	They;the real queries;real documents;the users;we;many queries;many documents;order;biased conclusions
#s7	For the matching of relevant documents, with the queries we also need to ensure that there exists a lot of relevant documents for each query.
#c7	the matching;relevant documents;the queries;we;a lot;relevant documents;each query
#s8	If a query has only one, let's say rather than the document in the collection, then you know it's not very informative to compare different methods using such a query, because there's not that much room for us to see difference, so ideally there should be more relevant documents in the collection, but yet the queries also should represent the real queries that we care about.
#c8	a query;'s;the document;the collection;you;it;different methods;such a query;that much room;us;difference;more relevant documents;the collection;the queries;the real queries;we
#s9	In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries, yet minimizing human effort because we have to use human labor to label these documents, it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries, especially considering a giant dataset like the web.
#c9	terms;relevance judgments;the challenge;complete judgments;all the documents;all the queries;human effort;we;human labor;these documents;it;a result;it;all the documents;all the queries;a giant dataset;the web
#s10	So this is actually a major challenge.
#c10	a major challenge
#s11	It's a very difficult challenge.
#c11	It;a very difficult challenge
#s12	For measures, It's also challenging because we want the measures that would accurately reflect the perceived utility of users.
#c12	measures;It;we;the measures;the perceived utility;users
#s13	We have to consider carefully what the users care about.
#c13	We;what;the users
#s14	And then design measures to measure that.
#c14	measures
#s15	If your measure is not measuring the right thing, then your conclusion would be misled.
#c15	your measure;the right thing;your conclusion
#s16	So it's very important.
#c16	it
#s17	So, we're going to talk about a couple of issues here.
#c17	we;about a couple;issues
#s18	One is a statistical significance test and this also is the reason why we have to use a lot of queries.
#c18	a statistical significance test;the reason;we;a lot;queries
#s19	And the question here is how sure can you be that observed difference that doesn't simply result from the particular queries you choose, so here are some sample results of average position for system A and system B in two different experiments.
#c19	the question;you;that observed difference;the particular queries;you;some sample results;average position;system A;system B;two different experiments
#s20	And you can see in the bottom we have mean average precision.
#c20	you;the bottom;we;average precision
#s21	So the mean if you look at the mean average precision.
#c21	you;the mean average precision
#s22	The mean average precisions are exactly the same in both experiments.
#c22	The mean average precisions;both experiments
#s23	So you can see this is 0.2.
#c23	you
#s24	This is 0.4 for system B and again here, It's also 0.2 and 0.4, so they are identical.
#c24	system B;It;they
#s25	Yet if you look at the these exact average precisions for different queries.
#c25	you;the these exact average precisions;different queries
#s26	If you look at these numbers in detail, You will realize that, in one case you would feel that you can trust the conclusion here given by the average.
#c26	you;these numbers;detail;You;one case;you;you;the conclusion;the average
#s27	In another case, in the other case, you will feel that well, I'm not sure.
#c27	another case;the other case;you;I
#s28	So why don't you take a look at all these numbers for a moment?
#c28	you;a look;all these numbers;a moment
#s29	Pause the video.
#c29	the video
#s30	So if you look at the average, the mean average precision, we can easily say that, well, system B is better, right?
#c30	you;the average;we;system B
#s31	So it's afterwards 0.4
#c31	it
#s32	and then this is twice as much as 0.2, so that's a better performance.
#c32	a better performance
#s33	But if you look at these two experiments.
#c33	you;these two experiments
#s34	Look at the detail results, You will see that would be more confident to say that in the case one in experiment one.
#c34	the detail results;You;the case;experiment
#s35	In this case, because these numbers seem to be consistently better for system B .
#c35	this case;these numbers;system B
#s36	Whereas in experiment 2, We're not sure because looking at some results like this.
#c36	experiment;We;some results
#s37	Actually System A is better and this is another case.
#c37	System A;another case
#s38	System A is better.
#c38	System A
#s39	But yet if we look at the only average, System B is better.
#c39	we;the only average;System B
#s40	So,  What do you think?
#c40	What;you
#s41	You know how reliable Is our conclusion, if we only look at the average?
#c41	You;our conclusion;we;the average
#s42	Now, in this case, intuitively we feel experiment one is more reliable.
#c42	this case;we;experiment
#s43	But how can we quantitatively answer this question?
#c43	we;this question
#s44	And, This is why we need to do statistical significance test.
#c44	we;statistical significance test
#s45	So the idea of statistical significance test is basically to assess the variance across these different queries.
#c45	the idea;statistical significance test;the variance;these different queries
#s46	If there is a A big variance that means the results could fluctuate a lot according to different queries.
#c46	a A big variance;the results;a lot;different queries
#s47	Then we should believe that unless you have used a lot of queries, the results might change if we use another set of queries.
#c47	we;you;a lot;queries;the results;we;another set;queries
#s48	Right, so this is not.
#s49	So, If you have see high variance then it's not very reliable.
#c49	you;high variance;it
#s50	So let's look at these results again in the second case, right?
#c50	's;these results;the second case
#s51	So here we show two different ways to compare them.
#c51	we;two different ways;them
#s52	One is signed test where we just look at the sign.
#c52	test;we;the sign
#s53	If system B is better than system A, we have a plus sign when System  A is better, we have a minus sign, etc.
#c53	system B;system A;we;a plus sign;System;we;a minus sign
#s54	Using this case, If you see this, well, there are seven cases.
#c54	this case;you;seven cases
#s55	We actually have 4 cases where system B is better, but 3 cases System A is better.
#c55	We;4 cases;system B;3 cases System A
#s56	You intuitively.
#c56	You
#s57	This is almost like a random result, right?
#c57	a random result
#s58	So if you just take a random sample of two to flip 7 coins, and if you use plus to denote the head and then minus to denote the tail, and that could easily be the results of just randomly flipping these 7 coins.
#c58	you;a random sample;7 coins;you;the head;the tail;the results;these 7 coins
#s59	So the fact that the average is larger doesn't tell us anything and we can reliably conclude that, and this can be quantitatively measured by a P value and that basically,  means the probability that this result is infected from random fluctuation.
#c59	the fact;the average;us;anything;we;a P value;the probability;this result;random fluctuation
#s60	In this case probability is 1.
#c60	this case;probability
#s61	It means it surely is random fluctuation.
#c61	It;it;random fluctuation
#s62	Now in Wilcoxon test, the nonparametric test and we would be not only looking at the science will be also looking at the magnitude of the difference, but we can draw a similar conclusion where you say it's very likely to be from random.
#c62	Wilcoxon test;we;the science;the magnitude;the difference;we;a similar conclusion;you;it
#s63	So to illustrate this, let's think about the such a distribution and this is called the null distribution.
#c63	's;the such a distribution;the null distribution
#s64	We assume that the mean is 0 here.
#c64	We;the mean
#s65	This say we start with the assumption that there's no difference between the two systems.
#c65	we;the assumption;no difference;the two systems
#s66	But we assume that because of random fluctuations depending on the queries.
#c66	we;random fluctuations;the queries
#s67	We might observe a difference, so the actual difference might be on the left side here or on the right side here, right, so?
#c67	We;a difference;the actual difference;the left side;the right side
#s68	And this curve kind of shows the probability that we will actually observe values that are deviating from zero here.
#c68	this curve;the probability;we;values
#s69	Now, so if we look at this picture, then we see that.
#c69	we;this picture;we
#s70	If a difference is observed here, then the chance is very high that this is in fact a random observation, right?
#c70	a difference;the chance;fact;a random observation
#s71	We can define a region of you know likely observation because of random fluctuation, and this is 95% of all the outcomes and in this interval then the observed values may still be from random fluctuation.
#c71	We;a region;you;likely observation;random fluctuation;95%;all the outcomes;this interval;the observed values;random fluctuation
#s72	But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation, right?
#c72	you;a value;this region;a difference;this side;the difference;random fluctuation
#s73	So there's a very small probability that you will observe such a difference just because of random fluctuation.
#c73	a very small probability;you;such a difference;random fluctuation
#s74	So in that case we can then conclude the difference must be real.
#c74	that case;we;the difference
#s75	So, System B is indeed better.
#c75	System B
#s76	So this is the idea of statistical significance test.
#c76	the idea;statistical significance test
#s77	The takeaway message here is that you have to use many queries to avoid jumping into a conclusion, as in this case to say System B is better.
#c77	The takeaway message;you;many queries;a conclusion;this case;System B
#s78	There are many different ways of doing this statistical significance test.
#c78	many different ways;this statistical significance test
#s79	So, now let's talk about the other problem of making judgments.
#c79	's;the other problem;judgments
#s80	And as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set.
#c80	we;it;all the documents;it;a very small data
#s81	So the question is if we can afford judging all the documents in the collection, which is subset, should we judge?
#c81	the question;we;all the documents;the collection;we
#s82	And the solution here is pooling and this is a strategy that has been used in many cases to solve this problem.
#c82	the solution;a strategy;many cases;this problem
#s83	So the idea of pulling is the following.
#c83	the idea;the following
#s84	We would first choose a diverse set of ranking methods.
#c84	We;a diverse set;ranking methods
#s85	These are text retrieval systems.
#c85	text retrieval systems
#s86	And we hope these methods can help us nominate likely relevant documents.
#c86	we;these methods;us;likely relevant documents
#s87	So the goal is to figure out the relevant documents we want to make judgments on relevant documents, because those are the most useful documents from users perspective.
#c87	the goal;the relevant documents;we;judgments;relevant documents;the most useful documents;users perspective
#s88	So then we're going to have each to return top K documents.
#c88	we;top K documents
#s89	"The ""K"" can vary from systems right?"
#c89	The ""K;systems
#s90	But the point is to ask them to suggest the most likely relevant documents.
#c90	the point;them;the most likely relevant documents
#s91	And then we simply combine all these top K sets to form a pool of documents for human assessors, to judge.
#c91	we;all these top K sets;a pool;documents;human assessors
#s92	So imagine you have many systems, each will return K documents, will take the top K documents and we formed the union.
#c92	you;many systems;K documents;the top K documents;we;the union
#s93	Now, of course there are many documents that are duplicated bcause many systems might have retrieved the same relevamnt documents.
#c93	course;many documents;many systems;the same relevamnt documents
#s94	So there will be some duplicate documents and there are also unique documents that are only returned by one system and so the idea of having diverse set of ranking methods is to ensure the pool is broad and can include as many possible relevant documents as possible.
#c94	some duplicate documents;unique documents;one system;the idea;diverse set;ranking methods;the pool;as many possible relevant documents
#s95	And then the users would.
#c95	the users
#s96	Human assistance would make a completely judgment on this data set this pool.
#c96	Human assistance;a completely judgment;this data;this pool
#s97	And the other unjudged documents are usually just assumed to be non relevant.
#c97	the other unjudged documents
#s98	Now if the pool is large enough, this assumption is OK.
#c98	the pool;this assumption
#s99	But the if the pool is not very large, this actually has to be reconsidered and we might use other strategies to deal with them, and there are indeed other methods to handle such cases, and such a strategy is generally ok for comparing systems that contributed to the pool.
#c99	the pool;we;other strategies;them;other methods;such cases;such a strategy;systems;the pool
#s100	That means if you participated in contributing to the pool, then it's unlikely that it will penalize your system because the top ranked documents have all been judged.
#c100	you;the pool;it;it;your system;the top
#s101	However, this is problematic for evaluating a new system that may have not contributed to the pool.
#c101	a new system;the pool
#s102	In this case, a new system might be penalized because it might have nominated some relevant documents that have not been judged, so those documents might be assumed to be non relevant.
#c102	this case;a new system;it;some relevant documents;those documents
#s103	That's unfair.
#s104	So to summarize, the whole part of text retrieval evaluation, it's extremely important because the problem is empirically defined problem.
#c104	text retrieval evaluation;it;the problem;problem
#s105	If we don't rely on users, there's no way to tell whether one method works better.
#c105	we;users;no way;one method
#s106	If we have inappropriate experiment design, we might misguide our research or applications and we might just draw wrong conclusions.
#c106	we;inappropriate experiment design;we;our research;applications;we;wrong conclusions
#s107	And we have seen this in some of our discussion, so make sure to get it right for your research or application.
#c107	we;our discussion;it;your research;application
#s108	The main methodology is Cranfield evaluation methodology and this is still the main paradigm used in all kinds of empirical evaluation tasks, not just the search engine evaluation.
#c108	The main methodology;Cranfield evaluation methodology;the main paradigm;all kinds;empirical evaluation tasks;not just the search engine evaluation
#s109	MAP and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms.
#c109	MAP;nDCG;the two main measures;they;ranking algorithms
#s110	You will see them often in research papers.
#c110	You;them;research papers
#s111	Pricision at the 10 documents is easier to interpret from the user's perspective, so that's also often useful.
#c111	Pricision;the 10 documents;the user's perspective
#s112	What's not covered is Some other evaluation strategy like A-B Test where the system would mix 2, the results of two methods randomly and then will show the mixed results to users.
#c112	What;Some other evaluation strategy;A-B Test;the system;the results;two methods;the mixed results;users
#s113	Of course the users don't see and which result is from which method the users would judge those results or click on those documents in search engine application.
#c113	the users;which result;which method;the users;those results;those documents;search engine application
#s114	In this case then the search engine can keep track of the, clicked documents and see if one method has contributed more.
#c114	this case;the search engine;track;the, clicked documents;one method
#s115	through the clicked documents, if the user tends to click on one, the results from one method, then it's just that the method may may be better, so this is the leverage the real users of a search engine to do evaluation.
#c115	the clicked documents;the user;one method;it;the method;the leverage;the real users;a search engine;evaluation
#s116	It's called A-B Test, and it's a strategy that's often used by the modern search engines.
#c116	It;A-B Test;it;a strategy;the modern search engines
#s117	Commercial search engines.
#c117	Commercial search engines
#s118	Another way to evaluate IR or Text retrieval is user studies, and we haven't covered that.
#c118	Another way;IR or Text retrieval;user studies;we
#s119	I've put some references here that you can look at if you want to know more about that.
#c119	I;some references;you;you
#s120	So there are three additional readings here.
#c120	three additional readings
#s121	These are three mini books about evaluation and they all excellent in covering a broad review of information retrieval, evaluation, and discovered some of the things that we discussed.
#c121	three mini books;evaluation;they;a broad review;information retrieval;evaluation;the things;we
#s122	But they also have a lot of others to offer.
#c122	they;a lot;others
410	f64adab4-578a-4868-8b2c-03fdd4ddf55d	72
#s1	This lecture is a continued discussion of generative probabilistic models for text clustering.
#c1	This lecture;a continued discussion;generative probabilistic models;text clustering
#s2	In this lecture, we're going to continue talking about the tax capture text clustering, particularly "generative
#c2	this lecture;we;the tax capture text clustering
#s3	So this is a slide that you have seen earlier where we have written down the likelihood function for a document.
#c3	a slide;you;we;the likelihood function;a document
#s4	With two distributions in two component mixture model for document clustering.
#c4	two distributions;two component mixture model;document clustering
#s5	Now in this lecture, we're going to generalize this to include the K clusters.
#c5	this lecture;we;the K clusters
#s6	Now if you look at the formula and think about the question how to generalize it, you will realize that all we need is to add more terms like what you have seen here.
#c6	you;the formula;the question;it;you;we;more terms;what;you
#s7	So you can just add more thetas and the probabilities of thetas and the probabilities of generating D from those thetas.
#c7	you;more thetas;the probabilities;thetas;the probabilities;D;those thetas
#s8	So this is precisely what we're going to use.
#c8	precisely what;we
#s9	This is general presentation of the mixture model for document clustering.
#c9	general presentation;the mixture model;document clustering
#s10	So as more cases we follow these steps using a generated model.
#c10	more cases;we;these steps;a generated model
#s11	First think about our data, right?
#c11	our data
#s12	So in this case our data is a collection of documents
#c12	this case;our data;a collection;documents
#s13	N documents denoted by the sub I.
#c13	N documents;the sub;I.
#s14	And then we talk about the model.
#c14	we;the model
#s15	Think about the model.
#c15	the model
#s16	In this case, we design a mixture of K unigram language models.
#c16	this case;we;a mixture;K unigram language models
#s17	It's a little bit different from the topic model.
#c17	It;the topic model
#s18	But we have similar parameters.
#c18	we;similar parameters
#s19	We have a set of theta i's denote the word distributions corresponding to the K unigram language models.
#c19	We;a set;theta;i;the word distributions;the K unigram language models
#s20	We have P of each theta I as the probability of selecting each of the K distributions to generate the document.
#c20	We;P;each theta;I;the probability;the K distributions;the document
#s21	Now note that, although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster.
#c21	our goal;the clusters;we;a more general notion;a probability;each cluster
#s22	And this, as you see later, would allow us to assign a document to the.
#c22	you;us;a document
#s23	Cluster that has the highest probability of being able to generate the document.
#c23	the highest probability;the document
#s24	So as a result, we can also recover some other interesting.
#c24	a result;we
#s25	Properties.
#c25	Properties
#s26	As you will see later.
#c26	you
#s27	So the model basically would make the following assumption about the generation of the document.
#c27	the model;the following assumption;the generation;the document
#s28	We first choose a theta I according to probability of theta I and then generate all the words in the document using this distribution.
#c28	We;a theta;I;probability;theta;I;all the words;the document;this distribution
#s29	Note that it's important that we use this distributed generator.
#c29	it;we;this distributed generator
#s30	All the words in the document.
#c30	All the words;the document
#s31	This is very different from topic model, so the likelihood function would be like what you are seeing here.
#c31	topic model;the likelihood function;what;you
#s32	So the.
#s33	You can take a look at the formula here.
#c33	You;a look;the formula
#s34	We have used the different.
#c34	We
#s35	Notation here in the second line of this.
#c35	Notation;the second line
#s36	Of this equation.
#c36	this equation
#s37	But you can see now the.
#c37	you
#s38	notation has been changed to use unique word in the vocabulary in the product instead of particular position in the document.
#c38	notation;unique word;the vocabulary;the product;particular position;the document
#s39	So from X sub J to W is a change of notation, and this change allows us to show the estimation formulas more easily and you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words.
#c39	X sub J;W;a change;notation;this change;us;the estimation;you;this change;the topic model presentation;it;just a product;the probabilities;all the words
#s40	I and so with the lack of functioning.
#c40	I;the lack;functioning
#s41	Now we can talk about how to do parameter estimation.
#c41	we;parameter estimation
#s42	Here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model.
#c42	we;the maximum likelihood estimator;just a standard way;things;you;it;just a different model
#s43	So after we have estimate the parameters, how can we then allocate clusters to the documents?
#c43	we;the parameters;we;clusters;the documents
#s44	Let's take a look at this situation more closely, so we just repeated the parameters here.
#c44	's;a look;this situation;we;the parameters
#s45	For this mixture model.
#c45	this mixture model
#s46	Now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right?
#c46	you;what;we;estimate;such a model;we;more information;what;we;clustering
#s47	So theta.
#c47	So theta
#s48	I, for example, represents the content of class I.
#c48	I;example;the content;class I.
#s49	This is actually a byproduct.
#c49	a byproduct
#s50	It helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution.
#c50	It;what;the cluster;the top terms;this cluster;this word distribution
#s51	And they will tell us what the cluster is about.
#c51	they;us;what;the cluster
#s52	An P of theta i can be interpreted as.
#c52	An P;theta;i
#s53	Indicating the size of cluster because it tells us how likely cluster would be used to generate the document.
#c53	the size;cluster;it;us;how likely cluster;the document
#s54	The more likely a cluster is used to generate the document, we can assume the larger the cluster size is.
#c54	The more likely a cluster;the document;we;the cluster size
#s55	Note that unlike in PLSA and this probability of theta I is not dependent on D. Now.
#c55	PLSA;theta;I;D. Now
#s56	You may recall that the topic choice in each document actually depends on D. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents.
#c56	You;the topic choice;each document;D.;each document;a potentially different choice;topics;we;a generic choice probability;all the documents
#s57	But of course, given a particular document that we still have to infer which topic is more likely.
#c57	course;a particular document;we;which topic
#s58	To generate the document so in that sense, we can still have a document dependent probability of clusters.
#c58	the document;that sense;we;a document dependent probability;clusters
#s59	So lets look at a key problem of assigning document to clusters or assigning clusters to documents Lets to compute the C sub D here and this will take one of the values in the range of one to k to indicate which cluster should be assigned to D. Let's first you might think about a way to use likelihood only, and that is to assign D to the cluster corresponding to the topic Theta I. That most likely has been used to generate D.
#c59	a key problem;document;clusters;clusters;documents;Lets;the C sub;D;the values;the range;k;which cluster;D.;'s;you;a way;likelihood;D;the cluster;the topic;Theta I.;D.
#s60	So that means we're going to choose one of those distributions that gives D highest probability.
#c60	we;those distributions;D highest probability
#s61	In other words, we see which distribution has a content that matches our D best.
#c61	other words;we;which distribution;a content;our D
#s62	Intuitively, that makes sense.
#c62	sense
#s63	However, this approach does not consider the size of clusters, which is also available to us.
#c63	this approach;the size;clusters;us
#s64	And so a better way is to use the likelihood together with the prior.
#c64	a better way;the likelihood;the prior
#s65	In this case the prior is P of Theta I. And together, that is, we're going to use the base formula to compute the posterior probability of Theta given D.
#c65	this case;the prior;P;Theta I.;we;the base formula;the posterior probability;Theta;D.
#s66	And if we choose theta based on this posterior probability and we would have the following formula that you see here.
#c66	we;theta;this posterior probability;we;the following formula;you
#s67	On the bottom of this slide, and in this case, we're going to choose the theta that has a large P of Theta I.
#c67	the bottom;this slide;this case;we;the theta;a large P;Theta I.
#s68	That means a large cluster and also a high probability of generating D.
#c68	a large cluster;also a high probability;D.
#s69	So we're going to favor a cluster that's large and also consistent with the document.
#c69	we;a cluster;the document
#s70	And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster.
#c70	sense;the chance;a document;a large cluster;a small cluster
#s71	So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering.
#c71	we;the parameters;the model;we;the problem;document clustering
#s72	So next we have to discuss how to actually compute the estimate of the model.
#c72	we;the estimate;the model
410	f736ee59-8c2e-495d-93c5-c1385c49a44a	7
#s1	this letter is a summary of this cause this map shows the major topics we have covered in this cause and here are some key high level takeaway messages first we talked about the metro media content analysis here the main takeaway messages natural language processing the foundation for text retrieval but currently NF he is in the robust enough so the bag of words representation is generally the main method used in modern search engines and it's often sufficient for most of the search tasks but obviously for more complex such tasks we need a deeper natural language processing techniques and we then talked about the high level strategies for text for access we talked about the push pull in poor we talked about the querying with this browsing now in general in future search engines we should integrate all these techniques to provide a multi bold information access and then we talked about a number of issues related to search engines without power to the search problem and we frame that as a ranking problem and we talked about a number of retrieval methods and started with the overview of vector space model and the probabilistic model and then we talked about the vector space model impacts we also later talked about language modeling approach that's probabilistic model and hear the main takeaway messages that modeling retrieval functions tend to look similar and they generally use where is heuristics most important ones are TF IDF weighting document length normalization in the TF is often transformed through seven linear transformation function and then we talked about how to implement retrieval system and here the main techniques that we talked about how to construct the inverted index so that we can prepare the system to answer query quickly and we talked about how to faster search by using the inverted index an we then talked about how to evaluate the text retrieval system main introduced the cran field evaluation methodology this was a very important evaluation methodology that can be applied to many tasks we talked about the major evaluation measures so the most important measures for search engine map mean average precision and end DCG normalized discounted cumulative gain an also precision and recorder two basic measures and we then talked about the feedback techniques an we talked about the rock you in the vector space model and the mixture model in the language modeling approach feedback is very important technique especially considering the opportunity of learning from a lot of clicks rules on the web we then talked about web search an here we talked about how to use parallel indexing to solve the scalability issue in indexing we introduce the mapreduce
#c1	this letter;a summary;this map;the major topics;we;this cause;some key high level takeaway messages;we;the metro media content analysis;the main takeaway;natural language;the foundation;text retrieval;he;the bag;words;representation;the main method;modern search engines;it;the search tasks;more complex such tasks;we;a deeper natural language processing techniques;we;the high level strategies;text;access;we;the push pull;we;the querying;future search engines;we;all these techniques;a multi bold information access;we;a number;issues;search engines;power;the search problem;we;a ranking problem;we;a number;retrieval methods;the overview;vector space model;the probabilistic model;we;the vector space model impacts;we;language modeling approach;probabilistic model;the main takeaway;retrieval functions;they;heuristics most important ones;TF IDF weighting document length normalization;the TF;seven linear transformation function;we;retrieval system;we;the inverted index;we;the system;query;we;how to faster search;the inverted index;we;the text retrieval system main;the cran field evaluation methodology;a very important evaluation methodology;many tasks;we;the major evaluation measures;the most important measures;search engine map;mean average precision;end;DCG;cumulative gain;an also precision;two basic measures;we;the feedback techniques;we;the rock;you;the vector space model;the mixture model;the language modeling approach feedback;very important technique;the opportunity;a lot;clicks rules;the web;we;web search;we;parallel indexing;the scalability issue;indexing;we;the mapreduce
#s2	and then we talked about how to use linked information on the web to improve search we talk about page rank and hits as the major algorithms to analyze links on the web we then talked about learning to rank this is use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach
#c2	we;linked information;the web;search;we;page rank;hits;the major algorithms;links;the web;we;use;multiple features;scoring;not only the effectiveness;this approach
#s3	but we can also improve the robustness of the ranking function so that it's not easy to spam or search engine with just some features through promote a page and finally we talked about the future of web search without about some major directions that we might have seen in the future improving currently generation of search engines and then finally we talked about recommended systems and these are systems to implement the push mode
#c3	we;the robustness;the ranking function;it;search engine;just some features;a page;we;the future;web search;about some major directions;we;the future;currently generation;search engines;we;recommended systems;systems;the push mode
#s4	and we talked about the two approaches one is content based one is a collaborative filtering and they can be combined together now an obvious listen tastes in this picture is the user can see so user interface is also important component in any search engine even though the current are searching face is relatively simple they actually have been a lot of the studies of user interface is relative to visualization for example and this is a topic that you can learn more by reading this book
#c4	we;the two approaches;a collaborative filtering;they;an obvious listen tastes;this picture;the user;user interface;important component;any search engine;the current;face;they;a lot;the studies;user interface;visualization;example;a topic;you;this book
#s5	so X men the book about all kinds of studies of search using the face if you want to know more about the topics that we talked about you can also read some additional ratings that are listed here in this short course we only manage a little cover some basically topics in text retrieval and search endings and these resources provide additional information about more advanced topics and they gave more feral treatment of some of the topics that we talked about and a main source is synthesis digital library where you can see a lot of short textbook oh textbooks or long tutorials they tend to provide a lot of information to explain a topic and there are multiple series is that are related to this calls one of information concepts retrieval and services another is human language technology and yet another 's artificial intelligence and machine learning there are also some major journals and conferences listed here that tend to have a lot of research papers related to the topic of this course and finally for more information about the resources including readings and toolkits etc you can check out this UI
#c5	X men;the book;all kinds;studies;search;the face;you;the topics;we;you;some additional ratings;this short course;we;a little cover;some basically topics;text retrieval;search endings;these resources;additional information;more advanced topics;they;more feral treatment;the topics;we;a main source;synthesis digital library;you;a lot;short textbook;they;a lot;information;a topic;multiple series;information concepts retrieval;services;human language technology;yet another 's artificial intelligence;machine;some major journals;conferences;a lot;research papers;the topic;this course;more information;the resources;readings;toolkits;you;this UI
#s6	so if you have not taken the text mining calls in this data mining specialization series then naturally the next step is to take that cause as this picture shows to mind big text there are we generally need two kinds of techniques one is text retrieval which is covering these costs and these techniques will help us converter or big text there are into small relevant the text data which you are actually needed in the specific application human plays important role in mining any tax there are becaus text data is written for humans to consume so involving humans in the process of data mining is very important and in this cause we have covered various strategies to help users get access to the most relevant data these techniques also essentially text mining system to help provide provenance to help users interpret the independence that user defined through texture data mining so in general the user would have to go back to the original data to better understand the patterns so the text mining calls rather text mining an analytics course would be dealing with what to do once the user has found information so this is the second step in this picture where we would convert that extra data into actionable knowledge and this has to do with helping users it'll further digest the founding formation or find the patterns and to reveal knowledge buried in text an such knowledge can then be used in application system to help decision making all to help user finish your task
#c6	you;the text mining calls;this data mining specialization series;the next step;that cause;this picture;big text;we;two kinds;techniques;text retrieval;these costs;us;converter;big text;small relevant the text data;you;the specific application human;important role;any tax;becaus text data;humans;humans;the process;data mining;this cause;we;various strategies;users;access;the most relevant data;these techniques;also essentially text mining system;provenance;users;the independence;user;texture data mining;the user;the original data;the patterns;the text mining calls;an analytics course;what;the user;information;the second step;this picture;we;extra data;actionable knowledge;users;it;the founding formation;the patterns;knowledge;text;an such knowledge;application system;user;your task
#s7	so if you have not taken that cause the natural step in the natural makes the step would be to take that course thank you for taking this course i hope you have found this caused be useful to you and i look forward to interacting with you at a future opportunity
#c7	you;the natural step;the natural;the step;that course;you;this course;i;you;you;i;you;a future opportunity
410	faaa4dbf-c3bb-44d1-ac49-b04834a20e2d	9
#s1	this lecture is about the evaluation of text retrieval systems in the previous lectures we have talked about a number of cattle retrieval methods different kinds of ranking functions but how do we know which one works the best in order to answer this question we have to compare them and that means we have to evaluate these retrieval methods so this is the main topic of this laptop first really think about the why do we have to do evaluation i already given one reason that is we have to use devaluation to figure out which retrieval method works better now this is very important for advancing our knowledge otherwise we wouldn't know whether a new idea works better than all the idea in the beginning of this cause we talked about the problem of text refillable we compare it with database retrieval there we mentioned that text retrieval is empirically defined the problem so evaluation must rely on users which is system works better with have to be judged by our users
#c1	this lecture;the evaluation;text retrieval systems;the previous lectures;we;a number;cattle retrieval methods;different kinds;ranking functions;we;one;order;this question;we;them;we;these retrieval methods;the main topic;this laptop;we;evaluation;i;one reason;we;devaluation;which retrieval method;our knowledge;we;a new idea;all the idea;the beginning;we;the problem;text refillable;we;it;database retrieval;we;text retrieval;the problem;evaluation;users;system;our users
#s2	so this becomes a very challenging problem becaus how can we get users involved in the evaluation how can we do a fair comparison of different methods so just go back to the reasons for evaluation i just didn't two reasons here the second reason is basically what i just said but there is also another reason which is to assess the actual utility of attacks retrieval system not imagine you're building your own search any applications it would be interested in knowing how well your search engine works for your users so in this case measures must reflect the utility to the actual users in a real application and typically this has to be done by using user studies and using the real search engine being the signal case or for the second reason the mesh shoes actually own need to be correlated with the utility to actual users that they don't have to accurately reflect the exactly utility to users so the measure only needs to be good enough to tell which method works better and this is usually down through a test collection and this is the main idea that will be talking about this calls this has been very important for comparing different algorithms and for improving a search engine system in general
#c2	a very challenging problem becaus;we;users;the evaluation;we;a fair comparison;different methods;the reasons;evaluation;i;n't two reasons;the second reason;what;i;another reason;the actual utility;attacks retrieval system;you;your own search;any applications;it;your search engine;your users;this case;measures;the utility;the actual users;a real application;user studies;the real search engine;the signal case;the second reason;the mesh shoes;need;the utility;actual users;they;the exactly utility;users;the measure;which method;a test collection;the main idea;this calls;different algorithms;a search engine system
#s3	so next we talk about what to measure right there are many aspects of a search engine that we can measure we can evaluate and here i listed the three major aspects one is effectiveness so accuracy how active the other search results in this case we're measuring systems capability of ranking relevant documents on top of mount read in the ones the second is the efficiency how quickly can i use are getting results how much are computing resources are needed to answer a query so in this case we need to measure the space and time overhead of the system the third aspect is usability basically the question is how useful is the system for real user tasks here obviously interfaces and many other things also important and we typically would have to do using studies now in this cause we're going to talk mostly about effectiveness an accuracy measures becaus the efficiency and usability dimensions are not really unique to search engines and so they are needed for evaluating any other software systems and there is also good coverage of such materials in other causes but how to evaluate a search engines quality accuracy is something unique to text retrieval and we're going to talk a lot about this the main idea that people have proposed before using attests attitude evaluate text retrieval algorithm is called the cranfield evaluation methodology this one actually was developed the long time ago in ninety six days its methodology for laboratory test of system components
#c3	we;what;many aspects;a search engine;we;we;i;the three major aspects;effectiveness;this case;we;systems capability;relevant documents;top;mount;the ones;the efficiency;i;results;how much are computing resources;a query;this case;we;the space;time;the system;the third aspect;usability;the question;the system;real user tasks;we;studies;this cause;we;effectiveness;an accuracy measures;the efficiency and usability dimensions;search engines;they;any other software systems;good coverage;such materials;other causes;a search engines;quality accuracy;something;text retrieval;we;a lot;the main idea;people;attests attitude;text retrieval algorithm;the cranfield evaluation methodology;this one;ninety six days;its methodology;laboratory test;system components
#s4	it's actually methodology that has been very useful not just before search engine evaluation but also for evaluating virtually all kinds of empirical tasks and for example in natural language processing or in other fields where the problems in paragraph defined with people you would need to use such a methodology and today with big data challenge with use of machine learning everywhere we this methodology has been very popular
#c4	it;methodology;search engine evaluation;virtually all kinds;empirical tasks;example;natural language processing;other fields;the problems;paragraph;people;you;such a methodology;big data challenge;use;machine learning;we;this methodology
#s5	but it was first available for search engine application in nineteen sixties so the basic idea of this approach is repealed are reusable test collections and define measures once such a test class and is build it can be used again and again to test the different algorithms
#c5	it;search engine application;nineteen sixties;the basic idea;this approach;reusable test collections;measures;such a test class;it;the different algorithms
#s6	and we're going to define measures that would allow you to quantify the performance of system or an algorithm so how exactly would this work
#c6	we;measures;you;the performance;system;an algorithm
#s7	well we're going to have a sample clashing of documents and this is just to simulate the real document collection you know search application we're going to also have a sample set of queries or topics this is a simulated the users queries then we'll have to have relevance judgments these are judgments of which document that should be returned for which queries ideally they have to be made by users who formulated the queries 'cause those are the people that know exactly what documents would be useful and then finally we have to have measures to quantify how well a systems result that matches the ideal rent list that would be constructed based on users relevance judgments so this methodology is very useful for starting retrieval algorithms becaus the tester crashing can be reused many times and it would also provide a fair comparison for all the methods we have the same criteria same that are set to be used to compare different algorithms this allows us to compare a new algorithm with an older algorithm that was divided with many years ago by using the same standard so this is the illustration of how this works so as i said we need a queries assume here we have two one two two etc we also need a documents that's called the document clashing and on the right side you see we need relevance judgments these are basically the binary judgments of documents with respect to a query so for example D wise judge it as being relevant to Q R D two is judged as being rather than the as well and T three is judging as non relevant the two two one etc these would be created by users but we also have these and then we basically have attested collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results let's say if the query is Q one and then we would have results here here i show ask subway as results from system
#c7	we;a sample clashing;documents;the real document collection;you;search application;we;a sample set;queries;topics;the users;we;relevance judgments;judgments;which document;they;users;who;the queries;the people;exactly what documents;we;measures;a systems;the ideal rent list;users relevance judgments;this methodology;retrieval algorithms becaus;the tester;it;a fair comparison;all the methods;we;the same criteria;different algorithms;us;a new algorithm;an older algorithm;the same standard;the illustration;i;we;a queries;we;we;a documents;the document;the right side;you;we;relevance judgments;the binary judgments;documents;respect;a query;example;it;Q R D;T;users;we;we;collection;you;two systems;you;them;you;each system;these queries;documents;each system;results;'s;the query;Q;we;results;i;subway;results;system
#s8	so this is remember we talked about task of computing approximation of relevant documents that are subway is system is approximation here anne also be is system bees approximation of relevant documents now let's take a look at these results so which is better now imagine for user which one would you like let's take a look at both results and there are some differences and there are some documents that are returned by both systems but if you look at the results you would feel that well maybe a is better in the sense that we don't have many non relevant documents an amount of three documents return the two of them are relevant so that's good
#c8	we;task;computing approximation;relevant documents;subway;system;approximation;anne;system bees approximation;relevant documents;'s;a look;these results;user;which one;you;'s;a look;both results;some differences;some documents;both systems;you;the results;you;the sense;we;many non relevant documents;an amount;three documents;them
#s9	it's precise on the other hand one council say maybe bees better becaus we've got more relevant documents we've got three instead of two so which one is better and how do we quantify this well obviously this question highly depends on a user 's task and it depends on users as well you might be able to imagine for some users may be system way is better if the user is not interested in getting all the random in the document like in this case this is the user doesn't have to read a million the user would see most of the relevant documents on the one hand and one can also imagine user might need to have as many relevant documents as possible for example if you're doing a literature survey you might be in the second category and you might find that system B is better so in that case we will have to also define measures that would qualify them and we might need to define multiple measures becaus users have different perspectives of looking at the results
#c9	it;the other hand;one council;we;more relevant documents;we;one;we;this question;a user 's task;it;users;you;some users;system way;the user;all the random;the document;this case;the user;the user;the relevant documents;the one hand;one;user;as many relevant documents;example;you;a literature survey;you;the second category;you;system B;that case;we;measures;them;we;multiple measures;becaus users;different perspectives;the results
410	fd7efa33-8e41-4bdc-9fd1-613a8f92bfd6	97
#s1	So now let's take a look at the specific  Method that's based on regression.
#c1	's;a look;the specific  Method;regression
#s2	Now this is one of the many different methods.
#c2	the many different methods
#s3	In fact, it's one of the simplest methods and I choose this to explain the idea because it's simple.
#c3	fact;it;the simplest methods;I;the idea;it
#s4	So in this approach, we simply assume that the relevance of document with respect to query is related to a linear combination of all the features.
#c4	this approach;we;the relevance;document;respect;query;a linear combination;all the features
#s5	Here are used Xi to denote the feature, so Xi of Q and D is a feature.
#c5	Xi;the feature;Xi;Q;D;a feature
#s6	And we can have as many features as we would like.
#c6	we;as many features;we
#s7	And we assume that these features can be combined in a linear manner.
#c7	we;these features;a linear manner
#s8	And each feature is controlled by a parameter here.
#c8	each feature;a parameter
#s9	And beta is a parameter that's a weighting parameter, a larger value would mean the feature would have a higher weight and would contribute more to the scoring function.
#c9	beta;a parameter;a weighting parameter;a larger value;the feature;a higher weight;the scoring function
#s10	The specific form of the function actually also involves a transformation of the probability of relevance.
#c10	The specific form;the function;a transformation;the probability;relevance
#s11	So this is the probability of relevance.
#c11	the probability;relevance
#s12	We know that the probability of relevance is within the range from 0 to 1.
#c12	We;the probability;relevance;the range
#s13	And we could have just assumed the scoring function is related to this linear combination, right?
#c13	we;the scoring function;this linear combination
#s14	So we can do a linear regression, but then the value of this linear combination could easily go beyond one, so this transformation here would map the zero to 1 range to the whole range of real values.
#c14	we;a linear regression;the value;this linear combination;this transformation;1 range;the whole range;real values
#s15	You can verify it by yourself.
#c15	You;it;yourself
#s16	So.
#s17	This allows us then to connect the probability of relevance which is between zero and one to a linear combination of arbitrary features.
#c17	us;the probability;relevance;a linear combination;arbitrary features
#s18	And if we rewrite this into a probability function we would get the next one, so on this on this equation then we will have the probability of relevance.
#c18	we;a probability function;we;this equation;we;the probability;relevance
#s19	And on the right hand side we would have this form.
#c19	the right hand side;we;this form
#s20	Now this form is clearly non negative and it still involves the linear combination of features.
#c20	this form;it;the linear combination;features
#s21	And it's also clear that if this value is, this is actually negative of the linear combination in the equation above, if this, This value here.
#c21	it;this value;the linear combination;the equation
#s22	If.
#s23	This value is large, then it would mean this value is small and therefore this probability this whole probability would be large and that's what we expect.
#c23	This value;it;this value;this probability;this whole probability;what;we
#s24	Basically it would mean if this combination gives us a high value, then the documents more likely relevant.
#c24	it;this combination;us;a high value;the documents
#s25	So this is our hypothesis.
#c25	our hypothesis
#s26	Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance.
#c26	the best hypothesis;a simple way;these features;the probability;relevance
#s27	So now we have this combination function.
#c27	we;this combination function
#s28	The next task is to see how we to estimate the parameters so that the function can actually be applied without knowing the beta values, it's harder to apply this function, so let's see how we can estimate beta values.
#c28	The next task;we;the parameters;the function;the beta values;it;this function;'s;we;beta values
#s29	Let's take a look at a simple example.
#c29	's;a look;a simple example
#s30	In this example we have 3 features.
#c30	this example;we;3 features
#s31	One is BM25 score of the document and query one is the page rank score of the document, which might or might not depend on the query.
#c31	BM25 score;the document;query;the page rank score;the document;the query
#s32	We might have a topic sensitive Pagerank that would depend on query.
#c32	We;a topic;sensitive Pagerank;query
#s33	Otherwise the general page rank doesn't really depend on query and then we have BM25 score on the anchor text of the document.
#c33	the general page rank;query;we;BM25 score;the anchor text;the document
#s34	These are then the feature values for a particular Doc document query pair.
#c34	the feature values;a particular Doc document query pair
#s35	An in this case, the document is D1, and the judgment that says that is relevant.
#c35	this case;the document;D1;the judgment
#s36	Here's another training instance and with these feature values.
#c36	another training instance;these feature values
#s37	But in this case it's not relevant.
#c37	this case;it
#s38	OK, this is overly simplified case where we just have two instances.
#c38	overly simplified case;we;two instances
#s39	But it is sufficient to illustrate the point.
#c39	it;the point
#s40	So what we can do is we use the maximum likelihood estimator to actually estimate the parameters.
#c40	what;we;we;the maximum likelihood estimator;the parameters
#s41	Basically, we are going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here.
#c41	we;the relevance status;the document;the feature values;we;these feature values
#s42	Can we predict the relevance?
#c42	we;the relevance
#s43	Yeah, and of course the prediction will be using this function.
#c43	course;the prediction;this function
#s44	That you see here.
#c44	you
#s45	And we hypothesize that the probability of relevance is related to features in this way, so we're going to see for what values of beta.
#c45	we;the probability;relevance;features;this way;we;what values;beta
#s46	We can predict the relevance well.
#c46	We;the relevance
#s47	What do we?
#c47	What;we
#s48	What do we mean by predicting the relevance well?
#c48	What;we;the relevance
#s49	we just mean in the first case for D1.
#c49	we;the first case;D1
#s50	This expression here right here should give a high values.
#c50	This expression;a high values
#s51	In fact, we hope this to give a value close to one, why?
#c51	fact;we;a value
#s52	because this is a relevant document.
#c52	a relevant document
#s53	On the other hand, in the second case for D2, we hope this value will be small.
#c53	the other hand;the second case;D2;we;this value
#s54	Why?
#s55	Because It's a non relevant document.
#c55	It;a non relevant document
#s56	So now let's see how this can be mathematically expressed.
#c56	's
#s57	This is similar to expressing the probability of document.
#c57	the probability;document
#s58	Only that we're not talking about the probability of words, but talking about probability of relevance one or zero.
#c58	we;the probability;words;probability;relevance
#s59	So what's the probability of this document?
#c59	what;the probability;this document
#s60	The relevant if it has these feature values.
#c60	it;these feature values
#s61	This is just this expression, right?
#c61	just this expression
#s62	We just need to plug in the Xi(s).
#c62	We;the Xi(s
#s63	So that's what we will get.
#c63	what;we
#s64	It's exactly like.
#c64	It
#s65	What we have seen above, only that we replaced these Xi(s) with now specific values, right?
#c65	What;we;we;these Xi(s;now specific values
#s66	So for example, this .7  goes to here and this .11. goes to here.
#c66	example
#s67	And these are different feature values and we combine them in this particular way.
#c67	different feature values;we;them;this particular way
#s68	The beta values are still unknown, but this gives us the probability that this document is relevant if we assume such a model.
#c68	The beta values;us;the probability;this document;we;such a model
#s69	OK, we would want to maximize this probability, since this is a relevant document.
#c69	we;this probability;a relevant document
#s70	What we do for the second document?
#c70	What;we;the second document
#s71	Well, we want to compute the probability that the prediction is non relevant.
#c71	we;the probability;the prediction
#s72	So.
#s73	This would mean we have to compute 1 minus.
#c73	we;1 minus
#s74	This expression.
#c74	This expression
#s75	Since this expression.
#c75	this expression
#s76	Is actually the probability of relevance.
#c76	the probability;relevance
#s77	So to compute the non relevance from relevance we just.
#c77	the non relevance;relevance;we
#s78	Do 1 minus the probability of relevance.
#c78	the probability;relevance
#s79	OK.
#s80	So this whole expression then just is our probability of predicting these two.
#c80	this whole expression;our probability
#s81	Relevance values one is 1 here, one is zero and this whole equation is our probability.
#c81	Relevance values;this whole equation;our probability
#s82	Of observing a 1 here.
#s83	And observing a zero here.
#s84	Of course, this probability depends on the beta values.
#c84	this probability;the beta values
#s85	Right so then our goal is to adjust the beta values to make this whole thing reach its maximum.
#c85	our goal;the beta values;this whole thing;its maximum
#s86	Make it as large as possible.
#c86	it
#s87	So that means we're going to compute this.
#c87	we
#s88	The beta is just the parameter values that would maximize this whole likelihood expression.
#c88	The beta;just the parameter values;this whole likelihood expression
#s89	And what that means is if look at the function is we are going to choose betas to make this as large as possible and make this.
#c89	what;the function;we;betas
#s90	Also, as large as possible, which is equivalent to, say, make this the part as small as possible.
#c90	this the part
#s91	And this is precisely what we want.
#c91	precisely what;we
#s92	So once we do the training now we will know the beta values.
#c92	we;the training;we;the beta values
#s93	So then this function would be well defined once beta values are known.
#c93	this function;beta values
#s94	Both this and this.
#s95	Would be completely specified, so for any new query and new documents, we can simply compute the features.
#c95	any new query;new documents;we;the features
#s96	For that pair and then we just use this formula to generate the ranking score and this scoring function can be used to rank documents for a particular query.
#c96	that pair;we;this formula;the ranking score;this scoring function;documents;a particular query
#s97	So that's the basic idea of learning to rank.
#c97	the basic idea
