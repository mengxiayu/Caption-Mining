410	01c0f454-a1bb-450d-babe-6201b372d9e9	So I showed you how we rewrite the query likelihood retrieval function into a form that looks like the formula of this slide. After we make the assumption about the smoothing the language model. Based on the collection language model. If you look at the this rewriting it actually would give us two benefits. The first benefit is it helps us better understand this ranking function. In particular, we're going to show that from this formula we can see smoothing with the collection language model will give us something like a TF IDF weighting and length normalization. The second benefit is that it also allows us to compute the query likelihood more efficiently. In particular, we see that the main part of the formula is a sum over the matched query terms. So this is much better than if we take a sum over all the words. After we smooth the document language model, we send you to have non zero probabilities for all the words. So this new form of the formula is much easier to score or to compute. It's also interesting to note that the last term here is actually independent of the document, since our goal is to rank the documents for the same query, we can ignore this term for ranking. Because it's going to be the same for all the documents. Ignoring it wouldn't affect the order of the documents. Inside the sum, We also see that each matched query term would contribute. weight And this weight actually is very interesting. because it looks like a TF IDF weighting. First, we can already see it has a frequency of the word in the query, just like in the vector space model. When we take a dot product we see the word frequency in the query to show up in such a sum. And so naturally, this pot would correspond to the vector element from the document vector, and here indeed we can see it actually encodes a weight that has similar factor to TF IDF weighting. I'll let you examine it. Can you see it? Can you see which part is capturing TF? and which part is capturing IDF weighting? So if you want you can pause the video to think more about it. So, have you noticed that this p  of seen is related to the term frequency? In the sense that if a word occurs very frequently in the document, then the estimated probability here would tend to be larger. So this means this term is really doing something like TF weighting. Have you also notice that? This term in the denominator. Is actually achieving the effect of IDF? Why? Because this is the popularity of the term in the collection. But it's in the denominator, so if. "The probability in the collection is   larger, then the weight is actually smaller, and this means a popular term. We actually have a smaller weight and this is precisely what IDF weighting is doing. Only that we now have a different form of TF and IDF. "Remember, IDF document frequency. But here we have something different. But intuitively it achieves a similar fact. Interestingly, we also have something related to the length normalization. Again, can you see which factor is related to the document length? In this formula. I just say that this term is related to IDF weighting. This. This collection probability, but it turns out that this term here is actually related to the document length. Normalization in particular alpha sub d might be related to document. length, so it encodes how much probability mass we want to give to unseen words. How much smoothing do we want to do ? Intuitively, if a document is long then we need to do less smoothing because we can assume that data is large enough. We probably have observed all the words that the author could have written, but the document is short. Then alpha sub d could be expected to be to be large. We need to do more smoothing. It's like that there are words that have not been written yet by the other. So this term appears to penalize long documenting in that the alpha sub d would tend to be longer than larger than. for a long document. But note that the alpha sub d also occurs here. And so this may not actually be necessary. Penalizing long documents effect is not so clear here. But as we will see later when we consider some specific smoothing methods, it turns out that they do penalize long documents just like in TF "IDF weighting  normalization formulas in the vector space model. So that's a very interesting observation, because it means we don't even have to think about the specific way of doing smoothing. We just need to assume that if we smooth with this collection language model, then we would have a formula. That looks like a TF IDF weighting and documents length normalization. What's also interesting is that we have very fixed form of the ranking function. And see we have not heuristically put a logarithm here. In fact, you can think about why we will have a logarithm here. If you look at the assumptions that we have made, it will be clear it's because we have. used logarithm of query likelihood for scoring. And we turned the product into a sum of logarithm of probability and that's why we have this logarithm. Note that if we only want to heuristically implement the TF weight and IDF weighting, we don't necessarily have to have a logarithm here. Imagine if we drop this logarithm, we would still have TF and IDF weighting. But what's nice with probabilistic modeling is that we are automatically given a logarithm function here. And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case, if you try to drop this logarithm, the more of probably one work as well as if you keep logarithm. So a nice property of probabilistic modeling is that by following some assumptions and probability rules we will get a formula automatically. And the formula would have a particular form, like in this case. And if we heuristically design the formula, we may not necessary end up  having such a specific form. So to summarize, we talked about the need for smoothing document language model, otherwise would give zero probability for unseen words in the document. And that's not good for scoring a query with such an unseen world. And it's also necessary in general to improve the accuracy of estimating the model represents the topic of this document. The general idea of smoothing in retrieval is to use the collection language Model to Give us some clue about the which unseen word should have a higher probability. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection. With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has the effect of TF IDF weighting and document length normalization. We also see that through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on match query terms, just like it in the vector space model. But the actual ranking function is given us automatically by the probability rules and the assumptions that we have made and unlike in the vector space model where we have to heuristically. Think about the form of the function. However, we still need to address the question how exactly we should smooth the document language model. How exactly we should use the reference language model based on the collection to adjust the probability of the maximum likelihood estimate? And this is the topic of the next lecture.
410	09a64f72-3fa7-4dc9-8385-498e17d0bd8b	This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model. In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method, and we're going to talk about the specifics smoothing methods used for such a retrieval function. So this is a slide from a previous lecture where we show that with the query likelihood ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following. So this is the retrieval function based on these assumptions that we have discussed, you can see it's a sum over all the matched The query terms here. And inside the sum is the count of the term in the query and some weight. For the term in the document. And we have TF IDF weight here and then we have another constant here in the end So clearly, if we want to implement this function using a program language, we will still need to figure out a few variables. In particular, we're going to need to know how to estimate the probability of a word Exactly and. How do we set alpha? So in order to answer these questions, we have to think about this very specifically, smoothing methods and that is the main topic of this lecture. We are gonna talk about the two smoothing  methods. The first is the simple linear interpolation with a fixed coefficient. And this is also called a Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows how we estimate. Document the language model by using maximum likelihood estimate that gives us word counts normalized by the total number of words in the text. The idea of using this method. Is to maximize the probability of the observed text as a result, if a word like network. Is not observed in the text, it's going to get zero probability as shown here. So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what non zero probability should be assigned to such a word. So we can note that Network has a non zero probability here. So in this approach in what we do is we do a linear interpolation between the maximum likelihood estimate here and the collection language model and this is controlled by the smoothing parameter Lambda. Which is. Between zero and one. So this is a smoothing parameter. The larger lambda is, the more smoothing  we will have. So by mixing them together we achieve the goal of assigning non zero probabilities to a word network. So let's see how it works for some of the words here. For example, if we compute the smooth probability for text. Now the maximum likelihood estimate gives us 10 / 100 and that's going to be here. But the collection probability is this, so we just combine them together with this simple formula. We can also see. The word network which used to have zero probability now is getting a non zero probability. Of this value, and that's because the count is going to be 0 for network. Here, but this part is non zero, and that's basically how this method works. If you think about this and you can easily see now the  Alpha sub D in this smoothing method is basically Lambda. Because that's remember the coefficient in front of the probability of the word given by the collection language model here, right? OK, so this is the first smoothing method. A second one is similar, but it has a dynamic coefficient for linear interpolation. It is often called the Dirichlet Prior or Bayesian smoothing. So again, here we face the problem of zero probability for an unseen word like network. Again, we will use the collection language model, but in this case we're going to combine them in somewhat different ways. The formula first can be seen as an interpolation of the. Maximum likelihood estimate and the collection language model as before as in the JM smoothing method. Only at the coefficient now is not the Lambda a fixed number, but that dynamic coefficient in this form where mu is a parameter. It's a non-negative value. And you can see what if we set the mu to a constant. The effect is that a long document would actually get a smaller coefficient here. Because a long document that will have longer length, therefore, the coefficient is actually smaller. And so a long document would have less smoothy as we would expect. So this seems to make more sense than fixed coefficient smoothing. Of course this part. Would be of this form so that the two coefficients would sum to one. Now this is one way to understand that this smoothing Basically it means it's a dynamic coefficient interpolation  There is another way to understand this formula. Which is even easier to remember, and that's this side. So it's easy to see. We can rewrite the smoothing method in this form. Now in this form we can easily to see what changes we have made to the maximum likelihood estimate which would be this part right? So normalized count by the document length So in this form we can see what we did is we add this to the count of every word. So what does this mean? This is basically. Something related to the probability of the word in the collection, and we multiply that by the parameter mu. And when we combine this with the count here, essentially we are adding pseudo counts. To the observed text. We pretend. Every word has got this many pseudo count. So the total counts would be the sum of these pseudo counts and the actual count. Of the word in the document. As a result, in total we would have added this many pseudo counts why? Because if you take a sum of this. "This one, over all the words that we see,  So this probability would still sum to one.  So in this case we can either see the method. Is essentially to Add these. As pseudo count to this data pretend we actually augment the data by including some pseudo data defined by the collection language model. As a result, we have more counts. I still. The total counts for a word would be like this and as a result even if a word has zero count here let's says we have zero account here and it would still have non zero count because of this part. So this is how this method works. Let's also take a look at some specific example here. Right, so for text again, we will have 10 as an original count that we actually observe, but we also add some pseudo count. And so the probability of the text would be of this form naturally. The probability of network would be just this part. And so here you can also see what's alpha sub d here. Can you see it if you want to think about, you can pause the video. Have you notice that this part is basically alpha sub D? So we can see this case. Alpha sub D does depend on the document. Because. This length depends on the document, whereas in the linear interpolation the JM smoothing method. This is a constant.
410	0be0ce0e-3101-4deb-90c8-e34762c20d57	this lecture is about how to do faster search by using inverted index in this latter we're going to continue the discussion of the system implementation in particular we're going to talk about how to support the faster search by using mercury index so let's think about watt a general scoring function might look like now of course the vector space model is a special case of this but we can imagine many other retrieval functions of the same form so the form of this function is as follows we see this scoring function of document D and query Q is defined as first a function of FA that's adjustment the function that would the consider two factors that are assume here at the end if somebody of the NF subq of Q these are adjustment factors of document and query so they are at the level of a document and query so and then inside of this function we also see there's another function called edge so this is the main part of the scoring function an these as i just said all the scoring factors at the level of the whole document and query for example document or lands and this aggregate function would then combine all these now inside this edge function there are functions that with the computer the weights of the contribution of a magical query term T so this is G the function G gives us the weight of a match to query term T I in document D and this H function would then aggregate all these weights so it will for example take a sum but of all the match the query terms but it can also be a product or it could be another way of aggregating them and then finally this adjustment function would then consider the document level or query level factors to further adjusted score for example of documents model addition so this general form would cover many state of large real functions let's look at how we can score such for documents with such a function using inverted index so here's a general algorithm that works as follows first these query level and document level factors can be precomputed in the indexing time of course for the query we have to compute it at the query in time but for document for example document length can be precomputed and then women hang a score accumulator for each document D to computer edge edges aggregation function over order magically returns so how do we do that well for each query term we're going to fetch the inverter list from the inverted index this would give us all the documents that match this query term an that includes D one F one anthro D N F N so each pair is document ID and the frequency of the term in the document then for each entry the subject an F subject a particular match of the term in this particular document the subject we're going to compute the function G that would give us something like a TF IDF weights of this come so will compute the weighted contribution of matching this query term in this document and then we're going to updated the score accumulator for this document and this would allow us to add this tour accumulator that would incrementally compute function edge so this is basically a general way to allow us to computer all functions of this form by using inverted index note that we don't have to touch any document that didn't match any query account this is why it's a fast we only need to process the document that that matched at least one query term in the end then we're going to adjust to the score to compute this function F subway and then we can sort so let's take a look at the specific example in this case let's assume the scoring function is a very simple one it just takes the sum of TF the road here the count of a term in the document now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include the other weights like the transformation of TF or document length normalization or IDF weighting so let's take a look at specific example where the queries information security and i show some entries of the inverted index on the right side information recording four documents and their frequencies also their security error code in three documents so let's see how the algorithm works so first we iterate over all the query terms and we fetch the first query them what is that that information so and imagine we have all these score accumulate yrs to slow scored up install the scores for these documents we can imagine there will be allocated but then they will only be allocated as needed so before we do any waiting of terms we don't even need a score communicators that concept if we have these the score accumulated eventually are located so let's fetch the entries from the inventory list for information for the first one so these score accumulate is obviously would be initialized zeros so the first avenger is D one and three three is occurrences of information in this document since our scoring function assume that the school is just a sum of these raw counts we just need to add a three to the score accumulator to account for the increase of score do two matching this term information a document that you are and then we go to the next entry that's D two and four and then we added four to the score accumulator of the two of course at this point that we were located the score accumulator as needed and so at this point we allocated D one and D two and the next wednesday three and we add one we allocate another score committed the parties and everyone do it and then find the default gets a five be cause the information the term information occured in five times in this document OK so this completes the processing of all the edges in the inverted index for information that process the all the contributions of matching information in these four documents so now our algorithm will go to the next query time that security so we're going to fetch order inverted index entries for security so in this case there are three entries and we're going to go through each of them the first is D two and three and that means security recorded three times in D two and what do we do well we do exactly the same as what we did for information so this time we're going to change the score accumulated D two since it's already allocated and what we do is to add a three to the existing value which is four so we now get a seven ford E two D two score is increased with the cause of the match to both the information and security go to the next entry that's D four and one so we would updated the score forty four and again we add one to default so default now goes from five to six finally we process the five and a three since we have not yet allocated a score accumulated forty five at this point with an allocated one forty five and we're going to add a three to it so those scores on the last roll are the final scores for these documents if our scoring function is just a simple sum of TF values now what if we actually would like to do length normalization well we can do the normalization at this point for each document so to summarize this so you can see we first are processed information turn query term information we process all the edges in the inverted index for this term then with process security it's was think about what should be the order of processing here when we consider query terms it might make difference especially if we don't want to keep all the score accumulated 's let's say we only want to keep the most promising score accumulate yrs what do you think it would be a good order to go through would you go would you process a common term first all with you processor rail home first the answer is we should go to who should process the rail term first auratum with metra fewer documents and then the score contribution would be higher because the idea of value would be higher and then it allows us to attach the most promising documents first so it helps pruning some non promising ones if we don't need a so many documents to be returned to the user so those are all heuristics for further improving accuracy here you can also see how we can incorporate the IDF weighting so they can be incorporated at when we process each query term when we fetch the inverted index we confess to the document frequency and then we can compute the idea or maybe perhaps the IDF value has already been precomputed when we index the documents at that time we already computed the IDF value that we can just attach it so all these can be down at this time so that would mean when we process all the entries for information these these words will be adjusted by the same idea which is the idea for information so this is the basic idea of using inverted index for fast research and works well for all kinds of formulas that are of the general form and this general general form covers actually most state of the art and retrieval functions so there are some tricks to further improve the efficiency some general mac technique techniques include cashing this is we just the store some results of popular queries so that next time when you see the same query you simply return the store results similarly you can also small the list of inverted index in the memory for popular term and if the query terms popular likely you will soon need to fetch the inverted index for the same term again so keeping the in the memory with help and these are general techniques for improving efficiency we can also keep only the most promising accumulate yrs because a user generators in the one to examine so many documents we only need to return high quality subset of documents that likely are ranked on the top in for that purpose we can then prune the accumulator is we don't have to store all the documentaries at some point we just keep the highest value accumulate yrs another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and the scale up through the web scale really do special to have a special techniques to do parallel processing and to distribute the storage of files on multiple machines so here as here's a list of some text retrieval tool kits it's not a complete list you can find more information at this URL on the bottom yeah i listed before here lucy 's one of the most popular tool kit that can support a lot of applications and it has very nice for the for applications you can use the builder search any application very quickly the downside is that it's not that easy to extend it an the algorithms implemented there also not the most advanced the algorithms lima or injury is another tool kit that that that does not have such a nice support of application as we'll see but it has many advanced search algorithms and it's also easy to extend terrier is yet another tool kit that also has good support for application capability and some amounts of algorithms so that's maybe in between lima or losing or maybe rather combining the strains of both so that's also useful tool kit metal is tool kit that we use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms and so top of the models are implement their and there are a number of text analysis algorithms implemented in the toolkit 's whereas basically search algorithms so to summarize all the discussion about the system implementation here are the major takeaway points an inverted index is the primary there are structural for supporting a search engine that's the key to enable fast response to a user 's query and the basic idea is a process preprocess data as much as we can and we want to do compression a proper it so that we can save disk space and can speed up IO and processing of inverter in there some general we talk about how to construct the inverted index when the data can fit into the memory and then we talk about fast search using that index basically it's exploited the inverted index accumulator scores for documents matching operator and we exploit the zipf 's law to avoid attaching many documents that don't match any query term and this algorithm can actually support a wide range of ranking algorithms so these basic techniques have have great potential for further scanning up using distributed file system parallel processing and the caching here are two additional readings that you can take a look and if you have time and you're interested in learning more about this the first one is the classical textbook efficiency of inverted index and the compression techniques and how to in general build efficient search engine in terms of the space overhead and speed the second one is a newer xbox that has a nice discussion of implementing and evaluating search engines
410	0c5d9020-626f-4f0e-87c7-04e310bf2c48	this lecture is about the document length normalization in the vector space model in this latter we're going to continue the discussion of the vectors based model in particular we're going to discuss the issue of talking the length normalization so far in the lectures about the vector space model we have used the various signals from the document to assess the matching of the document with the query in particular we have to consider the throne frequency to count of a term in the document we have also considered ITS global statistics such as IDF inverse document frequency but we have not considered a document lance so yeah i show two example documents T four is much shorter with only one hundred words these six on the other hand has a five thousand words if you look at the matching of these query words we see that in the six there are more matchings of the query words but one might reason that the six they have matched these query words in scattered manner so maybe the topic of the six is not really about the topic of the query so the discussion of campaigning at the beginning of the document may have nothing to do with the managing of presidential at the end in general if you think about long documents they would have a higher chance to match any query in fact if you generate a long document that randomly by it simply sampling words from the distribution of words then eventually you probably will match any query so in this sense we should penalize non documents because they just naturally have better chances for match any query and this is the idea of document lancer imitation we also need to be careful in avoiding to over penalize non documents on the one hand and want to penalize a long document but on the other hand we also don't want to over penalize them now the reason is be cause a document that may belong becaus of different reasons in one case the document may be long because it uses more words so for example think about four thirds article of a research paper it would use more words then the corresponding abstract so this is a case where we probably should penalize the matching of long document such as a four paper when we compare the matching of words in such a long document that with matching of the words in a short abstract then long papers generally have a higher chance of matching queer awards therefore we should penalize them however there is another case when the document is long and that is when the document is simply has more content now consider another case of a long document where we simply concatenate a lot of abstracts of different papers in such a case obviously we don't want to over penalize such a long document indeed we probably don't want to paralyze such a document becaus it's long so that's why we need to be careful about using the right degree of generalization a method that has been working well based on research results is called a pivotal length normalization and in this case the idea is to use the average document lens as a pivot as a reference point that means we'll assume that for the average lens documents the score is about right so the normalizer would be one but if the document is longer than the average document lens then there will be some parallelization whereas if it's a shorter then there's even some reward so this is illustrated using this slide on the axis X axis you can see the lens of document on the Y axis we show the normalizer in this case the people that immense normalization formula for the normalizer is is seem to be interpolation of one and the normalized the document lens controller by a parameter be here so you can see here voice first divide the length of the document by the average documents this not only gives us some sense about how this document is compared with the average documents but also gives us benefit of not worrying about the unit of length we can measure the lance by words or by characters anyway this normalizer has an interesting property first we see that if we set the parameter B two zero then the value would be one so there's no length normalization at also be in this sense controls the length normalization whereas if we set me to a non zero value then the normalizer would look like this so the value would be higher for documents that are longer than the average document lines whereas the value of the normalizer would be short away be smaller for shorter documents so in this sense we see there is a parallelization for long documents and there is a reward for short documents the degree of parallelization is controlled by BB cause if we set B to a larger value than the normal as a would look like this there's even more panelization for long documents the more reward for the short documents by adjusting P which varies from zero to one we can control the degree of length normalization so if we plug in this lens normalization factor into the vertical space model ranking functions that we have already examined then we will end up having the following formulas and these are in fact the state of that vector space model formulas so let's let's take a look at each of them the first one is called a pivotal lance normalization back to space model an a reference in the end that has detail about the derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed the idea of component that should be very familiar how to you there is also a query term frequency component here and then in the middle there is the normalizer TF and in this case we see we used a double logarithm as we discussed it before and this is to achieve a sub linear transformation but we also put poking the lens normalizer in the bottom so this would cause canalization for long document because the larger the denominators denominators then smaller than the FAA these and this is of course controller by the parameter be here and you can see again if these two zero then there there is no length normalization OK so this is one of the most effective about this base model formulas the next one called a PM twenty five or copy is also similar in that it also has a idea of component here and a query TF component here but in the middle the normalization is a little bit different as we explained there is this all copy TF transformation here and that the sub linear transformation with the upper bound in this case we have put the length normalization factor here we are adjusting K but it achieves similar factor just becaus we put a normalizer in the denominator therefore again if a document is longer than that i'm way that with this model so you can see after we have gone through all the analysis that we talked about and we have in the end reached basically the state of the art which is all functions so so far we have talked about the main eh how to place the document vector in the vector space and this has played an important role in determining the effectiveness of the retrieval function but there are also other dimensions where we did not really examine in detail for example can we further improve the instantiation of the dimension of the vector space model now we've just assumed the bag of words representation so each dimension is the war but obviously we can consider many other choices for example stem rewards those are the words that are having transformed into the same root form so that the computation and computing will all become the same and they can be matched we can do stop water removal this is remove some very common words that don't carry any content and i get the off we can use the phrase is to define dimensions we can even use latent semantic analysis will find some clusters of words that represent a lady in the concept as one by an engine we can also use smaller units like a character engrams those are sequences of N characters for dimensions however in practice people have found that the bag of words representation with the phrase is is there the most effective one and it's also efficient so this is still so far the most popular dimension instantiation method and it's used in all the major search engines i should also mention that sometimes we need to do languages specifically and domain specific that organization and this is actually very important as we might have variations of terms that might prevent us from matching them with each other even though they mean the same thing in some of them which is like chinese there is also the challenge in segmenting text to obtain word boundaries be cause it's just a sequence of characters award might correspond to one character or two characters or even three characters so it's easier in english when we have a space with separate the words but in some other languages we may need to do some natural language processing to figure out where are the boundaries for words there is also a possibility to improve the similarity function and so far we have used the dot product but one can imagine there are other measures for example we can measure the cosine of the angle between two vectors or we can use euclidean distance measure and these are all possible but dot product the sims is still the best and one reason is be cause it's very general in fact it's sufficient in general if you consider the possibilities of doing waiting in different ways so for example cosine measure can be regarded as the thought product of two normalized vectors that means we first normalize each vector and then we take the dot product that would be equivalent to the cosine measure i just mentioned that the PM twenty five seems to be one of the most effective formulas but there has been also further development in improving PM twenty five although none of these works have changed the VM qualified fundamentally so in one line work people have derived the PM twenty five F here have stands for field and this is to use VM twenty five and four documents with the structures so for example you might consider title field the abstract or body of the research article or even anchor text on the web pages those are the text fields that describer links to other pages and these can all be combined with a proper weights on different fields to help improve scoring for document when we use VM twenty five for such a document and the obvious choices will apply BM twenty five for each field and then combine the scores basically the idea of BM twenty five F is to first combine the frequency counts of terms in all the fields and then apply BM twenty five now this has advantage of avoiding over counting the first occurrence of the time remember in the sub linear transformation of TF the first occurrence is very important than contributes a large weight and if we do that for all the fields then the same term might have gained a lot of advantage in every field but when we combine these word frequencies together we just do the transformation one time at that time then the extra occurrences will not be counted as a fresh first recurrences and this method has been working very well for scoring structure the documents the other line of exchanging is called a BM twenty five plus in this line riches have addressed to the problem of overturn azatian of non documents by BM twenty five so to address this problem the fix is after a quite simple we're going to add a small constant to the TF normalization formula but what's interesting is that we can analytically prove that by doing such a small modification we will fix the problem of over canonization of long documents by the original BM twenty five so the new formula called BM twenty five plus is empirically anna nicholas soon to be petted VM twenty five so to summarize all what we have said about the vector space model here are the made you take away points first in such a model we use the similarity notion relevance assuming that the relevance of a document with respect to a query is basically proportional to the similarity between the query and document so naturally that implies that the query an document must be represented in the same way and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general and we generally need to use a lot of heuristics to design a ranking function we use some examples will show the need for several heuristics including TF wedding and transformation an IDF weighting and document and storm addition these major heuristics are the most important heuristics to ensure such a general ranking function to work for all kinds of text and finally BM twenty five and pivoted normalization seems to be the most effective formula 's out of them act space model now i have to say that i've put VM twenty five in the category of vector space model but in fact the VM twenty five has been derived using probabilistic modeling so the reason why i've put it in the vector space model is first the ranking function actually has a nice interpretation in the vectors based model we can easy to see it looks very much like a map of the space model with a special waiting function the second reason is because the original VM twenty five has a somewhat different form of IDF and that form of IDF actually doesn't really work so well as the standard IDF that you have seen here so as a effective retrieval function VM twenty five should probably use heuristic modification of the IDF to make it even more look like a baptist based model there are some additional readings the first is a paper about the pivotal length normalization it's excellent example of using empirical data analysis to suggest the need of all men summarization and then further derived length normalization formula the second there is the original paper where BM twenty five was proposed the third paper has a thorough discussion of BM twenty five and it's exchanging 's particularly BM twenty five F and finally the last paper has a discussion of improving PM twenty five to correct the over penetration of long documents
410	0d005f7e-0c3f-465b-b7fe-08f45355e3de	This lecture is about the Word Association mining and analysis. In this lecture we're going to talk about how to mine associations of words from text. This is an example of knowledge about natural language that we can mine from text data. Here's the outline. We are gooing to first talk about what is word Association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. In general there are two word relations, and these are quite basic. One is called a paradigmatic relation, the other is syntagmatic relations. A&B have paradigmatic relation if they can be substituted for each other. That means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the other without affecting the understanding of the sentence. That means we would still have a valid sentence. For example, cat and dog. And these two words have paradigmatic relation because they are in the same class of animal. And in general, if we replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of. Similarly, Monday and Tuesday have paradigmatic relation. The second kind of relation is called syntagmatic relation. In this case, the two words that have this relation can be combined with each other. So A&B have syntagmatic relation If they can be combined with each other in a sentence. That means these two words are semantically related. So for example, Cat and sit are related because a cat can sit somewhere. Similarly, car and drive are related semantically, and they can be combined with each other to convey meaning. However, in general we cannot replace cat with sit in a sentence or car with drive in a sentence to still get a valid sentence. Meaning that if we do that, the sentence will become somewhat meaningless. So this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. And definitely they can be generalized to describe relations of any items in the language. So A&B don't have to be words and they can be phrases example. And they can even be more complex phrases than just a noun phrase. If you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. So they occur in similar locations relative to the neighbors in the sequence. Syntagmatic relation on the other hand, is related to co-occurring elements that tend to show up in the same sequence. So these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data. Discovering such world relations has many applications. First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about the language. So if you know these two words or synonyms, for example, and then you can help a lot of tasks. And grammar learning can be also done by using such techniques because If we can learn paradigmatic relations, then we form classes of words. Syntactic classes for example. And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. So we'll learn the structure and what can go with what else. Word relations can be also very useful for many applications in text retrieval and mining. For example, in search in text retrieval we can use word associations to modify a query. And this can be used to introduce additional related words to a query to make the query more effective. It's often called query expansion. Or you can use related words to suggest related queries to the user to explore the information space. Another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. The user could navigate from one word to another to find information in the information space. Finally, such word associations can also be used to compare and summarize opinions. For example, we might be interested in understanding positive and negative opinions about iPhone 6. In order to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews. Such a syntagmatic relations would help us show the detailed opinions about the product. So how can we discover such associations automatically? Now, here are some intuitions about how to do that. Let's first look at the paradigmatic relation. Here we essentially can take advantage of similar context. So here you see some simple sentences about cat and dog. You can see they generally occur in similar context. And that, after all, is the definition of paradigmatic relation. So on the right side you can see I extracted explicitly the context of cat and dog from this small sample of text data. So I have taken away cat and dog from the corresponding sentences so that you can see just the context. Now of course we can have different perspectives to look at the context. For example, we can look at the what words occur in the left part of this context. So we can call this left context. What words occur before we see cat, cat or dog. So you can see in this case clearly dog and cat have similar left context. You generally say his cat or my cat, and you say also my dog and his dog. So that makes them similar in the left context. Similarly, if you look at the words that occur after cat and dog, which we can call right context and they also very similar in this case, of course it's extreme case where you only see eats and In general you will see many other words. Of course that can follow cat and dog. You can also even look at the general context. And that might improve the all words in the sentence or in sentences around this word. And even in the general context you also see some similarity between the two words. So this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. So for example, if we think about the following questions, how similar are context of cat and context of dog? In contrast, how similar are context of cat and context of computer? Now intuitively, with imagine the context of Cat and context of dog would be more similar than the context of cat and context of computer, that means the first in the first case, the similarity value would be high. Between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship. And then imagine what words occur after computer. In general they will be very different from what words occur after cat. So this is the basic idea of discovering paradigmatic relation. What about the syntagmatic relation? Here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. Here you see the same sample of text. But here we are interested in knowing what other words are correlated with the verb eats. And what words can go with eat? And if you look at the right side of the slide and you will see I've taken away the two words around eats. I've taken away the word to its left and also the world to its right, In each sentence. And then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? Now thinking about this question would help us discover Syntagmatic relations. Because syntagmatic relation essentially captures such correlations. So the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? So the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever you see eat, you tend to see the other words. And if you don't see eat, probably you don't see other words often either. So this intuition can help us discover syntagmatic relations. Now again, consider example- How helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence would generally help us predict the Whether meat also occurs indeed as if we will see eats occur in a sentence, and that should increase the chance that meat will also occur. In contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text? Because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence. So this is in contrast to the question about eats and meat. This also helps explain the intuition behind the methods for discovering syntagmatic relation. Mainly we need to capture the correlation between the occurrences of two words. So to summarize, the general ideas for discovering word associations are the following. For paradigmatically relation we represent each word by its context, and then compute the context similarity. We are gonna assume the words that have high context similarity to have paradigmatic relation. For syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. And we're going to compare their Co occurrences with their individual occurrences. We're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. Note that the paradigmatic relation and syntagmatic relation, are actually closely related. In that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. So these general ideas can be implemented in many different ways, and the course won't cover all of them, but we will cover at least some of the methods that effective for discovering these relations.
410	0f263731-f4fe-490a-aec2-188e637bbd19	This lecture is about how we can evaluate a ranked list. In this lecture we will continue the discussion of evaluation. In particular, we're going to look at how we can evaluate the ranked list of results. In the previous lecture we talked about. Precision and Recall. These are the two basic measures for quantitatively measuring the performance of search result. But as we talked about Ranking before, we framed the tax retrieval problem as a ranking problem. So, we also need to evaluate the quality of a ranked list. How can we use precision and recall to evaluate a ranked list? Naturally, we have to look at the precision and recall at different cut offs because in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing, right? If we assume the user sequentially browses the list of results, the user would stop at some point and that point will determine the set, and then that's the most important cut off that will have to consider when we compute the precision recall without knowing where exactly the user would stop, then we have to consider all the positions where the user could stop. So let's look at these positions. Look at this slide and then let's look at the what if the user stops at the first document? What's the precision and recall at this point? What do you think? It's easy to see, that this document is relevant, so the Precision is one out of one. We have got one document and that's relevant. What about the recall ? Note that we assume that there are 10 relevant documents for this query in the collection, so it's one out of 10. What if the user stops at the second position? Top 2. The precision is the same - 100% two out of two in the record 2 out of 10. What if the user stops at third position? Well, this is interesting because in this case we have not got any additional relevant document. So the recall doesn't change. But the precision is lower because we've now got a random number. So what exactly the Precision? It's two out of three, right? "And recall is the same - 2 out of 10. So when would we see another point where the recall would be different? Now if you look down the list it won't happen until we have seen another relevant document. In this case D5. At that point, the recall is increased to three out of 10. And the precision is a three out of five. So you can see if we keep doing this we can also get to D8 and then we will have a Precision of four out of eight. Because there are eight documents, and four of them are relevant and the recall is a four out of 10. Now when can we get a recall of five out of 10? Well, in this list we don't have it. So we have to go down on the list. We don't know where it is. But as a convenience, we often assume that the precision is 0. The precision is zero at all the other levels of Recall that are beyond the search results. So of course this is a pessimistic assumption. The actual precision would be higher, but we may make this assumption in order to have an easy way to compute another measure called average precision that we will discuss later. Now I should also say now here you see, we make these assumptions that are clearly not accurate. But this is usually OK for the purpose of comparing to text retrieval methods, and this is for the relative comparison, so it's OK if the actual measure or actually actual number deviates a little bit from the true number as soon as the deviation is not biased toward any particular retrieval method and we are OK, we can still accurately tell which method works better, and this is an important point to keep in mind. When you compare different algorithms, the keys to avoid any bias toward each method, and as long as you can avoid that, it's OK if you do transformation of these measures in anyway, so you can preserve the order. OK, so we just talked about that we can get a lot of precision recall numbers at different positions, so now you can imagine we can plot a curve and this just shows on the X axis we show recalls. And on the Y axis we show the precision. So the precision levels are marked as 0.1, 0.2, 0.3 and 1.0 . So this is the different levels of recall. And the Y axis also has different amounts that for precision. So we plotted these precision recall numbers that we have got as points on this picture. Now we can further link these points to form a curve as you see, we assumed that all the other precision that start high level records to be 0 and that's why they are down here, right, So they are zero in this. The actual curve probably will be something like this, but as we just discussed, it doesn't matter that much for comparing two methods. 'cause this would be an underestimate for all the methods. OK, so now that we have this precision recall curve, how can we compare 2 ranked lists right? So that means we have to compare two PR curves. And here I show 2 cases where the "system A is shown in red, System B is showing blue with crosses. Alright, so which one is better? I hope you can see here System A is clearly better. Why? Because for the same level of recall, it's the same level of recall here, and you can see the precision point by system is better than system B, so there's no question. Indeed, you can imagine what does the curve look like for ideal search system. It has to have perfect precision at all the recall points, so it has to be this line. That would be the ideal system. In general. The higher the curve is, the better, right The problem is that we might see a case like this actually happens often like the two curves across each other. Now in this case, which one is better? What do you think? Now this is a real problem that you actually might face. Suppose you build a search engine and you have old algorithm that's shown here in blue or system B and you have come up with a new idea and you test it and the results are shown in red curve A. Now your question is - is your new method better than the older method? Or more practically, do you have to replace the algorithm that you are already using your in your search engine with another new algorithm? So should we use system method A to replace method B? This is going to be a real decision that you have to make. If you make the replacement, the search engine would behave like system made here, whereas if you don't do that, It will be like a system B. So what do you do? Now, if you want to spend more time to think about this, pause the video and it's after a very useful to think about that. As I said, it's a real decision that you have to make if you are building your own search engine, or if you're working for a company that cares about search. Now if you have thought about this for a moment, you might realize that, well in this case it's hard to say there was. Some users might like system A, some users might like system B. What's the difference here? The difference is just that in the low level of recall in this region, system B is better, there's higher precision, but in high recall reading system A is better. Now, so that also means it depends on whether the user cares about the high recall or low recall, but high Precision. And imagine if someone is just going to check out what's happening today and you want to find some random in the news. Which one is better? What do you think? In this case, clearly system B is better because the user is unlikely examining a lot of results. The user doesn't care about high recall. On the other hand, if you think about a case where a user is doing, it's a literature survey, you're starting a problem. You want to find whether your idea has been started before. In that case, you emphasize high recall, so you want to see as many relevant documents as possible. Therefore, you might favor system A. So that means which one is better actually depends on users, and more precisely user's task. So this means you may not necessarily be able to come up with one number that would accurately depict the performance. You have to look at the overall picture yet as I said, when you have a practical decision to make whether you replace the algorithm with another, then you may have to actually come up with a single number to quantify each method. Or when we compare many different methods in research, ideally we have one number to compare them with, so that we can easily make a lot of comparisons. So for all these reasons it's desirable to have one single number to measure that. So how do we do that? And that needs a number to summarize a range. So here again, it's the precision recall curve, right. And one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve. Right, so this is one way to measure that, there are other ways to measure that, but it just turns out that this particular way of measuring it has been very popular and has been used since a long time ago for text retrieval evaluation. And this is basically computed in this way, and it's called Average Precision. Basically, we're going to take a look at every different recall point. And then look after the precision. So we this is one precision and this is another with different recall. Now this we don't count this one because the recall level is the same. An we can do then look at this number and that's the precision at a different recall level, etc. So we have all these, add it up. These are the provisions that had the different points corresponding to retrieving the first relevant document. The 2nd and then the third, the fourth, etc. Now we missed the mini random documents. So in all those cases we just assumed they have zero precisions. And then finally we take the average. So we divided by 10 and which is a total number of relevant documents in the collection. Note that here we are not dividing this sum by 4, which is a number of retrieved relevant documents. Now imagine if I divide by 4, what would happen? Now think about this for a moment. It's a common mistake that people sometimes overlook. So if we you divide this by 4, it's actually not very good. In fact, you are favoring a system that would retrieve very few rather than documents, as in that case the denominator would be very small, so this would be not a good measure. So note that this dinominator is 10. The total number of relevant documents. And this will basically compute the area underneath the curve. And this is the standard method used for evaluating a ranked list. Note that it actually combines recall and precision, but first we have precision numbers here. But second, that we also consider recall because if you miss the many, there will be many zeros here. So it combines precision and recall, and furthermore you can see this measure is sensitive to a small change of a position of a relevant document. Let's say if I move this relevant document up a little bit, now it would increase this average precision, whereas if I move any relevant document down, let's say I move this random document down, then it would decrease the average precision. So this is very good because it's a very sensitive to the ranking of every relevant document. It can tell small differences between 2 ranked lists and that's what we want. Sometimes one algorithm only works slightly better than another, and we want to see this difference. In contrast, if we look at the precision at the 10 documents. If you look at this whole set. What's the precision? What do you think? Well, it's easy to see. That's four out of 10, right? So that precision is very meaningful because it tells us what user would see. So that's pretty useful, right? So it's a meaningful measure from a user's perspective. But if we use this measure to compare systems, it wouldn't be good because it wouldn't be sensitive to where these four relevant documents are ranked. if I move them around the precision at 10 is still the same. Right, so this is not a good measure for comparing different algorithms. In contrast, the average precision is a much better measure. It can tell the difference of different difference in ranked lists in subtle ways.
410	0f2bbdfe-4bb9-4dba-a897-3a64efd8aeae	this lecture is about the feedback in catcher retrieval so in this lecture we're going to continue the discussion of text retrieval methods in particular we're going to talk about the feedback in pets are retrieval this is a diagram that shows the retrieval process we can see the user with typing the query and then the query would be sent to a retrieval engine or search engine and the engine will return results these results will be shown to the user now after the user has seen these results the user can actually make judgments so for example the user had to say well this is good and this document is not very useful this is a good again etc now this is called a relevance judgment or relevance feedback becaus we've got some feedback information from the user based on the judgments this can be very useful to the system remember what exactly is interesting to the user so the feedback module would then take this as input and also use the document collection through try to improve ranking typically would involve updating the query so the system can now rank the results more accurately for the user so this is all relevance feedback the feedback is based on relevance judgments made by the users now these judgments are reliable for the users generally don't want to make extra fault unless they have to so the downsides that it involves some extra effort by the user there is another form of feet about called a pseudo relevance feedback or blind feedback also called automatically back in this case you can see once the user has gotten without all in fact that we don't have to involve users so you can see there's no user involved here and we simply assume that the top ranked documents to be relevant let's say we had assumed top ten as well and then we would then use these assume the documents to learn and to improve the query now you might wonder how could this help if we simply assume the top ranked documents will be random well you can imagine these top rank the documents actually similar to relevant documents even if they are non random they look like relevant documents so it's possible to learn some related terms to the query from this set in fact that you may recall that we talked about using language model to analyze water association to learn related words to the world computer right and then what we did is your first we use computer to retrieve all the documents that contain computer so imagine now the query here is computer and then the results will be those documents that contain computer and what we can do then is to take the top end results they can match computer very well and we're going to count the terms in this set and then welcome to use the background language model to choose the terms that are frequently in this set but not frequently in the whole collection so if we make a contrast between these two what we can find is that we will learn some religious terms to the water computer as we have seen before and these will related was can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software so this was the effective for improving the search result but of course pseudorandom asleep back is completely unreliable we have to arbitrary set a kind of so there's also something in between called implicit feedback in this case what we do is we do involve users but we don't have to ask users to make judgments instead we're going to observe how the user interacts with search results so in this case we are going to look at the pixels so the user clicked on this one and the user view with this one and the user skip this one and the user view of this one again now this also is a claw about whether a document is useful to the user and we can even assume that we're going to use only the slip it here in this document that text that's actually seen by the user is there of the actual document them of this entry by the link there let's stay in web search maybe broken but now it doesn't matter if the user tried to fetch this document becaus of the display the text we consume these display the text is probably relevant is interesting to use our code so we can learn from such information and this is called implicit feedback and we can again use the information to update the query this is a very important technique using more than search engines you think about the global and being and they can collect a lot of user activities why they're serving us so they would observe what documents we click on what documents will scape and this information is very valuable and they can use this to improve the search engine so to summarize we talked about the three kinds of feedback here relevance feedback where the user makes explicit judgments it takes some user effort but the judgment information is reliable we talked about the pseudo feedback where we simply assume top ranked documents to be red and we don't have to involve the user therefore we could do that actually before we return the results to the user and the third is in crystal feedback where we use click throws well we don't we involve users but the user doesn't have to make explicit the effort to make judgment
410	11ef1861-7f1b-410f-81ba-f8ea7ccbb172	in this lecture we're going to talk about the text access in the previous lecture we talked about natural language.. of content analysis we explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner as a result bag of words representation remains very popular in applications like a search engines in this lecture we're going to talk about some high level strategies to help users get access to the text data this is also important step to convert roar big text data into small relevant data that are actually needed in a specific application so the main question would rest here is how kind of text information system help users get access through the relevant the text data we're going to cover two complementary strategies push versus pull and then we're going to talk about two ways to implement the poor mode querying versus browsing so first push versus pull these are two different ways to connect users with the right information at the right time the difference is which takes the initiative which part it takes the initiative in the poor mode the users would take the initiative to start the information access process and in this case a user typically would use a search engine to fulfill the goal for example the user may type in the query and then browser results two find the relevant information so this is usually a property for satisfying users eric information need an airplane information needed is temporary information need for example you want to buy a product so you suddenly have a need to read reviews about related products but after you have collected information have purchase your product you general no longer need such information so it's a temporary information need in such a case it's very hard for system to predict your need an it's more property for the users to take the initiative and that's why the search engines are very useful today be cause many people have many airport information needs all the time so as we're speaking google properties processing many queries from this and those are all or mostly out of information needs so this is a poor mode in contrast in the push mode in the system will take the initiative to push the information to the user or to recommend any information to the user so in this case this is usually supported by a recommender system now this would be a property if the user has a stable information mean for example you may have a research interest in some topic and that interest tends to stay for awhile so it's relatively stable your hobby is another example of a stable information need in such a case the system can interact with you and can learn your interest and then can monitor the information stream if the system has seen any rather than the items to interest the system could then take the initiative direct meant information to you so for example a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you this mode of information access maybe also a property when the system has good knowledge about the users need and this happens in the search context so for example when you search for information on the web a search engine might infer you might be also interesting some related information and they would recommend information to you so that should remind you for example advertisement placed on search page so this is about the two high level strategies or two modes of texter abscess now let's look at the poor mode in more detail in the poor mold we have further distinguishing two ways to help users querying versus browsing in querying a user would just enter a query typical the keyword query and search engine system would return relevant documents to users and this works well when the user knows what exactly are the key words to be used so if you know exactly what you're looking for you tend to know the right keywords and then querying would work very well and we do that all the time but we also know that sometimes it doesn't work so well when you don't know the right keywords using the query or you want to browse information in some topic area in this case browsing would be more useful so in this case in the case of browsing the users would simply navigate into the relevant information by following the paths supported by the structures documents so the system would maintain some kind of structures and then the user could follow these structures to navigate so this is really it works well when the user wants to explore information space or the user doesn't know what are the keywords to use in the query or simply becaus the user finds it inconvenient to type in the query so even if the user knows what query to typing if the user is using a cell phone to search for information then it's still hard to enter the query in such a case again browsing tends to be more convenient the relationship between browsing and the query is vessel in the store by making analogy to the site singing imagine if you are touring the city now if you know the exact address of attraction then taking a text there is perhaps the fastest way you can go directly to the site but if you don't know the exact address you may need to work around or you can take a text to a nearby place and then work around it turns out that we do exactly the same in the information space if you know exactly what you're looking for then you can use the right keywords in your query to finally implementing directory that's usually the fastest way to do final information but what if you don't know the exact keywords to use well your query probably won't work so well you amend on some related pages and then you need to also work around in the information space meaning by following the links or by browsing you can then finally getting to the right of the page if you want to learn about soplica again you would likely do a lot of browsing so just like that you are looking around in some area and you want to see some interesting attractions in related in the same region so this analogy also tells us that today we have very good support for query but we don't really have would support for browsing and this is becaus in order to browse effectively we need a map to guide us just like if you need a map of chicago the two of the city of chicago you need atop the map to tour the information space so how to construct the such a topical map is in fact that a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications so to summarize this lecture we've talked about the two high level strategies in for text to access push and pull push tends to be supported by recommended system and pull things will be supported by the search engine of course in the sophisticated in current in the information system we should combine the two in the poor more that we can further disclosure querying and browsing again we generally want to combine the two ways to help users so that you can support both querying and browsing if you want to know more about the relationship between pull and push you can read this article this gives excellent discussion of the relationship between information filtering and information retrieval here information filtering is similar to information recommendation or the push mode of information access
410	12ec877b-f300-4841-b334-95cc89e3672a	there are many more the monster learning algorithms then the regression based approaches and they generally attempt to direct their optimizer retrieval machine like a map or entity note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure by maximizing the prediction of one or zero we don't necessarily optimize the ranking of those documents one can imagine that while prediction may not be too bad and it's they both are around the point five so it's kind of in the middle of zero and one for the two documents but the ranking can be wrong so we might have got a larger value for D two and then he won so that won't be good from retrieval perspective even though by likelihood function is not bad in contrast we might have another case where we predicted values or around the point nine let's say and by the objective functioning the arrow the larger but if you can get the order of the two documents correct that's actually a better result so these new more advanced approaches will try to correct that problem of course then the challenges that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end no more about these are proteins now these learning to rank approaches are actually general so they can also be applied to many other ranking problems not just the retrieval problem so here i list assignment for example recommender systems computation or by advertise E or summarization and there are many others that you can probably encounter in your applications to summarize this lecture we have talked about using machine learning to combine multiple features to improve ranking results actually the use of machine learning information retrieval has started since many decades ago so for example of the rock hill feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback but the most reason the use of machine learning has been driven by some changes in the environment of applications of retrieval systems first it's mostly driven by the availability of a lot of training data in the form of click throughs such data want available before so the data can provide a lot of useful knowledge about relevance and machine learning methods can be applied to leverage this secondly it's also given by the meter for combining many features and this is not only just be cause there are more features available on the web that can be naturally used to improve scoring it's also be cause by combining them we can improve the robustness of ranking so this is desired for combatting this lamps modern search engines are all used some kind of machine learning techniques combined many features to optimize ranking at this major feature of these commercial engines such as google or bing the topic of learning the rank is still active research topic in the community and so you can expect to see new results being developer in the next few years perhaps here are some additional readings that can give you more information about how learning to rank works and also some advanced methods
410	1311ce11-68b0-42bf-82a4-17bc5d316e6f	This lecture is about the inverted index construction. In this lecture we will continue the discussion of system implementation. In particular, we're going to discuss how to construct the inverted index. The construction of the inverted index is actually very easy if the data set is very small, it's very easy to construct dictionary and then store the postings in a file. The problem is that when our data is not able to fit to the memory then we have to use some special methods to deal with it. Unfortunately, in most retrieval applications, that data set would be large and they generally cannot be loaded into memory at once. There are many approaches to solve the problem and sorting based method is quite common. It works in four steps as shown here. First, you collect the local term ID, document ID, and frequency tuples. Basically you will not count the terms in a small set of documents. And then once you collect those counts, you can sort those counts based on terms so that you build a local partial inverted index, and these are called rounds. And then you write them into a temporary file on the disk, and then you merge in step three would do pair-wise merging of these runs until you eventually merge all the runs to generate a single inverted index. So this is an illustration of this method. On the left, you see some documents. And on the right, we have shown Term Lexicon and DocumentID Lexicon. These lexicons are to map stream-based representations of document IDs or terms into integer representations or to map back from integers to the stream representation. The reason why we are interested in using integers to represent these IDs is because integers are often easier to handle. For example, integers can be used as index for array. They are also easy to compress. So this is one reason why we tend to map these strings into integers. So that well, so that we don't have to carry these strings around. So how does this approach work? Well, it's very simple. We're going to scan these documents sequentially, and then parse the document, and count the frequencies of terms. In this stage, we generally sort the frequencies by document IDs because we process each document sequentially. So we first encounter all the terms in the first document. Therefore, the document IDs are all 1s in this case. And this will be followed by document IDs 2s. They are naturally sorted in this order just because we process the data in the sequential order at some point we will run out of memory and then we would have to write them into the disk. But before we do that, we can sort them just use whatever memory we have. We can sort them and then this time we're going to sort based on term IDs. Notice that here we're using the term IDs  as a key to sort. So all the entries that share the same term would be grouped together. In this case, we can see  IDs of documents that match the term one will be grouped together. And we're going to write this into the disk as a temporary file, and that would allow us to use the memory to process the next batch of documents. And we're going to do that for all the documents. So we are going to write a lot of temporary files into the disk. And then the next stage is merge sort. We are going to merge them and sort them. Eventually we will get a single inverted index with their entries are sorted based on term IDs. And on the top we can see these are the old entries for the documents that match term ID1. So this is basically how we can do the construction of inverted index even though the data cannot be  loaded into the memory. Now we mentioned earlier that because of postings are very large, it's desirable to compress them. So let's now talk a little bit about how we compress inverted index. The idea of compression in general is to leverage skewed distributions of values, and we generally have to use variable length encoding instead of the fixed length encoding as we use by default in program languages like C++. So how can we leverage the skewed distributions of values to compress these values? In general, we would use fewer bits to encode those frequently awards at the cost of using longer bits to encode those rare values. So in our case, let's think about how we can compress the TF (term frequency). Now if you can picture what the inverted index would look like, and you see in postings, there are a lot of term frequencies. Those are the frequencies of terms in all those documents. Now, if you think about what kind of values are most frequent there, you probably will be able to guess that small numbers tend to occur far more frequently than large numbers. Why? Think about the distribution of words and this is due to the Zipf's law, and many words occur rarely. So we see a lot of small numbers. Therefore we can use fewer bits for the small but highly frequent integers  at the cost of using more bits for large integers. This is a trade off, of course. If the values are distributed uniformly, this one will save us a lot of space. But because we tend to see many small values, they are very frequent. We can save on average, even though sometimes when we see a large number we have to use a lot of bits. What about the document IDs that we also saw in postings? They are not distributed in the skewed way right. So how can we deal with that? Well, it turns out that you can use a trick called d-gap and that is to restore the difference of this term IDs. And we can imagine if a term has matched many documents, then there will be long list of document IDs. So when we take the gap, I'm going to take the difference between adjacent the document IDs. Those gaps will be small, so we will again see a lot of small numbers. Whereas if a term according only a few documents, then the gap would be large. The large numbers will not be frequent. So this creates some skewed distribution that would allow us to  compress these values. And this is also possible because in order to uncover or uncompress these document IDs. We have to sequential process the data. Because we store the difference and in order to recover the exact the document ID, we have to 1st recover the previous document IDs and then we can add the difference to the previous document ID to restore the current document ID. Now this was possible because we only need to have sequential access to those documents IDs. Once we look up a term we fetch all the document IDs that match the term. Then we sequentially process them. So it's very natural. That's why this trick works. And there are many different methods for encoding. For example, the binary code is a commonly used code in programming languages where we use basically fixed length encoding. Unary code, Gamma code, and Delta code are all possibilities and there are many other possibilities. So let's look at some of them in more detail. Binary coding is really equal length encoding, and that's our property for randomly distributed values. The unary coding is a variable length encoding method. In this case, an integer that's at least one would be encoded as x-1, one bits followed by zero. So for example, three would be encoded as two ones, followed by zero, whereas five will be encoded as 4 ones, followed by zero etc. So now you can imagine how many bits do we have to use for a large number like 100. So how many bits do we have to use exactly the full number like 100? "" Exactly, we have to use 100 bits, right? So it's the same number of bits as the value of this number, so this is very inefficient. If you will likely see some large numbers, imagine if you occasionally see a number like 1000. You have to use 1000 bits, so this only works well if you are absolutely sure that there will be no large numbers, mostly very, very often using very small numbers. How do you decode this code? Since these are variable length encoding methods and you can't just count how many bits, and then they just stop. You can say 8 bits or 32 bits, then you will start another code. There are variable lengths, so you'll have to rely on some mechanism. In this case, for unary you can see it's very easy to see the boundary. Now you can easily see zero with signal, the end of encoding. So you just count how many ones you have seen and until you hit zero you have finished one number. You will start another number. Now we just saw that the unary coding is too aggressive in rewarding small numbers. If you occasionally, you can see a very big number. It would be a disaster. So what about some other, less aggressive method? Well, Gamma coding is one of them. And in this method we are going to use unary coding for a transformed form of the value, so it's one plus the floor of the log of X. So the magnitude of this value is much lower than the original X. So that's why we can afford using unary code for that. So we will first have the Unary code for this log of X. This will be followed by a uniform code or binary code, and this is basically the same uniform code and binary code are the same, and we're going to use this code to code the remaining part of the value of X. And this is basically precise X - 1 to the floor of the log of X. So the unary code basically coded the floor of log of X, add one there. But the remaining part will be using uniform code to actually code the difference between the X and this  two to the log of X. And it's easy to show that for this value this difference, we only need to use up to this many bits and floor of log of X bits. This is easy to understand, if the difference is too large then we would have a higher floor of log of X. So here are some examples. For example, 3 is encoded as 101. The first 2 digits are the unary code, right. So this is for the value 2. 10 encodes 2 in unary coding. And so that means log of X, the floor of log of X is 1 because we want actually use unary code to encode one plus the floor of log of X. Since this is 2, then we know that the floor of log of X is actually one. But three is still larger than two to the one, so the difference is 1, and then one is encoded here at the end. So that's why we have 101 for 3. Similarly, 5 is encoded as 110 followed by 01 and in this case, the unary code encodes 3. And so this is the unary code 110. And so the floor of log of X is 2. And that means we are going to compute the difference between 5 and 2 to the two, and that's one, and so we now have again one at the end. But this time we're going to use 2 bits, cause with this level floor of log of X we could have more numbers 5, 6, 7 they would all share the same prefix here,110. So in order to differentiate them we have to use 2 bits in the end to differentiate them. So you can imagine six would be 10 here in the end instead of 01 after 110. It's also true that the form of gamma code is always: first odd number of bits and in the center in there is a zero. That's the end of the unary code. And before that, on the left side of this zero, there will be all ones an on the right side of this zero, it's binary coding or uniform coding. So how can you decode such a code? You again first do unary coding right Once you hit zero, you have got the unary code. And this also would tell you how many bits you have to read further to decode the uniform code. So this is how you can decode Gamma code. There was also the error code that's basically the same as gamma code, except that you replace the unary prefix with the gamma code, so that's even less conservative than gamma code in terms of rewarding the small integers. So that means it's OK if you occasionally see a large number. It's OK with delta code. It's also fine with gamma code, it's really a big loss for Unary code and they are all operating of course at different degrees of favoring small integers. And that also means. They would be appropriate for a certain distribution, but none of them is perfect for all distributions. Which method works the best with have to depend on the actual distribution in your data set. For inverted index compression, people have found that gamma coding seems to work well. So how do I uncompressed invert index when we just talked about this first, you decode those encode integers and we just I think discussed how we decode unary coding and gamma coding. So I won't repeat. And what about the document IDs that might be compressed using d-gap. We're going to do sequential decoding. So suppose the encoded ID list is x1, x2, x3, etc. We first decode x1 to obtain the first document ID ID1. Then we would decode x2, which is actually the difference between the second ID and the first one. So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this second position, right. So this is where you can see the advantage of converting document IDs into integers, and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position.
410	1579f35e-a2d3-4e13-9795-f203c6d41157	this lecture is overview of texture retrieval methods in the previous lecture we introduced to the problem of text retrieval we explained that the main problem is the designer ranking function into ranked documents for a query in this lecture will give overview of different ways of designing this ranking function so the problem is the following we have a query that has a sequence of words and a document that that's also a sequence of words and we hope to define a function F that can compute the score based on the query and document so the main channel here is redesigning a code breaking function that can rank all the relevant documents on top of all the non random ones now clearly this means our function must be able to measure the light hold that a document that he is relevant to a query kill that also means we have to have some way to define relevance in particular in order to implement the program to do that we have to have a computational definition of relevance and we achieve this goal by designing a retrieval model which gives us a formalization of relevance now over many decades researchers have the line of many different kinds of retrieval models and they fall into different categories first one thing many of the models are based on the similarity idea basically we assume that if a document is more similar to the query then another document is then we will say the first document is more relevant than the second one so in this case the ranking function is defined as the similarity between the query and the document one well known example in this case is vector space model which we will cover more in detail later in the lecture the second kind of models are called a probabilistic models in this family of models we follow a very different a strategy where we assume that queries and documents are observations from random variables and we assume there is a binary random variable called are here to indicate whether a document is relevant to a query we then define the score of a document with respect to a query as the probability that this random variable R is equal to one given a particular document and query there are different cases of such a general idea one is classical probability model and others language model yet another is diversions from randomness the model in the letter lecture we will talk more about one case which is language model the third kind of models are based on probability inference so here the idea is do associate and certainly to inference rules and we can then quantify the probability that we can show that the query follows from the document finally there is also a family of models that are using actual medical thinking here the idea is to define a set of constraints that we hope a good retrieval function to satisfy so in this case the problem is with sick a good ranking function that can satisfy all the desire the constraints interested in although these different models are based on different thinking in the end the retrieval function tends to be very similar and these functions tend to also involve similar variables so now let's take a look at the common form of a state of that retrieval model and to examine some of the common ideas used in all these models first these models are all based on the assumption of using bag of words to represent the text and we explained this in the natural language processing laptop bag of wars were intent itching remains the main representation using all the search engines so with this assumption the score of a query like presidential campaign news with respect to a document D here would be based on scores computed based on each individual war and that means the score would depend on the score of each word such as presidential campaign and news here we can see there are three different components each corresponding to how well the document matches each of the query words inside these functions we see a number of heuristics used so for example one factor that effects the function G here is how many times does the word presidential occur in the document this is all the term frequency or TF we might also D load as C of presidential and E in general if the word occurs more frequently in the document then the value of this function would be larger another factor is how long is the document ann this is to use the talking the length for scoring in general if a term occurs in a long document that many times it's not as significant as if it occured the same number of times in the shop document becaus in a long document anytime is expected to occur more frequently finally there is this effect that called a document frequency and that is we also want to look at how often presidential occurs in the entire collection and we call this document frequency or DF of presidential and in some other models we might also use a probability to characterize this information so here i show the probability of presidential in the collection so all these are trying to characterize the popularity of the term in the crash and in general matching array item in the collection is contributing more to the overall score than matching our common term so this captures some of the main ideas using pretty much all the state of larger retrieval models so thou a natural question is which model works the best now it turns out that many models work equally well so here i list to the four major models that are generally regarded as a state of the art retrieval models pivotal and summarization BM twenty five query likely hold P L two when optimizer these models tend to perform similarly and this was discussed in detail in this reference at the end of this laptop among all these BM twenty five is probably the most popular it's most likely that this has been used in virtually order search engines and you will also often see this method discussed in research papers and we'll talk more about this method data in some other actions so to summarize the main points made in the snatch or first the design of a good ranking function pretty requires a computational definition of relevance and we achieve this goal by designing a property retrieval model second the mini models are equally effective but we don't have a single winner yet researchers are still actively working on this problem trying to find the actually optimal mitchell model finally the state of that ranking functions tend to rely on the following ideas first bag of words representation second TF and document frequency of words such information is used in waiting function to determine the overall contribution of matching or world and locking the lens these are often combine are in interesting ways and were discussed how exactly they are combined to rank documents in the lectures later there are two suggested the additional ratings if you have time the first is a paper where you can find a detailed discussion and comparison of multiple state of the art models the second is a book with a chapter that gives a broad review of different retrieval models
410	1be44ca1-c700-426a-a4ea-0710a276278c	this lecture is about the smoothing of language models in this lecture we're going to continue talking about the probabilistic retrieval model in particular we're going to talk about smoothing of language model in the query it like a whole retrieval method so you have seen this slide from a previous lecture this is the ranking function based on the query like hold here we assume that the independence of generating chick we reward and the formula would look like a following where we take a sum over all the query awards and inside the some there is a log of probability of award given by the document or spoken language model so the main task now is to estimate this document language model as we said before different methods for estimating this model would need two different retrieval functions so in this lecture we're going to look into this in more detail so how do i expect this language model well the obvious choice would be the maximum macular estimated that we have seen before and that is working to normalize the word frequencies in the document and the estimator probability would look like this this is a step function here which means all the words that have the same frequency count will have identical probably there this is another frequently count that as a different probability note that for words that have not occured in the document here they all have zero probability so we know this is just like the model that we assume earlier in a lecture where we assume that the user with the simple word from the document to formulate a query and there's no chance of sampling any word that's not in the document and we know that's not good so how do we improve this well in order to assign a non zero probability to words that have not been observed in the document we would have to take away some probability mass from the words that are observing the document so for example here we have to take away some probably the mass because we need some extra problem in mass for the unseen words otherwise they want some to one so all these probabilities must send one so to make this transformation and to improve the maximum micro resonator by assigning nonzero probabilities two words that are not observed in the data we have to do smoothing and smoothing has to do with improving the estimated by considering the possibility that if the author had been written being asked to write more words for the document the the other might have retain other words if you think about this factor then smooth the language model would be more accurate representation of the actual topic imagine you have seen abstractor over research article let's say this document is abstract right if we assume and see words in this abstract that we have all our probability of zero that with me is no chance of sampling a word outside the abstract at the formula to query but imagine a user who is interested in the topic of this object the user might actually choose a word that's not in the abstract are too to use as a query so obviously if we had asked this author to write more the author would have retained A four text of that article so smoothing of the language model is attempt to try to recover the model for the whole whole article and then of course we don't have really knowledge about any words that are not observed in the abstract are so that's why smoothing is actually tricky problem so let's talk a little more about how this mother language model and the key question here is what probably there should be assigned to those unseen words and there are many different ways of doing that one idea here that's very useful for retrieval is late the probability of unseen world be proportional to its probability given by a reference language model that means if you don't observe the word in the data set we're going to assume that its probability is kind of governed by another reference language model that would work constructed it will tell us which unseen words we have like their higher probability being the case of retrieval a natural choice would be to take the collection language model as the reference language model that is reserved for don't know observe award in the document we're going to assume that the probability of this word would be proportional to the probability of the world in the whole collection so more formally will be estimating the probability of award given a document as follows if the war is seen in the document then the probability would be a discounted the maximum like roller estimated peace obscene otherwise if the war is not seeing the document we're going to let the probability be proportional to the probability of the world in the collection and here the coefficient alpha is to control the amount of probability mass that were assigned to unseen words obviously all these probabilities must sum to one so alpha sub D is constrained in some way so what if we plugging this smoothing formula into our query lighter running function this is what we looked at in this formula you can see we have this as a sum over all the query words and note that we have returned in the form of a sum over all the vocabulary is he here this is the sum of all the words in the vocabulary but not that we have account of the world in the query so in fact we're just taking a sum of query words right this is now a common way that we will use because of its convenience in some transformations so this is as i said this is some overall the query words in our smoothing method we assume that the words that are not observed in the document we have somewhat different form of probability name it's for this fall so we're going to then decompose this some into two parts one some is over all the query words that are matching the document that means in this sum or the words have a non zero probability in the document sorry it's now zero count of the world in the document they all occur in the document and they also have to of course have a non zero count in the query so these are the words that are matched these are the query words that are matching the document on the other hand this is some we are taking a sum over all the words that are not all query was not are not matched in the document so they occur in the query you do this term but they don't occur in the document in this case these words have this probability becaus of our assumption about the smoothie but that here these seem was have a different probability now we can go further by rewriting the second some as a difference of two other signs basically the first sum is actually some overall the query words now we know that the original sum is not over all the query words this is over order query was that are not matched in the document so here we pretend that they are actually over all the query it words so we take a sum over all the query words obviously this sum has extra terms that are this sum as extra terms that are not in this sum becaus here we're taking some overall the query was there it's not matched in the document so in order to make them equal we have to then subject to another some here and this is the sum over all the query words that are matching the document and this makes sense because here we are considering all current words and then we subtract the query words that are matched in the document that would give us the query words that not matched in the document and this is almost reversed process of the first step here and you might want to why do we want to do that well that's becaus if we do this then we have different forms of terms inside of these sums so now you can see in this some we have all the words matched the query was matching the document that was this kind of times here we have another some over the same set of terms match it queried homes in document but inside of some it's different but these two sums can clearly be merged so if we do that we'll get another form of the formula that looks like the following at the bottom yet and note that this is a very interesting formula because here we combine these two that our some over the query words matching the document in the want some here and the other son now is decomposing the two parts and these two parts look much simpler just because these are the probabilities of unseen worlds now this formula is very interesting because you can see the sum is not over all the match the query terms and just like a in the vector space model we take a son of terms that i in the intersection of query bakhtaran the document adapter so it already looks a little bit like the vector space model in fact there is even more similarity here as we explain on this slide
410	1cc2d7fa-3d11-49fa-b979-ef5e9442466f	This lecture is about the probabilistic latent semantic analysis or P LSA. In this lecture we're going to introduce probabilistic latent semantic analysis, often called the PLSA. Say this is the most basic topic model. Also, one of the most useful topic models. Now, this kind of models can in general be used to mine multiple topics from text documents, and PLSA is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. Here I show a sample article which is a blog article about Hurricane Katrina. An I showed some sample topics, for example government response, flooding of the city in new orlean's donation and the background. You can see in the article we use words from all these distributions. So we first for example. See there's a criticism of government response, and this is followed by the discussion of flooding of the city and donation, etc. We also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. So segment of the topics to figure out which words are from which distribution and to figure out the first one of these topics. So how do we know there's a topic about government response? There is a public about the flooding of the city. So these are the tasks of topical model. If we can discover these topics can color this words as you see here to separate the different topics, then you can do a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc. So the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocabulary set. And of course, the text data right? And then the output is of two kinds. One is the topic category characterization Seedies HCI is a water distribution and 2nd it's the topic coverage for each document. These are pie some ideas and they tell us which document covers which topic to what extent. So we hope to generate these as output because there are many useful applications if we can do that. So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced. The only difference is that we're going to have more than two topics. Otherwise it's essentially the same. So here I illustrate how we can generate the text that I was multiple topics. And naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. So we will also ask the question what's the probability of observing a world W from such a mixture model? Now if you look at this picture and compare this with the picture that you have seen earlier, you will see the only difference is that we have added more topics here. So before we have just one topic besides the background topical, but now we have more topics. Specifically we have K topics. Now all these are topics that we assume that exist in the text data, so the consequences that our switch for choosing a topic now is multiway switch before it's just a two way switch. Going to think of as flipping a coin. But now we have multiple is. First we can flip a coin to decide whether we will talk about the background. So it's the background. Lambda sub B versus non background. So this one minus Lambda B gives us the probability of actually choosing a topic. And background help. After we have made this decision, we have to make another decision to choose one of these K distributions. So there's a key way. Switch here, and this is characterized by the pies, and there's someone. So this is just the different design of switches, a little bit more complicated, but once we decide which distribution to use, the rest is the same. We're going to generate the world by using one of these distributions, assume here. OK, so now let's look at this question about the like hold. So what's the probability of observing a word from such a distribution? What do you think? Now we've seen this problem many Times Now, and if you recall, it's generally a sum over all the different possibilities of generating the world. So let's first look at the how the world can be generated from the background model. The probability that the world is generated from the background model is Lambda multiplied by the probability of the world from the background model, right? Two things must happen. First, we have to have chosen the background model. And that's probability of Lambda sub B and then the second we must have actually obtained the world W from the background, and that's probability of W given sit out submit. OK, so similarly we can figure out the probability of observing the world from another topic. A topical theta sub k. Now notice that here's a product of three terms, and that's the cause. The choice of topic theta sub K only happens if two things happen. One is we decided not to talk about background, so that's probability 1 minus Lambda sub b. Second, we also have to actually choose. Set us up. K among these K topics. So that's probability of serious up cake or pie. And similarly, the probability of generating the water from the second topic and his first topic popular like what you're seeing here and then. So in the end, the probability of observing the world is just a sum of all these cases. And I have to stress again, this is a very important formula to know because. This is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. Of W is indeed the some of these terms. So next, once we have the likelihood function, we would be interested in knowing the parameters right? So to estimate the parameters. But first let's put all these together to have the complete likelihood function for PLSA. Now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as I said. And so let's take a closer look at this. After that contains all the important parameters. So first we see Lambda sub b here. This represents the percentage of background words. That would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. We can use a large collection of text or use all the tests that we have available to estimate the water distribution. Now next in the rest of this formula. Excuse me, you see two interesting kinds of parameters. Those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. And the other is the word distributions that characterize all the topics. So the next line then is simply to plug this in to calculate the probability of document. This is again of the familiar form where you have some and you have account of world in the document and then log of a probability. Now it's a little bit more complicated than the two component because now we have more components. So the sum involves more terms and then this line is just the like holder for the whole collection and it's very similar. Just accounting for more documents in the collection. So what are the unknown primers? I already said there are two kinds on his coverage. One is awarded distributions. Again, it's a useful exercise for you to figure out exactly how many premise there on here. How many unknown parameters are there? Now trying to figure out that question would help you understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text data, and these are precisely the unknown parameters. So after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. And we can do the usual thing. Maximum likelihood estimator. So again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. One is awarded distributions. All the words must have probabilities that sum to 141 distribution. The other is the topic coverage distribution. Anna Document will have to cover precisely these K topics, so the probability of covering each topical would have to sum to one. So at this point it's basically where they find applied math problem. You just need to figure out the solutions to optimization problem. There's a function with many variables and we need to just figure out the values of these variables to make the function which its maximum.
410	20703c3c-ced6-4410-ace1-139baa46505c	So I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm with converge to a local maximum. So here is just the illustration of what happened an A detailed explanation this. Require more. Knowledge about some of the inequalities that we haven't really covered yet. So here what you see is on the X dimension. We have set up value. This is the parameter that we left on the Y axis. We see the likelihood function. So this curve is reaching or like roller function, right? So this one. And this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. But in the case of mixture model, we cannot easily find the analytical solution to the problem. So we have to resolve a numerical algorithm. An EM algorithm is such an algorithm. It's a Hill climb algorithm that would mean you start with some random guess. Let's say you start from here. That's your starting point and then you try to improve this by moving this to another point where you can have a higher like recorder. So that's the idea of Hill climbing. Any in the MRI was the way we achieve this is to do two things. First will fix a lower bound of likelihood function, so this is the lower bound you can see here. An once we fit the lower bound we can then maximise the lower bound and of course the reason why this works is because the lower bound is much easier to optimize so we know our current gas is here an by maximizing the lower bound will move this point to the top two here. I. And that we can then map to the original like role function. We find this point. Be cause it's a lower bound, we are guaranteed to improve this gas. Right, because we improve our lower bound and then the original lighter Holder curve which is above this lower bound will definitely be improved as well. I so we already know it's improving the lower bound, so we definitely improve this original like record function which is above this lower bound. So in our example, the current gas is parameter value given by the current generation and then the next guest is the RE estimated parameter values. From this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck there. So the two would be equal. So the E step is basically to compute this lower bound. And we don't direct it, just computed this likely or function, but we computed the latent variable values and. These are basically part of this lower bound. This helps determine the lower bound the M step on the other hand, is to maximize the lower bound. It allows us to move parameters to a new point. And that's why EML is gone. The little converge to a local maximum. Now, as you can imagine, when we have many local Maxima, we also have to repeat the EML with multiple times in order to figure out which one is the actual global maximum. And this actually in general is a difficult problem in numerical optimization. So here for example, how do we start from here? Then we gradually just climb up to this top, so that's not optimal, and we'd like to climb up all the way to here. So the only way to climb up to this here. This will start from somewhere here or here. Right so. In the EM algorithm, we generally would have to start from different points or have some other way to determine a good initial starting point. To summarize, in this lecture we introduce the EM algorithm. This is a general algorithm for computing. Maximum regular is made of all kinds of mixture models. So not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. The general idea is that we will have two steps to improve the estimate of parameters in the E step. We roughly all augmenting our data by predicting values of useful hidden variables that we would use to simplify the estimation. In our case, this is the distribution that has been used to generate the world. In the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters. Here improve is guaranteed in terms of the likelihood function. Note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. There are some properties that have to be satisfied in order for the parameters also too. Convert it to some stable value. Now he thought data augmentation is done probabilistically. That means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the world counts between the two distributions.
410	2736e0b3-cd3e-4760-b07e-e9aadcc588e2	This lecture is about the syntagmatic relation discovery. An entropy. In this lecture, we're going to continue talking about word Association mining. In particular, we can talk about how to discover syntagmatic relations. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. By definition, Syntagmatic relations hold between words that have correlated Co occurrences. That means when we see one word occurs in the context, we tend to see the occurrence of the other word. So take a more specific example, here we can ask the question whenever eats occurs, but other words also tend to occur. Now looking at the sentence is on the left. We see some words that might occur together with eats like a cat, dog or fish is right. But if I take them out and if you look at the right side where we only show eats and some other words. The question that is, can you predict what other words occur? To the left or to the right. Right, so this would force us to think about what other words are associated with eats. If they are associated with eats, they tend to occur in the context of eats. So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment. Right here we can ask the question about the world W is present or absent in this segment. Now, what's interesting is that some words are actually easier for it, in other words. If you take a look at the three words shown here, meet, the and Unicorn. Which one do you think it is easier to predict? Now, if you think about it for a moment, you might conclude that. The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence. Unicorn is also relatively easy. Because Unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence. But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately. But it may also not occur in the segment. So now let's start this problem more formally. Alright, so the problem can be formally defined as predicting the value of a binary random variable. Here we denoted by X sub w, w denotes a word. So this random variable is associated with precisely one word. When the value of the variable is 1, it means this word is present. When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1. Because a word is either present or absent in the segment. There's no other choice. So the intuition we discussed earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction would be. Now the question is, how does one quantitatively measure the randomness of a random variable like X sub w, how in general, can we quantify the randomness of a variable? And that's why we need a measure called entropy. And this is a measure introduced in information theory to measure the randomness of X. There is also some connection with the information here, but that's beyond the scope of this course. So for our purpose we just treat the entropy function as a function defined  on a random variable. In this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. Now the function form looks like this. There's a sum over all the possible values for this random variable inside the sum,  for each value we have a product of the probability that the random variable equals this value and log of this probability. And note that there is also an negative sign there. Now, entropy in general is not negative and that can be mathematically proved. So if we expand this sum will see the equation looks like a second one I explicitly plugged in the two values zero and one. And sometimes when we have 0 log of 0,  we would generally find that as zero because log of 0 is undefined. So this is the entropy function and this function will give a different value for different distributions of this random variable. And this clear it clearly depends on the probability that the random variable taking a value of one or zero. If we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this. At the two ends,  That means when the probability of X = 1 is very small or very large, then the entropy function has a lower value when it's .5 in the middle that it reaches the maximum. Now, if we plot the function against the probability that the X  is taking a value of 0 and the function would show exactly the same curve here. And you can imagine why and so that's because the two probabilities are symmetric  and completely symmetric. So an interesting question. You could think about in general  here is for what kind of X? Does the entropy reached maximum or minimum and we can in particular think about some special cases. For example, in one case we might have a random variable that always takes the value of one,  the probability is one  or there is a random variable that Is equally likely taking a value of 1 or 0. In this case, the probability that X = 1 is .5. Now, which one has a higher entropy? It's easier to look at the problem by thinking of simple example. Using coin tossing,  so when we think about the random experiment like a tossing a coin, it gives us a random variable that  can represent the result. It can be head or tail, so we can define a random variable X sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing. So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head hotel equally likely, so the two probabilities would be,  1/2 right so both will have both equal to 1/2. Another extreme case is completely biased coin, where the coin always shows up as head, so it's a completely biased coin. Now let's think about the entropies in the two cases, and if you plug in these values you can see the entropies,  would be as follows for a fair coin we see the entropy reaches its maximum, that's one. For the completely biased coin we see is 0 and that intuitively makes a lot of sense because a fair coin is most difficult to predict  whereas a completely biased coin is very easy to predict that we can always say it's a head because it is a head all the time so they can be shown on the curve as follows. So the fair coin corresponds to the middle point, or it's very uncertain. The completely biased coin corresponds to the end point. We have a probability of 1.0 and the entropy is 0. So now let's see how we can use entropy for word prediction. Now the problem, let's think about our problem right, still predicted whether W is present or absolutely in this segment. Again, think about the three words. Particularly, think about their entropies. Now we can assume high entropy words are harder to predict. And so we will now have quantitative way to tell us which word is harder to predict. Now if you look at the three words, meat, the and Unicorn again. An we clearly would expect the meat to have a high entropy, then the OR Unicorn. In fact, if you look at the entropy of the,  It's close to 0,  because it occurs everywhere. So, it's like a completed biased coin,  therefore the entropy is 0.
410	27d06808-2624-4922-a079-04dccb301dde	This lecture is about mixture model estimation. In this lecture, we're going to continue discussing probabilistic topic models. In particular, we're going to talk about how to estimate the parameters of a mixture model. So let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. So the idea is to assume that the text data actually contain two kinds of words. One kind is from the background here. So the is away etc and the other kind is from our topic word distribution that we're interested in. So in order to solve this problem of factoring out background words, we can set up our mixture model as follows. We're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub D, which is our target. So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in. But we are going to simplify other things. We are going to assume we have knowledge about others. And this is a powerful way of customizing a model for a particular need. Now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. So we assume the background model is already fixed. And the problem here is how can we adjust theta sub D in order to maximize the probability of the observed document here and we assume all the other parameters are known. Now, although we designed the model heuristically to try to factor out this background words. It's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. So now. In this case, it turns out that the answer is yes, and when we set up the probalistic model in  this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. So to understand why this is so, it's useful to examine the behavior of a mixture model. So we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here. So specifically in this case. Let's assume that the probability of choosing each of the two models is exactly the same, so we're going to flip fair coin to decide which model to use. Furthermore, we are going to assume there are precisely two words: the and text. Obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. So we further assume that the background model gives probability of point nine to the word the and text point one. Now, let's also assume that our data is extremely simple. The document has just the two words text and the. So now let's write down the likelihood function in such a case. First, what's the probability of text and what's the probability of the? I hope by this point and you will be able to write it down. So the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. And it accounts for the two ways of generating text. An inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model. Similarly, the would have a probability of the same form, just with different exact probabilities. So naturally our likelihood function is just the product of the two, so it's very easy to see that. Once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model. Now the interesting question now is, how can we then optimize this likelihood? Well, you will notice that there were only two variables. They are precisely the two probabilities of the two words text and the given by theta sub D. And this is because we have assumed all the other parameters are known. So now the question is a very simple algebra question, right? So we have a simple expression with two variables and we hope to choose the values of these two variables to maximize this function. And the exercise that we have seen some simple algebra problems and note that the two probabilities must sum to one. So there's some constraint. If there were no constraint, of course we would set both probabilities to their maximum value, which would be one to maximize this. But we can't do that because text and the must sum to one. We can't give both a probability of 1. So now the question is how should we allocate the probability mass between the two words? What do you think? Now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probabilities to maximize the value of this function. OK, if we look into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. But there are also competing in someway an in particular they will be competing on the words. And they will tend to bet high probabilities on different words to avoid this competition in some sense. Or to gain advantage in this competition. So again, looking at this objective function and we have a constraint. On the two probabilities. Now. If you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger than the. And this intuition can be well supported by a mathematical fact, which is when the sum of two variables is a constant. Then the product of them, which is maximum when they are equal and this is a fact that we know from algebra. Now if we plug that in, it will  mean that we have to make the two probabilities equal. And when we make them equal an, if we consider the constraint that we can easy to solve this problem and the solution is the probability of text would be point nine and probability of the is point one. And as you can see, indeed the probability of text is now much larger than probability of the. This is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. And if you look at the equation, you will see obviously some interaction of the two distributions here. In particular, you will see in order to make them equal and then the probability assigned by theta sub D must be higher for a word that has a smaller probability given by the background. And, this is obvious from examining this equation because the background part is weak for text it's small. So in order to compensate for that we must make the probability of text given by theta sub D somewhat larger so that the two sides can be balanced. So this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. Would tend to do the opposite. Basically it would discourage other distributions to do the same, and this is to balance them out so that we can account for all kinds of words. And this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. Meaning that they have a very small probability from the background model, like a text here.
410	2997c717-2552-411d-9dc4-7e648e16bbf0	So, as we explained, different textual representation tends to enable different analysis. In particular, we can gradually add more and more deeper analysis results to represent text data, and that would open up more interesting representation opportunities and also analysis capacities. So this table summarizes what we have just seen. So the first column shows the text recognition, the second visualizes the generality of such representation, meaning whether we can do this kind of representation accurate before all the text data, or only some of them, and third column shows the enabled analysis techniques. And the final column shows some examples of application that can be achieved through this level of representation. So let's take a look at them. So as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general. And there are still some interesting applications that can be done at this level. For example, compression of text doesn't necessarily need to know the word boundaries. Although knowing word boundaries might actually also help. Word based representation is very important level of representation. It's quite general and relatively robust. It can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. For example, Thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text. And this can be the case. In research literature, a scientist want to know what are the most important research topics today or customer service people might want to know what are the major complaints from their customers about by mining their email messages. And business intelligence people might be interested in understanding consumers opinions about their products and competitors products to figure out the what are the winning features of their products. And in general there are many applications that can be enabled by the representation at this level. Now moving down, we'll see we can gradually add additional representations. By adding syntactic structures we can enable, Of course, syntactic graph analysis. We can use graph mining algorithms to analyze Syntactic graphs. And some applications are related to this kind of representation. For example, stylistic analysis generally requires syntactical representation. Syntactical structure representation. We can also generate the structure based feature features and those are features that might help us classify text objects into different categories. By looking at the structures, sometimes the classification can be more accurate. For example, if you want to classify articles into different categories corresponding to different authors want to figure out which of the K authors has actually written this article. Then you generally need to look at the syntactic structures. When we add entities and relations, then we can enable lot of techniques such as knowledge graph analysis or information network analysis in general and this analysis would enable applications about entities, for example, discovery of all the knowledge and opinions about the real world energy entity. You can also use this level representation to integrate everything about entity from scattered sources. Finally, when we add logic predicates then we would enable logic inference ofcourse, and this can be very useful for integrative analysis of scattered knowledge. For example, we can also add ontology on top of the extracted information from text to make inferences. A good example of application in this enabled by this level of representation is a intelligent knowledge assistant for biologists. And this is intended program that can help biologists manage all the relevant knowledge from literature about the research problem, such as understanding functions of genes. And the computer can make inferences about some of the hypothesis that biologists might be interesting, for example, whether a gene has a certain function and then the intelligent program can read the literature to extract the relevant facts. Doing by doing information extraction and then using a logical system to actually track that's the answers to researchers questioning about what genes are related to what functions. So in order to support this level of application, we need to go as far as logical representation. Now this course is covering techniques mainly based on word based representation. These techniques are general and robust and thus are more widely used in various applications. In fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level. But obviously all these other levels can be combined and should be combined in order to support sophisticated applications. So to summarize, here are the major takeaway points. Text representation determines what kind of mining algorithms can be applied. And there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc. "And these different representations should in general be combined in real applications to the extent we can. For example, if even if we cannot do accurately, this application of syntactic structures we can stick at partial structures extracted and if we can recognize some entities and that would be great. So in general we want to do as much as we can. And when different levels are combined together, we can enable richer analysis. More powerful analysis. This course, however, focuses on word based representation. Such techniques have also several advantages. First, they are general and robust, so they are applicable to any natural language. That's a big advantage over other approaches that rely on more fragile natural language processing techniques. Secondly, it does not require much manual effort or sometimes it does not require any manual effort. So that's again important benefit, because that means you can apply directly to any application. Third, these techniques are actually surprisingly powerful and effective for many applications. Although not all, of course, as I just explained. Now they are very effective, partly because the words are invented by humans as basic units for communications. So they are actually quite sufficient for representing all kinds of semantics. So that makes this kind of word based representation also powerful. And finally such a word based representation and the techniques enabled by such a representation can be combined with many other sophisticated approaches. So they're not competing with each other.
410	2d0e46c7-df4e-48b3-9550-dac3fec3062d	This lecture is about the ordinal logistic regression for sentiment analysis. So this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. We have an opinionated text document D as input an we want to generate as output already in the range of one through K, so it's discrete rating and thus this is a categorization problem. We have K categories here. Now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories. Intuitively, the features that can distinguish Category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish K from K - 1. For example, positive words generally suggest a higher rating. Now when we train categorisation program by treating these categories as independent, we would not capture this. So what's the solution? In general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. Now let's first think about how we use logistic regression for binary setting categorization problem. So suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. So the predictors are represented as X and these are the features and there are M features altogether, which feature value is a real number, and this can be representation of a text document. And y has two values, binary response variable {0,1}. 1 means X is positive, 0 means X is negative. And then of course, this is a standard two category categorization problem. We can apply logistical regression. You may recall that in logistic regression we assume the log of probability that Y is equal to 1 is assumed to be a linear function of these features as shown here. So this would allow us to also write the probability of y = 1 given X in this equation that you are seeing on the bottom, and so that's logistical function and you can see it relates this probability to probability that y = 1 to the feature values. And of course, B_i is our parameters here. So this is just a direct application of logistical regression for binary categorization. What if we have multiple categories, multiple levels? We actually use such a binary logistic regression program to solve this multi level rating prediction. And the idea is we can introduce multiple binary classifiers and each case we ask the classifier to predict whether the rating is J or above all the ratings lower than J. So when Y_j is equal to 1, it means rating is J or above. When it's zero, that means the rating is lower than J. So basically, if we want to predict rating in the range of one through K, we first have one classifier to distinguish K versus others. And that's our classifier one, and then we're going to have another classifier to distinguish K - 1 from the rest. That's classifier two, and in the end we need a classifier to distinguish two and one So altogether we'll have K - 1 classifiers. Now if we do that of course, then we can also solve this problem, and the logistical regression program would be also very straightforward as you have just seen on the previous slide. Only that here we have more parameters because for each classify we need a different set of parameters. So now the logistic regression classifiers indexed by J, which corresponds to a reading level. And I have also used offer subject to replace beta 0. And this is to make the notation more consistent with what we can show in the ordinal logistic regression. So anyway, so here we now have basically K - 1 regular logistic regression classifiers. Each has its own set of parameters. So now with this approach we can now do rating prediction as follows. After we have trained these K - 1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision. So first let's look at the classifier that corresponds to level of rating K. So this classifier will tell us whether this object should have a rating of K or above. And if its probability according to this logistical regression classifier is larger than .5, we're going to say yes, the rating is K. Now, what if it's not as large as .5? Well, that means the reading is below K, right? So now we need to invoke the next class file, which tells us whether it's above K - 1. It's at least K - 1 and if the probability is larger than .5 then will say, well, then it's K - 1. What if it says no? Well, that means the rating will be even below K minus one, and so we're going to just keep invoking these classifiers until we hit the end. When we need to decide whether it's two or one, so this will help us solve the problem, right? So we can have a classifier that would actually give us a prediction of rating in the range of one through K, unfortunately, such a strategy is not the optimal way of solving this problem, and specifically there are two problems with this approach. So these equations are the same as you have seen before. Now the first problem is that there are just too many parameters. There are many parameters. Now can you count how many parameters we have exactly here? Now this may be interesting exercise. To do so you might want to just pause the video and try to figure out the solution how many premises we have for each classifier? And how many classifiers do we have? You can see the answer is that for each classifier we have N + 1 parameters. And we have K - 1 classifiers altogether, so the total number of premises (K - 1) * (M + 1). That's alot alot of parameters. So when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model? So that's not the idea. The second problem is that these problems these K - 1 classifiers are not really independent. These problems are actually dependent. In general, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers. So we should be able to take advantage of this factor. Now the idea of ordinal logistic regression is precisely that  A key idea is just the improvement over the K -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the Beta parameters these are the parameters that indicate the influence of those weights. And we're going to assume these better values are the same for all the K - 1 premise, and this just encodes our intuition that positive words in general would make a higher rating more likely. So this is intuitively appealing assumption. It's reasonable for our problem set up when we have this order in these categories. Now, in fact, this would allow us to have two positive benefit of one is it's going to reduce the number of parameters significantly. And the other is to allow us to share the training data, because all these parameters are assumed to be equal. So these training data for different classifiers. Can then be shared to help us set the optimal value for beta. So we have more data to help us choose a good beta value. So what's the consequence? The formula would look very similar to what you have seen before, only that now the beta parameter has just one index that correspond to the feature and no longer has the other index that corresponds to the level of rating. So that means we tie them together and there's only one set of beta values for all the classifiers. However, each classifiers there has a distinct Alpha value, the Alpha parameter, the except it's different and this is of course needed to predict the different levels of ratings. So apha subject is different. It depends on J. Different J has a different alpha, but the rest of the parameters of beta are the same. So now you can also ask the question, how many parameters do we have now? Again, that's an interesting question to think about. So if you think about it for a moment and you will see now the plan that we have far fewer parameters. Specifically, we have N + K - 1 because we have M beta values and plus K minus one alpha values. So that's just the basically that's basically the main idea of ordinal logistic regression. So now let's see how we can use such a method to actually assign ratings. It turns out that with this. Idea of tying all the parameters, the beta values, we also end up having a simpler way to make decisions, and more specifically now the criteria whether the predicted probabilities above is or at least .5 above and now is equivalent to whether the score of the object that is. Larger than or equal to negative of  alpha_j as shown here. Now the scoring function now is just taking linear combination of all the features weighted by beta values. So this means now we can simply make a desicion of rating by looking at the value of this scoring function and see which bracket it falls into. Now you can see the General Decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object. So in sum, in this approach we're going to score the object. By using the features and the parameter values, beta values. And this score will then be compared with a set of training the other values to see which range the score is in, and then using the range we can then decide which rating the object should be getting because these ranges of our values correspond to the different levels of ratings, and that's from the way we train these other values. Each is tied to some level of reading.
410	3103be2f-681e-41cf-b0f7-21cf6ba56616	This lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. In this lecture, we're going to continue discussing contextual text mining. And we're going to introduce contextual probabilistic latent semantic analysis As an extension of PLSA for doing contextual text mining. Recall that in contextual text mining we hope to analyze topics in text. In consideration of context so that we can associate the topics with appropriate context that we're interested in. So in this approach contextual  probabilistic latent semantic analysis or CPLSA The main idea is to explicitly add interesting context variables into a generated model. Recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics. But here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context. Or in other words, we can do let the context influence both coverage and content of a topic. The consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in. For example, a particular time period. As extension of PLSA model, CPLSA mainly does the following changes. Firstly it would model the conditional likelihood of text given context. That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. Secondly, it makes 2 specific assumptions about the dependency of topics on context. One is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. The other is that we assume. The topic coverage also depends on the context. And that means depending on the time or location, we might cover topics differently. And then again this dependency would then allow us to capture the association of topics with specific context. We can still use the EM algorithm to solve the problem of parameter estimation. And in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. And this would allow us to do contextual text mining. So this is the basic idea. Now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here. I just want to explain the high level ideas in more detail, particularly willing to explain the generation process of text data that has context associated in such a model. So as you see here, we can assume there are still multiple topics. For example, some topics might represent the themes like a government response donation or the city of New Orleans. Now this example is in the context of Hurricane Katrina and that hit New Orleans. Now, as you can see, we assume there are different views associated with the each of the topics. And these are shown as view one, view two and view three Each view is a different version of word distributions. And these views are tide to some context variables. For example, type to the location Texas or the time July 2005 or the occupation of the other being sociologist. Now on the right side you see now we assume the document has contact information, so the time is known to be July 2005, location is Texas, etc. Now such context information is what we hope to model as well. So we're not going to just model the text. And so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. Now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. Because in the. In the case of location like Texas, people might want to cover the red topics more at the new audience, as visualized here. But in a certain time period, maybe particular topic like donation will be covered more so this variation is also considered in CPLSA. So to generate such a document with context, we first also choose a view. And this view of course now could be from any of these contexts. Let's say we have taken this view. That depends on the time in the middle. So now we have a specific version of word distributions. Now you can see some probabilities of words for each topic. Now, once we have chosen a view, now the situation will be very similar to what happened in standard PLSA. We assume we have got a word distribution associated with each topic, right? And then next to the view we choose a coverage from the bottom. So we're going to choose particular coverage and that coverage. Before is fixed in PLSA and it's hard to a particular document. Each document has just one coverage distribution. Now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. So, for example, we might pick a particular coverage, let's say in this case. We pick We've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in PLSA. So what it means we're going to use the coverage to choose a topic to choose one of these three topics. Let's say we have picked up, let's say, the yellow topic, then withdraw a word from this particular topic on the top. So we might get the word like government. And then next time we might choose a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in PLSA. Now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. So in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage. And naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. And this is precisely what we want in contextual text mining. So here are some sample results from using such a model. Not necessary exactly the same model, but similar models. So on this slide you see some sample results of comparing news articles about Iraq war and Afghanistan war. Now we have about 30 articles. On Iraq war and 26 articles on Afghanistan war. Now, in this case, the goal is to. To review the common topics covered in both sets, articles and the differences or variations of the topic in each of the two collections. So in this case, the context that is explicitly specified by the topical collection. And we see the results here show that. There is a common theme that's corresponding to cluster around here in this column. That there is a common theme indicating that United Nations is involved in both wars is a common topic covered in both sets of articles, and that's indicated by the high probability words shown here United Nations. Now if you the background, of course this is not surprising and this is. This topic is indeed very relevant, to both wars. If you look at the column further and what's interesting is that the next two cells of word distributions actually tell US collection specific variations of the topic of United Nations. So it indicates that in Iraq war, United Nations was more involved in weapon inspections, whereas in Afghanistan war it was more involved in maybe aid to Northern Alliance as a different variation of the topic of United Nations. So this shows that by bringing the context, in this case, different wars are different collections of text. We can have topic variations, tied to these contexts to review the differences of coverage of United Nations in the two wars. Similarly, if you look at the second cluster. Cluster 2 has to do with the killing of people and again. Not surprising if you know the background about wars or the wars involved killing of people. But imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles. It can be used to review common topics in multiple sets of articles as well. If you look down, of course in that column of cluster 2 you see variations of killing of people and that correspond to in different different contexts. And here is another example of results. Obtain the front block articles about the Hurricane Katrina. Now in this case, what you see here is visualization of the. trends of topics overtime. And the top one shows just the temporal chains of two topics. One is oil price and one is. about the flooding of the city. New Orleans. This these topics are obtained from block articles about the Hurricane Katrina. And people talked about these topics. And in addition to some other topics. But the visualization shows that with this technique that we can have conditional distribution of time given a topic. So this allows us to plot this conditional probability. General curves like what you're seeing here. We see that initially the two curves tracked each other very well. But later we see the topic of New Orleans was mentioned again but oil price was not and This turns out to be the time period when another Hurricane Hurricane Rita hit the region that apparently tricked more discussion about the flooding of the city. The bottom curve shows the coverage of this topic about the flooding of the city by block articles in different locations and also shows some shift of coverage. That might be related to peoples migrating from the state of Louisiana to Texas, for example. So in this case we can see the time can be used as context to reveal trends of topics. This is some additional result on special patterns and this. In this case it's about the topic of government response. And there was some criticism about the slow response of government in the case of Hurricane Katrina, and discussion now is covered in different locations and these visualizations show the coverage in different weeks of the event, and initially it's covered mostly in the victim states in the South, but then gradually it's spreading to other. Locations, but in week four, which is shown on the bottom on the left, we see a pattern that's very similar to the very first week on the top left, and that's why again the hurricane Rita hit the region. So such a technique would allow us to use location as context to examine variations of topics. And of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns. Is yet another application of this kind of model where we look at the use of the model for event impact analysis. So here we are looking at the research articles in information retrieval, IR, particularly SIGIR papers. And the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left. And then we hope to examine the impact of two events. One is the start of TREC for text retrieval conference. This is a major evaluation effort sponsored by US government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information retrieval. The other is the publication of a Seminal paper by Croft and Ponte, and this is about the language modeling approach to information retrieval. It's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divide the time periods into a period before the event an another after this event, and then we can compare the differences of. The topics, the coverage and variations, etc. So in this case the results show I've seen that before TREC the study of retrieval models was mostly a vector space model, Boolean model, etc. But the after TREC. Apparently the study of retrieval models have involved a lot of other words that seem to suggest some different retrieval tasks though. So for example email was used in the enterprise search tasks and subtropical retrieval, with another task introduced later by TREC. On the bottom we see the variations that are correlated with the publication of the language model paper. Before we have those classical probabilistic model logic model, Boolean model etc. But after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. So this technique here can use event as context. To understand the impact of event again, the technique is general so you can use this to analyze the impact of any event. Here are some suggested readings. The first is paper about simple extension of PLSA to enable cross collection comparison. It's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. The second one is the main paper about the CPLSA model with a discussion of a lot of applications. The third one has a lot of details about this special temporal patterns for Hurricane Katrina, example.
410	347e8785-e040-43a6-b198-2dcb12a2ce2a	this letter is about the specific a smoothing methods for language models used in probabilistic of retrieval model in this laptop we will continue the discussion of language models for information retrieval particular the query like rover retrieval method and wouldn't talk about the specific a small the methods used for such a retrieval function so this is a slide from a previous lecture where we show that with the query like holder ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following so this is the how the retrieval function based on this assumption is that we have discussed you can see it's a sum of all the magic query terms here and inside of some it's the count of determine the query and some weight for the term in the document and we have tearful idea of wait here and then we have another constant that here and so clearly if we want to implement this function using a programming language we still need to figure out a few variables in particular we're going to need to know how to estimate the probability of world exactly and how do we set off on so in order to answer these questions we have to think about this very specific smoothing methods and that is the main topic of this lecture wouldn't talk about the tools move methods the first is simple linear interpolation with fixed coefficient and this is also halted jelinek immersive smoothie so the idea is actually very simple this picture shows how we estimate document the language model by using maximum like flores method that gives us ward accounts normalized by the total number of words in the text the idea of using this method is to maximize the probability of the opposite of the text as a result if a word like network is not observing the text it's going to get zero probability as soon here so the idea of smoothing then is to rely on collection language model where this world is not going to have a zero probability to help us decide what non zero probability should be assigned to such a world so we can note that network has a non zero probability here so in this approaching what we do is we do linear interpolation between the maximum like harassment here and the collection language model and this is controlled by the smoothing parameter lambda which is between zero and one so this is a smoothing parameter the larger lemme lisa to the more smoothing we have we will have so by mixing them together we achieve the goal of assigning larger probabilities to award night network so let's see how it works for some of the words here for example if we compute the smooth probability for text now the maximum micro S made gives us ten over one hundred and that's going to be here but the collection probability is this so we just combine them together with this simple formula we can also see the word network which used to have a zero probability now is getting a non zero probability of this value and that's be cause the count is going to be zero four network here but this spot is now zero and that's basically how this method works if you think about this and you can easy to see now the alpha sub T in this model method is basically lemma because that's remember the coefficient in front of the probability of the world came in by the clashing language model here right OK so this is the first model method the signal one is similar but it has a dynamical coefficient for linear interpolation is often called the original apply or bayesian smoothie so again here we face the problem of zero probability for unseen world like a network again we will use the collection language model but in this case we're going to combine them in somewhat different ways the formula first can be seen as an interpolation of the left my car is made and the collection and model as before as in the james multimethod only that the coefficient now is not a lambda a fixed number but that dynamically coefficient in this form where mu is a parameter it's a non negative value and you can see if we set mutua constant the fact is that a long document with actually get a smaller coefficient in here because a long document we have longer length therefore the coefficient is actually smaller and so a long document would have less smoothing as we would expect so this since to make more sense than a fixed for coefficient smoothie of course this pot would be of this form so that the two coefficients would someone now this is one way to understand that this is more the basically it means it's a dynamical coefficient interpolation there is another way to understand this formula which is even easier to remember and that's on this side so it's easy to see we can rewrite the smoothing method in this form now in this form we are easy to see what change we have made it to the maximum micro estimator which would be this part right so normalize account by the documents so in this form we can see what we did is we add this to the count of every word so what does this mean well this is basically something related to the probability of the world in the collection and we multiply that by the parameter mu and when we combine this with the count here essentially we are adding pseudocounts to the observed that text we pretend every word has got this many pseudo count so the total count would be the sum of these pseudo counts and the actual count of the war in the document as a result in total we would have added at this many pseudocounts why because if you take a some of this this one overall the words then we see the probability of the words would someone and that gives us just mute so this is a total number of pseudo comes that we added and so these probabilities would still something to one so in this case we can easily see the method is essentially too added these as a pseudo count to this day to pretend we actually augment the data by including some pseudo there are defined by the collection language model as a result we have more counts based the total counts for word award it would be like this and as a result evening for word has zero counter here let's say if we have zero come here and it would still have non zero account cause of this part so this is how this method works let's also take a look at the some specific example here i thought for text again will have ten as original count that we actually observe but we also added some pseudo count and so the probability of texture would be of this form naturally the probability of network would be just this part and so here you can also see what's offers up the ear can you see it if you want to think about you can pause the video have you notice that this part is basically of a sub D so we can see this case other submitters depend on the document because this length depends on the document whereas in the linear interpolation that james move method that this is a constant
410	380a7417-6702-4df8-9818-5aceba7cde2b	This lecture is  about topic mining and analysis. "We "As you see on this roadmap, we have just "we have just "about the language namely discovery of word  associations such as paradigmatic relations relations and syntagmatic relations. Now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. The main topics. In the text. And we call that topic mining and analysis. In this lecture we're going to talk about its motivation and the task definition. So first, let's look at the concept of topic. So topic is something that we all understand, I think, but it's actually not that easy to formally define it. Roughly speaking, topic is the main idea discussed in text data, and you can think of this as a theme or subject of discussion or conversation. It can also have different granularities. For example, we can talk about the topic of a sentence. A topic of article topic of a paragraph, or the topics of all the research articles in the digital library. So different granularities of topics obviously have different applications. Indeed, there are many applications that require discovery of topics in text and then analyze them. Here are some examples. For example, we might be interested in knowing what are Twitter users talking about today? Are they talking about NBA sports or talking about some international events, etc. Or we are interested in knowing about the research topics. For example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago. Now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past. And then we can make a comparison. We might be also interested in knowing what do people like about some product like iPhone 6 and what do they dislike? And this involves discovering topics in positive opinions about iPhone 6 and also negative reviews about it. Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election? And all these have to do is discovering topics in texts and analyzing them, and we're going to talk about a lot of techniques for doing this. In general, we can view topic as some knowledge about the world. So from text that we expected to discover a number of topics and then this topic generally provide the description about the world and it tells us something about the world, about the product, about the person, etc. Now when we have some non-text data then we can have more context for analyzing the topics. For example, we might know the time associated with the text data or locations where the text data will be produced or the authors of text or the sources of the text etc. All such meta data or context variables can be associated with the topics that we discover. And then we can use these context variables to help us analyze patterns of topics. For example, looking at topics overtime, we would be able to discover whether there's a trending topic or some topics might be fading away. Similar looking at the topics in different locations, we might know some insights about people's opinions in different locations. So that's why mining topics is very important. Now let's look at the tasks of topic mining and analysis. In general, it would involve first discovering a lot of topics. In this case K topics. And then we also would like to know which topics are covered in which documents, to what extent. So for example in. Document one we might see that topic 1 is covered a lot, topic 2 and topic k are covered with a small portion. And other topics perhaps are not covered. Document 2, on the other hand,  covered topic 2 very well, but it did not cover topic 1 at all and also covers topic K to some extent. etc., right? So now you can see there are generally two different tasks or subtasks. The first is to discover K topics from a collection of text data. What are these K topics? OK, major topics in the text data. The second task is to figure out which documents cover which topics to what extent. So more formally, we can define the problem as follows. First, we have as input a collection of N text documents. Here we can denote that text connection. as C. And denote a text article as di and we generally also need to have as input the number of topics K. But there may be techniques that can automatically suggest a number of topics, but in the techniques that we will discuss which are also the most useful techniques, we often need to specify a number of topics. Now the output would then be the K topics that we would like to discover denoted as theater sub one through theta sub k. Also we want to generate the coverage of topics in each document d sub i and this is denoted by π sub i j and "π sub i j document d sub i covering topic theta sub j. So obviously for each document we have a set of such values indicate. To what extent did the document covers each topic. An we can assume that these probabilities sum to one, because a document won't be able to cover other topics outside the topics that we discussed we discovered. So now the question is how do we define theta sub i? How do we define the topic now? This problem has not been completely defined until we define what is exactly theta. So in the next a few lectures, we're going to talk about different ways to define theta.
410	384d42d5-644d-4649-b3be-c744c3d84e1a	This lecture is about the feedback in the language modeling approach. In this lecture we will continue the discussion of feedback in text retrieval. In particular, we're going to talk about the feedback in language modeling approaches. So we derive the query likelihood ranking function by making various assumptions. As a basic retrieval function, that formula of those formulas worked well, but if we think about the feedback information, it's a little bit of awkward to use query like hold too. Perform feedback because a lot of times the feedback information is additional information about the query. But we assume that the query is generated by assembling words from a language model in the query likelihood method. It's kind of a natural to sample words that form feedback documents as a result, then researchers proposed a way to generalize query like hold function and it's called a callback labeler divergance retrieval model. And this model is actually going to make the query likelihood retrieval function much closer to vector space model. Yet, this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query likelihood as a special case. And in this case, then, feedback can be achieved through simple query model estimation or updating. This is very similar to Rock Hill which updates the query vector. So let's see. What is this KL Divergence retrieval model? So on the top, what you see is a query like hold. Retrieval function right this one. And then kill diversions or also called cross entropy. Whichever model is basically to generalize. The frequency part here. Into a language model. So basically it's the difference. Given by the probabilistic model here to characterize what the user is looking for versus the count of query words there. And this difference allows us to plug in various different ways to estimate this, so this can be estimated in many different ways, including using feedback information. Now this is called KL divergence because This can be interpreted as measuring the care divergent of two distributions. One is the query model denoted by this distribution. One is talking the language model. Here smoothly with collection language model, of course an. We're not going to talk about the detail of and then find it in some references. It's also called a cross entropy, because in effect we can ignore some terms in the care divergent function and we will end up having actually cross entropy that both our terms in information theory. But anyway for. Our purpose here you can just receive the two formulas look almost identical, except that here we have a probability of award given by a query language model. All this. And here the sum is over all the words that are in the document and also with the non zero probability for the query model. So it's kind of again a generalization of some overall the match query words. Now you can also easy to see we can recover the query like hold retrieval function by simply setting this query model to the relative frequency of award in the query. This is very easy to see once you plug this into. Here you can eliminate this query Lance. That's a constant and then you get exactly like that. So you can see the equivalence. And that's also why this care divergent model can be regarded as a generalization of query, like whole because we can cover query like Rd as a special case. But it would also allow such rule much more than that. So this is how we can use the care divergent model to them the feedback the picture shows that we first estimate the document language model. Then we estimate the query name model and we compute the KL diversions, as is often denoted by AD here. But this basically means this is exactly like a vector space model 'cause we computer vector for the document the computer, another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions. And then we got the results and we can find some feedback documents. Let's assume they are most inactive. Sorry, mostly positive documents, although we could also consider both kinds of documents. So what we could do is like in rock you'll ever know computer another language model, coder feedback, language model here. Again, this is going to be another vector, just like a computing central about the in Rock Hill. And then this model can be combined with the original query model. Using a linear interpolation. And this would then give us a update model just like again in Rock Hill. So here we can see the parameter Alpha can control the amount of feedback if it's set to 0, then it says here there's no feedback after set to one, we got 4 feedback. If we ignore the original query and this is generally not desirable, right? So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important. So of course the main question here is how do you compute this single F? This is the big question here, and once you can do that, the rest is easy. So here we will talk about one of the approaches and there are many approaches. Of course this approach is based on generated model. And I'm going to show you how it works. This is the user generated mixable. So this picture shows that we have this model. Here. The feedback model that we want to estimate. And will the basis is the feedback documents. Let's say we are observing the positive documents. These are the click the documents by users or relevant documents judged by users or simply top ranked blocking that we assume to be relevant. Now imagine how we can compute the centroid for these documents by using language Model 1. Approach is simply to assume these documents are generated from this language model as we did before. What we could do is do just normalize the word frequency here and then we got this world distribution. Now the question is whether this distribution is good for feedback. But you can imagine. The top ranked the world would be what? What do you think? Those words would be common words, right? As we always see in a language model, the top ranked words are actually common words like the etc. So it's not very good for feedback because we would be adding a lot of such words to our query when we interpret this with the original query model. So this is not good. So when it do something in particular will are trying to get rid of those common words and we are we have seen actually one way to do that by using background language model. In the case of learning the associations with words, words that are related to the water computer. We could do that and there will be another way to do this, but here we are going to talk about another approach which is more principled approach. In this case, we're going to state, well, you said that there are common words here in this. These documents that should not belong to this topic model, right? So now what we can do is to assume that, well, those words are generated from background language model, so they were generated those words like the. Example. And if we use maximum likelihood estimator, note that if all the words here must be generated from this model, then. This model is forced to assign high probabilities to award like that because it occurs so frequently here. Note that in order to reduce its probability in this model. We have to have another model which is this one to help explain the world. The here and in this case it's not appropriate to use the background language model to achieve this goal, because this model would assign high probabilities to these common words. So in this approach, then we assume this machine that was generated. These words would work as follows. We have a source controller here. Imagine we flip a coin here to decide what distribution to use with probability of Lambda. The coin shows up as head and we're going to use the background language model and we can do that simple word from that model with probability of 1 minus them. Now will do decide to use the unknown. Topic model here that we would like to estimate and we're going to then generate award here. If we make this assumption and this whole thing would be just one model and we call this mixture model 'cause there are two distributions that are mixed together and we actually don't know when each distribution is used. So again, think of this whole thing as one model. And we can still ask for words, and it will still give us a word in a random manner, right? And of course, which word will show up would depend on both this distribution and that distribution. In addition, it would also depend on this Lambda, because if you say Lambda is very high and it's going to always use the background distribution, you'll get different words than if you say lemme's very small, we're going to use this. Right, so all these are parameters. In this model. And then if you think in this way, basically we can do exactly the same as what we did before. We are going to use maximum likelihood estimator to adjust this model to estimate the parameters. Basically we're going to adjust well this parameter. So that we can best explain all the data. The difference now is that we are not asking this model alone to explain this. But rather, we're going to ask this whole model mixture model to explain the data becauses there has got some help from the background model. It doesn't have to assign high probabilities towards like the. As a result, it would then assign higher probabilities to other words that are common here. But not having high probability here. So those will be common here. I. And if they are common, they would have to have high probabilities according to maximum likelihood estimator. An if they are rare here. Right, so if they are rare here. Then you don't get much help from this background model. As a result, this topic model must assign high probabilities, so the high probability words according to the topic model would be those that are common here, but rare in the background. OK, so this is basically a little bit a IDF waiting here. But this would allow us to achieve the effect of removing this top awards that are meaningless in the feedback. So mathematically, what we have is to compute the like hold again local, like hold of the feedback documents and. And note that we also have another parameter Lambda here, but we assume that the Lambda denotes the noise in the feedback document. So we are going to, let's say set this to a parameter that say 50% of the words are noise or 9% are noise and this can be assumed to be fixed if we assume this is fixed. Then We only have these probabilities as parameters, just like in the simplest unigram language model. We have end parameters and is the number of words. And then the likelihood function would look like this. It's very similar to the likelihood function local. I can hold a function we see before, except that inside the logarithm there's a some here, and this sum is becausw. We consider two distributions. And which one is used would depend on Lambda and that's why we have this form. But mathematically, this is their function with theater as unknown variables, right? So this is just a function or the other values are known except for this guy. So we can then choose this probability distribution to maximize this locali code. The same idea as the maximum, like Horace made it as a mathematical problem. We just we just have to solve this optimization problem. We essentially would try all the theater values and until we find one that gives this whole thing the maximum probability. So it's a well defined math problem. Once we have done that, will obtain the serial F that can be there, interpreted with the original query model to do feedback. So here are some examples of the feedback model learned from a Web document collection, and we do sudo feedback. Are we just use the top ten documents and we use this mixture model so the query is airport security? What we do is we first retrieve 10 documents from the web database. And this is of course a pseudo feedback. I and then we're going to feed that mixture model to this 10. Document set. And these are the words learned using this approach. This is the probability of award given by the feedback model in both cases. So in both cases you can see the highest probability words include very relevant words to the query, so airport security, for example. This query words still show up as high probabilities in each case naturally becausw they occur frequently in the top ranked documents, but we also see beverage, alcohol, bomb, terrorists, etc. So these are relevant to this topic and they if combined with the original query. Can help us match more accurately documents and also they can help us bring up a documents that only imagine the some of these other words. Maybe for example just the airport and then bomb for example this. So this is how single feedback works. Issues that this model really works and picks up some related words to the query. What's also interesting is that if you look at the two tables here and you compare them and you see in this case when Lambda is set to a small value and we still see some common words here. And that means. When we don't use the background more often, remember Lambda can use the probability of using the background model to generate the text. If we don't rely much on background model, we still have to use this topic model to account for the common words, whereas if we set Lambda to a very high value, we will use the background model very often to explain these words. Then there's no burden on explaining those common words in the feedback documents by the topic model. So as a result of the topic model, here is very discriminant if it contains all the relevant words without common words. So this can be added to the original query to achieve feedback. So to summarize, in this lecture we have talked about the feedback in language model approach. In general, feedback is to learn from examples. These examples can be assumed, examples can be sued, examples like. Assume that the top ten documents that are assumed to be random in there could be based on using fractions like a feedback based on pixels or implicit feedback. We talked about the three major feedback scenarios, relevance feedback, sooner feedback, and in principle feedback. We talked about how to use Rock You to do feedback in vector space model and how to use query model is missing for feedback in language model and we briefly talked about the mixture model and the basic idea. There are many other methods, for example, the relevance model is a very effective model for estimating query model. So you can read more about these methods in the references that listed at the end of this lecture. So there are two additional readings here. The first one is a book that has a systematic review and discussion of language models for information retrieval and signal. One is important research paper that's about relevance based language models, and it's a very effective way of computing query model.
410	3956403f-f159-448a-9514-5dc69f314c5a	This lecture is about the evaluation of taxable categorization. So we've talked about many different methods for taxi categorisation, but how do you which method works better? And for a particular application, how do you this is the best way of solving your problem. To understand these will have to. How to we have to know how to evaluate categorisation results? So first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called Cranfield Evaluation Methodology. The basic idea is to help humans to create test collection. Where we already every document is tagged with the desired categories, or in the case of search for which query, which documents should have been retrieved and this is called ground truth. Now with this ground truth test collection, we can then reduce the collection to test many different systems and compare different systems. We can also turn off some component in system to see what's going to happen. Basically it provides. A way to do controlled experiments to compare different methods. So this methodology has been virtually used for all the tasks that involve empirically defined problems. So in our case, then we're going to compare our systems categorization results with the categorisation ground truth created by humans. And we're going to compare our systems decisions on which documents should get which category with what. Categories have been assigned to those documents by humans and we want to quantify the similarity of these decisions. Or equivalently, to measure the difference between the system output and desired ideal output generated by the humans? So obviously the higher similarity is, the better the results are. The similarity can be measured in different ways. And that would lead to different measures, and sometimes it's desirable also to measure the similarity from different perspectives just to have a better understanding of the results in detail. For example, it might be also interested in knowing which category performs better, which category is easy to categorize, etc. In general, different categorization mistakes, however, have different costs for a specific application, so some errors might be more serious than others. So ideally we would like to model such differences. But if you read many papers in texture catalyzation, you will see that they don't generally do that, and instead they will use a simplified measure. And that's the cause. It's often OK not to consider such a cost variation when we compare different methods. An we when we are interested in knowing the relative difference of these methods. So it's OK to introduce some bias as long as the bias is not correlated with a particular method. And then we should still expect the more effective method to perform better than a less effective one, even though the measure is not perfect. So the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions. So here you show that here you see that there are K categories denoted by C1 through CK and there are N documents in order by D1 through DN an for each pair of a category on the document that we can then look at the situation. And see if the system has said yes to despair. Basically has assigned this category to this document or no, so this is denoted by Y or N. That's the system to decision. And similarly we can look at the humans decision. Also, if the human has assigned a category to the document, there will be a plus sign here. That's just that just means a human would think this assignment is correct an if the incorrect, and then there's a minus. So we will see. All combinations of these ends yes and Nos with minus and plus. So there are four combinations in total and two of them are correct and when we have Y plus or minus and then there are also two kinds of errors. So the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. So we know that the total number of decisions is. In multiplied by K. And the number of characters decisions obviously are basically of two kinds. One is why pluses and the other is N minus is and we just put together the account. Now this is a very convenient measure that will give us a one number to characterize performance of method and the higher the better of course. But the method I also had some problems. First it has treated all the decisions equally so, but in reality there's some decision errors are more serious than others. For example, it may be more important to get the decisions right on some documents than others, and or maybe more important to get the divisions right on some categories than others, and this would call for some detailed evaluation of this results to understand. The strengths and weaknesses of different methods. And to understand the performance of these methods in detail. In APA category or per document basis? One example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. Missing a legitimate email is all is 1 type of error. But letting us ma'am to come into your folder is another type of error. The two types of errors are clearly very different because it's very important not to miss a legitimate email. It's OK to occasionally let us spam email to come into your inbox, so the error of the first missing a legitimate email is very high cost. It's very serious mistake. And classification error classification accuracy does not address this issue. There's also another problem with imbalanced tests at the Imagine there's a skew. The test set where most instances are in category one. And 98% of instances are in category one only 2% are in category Two. In such a case, we can have a very simple baseline that actually performs very, and the baseline would Simply put all instances in the major category. That would give us 98% accuracy. In this case, it's going to be appearing to be very effective, but in reality this is obviously not a good result. And so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced. And we wonder about equal number of instances. For example, in each class the minority categories or classes tend to be overlooked in the evaluation of classification accuracy. How to address these problems? We of course would like to also evaluate the results in other ways and in different ways. As I said, it's beneficial to look at the actual must multiple perspectives. So for example, we can look at the perspective from each document perspective based on each document. So the question here is, how could other divisions on this document? Now, as in the general cases of all decisions, we can think about four combinations of possibilities. Depending on whether the system has said yes, and depending on whether the human has said it correctly or incorrectly, or say yes or no, and so the four combinations are first. When both the human system said yes and that's true positives when the system says yes, it's actually positive. So when the system says yes, it's a positive. But when the human confirmed that it is indeed correct, that becomes true positive. When the system says yes, but human says no, that's incorrect. That's a false positive FP. And when the system says no, but the human says yes, then it's a false negative. We missed one assignment. When does the system and human said no? Then that's also corrected vision. That's true negatives. Alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. And these are also proposed by information retrieval researchers in 19, six days for evaluating searching results. But now they have become a standard measure used everywhere. So when the system says yes, we can ask the question how many are correct? What's the percentage of correct decisions when the system says yes? That's called precision. It's a true positive divided by all the cases when the system says yes all the positives. The other recall the other meshes called Recall an this measures. Whether the document that has called all the categories it should have. So in this case it's divided the true positive by true positives and false negatives. So these are all the cases where this human says the document should have this category. So this represents the old categories that it should have got an. So recall tells us whether the system has actually indeed assigned all the categories that it should have to this document. This gives us a detailed view of the decision on each document. Then we can aggregate them later. And if you're interested in some documents and this would tell us how well we did that those documents a subset of them might be more interesting than others. For example, and this allows us to analyze errors in more detail as well. We can separate the documents of certain characteristic from others and then look at the errors. You might see a pattern here for this kind of documents along documents it doesn't do as well as. For short documents. And this gives you some insight for improving the better. Similarly, we can look at the popular category valuation. This. In this case we're going to look at the how good are the decision on a particular category. And as in the previous case, we can define precision and recall and it will just basically answer the questions from a different perspective. I saw when the system says yes, how many are corrected that means looking at this category to see if all the documents that are assigned with this category are indeed in this category. An recall would tell us has the category being actually assigned to all the documents that should have this category. Is sometimes also useful to combine precision and recall as one measure, and this is often done by using if mesh. And this is just the harmonic mean of precision and recall defined on this slide. Ann It's also controlled by a parameter beta two to indicate the weather precision is more important, or recall is more important when beta is set to one, we have a measure called F1, and in this case we just take a equal weight on both precision and recall. If one is very often used as a measure for categorisation. Now, as in all cases when we combine results, you always should think about the best way of combining them. So in this case I don't know if you have thought about it and we could have combining them just with the arithmetic mean, right? So that would still give it the same range of values. But obviously there's a reason why we didn't do that and why. If one is more popular and it's actually useful to think about difference. And if you think about that, you will see that there is indeed some difference and sum. Undesirable property of this arithmetic mean. Basically, it would be obvious to you if you think about a case when the system says yes for all the category and nothing appears. And even tried to compute the precision and recall in that case and see what would happen. I basically this kind of measure will not the arithmetic mean is not going to be as reasonable FF1, which tends to prefer a tradeoff between precision and recall. So that the two values are about equal, so we if there's an extreme case where you have 041 value and one for the other, than F1 will be low, but the arithmetic mean would still be reasonably high.
410	39d13817-de51-4195-a33a-985b0b54e64d	This lecture is about the text categorization. In this lecture we're going to talk about the text categorization. This is a very important technique for a text, data mining and analytics. It is relevant to discovery of various different kinds of knowledge as shown here. First is related to topic mining analysis. And that's because it has to do with analyzing text data based on some predefined topics. Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. Because we can categorize the authors, for example, based on the content of the articles that they have written. We can in general categorize the observer based on the content. That they produce. Finally, it's also related to text based prediction. Because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. And so this is a very important technique for text data mining. This is the overall plan for covering the topic. First we're going to talk about what is text categorization and why we are interested in doing that in this lecture. And then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. The problem of texture categorisation is defined as follows. We're given a set of predefined categories. Possibly forming a hierarchy so. And often also a set of training examples or training set of labeled text objects. Which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. So the picture on the slide shows what happens. When we do text categorization, we have a lot of text objects to be processed by a categorisation system. And the system will in general assign categories to these documents as shown on the right. And the categorisation results. And we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in different categories, and this would further help the system then learn how to recognize. The categories of new tax objects that it has not seen. So here are some specific examples of text categorization and In fact, there are many examples. Here are just a few. So first text objects can vary, so we can categorize a document. Or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can vary a lot, so this creates a lot of possibilities. Secondly, categories can also vary, and we can generally distinguish two kinds of categories. One is internal categories. These are categories that characterize content of text object. For example, topic categories. Or sentiment categories and they generally have to do with the content of the tax objects, direct Characterization of the content. The other kind is external categories that can characterize the entity associated with the text object. For example, authors or entities associated with the content that they produce. And so we can use their content, determine which author has written which part, for example, and that's called author attribution. Or we can have any other meaningful categories associated with text data, as long as. There is a. There are, there's a meaningful connection between the entity and text data. For example, we might collect a lot of reviews about a restaurant. Or a lot of reviews about the product. And then these text data can help us infer properties of product or a restaurant. In that case, we can treat this as a categorization problem. We can categorize restaurants or categorize products based on their corresponding reviews. So this is example of external category. Here are some specific examples of applications. News categorization is very common, has been stuided. A lot. News agencies would like to assign predefined categories to categorize news generated every day. And literature article categorizations another important task, for example, in biomedical domain, Is this mesh annotations , mesh stands for medical subject heading. And this is ontology of terms characterize content of literature articles in detail. Another example of application spam, email detection or filtering right? So we often have a spam filter to help us distinguish spam from legitimate emails, and this is clearly a binary classification problem. Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize content into positive or negative or positive and negative or neutral. so you can have the same sentiment categories assigned. to text content. Another application is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category. There is also another important kind of applications of routing emails to the right person to handle. So in helpdesk email messages generally routed to a particular person to handle different people attempt to handle different kinds of requests and in many cases a person will manually assign the messages to the right people. But you can imagine you can build automatic text categorization system to help routing a request. And this is to classify the incoming request in to one of the categories where each category actually corresponds to a person to handle the request. And finally, author Attribution. As I just mentioned, is yet another application, and it's another example of using text to actually infer properties of some other entities. And there are also many variants of the problem formulation and so first we have the simplest case, which is a binary categorization where there are only two categories and there are many examples like that information retrieval or search engine applications would want to. Distinguish it relevant documents from non relevant documents for a particular query. Spam filter is interesting. Distinguishing spams from non spam. So also two categories. Sometimes classification of opinions can be in two categories, positive and negative. A more general case would be K-category categorization and there are also many applications like that. There could be more than two categories, so topical categorisation is often such example where you can have multiple topics. Email routing would be another example when you may have multiple folders, or if you route the email to the right person to handle it, then there are multiple people, to clasify so in all these cases there are more than two kinds of categories. And another variation to have hierarchical categorization, where categories form hierarchy, again, topical hierarchy is very common. Yet another variation is joint categorization. That's when you have multiple categorization tasks that are related. And then you hope to kind of do joint categorization. Try to leverage the dependents of these tasks to improve accuracy for each individual task. Now among all these, binary categorization is most fundamental and partly also because it's simple and partly it's cause it can actually be used to perform all the other categorization tasks. For example, K category categorisation task can be actually performed by using binary categorization. And basically we can look at each category separately and then the binary categorization problem is whether object is in this category or not. Meaning in other categories. And the hierarchical category categorisation can also be done by progressively doing flat categorisation at each level. So we can first categorize all the objects in tune. It's a small number of high level categories an inside each category. We can further categorize into sub categories etc. So why is text categories important well, I already showed you several applications, but in general there are several reasons. One is text Categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. So now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks. But we can also add categories and they provide 2 levels of representation. Semantic categories assigned can also be directly or indirectly useful for application. So for example, sentiment categories could be already very useful, or author Attribution might be directly useful. And. Another example is when semantic categories can facilitate aggregation of tax content, and this is another case of. Applications of text categorisation. For example, we if we want to know the overall opinions about the product, we could first categorize the opinions in each individual review as positive or negative, and then that would allow us to easily aggregate all the sentiments and it will tell us about 70% of the views positive and 30% are negative, etc. So without doing categorization it will be much harder to aggregate such opinions. So it provides a concise way of coding text in some sense based on our vocabulary. And sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary. The second kind of reasons is to use text categorization to infer properties of entities. And text categorisation allows us to infer the properties of such entities that are associated with text data. So this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. So it's useful to think about the information network that will connect the other entities with text data. The obvious entities that can be directly connected are authors, but you can also imagine the authors affiliations or the authors ages and other things can be actually connected to text data indirectly. Once we can make the connection, then we can make predictions about those values. So this is a general way to allow us to use text mining tool. Sorry, text categorization to discover knowledge about the world. Very useful, especially in big text data. Analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. Often together with non text data specifically to text. For example, we can also think of examples of inferring properties of entities. For example discovery of non native speakers of a language and this can be done by categorizing the content of. Speakers Another example is to predict the party affiliation of a politician based on the political speech at this is again example of using text data to infer some knowledge about real world. In nature this all the problems are all the same and that's as we defined and it's a text categorization problem.
410	3eca1e42-a7a7-433c-ba9f-6ee90351395f	So we talked about page rank as a way to. "" Capture the authorities now we also looked at the some other examples where a hub might be interesting, so there is another algorithm called hits and that's going to compute the scores for authorities and hubs. Intuitions are pages that are wider. Sites are good authorities, then where pages that cite many other pages are good hubs, right? But the I think the most interesting idea of this algorithm hits is. It's going to use reinforcement mechanism to kind of help improve the scoring for hubs and authorities, and here. So here's the idea. It would assume that good authorities are cited by good hubs. That means if you're cited by many pages with good hub scores, then that increases your authority score and similarly good hubs are those that pointed to good authorities. So if you get you pointed to a lot of good authority pages, then your hub score will be increased. So then we can iterate, reinforce each other 'cause you can point to some good hubs so that you can point to some good authorities to get a good hub score, whereas those authoritie scores. Would be also improved because they are pointed to by a good hub and this algorithm is also general. It can have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct the matrix, but this time we're going to construct the adjacency matrix and we're not going to normalize the values. So if there's a link, there is one. If there's no link that's zero again it's the same graph. And then we're going to define the Hub score of page as the sum of the authority scores of all the pages that it points to. So whether you are a hub really depends on whether you're pointing to a lot of good authority pages. That's what it says in the first equation. In the second equation, we define the authorities score over page as the sum of the hub scores of all those pages that the point to you. So whether you are good authoritie would depend on whether those pages that are pointing to you are good hubs so you can see this forms iterative reinforcement mechanism. Now these two equations can be also written in the matrix format. I saw what we get here is then the hub vector is equal to the product of the edges of the adjacency matrix and the authority vector. And this is basically the first equation right? And similarly the second equation can be returned as the authoritie vector is equal to the product of A transpose multiplied by the hub vector and these are just different ways of expressing these equations. But what's interesting is that if you look at the matrix form, you can also plug in the authority equation. Into the first one. So if you do that, you can actually then eliminate the authoritie vector completely and you get the equation of only hub scores, right? The Hub score vector is equal to A multiplied by A transpose multiplied by the hub score vector again. And similarly, we can do a transformation to have equation for just the authority scores. So although we framed the problem as computing hubs and authorities, we can actually eliminate one of them to obtain equation just for one of them. The difference between this and page rank is that now the matrix is actually a multiplication of the edges in the matrix and its transpose, so this is different from page rank. But mathematically, then we will be computing the same problem. So in hits we typically would initialize the values that said, one for all these values and then we would iteratively apply these equations, essentially and This is equivalent to multiply that by the Matrix A and A transpose. So the algorithm is exactly the similar page rank, but here because the adjacency matrix is not normalized. So what we have to do, what we have to do is after each iteration, we're going to normalize and this would allow us to control the growth of value, otherwise they would grow larger and larger. And if we do that and then we'll basically get hits algorithm to compute the hub scores an authority scores for all the pages. And these scores can then be used in ranking just like a page rank scores. So to summarize, in this lecture we have seen that link information is very useful. In particular, the anchor text is very useful to increase the. The text representation of a page and we also talk about page rank and hits on as two major link analysis algorithms. Both can generate scores for web pages that can be used in the ranking function. Note that page rank and it's also very general algorithms, so they have many applications in analyzing other graphs or networks.
410	3f166a89-1603-4016-909f-cac980864478	this letter is about learning to rank in this lecture we're going to continue talking about web search in particular we're going to talk about the using machine learning to combine different features to improve the ranking function so the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results in the previous lectures we have talked about a number of ways to rank documents we have talked about some retrieval models like a VM twenty five or query like code they can generate the content basis course for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranging now the question now is how can we combine all these features an potential million other features to do ranking and this will be very useful for ranking web pages not only just to improve accuracy but also the improve the robustness of the ranking function so that it's not easy for a spammer to just perturb a one or a few features to promote a page so the general idea of learning to rank is to use machine learning to combine these features were optimized the weights on different features to generate the optimal ranking function so we will assume that the given a query document pair Q M D we can define a number of features and these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as VM twenty five or query like whole door pivot condense formalization or P L two etc it can also be linked based on score like page ranks for it can be also application of retrieval models to the anchor text of the page like those are the types of descriptions of links that pointed to this page so these can all be cruise about whether this document is relevant or not we can even include a feature such as whether the URL has theater because this might be an indicator of home page or engine page so all these features can then be combined together through generated ranking function the question is of course how can we combine them in this approach if we simply hypothesize that the probability that this document is relevant to this query is a function of all these features so we can hypothesize is that the probability of relevance is related to these features through a particular form of the function that has some parameters these parameters can control the inference of different features on the final relevance this is of course just assumption whether this is something really makes sense is the big question still have to empirically evaluate the function but by hypothesizing that relevance is related to these features in the particular way we can then combine these features to generate potentially more powerful ranking function more robust ranking function naturally the next question is how do we estimate those parameters and how do we know which features we should have a higher weight an which features will have low weight so this is the task of training or learning so in this approach what we will do is to use some training data those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be rounded high for which queries and this information can be based on real judgments by users or this can also be approximated by just using click through information where we can assume the click the documents are better than the skip the documents or click the document are relevant than the skip your documents are not relevant so in general with the fit such a hypothesized ranking function through the training data meaning that we will try to optimize its retrieval accuracy on the training data we adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as map or NDC G so the training data would look like a table of couples each tuple has three elements the query the document and the judgment so it looks very much like our relevance judgment that we talked about evaluation of retrieval systems
410	3fa6d76f-2285-4cde-94db-6531698b9c21	This lecture is about natural language content analysis. As you see from this picture, this is really the first step to process any text data, text data in natural languages. So computers have to understand natural language to some extent in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. Finally, we're going to cover the relation between natural language processing and text retrieval. First what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand. Now what do you have to do in order to understand that text? This is basically what computers are facing, right? So looking at the simple sentence like a dog is chasing a boy on the playground. We don't have any problem with understanding this sentence. But imagine what the computer would have to do in order to understand it, or in general it would have to do the following. First we have to know dogs are a noun chasing is a verb etc. So this is called lexical analysis or part of speech tagging. And we need to figure out the syntactic categories of those words. So that's the first step. After that, we're going to figure out the structure of the sentence. So for example, here it shows that A and a dog would go together to form a noun phrase. And we won't have dog and is to go first, and there are some structures that are not just right. But this structure shows what we might get if we look at the sentence and try to interpret the sentence. Some words would go together 1st and then they will go together with other words. So here we show we have noun phrases as intermediate components and then verbal phrases. Finally we have a sentence. And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser. A computer program that would automatically create this structure. Now at this point you would know the structure of this sentence, but still you don't know the meaning of the sentence, so we have to go further to semantic analysis. In our mind, we usually can map such a sentence to what we already know in our knowledge base. And for example, you might imagine a dog that looks like that there's a boy and there's some activity here. But for a computer would have to use symbols to denote that, right? So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground. Now there is also chasing activity that's happening here, so we have a relation chasing here that connects all these symbols. So this is how computer would obtain some understanding of this sentence. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared. So this is some extra knowledge that you would infer based on understanding of the text. You can even go further to understand why the person said this sentence, so this has reduced the use of language. This is called. Pragmatic analysis. In order to understand the speech actor of a sentence. Like we say something to basically achieve some goal. There's some purpose there, and this has to do with the use of language. In this case, the person who said this sentence might be reminding another person to bring back the dog. That could be one possible intent to reach this level of understanding would require all these steps. And a Computer would have to go through all these steps in order to completely understand this sentence. Yet we humans have no trouble with understanding that, we instantly will get everything. And there is a reason for that. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. Computers unfortunately, are hard to obtain such understanding. They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties. So that makes natural language processing difficult for computers. But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers. They natural languages are designed for us to communicate. There are other languages designed for computers. For example program languages. Those are harder for us, so natural languages is designed to make our communication efficient. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context. There's no need to invent the different words for different meanings. We could overload the same word with different meanings without the problem. Because of these reasons, this makes every step in natural language processing difficult. For computers, ambiguity is the main difficulty. And common sense reasoning is often required. That's also hard. So let me give you some examples of challenges here. Consider the word level ambiguity. The same word can have different syntactic categories. For example, design can be a noun or a verb. The word root may have multiple meanings, so square root in math sense, or the root of a plant. You might be able to think of other meanings. There are also syntactical ambiguities, for example. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. Think for a moment to see if you can figure that out. We usually think of this as processing of natural language. But you could also think of this as you say, language processes is natural. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. Another common example of an ambiguous sentence is the following. A man saw a boy with a telescope. Now in this case, the question is who had the telescope? Right, this is called a prepositional phrase attachment. Ambiguity, or PP attachment ambiguity. Now we generally don't have a problem with these ambiguities. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. Another example of difficulties is anaphora resolution, so think about the sentence like John persuaded Bill to buy a TV for himself. The question here is does himself refer to John or Bill? So again, this is something that you have to use some background or the context to figure out. Finally, presupposition is another problem. Consider the sentence. He has quit smoking. This obviously implies that he smoked before. So imagine a computer wants to understand all these subtle differences and meanings. It would have to use a lot of knowledge to figure that out. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. So This is why it's very difficult. So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers. So this slide sort of gives simplified view of state of the art technologies. We can do part of speech tagging pretty well, so I showed 97% accuracy here. Now this number is obviously based on a certain data set, so don't take this literally. It just shows that we can do it pretty well, but it's still not perfect. In terms of parsing, we can do partial parsing pretty well. That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. Again, I have to say these numbers are relative to the data set in some other data sets. The numbers might be lower. Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data. Think about the social media data, the accuracy likely is lower. In terms of semantic analysis. We are far from being able to do a complete understanding of a sentence. But we have some techniques that would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. For example, recognizing the mentions of people, locations, organisations, etc in text. So this is called entity extraction. We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company. Such relations can be extracted by using the current natural language processing techniques. They're not perfect, but they can do well for some entities. Some entities are harder than others. We can also do word sense disambiguation to some extent. We can figure out whether this word in this sentence would have certain meaning in another context. The computer could figure out it has a different meaning. Again, it's not perfect, but you can do something in that direction. We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative. This is especially useful for review analysis, for example. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful. In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties. This is a general challenging in artificial intelligence. That's partly also because we don't have complete semantic representation for natural language text, so this is hard yet in some domains, perhaps in limited domains, when you have a lot of restrictions on the word uses, you maybe do may be able to perform inference. To some extent, but in general we cannot really do that. reliably. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. So this roughly gives you some idea about the state of the art. And then we also talk a little bit about what we can't do. And so we can't even do one hundred percent part of speech tagging. Now this looks like a simple task, but think about the example here. The two users of off may have different syntactic categories. If you try to make a fine grained distinguishing, it's not that easy to figure out such differences. It's also hard to do general, complete parsing, and again this same sentence that you saw before is example. This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope. So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long. Imagine it has four or five prepositional phrases, and there are even more possibilities to figure out. It's also hard to do precise deep semantic analysis, so here's example in the sentence. John owns a restaurant. How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. In a shallow way, meaning we only do superficial analysis. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text. And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well. The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing. Of course, within such a short amount of time, we can't really give you a complete view of NLP, which is big field an either expect to see multiple courses on natural language processing. topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background. In case you haven't been exposed to that. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. It's very hard to restrict the text to a certain domain. And we also often dealing with a lot of text data. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. In fact, most search engines today use something called a bag of words representation. Now, this is probably the simplest representation you can possibly think of. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. And we'll keep duplicated occurrences of words. So this is called a bag of words representation. When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order. But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. So in comparison, some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong. So in comparison, search task is all relatively easy. Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using. Of course I put in parentheses is here, but not all. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP. So one example is word sense disambiguation. Think about the world like Java. It could mean coffee, or could mean program language. If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words. For example, I'm looking for usage of Java applet. When I have applet there that implies. Java Means program language. And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. Another example is. Some technical code feedback which we will talk about later in some of the lectures. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. And these words can help matching documents where the original query words have not occurred. So this achieves to some extent. Semantic matching of terms. So those techniques also helped us bypass some of the difficulties in natural language processing. However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks. Or for question answering. Google has recently launched Knowledge Graph and this is one step toward that goal. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. Although this is still an open topic for research and exploration. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. If you want to know more, you can take a look at some additional readings. I only cited one here and that's a good starting point. Thanks.
410	402b4679-f01e-4d07-89a3-6c9817710ada	this lecture is about vector space between the model we're going to give introduction to its basic idea in the last lecture we talked about the different ways of designing our retrieval model which would give us a different the ranking function in this lecture whether they talk about the specific way of designing a ranking function called a vector space retrieval model and we're going to give a brief introduction to the basic idea after space model is a special case of similarity based models as we discussed before which means we assume relevance is roughly similarity between the document and the query now whether this assumption is true is actually a question but in order to solve a search problem we have to convert the vague notion of relevance into a more precise definition that can be implemented with programming languages so in this process we have to make a number of assumptions this is the first is something that we make here basically we assume that if a document is more similar to a query there another document then the first document would be assuming it'll be more relevant and then the second one and this is the basis for ranking documents in this approach again it's questionable whether this is really the best definition for relevance as we will see later there are other ways to model relevance the basic idea of vectors ways retrieval model is actually very easy to understand imagine or high dimensional space where each dimension corresponds to a term so here i issue a three dimensional space with three words programming library and presidential so each term here defines one dimension now we can consider vectors in this three dimensional space and we're going to assume that all our documents and the query will be placed in this vector space so for example one document might be represented by this vector D one now this means this document probably covers library and presidential but it doesn't really talk about programming right what does this mean in terms of representation of document that just means we're going to look at our document from the perspective of this vector we're going to ignore everything else basically what we see here is only the vector representation of the document of course the document has other information for example the orders of words are simply ignored and that's be cause we assume that the bag of words with temptation so with this representation you have already see T one central suggest a topic like a press agent library now this is different from another document which might be represented as a different about the D two here in this case the document covers programming and library but it doesn't talk about the presidential so what does this remind you well you can probably guess the topic is likely about programming language in the library is software library so this shows that by using this vector space recommendation week actually capture the differences between topics of documents now you can also imagine there are other vectors for example these three is pointing to that direction that might be about the presidential program and in fact that we can place all the documents in this vector space and there will be pointing to all kinds of directions and similarly we're going to place our query also in this space as another factor and then we're going to measure the similarity between the query vector an every document vector so in this case for example we can easily see D two seems to be the closest for two this query vector and therefore the two would be ranked above others so this was basically the main idea of the battle space model so to be more probably precise to be more precise that is based model is a framework in this framework we make the following assumptions first we represent a document that and query by a term vector so here are term can be any basic concept for example a word or a phrase or even engram of characters those are just sequence of characters inside the world each term is assumed that would define one dimension therefore end terms in our vocabulary with divine N dimensional space appear a vector would consist of a number of elements corresponding to the weights on different terms each document of actor is also similar it has a number of elements and each value of it your element his indicating the weight of the corresponding term here you can see we assume there are N dimensions therefore there are elements each corresponding to the weight on the particular term so the relevance in this case would be assumed to be the similarity between the two vectors therefore our ranking function is also defined as the similarity between the query vector and document vector now if i ask you to write a program to implement this approach in a search engine you would realize that this is far from clear right we haven't to say that a lot of things in detail therefore it's impossible to actually write a program to implement this that's why i said this is a framework and this has to be refined in order to actually suggest a particular ranking function that you can implement on a computer so what is this framework not self well it actually had in the set up many things that would be required in order to implement this function first it did not say how we should define or select the basic concepts exactly we clearly assume the concepts are orthogonal otherwise there will be redundancy for example if two synonyms are somehow distinguish it as a two different concepts then there would be defining two different line messages and that would clearly cause redundancy here over emphasizing of matching this concept because it would be as if you match the tool dimensions well you actually match one semantic concept secondly it did not say how we exactly should place documents and the query in this space basically i showed you some examples of query and document vectors but where exactly should the vector for particular document point two so this is equivalent to how to define the term weights how do you computer those element values in those vectors now this was a very important question becaus tom waits in the query vector indicates importance of term so depending on how you assign the weight you might prefer some terms to be matched all the others similarly the term weather in the document that is also very meaningful with indicates how well the term characterizes the document if you got it wrong then you clearly don't represent this document accurately finally how to define the similarity measure is also not cater so these questions must be addressed before we can have operational function that we can actually implement using a programming language so how do we solve these problems is the main topic of the next election
410	40831a36-c960-4eef-8c60-8ab212b52f8c	so average precision is computer for just one query but we generally experimented with many different queries and this is to avoid the variance across queries depending on the queries you use you might make different conclusions so it's better to use more queries if you use more queries than you would also have to take the average of the average precision over all these queries so how can we do that well you can naturally it would think of just doing arithmetic mean as we always tend to to think in this way so this would give us what's called a mean average precision or map in this case we take arithmetic mean of all the average precisions over a set of queries or topics but as i just mentioned in another lecture is this good we call that we talked about the different ways of combining precision and recall an we conclude that the arithmetic me is not as good as the F measure but here it's the same we can also think about the alternative ways of aggregating the numbers don't just automatically assume that that's just to take a terrorist metheny of the average precision over these queries let's think about what's the best way of aggregating if you think about different ways naturally you would probably be able to think about another way which is joe match coming and we called this kind of average G map but this is another way so now once you think about the two different ways of doing the same thing the natural question to ask is which one is better so so you use map orgy map again that's important question imagine you all again testing a new algorithms in by comparing it with your old algorithm in the search engine now you tested multiple topics now you've got the average precisions of all these topics now you are thinking of looking at the overall performance you have to take the average but which which is strategy would you use now first you should also think about the question would it make a difference can you think of scenarios where using one of them would make a difference that is they would give different rankings of those methods and that also means depending on the way you average or you take the average of these average positions you will get different conclusions this makes the question become even more important right so which one would you use well again if you look at the difference between these different ways of aggregating the average position you will realize in arithmetic me the sum is dominated by large values so what does the LG magic value have need it means the query is relatively easy you can have a hyper average position where is jim abbott tends to be affected more by little values and those are the queries that don't have good performance the average precision is low so if you think about improving the search engine for those difficult queries then G map would be preferred on the other hand that if you just want to have improvement all overall the kinds of queries or particularly popular queries that might be easy and you want to make the perfect and maybe map would be them prefer so again the answer depends on your users will users tasks and their preferences so the point that here is to think about multiple ways to solve the same problem and then come here then and think carefully about differences and which you want makes more sense often in one of them might make sense in one situation an another might make more sense in a different situation so it's important to freak out and what situations one is preferred as a special case of the mean average precision we can also think about the case where there is precisely one relevant document and this happens often for example in what's called a no item search where you know a target page let's say you wanted to find the amazon home page you have one random document there and you hope to find it that's called a known item search in that case this precise the one relevant document or in another application like a question answering maybe there's only one answer there so if you rank the answers then your goal is rank that one particular answer on top so in this case you can easily verify the average position we basically boil down two reciprocal rank that is one over R where R is the rank position of that single relevant document so if that document is ranked on the very top eyes one and then it's one for reciprocal rank if it's ranked at the second then it's one over two etc and then we can also take a average of all these average casino reciprocal rank over a set of topics and that would give us something called a mean receiver core rank at a very popular value for no item search any random problem where you have just one rather than the item now again here you can see this all actually is meaningful here and this are is basically indicating how much effort a user would have to make in order to find that relevant document if it's rendered on the top there's no effort that you have to make a little effort but if it is ranked at one hundred then you actually have to read presumably one hundred documents in order to find it so in this sense all is also meaningful measure and the reciprocal rank will take the reciprocal of our instead of using all directly so one natural question here is why not simply using all the imagine fewer designer measure to measure performance of a ranking system when there is only one relevant item you might have thought about using all directly as the measure after all that matches the users effort right but think about if you take an average of this over a large number of topics again it would make a difference for one single topic using our or using one over R wouldn't make any difference it's the same larger are with correspond to a small one overall but the difference would only show when show up when you have many topics so again think about the average of mean reciprocal rank versus average of just on what's the difference do you see any difference with this difference change the order of systems in our conclusion and this it turns out that there is actually a big difference and if you think about it if you want to think about it and then yourself then pause the video basically the difference is if you take some of our directory then again will be dominated by large values of our so what are those values those are basically large values that indicate the lowly ranked results that means the relevant item is ranked very low down on the list and the sum the orders also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list but from a user 's perspective we care more about the highly ranked documents so by taking this transformation by using reciprocal rank here we emphasize more on the difference on the top and think about the difference between one and two it will make a big difference in one over R gotta think about the one hundred and one hundred aware and one the one make much difference if you use this but if you use this there will be a big difference in one hundred and one thousand right so this is not the desirable on the other hand while two won't make much difference so this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense so to summarize we shows that the precision recall curve can characterize the overall accuracy of a ranked list and we emphasize that the actual utility over ranked list it depends on how many top random results are user would actually examine some users will examine more than others an average prison is the standard measure for comparing two ranking methods it combines precision and recall and it's a sensitive to the rank of every random the document
410	435befd6-e92e-4529-813e-e9bcacc58a6d	So to summarize, our discussion of recommender systems in some sense, the filtering task or recommending task is easy and in some other senses, and the task is actually difficult, so it's easy because the users expectations, though in this case the system it takes initiative to push the information to the user so the user doesn't really make. An effort, so any recommendation is better than nothing, right? So unless you recommend the order, noisy items or useless documents, if you can recommend some useful information, users general would appreciate it, so that's in that sense that's easy. However, filtering is actually much harder task than retrieval because it has you have to make a binary decision and you can't afford waiting for a lot of items. And then you're going to see. whether 1 item is better than others. You have to make a decision when you see this item. The thing about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user. If you wait for a few days. Even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased. Another reason why it's hard, it's because the data sparseness. If you think of this as a learning problem in collaborative filtering, for example, it's purely based on learning from the past ratings, so if you don't have many ratings, this really not that much you can do. And yeah, just mentioned this cold start problem. This is actually a very serious serious problem, but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem. You can use for example, more user information to assess their similarity instead of using the preferences of these users on these items, there may be additional information available about the user. etc and. And we also talked about the two strategies for filtering task one is content based, where we look at item similarity. The other is collaborative filtering where we look at the user similarity and they obviously can be combined in a practical system. You can imagine the general would have to be combined so that will give us a hybrid strategy for filtering. And we also could recall that we talked about push versus pull as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are certain users in the pull mode. Obviously the tool should be combined and they can be combined to have a system that can support the user with multiple mode information access. So in future we could anticipate the such a system to be more useful to user. An either this is the active research area, so there are a lot of new algorithms being proposed all the time. In particular, those new algorithms tend to use a lot of context information. Now the context here could be the context of the user and that it could be also context of documents or items. The items are not isolated and they are connected in many ways. The users might form social network as well, so there's a rich context there that we can leverage in order to really solve the problem well and then that's an active research area where also machine learning algorithms that have been applied. Here are some additional readings in the Handbook called Recommender Systems and has a collection of a lot of good. Articles that can give you an overview of a number of specific approaches to recommended systems.
410	4453a049-7597-4df4-9b9b-67c2d124a116	We can compute this maximum regular estimated by using the EM algorithm. So in the E-step, we now have to introduce more hidden variables because we have more topics. So our hidden variable Z now, which is a topic indicator, can take more than two values. Specifically, will take a K plus one values with B denoting the background and one through K to denote all the K topics. So now the E step as you can recall is augmented data and by predicting the values of the hidden variable. So we're going to predict for word whether the word has come from one of these K+1 distributions. This equation allows us to predict the probability that the word W in Document "D is generated from topic theta sub j And the bottom one is the predicted probability that this word has been generated from the background. Note that we use Document D here to index the word. Why? Because Whether a word is from a particular topic, actually depends on the document. Can you see why? Well, it's through the pi. The pis are tied to each document. Each document can have a potentially different pis, right? The pis will then affect our prediction, so the pis are here, and this depends on the document. And that might give a different guess of word for word in different documents, and that's desirable. In both cases we are using the bayes rule as I explained, basically assessing the likelihood of generating word in from each distribution and is normalized. What about the M-step? Well, we may recall the M step is to take advantage of the inferred Z values to split the counts and then collect the right counts to re estimate parameters. So in this case we can re estimate our coverage probability and this is re estimated based on collecting all the words in the document. And that's why we have the count of the word in document and sum over all the words. And then we're going to look at the to what extent this word belongs to the topic's theta sub-j, and this part is our guess from E-step. This tells us how likely this word is actually from theta sub-j, and when we multiply them together we get the discounted count that's allocated for topic theta sub-j and we normalize this over all the topics we get the distribution over all the topics to indicate the coverage. And similarly, the bottom one is to re-estimate the probability of word for topic. In this case we're using exactly the same count. You can see this is the same discounted count, it tells us to what extent we should allocate this word to topic theta sub-j. But the normalization is different because in this case we are interested in the word distribution. So we simply normalize this over all the words. This is different. In contrast, here we normalized among all the topics. It would be useful to take a comparison between the two. This gives us different distributions and these tells us how to improve the parameters? And as I just explained in both E step formulas, we have a maximum likelihood estimator based on the allocated word "counts to "topic theta sub-j. Now this phenomena is actually general phenomenon in all the EM algorithms in the M step, you generate computed expected count of event based on the E step result and then you just collect the relevant counts for a particular parameter. and re-estimate with normalizing. Typically. So in terms of computation of the EM algorithm, we can. Actually, just keep counting various events and then normalize them. And when we think in this way, we also have a more concise way of presenting the EM algorithm. It actually helps us better understand the formulas. So I'm going to go over this in some detail. So as the algorithm, we first initialize all the unknown parameters randomly. In our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. And we just randomly normalize them. This is the initialization step, and then we will repeat until likelihood converges. Now how do we know whether likelihood converges we're going to compute likelihood at each step and compare the current likelihood with the previous likelihood if it doesn't change much and we're going to say stop right? So in each step we can do E step and M step in the E step we're going to augment the data by predicting the hidden variable values. In this case the hidden variable Z sub DW indicates whether word in W in D is from topic or background, an if it's from a topic which topic? So if you look at the E step formulas essentially we're actually normalizing these counts. At all, sorry, these are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. And I said it's proportional to this because in completing the implementation of EM algorithm you can just keep count counter for this quantity and then in the end you just normalize it. So the normalization here is over all the topics and then you will get a probability. Now in the M step we do the same and we are going to collect these. Allocated counts for each topic. And we split words among the topics. And then we're going to normalize them in different ways to obtain the re-estimate. So, for example, we can normalize among all the topics to get re estimate of Pi the coverage. Or we can renormalize based on the. For all the words and that would give us a word distribution. So it's useful to think of the algorithm in this way, because when you implement, you can just use. Variables to keep track of these quantities in each case. And then you just normalize these variables to make them a distribution. Now I did not put the constraint for this one and I intentionally leave this as exercise for you and you can see what's the normalizer for this one. It's of a slightly different form, but it's essentially the same as the one that you have seen here. Namely this one. So in general, in the implementation of EM algorithm you will see you accumulated counts various counts and then you normalize them. So to summarize, we introduced the PLSA model, which is a mixture model with K unigram language models representing K topics. And we also added a predetermined background language model to help discover discriminating topics. Because this background language model can help attract the common terms. And, We show that with maximum likelihood estimator we can discover topical knowledge from text data. In this case PLSA allows us to discover two things. One is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. And such detailed characterization of coverage of topics in documents can enable a lot of further analysis. For example, we can aggregate the documents in the particular time period to assess the coverage of a particular topic in a time period that would allow us to generate the temporal chains of topics. We can also aggregate topics covered in documents associated with a particular author, and then we can characterize the topics written by this author, etc. And in addition to this, we can also cluster terms and cluster documents. In fact, each topic can be regarded as a cluster, so we already have term clusters. And the higher probability words can be regarded as in belonging to one cluster. Represented by the topic. Similarly documents can be clustered in the same way. We can assign a document to the topic cluster that's covered most in the document. So remember pis indicate to what extent each topic is covered in the document. We can assign the document to the topic cluster that has the highest pi. And in general, there are many useful applications of this technique.
410	44df41bc-04d3-41ca-ac51-dbd22dc98305	In general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. So if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we simply normalize the counts of segments that contain this word. So let's first take a look at the data here. On the right side you see I listed some hypothesizes that data these are segments. And in some segments you see both words occur. Their indicator as once for both columns. In some other cases, only one word occurs, so only that column has one and the other column has zero. And of course in some other cases, none of the words occur, so they are both zeros. And For estimating these probabilities, we simply need to collect the three counts. So the three counts of 1st, the count of W. 1 and that's the total number of segments that contain world W one. It's just the ones in the column of W one we can just count how many ones we have seen there. The second counter is for word 2 and we just count the ones in the second column. And these this would give us a total number of segments that contain W2. The third account is when both words occurred, so this is time we're going to count the segments where both columns have ones. And then so this would give us the total number of segments where we have seen both W and W2. Once we have these counts, we can just normalize. These counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. Now there is a small problem. When we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sample and in general we would believe that it's potentially possible for award to occur in any context. So to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we don't get a zero probability in any case. Now, the best way to understand the smoothing is imagine that we actually. Observed more data than we actually have. We will pretend we observe some pseudo segments that are illustrated on the top on the right side of the slide and these pseudo segments would contribute additional counts of these words so that no event will have zero probability probability. Now, in particular, we introduce the four pseudo segments. Each is weighted 1/4. And these represent the four different combinations of occurrences of these words. So now each event, each combination will have at least one count or at least non zero counter. From these pseudo segment. So in the actual segments that we observed, it's OK if we haven't observed all the combinations. So more specifically, you can see the point of five. Here actually comes from the two ones in the two pseudo segments, because each is weighted 1/4, we added them up. We get .5. And similarly this .05 comes from one single pseudo segment that indicates the two words occur together. And of course, in the denominator we add the total number of pseudo segments that we added. In this case we added a 4th through the segments. Each is weighted 1/4, so the total the sum is actually one. So that's why in the denominator you still want there. So this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. No. So, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. We introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable X conditional entropy, which measures the entropy of X. Given we know why. And mutual information of X&Y which matches the entropy reduction of X. Due to knowing why or entropy reduction of why do too knowing eggs? They are the same, so these three concepts are actually very useful for other applications as well. That's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. In particular, mutual information is a principled way for discovering such a relation. It allows us to have values computer on different pairs of words that are comfortable, and so we can rank these pairs and discover the strongest cinematical relationship from collection of documents. Now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. So we already discussed the possibility of using BM 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. But here, once we use mutual information to discover Syntagmatic relations, we can also represent the context with this mutual information as weights. So this would give us another way to represent the context. Of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity. So this provides yet another way to do term waiting for paradigmatic. A relation discovery an. So to summarize, this whole part about word Association mining, we introduce the two basic associations, called Paradigmatic and Syntagmatic relations. These are fairly general. They can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities. Are we introduced multiple statistical approaches for discovering them? Then it showing that pure statistical approaches are visible? Available for discovering both kinds of relations, and they can be combined to perform. Join the analysis as well. These approaches can be applied to any text with no helmet human effort. And mostly becausw. They are based on counting of words. Yet they can actually discover interesting relations of words. We can also use different ways to define context and segment and this would lead to some interesting variations of applications. For example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. And similarly, counting Co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. These discovery associations can support them. Any other applications in both information retrieval and text data mining. So here are some recommended readings. If you want to know more about the topic, the 1st is a book with a chapter on locations which is quite relevant to the topic of these lectures. The second is the article about the using various statistical measures to discover lexical atoms. Those are phrases that are non composition compositional or for example hot dog is not really a dog that's hot. Blue chip is not a chip that's blue, and the paper has a discussion about to some techniques for discovering such phrases. The third one is new paper on unified way to discover both paradigmatic a relation and select medical relations using random walks on world graphs.
410	48b37a2f-5ca3-4b7b-9bfc-d841da37c566	So now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. One is called maximum likelihood estimate that I already just mentioned. The other is Bayesian estimation. So in Maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here. Where we define the estimate as  arg max of the probability of X given Theta. And so arg max here just means it's actually a function that would return the argument that gives the function maximum value as the value, so the value of arg max is not the value of this function, but rather the argument that has made the function reach maximum. So in this case the value of argmax is Theta. It's the Theta that makes the probability of X given Theta reach its maximum, so this estimate intuitively also makes sense, and it's often very useful, and it seeks the parameters that best explain the data. But it has a problem when the data is too small, because when the data points are too small, there are very few data points. The sample is small, then if we trust data entirely and try to fit the data and then we will be biased. So in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. Because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. But this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. So one way to address this problem is actually to use Bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. We assume that we have some prior belief about the parameters. Now in this case, of course, so we are not going to look at just the data, but also look at the prior so the prior here is defined by P of Theta. And this means we will impose some preference on certain Thetas over others. And by using Bayes rule that I have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. Now a full explanation of Bayes Rule and some of these things related to Bayesian reasoning would be outside the scope of this course, but I just give a brief introduction because this is a general knowledge that might be useful for you, so the Bayes rule is basically defined here. And allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X. And you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often the rule is used for making inferences of a variable. So let's take a look at it again, we can assume that P of X encodes our prior belief about the X. That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others. And this probability of X given Y is a conditional probability, and this is our posterior belief about X, because this is our belief about X values after we have observed Y. Given that we have observed Y, now what do we believe about X now, do we believe some values have high probabilities than others? Now, the two probabilities are related through this can be regarded as the probability of the observed evidence Y here given a particular X. So you can think about X as our hypothesis. And we have some prior belief about which hypothesis to choose and after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior here and the likelihood of observing this Y if X is indeed true. So much for a detour about Bayes Rule. So in our case, what we're interested in is inferring the theta values so we have a prior here. That includes our prior knowledge about the parameters. And then we have the data likelihood here that would tell us which parameter value can explain the data well. The posterior probability combines both of them. So it represents a compromise of the two preferences. And in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. And this estimator is called the Maximum a Posteriori or MAP estimate. And this estimate is a more general estimate than the maximum likelihood estimate. Because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. The same as here. OK, but if we have some informative prior, some bias towards certain values, then MAP estimate can allow us to incorporate that, but the problem here of course is how to define the prior. There's no free lunch, and if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. Otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. Now let's look at the Bayesian estimation in more detail. OK, so I show the theta values as just one dimension value and that's a simplification of course. So we're interested in which value of data is optimal. So now first we have the prior. The prior tells us some theta values are more likely than others. We believe, for example, these values are more likely than the values like here or here or other places. So this is our prior. And then we have our data likelihood. In this case, the data also tells us which values of theta are more likely and that just means those theta values can best explain our data. And then when we combine the two, we get the posterior distribution and that's just a compromise of the two. It would say that it's somewhere in between, so we can now look at some interesting point estimates of theta. Now this point represents the mode of prior. That means the most likely parameter value according to our prior before we observe any data. This point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. Now this point is interesting. It's the posterior mode, it's the. It's the most likely value of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate. In general, in Bayesian inference we are interested in the distribution of all these parameter values. As you see, here is there's a distribution over Theta values that you can see here P of theta given X. So the problem of Bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on Theta. So I showed F of Theta here as an interesting variable that we want to compute. But in order to compute this value, we need to know the value of Theta. In Bayesian inference, we treat data as uncertain variable. So we think about all the possible values of Theta. Therefore we can estimate the value of this function F as the expected value of F according to the posterior distribution of data given the observed evidence X. As a special case, we can assume F of Theta is just equal to Theta. In this case we get the expected value of Theta. That's basically the posterior mean that gives us also one point of Theta.  And it's sometimes the same as posterior mode, but it's not always the same, so it gives us another way to estimate the parameters. So this is a general illustration of Bayesian estimation and Bayesian inference. inference. And later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. So to summarize, we introduced the language model which is basically probability distribution over text. It's also called a generative model for text data. The simplest language model is unigram language model. It's basically a word distribution. We introduced the concept of likelihood function which is the probability of data given some model. And this function is very important. Given a particular set of parameter values, this function can tell us which X, which data point has a higher likelihood, higher probability. Given a data point, sorry, given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. We also talked about the Bayesian estimation or influence. In this case we must define a prior on the parameters P of Theta, and then we're interested in computing the posterior distribution of the parameters which is proportional to the prior and the likelihood. And this kind of distribution would allow us, then, to infer any derived values from Theta.
410	4a54f790-991c-44bb-ab62-713cbef84ad1	This lecture is about the sentiment classification. If we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. So suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. Then we mainly need to decide the opinion sentiment of the review. So this is a case of just using sentiment classification for understanding opinion. Sentiment classification can be defined more specifically as follows: The input is opinionated text object. The output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. One is polarity analysis where we have categories such as positive, negative or neutral. The other is emotion analysis. That can go beyond polarity to characterize the feeling of the opinion holder. In the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web. Five might denote the most positive and one maybe at most negative, for example. In general you have just discrete categories to characterize the sentiment. In emotion analysis, of course, there are also different ways to design the categories. The six most frequently used categories are happy, sad, fearful, angry, surpised and disgusted. So as you can see, the task is essentially a classification task or categorisation task. As we've seen before, so it's a special case of text categorization. This also means any text categorization method can be used to do sentiment classification. Now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. In particular, it needs two kinds of improvements. One is to use more sophisticated features that may be more appropriate for sentiment tagging, as I will discuss more in a moment. The other is to consider the order of these categories. An especially polarity analysis, very clear that order here and so these categories are not all that independent. There is order among them, and so it's useful to consider the order. For example, we could use ordinal regression to do, and that's something that will talk more about later. So now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. So let's start from the simplest one, which is character n-grams. You can just have a sequence of characters as a unit, and they can be mixed with different n(s),  different lengths. And this is a very general way, and a very robust way to represent the text data. You could do that for any language pretty much. And this is also robust to spelling errors or recognition errors, right? So if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly. So misspelled word and the correct form can be matched because they contain some common n-grams of characters. But of course such a representation would not be as discriminative as words. So next we have word n-grams, a sequence of words and again we can mix them with different lengths. Uni Grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. For example, we might see a sentence like it's not good or it's not as good as something else. So in such a case, if you just take a good and that would suggest positive, it's not good, so it's not accurate, but if you take the bigram, not good together, and then it's more accurate, so longer n-grams are generally more discriminative and they are more specific. If you match it and it says a lot and it's accurate. It's unlikely, very ambiguous. But it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories. An obviously that kind of classifier won't generalize well to future data when such discriminating features will not necessarily occur. So that's a problem of overfitting. That's not desirable. We can also consider part of speech tag n-grams, if we can do part of speech tagging and for example, adjective, noun could form a pair. We can also mix N grams of words and N grams of part of speech tags. For example, the word great might be followed by a noun and this could become a feature, a hybrid feature. That could be useful for sentiment analysis. So next we can also have word classes, so these classes can be syntactic like a part of speech tags. Or could be semantic and they might represent concepts in the thesaurus or ontology like word net. Or they can be recognized the named entities like people or place and these categories can be used to enrich the representation as additional features. We can also learn word clusters empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words. And these clusters can be features to supplement the word based representation. Furthermore, we can also have frequent pattern syntax and these could be frequent word set. The words that formed a pattern do not necessarily occur together or next to each other. But we also have locations where the words might occur more closely together. And such patterns provide a more discriminative features than words, obviously, and they may also generalize better than just the regular n-grams because they are frequent, so you can expect them to occur also in test data so they have a lot of advantages, but they might still face the problem of overfitting as the features become more complex. This is the problem in general, and the same is true for parse tree based features where you can use a parse tree to derive features such as frequent subtrees or paths, and those are even more discriminating, but they also are more likely to cause overfitting. And in General, Patton discovery algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. So in general, natural language processing is very important to derive complex features. They can enrich text representation. So for example, this is a simple sentence that I showed you long time ago, and in another lecture. So from these words we can only derive simple world n-grams representations or character n-grams. But with NLP we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act. Now with such enriched information, of course, then we can generate a lot of other features, more complex features, like a mixed grams of word and part of speech tags. Or even a part of parse tree. So in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. In general, I think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features. So first you want to use domain knowledge and your understanding of the problem to design seed features. And you can also define a basic feature space with a lot of possible features for the Machine learning program to work on. And machine learning can be applied to select the most effective features or construct the new features that feature learning. And these features can then be further analyzed by humans through error analysis. And you can look at the categorization errors and then further analyze what features can help you recover from those errors or what features cause overfitting and cause those errors, and so this can lead to feature validation that would revise the feature set and then you can iterate and we might consider using a different feature space. So NLP enriches text representation. As I just said and because it enriches the feature space. It allows much larger search space of features. And there are also many meaningful features that can be very useful for a lot of tasks. But be careful not to use a lot of complicated features because it can cause overfitting or otherwise you have to do the training carefully, not to let overfitting happen. So a main challenge in designing features, a common challenge is to optimize the tradeoff between exhaustivity and specificity. And this trade off, it turns out to be very difficult. Now, exhaustivity means we want the features to actually have high coverage of a lot of documents. And so in that sense, you wanted features to be frequent. Specificity requires the feature to be discriminative, so naturally infrequent features tend to be more discriminating, so this really caused tradeoff between frequent versus infrequent features, and that's why feature design is generally an art. That's perhaps the most important part in applying machine learning to any problem in particular. In our case, for text categorization, or more specifically, sentiment classification.
410	4b0bd537-3a1f-4cae-a68f-f2845ecf4f35	This lecture is about the implementation of text retrieval systems. In this lecture we will discuss how we can implement text retrieval method to build a search engine. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. This is a typical text retrieval system architecture. We can see the documents are first processor via tokenizer to get that tokenized units, for example words, and then these words or tokens will be processed by a indexer that would create the index which is a data structure for the search engine to use to quickly answer query. And the query will be going through a similar process in step, so that tokenizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other. And the queries representation would then be given to the scorer which would use the index to quickly answer a users query by scoring the documents and then ranking them. The results will be given to the user and then the user can look at the results and provide some feedback that can be expressed judgments about which documents are good, which documents are bad or implicit feedback such as click slows so the user doesn't have to do any, anything extra the user would just look at the results and skip some and click on some results to view. So these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skiped ones. So a search engine system then can be divided into 3 parts. The first part is the indexer, and the second part is a scorer that responds to the users query, and the third part is a feedback mechanism. Now typically the indexer is done in the offline manner, so you can preprocess the collected data and to build the inverted index which we will introduce in the moment. And this data structure can then be used by the online module, which is a scorer to process users query dynamically and quickly generate search results. The feedback mechanism can be done online or offline depending on the method. The implementation of the indexer, and the scorer is fairly standard and this is the main topic of this lecture. In the next few lectures, the feedback mechanism, on the other hand, has variations depends on which method is used. So that is usually down in the algorithm specific way. Let's first talk about the tokenizer. Tokenization is to normalize lexical units into the same form so that semantically similar words can be matched with each other. In the language like English. Stemming is often used and this is where map all the inflectional forms of words into the same root form. So for example, computer computation and computing can all be matched to the root form compute. This way all these different forms of computing can be matched with each other. Normally this is good idea to increase the coverage of documents that matched with this query, but it's also not always beneficial because sometimes the subtle of this difference between computer and computation might still suggest the difference in the coverage of the content, but in most cases stemming seems to be beneficial. When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries, because it's not obvious where the boundary is as there's no space to separate them. So here, of course we have to use some language specific natural language processing techniques. Once we do tokenization then we would index the text documents and that is to convert the documents into some data structure that can enable fast search. The basic idea is do pre-compute as much as we can basically. So the most commonly used indexes, is called inverted index. And this has been used to in many search engines to support basically search algorithms, sometimes other indices, for example, a document index might be needed in order to support the feedback. Like I said, in this kind of techniques are not really standard in that they vary a lot according to feedback methods. To understand why we want to use inverted index, it would be useful for you to think about how you would respond to a single term query quickly. So if you want to use more time to think about that, pause the video. So think about how you can preprocess the text data so that you can quickly respond to a query with just one word? If you have thought about that question, you might realize that what the best is to simply create a list of documents that match every term in the vocabulary. In this way you can basically pre construct the answers. So when you see a term, you can simply just fetch the ranked list of documents for that term and return the list to the user. So that's the fastest way to respond to a single term query. Now the idea of inverted indexes actually basically like that, we're going to pre construct the such a index that would allow us to quickly find the all the documents that match a particular term. So let's take a look at this example. We have three documents here and these are the documents that you have seen in some previous lectures. Suppose we want to create inverted index for these documents. Then we would maintain a dictionary in the dictionary we will have one entry for each term, and we're going to store some basic statistics about the term. For example, the number of documents that match the term or the total number of  frequency of the term, which means we would count the duplicated occurrences of the term. And so, for example news. This term ocurred in all the three documents. So the count of documents is 3. And you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model. Can you think of that? So what weighting heuristic would need this count? That's IDF, right? Inverse document frequency. So IDF is the property over turn and we can compute it right here. So with the document account here, it's easy to compute the IDF, either at this time or when we build index or running time when we see a query. Now, in addition to these basic statistics, we also store all the documents that match the news, and these entries are stored in a file called postings. So in this case it matched three documents and store information about these three documents here. This is the document ID, document one, and the frequency is 1, the TF is 1 for news. In the second document, it's also 1 etc. So from this list that we can get all the documents that match at the term news and we can also know the frequency of news in these documents. So if the query has just one word news and we can easily look up this table to find the entry and go quickly to the postings and fetch all the documents that match news. So let's take a look at another term. This time, let's take a look at the word presidential. This word occured in only one document, document three, so the document frequency is one. But it occurred twice in this document, so the frequency count is 2 and the frequency count is useful in some other retrieval method where we might use the frequency to assess the popularity of a term in the collection, and similarly will have a pointer to the postings here. And in this case there is only one entry here, because, the term occured in just one document, and that's here. The document ID is 3 an it occured it twice. So this is the basic idea of inverted index. It's actually pretty simple, right? With this structure we can easily fetch all the documents that match a term and this will be the basis for scoring documents for a query. Now sometimes we also want to store the positions of these terms. So, in many of these cases, the term occured just once in the document, so there's only one position. For example in this case. But in this case, the time occurred twice, so we will store two positions. Now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say 5 words or 10 words or whether the matching of the two query terms is in fact a phrase of two words. This can all be checked quickly by using the position information. So why is inverted index good for fast search? We just talked about the possibility of using it to answer a single word query. And that's very easy. What about the multiple term queries? Let's first look at the some special cases of the Boolean query. Boolean query is basically boolean expression like this. So I want the relevant document to match both term A and the term B right? So that's one conjunctive query? Or I want the relevant documents to match term a or term b. That's a disjunctive query. Now how can we answer such a query by using inverted index? If you think a bit about it would be obvious cause we had simply fetch all the documents that match term a, and also fetch all the documents that match term B and then just take the intersection to answer a query A&B or to take the Union to answer the query A or B. So this is all very easy to answer. It's going to be very quick now. What about the multi term keyword query? We talked about vector space model for example and we would match such query with document generated score and the score is based on aggregated term weights. So in this case it's not a Boolean query. But the scoring can be acted out in a similar way. Basically, it's similar to disjunctive Boolean query. Basically, it's like a or b. We take the union of all the documents that match at least one query term, and then we would aggregate the term weights. So this is a basic idea of using that inverted index for scoring documents in general, and we're going to talk about this in more detail later, but for now, let's just look at the question why is inverted index a good idea. Basically why is it more efficient than sequentially just scanning documents? Like, this is the obvious approach. You can just compute the score for each document and then you can then score them. Sorry, you can then sort them. This is a straight forward method, but this is going to be very slow. Imagine the web it has a lot of documents. If you do this then it would take a long time to answer your query. So the question now is why would the inverted index be much faster it has to do with the word distribution in text. So here's some common phenomenon of word distribution in text. There are some language independent patterns that seem to be stable. And these patterns are basically characterized by the following pattern of few words, like the common words the, a, or we occur very frequently in text. So they account for a large percent of occurrences of words. But mostly words would occur just rarely. There are many words that occur just once, let's say in the document or once in the collection. There are many such singletons. It's also true that most frequently words in one corpus may have to be rare in another. That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context. So this phenomenon is characterized by what's called Zipf's law. This law says that the rank of word multiplied by the frequency of the word is roughly a constant. So formally, if we use F(w) to denote the frequency, r(w) to denote the rank of a word, then this is the formula. It basically says the same thing, just mathematical term we will see is basically a constant, right? So as so there is also parameter Alpha that might be adjusted to better fit any empirical observations. So if I plot the word frequencies in sorted order, then you can see this more easily. The X axis is basically the world rank and this is r(w), Y axis is a word frequency or F(w). Now, this curve basically shows that the product of the two is roughly the constant. Now if you look at these words, we can see they can be separated into three groups. In the middle it's the intermediate frequency words. These words tend to occur in quite a few documents, but they are not like those most frequent words and they're also not very rare. So they tend to be often used in queries, and they also tend to have high TF IDF weights, these intermediate frequency words. But if you look at the left part of the curve. These are the highest frequency words they occur very frequently. They are usually stop words like the, a, we, of, etc. Those words are very, very frequent. They are in fact too frequent to be discriminated and they are generally not very useful for retrieval. So they, are often removed and this is called stop words removal, so you can use pretty much just the count of words in the collection to kind of infer what words might be stop words. Those are basically the highest frequency words and they also occupy a lot of space in the inverted index. You can imagine the posting entries for such a word would be very long, and therefore if you can remove such words, you can save a lot of space in the inverted index. We also show that the tail part, which has a lot of rare words. Those words don't occur very frequently and there are many such words. Those words are actually very useful for search. Also, if a user happens to be interested in such a topic, but because they're rare. It's often true that the users are unnecessary interested in those words, but retain them would allow us to match such a document accurately, and they generally have very high IDFs. So what kind of data structures should we use to store inverted index? It has two parts, right? If you recall, we have a dictionary and we also have postings. The dictionary has modest size, although for the web it's still going to be very large, but compared with postings, it's modest. And we also need to have fast random access to the entries 'cause we want to look up with a query term very quickly. So therefore we prefer to keep such a dictionary in memory if it's possible, or if the collection is not very large, this is feasible; but the collection is very large, then it's in general not possible if the vocabulary size is very large. Obviously we can't do that so, but in general that's our goal, so the data structures that we often use for storing dictionary would be directly accessed data structures like hash table, or B-tree if we can't store everything in memory, we can use this and  try to build a structure that would allow you to quickly look up  entries. For postings, they're huge. And in general we don't have to have direct access to a specific entry we generate with. Just look up a sequence of document IDs and frequencies for all the documents that match  a query term, so we would read those entries sequentially. And therefore, because it's large, we generally have store postings on disk, so they have to stay on disk. And they would contain information such as document IDs, term frequencies or term positions etc. Now, because they are very large, compression is often desirable. Now this is not only to save disk space, and this is of course one benefit of compression. It's not going to occupy that much space. But it's also to help improving speed. Can you see why? We know that input and output would cost a lot of time in comparison with the time taken by CPU, so CPU is much faster. But IO takes time, and so by compressing the inverted index, the posting files will become smaller and the entries that we have to read into memory to process query time would be smaller and then so we can reduce the amount of trafficing IO and that can save a lot of time. Of course we have to then do more processing of the data when we uncompress the data in the memory. But as I said in the CPU is fast, so overall we can still save time. So compression here is both to save disk space and through speed up the loading of inverted index.
410	4c3d9363-8e43-44fe-ab21-8740cfd1a8c5	So this is indeed a general idea of the expectation maximization, or EM algorithm. So in all the EM algorithms, we introduce a hidden variable to help us solve the problem more easily. In our case, the hidden variable is a binary variable for each occurrence of word. And this binary variable would indicate whether the world has been generated from theta on Sunday or theater Super B. And here we show some possible values of these variables. For example for the it's from Background, Z value is 1 and text on the other hand is from the topic. Then it's 0 for Z etc. Now, of course we don't observe those Z values. We just imagine there are such a social values of Z attached to all the words. And that's why we call these hidden variables. Now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. The value of this hidden variable. And So. The algorithm, the EM algorithm then would work as follows. First will initialize all the parameters with random values. In our case the parameters are mainly the probability of a word given by status update. So this is the initialization stage. It is initialized values would allow us to use Bayes rule to take a guess of these Z values. So will guess these values we can say for sure whether taxes from background or not, but we can have our guesses. This is given by this formula. It's called E-step. And so the algorithm would then try to use the E Step 2 gas. These Z values. After that it would then invoke another spec step called M-step. In this step we simply take advantage of the inferred values and then just group words that are in the same distribution like this from background, including this as well. We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters. So let me also illustrate we can group the words that are believed to have come from Cedar sub D and as text mining algorithm for example and clustering. And we had group them together. To help us re estimate the parameters. That were interested in so these will help us re estimate these parameters. But note that before we just set these parameter values randomly, But with this guess we will have a somewhat improved estimate of this. Of course, we don't know exactly whether it's zero or one, so we're not going to really do the split in hardware, but rather we can do those soft split and this is what happened here. So we're going to adjust the count. By the probability that we believe this would has been generated by using the theta sub d. And you can see this. Where does this come from? Well, this has come from here right from the E step. So the EM algorithm with iteratively improve our initial estimate of parameters by using E-step first and then M step. the E step is to augment the data with additional information like Z. And the M step is to take advantage of the additional information to separate the data to split the data accounts and then collect the right data counts. re estimate our parameters. And then once we have a new generation of parameters, we're going to repeat this. We're going to use the E-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters. For the word distribution that we're interested in. OK, so as I said, the bridge between the two is really variable Z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. So this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of EM algorithm. Start with initial values that are often randomly set. And then we invoke E step followed by M step to get an improved setting of parameters, and then we repeat this. So this is a hill climbing algorithm that would gradually improve the estimate of parameters and as I will explain later, there's some guarantee for reaching a local maximum of the likelihood function. So let's take a look at the computation for specific case. So these formulas are the EM formulas that you see before, and you can also see there are superscripts here N to indicate the generation of parameters. I go here. For example, we have N + 1. That means we have improved parameters from here to. here we have improvement. So in this setting we have assumed that the two models have equal probabilities and the background model is known. So what are the relevant statistics? Well, these are the word counts. So assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. An in the first iteration you can picture what would happen. Well, we first we initialize all the values. So here this probability that we're interested in is normalized into an uniform distribution over all the words. And then the E step would give us a guess. Of the distribution that has been used to generate each word, we can see we have different probabilities for different words. Why that's be cause these words have different probabilities in the background. So even though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities. So these words are believed to be more likely from the topic. These, on the other hand, are less likely probably from background. So once we have the Z values, we know in the E step these probabilities would be used to adjust the counts. So 4 must be multiplied by this point three three in order to get the allocated counts toward the topic. And this is done by this multiplication. Note that if our guess says this is 100%. If this is 1.0 Then we just get the full Council of this word for this topic. But in general, as I said, it's not going to be 1.0, so we're going to just get some percentage of the counts toward this topic, and then we simply normalize these counts. To have a new generation of practice mate so you can see, compare this with the old one which is here. So compare this with this one and will see at the probability is different. Not only that, we also see some words that are believed to have come from the topic. We have high probability like this one text. And of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values. So we have a new generation of values because of the E step based on the new generation of parameters. And this these new in further values of these will give us then another generation of the estimate of probabilities of the words. And so on so forth. So this is what would actually happen when we compute these probabilities using the EM algorithm. And as you can see in the last rule where we showed the log like code and the likelihood is increasing as we do the iteration. And note that these log likelihood is negative becausw the probability is between zero and one when you take logarithm, it becomes a negative value. What's also interesting is do not last column, and these are the inferred word split, and these are the probabilities that a word is believed to have come from one distribution. In this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? So this is our primary goal. We hope to have a more discriminating world distribution. But the last column is also by product and this actually can also be very useful and you can think about that. And one use is to. For example, is made to what extent this document has covered background words. And this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explained well by the background.
410	4cac96b9-b869-4523-99b0-e36c0cca95b4	In this lecture we give an overview of text mining and analytics. First, let's define the term text mining and the term text analytics. The title of this course is called Text Mining and Analytics, but the two terms text mining and text analytics are actually roughly the same. So we are not going to really distinguish them, and we're going to use them interchangeably. But the reason why we have chosen to use both terms in the title is because there is also some subtle difference if you look at the two phrases literally. Mining emphasizes more on the process, so it gives us an algorithmic view of the problem. Analytics on the other hand emphasizes more on the result or having a problem in mind. We're going to look at the text data to help us solve a problem. But again, as I said, we can treat these two terms roughly the same, and I think in the literature you probably will find the same. So we're not going to really distinguish them in the course. Both text mining and text analytics mean that we want to turn text data into high quality information or actionable knowledge. So in both cases we have the problem of dealing with a lot of text data and we hope to turn these text data into something more useful to us than the raw text data. And here we distinguish two different results one is high quality information, the other is actionable knowledge. Now, sometimes the boundary between the two is not so clear, but I also want to say a little bit about these two different angles of the result of text mining. In the case of high quality information we refer to more concise information about the topic, which might be much easier for humans to digest than the raw text data. For example, you might face a lot of reviews of a product. The more concise form of the information would be very concise summary of the major opinions about the features of the product. Positive about, let's say, battery life of a laptop. Now, this kind of results are very useful to help people digest text data, and so this is to minimize the human effort in consuming text data in some sense. The other kind of output is actionable knowledge. Here we emphasize the utility of the information or knowledge we discover from text data. It's actionable knowledge for some decision problem, or some actions to take. For example, we might be able to determine which product is more appealing to us all, a better choice for a shopping decision. Now, such an outcome could be called actionable knowledge because a consumer can take the knowledge and make a decision and act on it. So in this case, text mining supplies knowledge for optimal decision making. But again, the two are not so clearly distinguished, so we don't necessarily have to make a distinction. Text mining is also related to text retrieval, which is an essential component in any text mining systems. Now text retrieval refers to finding relevant information from a large amount of text data. So I've taught another separate MOOC on text retrieval and search engines, where we discussed various techniques for text retrieval. If you have taken that MOOC, you will find some overlap and it would be useful to know the background of text retrieval for understanding some of the topics in text mining. But if you have not taken that MOOC it's also fine, because in this more context mining and analytics we're going to repeat some of the key concepts that are relevant for text mining. But at the high level, let me also explain the relation between text retrieval and text mining. Text retrieval is very useful for text mining in two ways: First, text retrieval can be a pre-processor for text mining, meaning that it can help us turn big text data into a relatively small amount of most relevant text data, which is often what's needed for solving a particular problem. And in this sense, text retrieval also helps minimize human effort. Text retrieval is also needed for knowledge provenance and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge. Once we find the patterns in text data or actionable knowledge, we generally would have to verify the knowledge by looking at the original text data so the users would have to have some text retrieval support to go back to the original text data to interpret the pattern, or to better understand the knowledge or to verify whether the pattern is really reliable. So this is a high level introduction to the concept of text mining and the relation between text mining and retrieval. Next, let's talk about text data as a special kind of data. Now it's interesting to view text data as data generated by humans as subjective sensors. So this slide shows an analogy between text data and non text data and between humans as subjective sensors and physical sensors such as network sensor or thermometer. So in general, a sensor will monitor the real world in some way it will sense some signal from the real world and then would report the signal as data in various forms, for example, a thermometer would watch the temperature of real world and then will report the temperature in particular format. Similarly a geo sensor would sense the location and then report the location specification, for example in the form of longitude value and lattitude value. Network sensor would monitor network traffic or activities in the network and report some digital format of data. Similarly, we can think of humans as subjective sensors that would observe the real world from some perspective, and then humans would express what they have observed in the form of text data. So in this sense human is actually a subjective sensor that would also sense what's happening in the world and then express what's observed in the form of data, in this case text data. Now looking at the text data in this way has the advantage of being able to integrate all kinds of data together, and that's indeed needed in most data mining problems. So here we are looking at the general problem of data mining, and in general we would be dealing with a lot of data about our world that are related to a problem. And in general would be dealing with both non text data and text data and of course the non text data are usually produced by physical sensors. And those non text data can be also of different formats -  numerical data or categorical or relational data or multimedia data like a video or speech. So, these non text data are often very important in some problems. But text data is also very important, mostly because they contain a lot of semantic content and they often contain knowledge about the users, especially preferences and opinions of users. So, but by treating text data as the data observed from human sensors, we can treat all these data together in the same framework. So, data mining problem is basically to turn such data, turn all the data into actionable knowledge that we can take the advantage to change the real world, of course, for better. So this means that data mining problem is basically taking a lot of data as input and giving actionable knowledge as output. Inside the data mining module you can also see we have a number of different kinds of mining algorithms and this is because for different kinds of data we generally need different algorithms for mining the data. For example, video data might require computer vision to understand video content and that would facilitate the more effective mining and we also have a lot of general algorithms that are applicable to all kinds of data, and those algorithms of course are very useful, although for a particular kind of data we generally want to also develop special algorithms. So this course will cover specialized algorithms that are particularly useful for mining text data.
410	4da6283d-6903-4be9-8bfc-ad5d330343c6	This lecture is about the discriminative classifiers for text categorization. In this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. This is a slide that you have seen from the discussion of Naive Bayes classifier, where we have shown that although naive Bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here. Now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. Of course the features don't have to be all the words and their features can be other signals that we want to use. And we mentioned that this is precisely similar to logistic regression. So in this lecture we're going to introduce some discriminative classifiers. They try to model the conditional distribution of labels given the data directly rather than using Bayes rule to compute that indirectly. As we have seen in naive bayes. So the general idea of logistical regression is to model the dependency of the binary response variable Y here, On some predictors. That are denoted as X. So here we have also changed the notation to X For feature values you may recall in the previous slide we have used Fi to represent the feature values. An here we use the notation of X vector, which is more common when we Introduce such machine learning algorithms, so X is our input, it's a vector. And with M features. And each feature has a value X sub I here and our goal is model the dependency of this binary response variable on all these features. So in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the Y value to denote the two categories. And when Y is 1 it means the category of the documents first class theta 1 Now the goal here is to model the conditional probability of Y given X directly as opposed to model the generation of X&Y as in the case of Naive Bayes. And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. Since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. So most specifically, in logistic regression the assumed functional form of y depending on X is the following, and this is very closed, closely related to the log or log odds that I introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide. So that this is what I meant, right? So in the case of Naive Bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this. That looks like this. But here we actually would assume explicitly that we would model our Probability of Y given X. As directly as a function of these features. So most specifically, we assume that log of the ratio of probability of y = 1 and the probability of y = 0. Is a function of X. And so it's a function of X, and it's a linear combination of these feature values, controlled by beta values. And since we know that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way. So this is a log odds ratio. Here. And so in logistic regression, we basically assume that the probability of y = 1  given X is dependent on this linear combination of all these features. So it's just one of the many possible ways of assuming that the dependency, But this particular form has been quite useful, and it has also has some nice properties. So if we rewrite this question to actually express the probability of Y given X in terms of X by taking by getting rid of the logarithm and we get this functional form and this is called a logistical function, it's a transformation of X into Y. As you see. on the right side here. So that the Xs will be mapped into a range of values from zero to 1.0. You can see, and that's precisely what we want. Since we have a probability here. And the function form looks like this. So this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. So as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. Once you set up the model set of objective function. To model the classifier, then the next step is to compute the parameter values. In general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. So in our case, let's assume we have training data. The training data here, X i and Y i and each pair is basically feature vector of X and a known label for that X Y, either one or zero. So in our case we are interested in maximizing this conditional likelihood. The condition likelihood here is basically to model y given the observed X. So it's not like a. It's not like a modeling X, but rather we're going to model this. Note that this is a conditional probability of Y given X. And this is also precisely what we want for classification. Now, so the likelihood function would be just a product over all the training cases. And in each case, this is the modeled probability of observing this particular training case. So given a particular XI, how likely we are going to observe the corresponding Y i of course, Y I could be one or zero and in fact the function form here would vary depending on whether Y sub I is one or zero. If it's one will be taking this form. And that's basically the logistical regression function. But what about this if it's 0? Well, if it's zero then we have to use a different form and that's this one. Now how do we get this one? That's just one minus the probability of y = 1, right? And you can easily see this now. The key point here is that the function form here depends on the observed. Y I if it's one, it has a different form than when it's 0. And if you think about when we want to maximize this probability we will basically going to want this probability to be as high as possible when the label is one. That means the document is in topic one. But if the document is not we are going to maximize this value, and what's going to happen is actually to make this value as small as possible. Because they sum to one. When I maximize this one. It's equivalent to minimize this one. So you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. So, as in other cases, when compute the maximum likelihood estimator Basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. And this again then gives us a standard optimization problem. In this case, it can be also solved in many ways. Newtons method is a popular way to solve this problem. There are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined scoring function to help us classify a document right? So what's the function? Well, it's this one. If we have all the betavalues already known, all we need is to compute The Xi's for that document. And then plugging those values that will give us a estimate. The probability that the document is in category one. OK, so much for logistical regression. Let's also introduce another discriminative classifier called K nearest neighbors. Now in general, I should say there are many such approaches. And thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. Here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. So the second classifier, is called k nearest neighbors. In this approach, we're going to also estimate the conditional probability of label. Given data, but in a very different way. So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set and that are most similar to this text object. Basically this is to find the neighbors of this text object in the training data set. So once we found we found the neighborhood and found the objects that are close to the. The object we're interested in classifying and say we have found the K nearest neighbors. That's why this method is called K nearest neighbors. And then we're going to assign the category that's most common in these neighbors. Basically, we're going to allow these neighbors to vote for the category of the object that we're interested in classifying. Now that means if most of them have a particular category, lets say category 1 then we're going to say this current object will have category one. This approach, can also be improved by considering the distance of a neighbor and the current object. Basically, we can assume a close neighbor will have more saying about the category of this object, so we can have we can give such a neighbor more influence on the vote and we can take weighted sum of their votes based on the distances. But the general idea is to look at the neighborhood and then try to assess the category based on the categories of the neighbors. Intuitively, this makes a lot of sense. But mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is P of Y given X. Now I'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work. I note that in naive base classifier we did not need a similarity function. An in logistical regression, we did not talk about the similarity function either. But here we explicitly requires a similarity function. Now this similarity function. Actually is a good opportunity for us to inject any of our insights about features. Basically, effective features are those that would make the objects that are in the same category look more similar, but distinguishing objects in different categories. So the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how K-NN works. Suppose we have a lot of training instances here. And I've colored them differently and to show just different categories. Now suppose we have a new object in the center that we want to classify. So according to this approach we're going to  find the neighbors. And let's first think of a special case of finding just one neighbor, the closest neighbor. Now in this case, if the, let's assume the closest neighbor is the box filled with diamonds and so then we're going to say. Well, since this is in, this object is in category of diamonds. Let's say then we're going to say, well, we're going to assign the same Category to our text object. But let's also look at the another possibility of finding a larger neighborhood. So let's think about the four neighbors. In this case, we're going to include a lot of other solid field boxes in red or pink. So in this case now we  are going to notice that among the four neighbors there are actually three neighbors in a different category. So if we take a vote, then we'll conclude the object is actually of a different category. So this both illustrates how K nearest neighbor works and also illustrates some potential problems of this classifier. Basically the results might depend on the K and indeed K is an important parameter to optimize. Now you can intuitively imagine if we have a lot of neighbors around this object and then we'll be OK because we have a lot of neighbors for help us decide categories. But if we have only a few, then the decision may not be reliable. So on the one hand we want to find more neighbors right? And then we have more votes, but on the other hand as we try to find the more neighbors, we actually could risk on getting neighbors that are not really similar to this instance, they might be actually far away as you try to get more neighbors, so although you get more neighbors to vote, but those neighbors aren't necessary so helpful because they are not very similar to the object. So the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation. Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose. The The parameter K here or some other parameters in other classifiers, and then you're going to assume this number that works well on your training set would be actually the best for your future data. So as I mentioned that KNN can be actually regarded as estimate of conditional probability of Y given X, and that's why we put this in the category of discriminative approaches. So the key assumption that we made in this approach is that the distribution of the label given the document or probability of a category given document. For example, probability of theta I given document D is locally smoothed and that just means we're going to assume that this probability is the same for all the documents in this region. R  here. And suppose we draw a neighborhood and we're going to assume in this neighborhood, since the data instances  are very similar, we're going to assume that the conditional distribution of the label, given the data, would be roughly the same. If D is not different, very different than we're going to assume that the probability of theta given D would be also similar, and so that's a very key assumption, and that that's. That's actually important assumption that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. If our similarity function captures objects that do follow similar distributions, then this assumption is OK. But if our similarity function could not capture that. Obviously the assumption would be a problem, and then the classifier would not be accurate. Let's proceed with this assumption. Then what we are saying is that in order to estimate the probability of a category given a document, we can try to estimate the probability of the category given that entire region. Now this has the benefit of course, of bringing additional data points to help us estimate this probability. And so this is precise idea of KNN. Basically now we can use the known categories of all the documents in this region to estimate this probability. And I have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region. So the numerator that you see here c of Theta and R is a count of the documents in region R with category Theta I. Since these are training documents, we know they're categories. We can simply count how many times we have seen sports here, how many times we have seen science etc. And then denominator is just a total number of documents training documents in this region, so this gives us a rough estimate of which category is most popular in this neighborhood, and we're going to assign the popular category to our data objective since it falls into this region.
410	4f58ef76-9ef1-4e19-8556-b84295a2afb3	This lecture is about the smoothing of language models. In this lecture, we're going to continue talking about probabilistic retrieval model. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method. So you have seen this slide from the previous lecture. This is the ranking function based on the query likelihood. Here we assume that the independence of generating each query word.  And the formula would look like the following where we take a sum over all the query words and inside the sum. There is a log of probability of word  given by the document or document language model. So the main task now is to estimate this document language model. As we said before, different methods for estimating this model would lead to different retrieval functions. So in this lecture we're going to look into this in more detail. So how do we estimate this language model? The obvious choice would be the maximum likelihood estimate that we have seen before, and that is we're going to normalize the word frequencies in the document. And the estimated probability would look like this. But this is a step function here. Which means all the words that have the same frequency count will have identical probability. Right, this is another frequent account that has a different probability. Note that for words that have not occurred in the document here they all have zero probability, so we this know this is just like the model that we assumed earlier in the lecture, where we assume that the user would sample word from the document. To formulate a query. And there's no chance of sampling any word that's not in the document, and we know that's not good. So how do we improve this, well? In order to assign a non zero probability to words that have not been observed in the document. We would have to take away some probability mass from the words that are observed in the document. So for example here we have to take away some probability mass because we need some extra probability mass for the unseen words. Otherwise they want to sum to one. So all these probabilities must sum to one. So to make this transformation and to improve the maximum likelihood estimate by assigning non zero probabilities to words that are not observed in the data. We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author had been written had been asked to write more words. For the document, the author might have written other words. If you think about this factor, then a smoother language model would be more accurate representation of the actual topic. Imagine you have seen an abstract of a research article. Let's say this document is abstract. Right, if we assume. unseen words in this abstract. We have all probability of zero. That would mean there is no chance of sampling a word outside the abstract to formulate a query. But imagine a user who is interested in the topic of this subject. The user might actually choose a word that's not in the abstract to use as query. So obviously if we had asked this author to write more, the author would have written the full text of that article. So smoothing of the language model is attempt to try to recover the model for the whole article and then of course we don't have really knowledge about any words that are not observed in the abstract there. So that's why smoothing is actually tricky problem. So let's talk a little more about how to smooth the language model. And the key question here is what probability should be assigned to those unseen words. And there are many different ways of doing that. One idea here that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by reference language model. That means if we don't observe the word in the data set, we're going to assume that it's probability is kind of governed by another reference language model that we will construct. It will tell us which unseen words will have likely higher probability. In the case of retrieval, a natural choice would be to take the collection language model as the reference language model. That is to say if we don't observe a word in the document. We're going to assume that the probability of this word would be proportional to the probability of the word in the whole collection. So more formally, we will be estimating the probability of a word given  a document as follows. If the word is seen in the document. Then the probability would be a discounted. maximum likelihood estimate P sub seen here. Otherwise. If the word is not seen in the document, we're going to let its probability be proportional to the probability of the word in the collection. And here the coefficient Alpha. Is to control the amount of probability mass that we assign to unseen words. Obviously, all these probabilities must sum to one, so Alpha sub D is constrained in some way. So what if we plug in this smoothing formula into our query likelihood running function? This is what we will get. Right, in this formula you can see. Right, we have. This as a sum over all the query words and note that we have written in the form of a sum over all the vocabulary. Can see here this is a sum over all the words in the vocabulary, but note that we have a count of the word in the query. So in effect we are just taking sum of query words right? This is now. A common way that we will use. Because of its convenience. In some transformations. So this is as I said, this is some of all the query words. In our smoothing method, we assume that the words that are not observed in the document we have somewhat different form of probability namely it's for this form. So we're going to then decompose this sum into two parts. One sum is over all the query words that are matching the document. That means in this sum, all the words have a non-zero probability in the document, sorry it's the non 0 count of the word in the document. They all occurred in the document. And they also have to of course have a non 0 count in the query, so these are the words that are matched. These are the query words that are matching the document. But on the other hand, in this sum we are taking sum over all the words that are not. All query words that are not matched in the document. So they occur in the query. Due to this this term, but they don't occur in the document. In this case, these words have this probability because of our assumption about the smoothing. But that here. These seen words have a different probability. Now we can go further by rewriting the second sum. As a difference of two other sums, basically the first sum is actually sum, over all the query words. We know that the original sum is not over all the query words. This is over all the  query words that are not matched in the document. So here we pretend that they are actually. Over all the query words, so we take a sum over all the query words. Obviously this sum has extra terms that are. This sum has extra terms that are not in this sum. Because here we are taking sum over all the query words there. It's not matched in the document. So in order to make them equal, we will have to then subtract another sum here. And this is the sum over all the query words that are matching the document. And this makes sense, because here we are considering all query words and then we subtract the query words that are matched in the document. That would give us the query words that not matched in the document. And this is almost reverse process of the first step here. And you might want to, why do we want to do that? Well, that's cause. If we do this, then we have different forms of terms inside these sums. So now you can see in this sum we have. All the words match the query words matched in the document and with this kind of terms. Here we have another sum. Over the same set of terms. match the query terms in document but inside the sum it's different. But these two sums can clearly be merged. So if we do that, we'll get another form of the formula that looks like the following. At the bottom here. And note that this is a very interesting formula because here we combined these two. That our sum. Over the query words matched in the document in the one sum here. And the other sum now is decomposed into two parts. And these two parts look much simpler just because these are the probabilities of unseen worlds. Now this formula is very interesting because you can see the sum is now over all the matched query terms. And just like in the vector space model, we take a sum of terms that are in the intersection of query vector and the document vector. So it all already looks a little bit like the vector space model. In fact, there's even more similarity here as we explain on this slide.
410	500e19a0-d0de-4358-8f74-427cf05f3c3e	And here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of. An object via active user. Using the ratings of similar users to this active user, this is called a memory based approach. Because it's a little similar to storing all the user information and when we are considering a particular user, we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that user information about those users to predict the preference of this user. So here's the general idea, and we use some notations here so X_ij denotes the rating of object o_j by user u_i. And n_i is the average rating of all objects by this user. So. This n_i is needed because we would like to normalize the ratings of objects by this user. So how do you do normalization? Well, we're going to just subtract the average rating from all the ratings. Now this is it will normalize these ratings so that the ratings from different users would be comparable. Because some users might be more generous and they generally given high ratings. But some others might be more critical, so their ratings cannot be directly compared with each other or aggregate them together. So we need to do this normalization. Now the prediction of the rating on the item by another user or active user. u_ a here. Can be based on the average ratings of similar users. So the user u_a is the user that we're interested in recommending items to and We now are interested in recommending this o_j, so we're interested in knowing how likely this user will like this objec. How do we know that? The idea here is to look at whether similar users to this user have liked this object. So mathematically, this is to say the predicted the rating of this user on this object user A on object o_j is basically combination of the normalized ratings of different users. And in fact here we're taking a sum over all the users. But not all users contribute equally to the average, and this is controlled by the weights. So this. Weight Controls the influence of user on the prediction. And of course, naturally, this way that should be related to the similarity between u_a and this particular user u_i. The more similar they are, then the more contribution would like a user UI to make in predicting the preference of u_a So the formula is extremely simple. You can see it's a sum over all the possible users. And inside the sum, We have their ratings, well, their normalized ratings as I just explained, the ratings need to be normalized in order to be comparable with each other. And then these ratings are weighted by their similarity. So you can imagine W of a an I is just a similarity of user A and user I. Now what's k here, well k is simply normalizer, it's just it's just one over the sum of all the weights. over all the users. And so this means basically, if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users. And it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction. Right, so this is basically the main idea of memory based approaches for collaborative filtering. Once we make this prediction. We also would like to map back to the rating that the user would actually make and this is to Further add the mean rating or average rating of this user u_a to the predicted value. This would recover a meaningful rating for this user. So if this user is generous than the average would be somewhat high and when we add that the rating will be adjust to a relatively high rating. Now when you recommend. Item to a user. This actually doesn't really matter 'cause you're interested in. Basically the normalized rating that's more meaningful, but when they evaluate these collaborative filtering approaches. is that typically assume the actual ratings of the user on these objects to be unknown, and then you do the prediction and then you compare the Predicted ratings with their actual ratings so they you do have access to their actual ratings, but then you pretend you don't know. And then you compare your system's predictions with the actual ratings. In that case, obviously the systems prediction would have to be adjusted to match the actual ratings of the user, and this is what's happening here, basically. OK, so this is the memory based approach. Now of course if you look at the formula if you want to write the program to implement it, you still face the problem of determining what is this W function right? Once you know the W function, then the formula is very easy to implement. So indeed there are many different ways to compute this function or this weight, w and specific approaches generally differ in how this is computed. So here are some possibilities and you can imagine. There are many other possibilities. One popular approach is to use the Pearson correlation coefficient. This would be a sum over commonly rated items and the formula is a standard Pearson correlation coefficient formula as shown here. So this basically measures whether the two users tend to all give higher ratings to similar items, or lower ratings to similar items. Another measure is the cosine measure, and this is to treat the rating vectors as vectors in the vector space. And then we're going to measure the angle and then compute the cosine of the angles of the two vectors, and this measure has been used in the vector space model for retrieval as well. So as you can imagine, there are many different ways of doing that. In all these cases, note that the user similarity is based on their preferences on Items and we did not actually use any content information of these items. It didn't matter what these items are. They can be movies they can book so they can be products. They can be text documents. We just didn't care about the content. And So this allows such approach to be applied to a wide range of problems. Now, in some new approaches, of course we would like to use more information about the user. Clearly we know more about the user, not just these preferences on these items. So, in the actual filtering system using collaborative filtering. We could also combine that with content based filtering. We could use more context information and those are all interesting approaches that people are still studying. There are new approaches proposed, but this memory based approach. It has been shown to work reasonably well and it's easy to implement and in practical application this could be a starting point to see if the strategy works well for your application. So there are some obvious ways to also improve this approach. And mainly we would like to improve the user similarity measure and there are some practical issues to deal with here as well. So for example, there will be a lot of missing values. What do you do with them? Or you can set them to default values, or the average ratings of the user and that would be a simple solution, but there are the advanced approaches that can actually try to predict those missing values. And then use the predicted values to improve the similarity. So in fact the memory based approach you can predict those missing values right? So you can imagine you have iterative approach where you first do some preliminary prediction and then you can use the predicted values to further improve the similarity function. So this is. Here is a way to solve the problem and the strategy obviously would affect the performance of collaborative filtering. Just like any other heuristics to improve these similarity functions, another idea which is actually very similar to the idea of IDF that we have seen in Text research is called inverse user frequency. Or IUF Now here the idea is to look at the where the two users share similar ratings. If the item is a popular item that has been viewed by many people, and seeing these two people. Interested in this item May not be so interesting, but if it's a rare item it has not been viewed by many users, but these two users viewed this item and they give similar ratings and that says more about their similarity, right? So it's kind of to emphasize more on similarity on items that are not viewed by many users.
410	506d605d-bcb9-448f-84b8-d0d53a6936fe	  This lecture is about the natural language content analysis. Natural language content analysis is the foundation of text mining. So we are going to first talk about this. And in particular, natural language processing. with a factor how we can represent text data? And this determines what algorithms can be used to analyze and mine text data. We're going to take a look at the basic concepts in natural language first. I'm going to explain these concepts using a simple example that you are seeing here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentence, we don't have to think about it to get meaning of it. But when a computer has to understand the sentence, the computer has to go through several steps. First, the computer needs to know what are the words, how to segment the words. In English this is very easy as we can just look at the space and then the computer would need to know the categories of these words, syntactical categories. So for example Dog is a noun, chasing is the verb, boy is another noun, etc. And this is called a lexical analysis. In particular tagging these words with these syntactic categories is called a part of speech tagging. After that, the computer also needs to figure out the relation between these words. So A and the dog will form a noun phrase. On the playground would be a  prepositional phrase, etc. And there are certain way for them to be connected together in order to generate the meaning. Some other combinations may not make sense. And this ..... This is called syntactic parsing. Or syntactical analysis or parsing of natural language sentence. The outcome is parse tree that you're seeing here. That tells us a structure of the sentence so that we know how we can interpret the sentence. But this is not semantics yet. So in order to get the meeting would have to map these phrases and these structures into some real world entities that we have in our mind. So dog is a concept that we know an A boy, the concept that we know. So connecting these phrases with what we know is understanding. For computer, would have to formally represent these entities by using symbols. So dog d1 means d1 is a dog, boy b1 means b1 refers to a boy, etc. And we also represented the chasing action as a predicate. So chasing is predic here with three arguments, d1, b1 and p1,  which is a playground, right? So this is a formal representation of the semantics of this sentence. Once we reach that level of understanding, we might also make inferences. For example, if we assume there's a rule that says if someone is being chased than a person can get scared, then we can infer this boy might be scared. This is the inferred meaning based on our additional knowledge. And finally, we might even further to infer... might further infer what  This sentence is requesting or why the person who said the sentence is saying this sentence. And so this has to do with understanding the purpose of saying this sentence, and this is called SPEECH Act analysis or pragmatic analysis. Which refers to the use of language. So in this case, person saying this might be reminding another person to bring back the dog. So this means when saying a sentence, the person actually takes the action. So the action here is to make a request. Now, this slide clearly shows that in order to really understand the sentence, there are a lot of things that the computer has to do now. In general, it's very hard for the computer to do everything, especially if we wanted to do everything correctly. This is the very difficult. Now the main reason why a natural language processing is very difficult because it's designed to make human communications efficient. As a result, for example, we omit a lot of common sense knowledge. Because we assume all the..... all of us have this knowledge, there's no need to encode this knowledge. And that makes communication efficient. We also keep a lot of ambiguities,  like ambiguities of words. And this is again because we assume that we have the ability to disambiguate a word, so there's no problem with having the same word to mean, possibly different things in different context. Yet, for a computer this would be very difficult because the computer does not have the common sense knowledge that we do, so the computer would be confused indeed, and this makes it hard for natural language processing. Indeed, it makes it very hard for every step in the slide that I showed you earlier. Ambiguity is a main killer, meaning that in every step there are multiple choices and the computer would have to decide what's the right choice and that decision can be very difficult, as you will see also in a moment. And in general we need common sense reasoning. In order to fully understand the natural language and computers today don't yet have that, and that's why it's very hard for computers to precisely understanding natural language  at this point. so here are some specific examples of challenges. Think about the word level ambiguity a word like design can be a noun or a verb, so we've got ambiguous part of speech tag. Root has also multiple meanings. It can be of mathematical sense, like in square root. Or it can be the root of a plant. Syntactic ambiguity refers to different interpretations of the sentence in terms of structures. So for example, natural language processing can actually be interpreted in two ways. So one is. The ordinary meaning that we will be getting. As well talking about this topic so it's processing of natural language. But there is also another possible interpretation, which is to say language processing is natural. Now we don't generally have this problem, but imagine for once a computer to determine the structure, the computer would actually have to make a choice between the two. Another classic example is a man saw a boy with a telescope. This ambiguity lies in  the question who had the telescope. This  is called a prepositional phrase attachment ambiguity meaning... where to attach this prepositional phrase with a telescope? Should it modify the boy or should it be modifying saw, the verb? Another problem Anaphora resolution """John " persuaded Bill to buy a TV for "himself.""" Does himself referred to John or Bill? Pre supposition is another difficulty. He has quit Smoking implies that he smoked before and we need to have such knowledge in order to understand the languages. Because of these problems, the state of the art natural language processing techniques cannot do anything perfectly, even for the simplest part of speech tagging, we still cannot solve the whole problem. The accuracy that I listed here just about 97% was just taken from some studies earlier, and these studies obviously have to be using particular datasets, so the numbers here are not really meaningful if you take it out of the context of the data set that are used for evaluation, but I show these numbers may need to give you some sense about the accuracy or how well we can do things like this. It doesn't mean on any data set to the accuracy will be precisely  97%. But in general we can do part of speech tagging fairly well, although not perfectly. Parsing would be more difficult, but for partial parsing, meaning to get that some phrases correct, we can probably achieve 90% or better accuracy. But to get the complete parse tree correctly is still very very difficult. For semantic analysis, we can also do some aspects of semantic analysis, particularly extraction of entities and relations. For example, recognizing this is the person, that's the  location, this person and that person met in some place etc. it can also do a word sense disambiguation for some extent. We can figure out the occurrence of root in this sentence refers to the mathematical sense etc. Sentiment analysis is another aspect of semantic analysis that we can do. That means we can tag the sentences general positive when it's talking about product. Or talking about the person. Influence, however, is very hard and we generally cannot do that for any big domain, and it's only feasible for very limited domain. And that's a generally difficult problem in artificial intelligence. Speech Act analysis is also very difficult, and we can only do this properly for very specialized cases with a lot of help from human. To annotate enough data for the computer to learn from. So the slides also shows that computers are far from being able to understand natural language precisely, and that also explains why the text mining problem is difficult, because we cannot rely on mechanical approaces or computational methods to understand the language precisely. Therefore, we have to use whatever we have today particular statistical machine learning methods, or. Statistical analysis methods to try to get as much meaning out from the text as possible. And later you will see that there are actually many such algorithms that can indeed extract the interesting knowledge from text, even though we cannot really fully understand the meaning of all the natural language sentences precisely.
410	50879266-52ce-4800-8022-5f2c69d13eea	This lecture is about the collaborative filtering. In this lecture, we're going to continue the discussion of recommender systems. In particular, we're going to look at the approach of collaborative filtering. You have seen this slide before when we talked about the two strategies to answer the basic question will user U like item X. In the previous lecture, we looked at the item similarity. That's content-based filtering. In this lecture, we will look at the user similarity. This is a different strategy called a collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based on the judgments of other users. And that is, we say, we will infer individual's interest or preferences from that of other similar users. So the general idea is the following. Given a user U, we're going to 1st find the similar users u_1 through u_m. And then we're going to predict the user preferences based on the preferences of these similar users. u_1 through u_m. Now the user similarity here can be judged based on their similarity in preferences on a common set of items. Now here you can see the exact content of item doesn't really matter. We're going to look at the only the relation between the users and items. So this means this approach is very general. It can be applied to any items. Not just the text options, so this approach it would work well under the following assumptions. 1st. Users with the same interest where have similar preferences. Second, the users with similar preferences probably share the same interest. So for example. If the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers. Alright, so those who are interested in information retrieval research probably all favor SIGIR papers. That's the assumption that we make and if this assumption is true, Then it would help collaborative filtering to work well. We can also assume that if we see people favor SIGIR papers, then we can infer their interest is probably information retrieval. So in this simple examples it seems to make sense. And in many cases, such assumption actually does make sense. So another assumption we have to make is that there are sufficiently large number of user preferences available to us. So for example, if you see a lot of ratings of users for movies, and those indicate their preferences for movies, and if you have a lot of such data, then collaborative filtering can be very effective. If not, there will be a problem, and that's often called cold start problem. That means you don't have many preferences available, so the system could not afford to take advantage of collaborative filtering yet. So let's look at the collaborative filtering problem in a more formal way, and so this picture shows that we are in general considering a lot of users showing we're showing M users here. So u_1 through u_m and we also considering a number of objects, let's say N objects denoted as o_1 through o_n and then we will assume that the users will be able to judge those objects and the user could for example give ratings for those items, for example, those items could be movies. Could be products and then the users would give ratings 1 through 5 let's say. So what you see here is that we have shown some ratings available for some combinations, so some users have watched some movies they have rated those movies. They obviously won't be able to watch all the movies and some users may actually only watch a few movies. So this is in general a sparse matrix. So many items, many entries. Have unknown values. And what's interesting here is we could potentially infer the value of an element in this matrix based on other values, and that's actually the central question in collaborative filtering, and that is we assume there's unknown function here F that would map a pair of a user and an object to the rating. And we have observed some values of this function. And we want to infer the value of this function for other pairs, That with that don't have values available here. So this is very similar to other machine learning problems where we know the values of the function on some training data sets and we hope to predict the values of this function on some test data, right? So this is a function approximation. And how can we figure out the function based on the observed ratings? So this is the setup. Now there are many approaches to solving this problem, and in fact this is a very active research area, a reason there are special conferences there are special conferences dedicated to the problem. R is the major conference devotes to the problem. the problem.
410	5190e288-54f7-4021-9083-8e8ceac11345	This lecture is about the latent Dirichlet allocation or LDA. In this lecture, we're going to continue talking about topic models. In particular, we are going to talk about some extensions of PLSA, and one of them is LDA or latent Dirichlet allocation. So the plan for this lecture is to cover two things. One is to extend the PLSA with prior knowledge that would allow us to have in some sense a user controlled PLSA, so it doesn't blindly just listen to data but also would listen to our needs. The second is to extend the PLSA as a generative model fully generated model. This has led to the development of Latent Dirichlet Allocation or LDA. So first let's talk about the PLSA with prior knowledge. In practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. The standard PLSA is going to blindly listen to the data by using maximum likelihood estimator. We are going to just fit data as much as we can and get some insight about data. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. For example, we might expect to see retrieval models as a topic in information retrieval. We also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects. Now, a user may also have knowledge about the topic coverage. And we may know which topic is definitely not covered in which document or is covered in the document. For example, we might have seen those tags topic tags assigned to documents. And those tag could be treated as topics if we do that, then a document that can only be generated using topics corresponding to the tags already assigned to the document. If the document is not assigned to a tag, we're going to say there's no way for using that topic to generate the document. The document must be generated by using the topics corresponding to the assigned tags. So the question is, how can we incorporate such knowledge into PLSA? It turns out that there's a. A very elegant way of doing that, and that's all incorporated such knowledge as priors on the models. And you may recall in Bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen. So in this case we can use maximum a posteriori estimate, also called map estimate, and the formula is given here. Basically is to maximize the posterior distribution probability and this is a combination of the likelihood of data and the prior. So what would happen is that we're going to have an estimate that listens to the data and also listens to our prior preferences. We can use this prior, which is denoted as P of Lambda to encode. All kinds of preferences and constraints. So for example, we can use this to encode the need of having precisely 1 background the topic. Now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. In other words, in other cases if it's not like that, we're going to say supplier says it's impossible. So the probability of that kind of model setting would be 0 according to our prior. So now we can also, for example use the prior to force particular choice of topic to have a probability of a certain number. For example, we can force the document D to choose topic one with probability of 1/2. Or we can prevent a topic from being used in generated document. So we can say the third topic should not be user generated. Document D will set to the Pi value to 0 for that topic. We can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words. In this case, we're not going to say it's impossible, but we're going to just strongly favor certain kind of distributions. And you will see example later. The map can be computed using a similar EM algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. And in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the prior will be similar to the data. As a result, we can combine the two and the consequences that you can basically convert the influence of the prior into the influence of having additional pseudo data because the two functional forms are the same and they can be combined. So the effect is as if we had more data. And this is convenient for computation. It doesn't mean conjugate prior is the best way to define the prior. So now let's look at the specific example. Suppose the user is particularly interested in battery life of a laptop, and we're analyzing reviews. So the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life. So we could do say there's a distribution that's entirely concentrated on battery life and we all priors is that one of your distributions should be very similar to this. Now if we use map estimator with the conjugated prior, which is Dirichlet prior Dirichlet distribution based on this preference, then the only difference in the EM algorithm is in the M step. When we re estimate word distributions, we are going to add. Additional counts to reflect our prior right? So here you can see the pseudocounts are defined the based on the probability of words in our prior. So battery obviously will have a high pseudocounts similar life would have also high pseudocounts or the other words. We have 0 pseudocounts because their probability is zero in the prior and when you see this is also controlled by a parameter mu and We're going to add mu multiplied by the probability of W given our prior distribution to the connected counts. When we re estimate the when we re estimate the this world distribution right? So this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been generated from this topic. But now we force this distribution. To give more probabilities to these words by adding them to the pseudocounts so to artificially in effect, we artificially inflated their probabilities and to make this distribution we also need to add this many pseudocounts to the denominator. This is the total sum of all the pseudocounts we have added for all the words. This would make this again a distribution. Now, this is a intuitively very reasonable way of modifying the EM algorithm and theoretically speaking, this deal works, and it computes the map estimator. It's useful to think about two specific extreme cases of Mu. Now can you picture. Think about what would happen if we set Mu to Zero. Well, that's essentially to remove this prior, so mu in some sense indicates our strength on prior. Now what would happen if we set Mu to positive Infinity? Well, that's to say this price is so strong that we're not going to listen to the data at all. So in the end you can see in this case we can do make one distribution fixed to the prior. You see why? When mu is Infinity, we basically let this one dominate. In fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. So in this case we can even force the distribution to entirely focused on battery life. But of course this won't work well 'cause it cannot attract other words, it would affect the accuracy of counting. Topics about the battery life so in practice mu is set somewhere in between, of course. So this is one way to impose our prior. We can also impose some other constraints. For example, we can set any parameters for constraints, including zero as needed. For example, we may want to set one of the pis to 0. And this would mean we don't allow that topic to participate in generating that document. And this is only reasonable, of course, when we have prior knowledge that strongly suggests this.
410	51a1afdf-e864-49de-a80a-760bda4173a7	this lecture is about the basic a measures for evaluation of text retrieval systems in this latter we're going to discuss how we design basically measures to quantitatively compare to regional systems this is the slide that you have seen earlier in the lecture well we talked about the cranfield evaluation methodology we can have that uh scratching that consists of queries documents and relevance judgments we can then run in two systems on these datasets to quantitatively evaluate their performance and we raised the question about which set of results is better in the system a better or system be better so let's now talk about how to actually quantify their performance suppose we have a total of ten relevant documents in the collection for this query now the relevance judgments shown on the right did not include the all the time obviously and we have only seen three relevant documents there but we can imagine there are other relevant documents in charger for this query so now intuitively we thought that system a is better becaus it did not have much noise and in particular we have seen among the three results two of them are relevant but in system be we have five results and only three of them are relevant so intuitively it looks like system is more accurate and this intuition can be captured by a measure called precision where we simply compute to what extend order retrieval results are relevant if you have one hundred percent precision that would mean all the retrieval documents are relevant so in this case the system a has a precision of two out of three system B has three over five and this shows that system a is better by cuisine but we also talked about system be might be preferred by some other users hold like to retrieve as many relevant documents as possible so in that case we have to compare the number of relevant documents that retrieve and there is another measure total recall this measures the completeness of coverage of relevant documents in your retriever result so we just assume that there are ten relevant documents in the collection an here we've got the two of them in system a so the recoil is two out of ten whereas system be has got a three so it's a three out of ten now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines and they are very important because they are also widely used in many other task evaluation problems for example if you look at the applications of machine learning you tend to see precision recall numbers being reported for all kinds of tasks OK so now let's define these two measures more precisely and these measures are to evaluate a set of retrieved documents so that means we are considering that approximation of the set of relevant documents we could distinguish it four cases depending on the situation of the document a document that can be retrieved or not retrieved right because we're talking about the set of results a document can be also relevant or non relevant depending on whether the user thinks this is useful document so we can now count of documents in each of the four categories we can have a to represent the number of documents that are retrieved and relevant the for documents that are not retriever but relevant etc now with this table then we have defined precision as the ratio of the random in the retrieval documents A to the total number of retrieve the documents so this is just a divided by the sum of A and C by the sum of this column similar records defined by dividing a by the sum of A and B so that's again the divider eight by the sum of the rule instead of the column right so we gotta see precision recall is all focused on looking at eight that's the number of retrieval relevant documents but we're going to use different denominators OK so what would be an ideal without well you can easy to see in the ideal case we have precision and recall all to be one point zero that needs we have got one percent of all the relevant documents in our results and all the results that will return all relevant at least there's no single not ready not in return he reality however high record hence to be associated with low precision and you can imagine why that's the case as you go down the list to try to get as many relevant documents as possible you tend to encounter a lot of non relevant documents so the precision will go down note that this set can also be defined by a cut off in the ranked list that's why although these two measures are defined for a set of retrieved documents they are actually very useful for evaluating a ranked list there are fundamental measures in terms of retrieval and many other tasks we often are interested in the position at ten documents for web search this means we look at the how many documents among the top ten results are actually relevant now this is a very meaningful measure becaus it tells us how many random documents are user can expect to see on the first page of search results where they typically shoot ten results so precision and recall the basically measures an we need to use them to further evaluate a search engine but there are the building blocks really we just to say that there tends to be a tradeoff between brazilian recalls so naturally it would be interesting to combine them and here's one measure that's often used called F measure anne it's harmonic mean of precision and recall is defined on this slide so you can see it first compute the adverse of are in the P here and then it would interpret the two by using the coefficients depending on a prem that beta and after some transformation you can easy to see it would be of this form and in any case is interesting combination of precision and recall and beta is the parameter that's often set to one it can control the emphasis on precision or recall when we set bait out to one we end up having a special case of F measure often called F one this is a popular measure that's often used little combined precision and recall at the formula looks very simple it's just this here now it's easy to see that if you have a larger precision or larger recall then F magic would be high but what's interesting is that the trade off between precision and recall is the capturing the interesting way in F one so in order to understand that we can first look at the natural question why not just combining them using a simple arithmetic mean as official here now would be likely the most natural way of combining them so what do you think if you want to think more you can post a video so why is this not as good as F one or what's the problem with this now if you think about the arithmetic mean you can see this is the sum of multiple terms in this case this is some of precision and recall in the case of the sum that total value tends to be dominated by the large values that means if you have a very high P or very high are then you really don't care about whether the other value is low so the whole sum would be higher now this is not desirable becaus one can easily have a perfect the recall we can have perfect recall easily can you imagine how it's probably very easy to imagine that we simply retrieve all the document in the collection and then we have a perfect recall and this will give us point five as the average but such results are clearly not very useful for users even though the the average using this formula would be relatively high in contrast you can see if one would reward the case where precision and recall are roughly similar so it would paralyze a case where you have extremely high value for one of them so this means FY encodes a different the tradeoff between that this example shows actually a very important methodology here but he tried to solve a problem you might naturally think of one solution let's say in this case it's arithmetic mean but it's important not to settle on this solution it's important to think whether you have other ways to combine them and once you think about the multiple variants it's important to analyze their difference and i think above which one makes more sense in this case if you think more carefully you will feel that F one probably makes more sense than the simple arithmetic mean although in other cases there may be different results but in this case the arithmetic mean seems not reasonable but if you don't pay attention to these subtle differences you might just take easy way to combine them and then go ahead with it and here later you will find that measure doesn't seem to work well i saw at this methodology is after very important in general in solving problems and try to think about the best solution try to understand the problem very well and then no why you needed this measure and why you need to combine precision and recall and then use that to guide you in finding a good way to solve the problem to summarize we talked about precision which addresses the question other retrieval results all relevant we also talk about the recall which address to the question have all the relevant documents being retrieved these two are the two basic measures in text retrieval evaluation they are useful for many other tasks so as well we talked about the F mash as a way to combine precision and recall we also talked about the trade off between precision recall at this turns out to depend on the users search tasks and will discuss this point more in the later lecture
410	51be74e8-eb10-47c2-a768-b688605de1e0	This lecture is about the syntagmatic relation discovery and conditional entropy. In this lecture, we're going to continue the discussion of word association mining an analysis. We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. Now we address the different scenario where we assume that we know something about the text segment. So now the question is, suppose we know eats occured in the segment, how would that help us predict the presence or absence of a word like meat? And in particular we want to know whether the presence of eats has helped us predict the presence of meat. And if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat. We can also ask the question, what if we know of the absence of eats? Would that also help us predict the presence or absence of meat. So these questions can be addressed by using. Another concept, called the conditional entropy. So to explain this concept, let's first look at the scenario we had before where we know nothing about the segment. So we have these probabilities indicating whether a word like meat occurs or does not occur in the segment, and we have the entropy function that looks like what you see on the slide. I suppose we know eats is present, so now know the value of another random variable that denotes eats. Now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat. Given that we know eats occured in the context. So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. So this equation now here. Would be. The conditional entropy conditioned on the presence of eats. Right? So you can see this is essentially the same entropy function as you have seen before, except that we all the probabilities now have a condition. And this then tells us the entropy of meat after we have known eats occurring in the segment. And of course, we can also define this conditional entropy for the scenario where we don't see eats. So if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. So now putting different scenarios together, we have the complete definition of conditional entropy as follows. Basically. We're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1. Basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. So if you expand this entropy, then you have the following equation. Where you see the involvement of those conditional probabilities. Now in general, for any discrete random variables X&Y we have. The conditional entropy is no larger than the entropy of the variable X, so basically this is upper bound for the conditional entropy. That means by knowing more information about the segment, we won't be able to increase the uncertainty. We can only reduce uncertainty, and that intuitively makes sense because as we know more information, it should always help us. Make the prediction and it cannot hurt the prediction in any case. Now what's interesting here is also to think about what's the minimum possible value of this conditional entropy. Now we know that the maximum value is the entropy of X. But what about the minimum? So what do you think? I hope you can reach the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this. So let's see how we can use conditional entropy to capture syntagmatic relations. Now, of course this conditional entropy gives us directly one way to measure the association of two words. Because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. So, here we listed the this conditional entropy in the middle. So it's here. So what is the value of this? Now. This means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence. Now of course this is zero because there's no uncertain there anymore Once we know whether the word occurs in the segment we will already know the answer for the prediction. So this is 0. And that's also when this conditional entropy reaches the minimum. So now let's look at some other cases. So this is a case of. Knowing the and trying to predict the meat and this is the case of knowing eats and trying to predict the meat. Which one do you think is smaller? Note that a smaller entropy means easier for prediction. Which one do you think is higher? Which one is smaller? If you look at the uncertainty, then in the first case the doesn't really tell us much about the meat, so knowing the occurrence of the doesn't really help us reduce the entropy that match, so it stays as fairly close to the original entropy of meat. Whereas in the case of eats, eats is related to meet, so knowing presence of eats or absence of eats would help us predict wether meat occurs so it can help us reduce entropy of meat, so we should expect the second term, namely, this one to have a smaller entropy. And that means there is a stronger association between meat and eats. So we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0? And for what kind of words would it reach its maximum? Well, that's when this W is not really related to meat. like the, for example, it would be very close to the maximum, which is the entropy of meat itself. So this suggests that we can use conditional entropy for mining syntagmatic relations. The algorithm would look as follows. For each word W1, we're going to enumerate the overall other words W2, and then we can compute the conditional entropy of W1 given W2. And we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word W1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with W1. Note that we need to use a threshold to find these words. The threshold can be the number of top candidates to take or absolute value for the conditional entropy. Now this would allow us to mine the most strongly correlated words with a particular word W1 here. But this algorithm does not help us mine the strongest K syntagmatic relations from entire collection. Because in order to do that, we have to ensure that these conditional entropies are  comparable across different words. In this case of discovering Syntagmatic relations for a target word like W1, we only need to compare the conditional entropies For W1 given different words. And in this case they all comparable right? So the conditional entropy of W1 given W2 and conditional entropy of W1 given  W3 are comparable. They all measure how hard it is to predict W1. But if we think about the two pairs where we share W2 in the same condition and we try to predict the W1&W3, then the conditional entropies are actually not comperable. And you can think about this question, why? So Why are they not comparable? Well, that was because they have a different upper bounds, right? So those upper bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds, so we cannot really compare them in this way. So how do we address this problem? Later we'll discuss we can use mutual information to solve this problem.
410	5350ccd0-beab-48fc-8484-d8e6a38c4cbf	Now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies. OK, So what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1. Now it's interesting to think about a scenario where we start adding more words to the document. So what would happen if we add many the's to the document? Now this will change the game, right? So how? Well, picture what would the likelihood function look like now? It started with the likelihood function for the two words. As we add more words, we know that,  we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. Since in this case all the additional terms are the, we're going to just multiply by this term for the probability of the. An if we have another occurrence of the, we multiply again by the same term and so on, so forth until we add as many terms as the number of the's that we added to the document D prime. Now this obviously changes the likelihood function, so what's interesting is now to think about how would that change our solution. So what's the optimal solution now? Intuitively, you would know the original solution. 0.9 and  0.1 will no longer be optimal for this new function, right? But the question is how should we change it? Well in general they sum to one. So in order to change it, we must take away some probability mess from one word. An added the probability mass to the other word. The question is which word to have a reduced the probability and which word to have a larger probability? And in particular, let's think about the probability of the. Should it be increased to be more than 0.1 or should we decrease it to less than  0.1? What do you think? Now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. Now if you look at the formula for a moment then you will see. It seems that now the objective function is more influenced by the than text before each contributed one turn. So now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the. Why? Because the is repeated many times if we increase it a little bit, it will have more positive impact, whereas a slight decrease of text. We have relatively small impact because it occurs just once. Right, so this means there is another behavior that we observe here that is high frequency words generally will have high probabilities from all the distributions. And this is no surprise at all, because after all we are maximizing the likelihood of the data. So all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. This is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. It also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. Now it's also interesting to think about the impact of probability of theta sub B. The probability of choosing one of the two component models. Now, we've being so far, assuming that each model is equally likely and that gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the probability of choosing a background model. Now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficient in front of 0.9 which is now 0.5 would be even larger. When this is larger the overall result would be larger and that also makes them less important for theta sub D to increase the probability for the because it's already very large so the impact here of increasing the probability of the is somewhat regulated by this coefficient 0.5. If it's a larger on the background then it becomes less important to increase the value so. So. This means the behavior here, which is high frequency words tend to get higher probabilities are affected or regularised somewhat by the probability of choosing each component. The more likely a component that is being chosen, it's more important than to have higher values for these frequent words. If you have a very small probability of being chosen, than the incentive is less. So to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st Every component component model attempts to assign high probabilities to high frequency words in the data. And this is to collaboratively maximize likelihood. Second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize the likelihood. 3rd, the probability of choosing each component regulates the collaboration and competition between the component models. It would allow some component models to respond more to the change, for example of frequency of data point in the data. We also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. A large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. Now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. And that would make the discovered  topic more discriminative. This is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in Bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. In fact, if it's not consistent, we're going to say the model is impossible, so it has a zero prior probability, and that effectively excludes such a scenario. This is also an issue that we will talk more later.
410	54ab232c-85cb-4829-abd0-6cbaed5f3fc8	This lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. In this lecture we're going to give an overview of statistical language models. These models are general models that cover probabilistic topic models as special cases. So first, what is the statistical language model? A statistical language model is basically the probability distribution over word sequences. So, for example, we might have a distribution that gives """Today is Wednesday"" a probability of 0.001" It might give """Today Wednesday is"" which is a non" grammatical sentence very, very small probability as shown here. "And similarly another sentence, ""The" "eigenvalue is positive"", might get a" probability of 0.00001 So as you can see, such a distribution clearly is context dependent. It depends on the context of discussion. Some word sequences might have higher probabilities than others. But the same sequence of words might have a different probability in a different context. And so this suggests that such a distribution can actually characterize topic. Such a model can also be regarded as a probabilistic mechanism for generating text. And that just means we can view text data as data observed from such a model. For this reason, we also call such a model generative model. So now Given a model, we can then sample sequences of words. So for example, based on the distribution that I have shown here on this slide, we might, let's say, sample "a sequence like ""today is Wednesday""" because it has a relatively high probability, we might often get such a sequence. "We might also get ""the eigenvalue is" "positive"", sometimes with a smaller" probability. Very, very occasionally, "we might get ""today" "Wednesday is"" because the probability is" so small. So in general, in order to characterize such a distribution, we must specify probability values for all these different sequences of words. Obviously it's impossible to specify that, because it's impossible to enumerate all the possible sequences of words. So in practice we will have to simplify the model in some way. So the simplest language model is called a unigram language model. In such a case, we simply assume that text is generated by generating each word independently. Now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model. Basically now the probability of a sequence of words w_1 through w_n would be just a product of each. The probability of each word. So for such a model we have as many parameters as the number of words in our vocabulary. So here we assume we have N words, so we have N probabilities, one for each word, and they sum to one. So now we can assume our text is a sample drawn according to this word distribution. That just means we're gonna draw a word each time and then eventually we'll get a text. So for example now again. We can try to sample words according to a distribution. We might get Wednesday often or today often and some other words like eigenvalue might have a small probability, etc. Now, with this we actually can also compute the probability of every sequence, even though our model only specifies the probabilities of words. This is because of the independence assumption. So specifically we can compute the "probability of ""today is Wednesday""." Because it's just a product of the probability of today, probability of is and probably Wednesday. For example, I showed some fake numbers here and we might then multiply these numbers together to get the probability "of ""today is Wednesday""." So as you can see, with N probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model. Ignore the word order, so it may not be effective for some problems such as speech recognition, where you may care about the order of words. But it turns out to be quite sufficient for many tasks that involve topic analysis, and that's also what we're interested in here. So when we have a model, we generally have two problems that we can think about. One is given a model. How likely we'll observe certain kind of data points. That is, we're interested in the sampling process. The other is the estimation process and that is to figure out the parameters of the model given some observed data, and we're going to talk about that in a moment. Let's first talk about the sampling. So here I show two examples of word distributions or unigram language models. The first one has higher probabilities for words,  text, mining, association, etc. Now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. So in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... The text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. Assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents. The second distribution show on the bottom has different words that with higher probability. Food, nutrition and healthy, diet etc. So this clearly indicates a different topic and in this case it's probably about health. So if we sample words from such distribution, then the probability of observing a text mining paper would be very very small. On the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher. So that just means given a particular distribution, different text will have different probabilities. Now let's look at the estimation problem. Now, in this case, we're going to assume that we have observed data. We know exactly what the text data looks like. In this case, let's assume we have a text mining paper. In fact, it's abstract of the paper, so the total number of words is 100, and I've shown some counts of individual words here. If we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? OK, so the problem now is just the estimated probabilities of these words as I've shown here. So what do you think? What would be your guess? Would you guess text that has a very very small probability or relatively large probability? What about the query? Your guess probably will be dependent on how many times we have observed this word in the text data, right? And if you think about it for a moment, and if you like many others, you would have guessed that text has a probability of 10 out of 100. Because I've observed text 10 times in the text that has a total of 100 words. And similarly, mining has five out of 100. And query as a relatively small probability, just observd once. So it's one out of 100. Right, so that, intuitively, is a reasonable guess, but the question is Is this our best guess or best estimate of the parameters? Of course, in order to answer this question we have to define what we mean by best. In this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. And it's the best in that it would give our observed data the maximum probability. Meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. And this is called a maximum likelihood estimate.
410	59806251-1c41-4c2c-9207-e95bc0618a0e	This lecture is about a probabilistic retrieval model. In this lecture, we're going to continue the discussion of tax retrieval methods. We can do look at the another kind of very different way to design ranking functions than the vector space model that we discussed before. Being probabilistic models, we define the ranking function based on the probability that this document is relevant to this query. In other words, we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are observations from random variables. Note that in the vector space model we assume they are vectors, but here we are assumed. We assume they are the data observed from random variables. And so the problem of retrieval now becomes two estimated. probability of relevance. In this category of models there are different variants. The classical problem is model has led to the BM 25 retrieval function which we discussed in the vector space model. Because it's a form is actually similar to objectives space model. In this lecture, we will discuss another subclass in this. Big class. Called a language modeling approaches to retrieval. In particular, we're going to discuss the query likelihood retrieval model. Which is one of the most effective models in probabilistic models. There is also another line called a divergent from randomness model which has led to. The PL-2 function. It's also one of the most effective state of the other travel functions. In query likelihood Our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively this probability. Just captures the following probability and that is if a user likes document D. How likely would the user enter query Q in order to retrieve documenting? So assume that the user likes D. Because we have a relevance value here and then we asked the question about the how likely will see this particular query from this user. So this is the basic idea. Not to understand this idea, let's take a look at the general idea or the basic idea of probabilistic retrieval models. So here are listed at some. Imagine the relevance of status values or relevance judgments. Often queries and documents. For example, in this line it shows that query one. is A query that the user typed in and the D1 is a document the user has seen and one means the user thinks the one is relevant to Q1. So this R here can be also approximated by the click through data that a search engine can collect by watching how you interact with the search results. So in this case, let's say the user clicked on this document, so there's one here. Similarly. The user clicked on D2 also, so there is 1 here. In other words, D2 is assumed to be relevant to Q1. On the other hand, D3  is non  relevant. There's a 0 here. At the voice down relevant and then  D5 is again relevant. And so on so forth. And this part. Maybe data collected from a different user. So this user typing Q1 and then found that D1 is actually not useful. So divine is actually non relevant. In contrast here we see it's relevant. And all this could be the same query typing by. The same user at different times. But D2  is also relevant, ET cetera. And here we can see more data. Then what about other queries? Now we can imagine we have a lot of such data. We can ask the question, how can we then estimate the probability of relevance? Right, so how can we compute this probability of relevance? Or intuitively that just means? If we look at the all the entries where we see this particular D and this particular Q, how likely will see a one on the third column? So basically that just means we can just collect those accounts. We can first count the how many times we have seen Q&D as a pair. in this table and then count how many times we actually have also seen one in the third column. So and then we just. Compute the ratio. So let's take a look at some specific examples. Suppose we're trying to compute this probability for D1D2 and D3 for Q1. What is the estimated probability? Now think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability. Have you seen that if we are interested in Q1 and D1 will be looking at these two pairs? And in both cases. Actually, in one of the cases. The user has said This is why this is relevant, so R is equal to 1 in only one of the two cases. In the other case it's 0. So that's one out of two. What about the D1 and D2? "" They are here.  in both cases. In this case R  is equal to 1, so it's two out of two. And so on, so forth. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for D1D2 and D3. For this query we can simply rank them based on these probabilities, and so that's the basic idea of probabilistic retrieval model, and you can see it makes a lot of sense. In this case it's going to rank D2 above all the other documents, because in all the cases when you have seen D1 and D2. Eyes equals one the user clicked on this document. So this also. Should. Show that with a lot of click through data, a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even a small number of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user who typing this query. Now of course the problems that we don't observe all the queries and all the documents and all the relevance values. There will be a lot of unseen documents. In general we only collected data from the documents that we have shown to the users. There are even more unseen queries because you cannot predict what queries would be typing by users. So obviously this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic idea of problems control model and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and then some queries where the solutions that we have to approximate in somewhere, right? So in this particular case code query like whole retrieval model, we just approximate this by another conditional probability. P of Q given D an R is equal to 1. So the condition part. We assume that the user likes the document because we have seen that the user clicked on this document. And this part shows that we're interested in how likely the user would actually enter this query. How likely will see this query in the same role? So no data here. We have made an interesting assumption here. Basically, we can do assume that whether the user types in this query has something to do with whether user likes the document. In other words, we actually make the following assumption. And that is a user formula to query based on an imaginary relevant document. If you just look at this is conditional probability. It's not obvious we're making this assumption. So what I really meant is that. To use this new conditional probability to help us score, then this knew conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Otherwise we would be having similar problems as before an by making this assumption, we have some way to bypass this big table and try to just model how the user formulates the query. OK, so this is how you can simplify the general model so that we can derive a specific Iranian function later. So let's look at how this model work for our example, and basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query. So we ask this question and we quantify the probability and this probability is conditional probability of. Observing this query if a particular document is infected, imaginary relevant document in the user's mind. Here you can see we compute all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values, we can then rank these documents based on these values. So to summarize, the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable R here, and then let's a scoring function be defined based on this conditional probability. We also talked about the approximate in this by using the query likelihood. And in this case we have a ranking function that's basically based on the probability of a query given the document, and this probability should be interpreted as the probability that a user who likes document D would pose queria Q. Now the question of course, is how do we compute this conditional probability? At this, in general has to do with how to compute the probability of text, because Q is attached. And this has to do with. Model called the Language model and this kind of models are proposed to model text. So more specifically, we would be very interested in the following conditional probability as issuing this here if the user. This document how likely the user would oppose this query. Ann In the next lecture working through, give an introduction to language models that we can see how we can model text with the probabilistic model in general.
410	5bb813bd-6b7d-4f77-8156-21995f5944ad	This lecture is about the similarity based approaches to text for clustering. In this lecture, we're going to continue the discussion of how to do a text clustering. In particular, we're going to cover a different kind of approaches than generative models. And that is similarity based approaches. So the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. Now this is in contrast with a generative model where we implicitly define the clustering bias. By using a particular objective function like a likelihood function. The whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. So once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups. Anne, try to maximize the intragroup similarity and minimize the intergroup similarity. That is, to ensure the objects that are put in the same group to be similar, but the objects that are put into different groups to be not similar, and these are the general goals of clustering. And there's often a tradeoff between achieving both goals. Now, there are many different methods for doing similarity based clustering. In general, I think we can distinguish two strategies at high level. One is to progressively construct the hierarchy of clusters. And so this often leads to hierarchical clustering an we can further distinguishes two ways to construct the hierarchy depending on whether we started with the collection to divide the collection or start with individual objects and gradually group them together. So one is bottom up that can be called agglomerative, where we gradually Group A similar object into larger and larger clusters until we group everything together. The other is top down or divisive. In this case we gradually partitioning the whole data set into smaller and smaller clusters. The other general strategy is to start with the initial tentative clustering and then iteratively improve it and this often leads to a flat clustering. One example is K means. So as I just said, there are many different clustering methods available and. A full coverage of all these custom methods would be beyond the scope of this course. But here we can talk about the two representative methods and. In some detail. One is hierarchical agglomerative clustering or agency, the other is K-MEANS. So first let's look at the agglomerative hierarchical clustering. In this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and then we can stop when some stopping criterions that. I could be either some number of classes has been achieved, or the threshold for similarity has been reached. There are different variations here and there mainly differ in the ways to computer group similarity based on the individual object similarity. So let's illustrate how can induce a structure based on just similarity. So start with all the text objects and we can then measure the similarity between them. Of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together. And then was going to see which pair is. The next one to group. Maybe these two now have the highest similarity. And then we can gradually group them together in every time we're going to pick the highest similarity similarity pairs to group. This will give us a binary tree eventually to group everything together. Now depending our applications, we can use the whole hierarchy as structure for browsing for example, or we can choose the cut off at say come here to get four clusters. Or we can use the threshold to cut or we can cut at this high level to get just the two clusters. So this is a general idea. Now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity. We are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups. And there are also different ways to do that, and there's the three popular methods are single link complete link an average link? So given two groups and singling algorithm is going to define the group similarity as the similarity of the closest repair of the two groups. Complete Link defines the similarity of two groups as the similarity of the father sister pair. Average link defines the similarity as average of similarity of all the pairs of the two groups. So it's much easier to understand these methods by illustrating them. So here are two groups G1 and G2 with some objects in each group, and we know how to compute the similarity between two objects. But the question now is, how can we compute the similarity between the two groups? And then we can in general basis on the similarities of the objects in the two groups. So in terms of single link and we're just looking at the closest pair. So in this case these two pairs objects would define the similarity of the two groups. As long as they are very close orders, say the two groups are very. Close so it's optimistic view of similarity. The complete link, on the other hand, will be in some sense pessimistic and by taking the similarity of the two farthest appear as the similarity for the two groups. So we're going to make sure that if the two groups are  having a high similarity, then every pair of the two the objects in the group will be insured to have high similarity. Every link is in between, so it takes average of all these pairs. Now, these different ways of computing group similarities will need to different clustering algorithms, and they will generally give different results. Now, so it's useful to take a look at their differences and to make comparison. Our first single link. Can be expected to generate the loose clusters. The reason is becausw. As long as two objects are very similar in the two groups, it would bring the two groups together. If you think about this is similar to having parties with people, then it just means two groups of two groups of people would be putting together as long as each group there is a person that is well connected with the other group. So the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups. In this case, the cluster is rules because there's no guarantee that other members of the two groups are actually very close to each other. Sometimes they may be very far away. Now in this case it's also based on individual decision, so it could be sensitive to outliers. The complete linker is in the opposite situation where we can expect the clusters to be tight. Anne, it's also based on individual decision, so it can be sensitive to outliers. Again, to continue the analogy to having a party of people then complete the link would mean when two groups come together they want to ensure that even the. Even the people that are unlikely to talk to each other would be comfortable with talking to each other, so ensure the whole class to be coherent. The average link, of course is in between and group decision, so it's going to be insensitive to outliers. In practice, which one is the best? Well, this will depend on the application and sometimes you need a loose classes and to aggressively on cluster objects together. Then maybe simple English good. But other times you might need a tight clusters, then completely completely better, but in general you have to empirically evaluate these methods for your application to know which one is better. Now let's look at another example of method for similarity based classroom in this case. Which is called K means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. Now we're going to start with some tentative clustering result by just selecting Kate randomly selected vectors as centroids of K clusters and treat them as sentence as they represent each cluster. So this is. This gives us the initial tentative classroom. And then we're going to iteratively improve it, and the process goes like this. And once we have these Central Eastside, we're going to assign a vector to the cluster hosts entry that is closest to the current vector. So basically we're going to measure the distance between this vector and each of the centroids, and see which one is closest to this one, and then just put this class this object into that cluster. Now this is to have tentative. Assignment of objects into clusters and we're going to partition or the objects into K clusters based on our tentative clustering centroids. And then we're going to recovery, compute the centroid based on the allocated objects in each cluster. And this is. To adjust the centroid and then we had repeated this process until the similarity based on objective function. In this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective function. Given K clusters. So it can be also shown this process will converge to a local minimum. I think about this process for a moment. It might remind you the EM algorithm for mixture model. Indeed, this algorithm is very similar to the EM algorithm for the mixture model for clustering. So more specifically, we also initialize these. Predators in the EM algorithm, so the random inner inner initialization is similar. And then in the EML with them, you may recall that we're going to repeat eastep and M step to improved our primary destinations. In this case, we're going to improve the clustering result iteratively by also doing 2 steps, and in fact the two steps are very similar to EM algorithm. In that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. So it's essentially similar to eastep. Also, what's the difference? While the differences here, we don't make a probabilistic on location as in the case of the step. But rather we make a choice. We're going to make a call if this data point is closest to cluster two that were going to say you are in class too. So there's no choice, and we're not going to say you are 70% belonging to class too. And so we're not going to have a probability, but we're going to just put one object into precisely one cluster. In the E step, however, we do a probabilistic location, so we split the counts. And we're not going to say exactly which distribution has been used to generate the data point. Now the next we're going to adjust the centroid, and this is very similar to M step where we re estimate the parameters. That's when we'll have a better estimate of the parameter. So here we have a better clustering result by adjusting the centroid. And note that the central is adjusted based on the average of the vectors in the. A cluster, so this is also similar to the M step, where we do counts pull together counter and normalize them, or the difference of course is also because of the difference in the instep, and we're not going to consider probabilities when we count the points in this case, for K means we're going to only count the objects allocated to this cluster, and this is only a subset of data points. But in the EM algorithm, we in principle consider all the data points. Based on probabilistic allocations. But in nature they are very similar and that's why it's also maximizing where defined objective function and it's guaranteed to convert converted local minimum. So to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. And here we use is implicitly similarity function. To define the clustering bias, there's no explicit definer similarity function. The model defines clustering bias. And the clustering structure is built into a generated model. That's why we can use potentially a different model to recover different instruction. Complex generative models can be used to discover complex clustering structures. We did not talk about it, but we can easily design generated model to generate a hierarchical clusters. We can also use prior to further customize clustering algorithm to for example, control the topic of 1 cluster or multiple clusters. However, one disadvantage of this approach is that there is no easy way to direct or control the similarity measure. Sometimes we want to do that, but it's very hard to inject the such a explicit definition of similarity into such a model. We also talked about the similarity based approaches. These approaches are more flexible. Directly specify similarity functions. But one potential disadvantage is that their object function is not always very clear. The K means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach. The hierarchical clustering algorithm, on the other hand, is. It's harder to. To specify the objective function so it's not clear what exactly is being optimized. Both approaches can and generate the term clusters and document clusters. An term clusters can be in general generated by representing each term with some text content. For example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. And then we can certainly cluster terms based on actually their tax representations. Of course, term clusters can be generated by using generative models as well as we have seen.
410	5e5aba04-1aee-4c63-9ca6-f9e1d0390689	so let's plugging these smoothing methods into the ranking function to see what we will get OK this is the general smoothing sorry general ranking function for smoothing with crashing and model you have seen this before and now we have a very specific and smoothing method the james move method so now let's see what value for our sub D here an what's the value for peace obscene here so we may need to decide this in order to figure out the exact form of the ranking function and we also need to figure out of course alpha so let's see well this ratio is basically this right so here this is the probability of singled on the top and this is the probability of unseen world or in other words lemma is basically the awful here this so it's easy to see that this can be them riveting is this very simple so we can plug this into here and then here what's the value for other what do you think it'll be just lamp anne what would happen if we plug in this value here if this is number what can we say about this does it depend on the document no so it can be ignored right so we end up having this ranking function shown here and in this case you can easy to see this is precisely vector space model becaus this part is the sum over all the matched queried homes this is the element of the query about what do you think is element of the document about there well it's this so that's all of document that the element and let's further examine what's inside this is always so one plus this so it's going to be done negative log of this it's going to be at least one right and these this is a parameter so lemme biased parameter and let's look at this and this is a TF now we see very clearly this TF waiting here and the larger the company 's the higher the waiting with me we also see idea of waiting which is given by this and with your document lance normalization here so all these heuristics are captured in this formula what's interesting that we kind of have got this waiting function automatically by making various assumptions whereas in the vector space model we had to go through those heuristic a design in order to get this and in this case note that there is a specific form and wanted to see whether this form actually makes sense i saw what do you think is the denominator here this is the length of document total number of words multiplied by the probability of the world given by the collection so this actually can be interpreted as expected account of award if we're going to draw a word from the collection language model and willing to draw as many as the number of words in the document if you do that the expected account of award W would be precisely a given by this denominator so this ratio basically is comparing the actual count here the actual count of the word in the document with the expected account given by this product if the world is in fact the following the distribution in the collection this and if this counter is larger than the expected counter in this part this ratio it would be larger than one so that's actually a very interesting interpretation it's very natural and intuitively it makes a lot of sense and this is why advantage of using this kind of probabilistic reasoning well we have made it explicit assumptions and we know precisely why we have a logarithm here and why we have these probabilities are here and we also have a formula that intuitive that makes a lot of sense and those TF IDF weighting and document instrumentation let's look at the dealership price mosey is very similar to the case of james moving in this case the smoothing parameter is mule and that's different from lemma that we saw before but the format looks very similar the form of the function looks very similar so we still have linear interpolation here and when we compute this ratio while we defined that is that the ratio is equal to this now what's interesting here is that we're doing another comparison here now we're comparing the actual count which is the expected account of the war if we sample mule words according to the collection water probability so not that it's interesting we don't even see document lanthier and like it in the james morning so this of course should be plugged into this part so you might want us where is document lens interestingly the documents is here in office of the so this would be plugging this spot as a result what we get is the following function here and this is again a sum over all the match the query words and we again see the query from frequency here and you can interpret this as the enemy of a document vector but this is no longer simple dot product right be cause we have this pot and note that end is the lens of the query i so that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document but it's still it's still clear that it does documents formalisation becaus this lens is in the denominator so a longer document will have a lower weight here and we can still see it has TF here and now idea only that this time the form of the formula is different from the previous one in james smooth but intuitively is still implements TF IDF weighting and documents from addition again the form of the function is dictated by the probabilistic reasoning and assumptions that we have made now their roles disadvantages of this approach and that is this look around here that such a form of the formula would actually work well so it look about at this material function although it's TF IDF weighting and stocking with anthem relishing for example it's unclear whether we have sub near transformation unfortunately and we can see here there is a logarithm function here so we do have also it's here so we do have some linear transformation but we did not intentionally do that that means there's no guarantee that will end up in this way suppose we don't have logarithm then there's no sub linear transformation as we discussed before perhaps a formula is not going to work so well so that's example of the gap between formal model like this an the relevance that we have to model which is really subjective machine that is titled to users so it doesn't mean we cannot fix this for example imagine if we did not have this logarithm right so we can heuristically add one or we can even add a double logarithm but then it would mean that the function is no longer probably small also the concequence of the modification is no longer as predictable as what we have been doing now so that's also why for example PM twenty five remains very competitive and still open challenger how to use probabilistic model to derive better model than BM twenty five in particular how do we use query like code to derive a model and that would work consistently better than BM twenty five currently we still cannot do that studio interesting open question so to summarize spot we've talked about the two smoothing methods generate mercer which is doing fixed coefficient of linear interpolation the richard applier this is will add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient that would be larger for shorter documents in both cases we can see by using these smoothing methods we would be able to reach a retrieval function where the assumptions are clearly articulated so they're less heuristic experiment results also show that these retrieval functions also are very effective and they are compara bulto BM twenty five or pivot in advance normalization so this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design yet in the end that we naturally implement the TF IDF weighting and documents on rotation each of these functions also has precisely one smoothing parameter in this case of course we still need to set the smoothing parameter but there are also methods that can be used to estimate these parameters so overall at this shows by using probabilistic model we follow very different strategy than the vector space model yet in the end we end up with some retrieval functions that look very similar to vectors based model with some advantages in having assumptions clearly stated and then the form dictated by probabilistic model now this also concludes our discussion of the query like holder problems with model anne this recall what assumptions we have made in order to derive the functions that we have seen in this lecture well we basically have made four assumptions that i listed here the first assumption is that the relevance can be more than by the query michael and the second assumption with maddie 's query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of all the words in the query and then the third is something that we have made is if award is not seeing the document that we're going to let its probability but proportional to its probability in the collection of the smoothing was a classy language model and find that they have made one of these two assumptions about the smoothing so we either use james moving authorship rise moving if we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier fortunately the function has a nice property in that implements TF IDF weighting and documents on releasing an these functions also work very well so in that sense these functions are less heuristic compared with the vector space model and there are many extensions this basically model and you can find the discussion of them in the reference at the end of this laptop
410	60d307bb-e329-4e74-9d8a-5ffd6ceed306	in this lecture we continue the discussion of vectors why is smaller in particular we're going to talk about the TF transformation in the previous lecture we have derived TF idea of weighting formula using the vectors space model and we have assumed that this model actually works pretty well for these examples as shown on this slide except for D five which has received very high score indeed it has received the highest score amount all these documents but this document is intuitively non relevant so this is not desirable in this lecture or going to talk about how we can use TF transformation to solve this problem before we discuss the details let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high school so this is the formula and if you look at the formula careful it and you will see it involves a sum over all the match the query terms an insider some each metric query term has a particular weight and this way it is TF IDF weighting so it has an idea of component where we see two variables one is the total number of documents in the collection and that is M the other is the document frequency this is a number of documents that contain this word W the other variables in involved in the formula include the count of the query tom W in the query and the count of the word in the document if you look at this document again now it's not hard to realize that the reason why it hasn't received the highest score isba cause it has a very high count of campaign so the count of campaigning in this document is a four which is much higher than the other documents and has contributed to the high score of this document so intuitively in order lower the score for this document we need to somehow restrict the contribution of the matching of this term in the document and if you think about matching of terms in a document carefully you actually would realize we probably should know reward multiple occurrences so generously and by that i mean the first recurrence of term says a lot about the matching of this time because it goes from zero count to account of one and then increase means a lot once we see a word in the document it's very likely the document is talking about this world if we see a extra occurrence on top of the first recurrence that is to go from one to two then we also can say that well the second occurrence kind of confirmed that it's not accidental managing of the world now we are more sure that this document is talking about this world but imagine we have seen let's say fifty times of the world in the document then adding one extra occurrence is not going to tell us more about evidence 'cause we already sure that this document is about this or so if you think in this way it seems that we should restrict the contribution of high count of term and that is the idea of tearful transformation so this transformation function is going to turn the raw counts of word into a term frequency wait for the word in the document so here i show in X access roll count and Y axis i show that um frequence of weight so in the previous ranking functions we actually have implicitly used some kind of transformation so for example in the zero one bit vector representation we actually use researcher transformation function as shown here basically if the count is zero then it has zero weight otherwise it would have a way to one is a flat now what about the using term count as TF wait well that's the linear function right so it has just exactly the same weight as the count now we have just seen that this is not desirable so what we want is something like this so for example with the logarithm function we can have a sub linear transformation that looks like this and this will control the influence of really high weight because it's going to lower its inference yet it will retain the inference of small counts or we might want to even then the curve more by applying logarithm twice now people have tried all these methods and they are indeed working better than than linear form of the transformation but so far what works the best seems to be this special transformation called a BM twenty five transformation VM stands for best matching now in this transformation you can see there's a parameter came here and this K controls the upper bound of this function it's easy to see this function has upper bound because if you look at the X divided by X plus K where K is nonnegative number then the numerator will never be able to exceed the denominator so it's upper bounded by K plus one this is also difference between this transformation function and the logarithm transformation which it doesn't have upper bound uh furthermore one interesting property of this function is that as we vary K we can actually simulate different transformation functions including the two extremes that i've shown here that is a zero one bit transformation and the linear transformation so for example if we set K two zero now you can see the function value would be one so we precisely recover the zero one bit transformation if you said kate were very large number on the other hand is going to look more like the linear transformation function so in this sense this transformation is very flexible it allows us to control the shape of the transformation it also has a nice property of the upper bound and this upper bound is useful to control the inference of a particular time and so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time in other words this outbound might also ensure that all terms would be counted when we aggregate the weights to compute the score as i said this transformation function has worked well so far so to summarize this lecture the main point is that we need to do some linear T hop TF transformation and this is needed to capture the intuition of diminishing return from higher term counts it's also to avoid dominance by one single term over others this PM twenty five transmission transformation that we talked about is very interesting it's so far one of the best performing TF trans morning formation formulas it has upper bound and social robust and effective my food plucking this function into our TF IDF weighting vectors based model then we would end up having the following ranking function which has a VM twenty five TF component now this is already very close two a state of that ranking function called BM twenty five anne will discuss how we can further improve this formula in the next rapture
410	628ab0a9-bfa9-4be2-96c8-42ecabcf6816	In this lecture we're going to talk about how to instantiate vector space model so that we can get a very In this lecture, we are going to talk about how to instantiate vector space model so that we can get a very specific ranking function. So this is to continue the discussion of the vector space model, which is one particular approach to design ranking function. And we're going to talk about how we use the general framework of the vector space model as a guidance to instantiate the framework to derive a specific ranking function. And we're going to cover the simplest instantiation of the framework. So as we discussed in the previous lecture, the Vector Space model is really a framework. It didn't say. As we discussed in the previous lecture. Vector space model is really a framework. It doesn't say many things. So for example, here it shows that it did not say how we should define the dimension. It also did not say how we place a document vector in this space. It did not say how we place a query vector in this vector space. And finally, it did not say how we should measure the similarity between the query vector and the document vector. So you can imagine in order to implement this model we have to say, specifically, how we compute these vectors? What is exactly Xi and what is exactly Yi? This will determine where we place a document vector, where we place a query vector. And of course, we also need to say exactly what should be the similarity function. So if we can provide a definition of the concepts that would define the dimensions and these Xi's or Yi's, namely weights of terms for query and document, then we will be able to place document vectors and query vector in this well defined space and then if we also specify similarity function then we'll have a well defined the ranking function. So let's see how we can do that and think about the simplest instantiation. Actually, I would suggest you to pause the lecture at this point, spend a couple of minutes to think about. Suppose you are asked to implement this idea. You come up with the idea of vector space model. But you still have to figure out how to compute these vectors. Exactly how to define the similarity function? What would you do? So think for. A couple of minutes and then proceed. So let's think about the some simplest ways of instantiating this vector space model? First, how do we define dimension where the obvious choice is to use each word in our vocabulary to define the dimension? And here we show that there are N words in our vocabulary, therefore there are N dimensions. Each word defines one dimension and this is basically the bag of words representation. Now let's look at how we place vectors in this space. Again here the simplest strategy is to use a bit vector to represent both the query and a document. And that means each element Xi and Yi would be taking a value of either zero or one. When it's one, it means the corresponding word is present in the document or in query. When it's zero it's going to mean that it's absent. So you can imagine if the user types in a few words in the query, then the query vector will only have a few ones, many many zeros. The document vector in general will have more ones of course. But it will also have many zeros, since the vocabulary is generally very large. Many words don't really occur in any document. Many words will only occasionally occur in the document. A lot of words will be absent in a particular document. So now we have placed the documents and the query in the vector space. Let's look at how we measure the similarity. So a commonly used similarity measure here is dot product. The dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. So here we see that it's the product of X1 and Y1. So here. And then X2 * Y2 and then finally XN multiplied by YN and then we take a sum here. So that's the dot product. Now we can represent this in a more general way using a sum here. So this is only one of the many different ways of measuring the similarity. So now we see that we have defined the the dimensions we have defined the vectors and we have also defined the similarity function, so now we finally have the simplest of vector space model. Which is based on the bit vector represntation dot product similarity and bag of words representation. And the formula looks like this. So this is our formula, and that's actually particular retrieval function, a ranking function, right? Now we can find the implement this function using a programming language and then rank documents for query. Now at this point you should again pause the lecture. So think about how we can interpret this score. So we have gone through the process of modeling the retrieval problem. Using a vector space model and then we make assumptions about how we place vectors in the vector space and how we define the similarity. So in the end that we've got a specific retrieval function, shown here. Now the next step is to think about whether this retrieval function actually makes sense. Can we expect this function to actually perform well when we used to rank the documents for users' queries? So it's worth thinking about. What is this value that we'll calculate? So in the end we get a number, but what does this number mean? Is it meaningful? So spend a couple of minutes to think about that. And of course the general question here is: Do you believe this is a good ranking function would they actually work? So again, think about how to interpret this value. Is it actually meaningful? Does it mean something? It's related to how well the document matches the query. So in order to assess whether this simplest vector space model actually works well, let's look at the example. So here I show some sample documents and a simple query. The query is news about the presidential campaign and we have 5 documents here. They cover different terms in the query. And if you look at the these documents for a moment, you may realize that some documents are probably relevant and some others are probably non relevant. Now, if I ask you to rank these documents how would you rank them? This is basically our ideal ranking: when humans can examine the documents and then try to rank them. So think for a moment and take a look at this slide and perhaps by pausing the lecture. So I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well. They match news, presidential and campaign. So it looks like these documents are probably better than the others, so they should be ranked on top. An the other three D2, D1, and D5 are really non relevant, so we can also say differently. D4 and D3 are relevant documents and D1, D2, and D5 are non relevant. So now let's see if our simplest vector space model could do the same or could do something closer. So let's first think about how we actually use this model to score documents. Right here I show 2 documents D1 and D3 and we have the query also here. In the vectors space model of course we want to 1st compute the vectors for these documents and the query. Now I show the vocabulary here as well, so these are the N dimensions that will be thinking about. So what do you think is the vector representation for the query? Note that we are assuming that we only use zero and one to indicate whether the term is absent or present in the query or in the document, so these are 0/1 bit vectors. So what do you think is the query vector? The query has four words here, so for these four words there will be one and for the rest will be 0. Now, what about the documents? It's the same, so T1 has two words news and about. So there are two ones here and the rest of zeros. Similarly. So. Now that we have the two vectors. Let's compute the similarity. And we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements, right? So these two will be. forming, forming product and these two will generate another product and these two will generate yet another product and so on so forth. Now you can easy to see if we do that. We actually don't have to care about. These zeros. Because if whenever we have zero the product will be 0. So when we take a sum over all these pairs then the zero entries will be gone. As long as you have 1 zero then the product will be 0. So in effect we're just counting how many Pairs of one and one, but in this case we have seen two, so the result would be two. So what does that mean? Well, that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document. Because if a document, if a term is matched in the document, then there will be 2 ones. If it's not then there will be 0 on the document side. Similarly, if the document has a term, but the term is not in the query, there will be a zero in the query vector, so those don't count. So as a result this scoring function basically matches how many unique query terms are matched in the document. This is how we interpret this score. Now we can also take a look at the D3. In this case you can see the result is 3 because D3 matched three distinct query words: news, presidential, campaign. Whereas D1 only match two. Now in this case, it seems reasonable to rank D3 on top of D1 and this simplest vector space model indeed does that, so that looks pretty good. However, if we examine this model in detail, we likely will find some problems. So here I'm going to show all the scores for these five documents. And you can easily verify their correct because we're basically counting the number of unique query terms matched in each document. Now note that this matching actually that makes sense, right? It basically means if a document matches more unique query terms then the document will be assumed to be more relevant, and that seems to make sense. The only problem is here we can notice that there are three documents D2, D3, and D4 and they tied with a 3 as a score. So that's a problem, because if you look at them carefully, it seems that D4 should be ranked above D3 because D3 only mentioned presidential once, but D4 mentioned it multiple times. In the case of D3 presidential, could be an extended matching. But D4 is clearly about the presidential campaign. Another problem is that D2 and D3 also have the same score. But if you look at the three words that are matched in the case of D2 it matched the news, about, and campaign. But in the case of D3, it matched news, presidential, and campaign. So intuitively D3 is better because matching presidential is more important than matching about even though about and presidential above in the query. So intuitively, we would like D3 we ranked above D2. But this model doesn't do that. So that means this model is still not good enough. We have to solve these problems. To summarize, in this lecture we talked about how to instantiate a vector space model. We may need to do three things. One is to define the dimension. The second is to decide how to place documents as vectors in the vector space. And to also place a query in the vector space as a vector. And 3rd is to define the similarity between two vectors, particularly the query vector and the document vector. We also talk about a very simple way to instantiate vector space model. Indeed, that's probably the simplest vector space model that we can derive. In this case, we use each word to define a dimension. When user 0/1 bit vector to represent a document or a query. In this case, we basically only care about word presence or absence. We ignore the frequency. And we use the dot product as the similarity function. And with such a instantiation, and we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document. We also show that such a such simple vector space model still doesn't work well and we need to improve it. And this is a topic that we're going to cover in the next lecture.
410	6382e23f-d54e-4ece-a231-8df819983fb5	This lecture is continued discussion of evaluation of textual categorisation. Earlier we have introduced measures that can be used to compute the precision and recall for each category and each document. Now in this lecture we're going to do further  examine how to combine the performance on these different categories or different documents. How do we aggregate them? How do we take average? You see on the title here, I indicated it's called a macro average and this is in contrast to micro average that will talk more about that later. So. Again, for each category, we can compute the precision recall and F1 so for example, for category C one. We have precision  P1 recall R1 and F value F1 and similarly we can do that for Category 2 and all the other categories. Once we compute that, then we can aggregate them. So for example, we can aggregate all the precision values for all the categories to compute the overall precision and this is often very useful. To summarize what we have seen in the whole data set and the aggregation can be done in many different ways. Again, as I said, in case when you need to to aggregate different values. It's always good to think about what's the best way of doing the aggregation. For example, you can consider arithmetic mean, which is very commonly used. Or you can use geometric mean which would have different behavior depending on the way you aggregate. You might have got different conclusions. In terms of which method works better, so it's important to consider these differences and choosing the right one or more suitable one for your task. So the difference, for example between arithmetic mean and geometric mean is that the arithmetic mean would be dominated by high values, whereas geometric mean would be more affected by low values, and so whether you want to emphasize low values or high values would be a question related to your application. And similar we can do that for recall and F score, so that's how we can then generate the overall precision, recall and F score. Now we can do the same for aggregation over all the documents, right? So it's exactly the same situation for each document or computer precision. Recall and F. And then after we have completed the computations for all these documents we were going to aggregate them to generate the overall precision, overall recall and overall F score. These are again examining the results from different angles and which one is more useful would depend on your application. In general, it's beneficial to look at the results from all these perspectives, and especially if you compare different methods in different dimensions. It might reveal which method is better, in which measure or in what situations, and this provides insight for understanding the strength of a method or weakness, and this provides further insight for improving them. So as I mentioned, there is also micro averaging in contrast to the macro average that we talked about earlier. In this case, what we do is to pull together all the decisions. An then compute the precision and recall. So we can compute the overall precision and recall by just counting how many cases are in true positive, how many cases in false positive, etc. Basically computing the values to fill in this contingency table and then we can compute precision recall just once. Now, in contrast, in macro averaging we're going to do that for each category 1st and then aggregate over these categories. Or we do that for each document and then aggregate over all the documents. But here we pulled them together. Now this will be very similar to the classification accuracy that we introduced earlier, and one problem here of course, is to treat all the instances, all the decisions equally. And, this may not be desirable. But it may be appropriate for some applications, especially if we associate, for example, the cost for each combination. Then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision. So there could be variations of these methods that would be more useful, but in general macro average tends to be more informative than micro averaging just because it might reflect the need for understanding performance on each category or performance on each document which are needed in many applications. But the macro averaging and micro averaging, they're both very common and you might see both reported in research papers on text categorisation. Also, sometimes categorisation results might actually be evaluated from ranking perspective. Now this is because. Categorisation results are sometimes or often indeed passed to human for various purposes. For example, it might be passed to humans for further editing. For example, news articles can be tentatively categorized by using the system and then human editors would then correct them. And all the email messages might be routed to the right person for handling in the help desk, and in such a case the categorizations do help prioritizing the task for a particular customer service person. So in this case, the results have to be prioritized. And if the system can give a score to the categorisation decision or confidence, then we can use the scores to rank these decisions and then evaluate the results as a ranked list, just as in search engine evaluation, where you rank the documents in response to the query. So for example, discovery of spam emails can be evaluated, based on ranking emails for the spam category and this is useful if you want people to verify whether this is really spam, right? The person would then take the ranked list to check one by one and then verify whether this is indeed a spam. So to reflect the utility for humans in such a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again. And in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. So for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. But typically we frame this as a ranking problem and we evaluated as a ranked list. That's be cause people tend to examine the results sequentially, so ranking evaluation more reflects the utility from users perspective. So, to summarize, categorization evaluation, first evaluation is always very important for all these tasks, so get it right. If you don't get it right, you might get misleading results an you might be misled to believe one method is better than the other, which is in fact not true. So it's very important to get it right. Measures must also reflect the intended use of the results for particular application. For example, in spam filtering and news categorization results are used in maybe  different ways. So then we would need to consider the difference and design measures appropriately. We generally need to consider how will the results be further processed by a user and then think from a user's perspective what quality is important. What aspect of quality is important. Sometimes there are tradeoffs between multiple aspects, like precision and recall, and then, so we need to know for this application is high recall more important or high precision is more important. Ideally we associate the different cost with each different decision error and this of course has to be designed in application specific away. Some commonly used measures for relative comparison of different methods or the following classification accuracy is very commonly used for especially balanced tester set. Precision, recall, and F scores are commonly reported to characterize the performances in different angles, and there are some also variations like per document based evaluation, per category evaluation and then take average of all of them in different ways. Micro versus macro averaging. In general, you want to look at the results from multiple perspectives and for particular application in some perspectives would be more important than others, but for diagnosis, analysis of categorization methods and it's generally useful to look at as many perspectives as possible to see subtle differences between methods or to see where a method might be weak, from which you can obtain insights for improving a method. Finally, sometimes ranking may be more appropriate, so be careful. Sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well. So here are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures. The second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.
410	64839f96-5182-452f-ae9f-1fa3e52fce70	so we talked about page rank as a way to capture the authorities now we also looked at some other examples where a hub might be interesting so there is another algorithm called hits and that's going to compute the scores for authorities and hubs the intuitions are pages that are wider site could sort of this then whereas pages at the site many other pages are good apps but i think the most interesting idea of this algorithm hits is it's going to use reinforcement mechanism for kind of help improve the scoring for haps and the authorities an here so here's the idea it will assume that could authorities are cited by could hubs that means if you're cited by many pages with good hub scores then that increases your authority score and similarly could helps all those that pointed to could authorities' so if you get you pointed to a lot of good authoritie pages then your help score will be increased so then we can iteratively reinforce each other BIH cause you can point to some good hubs also that you can point to some good authorities to get a good hub score whereas those also the scores would be also improved becaus they are pointed to by a good hub and this algorithm is also general it can have many applications in graphene network analysis so just briefly here's how it works with first also construct the metrics but this time we're going to construct the adjacent symmetrics and we're not going to normalize the values so if there's a link there is one if there's no link that's zero again it's the same graph and then we're going to define the hub score of page as the sum of the authorities scores of all the pages that it points to so whether you are happy it really depends on whether you're pointing to a lot of good authoritie pages that's what it says in the first equation in the second equation we define the authorities' score of a page as a sum of the hub scores of all those pages that appointed to you so whether you are good authors that would depend on whether those pages that are pointing you are good house so you can see this forms iterative reinforcement mechanism now these two equations can be also return in the metrics of all format so what we get here is then the hub vector is equal to the product of the edges and the metrics and the authoritie vector and this is basically the first equation a similar the second question can be returned as the authoritie vector is equal to the product of a transpose multiplied by the top vector and these are just different ways of expressing these equations but was interesting that if you look at the metrics form you can also plug in the authority equation into the first one so if you do that you have children that in limited authoritie raptor completely and you get the equation of only hub scores the hub score vector is equal to a multiplied by a transpose multiplied by the hub score vector again and similarly we can do a transformation to have equation for just the authorities calls so although with frame with a problem as computing haxan authorities' week after enumerate the one of them to obtain equation just for one of them now the difference between this end page ranger is that not the matrix is actually a multiplication of the edges in the metrics and its transfers so this is different from page rank but mathematics voted then we will be computing the same problem so it hits with typical will initialize the values let's say one for all these values and then with the iterative the apply these these equations essentially this is equivalent to multiply that by the metrics and a transpose and so the algorithm is exactly the same debate rank but here be cause the edges in the metrics is not normalized so what we have to do what we have to do is after each iteration working to normalize and this would allow us to control the growth of value otherwise there were the grew larger and larger and if we do that and then we're basically get hits algorithm to compute the hub scores and also this calls for all the pages and these are scores can then be use ranching just like a pager in schools so to summarize in this lecture we have seen that link information is very useful in particular the anchor text is very useful to increase the the text representation of a page and will spoke about the page rank and hits art and has two major link analysis algorithms both can generate the scores for web pages that can be used in the renton function loads that page rank and it's also very general algorithms so they have many applications in analyzing other graphs or networks
410	6691b6ff-51d5-4d89-8bc8-bd882398be6a	this lecture is about natural language of content (test 51) analysis as you see from this picture this is really the first step to process any text today to text data are in natural languages so computers have to natural language to some extent in order to make use of the data so that's the topic of this laptop we're going to cover three things first what is natural language processing which is the main technique for processing natural language to obtain understanding the second is the state of lot of NLP which is stands for natural language processing finally we're going to cover the relation between natural language processing and the texture retrieval first what is NLP well the best way to explain it is to think about if you see a text in a foreign language that you can understand now what you have to do in order to understand that text this is basically what computers are phasing so looking at the simple sentence like a dog is chasing a boy on the playground we don't have any problem with understanding this sentence but imagine what the computer would have to do in order to understand it well in general it would have to do the following first it will have to know doggies are now chasing the verb etc so this is the code lexical analysis or part of speech tagging and we need to figure out the syntactic categories of those words so that's the first step after that we're going to figure out the structure of the centers so for example here it shows that a anna dog would go together to form a noun phrase and we won't have dogan ease to go first and there are some structures that are not just right but this structure shows what we might get if we look at the sentence and try to interpret the sentence some words would go together first and then they will go together with other words so here we show we have non phrases as intermediate components and then verbal phrases finally we have a sentence and you get this structure we need to do something called a syntactic analysis or pausing and we may have a pazer computer program that would automatically create this structure at this point that you would know the structure of this sentence but still you don't know the meaning of the sentence so we have to go further to semantic analysis in our mind we usually can map such a sentence to what we already know in our knowledge base and for example you might imagine a dog that looks like that there's a boy and there's some activity here but for computer would have to use symbols denote that so we would use a symbol T one to denote a dog and P want to denote a boy and then P one to know the playground playground now there is also chasing activity that's happening here so we have a relation chasing here that connects all these symbols so this is how computer would obtain some understanding of this sentence now from this representation we could also further infer some other things and we might indeed naturally think of something else when we read the text and this is called inference so for example if you believe that if someone is being chased and this person might be scared with this rule you can see computers could also infer that this boy may be scaled so this is some extra knowledge that you would infer based on understanding of the text you can even go further to understand why the person said this sentence so this has to do with the use of language this is called pragmatic analysis in order to understand the speech actor of a sentence we say something too basically achieve some goal there's some purpose there and this has to do with the use of language in this case the person who said this sentence might be reminding another person to bring back the dog that could be one possible intent to reach this level of understanding would require all these steps and a computer would have to go through all these steps in order to completely understand this sentence yet we humans have no trouble with understand that we instantly we get everything and there is a reason for that that's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence Computers unfortunately are hard to obtain such understanding they don't have such a knowledge base there are still incapable of doing reasoning under uncertainties so that makes natural language processing difficult for computers but the fundamental reason why a natural language processing is difficult for computers is simply becaus natural language it has not been designed for computers the natural languages are designed for us to communicate there are other than we just design a full computers for example programming languages those are harder for us so natural languages is designed to make our communication efficient as a result we admit a lot of common sense knowledge becaus we assume everyone knows about that we also keep a lot of ambiguities becaus we assume the receiver or the hero could no how to decembrie good ambiguous word based on the knowledge or the complex there's no need to invent the different words for different meanings we could overload the same words with different meanings without the problem because of these reasons this makes every step in natural language processing difficulty for computers and B guild is the main difficulty and common sense reasoning is often required that's also hard so let me give you some examples of challenges here consider the word level ambiguity the same word came different syntactic categories for example design can be a noun or a verb the war route may have multiple meanings so square root in math sense or the root of a plant you might be able to think of other meanings there also syntactical ambiguities for example the main topic of this lecture natural language processing cap should be interpreted in two ways in terms of the structure think for a moment to see if you can figure that out we usually think of this as processing of natural language but you could also think of this as you say language processes natural so this is example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words another common example of ambiguous sentence is the following a man so a boy with the telescope now in this case the question is who had the telescope right this is hold a prepositional phrase attachment and ability or PP attachment ambiguity not we generally don't have a problem with these ambiguities becaus we have a lot of background knowledge to help us disambiguate the ambiguity another example of difficulties anaphora resolution so think about the sentence like a jam persuaded appeal to buy a TV for himself the question here is does himself refer to jam or bill so again this is something that you have to use some background or the context to figure out finally presupposition is another problem consider the sentence he has quit smoking now this obviously implies that he smoked before so imagine a computer wants to understand all these subtle differences and meanings it would have to use a lot of knowledge to fix that loud it also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world so this is why it's very difficult so as a result we are step not perfect in fact the far from perfect in understanding natural language using computers so this slide sort of gives simplified view of state of large technologies we can do part of speech tagging pretty well so i showed that ninety seven percent accuracy here now this number is obviously based on a certain data set so don't take this literally this just shows that we can do it pretty well but it's still not perfect in terms of pausing we can do partial pausing for the world that means we can get noun phrase structures or verbal phrases structure or some segment of the sentence and this dude correctly in terms of the structure an in some evaluation results we have seen about ninety percent accuracy in terms of partial pausing of sentences again i have to say these numbers are relative to the data set in some other data sets the numbers might be lower most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data think about the social media data the accuracy likely is lower in terms of semantic analysis we are far from being able to do a complete understanding of a sentence but we have some techniques that would allow us to do partial understanding of the sentence so i could mention some of them for example we have techniques that can allow us to extract the entities and relations mentioning tax articles for example recognizing the mentions of people locations organisations etc in text so this is called entity extraction we may be able to recognize the relations for example this person visited that place or this person met that person or this company acquired another company such relations can be extracted by using the current that natural language processing techniques they're not perfect but they can do well for some men today's so many days are harder than others we can also do word sense disambiguation to some extent we can figure out the weather this word in this sentence would have sort of meaning in another context the computer could figure out it has a different meaning again it's not perfect but you can do something in that direction we can also do sentiment analysis meaning to figure out the weather sentence is positive or negative this is especially useful for review analysis for example so these are examples of semantic analysis and they help us to obtain partial understanding of the sentences it's not giving us a complete understanding as i showed it before for this sentence but it would still help us gain understanding of the content and these can be useful in terms of inference we are not there yet probably be cause of the general difficulty of inference and uncertainties this is a general challenger in artificial intelligence that partly also be cause we don't have complete semantical representation for natural language text so this is hard yet in some domains perhaps in limited domains when you have a lot of restrictions on the world uses you maybe do may be able to perform inference to some extent but in general we cannot really do that reliable speech act analysis is also far from being down and we can only do that analysis for various special cases so this roughly gives you some idea about the state of the art and then we also talk a little bit about what we can't do and so we can't even do one hundred percent part of speech tagging now this looks like a simple task but think about the example here the two uses of off may have different syntactic categories if you try to make a fine grained distinctions it's not that easy to figure out the such differences it's also hard to do general complete pausing and again this same sentence that you saw before is example this ambiguity can be very hard to disambiguate and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope so although the sentence looks very simple it actually is pretty hard and in cases when the sentence is very long imagine it has four or five prepositional phrases and there are even more possibilities to figure out it's also harder to do precise deep semantic analysis so here's example in the sentence journal owns a restaurant how do we define owns exactly the word is something that we understand but it's very hard to precisely describe the meaning of for computers so as a result we have robust and general natural language processing techniques that can process a lot of text data in a shallow way meaning we only do superficial analysis for example a policy of speech tagging or party or passing or recognizing sentiment and those are not deep understanding be cause we're not really understanding the exact the meaning of a sentence on the other hand of the deep understanding techniques ten not to scale up a well meaning that they would fail on some unrestricted a text ann if you don't restrict the text domain or the use of words then these techniques tend not to work well they may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on but the general wouldn't work well on the there are that are very different from the training data so this pretty much summarizes the state of the art of natural language processing of course within such a short amount of time we can really give you complete view of NLP which is big field an either expect that to to see multiple causes on natural language processing topic itself but becaus of its relevance to the topic of we talk about it's useful for you know the background in case you haven't been exposed to that so what does that mean for text retrieval well in text retrieval we're dealing with all kinds of text it's very hard to restrict the texture to a certain domain and we also often dealing with a lot of text data so that means the NLP techniques must be general robust and efficient and that just implies today we can only use fairly shallow NLP techniques for text retrieval in fact most search engines do they use something called a bag of words representation now this is probably the simplest representation you can possibly think of that is the current text data into simply a bag of words meaning we will keep individual words but we'll ignore all the orders of words and we'll keep duplicated occurrences of words so this is called a bag of words repent nation when you represent the text in this way you ignore a lot of other information an that just makes it harder to understand the exact the meaning of a sentence becaus we've lost the order but yet this replenishing tends to after the work pretty well for most search tasks and this is partly be cause the search taskbar is not all that difficult if you see matching of some of the query words in a text document changes all that that document is about the topic although there are exceptions so in comparison some other tasks for example machine translation would require you to understand the language accurately otherwise the translation would be wrong so in comparison search task is all relatively easy such a representation is often sufficient and that's also their condition that the major search engines today like a google or bing are using of course i put in princess here but not all of course there are many queries that are not answered away by the current search engines and they do require representation that would go beyond bag of words representation that would require more natural language processing to be done there is another reason why we have not used the sophisticated NLP techniques in modern search engines and let's be cause some retrieval techniques actually naturally solve the problem of NLP so one example is water sense disambiguation think about the world like java it could mean coffee or could mean program language if you look at the world alone it will be ambiguous but when the user uses the word in the query usually there are other words for example i'm looking for usage of java applet when i have applet there that implies java means programming language and that context can help us naturally prefer documents where java is referring to program language becaus those documents would probably match app late as well if java a cursing that documented way the means coffee then you would never match applet with various more probability so this is the case when some retrieval techniques naturally achieve the goal of water sense disambiguation another example is some technique called feedback we will talk about later in some of the lectures this technique technique it would allow us to add additional words to the query and those additional words could be related to the query words and these words can't help matching documents where the original query words have not occured so this achieves to some extent semantic matching of terms so those techniques also helped us bypass some of the difficulties in natural language processing however in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines and it's particularly therefore complex search tasks or for question answering google has recently launched knowledge graph and this is one step toward that goal becaus knowledge graph would contain entities and their relations and this goes beyond the simple bag of words representation and such techniques should help us improve the search engine utility significantly although this is there open topic of all research and exploration in some in this actual we've talked about what is NLP ann we've talked about the state of the art techniques what we can do what we cannot do and finally we also explain the white bag of words representation remains the dominant replantation used in modern search engines even though deeper NLP would be needed for future search engines if you want to know more you can take a look at some additional readings i only cited one here and that's a good starting point thanks
410	675158b2-3979-4126-a098-e4d7a73c11e2	so let's take a look at this in detail so in this random surfing model at any page would assume random sofa would choose the next page to visit so this is a small graph here that's of course over simplification of the complicated web but let's say there are four documents here I T one T two T three and T four and let's assume that a random server or random walker can be on any of these pages and then the random suffer could decide to just randomly jumping to any page or follow a link and then visited the next page so if the random server is at A T one then and with some probability that random so far will follow the links now there are two out links here one is pointing to the three the other is pointing to default so the random so far could pick any of these two to reach E three and E four but it also assumes that the random sofa might get bored sometimes so the random server would decide to ignore the actual links and simply randomly jump to any page on the web so if it does that would be able to reach any of the other pages even though there's no link directly from the wild to that page so this is assumed random surfing model imagine and random server is really doing surfing like this then we can ask the question how likely on average the server would actually reach particular page in fact D one or D two or D three that's the average probability of visiting a particular page and this probability is precisely what page ranger computes so the page rank score of the document that is the average probability that the sofa visits a particular page now intuitively this would basically capture the in link account why becaus if a page has a lot of inlinks then it would have a higher chance of being visited because there will be more opportunities of having the sofa the following link to come to this page and this is why the random surfing model actually captures the idea of counting the inlinks loads that it also considers the interacting links why becaus if the pages that point too you have themselves a lot of innings that with me the random server we are very likely reach one of them and therefore it increases the chance of visiting you so this is a nice way to capture both indirect and direct links so mathematically how can we compute this probability in order to see that we need to take a look at how this probability is computed so first let's take a look at the transition matrix here and this is just the metrics with values indicating how likely the random server will go from one page to another so each row stands for a starting page for example low one would indicate the probability of going to any other four pages from he won and here we see there are only nine two nine zero entries each is one over two half so this is be cause if you look at the graph D Y is pointing to the three and D four is no link from D one to D one cell for D two so we've got zeros for the first of two columns and point five four E three and E four in general the element in this metrics M sub idea is the probability of going from D I two DJ an obviously for each rule the values should sum to one becaus the server would have to go to precisely one of these other pages so this is a transition matrix now how can we compute the probability of a surfer visiting a page well if you look at the surf model then basically we can compute the probability of reaching a page as follows so here on the left hand side you see it's the probability of visiting page DJ at time T plus one so it's the next time point on the right hand side you can see the equation involves the probability of at page D I at time T so you can see the subscript index T here and that indicates that the probability that the server was at a document at time T so the equation basically captures the two possibilities of reaching a DJ at the time three plus one what are these two possibilities but one is through random surfing and one is through following a link as we just explained so the first part captures the probability that the random server would reach this page by following a link and you can see the random sofa chooses this strategy with probability one minus R file as we assume and so there is a factor of one minus are here but the main part is really some over all the possible pages that server could have been at time T there are N pages so it's a sum over all the possible in pages inside the sum is a product of two probabilities one is the probability that the server was at the at the time T that's P sub T of the eye the other is the transition probability from D I two DJ and so in order to reach this DJ page the server must first be at D I at the time team and then also would have to follow the link to go from D I to DJ so the probability is the probability of being at D I at time T multiplied by the probability of going from that page to the target page DJ here the second pod is a similar sound the only difference is that now the transition probability is a uniform transition probability of one over N and this pilot captures the probability of reaching this page through random jumpy right so the form is exactly the same and this also allows us to see why pagerank essentially assumed smoothing of the transition matrix if you think about this one over N as coming from another transition matrix that has all the elements being one over N the uniform metrics then you can see very clearly essentially we can merge the two parts becaus they all the same form we can imagine there's a different metrics that's a combination of this M and that uniform matrix where every element is one over N and in this sense page rank it uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix now of course this is time dependent calculation of the probabilities now we can imagine if we're on the computer average probabilities the average probability is properly with the steps file this equation without considering the time index so let's drop the time index and just assume that there will be equal now this would give us any equations becaus for each page we have such equation and if you look at the what variables we have in these equations there also precisely N variables right so this basically means we now have a system of any equations within variables and these are linear equations so basically now the problem boils down to solve this system of equations and here i also show the equations in the metrics form it's the vector P here equals a metrics well the transpose of the metrics here and multiply by the vector again now if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for eigenvector right when you multiply the metrics by this vector you get the same value as this vector and this can be solved by using iterative algorithm so the question is here on the bob basically taken from the previous slide so you see the relationship between the PG rating scores of different pages an in this iterative approach or power approach we simply start with randomly initialized the vector P and then we repeatedly just updated this P by multiplying the metrics here by this P vector so i also show a concrete are example here so you can see this now if we assume are far is point two then with the example that we show here on this slide we have the original transition matrix here by that in croel that encodes the graph the actual links and we have this is smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with the linear interpolation to form another metrics that would be like this so essentially we can imagine now the web it looks like this can be captured by that there are virtual links between all the pages now so the page ranking algorithm would just initialize the P vector first and then just computed the updating of this P vector by using this metrics multiplication now if you rewrite this metrics model multiplication in terms of just individual questions you will see this and this is basically the updating formula for this particular page is page rank score so you can also see you if you want to compute the value of this updated score for D one you basically multiply this rule right by this column and we take the dot product of the two that will give us the value for this value so this is how we updated the vector we started with some initial values for these guys for for this and then we just to revise the scores which generate a new set of scores and the updating formula is this one so we just repeatedly apply this and here it converged and when the metrics is like this where there's no zero values and it can be guaranteed to converge and at that point that we were just have the page rank scores for all the pages now we typically set the initial values just to one over N so interestingly this updating formula can be also interpreter as propagating scores over the graph can you see why well if you look at this formula and then compare that with this graph and can you imagine how we might be able to interpret this as essentially propagating scores over the graph i hope you will see that indeed that we can imagine we have values initializer on each of these pages so we can have values here let's say that's one over four for each and then we're going to use this metrics to update this this calls and if you look at the equation here this one basically we're going to combine the scores of the pages that possibly would lead through reaching this page so will look at all the pages that are pointing to this page and then combine their schools and propagate the score some of the scores to this document D one so we look at the scores that represent the probability that the random server would be visiting the other pages before it reaches the D one and then just do the propagation to simulate the probability of reaching this page D one so there are two interpretations one is just the metrics multiplication repeated multiply vector by this matrix the other is to just think of it as propagating the scores repeatedly on the web so in practice the computation of pagerank score is actually efficient becaus the metrics is a sparse and there are some ways we transform the question so that you avoid actually literally computing the values for all those elements sometimes you may also normalize equation and that would give you a somewhat different form of the equation but then the ranking of pages will not change the results of this potential problem of zero out link problem in that case if the a page does not have any out link then the probability of these pages would not something that basically the probability of reaching the next page from this page will not someone mainly because we have lost some probability mass when we assume there's some probability that the server will try to follow links but then there's no link follow and one possible solution is simply to use a PP specific attempting factor and that could easy to fix this basically that's to say awful would be one point zero four page with no out linger in that case the server with just have to randomly jump through another page instead of trying to follow a link so there are many extensions of page rank one extension is topic specific page rank noted that page rank it doesn't really use the query information so we can make page rank query specific however so for example in the topic specific page rank we can simply assume when the server is board the server is not going to randomly jump to any page on the web instead it's going to jump to only those pages that are relevant to a query for example if the queries about the sports then we could assume that when it's doing random jumping is going to randomly jump to a sports page by doing this then we can bias in the page rank to topic line sports and then if you know the current query is about disposal anyone use this specialized for page rank score to wrangle documents that would be better than if you use the generic page rank so page rank is also a general algorithm that can be used in many other applications for network analysis particular example of social networks you can imagine if you compute the page rank scores for social network where link might indicate a friendship relation you will get some meaningful scores for people
410	68b9ad8e-db63-49c2-b060-46046c7dfac0	this latter is about the implementation of text retrieval systems in this lecture we will discuss how we can implement a text retrieval method to build a search engine the main channing is to manage a lot of the test data and to enable a query to be answered very quickly and to respond too many queries this is a typical text retrieval system architecture we can see the documents are first processor by a tokenizer to get tokenized units for example words and then these words or tokens will be processed by a indexer that would create index which is a data structure for the search engine to use to quickly ends or query and the query will be going through a similar process step so that organizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other an the queries representation would then be given to the scorer which would use index too quickly and so users query by scoring the documents and then ranking them results will be given to the user and then the user can look at the results and provide some feedback that can be expressed with judgments about which documents are good which documents are bad or implicit feedback such as click slows so user doesn't have to do any anything extra the user would just look at the results an skip some and click on some results to view so these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skip the ones so a search engine system then can be divided into three parts the first part is the indexer and the second part is a scorer that response to the users query in the third parties of feedback mechanism now typically the index is down in the offline manner so you can preprocess the collected data and to build the inverted index which will introduce in a moment and this data structure can then be used by the online module which is a scorer two processor users query dynamically quickly generate search results the feedback mechanism can be done online or offline depending on the method the implementation of the index and the score is a fairly standard and this is the main topic of this lecture and the next few lectures the feedback mechanism on the other hand has variations it depends on which method is used so that is usually down in algorithm specific away let's first talk about that at night position is the normalized lexical units into the same form so that semantically similar words can be matched with each other our in the language like english stemming is often use and this is what map all the inflectional forms of words into the same root form so for example computer computation in computing can all be matched to root form compute this way all these different forms of computing can be matched with each other and normally this is good idea to increase the coverage of documents that are matched with this query but it's also not always beneficial because sometimes the settle this difference between computer and computation might still suggest that the difference in the coverage of the content but in most cases stemming since will be beneficial when we tokenize the texting some other languages for example chinese we might face some special challenges in segmenting the text to find the water boundaries because it's not obvious where the boundary is as there's no space to separate them so here of course we have to use some languages specifically natural language processing techniques once we do tokenization then we would index the text documents and that is it'll convert the documents into some there are structure that can enable fast search the basic idea is to precompute as much as we can basically so the most commonly used indexes called inverted index and this has been used to in many search engines to support basically search algorithms sometimes other indexes for example a document in that might be needed in order to support feedback like i said and this this kind of techniques are not really standard in that they vary a lot according to feedback methods to understand why we want to use invert index it would be useful for you to think about how you would respond to a single term query quickly so if you want to use more time to think about that post the video so think about how you can preprocess the text there are so that you can quickly responded to a query with just one word well if you have thought about that question you might realize that or the best is to simply create a list of documents that match every term in the vocabulary in this way you can basically preconstructed benzos so when you see your term you can simply just fetch the random list of documents for that term every turn the disk to the user so that's the fastest way to respond to a single term query now the idea of the inverted index is actually basically like that we can do pre construct the such index that would allow us to quickly find the older documents that match a particular term so let's take a look at this example we have three documents here and these are the documents that you have seen in some previous lectures suppose we want to create invert index for these documents then we would maintain a dictionary in the dictionary will have one entry for each term and we're going to store some basically statistics about the term for example the number of documents that match the term or the total number of total frequency over the term which means we would counter duplicated occurrences of the term and so for example news this term occured in all the three documents so the count of documents is three and you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model can you think of that so what weighting heuristic would need this count well that's the idea right inverse document frequency so IDF is the property of the term and we can compute it right here so with the document that count here it's easy to compute the idea of either at this time when we build an index or running time when we see your query now in addition to these basically statistics we also store all the documents that match the news and these entries are store in fire called postings so in this case it meant three documents and store information about these three documents here this is the document ID document of one and the frequency is one the TF is one four news in the second document it's also well etc so from this list that we can get all the documents that match the term news and we can also know the frequency of news in these documents so if the query has just the one world news and we are easily look up this table to find the entry and go quickly through the postings perfect all the documents that matching yours so let's take a look at another term this time let's take a look at the world presidential this war occured in only one document document three so the document frequency is one but it occur to twice in this document so the frequency count is two an the frequency com is useful in some other retrieval method where we might use the frequency to assess the popularity of term in the collection and similarly will have a pointer to the postings here and in this case there is only one entry here becaus the term according just one document and that's here i talking ID is three an it could toys so this is the basic idea of inverted index it's actually pretty simple right with this structure we can easily fetch all the documents that match it on and this will be the basis for scoring documents for query now sometimes we also want to store the positions of these tones so in many of these cases the term occur just once in the document so there's only one position for example in this case but in this case the term occur twice so we would store two positions now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say five words or ten words or whether the matching of the two query terms is in fact a phrase of towards that this can we check the quickly by using the position information so why is inverted index good for faster search well we just talked about the possibility of using it to ends or single water clearly and that's very easy what about the multiple term queries well let's first look at the some special cases over the boolean query a boolean query is basically boolean expression like this so i want the relative in the document to match both term A and term be so that's one conjunctive query or i want the relevant documents to match term A or B that's a disadvantage query how can we answer such a query by using vert index well if you think a bit about it it would be obvious it cause we had simply fetch all the documents that matched ma an also fetch all the documents that match tom be and then just to take the intersection to answer query like A N B or to take the union to answer the query A or B so this is all very easy to when it's going to be very quick now what about the multi term keyword query we talked about vectors based model for example and we would match such query with document and generative score and the score is based on aggregated term weights so in this case it's not a boolean query but the scoring can be acted out in a similar way basically it's similar to disjunctive pony and query basically it's like a RV we take the union of all the documents that match it at least one query term and then we would aggregate the term weights so this is basic idea of using that index for scoring documents in general and we're going to talk about this in more detail later but for now let's just look at the question why is inverted index a good idea basically why is it more efficiently than sequential register scanning documents this is obvious approach you can just compute the score for each document and then you can scroll them sorry you can then sort them this is a straightforward method but this is going to be very slow imagine the web it has a lot of documents if you do this then it will take a long time to answer your query so the question now is why would the in the inverted index we've much faster than it has to do with the water distribution in text so here's some common phenomenon of water distribution in text there are some language independent the patterns that seem to be stable and these patterns are basically characterized by the following pattern a few words like the common words another or we occur very very frequently in text so they account for a large extent of occurrences of words but most are words would occur just rarely there are many words that occur just once let's say in the document or once in a collection there are many such singletons it's also true that the most frequent the words in one coppers layer to be raring another that means although the general phenomenon is applicable or is observed in many cases exactly words that are common may vary from context to context so this phenomena is characterized by what's called a zip of slaw this law says that the rank of word multiplied by the frequency of the world is roughly a constant so formally if we use F of WT node the frequency of W D noted the rank of world then this is the formula it basically says the same thing just mathematical term we'll see is basically a constant so 's so there is also parameter over that might be adjusted to better fit any empirical observations so if i plot the word frequencies in sorted order now you can see this more easily the X axis is basically the world rank and this is all of W and Y axis is a word frequency or F of W now this curve with shows that the product of the two is roughly the consonant now if you look at these words that we can see they can be separated into three groups in the middle it's the intermediate frequency words these words tend to occur in quite a few documents but they're not like those most frequent awards and they also not very rare so they tend to be often used in queries and they also tend to have high TF IDF whites these intermediate frequently words but if you look at the left path of the curve these are the highest frequency words they occur very frequently they are usually stopovers because the we of etc those words are very very frequently there are in fact the tool frequently to be discriminate him and they are generally not very useful for for retrieval so they are often removed and this is gotta stop awards removal so you can use pretty much just the counter words in the collection to kind of infer what words might be stopped with those are basically the heister frequency words and they also occupy a lot of space in the inverted index you can imagine the posting entries for such a world would be very long and therefore if you can remove such words you can save a lot of space in the inverted index we also show the tear apart which has a lot of rare words those was long a covered frequently and there are many such words those words are actually very useful for search also if a user happens to be interested in such a topic but be cause they're rare it's often true that the users are unnecessary interest in those words but retain them allow us to match such a document accurate and they generally have very high ID FS so what kind of data structures should we use to slow inverted index well it has two parts right if you recall we have a dictionary and we also have postings the dictionary has mostly size although for the web it's still going to be very large but computer was postings it's modest and we also need to have fast random access to the entries 'cause we want to look up with the query term very quickly so therefore we prefer to keep such a dictionary in memory if it's possible or if the connection is not very large this is visible but the connection is very large then it's in general not possible with the vocabulary size is very large obviously we can't do that so in general that's our goal so the data structures that we often used for storing dictionary it would be directly access data structures like hash table or P G if we can't store everything in memory we can use disco and but they try to build a structure that would allow you to quickly look up her entries i for postings they are huge an in general we don't have to have direct access to specific entry we generate with just look up a sequence of document i desana frequencies for all the documents that match it or query term so we were the read those entries sequential it and therefore be'cause its large and we generate store postings on disk so they have to stay on this an they would contain information such as document IDS tone frequencies or compositions etc now because they are very large compression is often desirable now this is not only to save disk space and this is of course one benefit of compression it's not an occupied that much space but it's also to help her improving speed can you see why well we know that input and output with cost a lot of time in comparison with time taken by CPU so CPU is much faster but i owe takes time so by compressing the inverted index the pulsing files will become smaller and the entries that we have to read into memory to process a query time with would be smaller and then so we can reduce the amount of traffic in I O and that can save a lot of time of course we have to then do more processing of the data when we uncompressed the data in the memory but as i say the CPU is fast so overall we can still save time so compression here is both a safe distance space and to speed up the loading of number three index
410	6962b043-7dd8-4050-bad0-bbdb13e2c302	This lecture is about how to use generative probabilistic models for text categorization. There are in general are two kinds of approaches to text categorization by using machine learning. One is generative probabilistic models, the other is discriminative approaches. In this lecture, we're going to talk about the generative models. In the next lecture, we're going to talk about discriminative approaches. So the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. Main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. In fact, that's the goal of text clustering. We want to find such clusters in the data. But in the case of categorization, we are given the categories. So we kind of have predefined categories and. then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. But because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. Or we can understand how we can use generative models to do text categorization from the perspective of clustering. And so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. Each topic is 1 cluster. So once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this question boils down to decide which theta i has been used to generate D. Suppose D has L words represent represent as Xi here. Now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? In general, we use bayes rule to make this inference. And you can see this Prior information here. That we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster, so we should favor such a cluster. The other is a likelihood part, that is this part. And this has to do with whether the topic word distribution can explain the content of this document well. And we want to pick a topic that's high by both values. So more specifically, we just multiply them together and then choose which topic has the highest product. So more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document. Get posterior becausw this one P of Theta i is the prior, that's our belief about which topic is more likely. Before we observe any document. But this conditional probability here Is the posterior probability of the topic after we have observed the document d. And Bayes rule allows us to update this probability based on the prior and I shown the details. Below here you can see how the prior here is related to the posterior on the left hand side. And this is related to how well this word distribution explains the document here, and the two are related in this way. So to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course. An we can then change the probability of document in your product of the probability of each word and that's just because we've made the assumption about the independence in generating each word OK. So this is just something that you have seen in document clustering. An we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. So this idea can be directly adapted to do categorization and This is precisely what Naive Bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if Theta i represents category I accurately that means the word distribution characterizes the content of documents in category i accurately. Then what we can do is precisely like what we did for text clustering. Namely, we are going to assign document D to the category that has the highest probability of generating this document. In other words, we're going to maximize this posterior probability as well. And this is related to the prior and the likelihood an as you have seen on the previous slide. And so naturally, we can then decompose this likelihood into a product. As you see here now here I changed the notation so that we will write down the product as product over all the words in the vocabulary and even if even though the document doesn't contain all the words and the product is there accurately representing the product of all the words in the document. because of this count here. when a word doesn't occur in the document. The count would be 0, so this count would just disappear. So effectively we're just have the product over all the words in the document. So basically with naive Bayes classifier, we're going to score each category for a document by this function. Now you may notice that here It involves the product of a lot of small probabilities and this can cause underflow problem. So one way to solve the problem is to take logarithm of this function, which doesn't change the order of these categories, but would help us preserve precision and so this is often the. This is often the function that we actually use to score each category, and then we're going to choose the category that has the highest score by this function. So this is called a Naiyes Bayes classifier. Now the keyword Bayes is understandable because we are applying a Bayes rule here. When we go from the posterior probability of the topic to a product of the likelihood and the prior. Now it's also called a Naive because We've made an assumption that every word in the document is generated independently, and this is indeed a naive assumption, because in reality they are not generated independently. Once you see some word and other words will more likely occur. For example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear And if you have not seen text. But this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. But you should know that this kind of model doesn't have to make this assumption. We could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. And of course you can even use a mixture model to model what the document looks like in each category. So in nature they will be all using Bayes rule to do classification, but the actual generative model for documents in each category. Can vary, and here we just talk about a very simple case. Perhaps the simplest case. So now the question is, how can we make sure each theta i actually represents category i accurate? Now, in clustering we learned this category i or the word distributions for category i from the data. But in our case what can we do to make sure this theta i represents indeed category i? If you think about the question and you're likely to come up with the idea of using the training data right. Indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. In other words, these are the documents with known categories assigned, and of course human experts must do that. And here you see that T1 represents the set of documents that are known to have been generated from category one, and T2 represents the documents that are known to have been generated from category two, etc. Now if you look at this picture, you see that the model here is really a simplified unigram language model. It is no longer mixture model. Why? Because already know which distribution has been used to generate which documents. There's no uncertainty here. There's no mixing of different categories here. So the estimation problem of course would be simplified, but in general you can imagine what we want to do is to estimate these probabilities that I marked here and what are the probabilities that we have to estimate in order to do categorization where there are two kinds. So one is the prior. The probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. The other kind is word distributions and we want to know what words have high probabilities for each category. So the idea then is to just use the observed training data to estimate these two probabilities. And in general we can do this separately for different categories. That's just because these documents are known to be generated from a specific category, so once we know that it's in some sense irrelevant what other categories we are also dealing with. So now this is statistical estimation problem. We have observed some data from some model and we want to guess the parameters of this model. We want to take our best guess of the parameters. And this is the problem that you have seen. Also several times in this course. Now, if you haven't thought about that this problem, haven't seen  naive Bayes classifier, it would be very useful for you to pause the video for a moment and to think about how to solve this problem. So let me state the problem again, so let's just think about category One. We know there is one word distribution that has been used to generate documents. And we generated each word in the document independently and we know that we have observed the set of N sub one documents in the set of T1. These documents have been all generated from category one, namely have been all generated using this same word distribution. Now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category? Of course, this second probability depends on how likely that you will see documents in other categories. Right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these K categories. To estimate all these parameters. Now if you spend some time to think about this and it would help you understand the following few slides. So do spend some time to make sure that you can try to solve this problem or do your best to solve the problem yourself. Now, if you have thought about it and then you will realize the following intuition. First, what's the basis for estimating the prior or the probability of each category? Well, this has to do with whether you have observed a lot of documents from that category. Intuitively, if you have seen a lot of documents in sports and very few in medical science, then your guess is that the probability of sports category is larger or your prior on the category would be larger. And what about the basis for estimating the probability of word in each category? Well, the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. So to estimate the probability of each category. And to answer the question which category is most popular, then we can simply normalize the count of documents in each category. So here you see n sub I denotes the number of documents in each category. And we simply just normalize this count to make this a probability. In other words, we make this probability proportional to the size of training dataset in each category. That's the size of the set T sub i. Now, what about the word distribution? Well, we do the same again. This time we can do this for each category. So let's say we are considering category I or Theta I. So which word has higher probability? Well, we simply count the word occurrences in the documents that are known to be generated from theta i. And then we put together all the all the counts of the same word in this set. And then we just normalize these counts to make this distribution of all the words make all the probabilities of all these words sum to one. So in this case you can see this is a proportional to the count of the word in the collection of training documents. T sub I and that's denoted by C of w and T sub I. Now you may notice that we often write down a probability estimate in the form of being proportional to certain number, and this is often sufficient. Becausw we have some constraints on these distributions and so the normalizer is dictated by the constraint. So in this case it will be useful for you to think about what are the constraints on these two kinds of probabilities. So once you figure out the answer to this question and you will know how to normalize, this counts and so this is a good exercise to work on it if it's not obvious to you. There is another issue in Naive Bayes which is a smoothing. In fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. So smoothing is the important technique to address data sparseness. In our case the training data set can be small and one data set is small. When we use maximum likelihood estimator we often face the problem of zero probability. That means if the event is not observed. Then the estimated probability would be 0 in this case if we have not seen a word in the training documents for, let's say, category I, then our estimate would be 0 for the probability of this word in this category. And this is generally not accurate. So we have to do smoothing to make sure it's not zero probability. The other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. When the data set is small, we tend to rely on some prior knowledge to to solve the problem. So in this case our prior knowledge  says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. There is also a third reason, which is sometimes not very obvious, but we'll explain that in a moment and that is to help achieve discriminative weighting of terms. And this is also called IDF weighting inverse document frequency weighting that you have seen in mining word relations. So how do we do smoothing? Well in general we added pseudo counts to these events. We'll make sure that no event has zero count. So one possible way of smoothing the probability of category is to simply add small nonnegative constant Delta to the count. We pretend that every category has actually some extra number of documents represented by Delta. And in the denominator we also add K multiplied by Delta because we want the probability to sum to one. So in total we've added Delta K Times because we have K categories. Therefore in the sum we have to also add K multiplied by Delta as a total pseudo counts that we add to the estimate. Now it's interesting to think about the influence of delta. Obvious Delta is a smoothing parameter here, meaning that the larger delta is and the more we will do smoothing and that means we'll more rely on pseudo counts and we might indeed ignore the actual counts if delta is set to Infinity. Imagine what would happen if delta approaches positive Infinity? Well, we're going to say every word has infinity amount of sorry, not every word every category has. infinity amount of documents, and then there's no distinction between them, so it becomes just a uniform. What if Delta is zero? Well we just go back to the original estimate based on the observed training data to estimate the probability of each category. Now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. So here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. Theta sub b Now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. But if we don't have to use this one, we can use larger text data that are available from somewhere else. Now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. So what are those words? Well those are the common words. Because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. Now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. Now this helps make the difference of the probability of such words smaller across categories. Because every category has some help from their background for words, like the, a which have high probabilities. Therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. From the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. You also see another smoothing parameter mu here, which controls the amount of smoothing, just like delta does for the other probability. And you can easily understand why we add mu to the denominator because that represents the sum of all the pseudo counts that we add for all the words. So mu is also non-negative constant and it's  empirically set to control smoothing. There are some interesting special cases to think about as well. First, let's think about when mu approaches Infinity. What would happen? Or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. And that essentially removes the difference between these categories. Obviously we don't want to do that. The other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary. So each word has the same probability. Then this smoothing formula is going to be very similar to the one on the top. When we add Delta because we're going to add a constant pseudo count to every word. So in general, in naiyes bayes categorization we have to do such smoothing and   once we have these probabilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier. Now it's useful to further understand whether the naive Bayes scoring function actually makes sense, so to understand that. And also to understand why adding a background language model will actually achieve the effect of idea of IDF weighting and to penalize common words. Right, so it's suppose we have just two categories and we're going to score based on their Ratio of probability, so this is Ann let's say this is our scoring function for two categories. So this is a score of a document for these two categories. And we're going to score based on this probability ratio. So if the ratio is larger  then it means it's more likely to be in category one, so the larger the score is, the more likely the document is in category One. So by using bayes rule we can write down this ratio as follows and you have seen this before. Now, we generally take logarithm of this ratio and to avoid small probabilities, and this would then give us this formula in the second line. And here we see something really interesting, because this is our scoring function for deciding between the two categories. And if you look at this function, we'll see it has several parts. The first part here is actually log of prior probability ratio and so this is the category bias. So it doesn't really depend on the document, it just says which category is more likely and then would. We would then favor this category slightly. So the second part has a sum of all the words. Right, so these are the words that are observed in the document, but in general we can consider all the words in the vocabulary. So here we're going to collect evidence about which category is more likely. So inside the sum you can see there is product of two things. The first is count of the word. And this count of the word serves as a feature and to represent the document. And this is what we can collect from document. The second part is the weight of this feature. Here it's the weight on each word and this weight. Tells us. To what extent observing this word helps contributing to our decision to put this document in Category One. I remember the higher the scoring function is more likely it's in category one. Now if you look at this ratio basically or sorry this weight It's basically based on the ratio of the probability of the word from of the two distributions. Essentially we are comparing the probability of the word from the two distributions and if it's higher according to theta one, then according to theta 2 then this weight would be positive and therefore it means when we observe such a word. We'll say that it's more likely to be from category One, and the more we observe such a word, the more likely the document will be classified as theta one. If, on the other hand, the probability of the word from theta one is smaller than the probability of the word from theta 2, then you can see this weight is negative. Therefore this is the negative evidence for supporting category one. That means the more we observe such a word, the more likely the document is actually from theta 2. So this formula now makes a lot of sense, so we're going to aggregate all the evidence from the document. We take a sum over all the words we can call this the features. That we collect from the document that would help us make the decision and that each feature has a weight that tells us how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio. Here in naive Bayes. And then finally we have this constant of bias here, so that formula actually is a formula that can be generalized to accommodate more features. And that's why I've introduced some other symbols here. So introduce the beta zero to denote the bias and Fi to denote each feature, and then beta sub i, to denote the weight on which feature. Now if we do this generalization, what we see is that in general we can represent the document by feature vector F, FI here. Of course in this case FI is the count of a word, but in general we can put any features that we think are relevant for categorization. For example document length or the font size or counts of other patterns in the document. And then our scoring function can be defined as a sum of constant beta zero and sum of the feature weights over all the features. So if HF sub I is a feature value then we multiply value by the corresponding weight beta sub i and we just take sum and this is to aggregate. All evidence that we can collect from all these features. And of course there are parameters here. So what are the parameters? Well These betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. Just like in the case of Naive Bayes we can clearly see naive Bayes classifier is a special case of this general classifier. Actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification. And we are going to talk more about such approaches later, but here I want you to know that there's a strong connection close connection between the two kinds of approaches, and this slide shows how naive Bayes classifier can be connected to a logistic regression. And you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to solve the problem.
410	69980b1f-e40c-4015-a85a-54ab27fbac41	This lecture is about how to mine text data with social network as context. In this lecture, we're going to continue discussing contextual text mining. In particular, we're going to look at the social network of authors of text as context. So first some motivation for using network context for analysis of text. The context of a text article can form a network. For example, the authors of research articles might form a collaboration network. Or authors of social media content might also form social networks. For example, in Twitter. People might follow each other, or in Facebook. Some people might claim as friends of others etc. So such context connects the content. Often the others. Similarly, locations associated with text can also be connected to form geographical network, but in general you can imagine the meta data of the text data can form some kind of network if they have some relations. Now there is some benefit in jointly analyzing text and its social network context or network context in general. And that's because we can use network to impose some constraints on topics text. So for example, it's reasonable to assume that authors connected in collaboration network tend to write about the similar topics. So such heuristic can be used to guide us in analyzing topics. Text also can help characterize the content associated with each subnetwork and this is to say that both kinds of data, the network and text can help each other. So for example, the difference in opinions expressed in two subnetworks let's say two social networks can be revealed by doing this kind of joint analysis. So here we are going to briefly introduce. a model called Network supervised topic model. And, this in this slide we're going to give some general ideas, and then in the next slide will give some more details. But in general, in this part of the course we don't have enough time to cover these frontier topics in detail, but we provide references that would allow you to. To read more about the topic. To know the details. But it should still be useful to know the general ideas and to know what they can do to know when you might be able to use them. So the general idea of network supervised topic model modeling is the following. Let's start with. Viewing the regular topic models like PLSA or LDA as solving optimization problem. Of course, in this case the optimization objective function is the likelihood function. So we often use maximum likelihood estimator to obtain the parameters and these parameters would give us useful information that we want to obtained from text data. For example topics. So we want to maximize the probability of text data given the parameters generated denoted by Lambda here. Now the main idea of incorporating network is to say that to think about the constraints that can be imposed based on the network. In general, the idea is to use the network to impose some constraints on the model parameters Lambda here. For example, the text at adjacent nodes of the network can be assumed to cover similar topics. Indeed, in many cases they tend to cover similar topics. We so we may be able to smooth the topic distributions on the graph on the network so that adjacent nodes will have very similar topic distributions, so you. They will share common distribution of the topics or have just slight variations of the topic distributions or topic coverage. So technically what we can do is simply to add a network induced regularizers to the likelihood objective function as shown here. So instead of just optimizing the probability of text data given parameters, Lambda. We're going to optimize another function F. This function combines the likelyhood with a regularizer function called r here, and the regularizer is defined the on the parameters Lambda and the network and tells us basically what kind of parameters are preferred from network constraint perspective, so you can easy to see this is in effect implemented the idea of imposing a prior on the model parameters only that will not necessarily having a probabilistic model. But the idea is the same. We're going to combine the two in one single objective function. So the advantage of this idea is that it's quite general here the topic model can be any generative model for text. Right, it doesn't have to be PLSA or LDA or the current topic models. And similarly, the network can be also any network. Any graph that connects these text objects. This regularizer can also be any regularizer. We can be flexible in capturing different heuristics that we want to capture. And finally, the function F can also vary, so there can be many different with combined them. So this general idea is actually quite powerful. Office general approach to combining these different types of data in a single optimization framework. And this general idea clearly can be applied for many problems, but here in this paper referenced here. A particular instantiation, called NetPLSA was started in this case it's just an extension of PLSA to incorporate some simple constraints imposed by network. And the prior here is the neighbors on the network must have similar topic distribution. They must cover similar topics in similar ways, and that's basically what it says in English. So technically we just have a modified object function here as defined on both the text collection C and the network graph G here. And if you look at this formula and you can actually recognize some part fairly familiar, because are they should be fairly familiar to you by now. So can you recognize which part? Is the likelihood for the text data given by a topic model? If you look at it that you will see this part is precisely the PLSA log likelihood that we want to maximize when we estimate the parameters for PLSA alone. But the second equation shows some additional constraints on the parameters. And in particular we see here. It's to measure the difference between the topic coverage at node U and the V. The two adjacent nodes on the network. We want their distributions to be similar, so they here we are computing the square of their differences and we want to minimize this difference. And note that there is a negative sign in front of this sum. This whole Sum. Here. So this makes it possible to find the parameters that are that are both to maximize the PLSA log likelihood. That means the parameters will fit the data well and also to respect this constraint from the network. And this is an active sign that I just mentioned, because there's a negative sign when we maximize this objective function we will actually minimize this second term here. So if we look further in this picture, will see there is also weight of edge between u and v here and that's based on our network. If we have a weight that says these two nodes are strong collaborators of researchers, or these two are strong. Connections between two people in a social network and then they will have a high weight then that means it will be more important to make sure that their topic coverages are similar and that's basically what it says here. And then finally use your parameter Lambda. Here. This is a new parameter to control the influence of network constraints. We can see easily if Lambda is set to zero, we just go back to the standard PLSA. But when the lambda is set to a larger value, then we'll let the network influence the estimate models more so as you can see, the effect here is that we can do basically PLSA but we're going to also try to make the topic coverages on the two nodes that are. strongly connected to be similar and we ensure their coverages are more similar. So here are some sample results from that paper, and this slide shows the regular results of using PLSA and the data here is DBLP data. bibliographic data about the research articles. And the experiments have to do with using a four communities of publications. IR information retrieval. That means DM, stand for data mining, ML for machine learning and web. There are four communities of articles and we will. Hoping to see that the topic mining can help us uncover these four communities, but from these sample topics that you are seeing here that are generated by PLSA and PLSA is unable to generate the four communities that correspond to our intuition and the reason was because they are all mixed together and there are many words that are shared by these communities, so it's not that easy to use the four topics. to separate them. If we use more topics, perhaps will have more coherent topics. But what's interesting is that if we use the NetPLSA where the network the collaboration network in this case of others is used to impose constraints. And this, in this case we also use four topics, but NetPLSA would give much more meaningful topics. So here we see that these topics correspond well to the four communities. The first is the information retrieval second is data mining. Third is machine learning the fourth is web. So that separation was. Mostly because of the influence of network where we leverage the collaboration network information. Essentially the people that form a collaboration network would then be kind of assumed to write about similar topics and that's why we can have more coherent topics. And if you just listen to text data alone based on the Co occurrences you won't get such a coherent topics, even though a topic model PLSA or LDA also. should be able to pick up a Co occurring words, so in general the topics that they generate represent words that Co occur with each other. But still they cannot generate such coherent results as NetPLSA is showing that the network context is very useful here. A similar model could have been also used to characterize the content associated with each subnetwork of collaborations. So a more general view of text mining in the context of network is to treat text as living in the rich information network environment. That means we can connect all the related data together as a big network and text data out can be associated with a lot of structures in the network, for example text. It can be associated with the nodes of the network, and that's basically what we just discussed in the NetPLSA But text data can be associated with edges as well, or paths or even sub networks and such way to represent taxes are in the big environment of all the context. Information is very powerful because it allows us to analyze all the data, all the information together. And so, in general analysis of text there should be using the entire network of information that's related to the text data. So here's one suggested reading and this is the paper about the NetPLSA where you can find more details about the model and how to estimate such a model.
410	6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb	This lecture is about the probabilistic topic models for topic mining and analysis. In this lecture we're going to continue talking about the top mining and analysis. We're going to introduce probabilistic topic models. So this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. So to solve these problems intuitively we need to use more words to describe the topic and this would address the problem of lack of expressive power. When we have more words that we can use to describe the topic, we can describe complicated topics, to address the second problem, we need to introduce weights of words. This would allow you to distinguish subtle differences in topics and to introduce semantically related words in the fuzzy manner. Finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. It turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. So the basic idea here is improved representation of topic as a word distribution. So what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. But now we're going to use a word distribution to describe the topic. So here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary. So for example, the high probability words here are sports, game, basketball, football, play, star, etc. These are sports-related terms and of course it would also give a non zero probability to some other words like """travel"" which might be related to" sports. But in general not so much related to the topic. In general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one. So that it forms a distribution of all the words. Now intuitively, this distribution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports. You can also see it as a very special case if the probability mass is concentrated entire of just one word. Let's sports, and this basically degenerates to the simple representation of topic with just one word. But as a distribution, this topic representation can in general involve many words to describe the topic and can model subtle differences in semantics of the topic. Similarly, we can model travel and science with their respective distributions. So in the distribution for travel we "see top words like ""attraction, trip," "flight, hotel etc.""" "Whereas in science, we see ""scientist," "spaceship, telescope or genomics"" and new" science-related terms, now, that doesn't mean sports-related terms  necessary have zero probabilities for science in general, we can imagine all these words. We have non zero probabilities, it's just that for a particular topic of some words we have very very small probabilities. Now you can also see there are some words that are shared by these topics. Well, when I say shared, that just means even with some probability threshold you can still see one word to occur in multiple topics. In this case I marked them in black so "you can see ""travel""," for example, occured in all the three topics here, but with different probabilities. It has the highest probability for the travel topic 0.05. But with much smaller probabilities for sports and science, which makes sense. "And similarly you can see ""star"" also" occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics. So with this representation it addresses the three problems that mentioned earlier. First, it now uses multiple words that describe topic, so it allows us to describe fairly complicated topics. Second, it assigns weights to terms, so now we can model several differences of semantics and you can bring in related words together to model topic. Third, because we have probabilities for the same word in different topics. We can disambiguate the sense of word in the text to decode its underlying topic, so we address all these three problems with this new way of representing a topic. So now, of course, our problem definition has been refined just slightly. The slide is very similar to what you have seen before, except that we have added refinement for what the topic is. So now each topic is word distribution. And for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. So you see a constraint here and we still have another constraint on the topic coverage, namely pis. So all the pis of ij's must sum to one for the same document. So how do we solve this problem? Well, let's look at this problem as a computation problem now. So we clearly specify the input and output as illustrated here on this side. The input, of course is our text data C is the collection, but we also generally assume we know the number of topics K or we hypothesize a number and then try to mine K topics, even though we don't know the exact topics that exist in the collection and these vocabulary set. As a set of words that determines what units would be treated as the basic units for analysis. In most cases, we use words as the basis. For analysis, and that means each word is a unit. Now the output would consist of as first a set of topics represented by Theta i's Each theta_i is a word distribution. And We also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before. So given a set of text data, we would like to compute all these distributions and all these coverages as you have seen on this slide. Now of course, there may be many different ways of solving this problem. Indeed, you can write a heuristic program to solve this problem, but here we're going to introduce a general way of solving this problem called  generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here I dim the picture that you have seen before in order to show the generation process. So the idea of this approach is actually to 1st design a model for our data. So we design a probabilistic model to model how the data are generated. Of course this is based on our assumption. The actual data aren't necessary generated this way, so that would give us a probability distribution of the data that you are seeing on this slide given a particular model and parameters that are denoted by Lambda. So this capital lambda actually consists of all the parameters that we're interested in. And these parameters in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. Now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. First, we have theta_i's Each is a word distribution and then we have a set of pi's for each document. And since we have N documents so we have N sets of pis. And each set of the pi values will sum to one. So this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. So how do we model the data in this way? And we assume that data are actually samples drawn from such a model that depends on these parameters. Now one interesting question here is to think about how many parameters are there in total. Now obviously we can already see N * K parameters for pi's. We also see K theta_i's, but each theta_i is actually a set of probability values. Right? It's a distribution over words. So I leave this as exercise for you to figure out exactly how many parameters there are here. Now, once we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data. In other words, we would like to adjust these parameter values until we give our data set the maximum probability. I just say that depending on the parameter values, some data points will have higher probabilities than others. What we're interested in here is what parameter values will give our data set the highest probability. So I also illustrate the problem with the picture that you see here. On the X axis, I just illustrate the Lambda, the parameters as one dimensional variable. It's oversimplification obviously, but it suffices is to show the idea and the Y axis shows the probability of the data observe. This probability obviously depends on the setting of Lambda, so that's why it varies as you change the value of Lambda. What we're interested in here is to find the Lambda star that would maximize the probability of the observed data. So this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. So this is a general idea of using a generative model for text mining. First, we design a model with some parameters that we are interested in, and then we model the data. We adjust the parameters to fit the data as well as we can. After we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. By varying the model, of course we can discover different knowledge. So to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic. It also allows us to assign weights on words so we can model subtle variations of semantics. We talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. The number of topics and vocabulary set and the output is a set of topics. Each is word distribution. And also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. The first is the constraint on the word distributions. In each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. The second constraint is on the topic coverage in each document. A document is not allowed to cover a topic outside the set of topics that we are discovering. So the coverage of each of these K topics would sum to one for a document. We also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. We simply assume that they are generated this way and inside the model, we embed some parameters that were interested in denoted by Lambda. And then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the Lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text. As you will see later in the other lectures.
410	6a9b1334-5f53-407b-8864-2cff7edbc603	This lecture is the first one about the text clustering. This is very important that technique for doing topic mining an analysis. In particular, in this lecture organ to start with some basic questions about the clustering: What is text clustering and why we are interested in text clustering? In the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. "So what is text  Clustering actually is a very general technique for data mining. As you might have learned in some other courses. The idea is to discover natural structures in the data. In other words, we want to group similar objects together. In our case, these objects are of course texture objects. For example, they can be documents, turns, passages, sentences or websites. And then our goal is to group similar texture objects together. So let's see a example here. You don't really see text objects, but I just use some shapes to denote objects that can be grouped together. Now, if I ask you what are some natural structures or natural groups well you, if you look at it, you might agree that we can group these objects based on shapes or their locations on this 2 dimensional space. So we got the three clusters in this case. And then may not be so much disagreement about these three clusters, but it really depends on the perspective to look at the objects. Maybe some of you have also seen it in a different way, so we might get different clusters. And you will see another example about this ambiguity more clearly, but the main point here is the problem is actually not so well defined. And the problem lies in how to define similarity. What do you mean by similar objects? Now this problem. Has to be clearly defined in order to have well defined clustering problem. And the problem is in general that any two objects can be similar that depending on how you look at them. So for example. Let's look at the two words like car and horse. So are the two words similar It depends on how you look at it. If you look at the physical. Physical properties of car and horse. They are very different. But if you look at the them functionally, a car in the horse can both be transportation tool, so in that sense they may be similar. So as you can see, it really depends on our perspective to look at the objects and so in order to make the clustering problem well defined, a user must define the perspective. For assessing similarity. And we call this perspective the clustering bias. And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be used to group similar objects 'cause otherwise. Similarity is not well defined. An one can have different ways to group objects. So let's look at a concrete example. Here you are seeing some objects or some shapes that are very similar to what you have seen on the 1st slide. But if I ask you to group these objects again, you might. Might. Feel there's more uncertainty here than on the previous slide. For example. You might think, well, we can still group by shapes, so that would give us cluster that looks like this. However, you might also feel that. Maybe the objects can be grouped based on the sizes, so that would give us a different way to cluster the data. If we look at the size and look at the similarity in size. So as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. Without perspective, it's very hard to define what is the best clustering result. So there are many examples of text clustering. Set up. And so, for example, we can cluster documents in the whole text collection. So in this case documents are the units to be clustered. We may be able to cluster terms in this case. Terms are objects. And Cluster of terms can be used to define the concept or theme or topic. In fact, the topic models that you have seen some previous lectures. Can give you cluster of terms in some sense. If you take the terms with high probabilities from world distribution. Another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. For example, we might extract all the text segments about the topic, let's say by using a topic model. Now, once we've got those text objects, then we can cluster. The segments that we've got to discover interesting clusters that might also represent the subtopics. So this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. The goal of doing more sophisticated mining and analysis of text data. We can also cluster fairly large text law gets, and by that I just mean text objects may contain a lot of documents. So for example we might cluster websites. Each website is actually composed of multiple documents. Similarly, we can also cluster articles written by the same author, for example. So we can treat all the articles published by author as one unit for Clustering. In this way, we might group authors together based on whether they are published papers or similar. Furthermore, text clusters can also be further clustered. Regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. So more generally, why is text clustering interesting? Well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. And so a typical scenario is that you are getting a lot of text data. Let's say all the email messages from customers in some time period, or all the literature, articles, etc. And then you hope to get the sense about what are the overall content of the collection. So, for example, you might be interested in getting. A sense about the major topics or what are some typical or representative document in the collection? And clustering help us achieve this goal. We sometimes also want to link similar text objects together and these. These objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents. Sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic. We may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the feature value would be one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later. So there are in general many applications of text clustering any. I just saw it with two very specific ones. One is to cluster search results for example and You can imagine a search engine can cluster the search results so that user can see overall structure of those. Results returned for a query. And when the query is ambiguous, this is particularly useful. Becausw clusters likely represent different senses of ambiguous word. Another application is to understand the major complaints from customers based on their emails, right? So in this case we can cluster email messages and then find the major clusters. From there. We can understand what other major complaints about.
410	6cf0ae0d-8ed2-4731-9da3-bcf60af9a356	this lecture is about a probabilistic retrieval model in this lecture we're going to continue the discussion of the text retrieval methods where can do look at another kind of very different the way to design ranking functions then the vectors based model that we discussed it before being probabilistic models we define the ranking function based on the probability that this document is relevant to this query in other words we are we introduce a binary random variable here this is the variable are here and we also assume that the query and the documents are observations from random variables note that in the vector space model we assume they're vectors but here we are assumed we assume they are the data observed from random variables and so the problem mod retrieval becomes to estimate the probability of relevance in this category of models there are different variants the classical problem is model has led to the BM twenty five retrieval function which we discuss in the vectors based model because it's a form is actually similar to affective space model in this lecture we will discuss another subclass in this big class call the language modeling approaches to retrieval in particular whether the discuss the query like hold retrieval model which is one of the most effective models in probabilistic models there is also another line called a divergent from randomness model which has led to the P L two function it's also one of the most effective state of the other retrieval functions in query like called our assumption is that this probability of relevance can be approximated at by the probability of query given a document and randomness so intuitively this probability just captures the following probability and that is if the user likes document D how likely would the user enter query Q in order to retrieve document so we assume that the user likes T because we have at relevance value here and then we asked the question about the how likely we will see this particular query from this user so this is the basic idea not to understand this idea let's take a look at the general idea or the basic idea of probabilistic retrieval models so here i listed at some imagine the relevance status values or relevance judgments of queries and documents for example in this line shows that query one is a query that the user tightly and T one is a document that user has a scene and one means the user thinks D one is relevant to kill one so this are here can be also approximator by the click through data that a search engine can collect it by watching how you interact with the search results so in this case let's say the user clicked on this document so there is a one here similarly the user clicked on D two also so there is one here in other words D two is assumed to be relevant to kill one on the other hand these three is non relevant there's a zero here at the voiced non relevant and then D five is again relevant and so on so false and this part of maybe there are collected from a different user so this user type thing kill one and then found that the T one is actually not useful so D wines actually non relevant in contrast here we see it's relevant and or this could be the same query typing by the same user at different times but the two is also relevant etc and then here we can see more data about other queries not we can imagine we have a lot of such data now we can ask the question how can we then estimate the probability of relevance right so how can we compute this probability of relevance or intuitively that just means if we look at it all the entries where we see this particular T and this particular Q how likely will see a one on the third column so basically that just means we can just collect these accounts we can first recount how many times we have seen Q and V as a pair in this table and then count how many times we actually have also seen one in the third column so and then we just compute the ratio so let's take a look at some specific examples suppose we're trying to compute this probability for D one D two and D three for Q one what is the estimated power penalty now think about that you can pause the video if needed try to take a look at the table and try to give your estimate of the probability have you seen that if we are interested in Q one and one will be looking at these two pairs and in both cases well actually in one of the cases the user has said this is why this is random and so i is equal to one in only one of the two cases in the other case it's zero so that's one out of two what about T one and T two well the idea here windy two D one D two in both cases in this case size equal to one so it's a two out of two and so on so forth so you can see with this approach we can actually score these documents for the query right we now have a score for D one D two and three for this query we can simply rank them based on these probabilities and so that's the basic idea of probabilistically retrieval model and you can see it makes a lot of sense in this case it's going to rank D two above all the other documents becaus in all the cases when you have seen Q one and Y two is equal to one the user clicked on this document so this also should show that with a lot of click through there are a search engine can learn a lot from the data to improve their search engine this is a simple example that shows that with even small number of entries here we can already estimate some probabilities these probabilities would give us some sense about which document might be more relevant or more useful to a user typing this query now of course the problems that we don't observe all the queries in all the documents and all the relevance values there will be a lot of unseen documents in general he only collected data from the documents that we have shown to the users and there are even more unseen queries becaus you cannot predict the word queries would be typing by users so obviously this approach won't work if we apply to unseen queries or anything documents nevertheless this shows the basic idea of problems material model and it makes sense intuitively so what do we do in such a case when we have a lot of unseen documents an answering queries or the solutions that we have to approximate in some way so in this particular case called query likelihood retrieval model we just approximate this by another conditional probability P of Q given the an all is equal to one so in the condition part we assume that the user likes the document because we have seeing that the user clicked on this document and this part shows that we're interested in how likely the user would actually enter this query how likely will see this query in the same role to note that here we have made an interesting assumption here basically we're going to assume that whether the user types in this query has something to do with whether user likes the document in other words we actually make the following assumption and that is a user formulas a query based on an imaginary realm in the document well if you just look at this this is conditional probability it's not obvious we're making this assumption so what i really meant is that to use this new conditional probability to help us score then this knew conditional probability we have to somehow be able to estimate this conditional probability without relying on this big table otherwise we would be having similar problems as before and by making this assumption we have some way to bypass this dick table and try to just model how the user formulas a query OK so this is how you can simplify the the general model so that we can derive a specific training function later so let's look at the how this model work for our example basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user 's mind when the user formulates this query so we asked this question can we quantify the probability in this probability is conditional probability of observing this query if a particular document it is in fact the imagine relevant document in the user 's mind here you can see we compute all these query likelihood probabilities the likelihood of queries given each document once we have these values we can then rank these documents based on these values so to summarize the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable are here and then let's a scoring function be defined based on this conditional probability we also talked about approximating this by using the query likely hold and in this case we have a ranking function that's basically based on the probability of a query given the document and this probability should be interpreted as the probability that a user who like stocking the D would pose query Q now the question of course is how do we compute this conditional probability at this in general has to do with how to compute the probability of text because Q is attacks and this has to do with model called and language model and this kind of models are proposed to model text so more specifically we will be very interested in the following conditional probability 's issuing this here if the user like this document how likely the user would oppose this query ann in the next lecture working to give introduction to language model so that we can see how we can model text with the probabilistic model in general
410	6ea11c1e-e924-4648-9ba7-3c1596bb0a5b	This lecture is about the future of web search. In this lecture we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general. In order to further improve the accuracy of search engines, it's important to consider special cases of information need, so one particular trend could be to have more and more specialized and customized search engines and they can be called vertical search engines. These vertical search engines can be expected to be more effective than the current general search engines because they could assume that the users are a special group of users that might have a common information need, and then the search engine can be customized to serve such users. And because of the customization is also possible to do personalization, so search can be personalized. Because we have a better understanding of the users. Because of the restriction of the domain, we also have some advantages in handling the documents because we can have better understanding of documents. For example, particular words may not be ambiguous in such a domain, so we can bypass the problem of ambiguity. Another trend that we can expect to see is The search engine will be able to learn over time. It's like a lifetime learning or lifelong learning. And this is of course very attractive, because that means the search engine will self improve itself as more people are using it. Search engines will become better and better and this is already happening because search engines can learn from the implicit feedback, more users use it and the quality of the search results for the popular queries that are typed in by many users will likely become better. So this is another feature that we would see. The third trend might be the integration of multi modes of information access. So search navigation and recommendation or filtering might be combined to form a full fledged information management system. And in the beginning of this course we talked about the push versus pull. These are different modes of information access, but these modes can be combined. And similarly, in the pull mode, querying and browsing could also be combined then in fact we're doing that basically today with the current search engines we are querying. Sometimes browsing, clicking on links. Sometimes we've got some information recommended, although most of the cases information recommended is because of advertising. But in the future you can imagine seamlessly integrate the system with multi mode for information access. And that would be convenient for people. Another trend is that we might see systems that try to go beyond, search it supports the user tasks. After all, the reason why people want to search is to solve a problem or to make a decision or perform a task. For example, consumers might search for opinions about products in order to purchase a product and choose a good product to buy. So in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product. In this area, after the current search engines already provided good support, for example, you can sometimes look at the reviews and then if you want to buy it and you can just click on the button to go to the shopping site directly, get it done. But it does not provide a good task support for many other tasks. For example for researchers, you might want to find the relevant literature or site of the literature, and then there's no support for finishing tasks such as writing a paper. So in general, I think there are many opportunities to innovate, and so in the following a few slides I would be talking a little bit more about some specific ideas or thoughts that hopefully you can help you imagine new application possibilities. Some of them might be already relevant to what you are currently working on. In general, you can think about any intelligent system, especially intended information system, as being specified by these three nodes, and so if we connect these three into a triangle, then we'll be able to specify information system, and I call this data user service triangle. So basically the three questions you ask would be who are you serving and what kind of data you are managing. And what kind of service you provide? Right there, this would help us basically specify your system and there are many different ways to connect them, depending on how you connect them, you will have a different kind of system, so let me give you some example. On the top, you can see different kinds of users, on the left side you can see different types of data or information, and on the bottom you can see different service functions. Now imagine we can connect all these in different ways. So for example if you can connect everyone with web pages. And the support search and browsing. What do you get? That's web search, right? What if we connect the UIUC employees with organization documents or enterprise documents to support the search and browsing. That's enterprise search. If you connect the scientists with literature information to provide all kinds of service, including search, browsing or alert of new relevant documents, or mining, analyzing research trends, or provide the task support or decision support, for example. You might be might be able to provide a support for automatically generating related work section for research paper, and this would be closer to task support, right? So then we can imagine this would be a literature assistant if we connect to online shoppers with blog articles or product reviews, then we can help these people to improve shopping experience so we can provide for example data mining capabilities to analyze the reviews to Compare products, compare sentiment of products and to provide a task support or decision support to help them choose what product to buy. Or we can connect the customer service people with emails from the customers. And we can imagine a system that can provide analysis of these emails to find the major complaints of the customers. We can imagine the system could provide task support by automatically generating a response to the customer email, or maybe intelligently attach also promotion. Message if appropriate if the detector that's a positive message, not a complaint, and then you might to take this opportunity to attach some promotion information. Whereas if it's a complaint that you might be able to. Automatically generate some generic response first and tell the customer that he or she can expect the detailed response later, etc. All these are trying to help people to improve the productivity. So this shows that the opportunities are really a lot, it's just only restricted by our imagination. So this picture shows the trend of the technology and also it characterizes the intelligent information system in three angles you can see in the center there is a triangle that connects keyword queries to search and bag of words representation. That means the current search engines basically provides search support. Users and mostly model users based on keyword queries. And it sees the data through a bag of words representation. So it's a very simple approximation of the actual information in the documents. But that's what the current system does. It connects these three nodes in such a simple way. Or it only provides you basic search function and doesn't really understand the user. And it doesn't really understand it that much information in the documents. Now I showed some trends to push each node. Toward more advanced function, so think about the user node here so we can go beyond the keyword queries. Look at the user search history and then further model the user completely to understand the users task, environment, task need context or other information. So this is pushing for personalization and complete the user modeling and this is a major direction in research. In order to build intelligent information systems. On the document side. We can also see we can go beyond bag of words representation to have entity relation representation. This means we'll recognize peoples names their relations, locations etc and this is already feasible with today's natural language processing technique and Google's recent initiative on the Knowledge Graph. If you have heard of it, it's a good step toward this direction and once we can get to that level of representation in a robust manner at large scale, it can enable the search engine to provide much better service. In the future, we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent. So this calls for large scale semantic analysis, and perhaps this is more feasible for vertical search engines. It's easier to make progress in a particular domain. Now on the service side, we see that we need to go beyond the search if support information access in general, so search is only one way to get access to information as well. Recommender systems and push and pull are different ways to get access to relevant information, but going beyond access we also need to help people digest information. Once the information is found and this step has to do with analysis of information or data mining. We have to find the patterns or converted the text information into your knowledge that can be used in application or actionable knowledge that can be used for decision making. And furthermore the knowledge would be used to help user to improve productivity in finishing a task. For example, a decision making task. So this is a trend and so basically in this dimension we anticipate in the futur. Intelligent Information systems will provide intelligent and interactive task support. Now I should also emphasize interactive here because it's important to optimize the combined intelligence of the users and the system so we can get some help from users in some natural way and we don't have to assume the system has to do everything when the human user and the machine can collaborate in an intelligent way in an efficient way, then the combined intelligence will be high and in general we can minimize the user's overall effort in solving problem. So this is the big picture of future intelligent information systems. And this hopefully can provide us some insights about how to make further innovations on top of what we can do today.
410	6ff3d617-b0b3-4f95-8b77-3aef7d9e0e76	so i showed you how we rewrite the query like hold mutual function into a form that looks like the formula from this slide after we make the assumption about the smoothing the language model based on the collection language model if you look at the this rewriting it actually would give us two benefits the first benefit is helps us better than standard this ranking function in particular we're going to show that from this formula we can see small thing with the collection language model with give us something like a TF IDF weighting and thence normalization the second benefit is that it also allows us to compute the query like order more efficiently in particular we see that the main part of the formula is a sum over the matching query terms so this is much better than if we take a sum over all the words after we spoke the document language model we essentially have nonzero probabilities for the words so this new form of the formula is much easier to score to compute it's also interesting to note that the last of time here is actually independent of the document since our goal is to rank the documents for the same query we can ignore this term for engin becaus it's going to be the same for all the documents ignore it wouldn't affect the order of the documents inside of the sun we also see that each magical query at home would contribute wait and this wait actually is very interesting becaus it looks like a TF IDF weighting first we can already see it has a frequency of the world in the query just like in the vector space model when we take a top product we see the word frequency in the query to show up in such a song and so naturally this pot would correspond to the vector element from the document back there and here indeed we can see it actually encodes a weight that has similar factor too TF idea winning i let you examine it can you see it can you see which part is captured in TF and which part is capturing idea of waiting so if you want you can pause the video to think more about it so have you noticed that this piece obscene is related to the term frequency in the sense that if a word occurs very frequently in the document then the S major probability here would tend to be larger so this means this term is really doing something like a TF way now have you also notice that this time in the denominator is actually achieving the factor of idea why becaus this is the popularity of the term in the collection but it's in the denominator so if the probability in the collection is larger than the width is after the smaller and this means a popular term we actually have a smaller weight and this is precisely what idea of waiting is doing only that we now have a different form of TF an idea remember ideas has a log logarithm of document frequency but here we have something different but intuitively it achieves similar fact interesting it we also have something related to the length normalization again can you see which factor is related documents in this formula well i just say that this term is related to IDF weighting this this collection probability but it turns out that this time here is after related to document lands normalization in particular offers of D might be related to document land length so it encodes how much probability mass we want to give to unseen words how much smoothing do we want to do intuitively if documents along then we need to do less mosey becaus it we can assume the data is large enough we probably have observed all the words that the author could have written but the document is a short then offers update could be expected to be to be large we need to do more smoothing it's like that there are words that have not been written yet by the other so this term appears to penalize long document in that the office update would tend to be longer than larger than before long document but note that offers of the also occurs here and so this may not actually be necessary paralyzing long documents that effect is not so clear but as we will see later when we consider some specifically smoothing methods it turns out that they do penalize long documents just like in TF IDF weighting and documents normalization formulas in the vector space model so that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing we just need to assume that if we smooth with this collection language model then we would have a formula that looks like a TF IDF weighting and documents normalization what's also interesting that we have very fixed form of the ranking function and see we have not the heuristic lee put a logarithm here in fact that you can think about why we will have logarithm here if you look at the assumptions that we have made will be clear it's be cause we have i used logarithm of query like hold for scoring and we turned the product into a sum of logarithm of probability and that's why we have this lower note that if we only want to heuristically implement the TF waiting an idea of waiting we don't necessarily have to have along with them here imagine if we drop this logarithm we would still have TF an idea of waiting but what's nice with probabilistic modeling is that we are automatically given a logarithm function here and that's basically a fixed reform of the formula that we did not really have to heuristic design and in this case if you try to drop this logarithm the model probably work as well as if you keep along with so a nice property of probabilistic modeling is that by following some assumptions and probability rules will get a formula automatically and the formula would have a particular form like in this case and if we heuristic it designed the formula we may not necessarily end up having such a specific form so to summarize we talked about the need for smoothing a document language model otherwise with gave zero probability for unseen words in the document and that's not good for scoring a query with such a unseen world it's also necessary in general to improve the accuracy of estimating the model represent the topic of this document the general idea of smoothing in retrieval is it will use the collecting language model two give us some cruel power the witch and seen was should have a higher probability that is the probability of an cemboard is assumed that would be proportional to its probability in the collection with this assumption we've shown that we can derive a general ranking formula for query like hold that has the effect of TF IDF weighting and documents normalization we also see that through some rewriting the scoring of such a ranking function is primary based on sum of weights on match query times just like in the vector space model but the actual ranking function is given us automatically by the probability rules and assumptions that we have made and liking the vector space model where we have to heuristically think about the form of the function however we still need to address the question how exactly we should SMS a document language model how exactly we should use reference language model based on the collection to adjust to the probability of the maximum micro S maiden and this is the topic of the next election
410	72313ed1-d189-4ee8-9d34-7e28faae4ce2	This lecture is about the recommender systems. So far we have talked about a lot of aspects of search engines. We have talked about the problem of search and ranking problem, different methods for ranking implementation of a<br> search engine and how to evaluate the search engine etc. The. This is probably because of we know that web search engines are by far the most important applications of text retrieval and they are the most useful tools to help people convert big raw text data into a small set of relevant documents. Another reason why we spend so many lectures on search engines is because many techniques used in search engines are actually also very useful for recommender systems, which is the topic of this lecture. And so overall, the two systems are actually well connected and there are many techniques that are shared by them. So this is a slide that you have seen before when we talked about the two different modes of text access - pull and push. An we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take initiative to recommend the information to user or push a relevant information to the user. And this often works well when the user has a relatively stable information need. When the system has a good knowledge about what the user wants. So a recommender system is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the useless articles. And so in this sense they are kind of similar. And in all these cases, the system must make a binary decision, and usually there's a dynamic source of information items. And you have some knowledge about this user's interest and then the system would make a delivery decision whether this item is interesting to the user and then if he's interested in, then the system would recommend the article to the user. So the basic of filtering question here is really, will this user like this item. Will you like item X? And there are two ways to answer this question. If you think about it, I wanted to look at what items you like. And then we can see if X actually like those items. The other is to look at the who likes X and we can see if this user looks like a one of those users. or like most of those users. And these strategies can be combined if we follow the first strategy that look at item similarity. In the case of recommending text objects. Then we are talking about the content based filtering or content based recommendation. If we look at the second strategy, then it will compare users and in this case will exploit the user similarity and the technique is often called collaborative filtering. So let's first look at the content based filtering system. This is what the system would look like. Inside the system there will be a binary classifier that would have some knowledge above the users‘ interest. And it's called a user interest profile. It maintains this profile to keep track of the users‘ interest. And then there was a utility function to guide the user to make decisions, and I explained the utility function in the moment. It helps the system decide where to set the threshold. And then the accepted document will be those that have passed the threshold according to the classifier. There should be also an initialization module that would take a users input, maybe from a users specified keywords or chosen category etc. And this will be to feed the system with the initial user profile. There is also typically a learning module that will learn from users feedback overtime. Now. Note that in this case typical users information need is stable, so the system would have a lot of opportunities to observe the users, if the user has taken a recommended item has viewed that, and this is the signal to indicate that the recommended item may be relevant if the user discarded it, it's not relevant, and so such feedback can be a long term feedback and can last for a long time. And the system Clock collect a lot of information about these users interest and this can then be used to improve the classifier. Now what's the criteria for evaluating such a system? How do we know this filtering system actually performs well? Now, in this case we cannot use the ranking evaluation measures like a map because we can afford waiting for a lot of documents and then rank the documents to make a decision for the user. And so the system must make a decision in real time in general to decide whether the item is above the threshold or not. So in other words, we're trying to decide the absolute relevance. So in this case, one commonly used strategies is user utility function to evaluate the system. So here I show a linear utility function that's defined as for example 3 multiplied by the number of good items that you delivered minus 2 multiplied by the number of bad items you deliver. So in other words, we could kind of just. treat this as almost a in a gambling game. If you delete, if you deliver one good item, let's say you win $3, you gain $3. But if you deliver a better one and you will lose $2 and this utility function basically kind of measures how much money you will get by doing this kind of game. And so it's clear that if you want to maximize this utility function, your strategy should be to deliver as many good articles as possible and to minimize the delivery of bad articles, that's obvious. One interesting question here is how should we set these coefficients? Now I just showed 3 and negative 2 as a possible coefficients. But one can ask the question, are they reasonable? So what do you think? Do you think that's a reasonable choice? What about the other choices? And also for example we can have 10 and minus one. Or one minus ten. What's the difference? What do you think? How would this utility function affect the system's threshold decision? But you can think of these two extreme cases, 10&nbsp; -1 versus 1 -10. Which one do you think it would encourage the system to overdeliver and which one would encourage the system to be conservative? If you think about it, they will see that when we get a big award for delivering a good document, you incur only a small penalty for delivering bad one. Intuitively, you would be encouraged to deliver more right? And you can try to deliver more. In hopes of getting a good one delivered, and then you'll get a big award. I saw, on the other hand, if you choose 1 -- 10, you don't really get such a big price if you deliver deliver a good document. On the other hand, you will have a big loss if you deliver a bad one. You can imagine that the system would be very reluctant to deliver a lot of documents. It has to be absolutely sure that it's not a non-relevant one. So this utility function has to be designed based on the specific application. Three basic problems in content based filtering are the following. Frst it has to make a filtering decision so it has to be a binary decision maker binary classifier given a text. Text document and profile description of the user. It has to say yes or no, whether this document should be delivered or not. So that's the decision module and there should be a initialization module. As you have seen earlier and this is to get the system started. And we have to initialize the system based on only very limited text description or very few examples from the user. And the third component is a learning module which he had to be able to learn from limited relevance judgments. Because we count in learn from the user about their preferences on the delivered documents if we don't deliver document to the user, we would never know would never be able to know whether the user likes it or not. And we can accumulate a lot of documents and learn from the entire history and all these modules would have to be optimized to maximize the utility. So how can we build such a system? And there are many different approaches. here. Here we're going to talk about how to extend retrieval system. A search engine for information filtering. Again, here's why. We've spent a lot of time to talk about the search engines because it's actually not very hard to extend the search engine for information filtering. So here's the basic idea for extending a retrieval system for information filtering. First we can reuse a lot of retrieval techniques to do scoring, right, so we know how to score documents against queries, etc. We can measure the similarity between profile text, description and document, and then we can use the score threshold for the filtering decision. We do retrieval, and then we kind of find the scores of documents and then we apply a threshold to see whether document is passing the threshold or not, and if it's passing the threshold we are going to say it's relevant, and we're going to deliver it to the user. And another component that we have to add is for is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring. And we know Rocchio can be used for scoring improvement. And but we have to develop a new approaches to learn how to set the threshold and we need to set it initially and then we have to learn how to update the threshold overtime. So here's what the system might look like if we just generalize the vector space model for filtering problems. Right, so you can see the document vector could be fed into a scoring module which is already exists in a search engine that implements a vector space model and the profile will be treated as a query essentially and then the profile vector can be matched with the document vector to generate the score. And then this score would be fed into a threshold module that would say yes or no, and then the evaluation would be based on utility for the filtering results. If it says yes and then the documents will be sent to the user and then the user could give some feedback and the feedback information would have been ,would be used to both adjust to the threshold and to adjust the vector representation so the vector learning is essentially the same as query modification or feedback. in the case of search. The threshold of learning is new component that we need to talk a little bit more about.
410	73582b0c-e010-4f39-891d-0c7837768c71	and here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of an object to buy active user using the ratings of similar users to this active user this is called memory based approach becaus its deliberative similar to storing all the using information and when we are considering a particular user we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that uses information about those users to predict the preference of this user so here's the general idea and we used some notations here so X sub by JD notes rating of object OJ by user UI and ends up i is average rating of all objects by this user so this N I is needed becaus we would like to normalize the railings of objects by this user so how do you do normalization well we're going to just subtract the average rating from all the ratings now this is it'll normalize these ratings so that the ratings from different users would be comfortable because some users might be more generous and they generally have high ratings but some others might be more critical so their ratings cannot be directly compared with each other or aggregated them together so we need to do this normalization now the prediction of the rating on the item by another user or active user use up a here can be based on the average ratings of similar users so the user you subway is the user that we are interested in recommending items to an we now interested in recommending this old subjects so we're interested in knowing how likely this user will like this object now how do we know that where the idea here is to look at the weather similar users to this user have liked this object so mathematically this is to say well the predicted rating of this user on this app object user a on object OJ is basically combination of the normalized the ratings of different users and in fact here we're taking a sum over all the users but not all users contribute equally to the average and this is controlled by the weights so this wait controls are inference of user on the prediction and of course naturally this way that should be related to the similarity between UA and this particular user UI the more similar they are then the more contribution would like user UI to make in predicting the preference of your a so the formula is extremely simple you can see it's a sum of all the possible users an insider some we have their ratings weather normalize ratings as i just explained the regions needs to be normalized in order to be comfortable with each other and then these ratings are weighted by their similarity so you can imagine W of an eye is just a similarity of user and user i now what's K here where katie is simply normalizer it's just it's just one over the sum of all the weights of all the users and so this means basically if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users and it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction right so this is basically the main idea of memory based approaches for collaborative filtering once we make this prediction we also would like to map back to the rating that the user the user would actually make and this is true further add the mean rating or average rating of this user use of a through the predicted value this would recover a meaning for rating for this user so if this user is generous then the average it would be is somewhat high and when we added that the rating will be adjusted to a relatively high rating now when you recommend the items we use this actually doesn't really matter 'cause you are interested in basically the normalized rating that's more meaningful but when they evaluate these collaborative filtering approach is that typically assume the actual ratings of the user on these objects to be unknown and then you do the prediction and then you compare the predictor ratings with their actual ratings so they do have access to their actual ratings but then you pretend you don't know and then you compare your systems predictions with the actual ratings in that case obviously the systems predicting would have to be adjusted to match the actual ratings of the user and this is what's happening here basically OK so this is the memory based approach now of course if you look at the formula if you want to write the program to implement it you still face the problem of determining what is this W function right once you know the W function then the formula is very easy to implement so indeed there are many different ways to compute this function for this weight W and specific approaches general differ in how this is computed so here are some possibilities an you can imagine there are many other possibilities one popular approach is we use the pearson correlation coefficient this would be a sum over common rated items and the formula is standard a pearson correlation coefficient formula as assume here so this basically meshes weather two users tend to all gave higher ratings two similar items or lower ratings two similar items another measure is the cosine measure and this is retrieved the rating vectors as vectors in the vector space and then we're going to measure the angle an computer the cosine of the angle of the two vectors and this measure has been using the vectors based model for retrieval as well so as you can imagine there are many different ways of doing that in all of these cases note that the user similarity is based on their preferences on items and we did not actually use any content information of these items it didn't matter waht these items are they can be movies we can be books they can be product they can be tax documents we just didn't care about the content ann so this allows such approach it will be applied through a wide range of problems now in some new approaches of course we would like to use more information about the user clearly we know more about the user not just these preferences on these items i saw in a actual filtering system is in collaborative filtering we could also combine that with content based filtering we could use more context information and those are all interesting approaches that people are still starting there are new approaches and propose but this memory based approach it has been shown to work reasonably well and it's easy to implement in practical applications this could be a starting point to see if the strategy works well for your application so there are some obvious ways to also improve this approach an maybe would like to improve the user similarity measure and there are some practical issues with deal with here as well so for example there will be a lot of missing values what do you do with them you can set them to default values or the average ratings of the user and that would be a simple solution but there are the monster approaches that can actually try to predict those missing values and then use the predicted values through improved the similarity so in fact the memory based approach it can predict those missing values right so you can imagine you have iterative approach where you first do some pre memory prediction and then you can use the predictor values to further improve the similarity function so this is here is the way to solve the problem and the square is obviously would affect the performance of collaborative filtering just like any other heroes heuristic to improve these similarity functions not idea which is actually very similar to the idea of IDF that we've seen text research is called inverse user frequency or I U F now here the idea is to look at the where the two users share similar ratings if the item is a popular item that has been viewed by many people an seeing these two people interested in this item may not be so interesting but if it's a rare item it has not been viewed by many users but these two users viewed this item and they gave similar ratings and that says more about their similarity so it's kind of to emphasize more on similarity on items that are not viewed by many users
410	74e36652-0c1a-4d5f-a708-0f55c1eb651f	So let's take a look at this in detail. So in this random surfing model. At any page would assume random surfer would choose the next page to visit, so this is a small graph here. That's of course oversimplification of the complicated web, but let's say there are four documents here, d1 d2 d3 and d4, and let's assume that a random surfer or random walker can be on any of these pages. And then the random surfer could decide to just randomly jump into any page. Or follow a link and then visit the next page. So if the random server is at d1. Then With some probability that random surfer will follow the links. Now there are two out links here. One is pointing to d3, the other is pointing to d4, so the random surfer could pick any of these two to reach d3 and d4. But it also assumes that the random surfer might get bored sometimes, so the random surfer will decide to ignore the actual links and simply randomly jump to any page on the web. So if it does that, it would be able to reach any of the other pages, even though there's no link directly from d1 to that page. So this is assumed random surfing model. Imagine a random surfer is really doing a surfing like this. Then we can ask the question how likely on average the surfer would actually reach a particular page like d1 or d2 or d3 really, that's the average probability of visiting a particular page. And this probability is precisely what Pagerank computes. So the Pagerank score of the document is the average probability that the surfer visits a particular page. Now, intuitively, this would basically capture the in link account. Why? Because if a page has a lot of in links, then it would have a higher chance of being visited because there will be more opportunities of having the surfer to follow a link to come to this page. And this is why. The random surfing model actually captures the idea of counting the in links. Note that it also considers the indirect in links. Why? Because if the pages that point to you have themselves a lot of in links. That would mean the random surfer will very likely reach one of them, and therefore it increases the chance of visiting you. So this is a nice way to capture both indirect and direct links. So mathematically, how can we compute this probability in order to see that we need to take a look at how this probability is computed. So first, let's take a look at the transition matrix here. And this is just the metrics with values indicating how likely I ran. The random surfer will go from one page to another, so each row stands for a starting page. For example, row one would indicate the probability of going to any other 4 pages from d1, and here we see there are only two non zero entries, each is 1 over 2. So, this is because if you look at the graph d1 is pointing to d3 and d4, there is no link from d1 to d1 itself or d2, so we've got zeros for the first 2 columns and .5 for d3 and d4. In general, the element in this matrix M sub i, j is the probability of going from d,i to d,j and obviously for each row the values should sum to 1 because the surfer would have to go to precisely one of these other pages, right? So this is the transition matrix. Now, how can we compute the probability of a surfer visiting a page? If you look at the surf model then basically we can compute the probability of reaching a page as follows. So. Here on the left hand side you see it's the probability of visiting page d,j at time T + 1, so it's the next time point. On the right hand side you can see the equation involves the probability of at Page d,i at time T. So you can see the subscript index t here, and that indicates that the probability that the surfer was at a document at time t. So. The equation basically captures the two possibilities of reaching at d,j at time T + 1. What are these two possibilities? One is through random surfing and one is through following a link as we just explained. So the first part captures the probability that the random surfer would reach this page by following a link, and you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume and so there is a factor of 1 minus alpha here, but the main part is really sum over all the possible pages that the surfer could have been at time t. There are N pages, so it's a sum over all the possible N pages. Inside the sum is a product of two probabilities. One is the probability that the surfer was at d,i the time t. That's p sub t of d,i. The other is the transition probability from the d,i to d,j. And so in order to reach this d,j page, the surfer must first be at d,i at time t and then also would have to follow the link to go from the d,i to d,j. So the probability is the probability of being at d,i at time t multiplied by the probability of going from that page to the target page. d,j here. The second part is a similar sound. The only difference is that now the transition probability is a uniform transition probability of 1 / N and this part of captures the probability of reaching this page through random jumping. Right, so the form is exactly the same and is. This also allows us to see why Pagerank essentially assume the smoothing of the transition matrix. If you think about this 1 / N as coming from another transition matrix that has all the elements being 1 / N The uniform matrix, then you can see very clearly. Essentially we can merge the two parts. And because they are of the same form, we can imagine there's a different matrix that's a combination of this M and that uniform matrix, where every element is 1 / N, and in this sense Pagerank uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix. Now of course this is time dependent calculation of probabilities. Now we can imagine if we want to compute the average probabilities, the average probabilities probably would satisfy this equation without considering the time index. So let's drop the time index and just assume that they will be equal. Now this would give us N equations because for each page we have such equation and if you look at the what variables will have in these equations there are also precisely N variables. Right? So this basically means we now have a system of N equations with N variables. And these are linear equations. So basically the problem boils down to solve this system of equations. And here I also showed the equations in the matrix form. It's the vector p here. Equals a metrics of the transverse of the matrix here. And multiply by the vector again. Now, if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for item vector, right when you multiply the matrix by this vector, you get the same value as this vector. And this can be solved by using iterative algorithm. So the equations here on above are basically taken from the previous slide, so you see the relation between the. The Pagerank scores of different pages and in this iterative approach or power approach, we simply start with. Randomly initialized vector p and then we repeatedly just updated this p by multiplying the matrix here by this p vector. So I also show a concrete example here. So you can see this now if we assume alpha is .2, then with the example that we show here on this slide we have the original transition matrix here. Right? That includes the graph, the actual links, and we have this smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with a linear interpolation to form another matrix. That would be like this. So essentially we can imagine now the web looks like this. Can be captured by that there are virtual links between all the pages now. So the Pagerank algorithm would just initialize the p vector first and then just compute the updating of this p vector by using this matrix multiplication. Now if you rewrite this matrix model multiplication in terms of just, individual equations, you will see this. And this is Basically the updating formula for this particular pages Pagerank score so you can also see you if you want to compute the value of this updated score for d1 you basically multiply this rule. Right, by this column. And we take the dot product of the two. That would give us the value for this value. So this is how we updated the vector. We started with some initial values for these guys. For this and then we just revise the scores we generate a new set of scores and the updating formula is this one. So we just repeatedly apply this and here it converges and when the metrics is like this where there's no zero values and it can be guaranteed to converge. And at that point that we will just have the Pagerank scores for all the pages. Now we typically set the initial values just to 1 / N. So Interestingly, this updating formula can be also interpreted as propagating scores over the graph. Can you see why? If you look at this formula and then compare that with this graph? And can you imagine how we might be able to interpret this as essentially propagating scores over the graph? I hope you will see that indeed we can imagine we have values initialized on each of these pages, so we can have values here. Let's say that's 1 /4 for each, and then we're going to use this matrix to update these scores. And if you look at the equation here. This one. Basically, we're going to combine the scores of the pages that possibly would lead to reaching this page, so we'll look at all the pages that are pointing to this page and then combine their scores and propagate the score. The sum of the scores to this document d1. So we look at the scores that represent the probability that the random surfer will be visiting the other pages before it reached d1, and then just do the propagation to simulate the probability of reaching this page. d1. So there are two interpretations. One is just the matrix multiplication and repeatedly multiply the vector by this matrix, but the other is to just think of it as propagating the scores repeatedly on the web. So in practice the computation of Pagerank score is actually efficient because the matrix is sparse and there are some ways to transform the equation so that you avoid actually literally computing the values for all those elements. Sometimes you may also normalize the equation and that would give you a somewhat different form of the equation, but then the ranking of pages will not change. The results of this potential problem of zero out link problem. In that case, if the page does not have any out link then the probability of these pages would not sum to one basically the probability of reaching the next page from this page will not sum to one. Mainly because we have lost some probability mass when we assume there's some probability that the surfer will try to follow links, but then there's no link to follow. And one possible solution is simply to use a page specific damping factor and that could easily fix this. Basically, that's to say alpha would be 1.0 for a page with no out out link. In that case the surfer would just have to randomly jump through another page instead of trying to follow a link. So there are many extensions of page rank. One extension is to do topic specific Pagerank. Noted that Pagerank doesn't really use the query information so. So we can make Pagerank query specific, however, so for example in the topic specific Pagerank we can simply assume when the surfer is bored the surfer is not going to randomly jump to any page on the web. Instead it's going to jump to only those pages that are relevant to a query. For example, if the queries is about the sports, then we could assume that when it's doing random jumping it's going to randomly jump to a sports page. By doing this, then we can bias and Pagerank to topic like sports and then if you know the current query is about sports and then you can use this specialized Pagerank score to rank documents that would be better than if you use a generic Pagerank score. Pagerank is also a general algorithm that can be used in many other applications for network analysis, particularly for example social networks. You can imagine if you compute the Pagerank scores for social network where a link might indicate friendship relation, you'll get some meaningful scores for people.
410	77f30708-8474-41f5-89e9-d0cbabc4f20c	This lecture is about the web indexing. In this lecture we will continue talking about the web search and we're going to talk about how to create a web scale index. So once we crawled the web, we've got a lot of web pages. The next step is to use the indexer to Create the inverted index. In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture. But there are new challenges that we have to solve for web scale indexing and the two main challenges. Our scalability and efficiency. The index would be so large that it cannot actually fit in into any single machine or a single disk, so we have to store the data on multiple machines. Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly. Now, to address these challenges, Google has made a number of innovations. One is the Google file system that's a general distributed file system that can help programmers manage files stored on a cluster of machines. The second is MapRecuce. This is a general software framework for supporting parallel computation. Hadoop is the most known open source implementation of map reduce, now used in many applications. So this is the architecture of the Google File System. It uses very simple centralized management mechanism to manage it all the specific locations of files, so that maintains the file name, space and look up a table to know where exactly each file is stored. The application client would then talk to this GFS master and then obtain specific locations of the files that they want to process. And once the GFS client obtained the. The specific information about the files, then the application client can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network. So when this file system stores the files on machines the system also would create a fixed size of chunks so that data files are separated into many chunks. Each chunk is 64 MB, so it's pretty big, and that's a property for large data processing. These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about. It's all taken care of by this fire system, so from the application perspective, the programmer would see this as if it's a normal file. The program doesn't have to know where exactly it's stored and can just invoke high level operators to process the file. (And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense. On top of the Google File System and Google also proposed map reduce as a general framework for parallel programming. Now this is very useful to support a task like a building inverted index. So this framework is hiding a lot of low level features from the program. As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel. And so some of the low level details hidden in the framework, including the specific network communications or load balancing or where the tasks are executed. All these details are hidden from the programmer. There is also a nice feature which is the built-in fault tolerance. If one server is broken, let's say service down and then some tasks may not be finished, then the map reduce mechanism would know that the task has not been done, so automatically dispatches the task on other servers that can do the job and therefore again the program doesn't have to worry about that. So here's how MapReduce works. The input data will be separated into a number of key value pairs. Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel. Each key value pair would be send to a map function. The programmer would write map function of course. And then the map function would then process this key value pair and would generator a number of other key value pairs. Of course the new key is usually different from the old key that's given to the map as input. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected. And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together. So now we've got a pair of a key and a set of values that are attached to this key. So this will then be sent to a Reduce function. Now of course, each Reduce function will handle a different  key, so we will send these output values to multiple, reduce functions, each handling unique key. A reduce function would then process the input. Which is a key and a set of values to produce another set of key values as the output. So these output values will be then collected together to form the final output. Right, so this is the general framework of MapReduce. Now the programmer only needs to write the Map function and the Reduce function. Everything else is actually taken care of by the MapReduce framework. So you can see the program really only needs to do minimum work. And with such a framework the input data can be partitioned into multiple parts, each is processed in parallel, first by map, and then in the process after we reach the reduce stage, then multiple reduce functions can also further process the different keys and their associated values in parallel, so it achieves (some) It achieves the purpose of parallel processing of large data set. So let's take a look at a simple example and that's what accounting. How the input is files containing words. And the output that we want to generate is the number of occurrences of each word, so it's the word account. We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging. Or search. So how can we solve this problem? One natural thought is that. This task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts, and that's precisely the idea of what we can do with MapReduce. We can parallelize on lines in this input file. So more specifically, we can assume the input to each map function is key value pair that represents the line number and the stream on that line. So the first line, for example, has the "key of 1 and the value is ""Hello World""" " ""Bye World""" and just 4 words. On that line so this key value pair will be sent to a map function. The map function would then just count the words in this line, and in this case of course there are only four words. Each word gets a count of one, and these are the output that you see here on this slide. From this map function. So the map function is really very simple if you look at the what the pseudocode looks like on the right side you see it simply needs to iterate over all the words in this line and then just call the collect function, which means it would then send the world and the counter to the collector. The collector would then try to sort all these. key value pairs from different Map functions, so the function is very simple and the programmer specifies this function as a way to process each part of the data. Of course, the second line will be handled by a different map function, which will produce a similar output. OK, now the output from the map functions will be then send to a collector and the collector will do the internal grouping or sorting. So at this stage you can see we have collected multiple pairs, each pair is a word and its count in the line. So once we see all these pairs then we can sort them based on the key which is the word. So we will collect all the counts of "a word like a ""Bye"" here together." An similarly we do that "for other words like ""Hadoop"", ""Hello"" etc." So each world now is attached to a number of values, a number of counts. And these counts represent the occurrences of this word in different lines. So now we have got a new pair of a key and a set of values and this pair will then be feeding to reduce function. So the reduce function now would have to finish the job of counting the total occurrences of this word. Now it has already got all these partial accounts, so all it needs to do is similarly to add them up so the reduce function shown here is very simple as well. You have a counter and then iterate over all the words that you see in this array, and then you just accumulated the count. And then finally output the key and the total count, and that's precisely what we want as the output of this whole program. So you can see this is already very similar to building an inverted index, and if you think about it, the output here is indexed by world and we have already got the dictionary. Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel. So here's one way to do that. So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document. So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example. It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs. Each key is a word. And the value is the count of this orld in this document, plus the document ID. Now you can easily see why we need to add document ID here. Of course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later. Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just "a key like ""Java"" associated with all the" documents that match this key or the "documents where ""Java"" occurred." And the counts. So the counts of Java in those documents. And this will be collected together and this will be also fed into the reduce function. So now you can see the reduce function has already got input that looks like a inverted index entry, right? So it's just the word and all the documents that contain the word and the frequencies of the word in those documents. So all it needs to do is simply to concatenate them into a continuous chunk of data, and this can be then written into a file system. So basically the reduce function is going to do very minimum work. So this is pseudocode for inverted index construction. Here we see two functions. Procedure Map and procedure Reduce. And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described. In the case of map, it's going to count the occurrences of word using associative array and will output the old accounts together with the document ID here. So this is the reduce function On the other hand, simply concatenates all the input that it has been given and then put them together as one single entry for this key. So this is a very simple MapReduce function, yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines. The program doesn't have to take care of the details. So this is how we can do parallel index construction for web search. So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system. And secondly, it requires creating the index in parallel because it's so large it takes a long time to create the index for all the documents. So if we can do it in parallel it will be much faster and this is done by using the MapReduce framework. Note that the post the GFS and MapReduce frameworks are very general so they can also support many other applications.
410	7be943e1-7bec-499e-9d91-b157f9ecb80d	So average precision is computed for just one query. But we generally experiment with many different queries and this is to avoid the variance across queries. Depending on the queries you use, you might make different conclusions, so it's better to use more queries. If you use more queries then you would also have to take average of the average precision over all these queries. So how can we do that? You can naturally think of just doing arithmetic mean as we know. Always tend to think in this way. So this would give us what is called Mean Average Precision or MAP. In this case we take arithmetic mean of all the average precisions over set of queries or topics. But as I just mentioned in another lecture, is this good? Recall that we talked about the different ways of combining precision and recall. And we conclude that the arithmetic mean is not as good as the F measure. But here it's the same. We can also think about the alternative ways of aggregating the numbers. Don't just automatically assume that. Let's just take the arithmetic mean of the average precision over these queries. Let's think about what's the best way of aggregating. If you think about different ways, naturally you would probably be able to think about another way, which is geometric mean. And we called this kind of average gMAP map. This is another way. So now, once you think about the two different ways of doing the same thing, the natural question to ask is which one is better so. So do you use MAP or gMAP? Again, that's important question. Imagine you are again testing a new algorithms by comparing it with your old algorithm in the search engine. Now you test it on multiple topics. Now you've got the average precisions for all these topics. Now you are thinking of looking at the overall performance you have to take average. But which which strategy would you use? Now first you should also think about the question, would it make a difference? Can you think of scenarios where using one of them would make a difference? That is, they would give different the rankings of those methods. And that also means depending on the way you average, or you take the average of these average precisions, you will get different conclusions. This makes the question become even more important. So which one would you use? Again, if you look at the difference between these different ways of aggregating the average position, you will realize in arithmetic mean the sum is dominant by large values. So what does a large menu value here mean? It means the query is relatively easy. You can have a high average precision, where as gMAP tends to be affected more by lower values and those are the queries that don't have good performance. The average precision is low. So if you think about improving the search engine for those difficult queries than gMAP would be preferred. On the other hand, that if you just want to have improvement over all the kinds of queries or particular popular queries, that might be easy and you want to make the perfect and maybe MAP would be them preferred. So again, the answer depends on your users, your user's tasks, and their preferences. So the point that here is. To think about the multiple ways to solve the same problem and then compare them and think carefully about differences and which one makes more sense. Often in one of them might make sense in one situation and another might make more sense in a different situation, so it's important to figure out under what situations one is preferred. As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document. And this happens often. For example, in what's called a known item search, where you know a target page. Let's say you want to find the Amazon home page, you have one relevant document there, and you hope to find it. And that's called the known item search. In that case, there is precisely one relevant document, or in another application like a question answering. Maybe there's only one answer there, so if you rank the answers, then your goal is ranked at one particular answer on top right? So in this case, you can easily verify, the average precision will basically boil down two reciprocal rank, that is one over R, where R is the rank position of that single relevant document. So if that document is ranked on the very top, R is 1 and then it's one for reciprocal rank. If it's ranked at the second, then it's 1 / 2 etc. And then we can also take a average of all these average position or reciprocal rank over a set of topics and that would give us something called Mean Reciprocal Rank. It is a very popular value for known item search or any ranking problem where you have just one relevant item. Now again, here you can see this R actually is meaningful here, and this R is basically indicating how much effort an user would have to make in order to find that relevant document. If it's ranked on the top is no effort that you have to make or little effort, but if it's ranked at 100 then you actually have to read presumably 100 documents in order to find it. So in this sense, R is also meaningful measure and the reciprocal rank will take the reciprocal of R instead of using R directly. So one natural question here is, why not simply using R? Now imagine if you are to design a measure to measure performance of the ranking system when there is only one relevant item. You might have thought about using r directly as the measure. After all that measures the users effort, right? But think about, if you take the average of this over a large number of topics, again, it would make a difference right, for one single topic using R or using one overall wouldn't make any difference. It's the same larger R with correspond to a small one overall, but the difference would only show when show up when you have many topics. So again think about average of mean reciprocal rank versus average of just R. What's the difference? Do you see any difference? And would this difference change the order of systems in our conclusion? And it turns out that there is actually a big difference, and if you think about it, if you want to think about it and then yourself, then pause the video. Basically the difference is if you take some of R directly, then again will be dominated by large values of R. So what are those values? Those are basically large values that indicate the lowly ranked results. That means the relevant item is ranked very low down on the list and the sum, the audacity. Also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list, but from a user's perspective we care more about the highly ranked documents. So by taking this transformation by using reciprocal rank, here we emphasize more on the difference on the top and think about the difference between one and two. It will make a big difference. In one over R, but think about 100 and 101 and one it won't make much difference if you use this. But if you use this, there will be a big difference. Being 100 and let's say 1000. Right, so this is not the desirable. On the other hand, one and two won't make much difference, so this is yet another case where there may be multiple choices of doing the same thing, and then you need to figure out which one makes more sense. So to summarize, we show the Precision recall curve, can characterize the overall accuracy of a ranked list. And we emphasized that the actual utility over ranking list that depends on how many top rankings results are user would actually examine. Some users will examine more than others and average precision is the standard measure for comparing two ranking methods. It combines precision and recall and it's sensitive to the rank of every relevant the document.
410	7c6ead79-a73f-4c15-ab06-7d76fada8172	but there are some interesting challenges in threshold in learning the filtering problem so here i show the historical data that you can collect in the filtering system so you can see the scores and the status of relevance so the first one has a score thirty six point five and it's relevant the second one is not relevant etc of course we have a lot of documents for which we don't know the status 'cause we have never delivered them through the user so as you can see here we only see the judgments of documents delivered to the user so this is not a random sample so it's a sense of the data it's kind of biased so that creates some difficulty for learning and signal there are in general very little label data an very few random in the data so it's it's also challenging for machine learning approaches typically they require require more training data in the extreme case at the beginning we don't even have any label data as well the system dear has to make a decision so that's very difficult problem at the beginning finally there is also this issue of exploration versus exploitation tradeoff now this means we also want to explore the document space A little bit and to see if the user might be interested in documents that we haven't been able so in other words we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently not matching the users interests so well so how do we do that well we could lower the threshold a little bit and we just deliver some near misses to the user to see what the user would respond to see how the user would respond to this extra document and this is a tradeoff becaus on the one hand you want to explore but on the other hand you don't want to really explore too much 'cause then you'd over deliver non relevant information so exploitation means you would exploit what you learn about the user let's say you know the user is interested in this particular topic so you don't want to deviate that much but if you don't deviate at all then you don't explore at all that's also not good you might miss opportunity another interest of the user so this is a dynama and that's also a difficult problem to solve now how do we solve these problems in general i think one can use the empirical utility optimization strategy and this is strategy is basically to optimize the threshold based on historical data that says you have seen on the previous slide so you can just compute the utility on the training data for each candidate the score threshold pretend what before i cut at this point what if i can cut at the different scoring threshold point what would happen what's utility since these are training there are we can kind of compute the utility we know there are relevant status or we assume that we know relevant status based on approximation of click throws so then we can just choose the threshold and that gives the maximum utility on the training data but this of course there's an account for exploration that we just talked about and there is also the difficulty of bias training sample as we mentioned so in general we can only get the upper bound of the true optimal thresh code 'cause the the threshold might be actually lower than this so it's possible that the discarded item might be actually interesting to the user so how do we solve this problem where we generate as i said we can lower the threshold to explore a little bit so here's one particular protein called a better gamma threshold and learning so the idea is following so here i show rent list of all the training documents that we have seen so far and they're ranked by their positions an on the Y axis we show the utility of course this function depends on how you specify the coefficient in the utility function but we can then imagine depending on the cut off position we will have a utility that means suppose i cut at this position and that would be the utility so we can for example identify some cutting cut off point the optimal point theater optimal is the point when we would achieve the maximum utility if we had chosen this thrash code and there is also zero threshold O utilities rush code and you can see at this cut off the utility is zero now what does that mean that means if i lower the threshold a little bit and i reach this threshold the utility would be lower but it's still it's still non elective at least so it's not as high as the optimal utility but it gives us a safe point to explore the threshold as i just explained it's desirable to explore the interest of space so it's desirable to lower the thresh code based on your training data so that means in general we want to set the threshold somewhere in this range let's say we can use offer to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of zero utility threshold and the optimal between the threshold now the question is how how should we set alpha when should we deviate a more from the optimal utility point well this can depend on multiple factors and the one way to solve the problem is to encourage this thresholding mechanism to explore up to the zero point and that's a safe point but we're not going to necessarily reach all the way to the zero point but rather we're going to use other parameters to further define alpha and this specifically is as follows so there will be a a beta parameter to control the deviation from the optimal threshold and this can be based on can be accounting for over fitting to the training data let's say an so this can be just adjustment factor but what's more interesting is this gamma parameter here and you can see in this formula game isaac controlling the inference of the number of examples in training data set so you can see in this formula as N which denotes the number of training examples becomes bigger than it would actually encourage less exploration in other words when is very small it would try to explore more and that just means if we have seen few examples we're not sure but we have exhausted the space of interests so we would explore but as we have seen many examples from the user many data points then we feel that we probably don't have to explore more so this gives us a dynamical strategy for exploration the more examples we have seen the less expressing we're going to do so the threshold will be closer to the optimal threshold so that's the basic idea of this approach obvious approach it actually has been working well in some evaluation studies and paracle effective and also can work on arbitrary utility with a proper their lower bound and expressively address this is exploration exploitation tradeoff the kind of uses the O utility threshold point as safeguard for exploring and exploiting tradeoff were not never going to explore further than the zero utility point so if you take the analogy of gambling and you don't want to risk on losing money so it's a safe square root for the conservative strategy for exploration and the problem is of course this approach is purely heuristic and the zero utility lower bound is also often too conservative and there are of course more advanced machine learning approaches that have been proposed for solving these problems and this is the active research area so to summarize there are two strategies for recommender systems or filtering systems one is content based which is looking at the item similarity the other is collaborative filtering which is looking at the user similarity in this lecture we've covered content based filtering approach in the next lecture we're going to talk about the collaborative filtering income hand basically filtering system we generally have to solve several problems related to filtering decisioning and learning etc and such a system character to be built based on a search engine system by adding a threshold mechanism and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user
410	7f9f9b1e-4527-4875-b6f7-4dc0d57ab719	This lecture is a continued discussion of. Discriminative classifiers for text categorization. So in this lecture will introduce yet another discriminative classifier called a support vector machine or VM, which is a very popular classification method, and there has been also shown to be effective for text categorization. So to introduce this classifier, let's also think about the simple case of two categories and we have two public categories, season one and Season 2 here. An we want to classify documents into these two categories and we're going to represent again a document by a feature vector X here. Now the idea of this classifier is do design. Also a linear separator. Here that you see and it's very similar to what you have seen or just for logistic regression. And we're going to also say that if the sign of this function value is positive, then we're going to say the object is in Category 1. Otherwise, we're going to say it's in Category 2, so that makes 0 value. The decision boundary between two categories. So in general in high dimensional space such a zero point corresponds to a hyperplane. I show you a simple case of two dimensional space with just X1 and X2. In this case this corresponds to a line that you can see here. So this is. A line defined by just three parameters "here beta0  Now this line. Is the heading in this direction, so it shows that as we increase X1, X2 will also increase. So know that beta1 and beta2 have different signs or one is negative and there is positive. I so let's just assume that beta one is negative and beta two is positive. Now it's interesting to examine then the data instances on the two sides of this line, so here that there are incidences are visualized as circles for one class and diamonds for the other class. Now one question is to take a point like this one and to ask the question what's the value of this expression or this classifier for this data point. So what do you think? Basically working to evaluate its value by using this function. And as we said, if this value is positive we're gonna say this is in category one, and if it's negative it's going to be in category Two. Intuitively, this line separates these two categories, so we expect the points on one side would be positive and points on the other side would be negative. Or the question is under the assumption that I just mentioned, let's examine a particular point like this one. So what do you think is the sign of this expression? To examine the sign, we can simply look at this expression. Here we can compare this with, let's say, value on the line. Let's say compare this with this point. They have identical X one, but then one has a higher value for its too. Now let's look at the sign of the coefficient for X2, where we know this is a positive. So what that means is that the F value for this point should be higher than the F value for this point on the line. That means this will be positive, right? So we know in general for all the points on this side, the. Functions about it would be positive. And you can also verify all the points on this side would be negative, and so this is how this kind of linear classifier or linear separator can then separate the points in the two categories. So now the natural question is, which linear separate is the best? Now I've again she want lying here that can separate the two classes. And this line, of course, is determined by the vector beta, the coefficients, different coefficient will give us a different line. So we could imagine there are other lines that can do the same job. So gamma, for example, could give us another line that can also separate these instances. And of course there are also lines that won't separate them, and those are bad lines. But the question is when we have multiple lines that can separate the both clauses, which line is the best? In fact, you can imagine there are many different ways of choosing the line. So the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best. But in this VM, we're going to look at another criteria for determining which lines best and this time the criteria is more tide to the classification error. As you will see. So the basic idea is to choose the separator. To maximize the margin. So what is the margin? Well, I choose. So I've shown some daughter lines here to indicate the boundaries of those data points in. In each class and the margin is simply the distance between the line, the separator and the closest points from each class. So you can see the margin of this side is as I've shown here. And you can also define the margin on the other side. And in order for the separate to maximizing the margin, it has to be kind of in the middle of the two boundaries, and you don't want this separator to be very close to one side. And then that inducing intuitively makes a lot of sense. So this is the basic idea of ecfmg. We're going to choose a linear separator to maximize the margin. Now on this slide I've also changed the notation so that I'm not going to use beta. Didn't know the parameters and, but instead I'm going to use W, although W was used to denote the words before. So don't be confused here. W here is actually wait set of weights. And. So I'm also using locates be to denote beta zero, the bias constant. And there are instances do represented as X. And I also use the vector form of multiplication here. So we see transpose of W vector multiplied by the feature vector. So P is a biased constant and W is a set of weights and with one wait for each feature we have M features and so have aim weights and are represented as a vector. An similarly the data instance. Here the text object is represented by also a feature vector of the same number of elements. XI is future value. For example word count. I can you can verify when we multiply these two vectors together, take the dot product that we get the same form of the NIA separate as you have seen before. It's just a different way of representing this. Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM. This way you can. Better connected the slides with some other readings you might do. OK. So. When we maximize the margins of separate, it just means with the boundary of. The separate is only determined by a few data points, and these are the data points that we call support vectors. So here are illustrated to support vectors for one class and two for the other class. At this, porters define the margin basically. And you can imagine once we know which are support vectors, then this center separate line will be determined by them so. The other data points actually don't really matter that much. And you can see if they you change other data points, it won't really affect the margin, so the separate with the stay the same mainly affected by the support vector machines. Sorry it's mainly affected by the support vectors and that's why it is called a support vector machine. OK, so. The next question is of course, how can we set it up to optimize the line? How can we actually find the line? Or the separator. Now this is equivalent to finding values for W&B because they would determine where exactly the separator is. So in the simplest case, the linear osfm is just a simple optimization problem. So again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights W&B. And the classifier will say X is in category one if it's positive. Otherwise it's going to say it's in the other category. So this is our assumption or setup. So in the linear is UVM, we're going to then seek these parameter values to optimize the margins and then the training error. The training laid out would be basically like a in other classifiers we have a set of training points where we know the X vector and then we also the corresponding label, why I? An here we define why I as two values, but these two values are not 01 as you have seen before, but rather negative one and positive one and their corresponding to these two categories as I've shown here. Now you might wonder why we don't define them as zero and one, but instead of having negative 11 and this is purely for mathematical convenience, as you will see in a moment. So the goal of optimization first is to make sure the labeling on training data is all correct. So that just means if Yi, the known label, for instance XI is one we would like this classify value to be large. And here we just choose threshold one here. But if you use another threshold, you can see you can easily affect that constant into the parameter values B&W to make the right hand side. Just one. Now, if, on the other hand, why I is negative one that means it's in a different class then we want this classifier to give us a very small value. In fact a negative value. And we want this value to be less than or equal to negative one. These are the two different instances, different kinds of cases and how can we combine them together now. This is where it's convenient when we have chosen why I as negative one for the other category cause it turns out that we can easily combine the two into one constraint. Why I multiplied by the classifier value must be larger than or equal to 1? An obviously when? Why is just one you see. This is the same as the constraint on the left hand side. But when Yi is negative one you also see a new. This is equivalent to the other inequality, so this one actually captures both constraints in a unified way, and that's a convenient way of capturing these constraints. What's our second goal? That's true. Maximizing margin, right? So we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin. Now the margin can be shown to be related to the magnitude of the weights. The sum of squares of all those weights. So this to have a small value for this expression. It means all the eyes must be small. So we've just assume that we have a constraint for the getting the data on the training set to be classified correctly. Now we also have the objective that's Tide to maximization of margin and this is simply to maximize sorry to minimize W transpose multiplied by W and we often denote this by file W. So now you can see this is basically optimization problem, right? We have some variables to optimize and these are the weights and B and we have some constraints. These are linear constraints and the objective function is a quadratic function of the weights. So this is a quadratic program with linear constraints and there are standard algorithms that are available for solving this problem. And once we solve, the problem, will obtain the weights W&B and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects. Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linearly separable. That means they may not look as nice as you have seen on the previous slide where align can separate all of them. And what would happen if we. Allow some errors. The principle can stay right, so we want to minimize the training error, but try to also maximize the margin. But in this case we have a soft margin because the data points may not be a completely separate bowl. So it turns out that we can easily modify it as VM to accommodate this. So what you see here is very similar to what you have seen before, but we have introduced the extra variables. Cassie I an we in fact will have one for each data instance and this is going to model the error that will allow for each instance. But the optimization problem will be very similar. So specifically, you will see we have added something to the optimization problem. First we have added some. Some error to the constraint so that now we allow. Allow the classifier to make some mistakes here, so this KCI is allowed error if we set KCI to 0, then we go back to the original constraint. We want every instance we classified accurately, but if we allow this to be. Zero, then we allow some errors here. In fact, the one CI is very large. The error can be very, very large, so naturally we don't want this to happen. So we want to then also minimize this CI. So Cassie, I needs to be minimized in order to control the error. And so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the errors as you see here, we simply take a sum over all the instances. Each one has a CI to model the error allowed for that instance an when we combine them together, we basically want to minimize the errors on. All of them. Now you see there's a parameter. See here and that's a constant to control the tradeoff between minimizing the errors and maximizing the region of the margin if C is set to zero, you can see we go back to the original object function where we only maximize margin. And we don't really optimize the training errors and then see I can be set to a very large value to make the constraints easy to satisfy. That's not very good of course, so see should be set to a non 0 value and a positive value. But when she is settled very, very large value would see the objective function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role. So if that happens, what would happen? What would happen is then we will try to do our best to minimize the training errors. But then we're not going to take care of the margin and that affects the generalization capacity of the classifier for future data. So it's also not good. So apparently this parameter C has to be actually set. Carefully, and this is just like in the case of nearest neighbor way you need to optimize the number of neighbors. Here you need to optimize the C and this is the general also achievable by doing cross validation. Basically you look at the empirical data to see what values should be set to in order to optimize the performance. Now with this modification in the problem, is there a quadratic program with linear constraints, so the optimization algorithm can be actually applied to solve this different version of the program? Again, once we have obtained the weights and the bias, then we can have classified. That's ready for classifying new objects. So that's the basic idea of Sven. So to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pros and cons, and the performance might also very different data sets for different problems. Ann One reason is also becausw. The feature representation is very critical an so that these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here. Although there are new machine learning methods like representation learning that can help with learning features. An another common scene is that they might be. Be performing similarly on the data set but with different mistakes and so their performance might be similar, but then the mistakes that make might be different, so that means it's useful to compare different methods for particular problem and then maybe combine multiple methods 'cause this can improve the robustness and they want to make the same mistakes so. And symbol approaches that would combine different methods and tend to be more robust and can be useful in practice. Most techniques that we introduce the use supervised machine learning and which is a very general method. So that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. Problems to solve the categorization problem. To allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. The computers of course here are trying to optimize the combinations of the features provided by human an. As I say that there are many different ways of combining them and they also optimize different objects and functions. But in order to achieve good performance, they all require effective features and also plenty of training data. So as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. So performance is often much more affected by the effectiveness of features and then by the choice of specific classifiers. So feature design tends to be more important than the choice of specific classifier. So how do we design effective features? Well, unfortunately this is very application specific, so there's no really much general thing to say here. But We can. And do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us design features. An another way to figure out effective features is to do error analysis on the categorisation results. You could, for example, look at the which category tends to be confused with each other categories and you can use a confusion matrix to examine the errors systematically across categories, and then you can look into specific instances to see why the mistake has been made and what features can prevent the. This can allow you to obtain. Insights for design new features. So error analysis very important in general, and that's where you can get the insights about your specific problem. And then finally we can leverage some machine learning techniques. So for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you actually trainer for classifier, and sometimes training a classifier would also help you identify which features have high values. And there are also other ways to ensure the sparsity of the model. Meaning to recognize the weights. So for example, the SVM actually tries to minimize the weights on features, but you can further for some features to falsely use only a small number of features. There are also techniques for dimension reduction, and that's to reduce the high dimensional feature space into a lower dimensional space. Typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. LDA can actually help us reduce the dimension of features. Imagine the words are original feature representation, but the representation can be mapped to the topic space representation. Let's say we have K topics, so a document cannot be represented as a vector of justice K values corresponding to the topics. So we can let each topic define one dimension. So we have K dimensional space instead of the original high dimensional space corresponding to words. And this is. Often another way to learn factor features, especially, we could also use the categories to supervise learning of such low dimensional structures. An so the original word features can be also combined with such such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful. Deep learning is a new technique that has been developed in machine learning. It's particularly useful for learning representations, so different learning refers to deep neural network. It's another kind of classifier where you can have intermediate features embedded in the model so that it's highly non linear classifier. An some reason advance has allowed us to train such a complex network effectively. Ann is the technique has been shown to be quite effective for speech recognition, computer vision and recently it has been applied through text as well. It has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization. Although in Texas domain cause words are excellent representation of text content because these are. Humans invention for communication and they are generous sufficient for representing content for many tasks. If there's a need for some new representation, people would have invented a new words and new World. So because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design. The words. As features. But deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. Regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. But there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. For example, if you take a reviews from the Internet, they might have overall ratings. So to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then. We could assume five star reviews are all positive training examples. OnStar negative but of course sometimes in five star reviews. We also mention negative opinions so that rain example is not all of that high quality, but they can still be useful. Another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. So in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. And then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. So you can in fact use the EM algorithm to actually combine both. That would allow you essentially to also pick up a useful words in the unlabeled data. You can think of this in another way. Basically, we can use, let's say a naive Bayes classifier to classify all the unlabeled text documents. And then we're going to assume the high confidence classification results, or actually reliable. Then you certainly have more training data. The cause from the unlabeled data we some are labeled as category ones and more labeled as category two. Although the label is not completely reliable. But then they can still be useful. So let's assume they are actually training label examples and then we combine them with the true training examples. To improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. That involves data that follow very different distributions from what we are working on. But basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data. So for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you. Do that effectively. Here's a suggestion reading an where you can find more details about some of the methods that we have covered.
410	804602d0-76c0-41f5-853b-82556a4ef6c6	This lecture is about the statistical language model. In this lecture we're going to give an introduction to statistical language model. This has to do with how do you model text data with probabilistic models so it's related to how we model query based on a document. We're going to talk about what is the language model and then we're going to talk about the simplest language model called a unigram language model, which also happens to be the most useful model for text retrieval. And finally, we discussed possible uses of language model. What is the language model? It's just a probability distribution over word sequences. So here I show 1. This model gives. The sequence today is Wednesday, a probability of 0.001. It gave today. Wednesday is a very very small probability. Becauses amount for the medical. You can see the probability is given to these sentences or sequences of words can vary a lot depending on the model. Therefore it's clearly context dependent. In ordinary conversation, probably today is Wednesday is most popular among these sentences. But imagine in the context of discussing applied math, maybe the eigenvalues positive would have a higher probability. This means it can be used to represent the topic of the text. The Mortal Council be regarded as a probabilistic mechanism for generating text. And This is why it's also often called a generating model. So what does that mean? We can imagine this is a mechanism. That's visualized hands here as a stochastic system that can generate the sequences of words. So we can ask for a sequence and it's too simple sequence from the device if you want, and it might generate. For example, today is Wednesday. But it could have generated any other sequences. So for example there are many possibilities, right? So this in this sense we can view our data as basically a sample observable from such a generating model. So why is such a model useful? So many because it can quantify the uncertainties in natural language. Where do  unvertainties Come from it. One source is simply the ambiguity in natural language that we discussed earlier in Lab 2. Another source is because we don't have complete understanding. We lack all the knowledge to understand language. In that case there will be uncertainties as well, so let me show some examples of questions that we can answer with the language model that would have interesting application in different ways. Given that we see John and Fields. How likely we see happy as opposed to habit as the next word in a sequence of words? Obviously this would be very useful for speech recognition, because happy and happy it would have similar acoustical sound acoustic signals. But if we look at the language model, will know that John feels happy would be far more likely than John feels habit. Another example, given that we observe baseball 3 times and game once in a news article, how likely is it about sports? This obviously is related to text categorization, an information retrieval. Also, given that a user is interested in Sports News, how likely would the user used baseball in a query? Now this is clearly related to the query likelihood that we discussed in the previous matching. So now let's look at the simplicity language model, called a unigram language model. In such a case. We assume that we generate the text by generating each word independently. So this means the probability of a sequence of words. Will be then the product of the probability of each world. And normally they're not independent. Right, so if you have seen a word like language that would make them far more likely to observe model than if you haven't seen the language. So this assumption is not necessarily true, but we make this assumption to simplify the model. So now the model has precisely in parameters wherein is vocabulary size. We have one probability for each word, and all these probabilities must sum to one. So strictly speaking we actually have N-1 parameters. As I said, text can be assumed to be assembled drawn from this world distribution. So for example, now we can ask the device or the model to stochastic in general words for us instead of sequences. So instead of giving a whole sequence like today's Wednesday, it now gives us just one word and we can get all kinds of words, and we can assemble these words in a sequence. So that would still allows little computer the probability of today's Wednesday as the product of the three probabilities. As you can see, even though we have not asked the model to generate the sequence, it actually allows us to compute the probability for all the sequences. But this model now only needs  N parameters to characterize. That means if we specify all the probabilities for all the words, then the models behavior is completely specified, whereas if we don't make this assumption we would have to specify probabilities for all kinds of combinations of words. In sequences. So by making this assumption, it makes it much easier to estimate these parameters, so let's see a specific example here. Here I show two unigram language models with some probabilities and these are high probability words that are shown on top. The first one clearly suggests a topic of text mining, because the high probability words are all related to this topic. The second one is more related to health. We can then ask the question, how likely will observe a particular text from each of these three models? I suppose we sample words. The former document. Let's say we take the first distribution, which had a simple words. What words do you think it would be generated? Well, maybe text or maybe mining. Maybe another word even food, which has a very small probability, might still be able to show up. But in general, high probability words with likely show up more often. So we can imagine what gender the text that looks like a text mining. In fact, there was a small probability you might be able to actually generate the actual text mining paper that would actually be meaningful, although the probability would be very very small. In the extreme case, you might imagine we might be able to generate the attacks paper text mining paper that would be accepted by a major conference. And in that case, the public in it would be even smaller. But it's a non zero probability if we assume none of the words have non zero probability. Similarly from the second topic, we can imagine we can generate the folder nutrition paper. That doesn't mean we cannot generate this paper from text mining. Distribution. We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mine. So the point here is that given distribution. We can talk about the probability of observing a certain kind of text. Some text will have higher probabilities than others. Now let's look at the problem in a different way. Suppose we now have available of particular document. In this case, maybe the abstract of a text reminding payroll. And we see these world accounts here. The total number of words is 100. Now the question will ask here is estimation question. We can ask the question which model which word distribution has been used to generate this text. Assuming that the text that has been generated by sampling words from the distribution. So what would be your guess? Have to decide what probability is. Text mining etc would have. So pause the video for a second and try to think about your best guess. If you're like a lot of people, you would have guessed that my best guess is. text has a probability of 10 out of 100 because I've seen text 10 times an there are in total 100 words, so we simply not simply normalize these counts. That's in fact the word justified, and your intuition is consistent with mathematical derivation, and this is called a maximum likelihood estimator. In this estimator we assume that the parameter settings are those that would give our observed data the maximum probability. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller. So you can see this has a very simple formula. Basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length. Normalized frequency. Or consequences of this is, of course we're going to assign zero probabilities to unseen words. if we have not oveserve a word there will be no incentive to assign a non zero probability using this approach. Why 'cause that would take away probability mass for these ovbserved words? And that obviously wouldn't maximize the probability of this particular observer text data. But one can still question whether this is our best estimate. Well, the answer depends on what kind of model you want to find, right? This is made. It gives the best model based on this particular data. But if you're interested in a model that can explain the content of the four paper of this abstract, then you might have a second thought, right? So for one thing, there should be other words in the body of that article. So they should not have zero probabilities even though they are not observed in abstract. So we're going to cover this a little more later in discussing the query likelihood retrieval model model. So let's take a look at the some possible uses of this language. One use is simply to use it to represent the topics. So here I show some general English background text. We can use this text to estimate the language model and the model might look like this. So on the top will have those all common words like the is way etc and then we'll see some common words like these and then some very very rare words in the bottom. This is the background language model. It represents the frequency of words in English in general. Right, this is the background model. Now let's look at the another text. Maybe this time we'll look at the computer science research papers. So we have a collection of computer science research papers we do estimation again. Again, we can just use the maximum microarrays better, where we simply normalize the frequencies. Now, in this case we will get the distribution that looks like this. On the top. It looks similar because these words occur everywhere. They are very common, but as we go down we will see words that are more related to computer science, computer software, text, etc. And so, although here we might also see these words, for example computer. But we can imagine the probability here is much smaller than the probability here, and we will see many other words here that would be more common in General English. So you can see this distribution characterizes the topic of the corresponding tents. We can look at the even the smaller text. So in this case, let's look at the text mining paper. Now if we do the same, we have another distribution again. There can be expected to occur on the top, but soon we will see text mining Association clustering. These words have. Relatively higher probabilities, in contrast in this distribution, will text has relatively small probability. So this means again based on different attacks today or we can have a different model and model captures the topic. So we call this document language model and we call this collection language model. And later you will see how they are used in retrieval function. But now let's look at the another use of this model. Can we statistically find what words are semantically related to computer? Now how do we find the such words? Well, our first thought is that let's take a look at the text that match computer so we can take a look at all the documents that contain the word computer. Let's build a language model. We can see what would we see there. Not surprisingly, we see these common words on top. As we always do so in this case, this language model gives us the conditional probability of seeing the world in the context of computer and these common words will naturally have high probabilities. But we also see computer itself and software will have relatively high probabilities. But if we just use this model, we cannot just say all these words are semantically related to computer. So intuitively we would like to get rid of these. Help. These common words. How can we do that? It turns out that it's possible to use, langage model to do that. I suggested you don't think about that. So how can we know what words are very common so that we want to kind of get rid of them? What model would tell us that? "" Maybe you can think about that. So the background language model precisely tells us this. Information tells us what words are common in general. So if we use this background model, we would know that these words are common words in general, so it's not surprising to observe them in the context of computer. Where is the computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software. So then we can use these two models to somehow figure out the words that are related to the computer. For example, we can simply take the ratio of these two probabilities or normalize the topic language model by the probability of the world in the background language model. So if we do that, we take the ratio, will see that, then on the top computer is ranked and then followed by software program. All these words are related to computer. Because they occur frequently in the context of computer, but not frequently in the whole collection. Whereas these common words will not have a high probability. In fact they have ratio about one down there because they are not really related to computer. By taking the sample of text that contains the computer, we don't really see more occurrences of them than in general. So this shows that the even with these simple language models we can do some limited analysis of semantics. So in this lecture we talked about. Language model, which is basically probability distribution over text. We talked about the simplest language model called unigram them model which is also just a word distribution. We talked about the two uses of a language model one is represented topic in a document in the collection or in general the other is rediscovered water associations. In the next lecture, we're going to talk about how, then which model can be used to design retrieval function. Here are two additional readings. The first is textbook on statistical natural language processing. The second is article that has a survey of statistical language models with a lot of pointers to research work.
410	813e04dd-8723-4326-9a07-ddadd23a8632	There are some interesting challenges in threshold learning in the filtering problem. So here I show the historical data that you can collect in a filtering system so you can see the scores and the status of relevance. So the first one it has a score of 36.5 and it's relevant. The second one is non-relevant and etc. Of course we have a lot of documents for which we don't know the status because we have never delivered them to the user. So as you can see here, we only see the judgments of documents delivered to the user, so this is not a random sample, so it's censored data. It's kind of biased. So that creates some difficulty for learning, and secondly there are in general very little labeled data,  and very few relevant data, so it's also challenging for machine learning approaches. Typically they require more training data, and in the extreme case at the beginning we don't even have any label data as well. The system still has to make a decision, so that's a very difficult problem at the beginning. Finally, there is also this issue of exploration versus exploitation tradeoff. Now this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we haven't delivered. So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the users' interests so well. So how do we do that? Well, we could lower the threshold a little bit and do just deliver some near misses to the user to see what the user would respond,  to see how the user would respond to this extra document. And this is the trade-off because on the one hand you want to explore, but on the other hand you don't want to really explore too much 'cause then you would overdeliver non-relevant information. So exploitation means you would exploit what you learned about user. Let's say you know the user is interested in this particular topic so you don't want to deviate that much. But if you don't deviate at all then you don't explore it all. That's also not good. You might miss opportunity to learn another interest of the user. So this is a dilemma. And that's also a difficult problem to solve. Now how do we solve these problems? In general,  I think one can use the empirical utility optimization strategy, and this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide. So you can just compute the utility on the training data for each candidate  score threshold. Pretend what if I cut at this point. What if I can cut at a different scoring threshold point what would happen, what's utility? Since these are training data, we can kind of compute the utility, right? We know their relevance status or we assume that we know relevant status that's based on approximation of clickthroughs. So then we can just choose the threshold that gives the maximum utility on the training data. But this of course doesn't account for exploration that we just talked about. And there is also the difficulty of bias training sample as we mentioned. So in general we can only get upper bound for the true optimal threshold, because the threshold might be actually lower than this. So it's possible that the discarded item might be actually interesting to the user. So how do we solve this problem where we generate and as I said, we can lower the threshold to explore a little bit, so here's one particular approach called better gamma threshold learning. So the idea is following. So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions. And on the y-axis,  We show the utility. Of course this function depends on how you specify the coefficients in the utility function, but we can then imagine. that depending on the cut off position, we will have a utility that means. Suppose I cut at this position and that would be the utility. So we can, for example identify some cutting cut off point. The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold. And there is also zero threshold zero utility threshold, and you can see at this cut off the utility is 0. Now what does that mean? That means if I lower the threshold a little bit and now I reach this threshold, the utility would be lower, but it's still positive it's still non negative at least. So it's not as high as the optimal utility. But it gives us a safe point to explore the threshold. As I just explained, it's desirable to explore the interest space, so it's desirable to lower the threshold based on your training data. So that means in general we want to set the threshold somewhere in this range. Let's say we can use alpha to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold. Now the question is how should we set alpha? And when should we deviate more from the optimal utility point? Well this can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point, but rather we're going to use other parameters to further define alpha, and this specifically is as follows. So there will be a beta parameter to control the deviation from the optimal threshold, and this can be based on for example can be accounting for the overfitting to the training data let's say. And so this can be just adjustment factor. But what's more interesting is this gamma parameter here and you can see in this formula,  gamma is controlling the influence of the number of examples in training dataset. So you can see it in this formula as N which denotes the number of training examples becomes bigger, then it would actually encourage less exploration. In other words, when N is very small, it would try to explore more, and that just means if we have seen few examples we're not sure whether we have exhausted the space of interests. So we would explore. But as we have seen many examples from the user, many data points, then we feel that we probably don't have to explore more. So this gives us a dynamic strategy for exploration, right? The more examples we have seen, the less explosion we're going to do, so the threshold would be closer to the optimal threshold. So that's the basic idea of this approach. Now this approach, it actually has been working well in some evaluation studies empirically effective. And also can work on arbitrary utility with a proper lower bound. And it explicitly addresses the exploration- exploitation tradeoff and it kind of uses the zero utility threshold point as a safeguard for exploration and exploitation tradeoff, we are never going to explore further than the zero utility point. So if you take the analogy of gambling and you don't want to risk on losing money, so it's a safe strategy. The conservative strategy for exploration. And the problem is, of course this approach is purely heuristic. And the zero utility lower bound is also often too conservative. And there are of course more advanced machine learning approaches that have been proposed for solving these problems, and this is the active research area. So to summarize, there are two strategies for recommender systems or filtering systems. One is content based which is looking at the item similarity. The other is collaborative filtering, which is looking at the user similarity. In this lecture, we've covered the content based filtering approach in the next lecture we're going to talk about collaborative filtering.  In content-based filtering system, We generally have to solve several problems related to filtering decision and learning etc. And such a system can actually be built based on a search engine system by adding a threshold mechanism, and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user.
410	83629c6e-4221-4f2e-a108-ff2c58784242	This lecture is about how to do fast search by using inverted index. In this lecture, we're going to continue the discussion of system implementation. In particular, we're going to talk about how to support fast search by using inverted index. So let's think about what a general scoring function might look like. Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form. So the form of this function is as follows. We see this scoring function of document d and query q is defined as first a function of f(a). That's adjustment function that would consider two factors that are shown here at the end f sub d of (d) and f sub q of (q). These are adjustment factors of document and query so they are at the level of a document and query. And then inside of this function we also see there's another function called h. So this is the main part of the scoring function. And these as I just said, are the scoring factors at the level of the whole document and query. For example document lengths. And this aggregate functioning would then combine all these. Now inside this edge function there are functions that would compute the weights of the contribution of a matched query term t(i). So this is g. The function g gives us the weight of a match query term t(i) in document d. And this h function would then aggregate all these weights, so it will for example, take a sum of all the matched query terms. But it can also be a product or could be another way of aggregating them. And then finally, this adjustment function would then consider the document level or query level factors to further adjust the score. For example, documents normalization. So this general form would cover many state of the art retrieval functions. Let's look at how we can score documents with such a function using inverted index. So here's a general algorithm that works as follows. First these query level and document level factors can be precomputed in the indexing time. Of course, for the query we have the computed at query time, but for document, for example, document lengths can be precomputed. And then we'll maintain a score accumulator for each document d to compute h. And h is the aggregation function of all the matched query terms. So how do we do that? Well, for each query term we're going to fetch the inverted list from the inverted index. This would give us all the documents that match this query term. And that includes d1, f1 through dn, fn. So each pair is document ID and the frequency of the term in the document. Then for each entry dj and fj are particular match of the term in this particular document dj. We're going to compute the function g. That would give us something like a TF if weights of this term. So we'll compute the weighted contribution of matching this query term in this document. And then we're going to update the score accumulator for this document. And this would allow us to add this to a accumulator that would incrementally compute the function h. So this is basically a general way to allow us to do computer all functions of this form by using inverted index. Note that we don't have to touch any document that didn't match any query term. But this is why it's fast. We only need to process the document that matched at least one query term. In the end, then, we're going to adjust the score to compute this function Fa, and then we can sort. So let's take a look at the specific example. In this case, let's assume the scoring function is very simple while it just takes the sum of TF. The raw TF. The count of a term in the document. Now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include other weights, like the transformation of TF or document length  normalization or IDF weighting. So let's take a look at a specific example where the queries information security. And I show some entries of the inverted index on the right side information occurred four documents and their frequencies. Also their security occurred in three documents. So let's see how the algorithm works. So first we iterate over all the query terms. An we fetch the first query them what is that? That's information. Imagine we have all these score accumulators to store the scores for these documents. We can imagine there will be allocated but then they will only be allocated as needed. So before we do any weighting of terms, we don't even need a score accumulator. But conceptually we have these score accumulators eventually allocated. Let's fetch the entries from the inverted list for information first. That's the first one. So these score accumulators obviously will be initialized at 0. So the first answer is d1 and 3. 3 is the occurrences of information in this document. Since our scoring function assumes that the score is just a sum of these raw counts, we just need to add 3 to the score accumulator to account for the increase of score due to matching this term information in document d1. And then we go to the next entry. That's d2 and 4 and then we added 4 to the score accumulator of d2. Of course, at this point that we will allocated the score accumulator as needed. And so, at this point we allocated d1 and d2. The next is d3 and we add 1. We allocate another score cumulative for d3 and add 1 to it. And then finding the d4 gets a 5 because the term information occurred five times in this document. OK, so this completes the processing of all the entries in the inverted index for information. It processed all the contributions of matching information in these four documents. So now our algorithm will go to the next query term that security. So we're going to fetch all the inverted index entries for security. So in this case there are three entries and we're going to go through each of them. The first is d2 and 3, and that means security occurs three times in d2. And what do we do? Well, we do exactly the same as what we did for information, so this time we're going to change the score accumulator d2, since it's already allocated. And what we do is to add 3 to the existing value which is 4. So we now get the 7 for d2. d2 score is increased because it matched both the information and security. Go to the next entry. That's d4 and 1 so we would update the score for d4 and again we add 1 to d4. So d4 now goes from 5 to 6. Finally, we process d5 and 3. Since we have not yet allocated a score accumulator for d5. At this point, we're going to allocate 1 for d5, and we're going to add 3 to it, So those scores on the last row are the final scores for these documents if our scoring function is just a simple sum of TF values. Now, what if we actually would like to do length normalization? We can do the normalization at this point for each document. So to summarize this you can see we first processed the query term information. We processed all the entries in the inverted index for this term. Then we processed the security. It's worth thinking about what should be the order of processing here. when we consider query terms. It might make difference, especially if we don't want to keep all the score accumulators. Let's say we only want to keep the most promising score accumulators. What do you think it would be a good order to go through? Would you go. Would you process a common term first or would you process a rare term first? The answer is we should process the rare term first. A rare term would match fewer documents and then the score contribution would be higher because the idea of value will be higher. And then it allows us to touch the most promising documents first, so it helps pruning some non promising ones if we don't need to so many documents to be returned to the user. Right, so those are all heuristics for further improving the accuracy here. You can also see how we can incorporate the IDF weighting so they can easily be incorporated when we process each query term. When we fetch the inverted index, we can fetch the document frequency and then we can compute the IDF. Or maybe perhaps the IDF value has already been precomputed. When we index the documents at that time, we already computed the IDF value that we can just fetch it. So all these can be done at this time, so that would mean when we process all the entries for information, these weights will be adjusted by the same IDF, which is IDF for information. So this is the basic idea of using inverted index for faster search and it works well for all kinds of formulas that are of the general form. This general form covers actually most state of the art retrieval functions. So there are some tricks to further improve the efficiency. Some general techniques include the caching. This is just to store some results of popular queries so that next time when you see the same query, you simply return the stored results. Similarly, you can also store the list of inverted index in the memory for popular term and if the query terms are popular, likely you will soon need to fetch the inverted index for the same term again. So keeping them in the memory would help and these are general techniques for improving efficiency. We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents. We only need to return high quality subset of documents that likely are ranked on the top. For that purpose, we can then prune the accumulators. We don't have to store all the accumulators. At some point, we just keep the highest value accumulators. Another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and to scale up to the web scale, we need a special to have special techniques to do parallel processing and to distribute the storage of files on multiple machines. So here, as here's a list of some text retrieval tool kits, it's not a complete list. You can find more information at this URL on the bottom. Here is the four here Lucene is one of the most popular toolkits that can support a lot of applications and it has very nice support for applications. You can use it to build a search engine application very quickly. The downside is that it's not that easy to extend it and algorithms implemented there also not the most advanced algorithms. Lemur/Indri is another tool kit that does not have such a nice support for application as lucene, but it has many advanced search algorithms. And it's also easy to extend. Terrier is yet another tool kit that also has good support for application capability and some advanced algorithms, so that's maybe in between Lemur or Lucene or maybe rather combining the strength of both, so that's also a useful tool kit. MeTA is the tool kit that we will use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms. And so topic of all those models are implemented there. There are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms. So to summarize, all the discussion about the system implementation, here are the major takeaway points. Inverted index is the primary data structure for supporting a search engine. That's the key to enable faster response to a user's query. And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate so that we can save disk space and can speed up IO and processing of inverted index. In general we talked about how to construct the inverted index when the data can't fit into the memory, and then we talk about the fast search using inverted index. Basically to exploit the inverted index to accumulate the scores for documents matching or query term. And we explore the Zipf's law to avoid attaching many documents that don't match any query term. And this algorithm can for his support a wide range of ranking algorithms. So these basic techniques have great potential for further scaling up using distributed file system, parallel processing and caching. Here are two additional readings that you can take a look if you have time and you're interested in learning more about this. The first one is a classic textbook about the efficiency of Inverted index and compression techniques and how to in general build efficient search engine in terms of the space, overhead and speed. The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.
410	83fb05d8-e122-4a84-b857-df197e4c5662	This lecture is about the query likelihood probabilistic retrieval model. In this lecture we continue the discussion of probabilistic retrieval model. In particular, we're going to talk about the query likelihood retrieval function. In the query likelihood retrieval model. Our idea is to model how likely a user who likes a document would pose a particular query. So in this case you can imagine if a user likes this particular document about the presidential campaign news. Then we can assume the user would use this document as a basis to post a query to try to retrieve this document. So we can imagine the user could use a process. That works as follows, where we assume that the query is generated by sampling words from the document. So for example, a user might pick a word like presidential from this document. And then use this as a query word. And then the user would pick another "word like ""campaign"" and that will be" the second query word. Now this of course is assumption that we have made about how a user would pose a query. Whether user actually followed this process. Maybe a different question, but this assumption has allowed us to formulate characterize this conditional probability. And this allows us to also not rely on the big table that I showed you earlier to use empirical data to estimate this probability. And this is why we can use this idea to them. Further derive retrieval function that we can implement with the program language. So as you see, the assumption that we've made here is each query word is independently sampled and also each word is basically obtained from the document. So now let's see how this works exactly. Well, since we are computing the query likelihood. Then the probability here is just the probability of this particular query, which is a sequence of words. And we make the assumption that each word is generated independently, so as a result, the probability of the query is just a product of the probability of each query word. Now, how do we compute the probability of each query word Well based on the assumption that a word is picked from the document. That the user has in mind. Now we know the probability of each word is just to the relative frequency of the word in the document. So for example, the probability of presidential given the document. Would be just the count of presidential in the document divided by the total number of words in the document or document length. So with this these assumptions, we now have actually simple formula for retrieval, right? We can use this to rank our documents. So does this model work? Let's take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here on the top. So how do we score these documents? It's very simple, right? We just count how many times we have "seen ""presidential"" or how many times we" have seen campaign et cetera and within here for d4 and we have seen presidential twice that's two over the length of Document 4 multiplied by 1 over length of document 4 for probability of campaign. And similarly we can get probabilities for the other two documents. Now if you look at this, these numbers or these formulas for scoring all these documents. It seems to make sense be cause if we assume D3 and D4 have about the same length than looks like we're going to rank D4 above D3, and which is above D2 as we would expect, looks like it did capture the TF heuristic. And so this seems to work well. However. If we try a different query like this one presidential campaign update. Then we might see a problem. What problem? Well think about the update now. None of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining a word like update. Would be what? Would be 0, right? So that caused a problem because we cause all these documents to have zero probability of generating this query. Now, while it's fine to have zero probability for D2 which is non relevant, it's not OK to have zero for D3 and D4, because now we no longer can distinguish them. What's worse, we can't even distinguish them from D2, right? So that's obviously not desirable. Now, whenever we've had such result. We should think about what has caused this problem. So we have to examine what assumptions have been made. As we derive this ranking function. Now, if you examine those assumptions carefully, you would realize what has caused this problem. Right? So take a moment to think about what do you think is the reason why update has zero probability. And how do we fix it? Right, so if you think about this for a moment, you realize that that's because we have made assumption that every query word must be drawn from the document in the user's mind. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document, so let's improve the model and the improvement here is to say that instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model. So I showed model here. We assume that this document is generated using this unigram language model. Now this model. Doesn't necessarily assign zero probability for update. In fact that we consume this model does not assign zero probability for any word. Now if we think in this way, then the generation process is a little bit different. Now the user has this model in mind. Instead of this particular document. Although the model has to be estimated based on the document. So the user can again generate the query using a similar process, namely pick a word. For example, presidential. And another word, campaign. Now the difference is that this time we can also pick a word like update even though update does not occur in the document to potentially generate the query word like update so that a query with update want to have zero probabilities. So this will fix our problem, and it's also reasonable because we're now thinking of what the user is looking for in a more general way. That is unigram language model instead of a fixed document. So how do we compute this query  likelihood? If we make this assumption? Well, it involves 2 steps, right? The first is to compute this model. And we call it the document language model here. For example, I've shown two possible language models here is made based on two documents. And then given a query and I get data mining algorithms. The second step would just compute the likelihood of this query and by making independent assumptions we could then have this probability as a product of the probability of each query word. But we do this for both documents and then we're going to score these two documents and then rank them. So that's the basic idea of this query, likelihood retrieval function. So more generally then, this ranking function would look like the following right here we assume that the query has N words. W one through WN, and then the scoring function. The ranking function is. Probability that we observe this query given that the user is thinking of this document. And this is assumed to be product of probabilities of all individual words. This is based on the independence assumption. Now we actually often score the document for this query by using log of the query likelihood as shown on the second line. Now we do this. To avoid having a lot of small probabilities. We multiply together and this could cause underflow and we might lose precision by transforming the value with a logarithm function. We maintain the order of these documents, yet we can avoid the underflow problem. So if we take logarithm transformation, of course the product that would become a sum as shown on the second line here. So it's a sum over all the query words inside the sum. The value is log of the probability of this word given by the document. And then we can further rewrite the sum into a different form. So in the first sum here. In this sum, We have it all over the query words N query words. And in this sum we have a sum over all the possible words, but we put a count here of each word in the query. Essentially we are only considering the words in the query because if a word is not in the query, the count would be 0. So we're still considering only these N words. But we are using a different form, as if we're going to take sum over all the words in the vocabulary. And of course, a word might occur multiple times in the query. That's why we have a count here. And then this part is log of the probability of the word given by the document language model. So you can see in this retrieval function we actually know the count of the word in the query. So the only thing that we don't know is this document language model. Therefore, we have converted the retrieval problem, include the problem of estimating this document language model. So that we can compute the probability of each query word given by this document. And different estimation methods here would lead to different ranking functions. Now this is just like a different ways to place a document vector in the vector space would lead to a different  ranking function in the vector space model. Here different ways to estimate these document language model would lead to a different ranking function for query likelihood.
410	8717e27a-33fb-4d06-ae68-2e0d915b1568	This lecture is about the generative probabilistic models for text clustering. In this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering So this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. In this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. One is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. So to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. Because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. Here we show that we have input of text collection C and number of topics K and vocabulary V, and we hope to generate as output two things. One is a set of topics denoted by Theta i's. Each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. So this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. Now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. In text clustering, however, we only allow a document to cover one topic. If we assume one topic is a cluster. So. That means if we change the topic definition just slightly by assuming that each document can only be generated by using precisely one topic. Then we'll have a definition of the clustering problem. As shown here. So here the output is changed so that we no longer have the detailed coverage distributions pi ij's, but instead will have cluster assignment decisions. An CI and CI is decision for the document i. And c sub i is going to take a value from one through K to indicate one of the K clusters. And basically tells us document Di is in which cluster. As illustrated here, we no longer have multiple topics covered in each document is precisely one topic, although which topic is still uncertain. There is also a connection with the. Problem of mining. One topic that we discussed earlier. So here again it's a slide that you have seen before. And here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. But we can also consider some variations of the problem. For example, we can consider there are N documents, each covers different topic. So that's N documents and topics. Of course, in this case these documents are independent and these topics also independent. But we can further allow these documents share topics and then. We can also assume that we are going to assume there are fewer topics. The number of documents. So this document must share some topics. And if we have N documents for share k topics, then will again have precisely the document clustering problem. So because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. So the question now is what generating model can be used to do clustering. As in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. So in this case it's a clustering structure. The topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. But if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of K topics? As in the topic model. So let's revisit the topic model again in more detail. So this is a detailed view of two component mixture model and when we have K components it looks similar. So here we see that when we generate a document. We generated each word independently. And we generated each word First make a choice between these distributions with decided to use one of them with probability. So P of theta one is the probability of choosing the distribution on the top. Now we first make this decision regarding which distribution should be used to generate the world, and then we're going to use this distribution to sample word. Now. Notice that in such a generative model. The decision on which distribution to use for each word is independent, so "that means, for example, ""the"" here could" have been generated from the second distribution. Theta two, whereas text is more likely generated from the first one on the top. That means the words in the document could have been generated in general from multiple distributions. Now this is not what we want to see for text clustering. For document clustering where we hope this document will be generated from precisely one topic. So now that means we need to modify the model, but how well, let's first think about why this model cannot be used for clustering, and I just say the reason is because. It has allowed multiple topics to contribute the words to the document. And that causes confusion because we're not going to know which cluster this document is from an it's more importantly, it's violating our assumption about the partitioning of documents in the clusters. If we really have one topic to correspond to one cluster of documents, then we would have a document to be generated from precisely one topic. That means all the words in the document must have been generated from precisely one distribution, and this is not true for such a topic model that we're seeing here, and that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate. All the words in one document. So if you realize this problem, then we can naturally design alternative mixture model for doing clustering. So this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the K word distributions that we have. But this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the all the words in the document. And that means once we have made the choice of the distribution for in generating the first word. We're going to stay with this decision in generating all the other words in the document. So in other words, we only make the choice once. for all. Basically we make the decision once for this document and stay with this to generate all the words. Similarly, if I had chosen the second distribution, theta sub two here, you can see we will stay with this one and then generate the entire document D. Now, if you compare this picture with the previous one, you will see the desicion of. Of using a particular distribution is made of just once for this document. In the case of document clustering. But in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models. But this is obviously also a mixture model, so we can just group them together as one box to show that this is. Model that will give us a probability of a document. Now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. And of course, the main problem in document clustering is to infer. Which distribution has been used to generator a document and that would allow us to recover the cluster identity over document So it would be useful to think about the difference from the topic model, as I have also mentioned multiple times. There are many. Two differences. One is the choice of. Using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. Four different words. The second is that word distribution here is going to be used to generate all the words for a document. But in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. Multiple distribution could have been used to generate the words in the document. It's also think about the special case when one of the one of the probability of choosing a particular distribution is equal to 1. Now that just means we have no uncertainty now. We just stick with one particular distribution. Now in that case, clearly we will see this is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on one document. So that's the connection that we discussed earlier. But now you can see more clearly. So as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model. But once we design model, the next step is to write down the likelihood function. And after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. If you still recall what the likelihood function looks like in PLSA, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data. I in this case, so it's going to be some over these K topics because everyone can be used to generate the document and then inside the sum you can still recall what the formula looks like an it's going to be. A product of two probabilities and one is the probability of choosing a distribution. The other is the probability of observing a particular data point from that distribution. So if you are map, this formula is kind of formula to our problem. Here you will see the probability of observing a document D is basically a sum, in this case over two different distributions. Because we have a very simplified situation of just two clusters. And so in this case you can see it's a sum of two cases. In each case it's indeed the probability of choosing the. Choosing the world distribution. Is theta one or theta two right? And then it's this probability is multiplied by the probability of observing this document from this particular distribution. And if you further expand this probability of observing the whole document, we see that it's product of observing each word X sub i. Here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document. So this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose I am also copying the probability of. topic model with two components here. So here you can see at the formula looks very similar or in many ways they are similar. But there's also some difference. And in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. And that's corresponding to our assumption of 1st make a choice of choosing one distribution and then stay with this distribution to generate all the words. And that's why we had the product inside the sum. The sum corresponds to the choice. right. Now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently. And that's why we have the product outside. But when we generate each each word, we have to make a decision regarding which distribution we use. So we have sum there for each word. But in general, ideas are all mixture models that we can estimate these models by using the EM algorithm as we will discuss more later.
410	885ba47a-e1d9-452f-a8c2-78533d27a5d0	this lecture is about the collaborative filtering in this lecture we're going to continue the discussion of recommender systems in particular we're going to look at the approach of collaborative filtering you have seen this slide before where we talked about the two strategies to answer the basic question where user you like item X in the previous lecture we looked at the item similarity that's compounded based filtering in this lecture we're going to look at the user similarity this is a different strategy called a collaborative filtering so first what is collaborative filtering it is to make filtering decisions for individual user based on the judgments of other users and that is we say we were infer individuals interest or preferences from that of other similar users so the general idea is the following give him a user you were going to first find the similar users you want through U M and there were no predictor use preferences based on the preferences of these similar users you want through your M now the user similarity here can be judged based on their similarity in preferences on a common set of items now here you can see the exact content of item doesn't really matter we're going to look at the only the relation between the users and items so this means this approach is very general it can be applied to any items not just the text log kins so this approach will work well under the following assumptions first users with the same interest where have similar preferences second that users with similar preferences probably share the same interest so for example if the interest of the user is in information retrieval then we can infer the user probably favor signal our papers and so those quite interesting information retrieval researcher probably all failures eli up papers that's something that we make and with this assumption is true then it would help collaborative filtering work well we can also assume that if we see people favor cigar papers then we can infer their interest is probably information retrieval so in these simple examples this seems to make sense and in many cases such assumption actually does make sense so another something we have to make is that there are sufficient in a large number of user preference is available to us so for example if you see a lot of ratings of users for movies and those indicate their preferences on movies and if you have a lot of such data than collaborative filtering can be very effective if not there will be a problem and that's often called a cold start problem that means you don't have many preferences available so the system could not fully take advantage of collaborative filtering yet so let's look at the collaborative filtering problem in a more formal way and so this picture shows that we are in general considering a lot of users showing were showing em users here so you want through U M and we also considering a number of objects let's say N objects in order as oh one through oh N and then we will assume that the users will be able to charge those objects and the user could for example give ratings for those items for example those items could be movies could be products and then the users would give ratings one through five so what you see here is that we have assumed some ratings available for some combinations so some users have watched some movies with their rated those movies they obviously i won't be able to watch all the movies and some users may actually only watch a few movies so this is in general smalls metrics so many item many entries have unknown values and what's interesting here is we could potentially infer the value of element in this metrics based on other values and that's actually the central question in collaborative filtering and that is we assume there's unknown function here if that would map a pair of a user an object to rating and we have observed that some values of this function and we want to infer the value of this function for other pairs that with that don't have values available here so this is very similar to other machine learning problems where we have no values of the function on some training data set and we hope to predict the values of this function on some test there so this is a function approximation and how can we figure out the function based on the observed ratings so this is the setup now there are many approaches to solving this problem and in fact this is a very active research area or reason there are special conferences dedicated to the problem praxis is major conference devoted to the problem
410	8aa65515-aae3-4e5a-bb55-5709b1ea6368	this latter is about the query likely whole probabilistic retrieval model in this lecture with continue the discussion of probabilistic retrieval model in particular we're going to talk about the query like holder retrieval function in the query like a whole retrieval model our idea is the model how likely a user who likes a document with pose a particular query so in this case you can imagine if the user likes this particular document about the presidential campaign news then we can assume the user would use this token net basis to pose a query to try to retrieve the socket so we can't imagine the user could use a process that works as follows where we assume that the query is generated by sampling words from the document so for example a user might pick a war like presidential from this document and then use this as a query ward and then the user would pick another word like the campaign and that will be the second query ward now this of course is a something that we have made about how user would pose a query whether user actually follow this process maybe a different question but this assumption has allowed us to formally characterize this conditional probability and this allows us to also not rely on the big table that i showed you earlier to use empirical data to estimate this probability and this is why we can use this idea to them further deriver retrieval function that we can implement with the program language so as you see the assumption that we made here is each query water is indypendent assembled and also each word is basically obtained from the document so now let's see how this works exactly well since we are computing the query like hold then the probability here is just the probability of this particular query which is a sequence of words an we make the assumption that each word is generated independently so as a result the probability of the query is just a product of the probability of each query world now how do we compute the property in their view query word well based on the assumption that award is picked from the document that the user has in mind then we know the probability over to water is just to the relative frequency of the world in the document so for example the probability of presidential given the document would be just the count of presidential in the document divided by the total number of words in the document or documents so with these assumptions we now have actually simple formula for retrieval right we can use this to rank up document so this model walk let's take a look here are some examples documents that you have seen before suppose now the queries presidential campaign and we see the formula here on the top so how do we solve these documents well it's very simple right we just count how many times have seen presidential times web scene campaign etc and with a here forty four and we've seen president or twice so that's two over the length of document four multiplied by one over length of document four four probability of campaign and similar we can get probabilities for the other two documents now if you look at these numbers or these formulas for scoring all these documents it seems to make sense be cause if we assume T three and T four have about the same length then looks like we're going to rank D four above the three and which is above the two as we would expect looks like it did capture the TF heuristic and so this seems to work well however if we try a different query like this one presidential campaign update then we might see a problem what problem well think about the update now none of these documents has mentioned update so according to our assumption that a user would pick a water from a document to generate a query then the probability of obtaining a warden like update would be what would be zero right so that cause a problem becaus own cause all these documents to have zero probability of generating this query now while it's fine to have zero property and therefore D two which is not relevant it's not OK to have zero four D three and D four because now we no longer can distinguish them what's worse we can't even distinguish them from D two so that's obviously not desirable now when i move has such result we should think about what has caused this problem so we have to examine what assumptions have been made as we divide this ranking function now if you examine those assumptions carefully you would realize what has caused this problem hi so take a moment to think about it what do you think is the reason why update has zero probability and how do we fix it right so if you think about this for moment you realize that that's be cause we have made assumption that every query will must be drawn from the document in users mine so in order to fix this we have to assume that the user could have drawn award not necessary from the document so let's improve the model of the improvement here is to say that well instead of drawing a word from the document let's imagine that the user would actually draw a word from a document model so i show model here here we assume that this document is generated using this unigram language model not this model doesn't necessarily assign zero probability for update in fact that we consume this model does not assign zero probability for any word now if we were thinking this way then the generation process is a little bit different now the user has this model in mind instead of this particular document although the model has to be estimated based on the document so the user can again generate the query using a similar process namely pick award for example presidential and another water campaign now the difference is that this time we can also pick up automatic update even though updated does not occur in the document to pretend you generate a query warden i update so that a query with updated will only have zero probabilities so this would fix our problem and it's also reasonable becaus we're now thinking of what the user is looking for in a more general way that is unigram language model instead of a fixed document so how do we compute this query like if we make this or some well it involves two steps right the first is to compute this model and we call it talking the language model for example drive assume two possible language models here is made based on two documents and then given a query i get data mining algorithms the second step will just compute the likelihood of this query and by making independence assumptions we could then have this probability as a product of the probability of each query word right we do this for boats documents and then we're going to score these two documents and then rank them so that's the basic idea of this query like hold retrieval function so more generally then this ranking function would look like in the following we assume that the query has N words W one throw WN and then the scoring function ranking function is probability error that we observe this query given that the user is thinking of this document and this is assumed it would be product of probabilities of all individual words this is based on the independence assumption now we actually often score the document for this query by using log of the query mike code as you on the signal line now we do this to avoid having a lot of small probabilities be multiplied together and this could cause underflow and we might lose precision by transforming the value is the logarithm function we maintain the order of these documents yet we can avoid the flow problem so if we take logarithm transformation of course the product that would become a sum as you cigna lying here so it's a sum over all the query words inside of the sum value is log over the probability of this word given by the document and then we can further rewrite the sum into a different form so in the first some here i think this is something we have it over all the query words and query words and in this some we have a sum over all the possible words but we put a condom here of each word in the query essentially we are only considering the words in the query becaus if a word is not in the query account would be zero so we are still considering only these end words but we are using a different form as if we're going to take some over all the words in the vocabulary and of course award might occur multiple times in the query that's why we have account here and then this part is log of the probability of the world given by the document language model so you can see in this retrieval function we actually know the count of the world in the query so the only thing that we don't know is this document i am anymore therefore we have convert through the retrieval problem into the problem of estimating this talk in the language model so that we had computer the probability of each query award given by this document and different the estimation methods here would lead to a different ranking functions and this is just like a different ways to place document vector in the vector space with leader to a different ranking function in the vector space model here different noise estimate this document language model will need to do a different ranking function for query like trickle
410	8b3827ae-3009-4a46-afc1-e875c14640d2	This lecture is about using a time series as context to potentially discover causal topics in text. In this lecture we're going to continue discussing contextual text mining. In particular, we're going to look at the time series as a context for analyzing text to potentially discover causal topics. As usual, let's start with motivation. In this case, we hope to use text mining,  to understand the time series. Here what you're seeing is Dow Jones industrial average and stock price curves and you see a sudden drop here. Right, so one would be interested in knowing what might have caused the stock market crash. Well, if you know the background and you might be able to figure out if you look at the time stamp or there are other data that can help us figure it out. But the question here is can we get some clues about this from the companion news stream and we have a lot of news data that are generated during that period. So if you do that, we might actually discover the crash actually happened at the time of September 11 attack. And that's the time when there is a sudden rise of the topic,  about the September 11 attack in news articles. Here's another scenario where we want to analyze the presidential election. This is the time series data from a presidential prediction market. For example, the Iowa electronic market would have stocks for each candidate, and if we believe one candidate will win, then you tend to buy the stock for that candi date causing the price of that candidate to increase. So that's a nice way to actually do survey of people's opinions about these candidates. Suppose you see a sudden drop of price for one candidate. You might also want to know what might have caused the sudden drop. Or in social science study, you might be interested in knowing what mattered in this election, what issues really mattered to people. Now, again in this case, we can look at the companion news stream and ask the question. Are there any clues in the news stream that might provide insight about this. So, for example, we might discover the mention of tax cut has been increasing since that point. So maybe that's related to the drop of the price. So all these cases are special cases of a general problem of joint analysis of text and the time series data to discover causal topics. The input in this case is a time series plus text data that are produced in the same time period- the companion text stream. And this is to see this was different from the standard talking models where we have just the text collection. That's why we set time series here to  serve as context. Now the output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series. For example, whenever the topic is mentioned, the price tends to go down, etc. "Now we call these topics ""causal" "topics""." Of course, they're not, strictly speaking, causal topics or we are never going to be able to verify whether they are causal or there's a true causal relationship here. That's why we put causal in quotation marks. But at least they are correlated topics that might potentially explain the cause, and humans can certainly analyze such topics to understand the issue better. And the output so would contain topics just like in topic modeling. But we hope these topics are not just regular topics.  With these topics, we certainly don't have to explain the data the best in text, but rather they have to explain the data in the text, meaning that they have to represent a meaningful topics in texts semantically coherent topics, but also more important they should be correlated with the external time series that is given as a context. So to understand how we solve this problem, let's first just to solve the problem with the regular topic model. For example, PLSA and we can apply this to text streams. And, with some extension like a CPLSA or contextual PLSA, then we can discover these topics in the collection and also discover their coverage overtime. So one simple solution is to choose the topics from this set that have the strongest correlation with the external time series. But this approach is not going to be very good. Why, because we are restricted to the topics that were discovered by PSA or LDA? And that means the choice of topics will be very limited and we know these models try to maximize light role of the text data, so those topics tend to be the major topics that explain the text data well, and they're not necessarily correlated with time series. Even if we get the best one, the most correlated and the topics might still not be so interesting from causal perspective. So here in this work site here, a better approach, it was proposed, and this approach is called Iterative causal topic model. The idea is to do a iterative adjustment of topics discovered by topic models using time series to induce a prior. So here's an illustration of how this works. How this works. Take the text stream as input and apply regular topic modeling to generate a number of topics. That said, four topics shown here. And then we're going to use the external time series to assess which topic is more causally related or correlated with the external time series, so we can certainly rank them. And we might figure out that topic one and topic four are more correlated and topic two and topic three are not. Now we could have stopped here and that would be just like the simple approaches that I talked about earlier, right, then we can get these topics and call them causal topics. But as I also explained that these topics are likely very good because they are general topics that explained the whole text collection, they're not necessarily the best topics that are correlated with our time series. So what we can do in this approach is to further zoom in the word level. And we're going to look into each word in the top ranked word list for each topic. Let's say we take topic one as the target to examine. We know topic one is correlated with the time series. Or this is the best that we could get from this set of topics so far. And we're going to look at the words in this topic -  the Top words. And if the topic is correlated with the time series, there must be some words that are highly correlated with the time series. So here, for example, we might discover W1 and W3 are positively correlated with time series. But W2 and W4 are negatively correlated. So as a topic and it's not good to mix these words with different correlations, so we can then further separate these words, we're going to get all the red words that indicate positive correlation W1and W3, and we're going to also get another subtopic, if you  want, that represents a negatively correlated words W2 and W4. Now these subtopics, all these variations of topics based on the correlation analysis, are topics that are still quite related to the original topic topic one, but they already deviating because of the use of time series information, to bias selection of words. So they in some sense, well, we should expect so, they are in some sense more correlated with time series than the original topic one because the original topic one has mixed words here, we separate them. So each of these two subtopics can be expected to be better correlated with time series. However, they may not be so coherent semantically. So the idea here is to go back to topic model by using these, each as a prior, to further guide the topic modeling, and that's to say we ask our topic models to now discover topics that are very similar to  each of these two subtopics, and this will cause a bias toward more correlated topics with the time series. Of course, then we can apply topic models to get  another generation of topics, and that can be further ranked based on the time series to select the highly correlated topics. Then we can further analyze the component words in the topic and then try to analyze word level correlation. And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic model discovery. So this whole process is just heuristic way of optimizing causality and coherence. That's our ultimate goal, right? So here you see the pure topic models will be very good at maximizing topical coherence. The topicals will be all meaningful. If we only use causality test or correlation measure then we might get a set of words that are strongly correlated with time series, but they may not necessarily mean anything. They might not be semantically connected, so that will be at the other extreme on the top. Now the ideal is to get the causal topic that's scored high both in topical coherence, and also causal relation. And this approach can be regarded as an alternate way to maximize both dimensions. So when we apply the topic models we are maximizing the coherence. But when we decompose the topic model words into sets of words that are strongly very strongly correlated with time series, we select the most strongly correlated words with the time series we are pushing the model back to the causal dimension to make it better in causal scoring. And then when we apply the selected words, as a prior to guide the topic modeling, we again go back to optimize the coherence because topic models will ensure the next generation of topics to be coherent and we can iterate, iterate, and optimize in this way as shown on this picture. So the only component that you haven't seen in such a framework is how to measure the causality because the rest is just topic model. So let's have a little bit discussion of that. So here we show that. Let's say we have a topic about government response here and then with topic model, we can get the coverage of the topic overtime. So we have a time series Xt. Now we also have our given a time series that represents external information. It's a non text time series,  Yt, is the stock prices. Now the question here is, does Xt cause Yt? Or in other words, we want to match the causality relation between the two. Or maybe just measure the correlation of the two? There are many measures that we can use in this framework. For example, Pearson correlation is a commonly used measure and we can consider time lag here so that we can try to capture causal relation using somewhat past data, using the data in the past, to try to correlate that with the data on points on why that represents the future, for example, and by introducing such lag we can hopefully it captures on causal relation by even using correlation measures like Pearson correlation. But a commonly used measure for causality here is Granger causality test. And the idea of this test is actually quite simple. Basically you're going to have auto regressive model to use the history information of Y to predict itself. And this is the best we could do without any other information. So we're going to be able to use such a model. And then we're going to add some history information of X into such a model to see if we can improve the prediction of Y. If we can. If we can do that with a statistically significant difference, then we just say X has some causal influence on Y or otherwise we wouldn't have caused the improvement of prediction of Y. If, on the other hand, the difference is insignificant, and that would mean X does not really have a causal relation with Y, and so that's the basic idea. Now we don't have time to explain this in detail, so you could read, but you would read this cited reference here to know more about this measure. It's very frequently used measure, it has many applications. So next, let's look at some sample results generated by this approach. Here the data is New York Times and in the time period of June 2000 through December of 2011. And here the time series we used is stock prices of two companies, American Airlines and Apple, and the goal here is to see if we inject some time series bias or time series context whether we can actually get topics that are biased towards these time series. Imagine if we don't use any input, we don't use any context, then the topics from New York Times discovered by PLSA would be just general topics that people talk about in news. Those major topics in the news event. But here you see, these topics are indeed biased toward each time series. In particular, if you look at the underlined words here in the American Airlines result and you see airlines, Airport, Air, United, trade, terrorism, etc. So it clearly has topics that are more correlated with the external time series. On the right side you see some of the topics are clearly related to Apple. Right. So you can see computer technology, software, Internet, com, web etc. So that just means the time series has effectively served as a context to bias the discovery of topics. From another perspective these result help us what people have talked about in each case, so in the not just the people what people have talked about, but what are some topics that might be correlated with their stock prices? And so these topics can serve as a starting point for people to further look into the issues and to find the true causal relations. Here are some other results from analyzing presidential election Time series. And the time series data here is from Iowa electronic market. And that's a prediction market and the data is the same. New York Times from May 2000 to October 2000, for 2000 presidential campaign election. Now what you here are the top 3 words insignificant topics from New York Times. And if you look at these topics and they are indeed quite related to the campaign. Actually, here the issues are very much related to the important issues of this presidential election. Now here I should mention that the text data has been filtered by using only the articles that mention these candidate names. At. So it's a subset of these news articles, very different from the previous experiment. But the results here clearly show that the approach we can uncover some important issues in that presidential election. So tax cut, oil, energy, abortion and gun control are all known to be important issues in that presidential election, and that was supported by some literature in political science. And also it was discussed in Wikipedia. So basically the results show that the approach can effectively discover possibly causal topics based on the time series data. So there are two suggested readings here. One is the paper about this iterative topic modeling with time series feedback, where you can find more details about how this approach works. And the second one is reading about Granger causality test. So in the end, let's summarize the discussion of text based prediction. Now, Text based prediction is generally very useful for big data applications that involve text, because, you can help us infer new knowledge about the word and the knowledge can go beyond what's discussed in the text. As a result, they can also support optimizig of our decision making, and this has widespread applications. Text data is often combined with non text data for prediction, because for this purpose, for the prediction purpose, we generally would like to combine non text data and text data together as much clues as possible for prediction. And so as a result, joined analysis of text and non text is very necessary. And it's also very useful now when we analyze text data together with non text data we can see they can help each other. So non text data provide context for mining text data and we discussed a number of techniques for contextual text mining. And on the other hand, text data can also help interpret the patterns discovered from non text data and this is called a pattern annotation. And in general this is a very active research topic and there was there new papers being published and there are also many open challenges that have to be solved.
410	8eaf2971-31ff-40b7-9fc6-b91c7637f916	This lecture is about a mixture of unigram language models. In this lecture we will continue discussing probabilistic topic models. In particular, we're going to introduce a mixture of unigram language models. This is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. So if you want to solve the problem. It will be useful to think about why we end up having this problem. Well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. So in order to get rid of them, that would mean we have to do something different here. In particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution. So one natural way to solve the problem is to think about using another distribution to account for just these common words. This way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. This way our target is the topic theta here would be only generating the content words that characterize the content of the document. So how does this work? It's just a small modification of the previous set up where we have just one distribution. Since we now have two distributions, we have to decide which distribution to use when we generate the word, but each word will still be sampled from one of the two distributions, right? So text data is still generating the same way. Namely, we're going to generate a one word at each time. An eventually we generated a lot of words. When we generate the word, however, we're going to 1st decide which of the two distributions to use, and this is controlled by another probability: probability of theta sub D and probability of theta sub B here. So this is the probability of selecting the topic word distribution. This is the probability of selecting the background word distribution denoted by Theta sub B. Now in this case I just give example where we can set both to .5. So if you can do basically flip a coin a fair coin to decide which one to use. But in general these probabilities don't have to be equal, so you might bias towards using one topic more than the other. So now the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. Let's say the coin shows up as head, which means we're going to use the topic word distribution. Then we're going to use this word distribution to generate a word. Otherwise we might be going through this path. And we're going to use the background word distribution to generate the word. So in such a case we have a model that has some uncertainty associated with the use of a word distribution. But we can still think of this as a model for generating text data and such a model is called a mixture model. So now let's see. In this case, what's the probability of observing the word w? "Now here I showed some words like ""the""" "and ""text"", so as in all cases, once we" set up the model, we're interested in computing the likelihood function. The basic question is, so what's the probability of observing a specific word here? Now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. Therefore it's a sum over these two cases. The first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of Theta sub D, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model. Both events must happen in order to "observe ""the""." We first must have chosen the topic of  and then we also have to actually "have sampled the word ""the"" from the" distribution and similarly the second part accounts for a different way of generating the word from the background. Now obviously the probability of text the same is all similar, right? So we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution. Now later you will see this is actually general form, so you might want to make sure that you have really understood this expression here. And you should convince yourself that this is indeed the probability of observing text. So to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. And in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. So the basic idea of a mixture model is just to treated these two distributions together as one model. So I use the box to bring all these components together. So if you view this whole box as one model, it's just like any other generative model. It would just give us the probability of a word. But the way that determines this probability is quite different from when we have just one distribution. And this is basically a more complicated mixture model. Sorry, more complicated model than just one distribution, and it's called a mixture model. So as I just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. The illustration that you have seen before, which is dimmer now is just the illustration of this generation model. So mathematically, this model. This is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. The form you're seeing now is more general form than. what you have seen in the calculation earlier. I just used a simple w to denote any word, but you can still see. This is basically the first sum. Like And this sum is due to the fact that the word can be generating multiple ways. Two ways in this case. At inside sum, each term is a product again of two terms. And the two terms are ,first, the probability of selecting a component like Theta sub D. Second, the probability of actually observing the word from this component model. And so this is a very general description of, in fact, all the mixture models. And I just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. So now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? Well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover? Well, these are represented by our parameters, and we have two kinds of parameters. One is the two word distributions. Those are two topics and the other is the coverage of each topic in each. The coverage of each topic and this is determined by probability of Theta sub D. and probability of Theta sub B. Note that they sum to one. Now what's interesting is also to think about the special cases, like when we set one of them to one. What would happen? Well, the other would be 0, right? And if you look at the likelihood function. It will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0. So in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. So to summarize, and we talked about the mixture of two unigram language models. And the data we consider here is just still 1 document. And the model is a mixture model with two components: two unigram language models. Specifically, Theta sub D which is intended to denote the topic of document D and Theta sub B which is representing a background topic that we can set to attract the common words. Because common words would be assigned high probabilities in this model. So the parameters can be collectively called a Lambda, which I show here again, and you can again think about the question about how many parameters are we talking about exactly. This is usually good exercise to do because it allows you to see the model index and to have a complete understanding of what's going on in this model and we have mixing weights of course also. So what is the likelihood function look like? It looks very similar to what we had before, so for the document, first, it's a product of all the words in the document exactly the same as before. The only difference is that inside here now it's a sum instead of just one, so you might recall before we just had this one. But now we had this sum because of the mixture model and because of the mixed model We also have to introduce the probability of choosing that particular component distribution. And so this is just another way of writing it again by using a product over all the unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. And the maximum likelihood estimator  is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. One is the word probabilities in each topic must sum to one, the other is the choice of each topic must sum to one.
410	8ef16cab-abef-4a2d-adfd-25dc097fe2a1	This lecture is about the feedback in text retrieval. So in this lecture we're going to continue the discussion of text retrieval methods. In particular, we're going to talk about the feedback impacts retrieval. This is a diagram that shows the retrieval process. We can see the user with typing the query. And then the query would be sent to a retrieval engine or search engine. And the engine will return results. These results will be shown to the user. After the user has seen these results. The user can actually make judgments. So for example, the user had said this is good and this document is not very useful. This is good again, etc. Now this is called a relevant judgment or relevance feedback because we've got some feedback information from the user based on the judgments. This can be very useful to the system we learn. What exactly is interesting to the user. So the feedback module would then take this as input. And also use the document collection to try to improve ranking. Typically it would involve updating the query so the system can now rank the results more accurately for the user. So this is all relevance feedback. The feedback is based on relevance judgments made by the users. Now these judgments are reliable, but the users generally don't want to make extra effort unless they have to. So the downside is that it involves some extra effort by the user. There was another form of feedback called pseudo relevance feedback or blind feedback, also called automatic feedback. In this case you can see. Once the user has got without all in fact we don't have to involve users so you can see there's no user involved here. And we simply assume that the top ranked documents to be relevant. Let's say we can assume top ten as relevant. And then we would then use these. Assumed documents to learn and to improve the query. Now you might wonder how could this help if we simply assume the top ranked documents to be relavant well. You can imagine these top ranked documents are actually similar to relevant documents, even if they are not relevant. They look like relevant documents, so it's possible to learn some related terms to the query from this set. In fact, there you may recall that we talked about using language model to analyze word association to learn "related words to the word ""computer""" right, and then what we did is we first use computer to retrieve all the documents that contain computer. So imagine now the query here is a computer right? And then the results will be those documents that contain computer and what we can do then is to take the top N results. They can match computer very well and we're going to count the terms in this set. And then we're going to then use the background language model to choose the terms that are frequent in this set, but not frequent In the whole collection. So if we make a contrast between these two, what we can find these that would learn some related terms to the word computer? As we have seen before and these related words can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software. So this is the effective for improving the search result. But of course, pseudo relavance feedback is completely unreliable. We have to arbitrary set a cut off, so there's also something in between called implicit feedback. In this case, what we do is we do involve users, but we don't have to have asked users to make judgments instead, or even the observe how the user interacts with the search result. In this case, we're going to look at the clickthroughs so the user clicked on this one and the user viewed this one and the user skipped this one and the user view this one again. Now this also is a clue about whether a document is useful to the user. And we can even assume that we're going to use only the snippet here in this document. The text that's actually seen by the user instead of the actual document of this entry. The link there, let's say in web search may be broken, but then it doesn't matter if the user tried to fetch this document, because of the display, the text. We can assume these display. The text is probably relevant, is interesting to user. So we can learn from such information and this is called implicit feedback. And we can again use the information to update the query. This is a very important technique used in modern search engines. Will think about the Google and Bing and they can collect a lot of user activities while they're serving us, right? So they would observe what documents we click on, what documents will skip, and this information is very valuable and they can use this to improve the search engine. So to summarize, we talked about the three kinds of feedback here. Relevance feedback, where the user makes explicit judgments. It takes some user effort, but the judgment the information is reliable. We talk about pseudo feedback where we simply assume top-ranked documents to be relevant. We don't have to involve the user, therefore we could do that actually, before we return the results to the user. And the third is implicit feedback, where we use click clues. We don't we involved users, but the user doesn't have to make explicit effort to make judgment.
410	90bca100-98a3-41f9-b699-80587ff61294	this lecture is about the text retrieval problem this picture shows our overall plan for lectures [test] in the last lecture we talked about the high level strategies for text to access we talked about push [test] versus pull search engines are the main tools for supporting the poor mode starting from this lecture we're going to talk about how search engines work in detail so first is about the text retrieval problem we're going to talk about the three things in this lecture first were define text your table second working to make comparison between text retrieval and the related task database retrieval finally we're going to talk about the document selection versus document ranking as two strategies for responding to a user 's query so what is texting retrieval it should be a task that's familiar to most of us becaus we're using web search engines all the time so text retrieval is basically a task where the system would respond to a user 's query with relevant documents basically to support the query as one way to implement the poor mode of information access so the scenario is the following you have a clashing off text documents these documents could be all the web pages on the web all the literature articles in digital library or maybe all the text files in your computer a user were typically give a query to the system to express' information need and then the system would return relevant documents to users relevant documents refer to those documents that are useful to the user who typing in the query now this task is often called the information retrieval but literally information retrieval with broader include retrieval of other non textual information as well for example audio video etc it's worth noting that text retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data so for example current images search engines actually it match the user 's query with the companion text data of image this problem is also called search problem and the technology is often called a search technology industry if you have taken cause in databases will be used for the pause the lecture at this point and think about the differences between text retrieval and database retrieval now these two tasks are similar in many ways but there are some important differences so spend a moment to think about the differences between the two think about the data and information manage it by a search engine versus those managed by a database system think about the difference between the queries that you typically specify for elevated system versus the queries that are typed it typed in by users on the search engine and then finally think about the answers what's the difference between the two OK so if we think about the information or there are managed by the two systems will see that intex retrieval the data is unstructured is free tax but in databases they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages etc in unstructured text it's not obvious what are the names of people manage in the in the text becaus of this difference we also see that text information tends to be more ambiguous and we talked about that in the natural language processing lab too whereas in databases that data tended to have well defined the semantics there is also important difference in the queries and this is a positive due to the difference in the information or data so text queries tend to be ambiguous whereas in database search the queries are typically well defined think about the SQL query that would clear this specify what records to be returned so it has very well defined semantics keyword queries or natural language queries tend to be incomplete also in that it doesn't really folder specify what dawkins should be retrieved where is in the database search the SQL query can be regarded as a computer specification for what should be returned and be cause of these differences that ends would be also different in the case of text retrieval we're looking for random in the documents in the database search we are retrieving records or match records with the SQL query more precisely now in the case of text retrieval what should be the right answers to a query is not very well specified as we just discussed so it's unclear what should be the right answers to a query and this has very important consequences and that is text retrieval is an empirically defined problem so this is a problem be cause if it's empirically defined then we cannot mathematically prove one method is better than another method that also means we must rely on empirical evaluation involving users to know which method works better and that's why we have an lecture actually more than one lectures to cover the issue of evaluation be cause this is a very important topic for search engines without knowing how to evaluate algorithm a properly there's no way to tell whether we have got a better algorithm or whether one system is better than another so now let's look at the problem in a formal way so this slide shows a formal formulation of the text retrieval problem first we have our vocabulary set which is just a set of words in a language now here we're considering just one language but in reality on the web there might be multiple natural languages we have text there are in all kinds of languages but here for simplicity we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language although there is important difference the principles and methods a very similar next we have the query which is a sequence of words and so here you can see the query is defined as a sequence of words each Q sub i is word in the vocabulary a document is defined in the same way so it's also a sequence of words and here the supply J is also a word in the vocabulary now typically the documents are much longer than queries but there are also cases where the documents may be very short so you can think about what might be a example of that case i hope you can think of twitter search write tweets are very short but in general documents are longer than the queries now then we have a collection of documents and this collection can be very large so think about the web they could could be very large and then the goal of text retrieval is it'll find the set of relevant documents which we denote by R of Q becaus it depends on the query and this is in general a subset of all the documents in the collection unfortunately this set of relevant documents is generally unknown and user dependent in the sense that for the same query typing by different users they expect the relevant documents may be different the query given to us by the user is only a hint on which document should be in this set and indeed the user is generally unable to specify what exactly should be in this set especially in the case of web search where the collection is so large the user doesn't have complete knowledge about the whole collection so the best search system can do is to compute approximation of this relevant document set so we denoted by our prime of Q so formally we can see the task is to compute this our prime of Q approximation of the relevant documents so how can we do that now imagine if you are now asked to write a program to do this what would you do now think for a moment so these are the your input the query the documents and then you have to compute the answers to this query which is a set of documents that would be useful to the user so how would you solve the problem out in general there are two strategies that we can use like the first strategies will document selection and that is we're going to have a binary classification function or binary classifier that's a function that would take a document and query as input and then give zero or one as output to indicate whether this document is relevant to the query or not so in this case you can see the document the the red when the document is set is defined as follows it basically all the documents that have a value of one by this function and so in this case you can see the system must decide if a document is relevant or not basically it has to say whether it's one or zero and this is called absolutely redness basically it needs to know exactly whether it's going to be useful to the user alternatively there's another strategy called document ranking now in this case the system is not going to make a call weather document is relevant or not but rather the system is going to use real value function F here that would simply give us a value that would indicate a which document is more likely relevant so it's not going to make a call weather this document is relevant or not but rather it would say which document is more likely relevant so this function then can be used to rank the documents and then we're going to let the user decide where to stop when the user looks at the documents so we have a threshold see dark here to determine what documents should be in this approximation set and we're going to assume that all the documents that are ranked above this threshold or in this set becaus in the fact these are the documents that we deliver to the user and theater is cut off terminal by the user so here we've got some collaboration from the user in some sense be cause we don't really make a cut off and then use a kind of helped the system make a cut off so in this case the system only needs to decide if one document is more likely relevant there another and that is it only needs to determine relative relevance as opposed to absolute the randomness now you can probably already sense that relevant relative relevance would be easier to determine their absolute relevance becaus in the first case we have to say exactly weather document is relevant or not and it turns out that ranking is indeed that generally preferred to document selection so let's look at this these through series in more detail so this picture shows how it works so on the left side we see these documents and we use the pluses to indicate the relevant documents so we can see that rule relevant documents here consists this set of true relevant documents consist of these classes these documents and with the document is selection function we're going to basically classify them into two groups relevant documents and non relevant ones of course the classifier will not be perfect so it will make mistakes so here we can see in the approximation of the relevant documents we have got some non relevant documents and similarly there is a relevant document let miss cat as wide as non relevant in the case of document ranking we can see the system seems they simply ranks all the documents in the descending order of the scores and then we're going to let the users stop wherever the user wants to stop so if a user wants to examine more documents then the user would go down the list to examine more and stop at the lower position but if the user only wants to read a few relevant documents the user might stop at the top position so in this case the user stops at D four so in effect we have delivered these four documents to our user so as i said ranking is generally preferred and one of the reasons is becaus the classifier in the case of documents that action is unlikely accurate whi becaus the only crew is usually the query but the query may not be accurate in the sense that it could be overly constrained for example you might expect the relevant documents to talk about all these topics you by using specific vocabulary and as a result you might match no random documents becaus in the collection know authors have discussed the topic using these vocabularies so in this case will see there is this problem of no relevant documents to return in the case of overly constrained query on the other hand if the query is under constrained for example if the query does not have sufficient that discriminate if words will find the relevant documents you may actually end up having over delivery and this is when you thought these words might be sufficient to help you find the relevant documents but it turns out that they are not sufficient and there are many distraction documents using similar words right so this is the case of over delivery unfortunately it's very hard to find the right position between these two extremes why be cause when the user is looking for the information in general the user does not have a good knowledge about the information to be found and in that case the user does not have a good knowledge about watt vocabularies will be used in those rare in documents so it's very hard for user to pre specify the right level of of constraints even if the classifier is accurate we also still want to rank these relevant documents be cause they are generally not equally relevant relevance is often a matter of degree so we must prioritize these documents for a user to examine and this note that this prioritization is very important becaus a user cannot digest all the contents at once the user general would have to look at each document sequentially and therefore it would make sense to feed it users with the most relevant documents and that's what ranking is doing so for these reasons ranking is generally preferred now this preference also has a theoretical justification and this is given by the probability ranking principle in the end of this lecture there is a reference for this this principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions first the utility of the document into a user is independent of the utility of any other document second a user would be assumed that who browser results sequentially now it's easy to understand the why these two assumptions are needed in order to justify for the ranking strategy becaus if the documents are independent then we can evaluate the utility of each document separately and this would allow us to compute the score for each document independently and then we're going to rank these documents based on those scores the second assumption is to say that the user would indeed a follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially then obviously the ordering would not be optimal so under these two assumptions we can theoretically justify the ranking strategy is in fact the best you could do now i've put one question here do these two assumptions hold now i suggest you to pause the lecture for a moment to think about these but can you think of some examples that would suggest these assumptions aren't necessarily true now if you think for a moment you may realize none of the assumptions is actually true for example in the case of independence assumption we might have identical documents that have similar content or exactly the same content if you look at each of them alone each is redmond but if the user has already seen one of them we assume it's generally not very useful for the user to see another similar or duplicated one so clearly the utility of the document is dependent on other documents that the user has seen in some other cases you might see a scenario where one document that may not be useful to the user but when three particular documents are put together they provide the answer to the user 's question so this is a collective redness and that also suggests that the value of a document might depend on other documents sequencer browsing generally would make sense if you have a ranked list there but even if you have a ramp list there was evidence showing that users don't always just go strictly sequentially through the entire list there sometimes would look at the bottom for example or skip some and if you think about it the more complicated interface that we could possible to use like two dimensional interface where you can put additional information on the screen then sequential browsing is a very restrictive assumption so the point here is that none of these assumptions is really true but nevertheless the probability ranking principle established some sort of the foundation for ranking as primary task for search engines and this has actually been the basis for a lot of research work information retrieval and many algorithms have been designed that based on this assumption despite that the assumptions of nessus ritual and we can address this problem by doing post processing of a ranked list for example to remove redundancy so to summarize this lecture the main points that you can take away the following first text retrieval is an empirical defined problem and that means which algorithm is better must be judged by the users second document ranking is generally preferred and this will help users prioritize examination of search results and this is also the bypass the difficulty in determining absolute relevance becaus we can get some help from users in determining where to make the cut off it's more flexible so this further suggests that the main technical challenge in designing a search engine is redesign effective ranking function in other words we need to define what is the value of this function F on the query and document pair a whole are designed such a function is the main topic in the following lectures there are two suggest to the additional readings the first is the classical paper on probability ranking principle the second is a must read for anyone doing research information tribble it's classical IR book which has excellent coverage of the main researcher results in early days up to the time when the book was written chapter six of this book has indexed discussion of the probability ranking principle and probabilistic retrieval models in general
410	93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee	This lecture is about the expectation maximization algorithm, also called the EM algorithm. In this lecture, we're going to continue the discussion of probabilistic topic models. In particular, we're going to introduce the EM algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. So this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. So we are interested in computing this estimate. And we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all the other parameters are known. So the only thing unknown is this word probabilities are given by theta sub d update. And in this lecture were going to look into how to compute this maximum likelihood  estimator. Now let's start with the idea of separating the words in the text data into two groups. One group would be explained by the background model, the other group would be explained by the Unknown topic word distribution After all, this is the basic idea of mixture model. But suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. On the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed to be from the topic word distribution. If we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? If you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and normalize them. So indeed this problem will be very easy to solve. If we had known which words are from which distribution precisely. And this is in fact Making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the Single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub I. And that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. This idea, however, doesn't work, because we in practice don't really know which word is from which distribution. But this gives us the idea of perhaps we can guess which word is from which. distribution. Specifically, given all the parameters can we infer the distribution the word is from So let's assume that we actually know tentative probabilities for these words in theta sub D. So now all the parameters are known for this mixture model. And now let's consider word like  text. So the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub B? So in other words, we want to infer which distribution has been used to generate this text. Now, this inference process is a typical Bayesian inference situation where we have some prior about These two distributions so can you see what is our prior here? Well the prior here is the probability of each distribution, right? So the prior is given by these two probabilities. In this case the prior is Saying that each model is equally likely, but we can imagine perhaps a different prior possible. So this is called prior because this is our guess of which distribution has been used to generate the world before we even observe the word. So that's why we call it prior. if we don't observe the word or we don't know what word has been observed, our best guess is just say well. They are equally likely. Alright, so it's just a flipping a coin. Now in Bayesian inference, we typically then would update our belief after we have observed evidence. So what is evidence here? While the evidence here is the word text. Now that we are interested in the word text, so text can be regarded as evidence. And in the if we use Bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basically the probability of the word text from each distribution and we see that in both cases text is possible that even in the background it is still possible. It just has a very small probability. So intuitively, what would be your guess? So in this case Now, if you're like many others, you will guess text is probably from theta sub d is more likely from  theta sub d, why? And you will probably see that it's because. Text has a much higher probability here. By the theta sub d, than by the background model, which has a very small probability. And by this we are going to say text is more likely from theta sub d. So you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution. We are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so. We're going to choose word that has a higher likelihood. So in other words, we're going to compare these two probabilities. Of the word given by each distributions. But our guess must also be affected by the prior, so we also need to compare these two priors why? because imagine if we adjust these probabilities, we're going to say the probability of choosing a background model is almost 100%. Now if we have that kind of strong prior, then that would affect your guess. You might think well, wait a moment, maybe text could have been from the background as well, although the probability is very small here. The prior is very high. So in the end we have to combine the two and the bayes formula provides provides us a solid and principled way of making these kind of guess to quantify that. So more specifically, let's think about the probability that this word text has been generated. In fact from theta sub D, The in order for texture to be generated from theta sub d two things must happen first. The theta sub d must have been selected, so we have the selection probability here, and Secondly, we also have to actually have observed text from the distribution. So when we multiply the two together, we get the probability that text has in "fact has been generated from theta sub d  Similarly, for the background model an. The probability of generating text is another product of similar form. We also introduced a latent variable Z here to denote whether the word is from the background or the topic. When Z is zero, it means it's from the topic theta sub d when it's one, it means it's from the background theta sub b. So now we have the probability that text is generated from each. Then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub b. And equivalently, the probability that Z is equal to 0 given that the observed evidence is text. So this is Application of Bayes rule. But this step is very crucial for understanding the EM algorithm. Because if we can do this, then we would be able to 1st initialize the parameter values Somewhat randomly, and then we're going to take a guess of these Z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply Bayes rule to infer which distribution is more likely to generate Each word and this prediction essentially helped us to separate words from the two distributions, although we can't separate them for sure, but we can separate them probabilistically as shown here.
410	944e5a7f-6bd7-42ec-be49-afb0d307fb85	This lecture is about the feedback in the vector space model. In this lecture we continue talking about feedback in text retrieval. Particularly, we're going to talk about feedback in the vector space model. As we have discussed before, in the case of feedback. The task of a Text Retrieval system is to learn from examples to improve retrieval accuracy. We will have positive examples. Those are the documents that are assumed to be relevant or judgement be relevant. Or the document that are viewed by users. We also have negative examples. Those are documents known in non-relevant, and they can also be the document that escaped by users. The general method in the vector space model for feedback. Is to modify our query vector and we want to place the query vector in a better position to make the accurate. And what does that mean exactly? If we think about the query vector, that would mean we have to do something to the vector elements. And in general, that would mean we might add new terms. We might adjust weights of all terms or assign weights to new terms. And as a result, in general the query will have more terms, so we often call this query expansion. The most effective method in the vector space model for feedback is called Walker feedback, which was after he proposed at several decades ago. So the idea is quite simple. We illustrate this idea by using a 2 dimensional display of all the documents in the collection and also the query vector. So now we can see the query vector is here in the center. And these are all the documents. So when we use the query vector and use a similarity function to find the most similar documents, we are basically drawing a circle here and then. These documents would be basically the top ranked of the documents. And these pluses are relevant documents. And these are relevant documents. For example is relevant, etc. And then these minuses are negative documents like this. So our goal here is trying to move this query vector to some position to improve the retrieval accuracy. By looking at this diagram. What do you think? Where should we move the query of after so that we can improve the retrieval accuracy? Intuitively, where do you want to move the query vector? Now, if you want to think more, you can pause the video. Now, if you think about this picture, you can realize that in order to work well in this case you want to query about that to be as close to the positive vectors as possible. That means ideally you want to place the query vector somewhere here, or you want to move the query vector closer to this point. Now, So what exactly is this point? Well, If you want these relevant documents to be ranked on the top, you want this to be in the center of all these relevant documents, right? Because then if you draw a circle around this one, you get all these relevant documents. So that means we can move the query vector towards the centroid of all the relevant locking vectors. And this is basically the idea of Rocchio you. Of course you can consider the centroid of negative documents, and we want to move away from the negative documents. Not geometrically, we're talking about the movie vector closer to some other bath and away from other vectors. Algebraically, it just means we have this formula. Here you can see this is original query vector. And, this average basically is the centroid vector of relevant documents. When we take the average of these vectors, then we are computing the centroid of these vectors and similarly this is the average non relevant document vectors. So it's essentially of non relevant documents and we have these three parameters here of our alpha beta and gamma they're controlling the amount of movement. When we add these two vectors together, we are moving the query  closer to the centroid. I said add them together when we subtract this part. We kind of move the query vector away from that Central. So this is the main idea of Rocchio feedback and after we have done this we will get a new query vector which can be used to store documents. This newer query vector will then reflect the move of this original query vector toward this relevant centroid vector and away from the non relevant centroid vector. OK, so let's take a look at the example. This is the example that we have seen earlier, only that I give the display of the actual documents. I only showed the vector representation of these documents. We have 5 documents here. And we have. Two relevant the documents here. Right? They are displayed in red and these are the term vectors and I have just assumed some TF IDF weights. A  lot of terms. We have zero weights of course and these are negative documents. There are two here. There was another one here. Now in this Rocchio method we first compute the centroid of each category I so let's see. Look at the centroid vectors of the positive documents. What we simply just, so it's very easy to see. We just add this with this one. The corresponding element, and that's down here and take the average and then we will know added the corresponding elements and then just take the average right? So we do this for all these. In the end what we have is this one. This is the average vector of these two. So it's a centroid of this two. That's also look at the centroid of the negative documents. This is basically the same. we are gonna take the average of three elements. And these are the corresponding elements in the three vectors, and so on so forth. So in the end we have this one. Now in the Rocchio Feedback Method, we're going to combine all these with the original query vector, which is this. So now, let's see how we combine them together. That's basically this. I saw we have a parameter alpha controlling the original period  weight, that's one, and then we have beta to control the influence of the positive centroid weight that's 1.5 that comes from here. Alright, so this. Goes here. And we also have this negative weight here, controlled by Gamma here and this weight that has come. From of course the negative centroid here. And we do exactly the same for other terms. Each is for one term. And this is our new of vector. And we're going to use this new query vector. This one to rent the documents. You can imagine what would happen right because of the movement that this one would match these red documents much better because we move this vector closer to them and it's going to penalize these black documents in these non relevant documents. So this is precisely what we want from feedback. Now, of course, if we apply this method in practice. We will see one potential problem. And that is the original query has only four terms that are. non-zero But after we do query expansion, you can imagine it would have many terms that would have nonzero weights. So the calculation would have to involve more terms. In practice, we often truncate this vector, and only retain the terms with highest weights. So let's talk about how we use this method in practice. I just mentioned that we often truncated adapter and see the only a small number of words that have highest weights in the centroid vector. This is for efficiency concern. I also say that here that negative examples or non relevant examples tend not to be very useful, especially compared with positive examples. Now you can think about why. One reason is because negative documents tend to distract the query in all directions. So when you take the average it doesn't really tell you where exactly should be moving too. Where is positive documents tend to be clustered together and they will point you to a consistent direction. So that also means sometimes we don't have to use those negative examples. But note that in some cases in difficult queries where most top ranking results are negative, negative feedback factor is very useful. Another thing is to avoid overfitting. That means we have to keep relatively high weight on original query terms. Why? Because The sample that we see in feedback is a relatively small sample. We don't want to overly trust the small sample. An the original query terms are still very important. Those terms of typing by the user and the user has decided that those terms are most important. So in order to prevent us from overfitting or drifting the topic with preventing topic drifting due to bias toward the feedback examples, we generally would have to keep a pretty high weight on the original terms. It's always safe to do that. And this is especially true for pseudo relevance feedback. Now this method can be used for both relevance feedback and pseudo relevance feedback. In the case of pseudo feedback up, the parameter beta should be set to a smaller value because the relevant examples are assumed to be relevant. They are not as reliable as the relevance feedback. In the case of relevance feedback, we obviously could use a larger value, so those parameters they have to be set in imparallelly. And the root Rocchio. method is usually robust. and effective. It's still very popular method of all feedback.
410	955ff55e-55e4-4de9-bd5b-34c4e2a32f4f	This lecture is about the web search. In this lecture we're going to talk about one of the most important applications of text retrieval: web search engines. So let's first look at the some general challenges and opportunities in web search. Now, many information retrieval algorithms had been developed before the web was born, so when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about. So naturally there had to be some further extensions of the classical search algorithms. To address some new challenges encountered in web search. So here are some general challenges. Firstly, there is a scalability challenge. How to handle the size of the web an ensure completeness of coverage of all the information. How to serve many users quickly and by answering all their queries? So that's one major challenge before the web was born, the scale of search was relatively small. The second problem is that this low quality information and there are often spams. The third challenge is dynamics of the web. The new pages are constantly created and some pages may be updated very quickly, so it makes it harder to keep the index fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine. On the other hand, there are also some interesting opportunities that we can leverage to improve search results. There are many additional heuristics. For example, using links that we can leverage to improve scoring. Now the algorithm that we talked about, such as a vector space model are general algorithms. And there can be applied to any search applications, so that's the advantage. On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search. Web pages are linked with each other, so obviously the link information is something that we can also leverage. So because of these challenges and opportunities, there are new techniques that have been developed for web search or due to the need for web search. One is parallel indexing and searching and this is to address the issue of scalability. In particular, Googles's imagine of MapReduce is very influential and has been very helpful in that Aspect. Second, there are techniques that are developed for addressing the problem of spams. So spam detection we have to prevent those spam pages from being ranked high. And there are also techniques to achieve robust ranking, and we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with a particular trick. And the third line of techniques is link analysis. And these are techniques that can allow us to improve search results by leveraging extra information. And in general, in web search we're going to use multiple features for ranking, not just a link analysis, but also exploit in all kinds of clues, like the layout of web pages or anchor text that describes a link to another page. So here's a picture showing the basic search engine technology is. Basically this is the web on the left, and then user on the right side, and we're going to help this user to get the access for the web information and the first component is the crawler. That would crawl pages. And then the second component is indexer that would take these pages and create a inverted index. The third component that is a retriever that would use inverted index to answer users query by talking to the users browser and then the search results will be given to the user and then the browser would show those results and to allow the user to interact with the web so we are gonna talk about each of these components. First we're going to talk about the crawler. Also called a spider or software robot that would do something like a crawling pages on the web. To build a toy crawler is relatively easy 'cause you just need to start with a set of seed pages and then fetch pages from the web and pause these pages or and figure out the new links and then add them to the priority queue and then just explore those additional links. But to build a real crawler actually is tricky and there are some complicated issues that you have to deal with. So for example, robustness. What if the server doesn't respond? What if there's a trap that generates dynamically generated web pages that might attract your crawler to keep crawling the same site and to fetch dynamically generated pages? The results with this issue of crawling courtesy and you don't want to overload one particular server with many crawling requests. You have to respect the robot exclusion protocol. You also need to handle different types of files. There are images, PDF files, all kinds of formats on the web. Ann, you have to also consider UI or extension, so sometimes those are CGI script an there are internal references etc and sometimes you have JavaScripts on the page that they also create the challenges and you ideally should also recognize redundant pages 'cause you don't have to duplicate the those pages. And finally, you may be interested in the discover hidden urls. Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path that you might be able to get some additional pages. So what are the major crawling strategies in general? Breadth first is most common becauses naturally balance balance is the server load. You would not keep probing a particular server with many requests. Also, parallel crawling is very natural because this task is very easy to parallelize. And there are some variations of the crawling task, and one interesting variation is called focused crawling. In this case, we're going to crawl just some pages about a particular topic. For example, all pages about the automobiles. And this is typically going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and gradually crawl more. So one challenge in crawling is to find the new pages that people have created and people probably are creating new pages all the time. And this is very challenging if the new pages have not been actually linked to any old page. If they are, then you can probably find them by re-crawling the old page. So these are also some interesting challenges that have to be solved. And finally, we might face the scenario of incremental crawling or repeated crawling, right? So your first, let's say if you want to build a web search engine and you're the first crawl a lot of data from the web. And then but then, once you have collected all the data and in the future we just need to crawl the updated pages. In general you don't have to re-crawl everything right? Or it's not necessary. So in this case you your goal is to minimize the resource overhead by using minimum resources to just still crawl the updates to pages. So this is actually very interesting research question here, and it's still open research question in that there aren't many standard algorithms established yet for doing this task. But in general, you can imagine you can learn from the past experience. So the two major factors that you have to consider are first will this page be updated frequently and do I have to crawl this page again if the page is a static page that hasn't been changed for months, you probably don't have to re-crawl it everyday. Because it's unlikely that it will be changed frequently. On the other hand, if it's a sports score page that gets updated very "frequently and you may need to  the maybe even multiple times on the same day and the other factor to consider is this page frequently accessed by users. If it is, then it means it's a high utility page and then that's it's more important to ensure such a page to be fresh. Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot, then it's probably not necessary to crawl that page, or at least it's not as not as urgent as to maintain the freshness of frequently accessed page by users. So to summarize, web search is one of the most important applications of text retrieval, and there are some new challenges, particularly scalability, efficiency, quality information. There are also new opportunities, particularly rich link information and layout, etc. A crawler is an essential component of web search applications. And in general we can classify two scenarios. One is initial crawling and here we want to have complete crowding. Of the web. If you are doing a general search engine or focused crawling, if you want to just to target at a certain type of pages. And then there is another scenario that's incremental updating of the  crawled data or incremental crawling. In this case you need to optimize the resource. Try to use minimum resource to get the needed fresh information.
410	95f92696-1963-4307-83c6-a8370ff03b30	This lecture is about the contextual text mining. Contextual text mining is related to multiple kinds of knowledge that we mine from text data. As I'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. It's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. So more specifically, why are we interested in contextual text mining? Well that's, first, because text often has rich context information and this can include direct context such as meta data. And also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data. And they almost always available to us. Indirect text context refers to additional data related to the meta data. So, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age. And such information is not, in general, directly related to the text yet through the authors we can connect them. There could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. So in general, any related data can be regarded as context, so there could be remotely related to context. context. And so what's the use of, why is text context useful? Well, context can be used to partition text data in many interesting ways. It can almost allows partition text data in arbitrary ways as we need. And this is very important because this allows us to do interesting comparative analysis. It also in general provides meaning to the discovery topics if we gonna associate the text with context. So here's illustration of how context can be regarded as interesting ways of partitioning of text data. So here I just show some research papers published in different years. on different venues, different conference names here listed on the bottom, like SIGIR, ACL, etc. Now, such text data can be partitioning in many interesting ways because we have context. So the context here just includes time and the conference venues. And but perhaps we can include some other variables as well. But let's see how we can partition data in interesting ways. First, we can treat each paper as a separate unit. So in this case, a paper ID and each paper has its own context, it's independent. And. But we can also treat all the papers written in 1998 as one group, and this is only possible because of the availability of time and we can partition data in this way. This would allow us to compare topics, for example in different years. Similarly, we can partition the data based on the venues. We can get all the SIGIR papers and compare those papers with the rest or compare SIGIR papers with KDD papers with ACL papers. We can also partition the data to obtain the papers written by authors in the US, and that of course uses additional context. of the authors and this would allow us to then compare such a subset with another set of papers written by authors in other countries. Or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. topic. And note that these partitioning can be also intersect with each other to generate even more complicated partitions. And so in general, this enables discovery of knowledge associated with different context as needed. And in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. For example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. So there are many interesting questions that require contextual text mining here, I list some very specific ones. For example, what topics have been gaining increasing attention recently in data mining research? Now to answer this question, obviously we need to analyze text in the context of time. So time is a context in this case. Is there any difference in the responses of people in different regions to the event, to any event? So this is a very broad analysis question, in this case, of course, location is the context. What are the common research interests of two researchers? In this case, authors can be the context. Is there any difference in the research topics published by authors in the USA and those outside? Now, in this case, the context would include the authors and their affiliation and location. So this goes beyond just the author himself or herself. We need to look at the additional information connected to the author. Is there any difference in the opinions about the topic expressed on one social network and another? In this case, the social network of authors and the topic can be the context. Are there topics in news data that are correlated with sudden changes in stock prices? In this case, we can use a time series such as stock prices as context. What issues mattered in the 2012 presidential campaign or presidential election? Now in this case, time series again as context. df So, as you can see, the list can go on and on, basically contextual text mining can have many applications.
410	99be14e5-691b-4843-8574-4e424e1d8c41	this letter is about the recommender systems so so far we have talked about a lot of aspects of search engines and we have talked about the problem of search and the ranking problem different methods for angee implementation of search engine and how to evaluate the search engine etc this was a positive cause we know that web search engines are by far the most important application itself text retrieval and they are the most useful tools to help people convert the big raw test data into small set of random the documents another reason why we spend the so many actors on search engines is becaus many techniques used in search engines actually also very useful for recommender systems which is the topic of this left him and so overall the two systems are actually well connected and there are many techniques that are shared by them so this is a slide that you have seen before when we talked about the two different modes of text for access poor and push an we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take the initiative to recommend the information to user order pushes a relevant information to the user and this often works well when the user has a relatively stable information data when the system has a good knowledge about the what the user wants so a recommender system is sometimes called a filtering system and its becaus recommending useful items to people is like a discarding or filtering out the users of course and so in this sense they are kind of similar and in all the cases the system must make a binary decision and usually there is a dynamic source of information items and you have some knowledge about the user 's interest and then the system would make a deliberate decision whether this item is interesting through the user and then if it is interesting than the system would recommend the article to the user so the basic of filtering question here is really weird this user like this item where you like item X and there are two ways to answer this question if you think about it i wanted to look at one items you likes and then we can see if X is actually like those items the other is to look at the whole like sex and we can see if this user looks like one of those users or like most of those users and these strategies can be combined if we follow the first strategy it had to look at item similarity in the case of recommending text objects then we are talking about the content based filtering or content based recommendation if we look at the second strategy then it will compare users and in this case we are exploiting user similarity and the technique is often called collaborative filtering so let's first look at the content based filtering system this is what the system would look like inside the system there will be a binary classifier that would have some knowledge about the user 's interest and it's called a user interest profile it maintains this profile to keep track of the users interest and then there was a utility function to guide the user to make decisions and i explained that utility function in a moment it helps system decide where to set the threshold and then the accepted documents will be those that have passed the threshold according to the classifier there should be also in initialization module that would take a users input maybe from a users specified keywords or choosing category etc and this will be to feed the system with the initial user profile there is also typically a learning module that will learn from users feedback overtime now note that in this case typical to use this information either is stable so the system would have a lot of opportunities to observe the users if the user has taken recommended item has viewed that and this is the signal to indicate that the redmond item may be relevant if we use discarded relevant and so such feedback can be a long-term feedback and you can ask for a long time and the system can clock collect a lot of information about the user 's interest and this can then be used to improve the classified now what's criterion for evaluating such a system how do we know this filtering system actually performs well now in this case we cannot use the ranking evaluation measures like a map because we can afford a waiting for a lot of documents and then rank the documents to make the decision for the user and so the system must make a decision in real time in general to decide whether the item is above the threshold or not so in other words we are trying to decide absolutely relevance so in this case one commonly used strategies you use a utility function to evaluate the system so here i show linear utility function that's defined as for example three multiplied by the number of good items that you delivered minus two multiplied by the number of data items that you deliver so in other words we could kind of just treat this as almost in a gambling game if you delete you deliver one good item let's say you win three dollars you gain three down three dollars but if you believe are a bad awhile and you will lose two dollars and this utility function basically kind of meshes how much money you get by doing this kind of game and so it's clear that if you want to maximize this utility function you strategy should be to deliver as many good articles as possible and minimize the delivery of better articles that's obvious now interesting question here is how should we set these coefficients now i just show the three and negative two as the post local efficient but one can ask the question are they reasonable so what do you think do you think that that's a reasonable choice what about the other choices also for example we can have ten and minus one or one minus ten what's the difference what do you think how would this utility function affected systems threshold decision but you can think of these two extreme cases ten minus one verses one minus ten which one do you think it would encourage the system to over the lid of which you are with encourages system to be conservative if you think about that they will see that when we get a big award for delivering a good document you incur only a small penalty for delivering a better one intuitively you would be encouraged to deliver more right and you can try to deliver more in hope of getting a good one delivered and then we'll get a big award so on the one hand if you choose one minus ten you don't really get such a big price if you deliver deliver good document on the on hannah you will have a big los if you deliver better while you can imagine that the system would be very reluctant that you deliver a lot of documents it has to be absolutely sure that it's not non rather than the one so this utility function has to be designed based on specific application the three basic problems in content based filtering the forum first it has to make a filtering decision so it has to be a binary decision maker a binary classifier given a text text document and a profile description of the user it has to say yes or no whether this document should be delivered or not so that's a decision module and they should be an initialization module as you have seen earlier and this is to get the system started and we have to initialize the system based on only very limited text description or very few examples from the user and the third component is a learning module which you have to be able to learn from limited at relevance judgments becaus we county learn from the user about their preferences on the delivery documents if we don't deliver document of the user would never know would never be able to know whether the user likes it or not we can accumulate a lot of documents from the entire history and all these modules would have to be optimized to maximize the utility so how can we be over such a system and there are many different approaches here we're going to talk about how to extend retrieval system a search engine for information filtering again here's why we've spent a lot of time to talk about the search engines becaus it's actually not very hard to extend the search engine for information filtering so here's the basic idea for extending a retrieval system for information filtering first we can reuse a lot of retrieval techniques to do scoring so we know how to score documents against queries etc measure similarity between profile text description and document and then we can use a score threshold for the filter indecision we do retrieval and then we kind of find the scores of documents and then apply a threshold to say to see whether document is passing the threshold or not if it's passing the threshold are going to say it's relevant and we're going to deliver it to the user an another component the width added is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring and we know rock hill can be using for scoring improvement hi ann but we have to develop a new approaches to learn how to set the threshold and we need to set it to initially and then we have to learn how to update the threshold overtime so here's what the system might look like if we just generalize the vector space model for filtering problems so you can see the document of actor could be fed into a scoring module which is already exists in search engine that implements in fact space model and the profile will be treated as a query essentially and then the profile back that can be matched with the document vector to generate the score and then this score would be fed into a threshold in module that would say yes or no and then the evaluation would be based on utility for the filtering results if it says yes and then the document would be sent into the user and then the user could give some feedback and the feedback information would have been would be used to both adjust to the threshold and to adjust to the vector representation so the vector learning is essentially the same as query modification or feedback in the case of search the threshold are learning is new component and that we need to talk a little bit more about
410	9a443634-7f2e-4d3a-9ccd-0f1b6604c939	In this lecture, we continue discussing paradigmatic relation discovery. Earlier, we introduced a method called expected overlap of words in context. In this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the DOT product. Which can be interpreted as the probability that to randomly pick the words from the two contexts  are identical, we also discuss the two problems of this method. The first is that it favors matching one frequent term very well over matching more distinct terms. It put too much emphasis on matching one term very well. The second is that it treats every word equally. Even a common word like 'the' would contribute equally as content word like 'eats'. So now we are going to talk about how to solve these problems. Most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. So to address the first problem, we can use a sub linear transformation of term frequency. That is, we don't have to use the raw frequency count of term to represent the context. We can transform it into some form that wouldn't emphasize so much on the raw frequency. To address the second problem, we can put more weight on rare terms. That is, we can reward matching a rare word and this heuristic is called IDF term weighting in text retrieval. IDF stands for inverse document frequency. So now we're going to talk about the two heuristics in more detail. First, let's talk about the TF transformation. That is, to convert the raw count of word in the document into some weight that reflects our belief about how important this word in the document. And so that will be denoted by TF of W&D as shown in the Y axis. Now in general there are many ways to map that, and let's first look at the simple way of mapping. In this case, we're going to say, any non zero counts will be mapped to one. And then zero count will be mapped to 0. So with this mapping, all the frequencies will be mapped to only two values, zero or one, and the mapping function is shown here as a flat line here. Now this is naive because it ignored the frequency of words. However, this actually has the advantage of emphasizing matching all the words in the context, so it does not allow a frequent word to dominate the matching. Now the approach that we have taken earlier in the expected overlap account approach is a linear transformation. We basically take Y as the same as X. So we use the raw count as representation. And that created the problem that we just talked about. Namely it answers too much on just matching one frequent term. Matching one frequent term can contribute a lot. So we can have a lot of other interesting transformations in between the two extremes. And they generally form a sub linear transformation. So for example, one possibility is to take logarithm of the raw count, and this will give us curve that looks like this, right? That you're seeing here. In this case, you can see the high frequency counts, the high counts are penalized a little bit right? So the curve is a sub linear curve, an it brings down the weight of really those really high counts. And this is what we want, because it prevents that kind of terms from dominating the scoring function. Now there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that. Looks like this. I saw it's (K + 1) * X /( X + K) where K is a parameter. X is the count, the raw count of word. Now the transformation is very interesting in that it can actually kind of go from one extreme to the other extreme by varying K. And it also is interesting that it has upper bound K +1 in this case. So this puts a very strict constraint on high frequency terms, because their weight would never exceed K+1. As we vary K, if we can simulate the two extremes. So one case is set to zero. We roughly have the 01 vector. Whereas when we set the key to a very large value, it would behave more like the linear transformation. So this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. So we just talk about how to solve the problem of over emphasizing a frequently frequent term. Now let's look at the second problem, and that is how we can penalize popular terms. Matching 'the' is not surprising because 'the' occurs everywhere, but matching 'eats' will account a lot. So how can we address that problem? In this case we can use the  IDF weighting that's commonly used in retrieval. IDF stands for inverse document frequency. Document frequency means the count of the total number of documents that contain a particular word. So here we show that the IDF measure is defined as a logarithm function of the number of documents that match the term, or document frequency. So K is the number of documents containing word or document frequency and M here is the total number of documents in the collection. The IDF function is giving a higher value for a lower K, meaning that it rewards a rare term. And the maximum value is log of M + 1. That's when the word occurs just once in the context. So that's a very rare term, the rarest term in the whole collection. The lowest value you can see here is when K reaches its maximum, which would be M. That would be a very low value close to 0 in fact. Right so this. This of course measure is used in search where we naturally have a collection. In our case, what will be our collection? We can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low IDF. Because depending on the data set, we can construct the context vectors in different ways, but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected context documents. So how can we add these heuristics to improve our..... Our similarity function. Here's one way, and there are many other ways that are possible. But this is a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining. So here we define in this case we define the document vector. As containing elements representing normalized BM 25 values. So in this normalization function we see we take sum over some of all the words and then we normalize the weight of each word by the sum of the weights of  all the words. This is to again ensure all the x(i) will sum to one in this vector. So this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one. Now the weight of BM25 for each word is defined here. And if you compare this with our old definition where we just have a normalized count. On this one, right? So we only have this one and the document length or the total count of words in that context document. And that's what we had before. But now with the BM 25 transformation, we introduced something else. First, of course, this extra occurrence of this count is just to achieve the sub linear normalization. But we also see we introduce the parameter K here. And this parameter is generally non negetive number, although zero is also possible. This controls the upper bound and the kinds controls can choose To what extent is simulates the linear transformation. And so this is 1 parameter. But we also see there is another parameter here b and this will be within zero and one. And this is a parameter to control  length normalization. And, and in this case the normalizing formula has average document length here. And this is the computed by taking the average of the lengths of all the documents in the collection. In this case, all the lengths of all the context documents that we are considering. So this average documents will be a constant for any given collection, so it actually is only affecting the effect of the parameter B here. Because this is a constant. But I kept it here because it's constant that's useful in retrieval, where it would give us a stabilized interpretation of parameter b. But for our purpose this will be a constant, so it would only be affecting the length formalization together with parameter B. Now with this definition, then we have a new way to define our document vectors and we can compute the vector D2 in the same way. The difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms. Now the IDF can be added here in the scoring function. That means we'll introduce weight for matching each term. So you may recall this sum indicates all the possible words that can be a overlap between the two contexts. And the Xi and Yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word. Now IDF would give us the importance of matching this word. A common word will be worth less than rare word, so we emphasize more on matching rare words now. So with this modification, then the new function will likely address those two problems. Now interestingly we can also use this approach to discover syntagmatic relations. In general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context. So let's take a look at the term vector in more detail here. And we have each Xi, defined as a normalized weight of BM 25. Now this weight alone only reflects how frequently the word occurs in the context. But we can't just say any frequent term in the context that would be correlated with the candidate word. Because many common words like 'the' will occur frequently in all the context. But if we apply IDF weighting as you see here, we can then we weight these terms based on IDF That means the words that are common, like 'the' will get penalized. So now the highest weighted terms will not be those common terms because they have lower IDFs. Instead, those terms would be the terms that are frequent in the context, but not frequently in the collection. So those are clearly the words that tend to occur in the context of the candidate word, for example, cat. So for this reason, the highly weighted terms in this IDF weighted vector can also be assumed to be candidate for Syntagmatic relations. Now of course, this is only a bi-product of our approach for discovering paradigmatic relations. And in the next lecture, we're going to talk more about how to discover Syntagmatic relations. But it clearly shows the relation between discovering the two relations. And indeed they can be discussed, discovered in a joint manner by leveraging such associations. So to summarize, the main idea for discovering  paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. And then compute the similarity of the corresponding context documents of two candidate words. An then we can take the highly similar word pairs and treat them as having paradigmatic relations. These are the words that share similar context. And there are many different ways to implement this general idea and we just talk about some of the approaches. And more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. More specifically, we have used the  BM25 and IDF weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. Finally, Syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.
410	9b2615a8-42c5-485d-a616-fb8dba74a888	This lecture is about the basic measures for evaluation of text retrieval systems. In this lecture we're going to discuss how we design basic measures. To quantitatively compared to retrieval systems. This is a slide that you have seen earlier in the lecture where we talked about. Cranfield evaluation methodology. We can have a test collection that consists of queries, documents and relevance judgments. We can then run two systems on these datasets to quantitatively evaluate their performance. And we raised the question about which set of results is better. Is system a better or system B better? So let's now talk about how to actually quantify their performance. Suppose we have a total of 10 relevant documents in the collection for this query. Now the relevance judgments shown on the right. Did not include all the ten, obviously and we have only seen three relevant documents there, but we can imagine there are other relevant documents in judge before this query. So now intuitively we thought that system A is better because it did not have much noise, and in particular we have seen that among the three results, two of them are relevant. But in system B. We have 5 results and only three of them are relevant. So intuitively it looks like system A is more accurate and this intuition can be captured by a measure called precision where we simply compute: to what extent, all the retrieval results are relevant if you have 100% precision, that would mean all the retrieval documents are relevant. So in this case the system A has a precision of two out of three, system B has 3 / 5. And this shows that system A is better by precision. But we also talked about System B might be preferred by some other users who like to retrieve as many relevant documents as possible. So in that case will have to compare the number of relevant documents, then retrieve and there is another measure called recall. This measures the completeness of coverage of relevant documents in your retrieval. Result, so we just assume that there are 10 relevant documents in the collection. An here we've got two of them in system A, so the record is 2 out of 10 Whereas System B has got three. So it's a 3 out of 10. Now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines, and they are very important because they also widely used in. Many other task evaluation problems, for example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported for all kinds of tasks. OK, so now let's define these two measures more precisely, and these measures are to evaluate a set of retrieval documents. So that means we are considering that approximation of the set of relevant documents. We can distinguish it into 4 cases depending on the situation of the document. A document that can be retrieved or not retrieved, right? Because we're talking about the set of results. A document can be also relevant or non relevant depending on whether the user thinks this is useful document. So we can now have counts of documents in each of the four categories. We can have A to represent the number of documents that are retrieved and relevant. B for documents that are not retrieved but relevant, etc. Now with this table, then we could define precision as the. Ratio of. The relevant retrieved documents A to the total number of retrieval documents, so this is just A divided by the sum of A&C. sum of this column. Similarly, recall is defined by dividing A by the sum of A&B, so that's again the divide A by the sum of the rule instead of the column. Right, so we can see precision and recall is all focused on looking at A. That's the number of retrieval relevant documents. But we're going to use different denominators. OK, So what would be an ideal result? You can easily see in the ideal case we have precision and recall, or to be 1.0 that means we have got 1% of all the relevant documents in our results and all the results that we return are relevant. At least there's no single nonrelevant document in return. In reality, however, high record tends to be associated with low precision. And you can imagine why that's the case as you go down the list to try to get as many relevant documents as possible, you tend to encounter a lot of non relevant documents, so the precision would go down. Note that this set can also be defined by a cut off in the ranked list. That's why although these two measures are defined for a set of retrieval documents, they are actually very useful for evaluating a ranked list. There are fundamental measures in text retrieval and many other tasks. We often are interested in the precision at 10 documents for web search. This means we look at the how many documents among the top 10 results are actually relevant. Now this is a very meaningful measure because it tells us how many relevant documents the User can expect to see on the first page of search results where they typically show 10 results. So precision and recall are the basic measures and we need to use them to further evaluate search engine. But there are the building blocks really. We just said that there tends to be a tradeoff between precision and recall, so naturally it would be interesting to combine them. And here's one measure that's often used called F measure. And, it's a harmonic mean of precision and recall is defined on this slide. So you can see. It first. Compute the. Inverse of R and P here and then it would interpret the two by using the coefficients. Depending on a parameter beta. And after some transformation you can easily see it would be of this form. And in any case, this is just a combination of precision and recall. Beta is a parameter that's often set to one. It can control the emphasis on precision or recall when we set Beta to one, we end up having a special case of F measure, often called F1. This is a popular measure that's often used little combine precision and recall, and formula looks very simple. It's just this here. Now it's easy to see that if you have a larger precision or larger recall than F measure would be high. But what's interesting is that. The tradeoff between precision and recall is captured in the interesting way in F1. So in order to understand that, we can first look at the natural question, why not just combine them using a simple arithmetic mean as shown here? That would be likely the most natural way of combining them. So what do you think? If you want to think more, you can pause the video. So why is this not as good as F1? Or what's the problem with this? Now. If you think about the arithmetic mean, you can see this is the sum of multiple terms. In this case is a sum of precision and recall. In the case of a sum, the total value tends to be dominated by the large values. That means if you have a very high P or very high R, then you really don't care about the weather. The other value is low, so the whole sum would be high. Now this is not desirable because one can easily have a perfect recall. We can have perfect recall easily. Can you imagine how? It's probably very easy to imagine that we simply retrieve all the document in the collection. Then we have a perfect recall. And this will give us point of five as the average. But such results are clearly not very useful for users, even though the average using this formula would be relatively high. But in contrast, you can see F1 would reward the case where precision and recall are roughly similar, so it would penalize a case where you have extremely high value for one of them. So this means F1 encodes different.  tradeoffs between them. For this example, shows actually a very important methodology here. When you try to solve a problem. You might naturally think of 1 solution, let's say in this case it's this arithmetic mean. But it's important not to settle on this solution. It's important to think whether you have other ways to combine them. And once you think about multiple variants, it's important to analyze their difference. And then think about which one makes more sense in this case. If you think more carefully, you will feel that F1 probably makes more sense than the simple arithmetic mean, although in other cases there may be different results, but in this case the arithmetic mean seems not reasonable. But if you don't pay attention to these subtle differences, you might just take a easy way to combine them and then go ahead with it. And there later you will find that measure. It doesn't seem to work well. so at this methodology is actually very important in general in solving problem and try to think about the best solution. Try to understand the problem very well and then know why you need this measure and why you need to combine precision and recall and then use that to guide you in finding a good way to solve the problem. To summarize, we talked about precision. Which addresses the question. Are the retrieval results all relevant. We also talked about the recall. Which addresses the question have all the relevant documents being retrieved. These two are the two basic measures in text retrieval evaluation. They are useful for many other tasks as well. We talked about F measure as a way to combine precision and recall. We also talked about the tradeoff between precision and recall and. This turns out to depend on the users search tasks and will discuss this point more in the later lecture.
410	9e5a0c5f-ff4a-42a6-b37b-f43628632860	This lecture is about the syntagmatic relation discovery and mutual information. In this lecture, we're going to continue discussing syntagmatic relation discovery. In particular, we're going to talk about another concept, the information theory, called mutual information. And how it can be used to discover syntagmatic relations? Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. In particular, mutual information, denoted by I(X;Y), measures the entropy reduction of X obtained from knowing Y. More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y. So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y. And you might see here you can see here. It can also be defined as a reduction of entropy of Y, because of knowing X. Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal.  But interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by I here and this function has some interesting properties. First, it's also non negative. This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. In other words, the conditional entropy would never exceed the original entropy. Knowing some information can always help us potentially, but won't hurt us in predicting X. The second property is that it's symmetric while conditional entropy is not symmetrical. Mutual information is. The third property is that it reaches its minimum zero if and only if the two random variables are completely independent. That means knowing one of them doesn't tell us anything about the other. And this last property can be verified by simply looking at the equation above. And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X. So that means knowing why did not help at all, and that's when X&Y are completely independent. Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed. So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y. But the mutual information allows us to compare different pairs of X&Y, so that's why mutual information is more general and in general more useful. So let's examine them intuition of using mutual information for syntagmatic relation mining. Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. So we're going to compute the mutual information between eats and other words. And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. We have lower mutual information, so this I give some example here. The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the. Because knowing the doesn't really help us predict eats. Similarly knowing eats doesn't help us predicting the as well. And you also can easily see that the mutual information between  a word and itself is the largest which is equal to the mutual info. The entropy of this word. So because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. Therefore the mutual information reaches its maximum. It's going to be larger than or equal to the mutual information between eats and another word. In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. So now let's think about how to compute the mutual information. Now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance. This is another term in information theory that measures the divergance between two distributions. Now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions. The numerator has the joint actual observed. Join the distribution of the two random variables. The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. If there were independent. Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities. So this comparison would tell us whether the two variables are indeed independent if there indeed independent, then we would expect that the two are the same. But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association. The sum is simply to take into consideration of all the combinations of the values of these two random variables. In our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. The larger this divergence is, the higher the mutual information would be. So now let's further look at the what are exactly the probabilities involved in this formula of mutual information. And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word. So for W1, we have two probabilities shown here. They should sum to 1 because a word can either be present or absent in the segment. And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well. And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words. And they are shown here. Right, so this sums to 1 because the two words can only have these four possible scenarios. Either they both occur. So in that case both variables will have a value of one or one of them occurs. There are two scenarios. In these two cases, one of the random variables will be equal to 1 and the other would be 0. And finally we have the scenario when none of them occurs. So this is when the two variables taking a value of 0. And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. here. Once we know how to calculate these probabilities, we can easily calculate the mutual information. It's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the marginal probabilities of these words sum to one, and we also have seen this constraint that says the two words can only have these four different scenarios of Co occurrences, but we also have some additional constraints listed in the bottom. And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed. In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed. So this probability captures the first scenario when the signal word actually is also observed. And this captures the second scenario when the seond word is not observed, so we only see the first word. And it's easy to see the other equations also follow the same reasoning. Now these equations allow us to compute some probabilities based on other probabilities. And this can simplify the computation. So more specifically, and if we know the probability that a word is present, and in this case right? So if we know this. And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right? It's very easy to use this equation to do that. An so we this will take care of the computation of these probabilities of presence or absence of each word. Now let's look at their joint distribution, right? Let's assume that we also have available probability that they occur together. Now it's easy to see that we can actually compute the all the rest of these probabilities based on these. Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes. And similarly, using this equation we can compute the probability that we observe only the second word. And then finally we. This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right? So this can be easier to calculate. Right, so now this can be calculated. So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, namely the presence of each word and the Co occurrence of both words in a segment.
410	9faafef6-a1cb-4e0d-a06d-e134eaec84d9	in this lecture we're going to talk about how to instantiate vectors based model so that we can get very specific ranking function so this is to continue the discussion of the vectors based model which is one particular approach to design (test) ranking function and we're going to talk about how we use the general framework of the vectors based model as a guidance to instantiate the framework to derive a specifical ranking function and we're going to cover the simplest instantiation of the framework so as we discussed in the previous lecture the vectors based model is really a framework it didn't didn't say as we discussed in the previous lecture vector space model is really a framework it doesn't say many things so for example here it shows that it did not say how we should define the dimension it also did not say how we place a document a vector in this space he did not say how we place a query vector in this space and finally it did not say how we should measure the similarity between the query vector and the document vector so you can imagine in order to implement this model we have to say specifically how we compute these vectors what is exactly X I and what is exactly why i this would determine where we place a document about that where we place a query about them and of course we also need to say exactly what should be the similarity function so if we can provide a definition of the concepts that would define the dimensions and these excise or why eyes and then the weights of terms for query and document then we will be able to play stocking the raptors and query back in this where they find the space and then if we also specify similarity function then we'll have well defined the ranking function so let's see how we can do that and think about the simplest instantiation actually i would suggest you to pause the lecture at this point spend a couple of minutes it will think about it suppose you i ask the two implemented this idea you've come up with the idea of vector space model but you still haven't figured out how to compute these vectors exactly how to define the similarity function what would you do so i think for a couple of minutes and then proceed so let's think about some simple ways of instantiating this vector space model firstly how do we define dimension where the obvious choice is to use each word in our vocabulary the diviner dimension and who we sure that there are N words in our vocabulary therefore there RN dimensions each word defines one dimension and this is basically the bag of words with temptation now let's look at how we place vectors in this space again here the simplest strategy is to use a bit vector to represent both the query and a document and that means each element X I N Y I would be taking a value of either zero or one when it's one it means the corresponding world is present in the document or in query what's zero is going to mean that it's absolute so you can imagine if the user types in a few words in the query then the query back that we only have a few ones many many zeros the docking the vector general we have more ones of course but it will also have many zeros since the vocabulary is generally very large many words don't really occur in any document many words we only occasionally occur in the document a lot of words will be absent in a particular document so now we have placed the documents and the query in the vector space let's look at how we measure the similarity so a commonly used the similarity measure here is dot product the thought product of two vectors is simply defined as the some of the products of the corresponding elements of the two vectors so here we see that it's the product of X one and why one so here and then X two multiplied by Y two and then finally eggs an multiplied by Y N and then we take a sum so that's the thought product now we can represent this in a more general way using a some here so this is only one of the many different ways of measuring the similarity so now we see that we have defined the the dimensions we have defined vectors an we have also define the similarity function so now we find that they have the simplest the vector space model which is based on the bit vector recommendation dot product similarity and bag of words recommendation and the formula looks like this so this is our formula and that's actually a particular retrieval function a ranking function right now we can finally implemented this function using a programming language and then rank documents for query now at this point you should again pause the lecture to think about how we can interpret this school so we have gone through the process of modeling the retrieval problem using a vector space model and then we make assumptions about how we place about this in the vector space and how we define the similarity so in the end that we've got a specifically retrieval functioning she won't hear now the next step is to think about the weather this retrieval function actually makes sense i can we expect that this function to after perform well when we use it to ranger documents for users queries so it's worth thinking about what is this value that will calculate so in the end we get a number but what is this number mean is it meaningful so spend a couple of minutes to think about that and of course the general question here is do you believe this is good ranking function what they'd actually work well so again think about how to interpret this value is it actually meaningful there's the mean something it's related to how well the document match the query so in order to assess whether this simplest vectors with model actually works well let's look at the example so here i show some sample documents anna sample query the query is news about the presidential campaign and we have five documents here they cover different terms in the query and if you look at the these documents for a moment you may realize that some documents are probably relevant and some others are probably non relevant now if i ask you to rank these documents how would you rent them this is basically our ID or ranking when humans can examine the documents and then try to rank them not so think for a moment and take a look at this slide and perhaps by pausing the lap gym so i think most of you would agree that T four and T three are probably better than others becaus they really cover the query well they match news press value and the campaign so it looks like these through documents are probably better than the others they should be ranked that on top an the other three D two D one and D five really non relevant so we can also say D four and D three are relevant documents and D one D two and D five are non relevant so now let's see if our simplicity vectors based model could do the same or could do something closer so let's first think about how we actually use this model to school documents right here i show two documents D one and D three and we have the query also here in the vectors with model of course we want to first compute the vectors for these documents and the query now i show the vocabulary here as well so these are the end dimensions that will be thinking about so what do you think is vector repetition for the query note that we are assuming that we only use zero and one to indicate whether a term is absent or present in the query or in the document so these are zero one bit vectors so what do you think is query raptor well the query has four words here so for these forwards there will be a one and for the rest will be zero now what about documents it's the same so T one has two words news and about so there are two ones here and the rest of zeros similarly so uh now that we have the two vectors let's computer the similarity and we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements right so these two will be former they forming product and these two general another product and these two which end with yet another product after and so on so forth now you can easy to see if we do that we actually don't have to care about these zeros becaus if whenever we have a zero the product will be zero so when we take a sum over all these pairs then the zero entries will be gone as long as you have one zero then the product will be zero so in fact we're just counting how many pairs of one and one in this case we have seen two so the result would be too so what does that mean well that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document becaus if a document if the term is matched in the document then there will be two ones if it's not then there will be zero on the document aside similarly if the document has a term but the term is not in the query there will be a zero in the query back then so those don't count so as a result this scoring function basically meshes how many unique query terms are matched in a document this is how we interpreted this score now we can also take a look at the D three in this case you can see the result is three becaus these three match to three distinct query was news presidential campaign where is eva only match the two now in this case it seems reasonable to rank the three on top of E one and this simplest of X is based model indeed it does that so that looks pretty good however if we examine this model in detail we likely will find some problems so here i'm going to show all the scores for these five documents and you can easily verify they are correct becaus we basically counting the number of unique query terms match the in each document note that this measure after that makes sense right it basically means if the document in matches more unique query terms then the document are we assuming to be more relevant and that seems to make sense the only problem is here we can note set there are three documents D two D three and D four and they tide with a three as a school so that's a problem becaus if you look at them carefully is seems that D four should be ranked above the three becaus is re only imagine the presidential once the default messaging data multiple times in the case of the three presidential could be an extender managing but the four is clearly about presidential campaign another problem is that D two and D three also have the same score but if you look at the three words that are matched in the case of the two it matched the news about an campaign but in the case of the three it match the news presidential and campaign so intuitively these reads better becaus matching presidential is more important than matching about even though about and presidential are both in the query so intuitively would like T three it will rank the above D two but this model doesn't do that so that means this model is there not good enough we have to solve these problems to summarize in this lecture we talked about how to instantiate a vector space model we may need to do three things one is too define the dimension the second is to decide how to place documents as vectors in the vector space and to also plays a query in the baptist space as a vector and third is to define the similarity between two vectors particularly the query about the end document map we also talk about various simple way to instantiate a vector space model indeed that probably the simplest of vectors based model that we can arrive in this case we use each word with the final dimension we use zero one bit vector to represent a document or query in this case we basically only care about what the presence or absence we ignore the frequency and we use the thought product as similarity function and with such in situation and we showed that the scoring function is basically to score a document based on the number of distinct query words matching the document we also show that such a simple vector space model still doesn't work well and we need to improve it and this is a topic that we're going to cover in the next election
410	a09e9c43-a8d0-4347-873b-041c21e3b725	this lecture is about how we can evaluate a ranked list in this lecture we will continue the discussion of evaluation in particular we're going to look at how we can evaluate the ranked list of results in the previous lecture we talked about precision and recall these are the two basically measures for quantitatively measuring the performance of search result but as we talked about ranking before we frame with the texture retrieval problem as a ranking problem so we also need to evaluate the quality of a ranked list how can we use precision and recall to evaluate a ranked list well naturally we have to look at the precision and recall at different cut offs becaus in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing if we assume the user sequentially flowers is the list of results the user would stop at some point and that point would determine the set and then that's the most important cut off that will have to consider when we compute the brazilian recall without knowing where exactly the user would stop then we have to consider all the positions where the user could stop so let's look at these positions look at this slide and then let's look at the weather for the user stops at the first document what's the precision and recall at this point what do you think well it's easy to see that this document is relevant so the position is one out of one we have gotten one document and that's random what about the recall well note that we assume that there are ten relevant documents for this query in the collection so it's one out of ten what if the user stops at the second position top two well the procedure is the same one hundred percent two out of two and the records two out of ten what if the user stops at the third position well this is interesting becaus in this case we have not got any additional random in the document so the recall it doesn't change but the precision is lower because we've gotten our eleanor so what exactly the position well it's two out of three right and recall is the same throughout of ten so when would we see another point where the recall would be different if you look down the list well it won't happen and here we have seen another reading the document in this case D five at that point the recoil is increased to three out of ten and the precision is the three out of five so you can see if we keep doing this we can also get to T eight and then we will have a procedure of four out of eight because there are eight document san four of them are relevant and the recall is a four out of ten now when can we get a recall of five out of ten well in this list we don't have it so we have to go down on this we don't know where it is but as a convenience we often assume that the position is zero at all the precision is zero at all the other levels of recall that are beyond the search results so of course this is a pessimistic assumption the actual precision would be higher but we make this assumption in order to have easy way to compute another metric called average precision that will discuss later now i should also say now here you see we make these assumptions that are clearly not accurate but this is usually OK for the purpose of comparing to text richard methods and this is for the relative comparison so it's OK if the actual measure or actual actual number deviates a little bit of from that rule number as well as the deviation is not biased toward any particular retrieval method and we are OK we can still accurate detail which method works better and this is an important point to keep in mind when you compare different algorithms the keys to avoid any biased toward each method and as long as you can avoid that it's OK you do transformation of these measures in anyway so you can preserve the order OK so we just talk about we can get a lot of precision recall numbers at different positions so now you can imagine we can plot the curve and this just shows on the X axis we show the recalls and on the Y access we show the position so the precision levels are marked as point one point two point three and one point zero so this is different levels of recall and the Y axis also has different amounts that's for precision so we plot these precision recall numbers that we have got as points on this picture now we can further link these points performer curve as you see we assume the all the other precision that high level recalls is zero and that's why they are done here i thought the hours europe the actual curve problem will be something like this but as we just discuss it doesn't matter that much for comparing two methods 'cause this would be and there is made for all the methods OK so now that we have this precision recall curve how can we compare it to read the list right so that means we have to compare to PR curves and here is still two cases where system a is shown in red system be showing blue with crosses right so which one is better i hope you can see more system A is clearly better why be cause for the same level of recall and see same level of recall here and you can see the precision point buy system is better than system be so there's no question india you can imagine what is the curve look like for idea of searching system well it has to have perfect the position at all the recall points so it has to be this line that would be the ideal system in general the higher the curve is the better the problem is that we might see a case like this this actually happens often like the two curves cross each other now in this case which one is better what do you think now this is a real problem that you actually might face suppose you build a search engine an you have older algorithm that's shown here in blue or system be and you have come up with a new idea and you test it and the results are shown in red curve eight now your question is is your new method better than the old method or more practically do you have to replace the algorithm that you already using your in your search engine with another new algorithm so we use system method A to replace method would be this is going to be a real decision that you have to make if you make the replacement the search engine would behave like a system may here whereas if you don't do that it will be like a system be so what do you do now if you want to spend more time to think about this pause the video and it's after a very useful to think about that as i said it's a real decision that you have to make if you are building your own search engine or if you're working for a company that cares about the search now if you have thought about this woman you might realize that well in this case it's hard to say there was some users might like a system made some users might like like system be so what's the difference here well the difference is just that in the low level of recording this region system bees better that's higher precision but in high recall reading system is better now so that also means it depends on whether the user cares about the high recall or lower recall but high position and you can imagine someone is just going to check out what's happening today and you want to find some random in the news well which one is better what do you think in this case clearly system bees better because the user is unlikely examining a lot of results the user doesn't care about high recall on the other hand if you think about a case where a user is doing it's a little too so you starting problem you want to find whether your idea has been started before in that case you emphasize high recall so you want to see as many reading the documents as possible therefore you might have favored system eh so that means which ones better actually depends on users an more precisely users task so this means you may not necessarily be able to come up with one number that would accurately depict the performance you have to look at the overall picture yet as i said when you have a practical decision to make whether you replace the algorithm with another then you may have to actually come up with a single number to quantify each method or when we compare many different methods in research ideally we have one number to compare them with so that we can easily make a lot of comparisons so for all these reasons it's desirable to have one single number to measure that so how do we do that and that needs a number to summarize arrange so here again it's the precision recall curve and one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve right so this is one way to measure that there are other ways to measure that but it just turns out that this particular way of measuring has been very popular and has been used since a long time ago for text retrieval evaluation and this is basically computed in this way and it's called average position basically we're going to take a look at every different recall point and then look at the precision so we know this is one precision this is another with different recall now this we don't count this one becaus the record level is the same and we're going to then look at this number and that's the precision at a different recall level etc so we have all these you know added up these are the positions that had the different points corresponding to retrieving the first irrelevant document the second and then the third the force etc now we miss them any random documents so in all of those cases we just assume that they have zero precisions and then finally we take the average so with divided by ten and which is a total number of relevant document in the collection note that here we are not dividing the sum by four which is a number of retrieve the relevant documents now imagine if i divide by four what would happen now think about this for a moment it's a common mistake that people sometimes overlook so if we divide this by four it's actually not very good in fact that you are favoring a system that would retrieve very few random documents as in that case the denominator would be very small so this would be not a good measure so not that this is ten the total number of random documents and this will basically compute the area and needs the curve and this is the standard method used for evaluating a ranked list note that it actually combines recall and precision but first we have precision numbers here but second that we also consider recalled becaus if you miss the minute there would be many O scale so it combines precision and recall and furthermore you can see this measure is sensitive to a small change of a position of the relevant document that let's say if i move this relevant document up a little bit now it would increase this mean at this average precision whereas if i move any relevant document down let's say i move this random not gonna let down then it would decrease the average precision so this is very good be cause it's a very sensitive to the ranking of every relevant document it can tell small differences between two ranked lists and that's what we want sometimes one algorithm only works a slightly better than another and we want to see this difference in contrast if we look at the precision at the ten documents to look at this this whole set well what's the procedure whether you think well it's easy to see that four out of ten right so that precision is very meaningful because it tells us what user would see so that's pretty useful right so it's a meaningful measure from users perspective but if we use this measure to compare systems it wouldn't be good be cause it wouldn't be sensitive to wear these fall rather than documents are ranked if i move them around the precision at ten is there the same right so this is not a good measure for comparing different algorithms in contrast the average position is much better measure it can tell the difference of different difference in ranked lists in subtle ways
410	a43eb3f2-84ee-45a2-b9ec-1f97d6de3c6e	so to summarize our discussion of recommended systems in some sense the filtering task or recommended tasker is easy and some other senses the task is actually difficulty so it's easy becaus the users expectations though in this case the system takes initiative to push the information to the user so the user doesn't really make any effort so any recommendation is better than nothing right so well unless you recommend all the noise items or useless documents if you can recommend some useful information users general with appreciated so that's in that sense that's easy however filtering is actually much harder task and then retrieval becaus you have to make a binary decision and you can afford a waiting for a lot of items and then you're going to see whether one item is better than others you have to make a decision when you see this item will think about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user wait for few days well even if you can make accurate recommendation of the most relevant news the utilities will be significantly decreased another reason why it's hard it's be cause of data sparseness if you think of this as a learning problem in collaborative filtering for example it's purely based on learning from the past ratings so if you don't have many ratings there's not much you can do and yeah just mentioned this cold start problem this is actually a very serious serious problem but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem you can use for example more user information to assess their similarity instead of using the preferences of these users on these items that may be additional information available about the user etc and we also talk about the two strategies for filtering task one is content based where we look at item similarity the others collaborative filtering where we look at the user similarity and the obviously can be combined in a practical system you can imagine the general would have to be combined so that will give us a hybrid strategy for filtering and we also could recall that we talked about push this is a poor as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are serving users in the pool mode obviously the tool should be combined and they can be combined to have a system that we can support user with multiple mode information access so in the future we could anticipate is such a system to be more useful to the user and i saw this is the active research area so there are a lot of new algorithm the being proposed all the time in particular those new algorithms tend to use a lot of context information now the context here could be the context of the user and then it could be also context of documents or items the items are not isolated and they're connected in many ways the users might form social network are as well so there's a rich context there that we can leverage in order to really solve the problem well and then that's active research area where also machine learning algorithms that have been applied there are some additional readings in the handbook called recommended systems and has a clashing of a lot of good articles that can give you an overview of a number of specific approaches to recommended systems
410	a893068a-73ee-4771-8c52-43d8a6133363	So, here are some specific examples of what we can't do today, and part of speech tagging is not easy to do one hundred percent correctly. "So in the example: ""He turned off the" "highway"" versus ""He turned off the fan"" "and the two ""off""'s actually have" somewhat different syntactic categories. And also it's very difficult to get complete parsing correct. Again, "the example ""A man saw a boy with a" "telescope"" can actually be very" difficult to parse depending on the context. Precise deep semantic analysis is also very hard. For example, to define the meaning of """own"" precisely is very difficult in a" "sentence like ""John owns a restaurant""." So the state of the art can be summarized as follows. Robust and general NLP tends to be shallow, while deep understanding does not scale up. For this reason, in this course, the techniques that we cover are, in general, shallow techniques for analyzing text data, to mine text data. And they are generally based on statistical analysis, so they are robust, and general, and, And they are in the category of shallow analysis, so such techniques have the advantage of being able to be applied to any text data in any natural language about any topic. But the downside is that they don't give us a deeper understanding of text. For that we have to rely on deeper natural language analysis techniques. That typically would require human effort to annotate a lot of examples of analysis that we'd like to do, and then computers can use machine learning techniques to learn from these training examples to do the task. So in practical applications we generally combine the two kinds of techniques with the general statistical and methods as backbone as the basis, since it can be applied to any text data and on top of that we're going to use humans to annotate more data and to use supervised machine learning to do some tasks as well as we can, especially for those important tasks. So to bring humans in, into the loop to analyze, fix, to analyze text data more precisely. But this course will cover the general statistical approaches that generally don't require much human effort. So they are practically more useful than some of the deeper analysis techniques that require a lot of human effort to annotate text data. So to summarize this lecture, the main points to take away are, first NLP is the foundation for text mining. So obviously the better we can understand the text data, the better we can do text mining. Computers today are far from being able to understand the natural language. Deeper NLP requires common sense knowledge and inferences, thus only working for very limited domains. Not feasible for large scale text mining. Shallow NLP based on statistical methods can be done in large scale. And is the main topic of this course and they are generally applicable to a lot of applications. They are in some sense also more useful techniques. In practice, we use statistical NLP as the basis. And we have humans to help as needed in various ways.
410	a93e9263-8950-4d65-a5db-a2114d5774d9	This lecture is about document Length normalization. In the vector space model. In this lecture, we're going to continue the discussion of the vector space model. In particular, we're going to discuss the issue of document length normalization. So far in the lectures about the vector space model, we have used various signals from the document to assess the matching of the document, with the query. In particular, we have considered the term frequency. The count of the term in the document. We have also considered its global statistics. Such as IDF inverse document frequency. But we have not considered document length. So here, I show 2 example documents. D4 is much shorter with only 100 words. D6, on the other hand, has 5000 words. If you look at the matching of these query words, we see that in D6 there are more matchings of the query words. But one might reason that D6 May have matched these query words  In a scattered manner. So maybe the topic of D6 is not really about the topic of the query. So the discussion of campaign at the beginning of the document may have nothing to do with the mention of presidential at the end. In general, if you think about long documents, they would have a higher chance to match any query in fact. If you generate a long document randomly by simply sampling words. From a distribution of words, Then eventually you probably will match any query. So in this sense we should penalize long documents because they just naturally have better chances for matching any query. And this is the idea of document length normalization. We also need to be careful in avoiding to over penalize long documents. On the one hand, we want to penalize a long document, but on the other hand we also don't want to over penalize them. And the reason is because a document may be long because of different reasons. In one case, the document may be long because it uses more words. So for example. Think about the full text article of a research paper. It would use more words than the corresponding abstract. So this is the case where we probably should penalize the matching of. Long documents such as full paper. When we compare the matching of words in such a long document with matching of the words in a short abstract. Then long papers generally have higher chance of matching query words. Therefore we should penalize them. However, there is another case when the document is long and that is when the document simply has more content. Now consider another case of a long document where we simply concatenated a lot of abstracts of different papers. In such a case, obviously we don't want to over penalize such a long document. Indeed, we probably don't want to penalize such a document because it's long. So that's why we need to be careful about. Using the right degree of penalization. A method that has been working well based on research results is called  pivotal length normalization, and in this case the idea is to use the average document length as a pivot as a reference point. That means we'll assume that for the average length documents, the score is about right, so the normalizer would be 1. But if a document is longer than the average document length, then there will be some penalization, whereas if it's a shorter, then there's even some reward. So this is illustrated using this slide. On the axis, X-axis, you can see the length of document. On the Y axis we show the normalizer in this case, the pivoted length normalization formula for the normalizer is. is seem to be Interpolation of 1 and the normalized document length controlled by a parameter b here. So you can see here, when we first divide the length of the document by the average document length, This not only gives us some sense about how this document is compared with the average document length, but also gives us a. Benefits of not worrying about the unit of. Length, we can measure the length by words or by characters. Anyway, this Normalizer has an interesting property. First we see that if we set the parameter B to 0 then the value would be one, so there's no  normalization at all. So b in this sense controls the length normalization. Whereas if we set b to a nonzero  value then the normalizer would look like this so the value would be higher for documents that are longer than the average document length. Whereas the value of the normalizer would be smaller for shorter documents. So in this sense we see there is a panelization for long documents. And there is a reward for short documents. The degree of penalization is controlled by b because if we set B to a larger value than the normalizer would look like this, there's even more penalization for long documents and more reward for the short documents. By adjusting b which varies from zero to one, we can control the degree of Length normalization. So if we plug in this length normalization factor into the vector space model ranking functions that we have already examined. Then we will end up having the following formulas. And these are in fact the state of art vector space model formulas. So let's look at this. Take a look at each of them. The first one is called a pivoted length Normalization vector space model. And a reference in the end has details about derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed. The idea of component should be very familiar now to you. There is also a query term frequency component. Here. And then in the middle, there is the normalized TF. And in this case we see we used a double logarithm. As we discussed before, and this is to achieve a sub linear transformation. But we also put a document length Normalizer in the bottom. right, so this would cause penalization for long document, because the larger the denominator is then the smaller TF weighting is. And this is of course controlled by the parameter b here. And you can see again if b is set to  zero and there is no length normalization. OK, so this is one of the two most effective vector space model formulas. The next one, called BM25 or okapi. Is. Also similar in that it also has a IDF of component here. And a query TF component here. But in the middle, the normalization is a little bit different. As we explained there is this. okapi TF transformation here. That does sublinear transformation with the upper bound. In this case, we have put the length normalization. Factor here we are adjusting K, but it achieves a similar factor, just because We put a normalizer in the denominator therefore. Again, if a document is longer than the term weight of this model So you can see after we have gone through all the analysis that we talked about. And we have. In the end, reached basically the state of the art retrieval functions. So. So far we have talked about the mainly how to place the document vector in the vector space. And this has played an important role in determining the effectiveness of the retrieval function. But there are also other dimensions where we did not really examine in detail. For example, can we further improve the instantiation of the dimension of the vector space model? Now We've just assumed that the bag of words representation, so each dimension is the word. But obviously we can consider many other choices. For example, stemmed words. Those are the words that are have been transformed into the same root form. So that the computation and computing will all become the same and they can be matched. We can do stop word removal. This is to remove Some very common words that don't carry "any content like ""the"", ""a"" or ""of""." We can use phrases to define dimensions. We can even use latent semantic analysis to find some clusters of words that represent a latent concept as one dimension. We can also use smaller units, like a character N-grams. Those are sequences of N characters for dimensions. However, in practice, people have found that the bag of words representation with phrases is still the most effective one, and it's also efficient. So this is still so far the most popular dimension instantiation method. And it's used in all the major search engines. I should also mention that sometimes we need to do language-specific and domain-specific tokenization, and this is actually very important as we might have variations of terms. That might prevent us from matching them with each other, even though they mean the same thing in some languages like Chinese There is also the challenge in segmenting Text to obtain word boundaries because it's just a sequence of characters. a word might correspond to 1 character or two characters or even 3 characters. So it's Easier in English when we have a space to separate the words, but in some other languages we may need to do some natural language processing to figure out where all the boundaries for words. There is also a possibility to improve the similarity function, and so far we have used the dot product, but one can imagine there are other measures. For example, we can measure the cosine of the angle between two vectors, or we can use Euclidean distance measure. And these are all possible. But dot product seems still the best and one reason is because it's very general. In fact, it's sufficiently general. If you consider the possibilities of doing weighting in different ways. So for example, cosine measure can be regarded as the dot product of two normalized vectors. That means we first normalize each vector and then we take the dot product that would be equivalent to the cosine measure. I just mentioned that the BM 25 seems to be one of the most effective formulas. But there has been also further development in improving BM. 25, Although none of these works have changed the BM25 fundamentally. So in one line work people have derived BM25F. Here F stands for Field and this is to use BM25 for documents with the structures. So for example you might consider Title Field, the abstract or body or the research article or even anchor text. On the web pages, those are the text fields that describe links to other pages, and these can all be combined with appropriate weights of different fields to help improve scoring for a document. When we use BM25 for such a document. And the obvious choice is to apply the BM25 for each field and then combine the scores. Basically the idea of BM25F is to first combine the frequency counts of terms in all the fields. And then apply BM 25. Now this has advantage of avoiding over counting the first occurrence of the term. Remember, in the sub linear transformation of TF, the first occurrence is very important and it contributes a large weight and if we do that for all the fields than the same term might have gained a lot of advantage in every field. But when we combine these word frequencies together, we just do the transformation. A one time at that time, then the extra occurrences will not be counted as fresh first occurrences. And this method has been working very well for scoring structure documents. The other line of extension is called a PM 25 plus. In this line, researchers have addressed the problem of over penalization of long documents by BM 25. So to address this problem, the fix is actually quite simple. We can simply add a small constant to the TF normalization formula, But what's interesting is that we can analytically prove that by doing such a small modification. We will fix the problem of over penalization of long documents by the original BM25 so the new formula, called BM25+. Is empirically an analytically shown to be better than BM25. So to summarize, all what we have said about the vector space model. Here are the major takeaway points. First, in such a model we use the similarity notion relevance, assuming that the relevance of a document with respect to a query is. Basically proportional to the similarity between the query and document, so naturally that implies that the query an document must be represented in the same way, and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general. And we generally need to use a lot of heuristics to design the ranking function. We use. Some examples to show the need for several heuristics, including TF weighting and transformation. and IDF weighting and document length normalization. These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of text. And finally, BM25 and pivoted normalization seems to be the most effective formulas out of the vector space model. Now I have to say that I've put BM 25 in the category of vector space model. But in fact the BM 25 has been derived using probabilistic modeling. So the reason why I've put it in the vector space model is. First, the ranking function actually has a nice interpretation in the vector space model, we can easily see it looks very much like a vector space model with a special weighting function. The second reason is because the original BM25 has somewhat different form of IDF. And that form of IDF actually doesn't really work so well as the standard IDF. That you have seen here, so as effective retrieval function BM25 should probably use a heuristic modification of the IDF to make it even more look like a vector space model. There are some additional readings. The first is a paper about the pivoted length normalization. It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derived length normalization formula. The second is the original paper where BM25 was proposed. The 3rd paper has a thorough discussion of BM25 and its extensions. Particularly BM25F. And finally, the last paper has a discussion of improving BM25 to correct the over penalization of long documents.
410	a9456b11-885e-4481-b745-9706b5fd893e	So let's plug in these smoothing methods into the ranking function to see what we will get. So, this is a general smoothing... sorry, general ranking function for smoothing with collection language model. You have seen this before. And now we have a very specific smoothing method, the JM smoothing method. So now let's see what what's the value for alpha sub D here. And, what's the value for P sub seen here? So we may need to decide this in order to figure out the exact form of the ranking function, and we also need to figure out, of course, Alpha. So let's see, well, this ratio. Is basically this right? So Here this is the probability of seeing word on the top. And this is the probability of unseen word, or in other words, lambda is basically the alpha here. So it's easy to see that this can be rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? It will be just Lambda, right? And, what would happen if we plug in this value here? If this is lambda, what can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we end up having this ranking function shown here. And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms. This is the element of the query vector what do you think is the element of the document vector? It's this, so that's our document. vector element. And let's further examine what's inside this logarithm. So one plus this, so it's going to be a non-negative  log of this. It's going to be at least one, right? And this is a parameter. So Lambda is parameter and let's look and this is a TF. Now we see very clearly this TF weighting here. And. The larger the count is, the higher the weight will be. We also see IDF weighting which is given by this. And with our document length normalization here. So all these heuristics are captured in this formula. What's interesting that we kind of have got this weighting function automatically by making various assumptions, whereas in the vector space model we had to go through those heuristic design in order to get this. And in this case note that there is a specific form and we can see whether this form actually makes sense. So what do you think Is the denominator here? This is the length of document, total number of words multiplied by the probability of the word given by the collection. So this actually can be interpreted as expected count of the word. If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document. If you do that, the expected count of a word W would be precisely given by this denominator. So this ratio basically is comparing the actual count here. The actual count of the word in the document with the expected count given by this product. If the word is in fact the following, the distribution in the collection this. And if this counter is larger than the expected count, this part, this ratio would be larger than one. So that's actually a very interesting interpretation, right? It's very natural. And intuitively it makes a lot of sense. And this is one advantage of using this kind of probabilistic reasoning. Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here. And we also have a formula that intuitively makes a lot of sense. And does TF-IDF weighting and document length normalization. Let's look at the Dirichlet Prior Smoothing. It's very similar to the case of JM smoothing. In this case, the smoothing parameter is Mu and that's different from lambda that we saw before, but the format looks very similar. The form of the function looks very similar. So we still have linear interpolation here. And when we compute this ratio, while we defined that is that the ratio is equal to this. But what's interesting here is that we are doing another comparison here now. We're comparing the actual count with the expected count of the word if we sample Mu words according to the collection of the probability. So note that it's interesting we don't even see document length here. Unlike in the JM smoothing. So this of course should be plugged into this part. So you might wonder, where  is the document length? Interestingly, the document length is here. In alpha sub d so this would be  plugged into this part. As a result, what we get is the following function here, and this is again a sum over all the matched query words. And we again see the query term frequency here. And you can interpret this as the element of a document vector. But this is no longer a simple dot product, right? Because we have this part. And note that n is the length of the query. So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. But it's still It's still clear that it does document length normalization because this lens is in the denominator, so a longer document will have a lower weight here. And we can also see it has TF here and then IDF. Only that this time the form of the formula is different from the previous one in JM smoothing. But intuitively, is still implements TF IDF weighting and document length normalization. Again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now there are also disadvantages of this approach, and that is there's no guarantee that such a form of the formula would actually work well. So if you look back at this retrieval function. Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation. But Fortunately we can see here. There is a logarithm function here, so we do have also the here, right? So we do have the sub-linear transformation, but we did not intentionally do that. That means there's no guarantee that will end up in this in this way. Suppose we don't have logarithm, then there's no sub linear transformation. As we discussed before, perhaps the formula is not going to work so well. So that's example of the gap between formal model like this and the relevance that we have to model, which is really a subjective machine. That is tight to users. So it doesn't mean we cannot fix this. For example, imagine if we did not have this logarithm, right? So we can heuristically add one, or we can even add a double logarithm, but then it would mean that the function is no longer probabilistic model. So the consequence of the modification is no longer as predictable as what we have been doing now. So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25. In particular, how do we use query likelihood to derive a model that would work consistently better than BM25? Currently we still cannot do that. It's still an interesting open question. So to summarize this part we've talked about the two smoothing methods. Jelinek-Mercer, which is doing fixed coefficient linear interpolation. Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. In both cases we can see by using these smoothing methods we would be able to reach a retrieval function, whether assumptions are clearly articulated, so they're less heuristic. Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization. So this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design. Yet in the end, we naturally implemented TF IDF weighting and document length normalization. Each of these functions also has precisely one smoothing parameter. In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters. So overall this shows by using probabilistic model we follow very different strategy than the vector space model. Yet in the end we end up with some retrieval functions that look very similar to a vector space model with some advantages in having assumptions clearly stated. And then the form dictated by probabilistic model. Now, this also concludes our discussion of the query likelihood problems model. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. We basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made. It is a query words are generated independently that allows us to decompose the probability of the whole query. Into a product of probabilities of all the words in the query. And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made  one of these two assumptions about the smoothing. So we either use JM smoothing or the Dirichlet smoothing. If we make these four assumptions, then we have no choice but to take the form of the retrieval function that we have seen earlier. Fortunately, the function has a nice property in that implements TF IDF weighting and documents length normalization And, these functions also work very well, so in that sense these functions are less heuristic compared with a vector space model. And there are many extensions. This basic model and you can find the discussion of them in the reference at the end of this Lecture.
410	a97a9d5e-48b7-4f4e-9754-4c5b30a31424	This lecture is about text based prediction. " In this lecture we're going to start talking about mining a different kind of knowledge as you, you can see here on this slide. here on this slide. Namely, we're going to use text data to infer values of some other variables in the real world. That may not be directly related to the text, or only remotely related to text data. So this is very different from content analysis or topic mining where we directly characterize the content of text. " It's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective content  which reflects what we know about the opinion holder. But this only provides limited view of what we can predict. In this lecture and the following lectures, we're going to talk more about how we can predict more information about the world. How can we get sophisticated patterns of text together with other kinds of data? " It would be useful to first take a look at the big picture of prediction in data mining in general and I call this data mining loop. " So the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the real world in the form of data. " And of course the data are in the form of non text data and text data. And our goal is to see if we can predict some values of important real world variables that matter to us. For example, someone's health condition or the weather, or etc. So these variables would be important because we might want to act on that. We might want to make decisions based on that. So how can we get from the data to these predicted values? Well, in general we first have to do data mining and analysis of the data. Because we in general should treat all the data that we collected. in such a prediction problem set up, we are very much interested in joint mining of non text and text data. We should mine all the data together. And then through the analysis, we generally can generate the multiple predictors of this interesting variable to us, and we call these features. And these features can then be put into a predictive model to actually predict the value of any interesting variable. So this then allows us to change the world and so this basically is the general process for making a prediction based on data, including text data. Now it's important to emphasize that human actually plays very important role in this process. Especially because of the involvement of text data. And so human first would be involved in the mining of the data. It will control the generation of these features. And also help us understand the text data because text data are created to be consumed by humans. Humans are the best in consuming or interpreting text data. But when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. " Sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text data in all applications. Next human also must be involved in predictive model building and adjusting or testing. So in particular we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model, and then next, of course, when we have predicted values for the variables, then humans would be involved in taking actions to change the world or make decisions based on these predictive values. And finally, it's interesting that human could be also involved in controlling the sensors. And, this is so that we can adjust the sensors to collect the most useful data for prediction. So that's why I called this data mining loop because as we perturb the sensors to collect the new data and more useful data then we will obtain more data for prediction. This data generally will help us improve the prediction accuracy and in this loop are humans will recognize what additional data needs to be collected and machines would of course help humans identify what data should be collected next. In general, we want to collect data that are most useful for learning. And this there is actually a subarea in machine learning called active learning that has to do with this. How do you identify data points? That would be most helpful for machine learning programs if you can label them, right. So in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the world and then observe what happens. And then you can then decide what additional data. Have to be collected by adjusting the sensor. Or from the prediction errors you can also know what additional data we need to acquire in order to improve the accuracy of prediction. And this big picture is actually very general and it's reflecting a lot of important applications of big data. So it's useful to keep that in mind while we're looking at some text mining techniques. " So from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. And this is most useful for prediction about human behavior or human preferences or opinions. But in general text data will be put together with non text data. So the interesting questions here would be first how can we design effective predictors? And how do we generate such effective predictors from text? This question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. It has also been addressed to some extent by talking about the other knowledge that we can mine from text. So for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. So topics can be intermediate representation of text. That would allow us to design high level features or predictors that are useful for prediction of some other variable. It maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors. And similarly, sentiment analysis can lead to such predictors as well. So those are the data mining or text mining algorithms can be used to generate the predictors. The other question is how can we join mine text and non text data together? Now this is a question that we have not addressed yet. So in this lecture and the following lectures we're going to address this problem because this is where we can generate the much more enriched features for prediction and allows us to review a lot of interesting knowledge about the world. These patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction. Basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. The goal here is mainly to infer values of real world variables. But in order to achieve the goal, we can do some other preparations and these are sub tasks. So one sub task could be mine, mine the content of text data like topic mining. And the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. And both can help provide predictors for the prediction problem. And of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. And such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. As we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. In one perspective, we can see non text data can help text mining. Because non text data can provide a context for mining text data. Provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. And that's to mine text in the context defined by non text data. And you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures. Now the other perspective is text data can help non text data mining as well. And this is because text data can help interpret patterns discovered from non text data. This helps discover some frequent patterns from non text data. Now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur. And this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content " is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns. And this technique is called pattern annotation. And, you can see this reference listed here for more detail. So here are the reference that I just mentioned. The first is reference for pattern annotation. " The second is a Qiaozhu Mei dissertation on contextual text mining. It contains a large body of work on contextual text mining techniques. "
410	aac4e33c-97bb-46c8-a108-3e3e3322a85c	This lecture is about the methods for text categorization. So in this lecture were going to discuss how to do text categorization. 1st. There are many methods for text categorization In such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. So, for example, if you want to do topical categorisation for news articles, you can say if the news article mentions word like game and Sports three times that we're going to say it's about sports. Things like that and this would allow us to deterministically decide which category A document should be put into. Now such a strategy would work well if the following conditions hold. First, the categories must be very well defined, and this allows the person to clearly decide the category based on some clear rules. Secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. You can easily identify text data. For example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category. Now we also should have sufficient knowledge. For designing these rules and so if that's the case, then such a method can be effective, and so it does have a provisions in some domains and sometimes. However in general there are several problems with this approach. First, of course it's labor intensive. It requires a lot of manual work. Obviously we can't do this for all kinds of categorization problems. We have to do it From scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well.  Secondly, it cannot handle uncertainties in rules. Often the rules aren't 100% reliable take for example, and looking at the occurrences of words in text and try to decide the topic. It's actually very hard to have 1% correct the rule. So for example, you can say if it has games, sports, basketball, then for sure it's about sports. But one can also imagine some text articles that mention these keywords. But that may not be exactly about the sports, or only marginally touching sports. The main topic could be another topic, different topic then sports. So that's one disadvantage of this approach, and then finally the rules may be inconsistent and this would need to concern about robustness more specifically, and sometimes the results of categorization may be different depending on which rule to be applied. So in that case then you will face uncertainty and you will also have to decide the order of applying the rules or combination of results that are contradictory. So all these. Problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning. So these machine learning methods are more automatic, but I still put automatic in quotation marks cause they're not really completely automatic because it still require manual work. More specifically, we have to use human experts to help in two ways. First, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories. And this is called a training data. And then Secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. So we need to provide some basic features for the computers to look into. And in the case of text, natural choice would be the words. So using each word as a feature is a very common choice to start with. But of course there are other sophisticated features like phrases or even policy feature tags or even syntactic structures. So once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. So soft rules just means we're going to still decide which category should be assigned to the document. But it's not going to be used using a rule that is deterministic, so we might use something similar to saying that if it matches game sports many times, it's likely to be a sports. But we're not going to say exactly for sure, but instead we're going to use probabilities or weights so that we can combine multiple evidences, so the learning process basically is going to figure out which features are most useful for separating different categories. And it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. It's the basis for learning. And then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. If the human would to make a judgement. So when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. So the setup is. To learn a classifier to map a value of X into a map of Y. So here X is all the text objects. And Y is all the categories a set of categories, so the classifier would take any value in X as input and we generate the value in Y as output, and we hope the output Y would be the right category for X, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of Input and output for function and then the computer is going to figure out how the function behaves like based on these examples and then try to be able to compute the values for future access that we have not seen. So in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. And they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. So ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. Different methods tend to vary in their ways of measuring the errors on the training data. They might optimize a different object function, which is often also called a loss function or cost function. They also tend to vary in their ways of combining the features, so linear combination for example is simple is often used. But they're not as powerful as non linear combination, but nonlinear models might be more complex for training. So there are tradeoffs as well, but that would lead to different variations of. Many variations of these learning methods. So in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers. The other is called discriminative classifiers. The generative classifiers try to learn what the data looks like in each category. So it attempts to model the join the distribution of the data and the label X&Y. And, this can then be factored out to a product of Y. The distribution of labels and join the probability of sorry the conditional probability of X given Y so it's Y. So we first model distribution of labels and then we model how the data is generated given a particular label here. And once we can estimate these models, then we can compute this conditional probability of label given data based on. The probability of data given label. And the label distribution here by using the base rule. Now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. So in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors. But if we can model the data in each category accurately, then we can also classify accurately. One example is naive Bayes classifier. In this case. The other kind of approaches are called discriminative classifiers. These classifiers try to learn what features separate categories, so they directly tackle the problem of categorisation or separation of classes. So sorry for the problem. So these discriminative classifiers attempted to model the. Conditional. Probability of the label given the data point directly. So the objective function tends to directly measure the errors of categorisation on the training data. Some examples include the logistical regression support vector machines and the K nearest neighbors. We will cover some of these classifiers in detail in the next few lectures.
410	ab2240b1-d836-4dbc-bcca-0a99472355d6	In this lecture we're going to talk about text access. In the previous lecture we talk about natural language content analysis. We explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner. As a result, bag of words representation remains very popular in applications like search engines. In this lecture we're going to talk about some high level strategies. To help users get access to the text data. This is also important step to convert raw big text data into small relevant data that are actually needed in a specific application. So the main question we will address here is how can a text information system help users get access to the relevant text data we're going to cover two complementary strategies, push versus pull. And then we're going to talk about the two ways to implement the pull mode: querying versus browsing. So first push versus pull. These are two different ways to connect users with the right information at the right time. The difference is. Which takes the initiative. Which party takes the initiative? In the pull mode, the users would take the initiative. To start the information access process. And in this case, a user typically would use a search engine to fulfill the goal. For example, the user may type in the query and then browse results to find the relevant information. So this is usually appropriate for satisfying a users Ad hoc information need. An ad hoc information need is temporary information need, for example. You want to buy a product so you suddenly have a need to read reviews about related products. But after you have collected information and have purchased your product. You generally no longer need such information, so it's a temporary information need. In such a case, it's very hard for a system will predict your need and it's more appropriate for the users to take the initiative, and that's why search engines are very useful today because many people have many ad hoc information needs all the time. So as we're speaking Google is probably processing many queries from us and those are all or mostly all ad hoc information needs. So this is a pull mode in contrast, in the push mode, the system will take the initiative to push the information to the user, or to recommend that information to the user. So in this case this is usually supported by a recommender system. Now this would be appropriate if the user has a stable information need. For example, you may have a research interest in some topic and that interest tends to stay for awhile, so it's relatively stable. Your hobby is another example of a stable information need. In such a case, the system can interact with you and can learn your interest and then can monitor the information stream. If it is, the system has seen any relevant items to your interest the system could then take the initiative to recommend information to you. So for example, a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you. This mode of information access maybe also appropriate when the system has good knowledge about the users need and this happens in the search context. So for example, when you search for information on the web, a search engine might infer you might be also interested in some related information. And they would recommend the information to you, so that should remind you, for example advertisement placed on search page. So this is about the two high level strategies or two modes of text access. Now let's look at the pull mode in more detail. In the pull mode, we can further distinguish in two ways to help users querying versus browsing. In querying the user will just enter a query. Typical keyword query and the search engine system would return relevant documents to users. And this works when the user knows what exactly are the keywords to be used. So if you know exactly what you're looking for, you tend to know the right keywords, and then querying would work very well and we do that all the time. But we also know that sometimes it doesn't work so well, when you don't know the right keywords to use in the query or you want to browse information in some topic area. In this case browsing would be more useful. So in this case. In the case of browsing, the users would simply navigate into the relevant information by following the paths supported. By the structures documents. So the system would maintain some kind of structures and then the user could follow these structures to navigate. So this really works well when the user wants to explore the information space. Or the user doesn't know what are the key words to use in the query. Or simply because the user finds it inconvenient to type in a query. So even if the user knows what query to type in, if the user is using a cell phone. To search for information there, it's still hard to enter the query in such a case. Again, browsing tends to be more convenient. The relationship between browsing and the query is best understood by making an analogy to sight seeing. Imagine if you are touring the city now. If you know the exact address of the attraction then taking a taxi, there is perhaps the fastest way you can go directly to the site, but if you don't know the exact address you may need to walk around, or you can take a taxi to a nearby place and then walk around. It turns out that we do exactly the same in the information space. If you know exactly what you're looking for, then you can use the right keywords in your query to find the information directly. That's usually the fastest way to do find information. But what if you don't know the exact keywords to use? Your query probably won't work, so you'll land on some related pages, and then you need to also walk around in the information space, meaning by following the links or by browsing. You can then finally get into the relevant page. If you want to learn about the topic again you will likely do a lot of browsing. So just like you are looking around in some area and you want to see some interesting attractions in a related- in the same region. So this is analogy also tells us that today. We have very good spot for query but we don't really have good support for browsing. And this is because. In order to browse effectively, we need a map to guide us. Just like you need a map of Chicago to tour the city of Chicago, you need a topic map to tour the information space. So how to construct such a topic map is in fact a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications. So to summarize this lecture we've talked about the two high level strategies for text access, push and pull. Push tends to be supported by recommender systems and pull tends to be supported by a search engine. Of course in a sophisticated intelligent information system we should combine the two. In the pull mode we can further distinguish querying and browsing again, we generally want to combine the two ways to help users so that you can support both querying and browsing. If you want to know more about the relationship between pull and push. You can read this article. This gives excellent discussion of the relationship between information filtering and information retrieval. Here, information filtering is similar to information recommendation or the push mode of information access.
410	aca8d826-412b-4134-8d2c-87537fdc4a76	So looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. Especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. So a more general picture would be to include non text data as well. And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. But we can also touch how to join the analysis of both text data and non-text data. With this problem definition we can now look at the landscape of the topics in text mining analytics. Now this slide shows the process of generating text data in more detail. Most specifically, human sensor or human observer would look at the world from some perspective. Different people would be looking at the world from different angles and they will pay attention to different things. The same person at a different time might also pay attention to different aspects of the observed world. And so the human sensor would perceive the world from some perspective. And that human... The sensor would then form a view of the world and that can be called the observed world. Of course this would be different from the real world because of the perspective that the person has taken. This can often be biased also. Now the observable world can be represented as for example entity relation graphs or more in a more general way, using knowledge representation language. But in general, this is basically what a person has in mind about the world, and we don't really know what exactly it looks like, of course. But then the human would express what the person has observed using a natural language such as English, and the result is text data. Of course, the person could have used a different language to express what he or she has observed. In that case, we might have text data of mixed languages for different languages. So the main goal of text mining is actually to revert this process of generating test data. And we hope to be able to uncover some aspect in this process. And so specifically we can think about the mining, for example, knowledge about the language. And that means by looking at text data in English, we may be able to discover something about English... Some usage of English... Some patterns of English. So this is 1 type of mining problems where the result is some knowledge about language which may be useful in various ways. If you look at the picture, we can also "then mine knowledge about the ""Observed" "World""." As so, this has much to do with mining the content of text data. We're going to look at the what the text data are about and then try to get the essence of it. Or extracting high quality information about a particular aspect of the world that we're interested in. For example, everything that has been said about a particular person or particular entity, and this can be regarded as mining content to describe the observed world in the user's mind, in the person's mind. If you look further then you can also imagine we can mine knowledge about this observer himself or herself. So this has also to do with using text data to infer some properties of this person. And these properties could include the mood of the person or sentiment of the person. And note that we distinguish the observed the world from the person because text data can describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? So indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. And we want to predict the value of certain interesting variables. So this picture basically covered multiple types of knowledge that we can mine from text in general. When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. For example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world. Now of course, this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important. And that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction. In fact, when we have a non-text data, we could also use the non-text data to help prediction. And of course, it depends on the problem. In general, non-text data can be very important for such prediction tasks. For example, if you want to predict the stocks. Stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables. Now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction. Now non-text data can be also useful for analyzing text by supplying context. When we look at the text data alone will be mostly looking at the content and opinions expressed in text. But text data generally have also context associated. For example, the time, the location, of that associated with the text data and these are useful context information. And the context can provide interesting angles for analyzing text data. For example, we might partition text data into different time periods because of the availability of time. Now we can analyze text data in each time period and then make a comparison. Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. So in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. In this course we're going to selectively cover some of those topics. We actually hope to cover most of these general topics. First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. It's also one of the most useful techniques in text mining. And then we're going to talk about opinion mining and sentiment analysis. So this can be regarded as one example of mining knowledge about the observer. And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data. So this slide also serves as a road map for this course. And will use this as outline for the topics that will cover in the rest of this course.
410	acd44eef-f129-496a-adca-4e46d40b0a44	In this lecture we are going to talk about how to improve the instantiation of the vector space model. This is the continued discussion of the vector space model. We're going to focus on how to improve the instantiation of this model. In the previous lecture, you have seen that with simple same instantiations of the vector space model. We can come up with a simple scoring function that would give us basically a count of how many unique query terms of matching the document. We also have seen that this function has a problem. As shown on this slide, in particular, if you look at these three documents, they will all get the same score because they matched 3 unique query words. But intuitively we would like D4 to be ranked above D3 and D2 is really non relevant. So the problem here is that this function could not capture. At the following heuristics: First, we would like to give more credit to D4 because it matched the presidential more times than these three; Second, Intuitively, matching presidential should be more important than matching about because about this very common word that occurs everywhere. It doesn't really carry that much content. So in this natural, let's see how we can improve the model to solve these two problems. It's worth thinking at this point about the """why do we have these two problems?""" If we look back at assumptions we have made while instantiating the vector space model, we will realize that the problem is really coming from some of the assumptions. In particular, it has to do with how we place the vectors in the vector space. So then, naturally, in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different ways to instantiate the vector space model. In particular, we have to place the vectors in a different way. So let's see how can improve this. One natural thought is, in order to consider multiple times of the term in the document, we should consider the term frequency instead of just absence or presence. In order to consider the difference between a document where aquarium occurred multiple times and one where the query term occurs just once, we have to consider the term frequency: The count of a term in the document. In the simplest model, we only model the presence and absence of the time. We ignore the actual number of times that term occurs in the document. So let's add this back so we can do then represent a document by a vector with term frequency as elements. So that is to say, now the elements of both queries vector and document vector will not be 0 or 1s, but instead they will be the counts of word in the query or the document. So this would bring in additional information about the document. So this can be seen as a more accurate representation of our documents. So now let's see what the formula would look like if we change this representation. So as you see on this slide we still use DOT product, and so the formula looks very similar in the form. In fact it looks identical, but inside the sum of course Xi and Yi are now different, and all the counts of word I in the query and in the document. Now at this point I also suggest you to pause the lecture for a moment and just think about how we can interpret the score of this new function. It's doing something very similar to what the simplest VSM is doing. But because of the change of the vector, now the new score has a different interpretation. Can you see the difference? And it has to do with the consideration of multiple occurrences of the same term in a document. More important, they would like to know whether this would fix the problems of the simplest of vector space model. So let's look at this example again. So suppose we change the vector representation into term frequency vectors. Now let's look at these three documents again. The query vector is the same because all these words occur exactly once in the query, so the vector is 001 vector. And in fact, D2 is also essentially representing the same way, because none of these words has been repeated many times as a result of the score is also the same, still 3. The same goes for D3. And we still have a 3. But D4 would be different. Because now presidential occured twice here. So the element for presidential in the document factor would be 2 instead of 1. As a result, now the score for D4 is high. It's 4 now. So this means, by using term frequency we can now rank D4 above D2 and D3 as we hope to. So this solved the problem with D4. But we can also see that, D2 and D3 are still treated in the same way. They still have identical scores, so it did not fix the problem here. So how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about, but how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly, and which word can be basically ignored about is such a word at which does not really carry that much content? We can essentially ignore that we sometimes call such a word stop word. Those are generally very frequently occur everywhere matching it doesn't really mean anything, but computationally. How can we capture that? So again, I encourage you to think a little bit about this. Can you come up with any statistical approaches to somehow distinguish presidential from about? If you think about it for a moment, you will realize that what differences that words like about occurs everywhere. So if you count the occurrence of the word in the whole collection, then we would see that about has much higher frequency than presidential which tends to occur only in some documents. So this idea suggest that we could somehow use global statistics of terms or some other information to try to down weight the element that for about in the vector representation of D2. At the same time, we hope to somehow increase the weight of presidential in the vector of these three. If we can do that, then we can expect the D2 will get the overall score to be less than 3. While D3 will get the score above 3, then we would be able to rank this lead on top of D2. So how can we do this systematically? Again, we can rely on some statistical counts, and in this case the particular idea is called the inverse document frequency. We have seen document frequency as one signal used in the modern retrieval functions. We discussed this in previous lecture, so here's a specific way of using it. Document frequency is the count of documents that contain a particular term. Here we said inverse document frequency because we actually want to reward a word that doesn't occur in many documents. And so the way to incorporate this into our vector representation is then to modify the frequency count. By multiplying it by the idea of the corresponding word as shown here. If we can do that, then we can penalize common words which generally have a low IDF and reward rare words which will have high IDF. So more specifically, the IDF can be defined as a logarithm of (M + 1) / K, where M is the total number of documents in the collection, K is the DF or document frequency, the total number of documents containing the word W. Now if we plot this function by varying k, then you will see the curve would look like this. In general, you can see it would give a higher value for a low DF word, a rare word. You can also see the maximum value of this function is log of M + 1. Will be interesting for you to think about what's minimum value for this function? This could be interesting exercise. Now a specific function may not be as important as the heuristic to simply penalize popular terms. But it turns out that this particular function form has also worked very well. Now, whether there is a better form of function here is still open research question, but it's also clear that if we use a linear panelization like what's shown here with this line, then it may not be as reasonable as the standard IDF. In particular, you can see the difference. In the standard IDF, and we somehow have a turning point here. After this point they were gonna say these terms are essentially not very useful. They can be essentially ignored and this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important, and it's basically a common term. It's not very important to match this word, so with the standard idea you can see it's basically assuming that they all have lower weights. There's no difference. But if you look at the linear panelization at this point that there is some difference. So intuitively we want to focus more on the discrimination of Low DF words rather than these common words. Of course, which one works better still has to be validated by using the empirical created dataset and we have to use users to judge which results are better. So now let's see how this can solve problem 2. Alright, so now let's look at the two documents again. Now, without the IDF weighting before, we just have term frequency vectors, but with IDF weighting, we now can adjust the TF weight by multiplying with the IDF value. For example, here we can see is adjustment, and in particular for about there is adjustment by using the IDF  value of about which is smaller than the IDF value of presidential. So if you look at these, the IDF will distinguishing these two words as a result of adjustment here would be larger, would make this weight larger. So if we score with these new vectors, then what would happen is that of course they share the same weights for news and campaign. But the matching of about and presidential with discriminate them. So now as a result of IDF weighting, we will have D3 to be ranked above D2 becauses it matched rare word whereas D2 matched common word. So this shows that the IDF weighting can solve problem 2. So how effective is this model in general? When we use TF IDF weighting, let's look at the obvious documents that we have seen before nicely. These are the new scores of the new documents, but how effective is this new weighting method and new scoring function? So now let's see overall how effective is this new ranking function with TF IDF weighting? Here we show all the five documents that we have seen before, and these are their scores. Now we can see the scores for the first 4 documents here seem to be quite reasonable. They are as we expected. However, we also see a new problem. Because Now D5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score. In fact, it has the highest score here. So this creates a new problem. This is actually a common phenomenon in designing retrieval functions. Basically, when you try to fix one problem, you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function. Is there open research questions researchers are still working on that? But in the next few lectures, we'll are also gonna talk about some additional ideas to further improve this model and try to fix this problem. So to summarize this lecture we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TF IDF weighting. So the improvement mostly is on the placement of the vector where we give higher weight to a term that occured many times in the document but infrequently in the whole collection. And we have seen that this improvement model indeed works better than the simplest vector space model. But it also still has some problems. In the next lecture, we're going to look at the how to address these additional problems.
410	ad71da51-7567-4149-9eef-0139aad79369	so now let's take a look at the specific method that's based on regression now this is one of the many different methods and in fact it's one of the simplest methods i choose this to explain the idea because it's simple so in this approach we simply assume that the relevance of the document with respect to the query is related to a linear combination of all the features here i used X I two in note the feature so X I of Q M D is a feature and we can have as many features as we would like and we assume that these features can be combined in the linear manner each video is controlled by a parameter here and this parizer parameter that's a waiting parameter a larger value with me the feature would have highway tan with contribute more through the scoring function the specific form of the function actually also involves a transformation of the probability of relevance so this is a probability of relevance we know that the probability of relevance is within the range from zero to one and we could have just assumed the scoring function is related to this linear combination so we can do linear regression but then the value of this linear combination could easy to go beyond one so this transformation here would map zero two one range through the whole range of real values you can you can verify it by yourself so this allows us meant to connect with the probability of relevance which is between zero and one to a linear combination of arbitrary features and if we rewrite this into a probability function will get the next one so on this equation that we will have the probability of relevance an on the right hand side it will have this form now this form is clearly non active and it still involves the linear combination of features and it's also clear that is if this value is this is actually negative of the linear combination in the question above if this value here if this value is large than it would mean this value is small and therefore this probability is whole probability would be large and that's what we expect basically it would mean if this combination gives us a high value then the documents more likely relevant so this is our hypothesis again this is not necessarily the best of hypothesis but this is a simple way to connect these features with the probability of relevance so now we have this combination function the next task is to see how to estimate the parameters so that the function character will be applied without knowing the beta values it's harder to apply this function so let's see how we can estimate data values let's take a look at simple example in this example we have three features once BM twenty five score of the document and query one is the page rank score of the document which might or might not depend on the query we might have a topic sensitive page brand that would depend on query otherwise the general page rank doesn't really depend on query and then we have P M twenty five score on the anchor text of the document these are then the feature values for a particular top document query pair an in this case the document is D one and the judgment says that is relevant here's another training instance and with these feature values but in this case it's not relevant OK this is over simplified case where we just have two instances but it's sufficient illustrated the point so what we can do is we use the maximum micro estimator to actually estimate the parameters basically we're going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here can we predict the the relevance yeah and of course the prediction will be using this function that you see here and we hypothesize that the probability of relevance is related to features in this way so we are going to see for what values of beta we can predict the relevance well what do we mean well what do we mean by predicting the relevance well well we just mean in the first case for DVR this expression here right here should give a high values in fact that we hope this to give a value closely one why becaus this is relevant document on the other hand in the second case for D two we hope this value will be small becaus it's nonrandom the document so now let's see how this can be mathematical expressed this is similar to expressing the probability of document only that we're not talking about the probability of words but talking about probability of relevance one or zero so what's the probability of this document the relevant if it has these feature values well this is just this expression right we just need to plug in the excise so that's what we will get it's exactly like a what we have seen about only that we replaced these excise with now specific values i saw for example this point seven goes to here and this point eleven close to here and these are different feature values then we combine them in this particular way the better values are still unknown but this gives us the probability that this document is relevant if we assume such a model OK we want to maximize this probability since this is a random in the document what do we do for the second document well we want to compute the probability that the predictions is not relevant so this would mean we have to compute one minus this expression since this expression is actually the probability of relevance so to compute the non relevance from relevance we just one minus the probability of relevance OK so this whole expression then just is our probability of predicting these two relevance values one is one here one is zero and this quick question is our probability of observing a one here an offer zero here of course this probability depends on the beta values right so then our goal is to adjust the beta values to make this whole thing which is maximum make the as large as possible so that means we're going to compute this the beta is just the the parameter values that would maximize this whole like hold expression and what that means is if you look at the function is working to choose battles to make this as large as possible and make this also as large as possible which is equivalent to say make this the other as small as possible and this is precisely what we want so once we do the training now we will know the beta values so then this function would be well defined once better values are known propose this and this would be completed this best file so for any new query an new documents we can simply compute the features for that pair and then we just use this formula to generate the ranking scope and this scoring function can be used to rank documents for a particular query so that's the basic idea of learning to rank
410	ae943406-56a1-4b3b-9515-637ea27e438a	This lecture is a summary of this whole course. First, let's revisit the topics that we covered in this course. In the beginning, we talked about the natural language processing and how it can enrich text representation. We then talked about how to mine knowledge about the language, natural language used to express what's observed in the world of text data. And in particular, we talked about how to mine word associations. We then talked about how to analyze topic syntax, how to discover topics, and analyze them. This can be regarded as knowledge about the oberved of the world, and then we talked about how to mine knowledge about the observer and particularly talk about how to mine opinions and do sentiment analysis. And finally, we talked about the text based prediction, which has to do with predicting values of other real world variables based on text data. And in discussing this, we also discussed the rule of non-text data which can contribute to additional predictors for the prediction problem and also it can provide a context for analyzing text data. And in particular, we talked about how to use context to analyze topics. So, here are the key high-level takeaway messages from this course. I'm gonna go over each of these major topics and point out what are the key takeaway messages that you should remember. First in NLP and text representation, you should realize that NLP is always very important for any text applications because it enriches text representation the more NLP, better text representation we can have and this further enables more accurate knowledge discovery, to discover deeper knowledge buried in text. However, the current state of the art of natural language processing is still not robust enough, and so as a result, the robust text mining technologies today tend to be based on word representation and tend to rely a lot on statistical analysis, as we have discussed in this course. You may recall we mostly used word based representations, and we've relied a lot on statistical techniques,  on statistical learning techniques particularly. In Word Association Mining and analysis,  the important points are first are we introduced the two concepts for two basic plan complementary relations of words, paradigmatic and syntagmatic relations. These are actually very general relations between elements in any sequences. If you take it as meaning elements that occur in similar context in the sequence and elements that tend to co-occur with each other and these relations might also be meaningful for other sequences of data. We also talked a lot about the text similarity when we discuss how to discover paradigmatically relations, we compare their context of words, discover words that share similar contexts. At that point that we talked about, representing text data with a vector space model, and we talked about some retrieval techniques such as BM25 for measuring similarity of text and for assigning weights to terms, TF-IDF weighting, etc. And this part is well connected to text retrieval. There are other techniques that can be relevant here also. The next point that is about the co- occurrence analysis of text and we introduced some information theory concepts such as entropy, conditional entropy and mutual information. These are not only very useful for measuring the co-occurrences of words, and they're also very useful for analyzing other kind of data, and they're useful for example for feature selection in text categorization as well. So this is another important concept to know. And then we talked about the topic mining and analysis and that's where we introduced the probabilistic topic model. We spend a lot of time to explain the basic topic model PLSA in detail. And this is also the basis for understanding LDA, which is a theoretically more appealing model. But we did not have enough time to really go in depth in introducing LDA. But in practice, PLSA seems as effective as LDA, and it's simpler to implement. It's also more efficient. In this part we also introduce some general concepts that would be useful to know. One is generating model and this is a general method for modeling text data and modeling other kinds of data as well. And we talked about the maximum likelihood estimator and the EM algorithm for solving the problem of computing maximum likelihood estimator. So these are all general techniques that tends to be very useful in other scenarios as well. Then we talked about the text clustering and text categorization. Those are two important building blocks in any text mining application systems. In text clustering we talked about the, how we can solve the problem by using a slightly different than mixture model than the probabilistic topic model. And we then also briefly reviewed some similarity based approaches to text clustering. In categorization, we also talked about the two kinds of approaches. One is generative classifiers, they rely on base rule to infer the conditional probability of a category given text data. In particular, we introduced Naive Bayes in detail. This is a practical, useful technique for a lot of text categorization tasks. We also briefly introduce some discriminative classifiers, particularly logistical regression K nearest neighbor and SVN. There also very important that they are very popular and they're very useful for text categorization as well. In both parts we also discussed how to evaluate the results, and evaluation is quite important because if the measures that you use don't really reflect the utility of the method, then it would give you misleading results. So it's very important to get evaluation right and we can talk the about evaluation of categorisation in detail with a lot of specific measures. Then we talked about the sentiment analysis and opinion mining, and that's where we introduced sentiment classification problem. And although it's a special case of text categorization, but we talked about how to extend or improve the text categorisation method by using more sophisticated features that would be needed for sentiment analysis. We did the review of some commonly used the complex features for text analysis and then we also talked about how to capture the order of these categories in sentiment classification, and in particular we introduced the ordinal logistical regression. Then we also talk about latent aspect rating analysis. This is unsupervised way of using a generated model to understand the review data in more detail. In particular, it allows us to understand the decomposed ratings of reviewer on different aspects of the topic. So given text reviews with overall ratings, the method would allow us to infer the ratings on different aspects. And it also allows us to infer the reviewers latent weights on these aspects on which aspects are more important to the reviewer, can be reviewed as well, and this enables a lot of interesting applications. Finally, in the discussion of text based prediction, we mainly talked about the joint mining of text and non text data as they are both very important for prediction. And we particularly talked about how text data can help non text data and vice versa. In the case of using non text data to help the text data analysis, we talked about the contextual text mining. We introduce the contextual PLSA as a generalization or generalized model of PLSA to allow us to incorporate context variables such as time and location and this is a general way to allow us to review a lot of interesting topical patterns in text data. We also introduced the net PLSA. In this case we use social network or network in general of text data to help analyzing topics. And finally we talked about how time series data can be used as context to mine, potentially causal topics in text data. Now in the other way of using text to help, to help interpreting patterns discovered from non text data, we did not really discuss anything in detail but just provide the reference, but I should stress that that's actually very important direction to know about if you want to build a practical text mining systems, because understanding and interpreting patterns is quite important. So this is a summary of the key takeaway messages. And, I hope these would be very useful to you for building any text mining applications or doing further study of these algorithms and this should provide a good basis for you to read Frontier Research papers to know about more advanced algorithms or to invent new algorithms yourself. So to know more about this topic, I would suggest you to look into other areas in more depth. And during this short period of time of this course, we could only touch the basic concepts, basic principles of text mining and we emphasize the coverage of practical, useful algorithms, and this is at the cost of covering some more advanced algorithms only briefly, or in many cases we omitted the discussion of a lot of advanced algorithms. So to learn more about this subject, you should definitely and learn more about the natural language processing, because this is the foundation for all text based applications. The more NLP you can do, the better representation of texts that you can get and then the deeper knowledge you can discover. So this is very important. The second area that you should look into is statistical machine learning and these techniques are now the backbone techniques for not just text analysis applications, but also for NLP. A lot of NLP techniques are nowadays actually based on supervised machine learning. So they are very important  because they are key to also understanding some advanced NLP techniques and naturally they would provide more tools for doing text analysis in general. Now, a particularly interesting area called Deep Learning has attracted a lot of attention recently. It has also shown promise in many application areas, especially in speech and vision, and it has been applied to text data as well. So, for example, recently there has been work on using deep learning to do sentiment analysis to achieve better accuracy, and so that's one example of advanced techniques that we weren't able to cover. But that's also very important. And the other area that has emerged in statistical learning is the word embedding technique where they can learn vector representation of words and then these vector representations would allow you to compute the similarity of words. As you can see, this provides directly a way to discover potentially paradigmatically relations of words and results that people have got so far are very impressive. That's another promising technique that we did not have time to touch. But of course, whether these new techniques would lead to practical use for techniques that work much better than the current technologies, is the open question that has to be examined. And no serious evaluation has been done yet in, for example, examining the practical value of word embedding other than word similarity based evaluation. But nevertheless, these are advanced techniques that surely will make impact in text mining in the future. So it's very important to know more about these. Statistical learning is also key to predictive modeling, which is very crucial for many big data applications. We did not talk about that predictive modeling component, but this is mostly about the regression or categorization techniques, and this is another reason why statistical learning is important. We also suggested you to learn more about data mining, and that's simply because general data mining algorithms can always be applied to text data which can be regarded as a special case of general data. So there are many applications of data mining techniques in particular, for example, a pattern discovery would be very useful to generate a interesting features for text analysis. Recently, information network mining techniques can also be used to analyze text information network. So these are all good to know in order to develop effective text analysis techniques. And finally, we also recommend you to learn more about the text retrieval information retrieval or search engines. And this is especially important if you're interested in building practical text data application systems. And a search engine would be essential system component in any text based applications, and that's because text data are created for humans, to us to consume. So humans are at the best position to understand the text data. It's important to have human in the loop in a big text data applications. So it can in particular help text mining systems in two ways. One is to effectively reduce the data size from a large collection to a small collection with the most relevant text data that only matter for the particular application. So the other is to provide a way to annotate it, to explain patterns, and this has to do with knowledge provenance. Once we discover some knowledge, we have to figure out whether the discovery is really reliable and so we need to go back to the original text that are verified and that's when the search engine is very important. Moreover, some techniques and information retrieval, for example BM 25, vector space and language models, also very useful for text data mining. We only mention some of them, but if you know more about the text retrieval, you'll see that there are many techniques that are useful. Another technique that's useful is indexing technique that enables quick response of search engine to users query and such techniques can be very useful for building efficient text mining systems as well. So finally I want to remind you of this big picture for harnessing big text data that I showed you at the very beginning of the semester. So in general, to build a big text data application system, we need two kinds of techniques, text retrieval and text mining. And text retrieval as I explained, is to help convert the big text data into a small amount of most relevant data for a particular problem, and can also help providing knowledge prominence, help interpreting patterns later. Text mining has to do with further analyzing the relevant data to discover the actionable knowledge that can be directly useful for decision making or many other tasks. So this course covered text mining, and there's a companion course called text retrieval and search engines that covers text retrieval. If you haven't taken that course, it would be useful for you to take it, especially if you are interested in building a text application system and taking both courses would give you a complete set of practical skills for building such a system. So in very end I just would like to thank you for taking this course. I hope you have learned useful knowledge and skills in text mining and analytics. As you see from our discussions, there are a lot of application opportunities for this kind of techniques, and there are also a lot of open challenges, so I hope you can use what you have learned to build a lot of useful applications to benefit the society and to also join the research community to discover new techniques for text mining and analytics. Thank you.
410	b1854d1c-3199-4c42-ab7d-f219f70259a3	This lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. In this lecture, we're going to continue discussing opinion mining and sentiment analysis. In particular, we're going to introduce. Late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings. First, motivation. Here are two reviews that you often see on the Internet about the Hotel and You see some overall ratings. In this case, both reviewers have given five stars. And of course there are also reviews that are in text. Now, if you just look at these reviews, it's not very clear whether a hotel is good for its location or for its service, and it's also unclear why are. If you are like this hotel. So what we want to do is to decompose this overall rating. Into ratings on different aspects such as value, rooms, location and service. So if we can decompose overrating two ratings on these different aspects. Then we can obtain more detailed understanding of the reviewers opinions about the hotel. And this would also allow us to rank hotels along different dimensions, such as valuable rooms, but in general such detailed understanding would reveal more information about the users, preferences, reviews, preferences and also we can understand better how reviewers view this hotel from different perspectives. Now, not only do we want to. Infer this aspect ratings. We also want to infer the aspect of weights, so some reviewers may care more about values as opposed to service, and that would be a case like what's shown on the left for the weight distribution where you can see a lot of weight is placed on value. But others might care more about service and therefore they might place more weight on service then value. Now, the reason why this is also important that is be cause do you think about a five star on value? It might still be very expensive if the reviewer cares a lot about service, right? For this kind of service, this price is good, so the reviewer might give it a five star. But if reviewer really cares about the value of the hotel, then the five star most likely would mean really cheaper prices. So in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights. When they are combined together, we can have a more detailed understanding of the opinion. So the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. And this is a problem called latent aspect rating analysis. So the task in general is given a set of review articles about the topic with overall ratings. An we hope to generate the three things. One is the major aspects comment on in the reviews. The second is the ratings on each aspect, such as value and room or service. And 3rd is the relative weights placed on different aspects by the reviewers, and this task has a lot of applications. If we can do this and we would enable a lot of applications, I just listed some here and later. I will show you some results. And for example, we can do opinion based and the ranking. We can generate a aspect level opinion summary. We can also analyze reviewers preferences, compare them or compare their preferences on different hotels. And we can do personalized recommendation of products. So of course the question is how can we solve this problem? Now, as in other cases of these advanced topics, we won't have time to really cover the technique in detail, but I'm going to give a press basic introduction to the technique developed for this problem. So first we're going to talk about how to solve the problem in two stages. Later, we're going to also mention that we can do this in the unified model. Now take this review with the overall reading as input. What we want to do is first we're going to segment the aspects. So we're going to figure out what words are talking about location in what words are talking about, the room conditioning, etc. So with this we would be able to obtain aspect segments. In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C supply of WND. This can be done by using seed words like location and room. Or price to retrieve the relevant the segments and then from those segments we can further mine correlated words. With these seed words and that would allow us to segment the text into segments. Discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, But anyway, that's the first stage where we would obtain the counts of words in each segment. In the segmentation stage, which is called latent rating regression, we're going to use these words and their frequencies in different aspects to predict the overall rating, and this prediction happens in two stages. In the first stage, we're going to use the sentiment weights of these words in each aspect to predict the aspect rating. So, for example, if in the discussion of location using a word like amazing mentioned many times and it has a high weight. For example, here is 3.9. Then it would increase the aspect rating for location. But another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating. So the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words. Now of course these sentiment weights might be different for different aspects. So we have for each aspect a set of sentiment weights. As shown here, and that's denoted by beta sub I and W. In the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings. So we're going to assume we have aspect weights in order by of R sub of D. And this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the. And we can assume the overall rating is simply a weighted average of this aspect ratings. So this setup allows us to predict the overall rating based on the observed word frequencies. So on the left side you will see all these observed information, the arts, the and the count. But on the right side you see all the information that we're interested in is actually latent. So we hope to discover them. Now this is a typical case of generating model where we would embed the interesting variables in the generating model. And then we're going to set up a generation probability for the overall rating given the observed words. And then of course, then we can adjust these parameter values including betas, rs, alpha i. In order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document. And so we have seen such cases before in, for example, PLSA, where we predict the text data. But here we predicting the rating and the parameters of course are also very different. But if you can see if we can uncover these parameters, that would be nice because also R of D is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub ID is precisely the aspect weights that we hope to get. As a bi product that will also get the beta vector and these are the aspects of specifica sentiment, weights of words, so more formally. They thought we are modeling. Here is a set of review documents with overall ratings. And each review documents denoted by AT and overall rating is denoted by R sub D and these pre segmented into K as their segments and we're going to use C sub W and D and to denote the count of world W in aspect segment I. Of course it's zero if the world doesn't occur in the segment. Now the model is going to predict the rating based on the. So we are interested in the conditional probability of R sub T given D. And this model is set up as follows. So all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings. R sub of D as shown here is normal distribution has a variance of or square. Now of course, this is just what our assumption in the actual reading is not necessary generating this way. But as always when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. In this case, the aspect ratings and aspect of weights. Now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just sentiment wait. So. As I said, the overall rating is assumed to be a weighted average of aspect ratings. Now this alpha Values of a alpha sub of D together by our vector that depends on D is the document specific weights and we can assume this factor itself is drawn from another multivariate Gaussian distribution with mean denoted by a mule vector and covariance matrix Sigma, yeah. Now, so this means when we generate our overall rating, we're going to first draw. A set of other values from this multivariate Gaussian prior distribution and once we get these alpha values were going to use, then the weighted average of aspect ratings as the mean here to use the normal distribution. And to generate the overall rating. Now the aspect rating as I just said is the sum of the sentiment weights of words in their spectrum. Note that here the sentiment weights are specifically to aspects, so beta is indexed by I. And As for aspect. And that gives us way to model different segment of award. This is neither because of the same word might have positive sentiment for once back, but negative sentiment for another aspect. It's also useful to then see. What premise we have here, but I just said that the beta sub I W gives us a aspect specific sentiment of W. So obviously that's one of the important parameters, but in general we can see we have these parameters. The beta values that Delta and then the mu and Sigma. So next question is, how can we estimate these parameters and so we collectively denote all the parameters by Lambda here. Now we can, as usual, use The maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed. Observer ratings condition on their respective reviews. And of course, this would then give us all the useful variables that will interest in computing. So now more specifically, we can now once we estimate the parameters, we can easily compute the abstract rating for aspect I or sub I of D and that's simply to take all the words that occurred in the segment I and then take their accounts and then multiply that by the sentiment weight of each word and take a sum. So of course this counter would be 04 words that are not occurring in the aspect I, and that's why we can take some over all the words in the vocabulary. Now, what about the aspect weights? Alpha sub I of D? It's not part of our parameter, right? So we have to use Bayesian inference to compute it. And in this case we can use the maximum a posteriori. 2 computer this alpha value. Basically we're going to maximize the product of the prior of our according to our assumed market valued Gaussian distribution and the likelihood in this case likely is the probability of generating this observed overall rating given this particular Alpha value and some other parameters. As you see here. So for more details about this model, you can read this paper cited here.
410	b355e801-bbf0-41a7-bdcd-75906c167014	This lecture is about the how to evaluate the text retrieval system when we have multiple levels of judgments. In this lecture we will continue the discussion of evaluation. We're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments. So, so far we have talked the about binary judgments. That means a document is judged as being relevant or non relevant. But earlier we also talk about the relevance as a matter of degree, so we often can distinguishing very high relative documents. Those are very useful documents from your moderately relevant documents. They are ok,  they are useful perhaps. And further from non relevant documents, those are not useful. So imagine you can have ratings for these pages. Then you would have multiple levels of ratings. For example here I show example of three levels, 3 for relevant sorry 3 for very relevant, two for marginally relevant and one for non relevant. Now how do we evaluate search engine system using these judgments? Obviously the map doesn't work. Average precision doesn't work. Precision and recall doesn't work because they rely on binary judgments. So let's look at some top ranked results when using these judgments, right? Imagine the user would be mostly care about the top 10 results here. Right? And we marked the reading levels or relevance levels for these documents as shown here, 32113, etc. "And we call these ""Gain""." "And the reason why we call it ""Gain""" is because the measure that we're introducing is called nDCG(Normalized Discounted Cumulative Gain). So this gain basically can measure how much gain of relevant information the user can obtain by looking at each document. Alright, so looking at the first document that the user can gain three points. Looking at the non random document, the user would only gain one point. By looking at the moderately relevant or marginal relevant documents, the user would get two points. Etc. So this gain intuitively matches the utility of a document from a user's perspective. Of course, if we assume the user stops at the 10 documents and we're looking at the cut off at 10, we can look at the total game of the user. And what's that? Well, that's simply the sum of these and we call it a cumulative gain. So if the user stops at the position one where there's just three, if the user looks at the another document, that's  3 + 2. If the user looks at the more documents, then the cumulative gain is more. Of course, this is at the cost of spending more time to examine the list. So cumulative gain gives us some idea about the how much total gain the user would have if the user examines all these documents. Now in nDCG we also have another letter here, D discounted. Cumulative gain. So why do we want to do discounting? Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents. So, for example, looking at the this sum here. And we only know there is one highly relevant document one marginally relevant document, two non relevant documents. We don't really care where they are ranked. Ideally we want these two to be ranked on the top and which is the case here. But how can we capture that intuition? Well, we have to say this is 3 here. is not as good as this three on the top. And that means the contribution of the gain from different positions has to be weighted by their position, and this is the idea of discounting, basically. So we're going to say well, the first one doesn't need to be discounted, because the user can be assumed to always see this document, but the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it. So we divide this gain by the weight based on the position, so log of 2. Two is the rank position of this document. And when we go to the third position, we discount even more because the normalizes log of three and so on, so forth. So when we take a such a sum than a lower rank document will not contribute contribute that much as a highly ranked document. So that means if you for example switch the position of this, let's say this position and this one, and then you would get more discount if you put. For example, very relevant document here, as opposed to here. Imagine if you put three here, then it would have to be discounted, so it's not as good as if we would put the three here. So this is the idea of discounting. OK, so now at this point that we have got that discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments. So are we happy with this? Well, We can use this rank systems. Now, We still need to do a little bit more in order to make this measure comfortable across different topics. And this is the last step. And, By the way, here we just showed the DCG at the ten right? So this is the total sum of DCG. Overall these 10 documents. So the last step is called the N normalization and if we do that then we will get a normalized DCG. So how do we do that? Well the idea here is we're going to normalize DCG by the ideal DCG at the same cut off. What is the ideal DCG? This is the DCG of ideal ranking. So imagine if we have 9 documents in the whole collection. Rated 3 here. And that means in total we have 9 documents rated 3. Then our ideal rank, the Lister would have put all these nine documents on the very top. So all these would have to be 3 and then this will be followed by a two here because that's the best we could do after we have run out of threes. But all these positions would be threes. Right? So this will be an ideal ranked list. And then we can compute the DCG for this ideal ranked list. So this would be given by this formula that you see here, and so this ideal DCG would then be used as the normalizer DCG..., here. And this ideal DCG will be used as a normalizer. So you can imagine now normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic. Now, why do we want to do this? Well, by doing this will map the DCG values into a range of zero through one, so the best value or the highest value for every query would be one. That's when your ranked list is in fact the ideal list. But otherwise, in general you will be lower than one. Now, what if we don't do that? Well, you can see this transformation or this normalization doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG. The difference however is when we have multiple topics. because if we don't do normalization, different topics will have different scales of DCG. For a topic like this one we have 9 highly relevant documents. The DCG can get really high, but imagine in another case, There are only two very relevant documents in total, in the whole collection. Then the highest DCG that any system could achieve for such a topic will not be very high. So again, we face the problem of different scales of DCG values, and we take an average. We don't want the average to be dominated by those high values. Those are again easy queries, so by doing the normalization we can avoid the avoid the problem making all the queries contribute equally to the average. So this is the idea of nDCG. It's useful for measuring ranked list based on multiple level relevance judgments. So more in the more general way, this is basically a measure that can be applied to any rank the task with multiple level of judgments. And The scale of the judgments can be multiple. Can be more than binary, not only more than binary. They can be multiple levels like a 1 through 5 or even more depending on your application. And the main idea of this measure I just to summarize is to measure the total utility of the top K documents. So you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower ranked document. And finally it will do normalization to ensure comparability across queries.
410	b36805e6-d5d6-4d4c-a58c-7f1a5f6c233c	this letter is about the statistical language model in this lecture we're going we're going to give an introduction to statistical language model just has to do with how do you model text data with problems models so it's related to how we model query based on a document we're going to talk about what is the language model and then we're going to talk about the simplest language model called unigram language model which you also happens to be the most useful model for text retrieval and finally we discussed possible uses of language model what is the language model well it's just the probability distribution over water sequences so here i show one this model gives the sequence today is wednesday a probability of zero point zero zero one it gave today wednesday is a very very small probability be'cause its amankila medical you can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model therefore it's clear the context dependent in ordinary conversation probably today's wednesday is most popular among these sentences but imagine in the context of discussing apply the math maybe the eigenvalue is positive would have a higher probability this means it can be used to represent the topic of the text the model can also be regarded as a probabilistic mechanism for generating text and this is why it's also often called a generating model so what does that mean we can imagine this is a mechanism that's visualizer hands here as a stock ask the system that can generate the sequences of words so we can ask for a sequence and it's too simple sequence from the device if you want and they might generate for example today is wednesday by the could have generated any other sequences so for example there are many possibilities so this in this sense we have you our data as basically a sample observer from such a generating model so why is such a model useful well so many becaus it can quantify the uncertainties in natural language where do i insert in this come from well it one source is simply the ambiguity in natural language that we discussed earlier in the rapture another source is because we don't have complete understanding we lack or the knowledge to understand language in that case they will be answered in this as well so let me show some examples of questions that we can answer with the language model that would have interesting application in different ways given that wizzy john and feels how likely will see happy as a possible habit as the next war in a sequence of words obviously this would be very useful for speech recognition because happy and happy with having similar acoustical sound acoustic signals but if we look at the language model will know that john feels happy would be far more likely than john feels habit another example given that we observe baseball three times and game once in a news article how likely is it about the sports this obviously is related to text the categorisation and information retrieval also given that the user is interested in sports news how likely would the user use baseball in a query now this is a career related to the query likelihood that we discussed in the previous raptor so let's look at the simplicity language model called unigram language model in such a case we assume that we generate the text by generating each were independent so this means the probability of a sequence of words will be then the product of the probability of each world and normally they are not independent right so if you have seen a warden like a language that would make them far more likely to observe model then if you haven't seen language so this is something is not necessarily true but we make this assumption into simplified model so now the model has precisely N parameters where N is vocabulary size we have one probability for each word and all these probabilities muscle some lap so strictly speaking we actually have minus one primers as i say that text can then be assumed to be assembled drawn from this word distribution so for example now we can ask the device or the model to stochastically general the words for us instead of sequences so instead of giving a whole sequence mega today's wednesday it now gives us just one word and we can get all kinds of words and we can assemble these words in a sequence so that would still allow the computer the probability of today's wednesday as the product of the three probabilities as you can see even though we have not asked the model the generator the sequences it actually allows us to compute the probability for all the sequences but this model now only needs in parameters to characterize that means if we specify all the probabilities for all the words then the models behavior is completely specified whereas if we don't make this assumption what would have to specify probabilities for all kinds of combinations of words in sequences so by making this assumption it makes it much easier to estimate these parameters so let's see a specific example here he also to unigram language models with some probabilities and these are high probability words that are shown on top the first one clearly suggests a topic of attacks reminding because the high probability awards are all related to this topic the second one is more related to health now we can they ask the question how likely will observe a particular text from each of these two models i suppose we assemble words to form the document let's say we take the first distribution which might assemble words what words do you think it would be generated well maybe text or maybe mining maybe another war even for which is a very small probability might disturb you able to show up but in general high probability was will likely show up more often so we can imagine what gender the texture that looks like a text mining in fact up with a small probability you might be able to actually generate the actual text mining paper that would actually meaningful although the probability would be very very small in the extreme case you might imagine we might be able to generate a text paper text mind never that would be accepted by major conference and in that case the probability would be even smaller but it's a non zero public anything if we assume none of the words have non zero probability similarly from the signal topic we can imagine we can generate the folding using paper that doesn't mean we cannot generate this paper from text mining distribution we can but the probability would be very very small maybe smaller than even generating a paper that can be accepted by a major conference on text mine so the point here is that the key point distribution we can talk about the probability of observing a certain kind of text some text that we have higher probabilities now those now let's look at the problem in a different way suppose we now have available a particular a document in this case maybe the abstract of the text mining paper and we see these word counts here the total number of words is one hundred now the question will ask here is estimation question we can ask the question which model which water distribution has been used it we generated this text assuming that the text that has been generated by assembling words from the distribution so what would be your guest let have to decide what probabilities text mining etc would have suppose the video for a second and try to think about your best gas if you like a lot of people you would have guessed that well my god best guesses in text it has a probability of ten out of one hundred because i've seen text ten times an there are in total one hundred words so we simply not simply normalizing these counts and that's in fact they were justified and your intuition is consistent with mathematical derivation and this is called a maximum likelihood basement in this estimator we assume that the parameter settings are those that would give our observe the data the maximum probability that means if we change these probabilities then the probability of observing the particular task data would be somewhat a smaller so you can see this has a very simple formula basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length normalized frequency or consequences of this is of course we're going to assign zero probabilities to unseen words if we're having the observer ward there will be no incentive to assign a non zero probability using this approach why be cause that would take away probability mass for these observ the words and that obviously wouldn't maximize the probability of this particular observer text there but once you question whether this is our best estimate well then that depends on what kind of model you want to find right this is made it gives the best model based on this particular data but if you interest rema model that can explain the content of the four paper of this abstract then you might have a second all right so for one thing they surely be other words in the body of the article so they should not have zero probabilities even though they're not observing the abstract and we're going to cover this a little later in discussing the query like retrieval model so let's take up a look at the some possible uses of this language models one uses simply to use it to represent the topics so here i show some general english background text we can use this text to estimate a language model and the model might look like this i saw on the top we have those all common words like the is way etc and then we'll see some common words like these and then some very very rare words in the bottom this is the background language model it represents the frequency of words in english in general this is the background model not let's look at another text maybe this time we look at the computer science research papers so we have a collection of computer science research papers we do as matching again we can just use the maximum micro arrays matter where we simply normalize the frequencies now in this case will get the distribution that looks like this on the top it looks similar because these words occur everywhere they're very common but as we go down will see words that are more related to computer science computer software attacks etc so although here we might also see these words for example computer but we can imagine the probability here is much smaller than the probably in there here and we will see many other words here that would be more common in general image so you can see this distribution characterize the topic of the corresponding tents we can look at the even a smaller text so in this case let's look at the text mining paper now if we do the same we have another distribution again there can be expected to occur on the top assume we will see text mining association clustering these words have relatively high probabilities in contrast in this distribution will text has relatively small probability so this means again based on different the text that we can have a different model and model captures the topic so we call this document the damage model and we call this collection language model and later you will see how they're used in retrieval function but now let's look at that another use of this model can we statistically find the what wars are semantically related to computer now how do we find such words well office thought is that let's take a look at the text that match computer so we can take a look at all the documents that contain the word computer let's build a language model we can see what words we see there well not surprisingly we see these common was on top as we always do so in this case this language model gives us a conditional probability overseeing award in the context of computer and these common words were naturally have higher probabilities but we also see computer itself and software we have relatively higher probabilities but if we just use this model we cannot just say all these words are semantically related to computer so intuitively would like to get rid of these yep these common was how can we do that it turns out that it's possible to use language model will do that i suggested to think about that so how can we know what words are very comments that we want to kind of get rid of them what model would tell us that well maybe you can think about that so the background language model precisely tells us this information that tells us what words are common in general so if we use this background model we would know that these words are common words in general so it's not surprising observe them in the context of computer where is computer has a very small probability in general so it's very surprising that we have seen computer with this probability and the same is true for software so then we can use this to models to somehow figure out the words that are related to computer for example we can simply take the ratio of these who probabilities or normalize the topic language model by the probability of the world in the background language model so if we do that we take the ratio will see that then on the top of computer is ranked and then followed by software program all these words are related to computer be cause they're can very frequently in the context of computer but not frequently in the whole collection whereas these common words will not have a high probability in fact they have ratio about of one down there because they are not really related to computer by taking the sample of text that contains the computer we don't really see more occurrences of them then in general so this shows that even with this simple language models we can do some limited analysis of semantics so in this lecture we talked about then with model which is basically a probability distribution over text we talked about the simplest of empty model called unigram them model which is also just a word distribution we talked about the two uses of a language model one is will represent the topic in a document in the collection or in general the other is rediscovered associations in the next lecture we're going to talk about how them with model can be used to design retrieval function here are two additional readings the first is textbook on statistical natural language processing the second is article that has a survey of statistical language models with a lot of pointers to research work
410	b599210f-ecad-4734-895d-83a3cc025112	This lecture is about link analysis for web search. In this lecture we're going to talk about web search. And particularly focusing on how to do link analysis and use the results to improve search. The main topic of this lecture is to look at the ranking algorithms for web search. In the previous lecture we talked about how to create index now that we have got index. We want to see how we can improve ranking of pages. Our standard IR models can be also applied here. In fact they are important building blocks for improvement for supporting web search, but they aren't sufficient and mainly for the following reasons. First, on the web we tend to have very different information needs. For example, people might search for a web page or entry page and this is different from the traditional library search where people are primarily interested in collecting literature information. So this kind of query is often called navigational queries. The purpose is to navigate into a particular target page. So for such queries we might benefit from using link information. Secondly, documents have additional information and on the web web pages are well format. There are a lot of other clues such as the layout, title or link information. Again, so this has provided the opportunity to use extra context information. Of the document to improve scoring and finally information quality varies a lot, so that means we have to consider many factors to improve the ranking algorithm. This would give us a more robust way to rank the pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page. So as a result, people have made a number of major extensions to the ranking algorithms. One line is to exploit links to improve scoring. And that's the main topic of this lecture. People have also proposed algorithms to exploit the large scale implicit feedback information in the form of click throughs, and that's of course in the category of feedback techniques. And machine learning is often used there. In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features. Many of them are based on the standard visual models such as BM25 that we talked about. Or query likelihood to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring. Signals. So let's look at links in more detail on the web. So this is a snapshot of some part of the web and say so we can see there are many links that link the different pages together, and in this case you can also look at the center here. There is a description of a link that's pointing to the document on the right side. Now this description text is called anchor text. Now if you think about the this text, it's actually quite a useful because it provides some extra description of that page being pointed to. So for example, if someone wants to bookmark Amazon.com, front page the person might say. The biggest online bookstore and then with the link to Amazon. Right, so the description here actually is very similar to what the user will type in the query box when they are looking for such a page, and that's why it's very useful for ranking pages. Suppose someone types in query like online bookstore or fixed online bookstore. The query would match this anchor text. In the page. Here and then, this actually provides evidence for matching the page that's being pointed to. That is the Amazon entry page. So if you match the anchor text that describes a link to a page, actually that provides good evidence for the relevance of the page being pointed to, so anchor text is very useful. If you look at the bottom part of this picture you can also see there are some patterns of links, and these links might indicate the utility of a document. So for example on the right side you can see this page has received, many in links. That means many other pages are pointing to this page and this shows that this page is quite useful. On the left side you can see, this is another page that points to many other pages, so this is a directory page that would allow you to actually see a lot of other pages. So we can call the first case authority page and the second case hub page. This means the link information can help in two ways. One is to provide extra text for matching and the other is to provide some additional scores for the web pages to characterize how likely a page is a hub, how likely a page is authority. So people then of course proposed ideas to leverage these in this link information. Now Google's Pagerank, which was the main technique that they used in early days, is a good example and that is an algorithm to capture page popularity, basically to score authority. So the intuition's here are links are just like a citations in the literature. Think about one page pointing to another page. This is very similar to one paper citing another paper. So of course, then if a page is cited often, then we can assume this page to be more useful, in general. So that's a very good intuition. Now Pagerank is essentially to take advantage of this intuition to implement it with the principled approach. Intuitively, it's essentially doing citation counting or in link counting. It just improves this simple idea in two ways. One is it would consider indirect citations. So that means you don't just look at the how many in links you have. You also look at the what are those pages that are pointing to you. If those pages themselves have a lot of in links, well that means a lot. In some sense you will get some credit from them. But, if those pages that are pointing to you are not being pointed to by other pages, they themselves don't have many in links then, well, you don't get that much credit. So that's the idea of getting indirected citation. Alright you can also understand this idea by looking at again the research papers. If you're cited by let's say 10 papers and those 10 papers are just workshop papers and or some papers that are not very influential, right? So although you get 10 in links and that's not as good as if you were cited by 10 papers that themselves have attracted a lot of other citations. So this is. A case where we would like to consider indirect links and Pagerank does that. The other idea is it's going to smooth the citations or assume that basically every page is having a non zero pseudo citation count. Essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone. The reason why they want to do that is this would allow them to solve the problem elegantly with. Linear algebra technique. So. I think maybe the best way to understand the page rank is through. Think of this as do computer the probability of random surfer, visiting every web page.
410	b6f9dd1a-1d38-48b8-8942-d107c9d4d2b7	this latter is about the web indexing in this lecture we will continue talking about the web search and we're going to talk about how to create web scale index so once we cross the web we've got a lot of web pages the next step is to use the indexer to create the inverted index in general we can use the standard information retrieval techniques for creating the index and that is what we talk about in previous lecture but there are new challenges that we have to solve for web scale in dancing and the two main challenges our scalability and efficiency the index would be so large that it cannot actually fit into any single machine or single disk so we have to store the data on multiple machines also because the data is so large it's beneficial to process that data in parallel so that we can produce the index quickly how to address these challenges google has made a number of innovations one is the google fire system that's a general distributed file system that can help the program 's manage files stored on a cluster of machines the second is map reduce this is a general software framework for supporting parallel computation paducah is the most well known open source implementation of map reduce now used in many applications so this is the architecture of the google file system it uses very simple centralized management that mechanism to manage it all the specific locations of files so the maintains the filename space and look up a table to know where exactly each files installed the application client that would then talk to this chi ever semester and that obtains specifica locations of the files that they want the process and once the GF 's kind obtained the the specific information about the files then the application climbed can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network so when this file system stores the fire zone machines they system also would create a fixed sizes of chunks so that data files are separately in too many chunks each chunk is sixty four megabytes so it's pretty big and that's a property for large data processing these chunks all replicated to ensure reliability so this is something that problem doesn't have to worry about and it's all taken care of by this fire system so from the application perspective the programmer would see this as if it's a normal file the program doesn't have to know exact rates are stored and can just invoke high level operate this to process the file ann another feature is that the data transfer is directed between application and chunk servers so it's efficient in this sense on top of the google file system and over also propose map reduce as a general framework for parallel programming now this is very useful to support a task like a building inverted index i saw this framework is hiding a lot of low level features from the program as a result the programmer can make minimum effort to create application that can be run large cluster in parallel and so some of the low level details hitting in the framework including the specific network communications or load balancing or where the tasks are executed all these details are hidden from the programmer there is also a nice visual which is the building for the tolerance if one server is broken let's say the service down and then some tasks may not be finished then the map reduce mechanism would know that the task has not been down so it will automatically dispatch the taskbar on other servers that can do the job and therefore again the program it doesn't have to worry about that so here's how mapreduce works the input they're not would be separately into a number of key value pairs now what exactly is in the value will depend on the data and it's actually a fairly general framework to allow you to just partition the data into different parts and she probably can be there processed in parallel each key value pair would be and send it to a map function the problem with the right map function of course and then the map function with the process this key value pair and with generate the a number of other key value pairs of course the new key is usually different from the old key that given through the map as in fault and these key value pairs are the output of the mac function and all the outputs of all the map functions would be then collected and then there would be for the sort based on the key and the result is that all the values that are associated with the same key would be land grouped together so now we've got a pair of a key and a set of values that are attached to this P so this would then be sent to a reduce function now of course each reduce function will handle a different each different key so we will send this output values to multiple reduce functions each handling unique key a reduce function with them process the input which is a key and a set of values to produce another set of key values as the output so these output values will be then collected together to form the final output so this is the general framework of map reduce now the programmer only needs right the map function and the reduce function everything else is actually taken care of by the mapreduce framework so you can see the program really only needs to do minimum work and with such a framework of the input data can be partitioned into multiple parts each is processed in parallel first by map and then in the process after we reach the reduce stage then multiple reduce functions can also further process the different keys and their source their values in parallel so it achieves some it achieves the purpose of parallel processing of large data set so let's take a look at a simple example and that's what accounting are the input is is files containing words and the articles that we want to generate this is a number of occurrences of each word so it's the word account we know this kind of accounting would be useful to for example assess the popularity over word in a large collection and this is useful for achieving a factor of IDF weighting all search so how can we solve this problem well one natural thought is that well this this task can be down imperial by simply counting different parts of the fire imperial and then in the end we just combine all the counts and that's precisely the idea of what we can do with mapreduce we can parallelize on lines in this input file so more specifically we can assume the input to each map function is key value pair that represents the line number and the stream on that line so the first line for example has key of one and the value is hello word by word and just the four words on that line so this key value pair will be sent to a map function the map function would then just count the words in this line and in this case of course there are only four words each water gets account of one and these are the output that you see here on this slide from this map function so the map function is really very simple if you look at what the pseudo code looks like on the right side you see it simply needs to iterate over all the words in this line and then just cover collect function which means it would then send the word and the counter to the collector the collector would then try to sort all these key value pairs from different functions so the function is very simple and the programmer specifies is this function as a way to process each part of the data of course the second line will be handled by a different map function which would produce a similar output OK now the output of from the map functions will be then send it to a collector and the clap that will do the internal grouping or sorting so at this stage you can see we have collected much board pairs each pair is award and it's count in a lie so once we see all these pairs then we can sort them based on the key which is the world so we will collect all the counts of award like a buy here together and similar will do that for other words like hadoop hello etc so each word now is attached to a number of values a number of accounts and these counts represented the occurrences of this word in different lines so now we have got a new pair of kiana set of values and this pair will then be fitting to reduce function so reduce functional would have to finish the job of counting the total occurrences of this world now it has already got all these partial counts so it needs to do is similar to add them up so they're reduced function show here is very simple as well you have counter and then iterate over all the words that you see in this array and then you just accumulated account and then finally you output the key and the total account and that's precisely what we want as the output of this whole program so you can see this is already very similar to building a inverted index and if you think about it at the output here is indexer bio world and we have already got the dictionary basically we have got to the counts but what's missing is the document i DS and the specific frequency counts words in those documents so we can modify this is slightly to actually build a inverted index in parallel so here's one way to do that so in this case we can assume the input for map function is a pair of a key which denotes the document ID and value denoting the stream for that document so it's all the words in that document and so the map function will do something very similar to what we have seen in the water company example simply groups all the counts of this word in this document together and it would that generate a set of key value pairs each key is award and the value is the count of this word in this document plus the document ID now you can easily see why we need to add document ID here course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later now similarly another document of the tool can be processed in the same way so in the end again there is a sorting mechanism that with a group them together and then we will have just a key like a java associated with all the documents that matches this key or all the documents where java occured and the account i saw the counts of java in those documents and this will be collected together and this will be so fed into the reduce function so now you can see the reduce function has already got info that looks like a inverted index entry right so it's just the word and all the documents that contain the word and the frequencies of the world in those documents so all it needs to do is simply to concatenate them into a continuous chunk of data and this can be then written into a fire system so basically the reduce function is going to do very minimum work and so this is pseudocode for inverted index construction here we see two functions procedure map and procedure reduce an a programmer would specify these two functions to program on top of map reduce and you can see basically they're doing what i just described in the case of map it's going to count the occurrences of word using associative array and will output all the counts together with the document ID here i saw this is the reduce function on the other hand simply concatenates or the input that it has been given and then put them together as one single entry for this key so this is very simple map reduce function yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines the program doesn't have to take care of the details so this is how we can do parallel index construction for web search so to summarize web scaling that scene requires some new techniques that go beyond the standard traditional indexing techniques mainly we have to store the index on multiple machines and this is usually done by using file system like a google file system that distributed file system an secondly it requires creating the index in parallel because it's so large it takes a long time to create an index for all the documents so if we can do it in parallel it'll be much faster and this is done by using the mapreduce framework note that oppose the GFS an mapreduce frameworks are very general so they can also support many other applications
410	b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7	This lecture is about the topic mining and analysis. We are going to talk about using a term as topic. This is a slide that you have seen in the earlier lecture where we defined the task of top mining and analysis. We also raised the question how do we exactly define the topic theta? So in this lecture we are going to offer one way to define it, and that's our initial idea. Our idea here is to define a topic simply as a term. A term can be a word or a phrase. And in general, we can use these terms to describe topics, so our first thought is just to define a topic as one term. For example, we might have terms like sports, travel or science as you see here. Now if we define a topic in this way, we can analyze the coverage of such topics in each document. Here, for example, we might want to discover to what extent document 1 covers sports and we found that 30% of the content of document 1 is about sports. And 12% is about the travel etc. We might also discover Document 2 does not cover sports at all, so the coverage is zero, etc. So now of course, as we discussed. In the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. So let's first think about how we can discover topics if we represent each topic by a term. So that means we need to mine K topical terms from a collection. Now there are of course many different ways of doing that and. We're going to talk about a natural way of doing that, which is also likely effective. So first we're going to parse the text data in the collection to obtain candidate terms. Here, candidate terms can be words or phrases. Let's say the simplest solution is to just take each word as a term. These words then become candidate topics. Then we're going to design a scoring function to measure how good each term is as a topic. So how can we design such a function? Well, there are many things that we can consider. For example, we can use pure statistics to design such as scoring function. Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. So that would mean we want to favor a frequent term. However, if we simply use the frequency to design the scoring function, then the highest scored terms would be "general terms or functional terms, like ""the"", ""a""" etc. Those terms are very frequent in English. So we also want to avoid having such words on the top, so we want to penalize such words, but in general would like the favor terms that are fairly frequently but not so frequent. So a particular approach could be based on, TF-IDF weighting from retrieval. And TF stands for term frequency  IDF stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. So these are statistical methods, meaning that the function is defined mostly based on statistics. So the scoring function would be very general. It can be applied to any language and any text. But when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. For example, in news we might favor title words. Actually, in general, we might want to favor title words becauses the authors tend to use the title to describe the topic of an article. If we're dealing with tweets, we could also favor hashtags which are invented to denote topics. So naturally hashtags can be good candidates for representing topics. Anyway, after we have designed the scoring function, then we can discover the K topical terms by simply picking K terms with the highest scores. Now of course we might encounter a situation where the highest scored terms are all very similar. They are semantically similar or closely related or even synonyms. So that's not desirable, so we also want to have coverage over all the content in the collection. So we would like to remove redundancy and one way to do that is to do a greedy algorithm, which is sometimes called maximal marginal relevance ranking. Basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the K topical terms. The first term of course will be picked when we pick the next term. We're going to look at the what terms have already been picked and try to avoid picking a term that's too similar. similar. So while we are considering the ranking of term in the list, we're also consider in the redundancy of the candidate term with respect to the terms that we already picked. With some thresholding then we can get balance of redundancy removal and also high score over term. OK so after this then we will get K topical terms and those can be regarded as the topics that we discovered from the collection. Next let's think about how we can "compute the topic coverage i j So looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document How should we figure out the coverage of each topic in the document? One approach can be to simply count occurrences of these terms. So for example, sports might have occurred four times in this document, and travel occurred twice, etc, and then we can just normalize. these counts as our estimate of the coverage probability for each topic. So in general the formula would be to collect the counts of all the terms that represented the topics and then simply normalize them so that. The coverage of each topic in the document would add to one. This forms a distribution over the topics for the document to characterize coverage of different topics in the document. Now, as always when we think about the idea for solving problem, we have to ask the question, how good is this one? Or is this the best way of solving the problem? So now let's examine this approach. In general, we have to do some empirical evaluation by using actual datasets and to see how well it works. In this case, let's take a look at a simple example. Here we have the text document that is about the NBA basketball game. So in terms of the content, it's about the sports. But if we simply count these words that represent our topics, and we will find that the word sports actually did not occur in the article, even though the content is about the sports. So the count of sports is zero. That means the coverage of sports will be  "estimated Now of course. The term science also did not occur in the document, and it's estimated also zero. That's OK, but sports certainly is not OK. 'cause we know the content is about sports. So this estimate has problem. What's worse, term travel actually occurred in the document, so when we estimate the coverage of the topic travel, we have gotten a non-zero count, so it's estimated coverage would be non zero. So this obviously is also not desirable. So this simple example illustrates some problems of this approach. First, when we count what words belong to the topic, we also need to consider related words. We can't simply just count the topic. word sports. In this case, it did not occur at all, but there are many related words like  basketball, game, etc. So we need to count related words. Also. The second problem is that a word like star can be actually ambiguous. So here it probably means a basketball star But we can imagine it might also mean a star on the Sky. So in that case the star might actually suggest perhaps a topic of science. So we need to deal with that as well. Finally, the main restriction of this approach is that we have only one term to describe this topic So it cannot really describe complicated topics. For example a very specialized topic would be hard to describe by using just a word or one phrase, we need to use more words, so this example illustrates some general problems with this approach of treating a term as topic. First, it lacks expressive power, meaning that it can only represent the symbol general topics. But it cannot represent the complicated topics that might require more words to describe. Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term. It does not suggest what other terms are related to the topic, even if we're talking about the sports, there are many terms that are related, so it does not allow us to easily count related terms toward contributing to coverage of this topic. Finally, there's this problem of word sense ambiguation, a topical term or related term can be ambiguous. For example, basketball star versus star in the Sky. So in the next lecture we're going to talk about how to solve the problem with probabilistic modeling of the topic.
410	b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf	This lecture is about the paradigmatic relation discovery. In this lecture we're going to talk about how to discover a particular kind of word Association called paradigmatic relations. By definition, 2 words are paradigmatically related if they share similar contexts. Namely, they occur in similar positions in text. So naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. So here's an example of context of word Cat. Here I have taken the word cat out of the context. And you can see we are seeing some remaining words in the sentences that contain cat. Now we can do the same thing for another word like a dog. So in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog. So now the question is, how can we formally represent the context and then define the similarity function? So first we note that the context actually contains a lot of words. So they can be regarded as a pseudo document. An imaginary document. But there are also different ways of looking at the context. For example, we can look at the word that occurs before the word cat. We can call. We can call this context left1 context. So in this case you will see words like my, his or big, a, the, etc. These are the words that can occur to the left of the world cat. So we say my cat, his cat big cat. a cat etc. Similarly, we can also collect the words that occur right after the word cat. We can call this context right1. And here we see words eats, ate, is, has, etc. Or more generally, we can look at the all the words in the window of text around the word cat. Here let's say we can take a window of eight words around the world cat. We call this context Window8 Now of course, you can see all the words from left or from right, and so we have a bag of words in general to represent the context. Now, such a word based representation would actually give us interesting way to define the perspective of measuring the similarity. "  similarity of left1, then we'll see words that share just the words in the left context and we kind of ignore the other words that are also in the general context. So that gives us one perspective to measure the similarity. And similarly, if we only use the right1 context will capture the similarity from another perspective. Using both left1 and right1, ofcourse would allow us to capture the similarity with even more strict criteria. So in general, context may contain adjacent words like eats and my that you see here or non-adjacent words like Saturday, Tuesday or some other words in the context. And this flexibility also allows us to measure the similarity similarity in some other different ways. Sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics. So the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. So here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. In general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. And of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. So next, let's see how we exactly compute these similarity functions. Now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. Now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. But here we also find it convenient to model the context of a word for paradigmatically relation discovery. So the idea of this approach is to view each word in our vocabulary as defining one dimension in high dimensional space so we have N words in total in the vocabulary. Then we have N dimensions as illustrated here. And on the bottom you can see frequency vector representing a context. And here we see when eats occured five times in this context, ate occurred three times etc. So this vector can then be placed in this vector space model. So in general, we can represent a pseudo document or context of cat as one vector. d1. An another word dog might give us a different context, so d2. And then we can measure the similarity of these two vectors. So by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. So the two questions that we have to address is first how to compute each vector, that is, how to compute the xi or yi? And the other question is, how do you compute the similarity? Now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. And they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. So let's first look at the one possible approach, where we try to measure the similarity of context based on the expected overlap of words and we call this EOWC. So the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word. So in other words. xi is defined as the normalized count of word wi in the context. And this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word. Now of course these xi's will sum to 1 because they are normalized frequencies. And this means the vector is actually probability distribution over words. So, the vector d2 can be also computed in the same way. And this would give us then two probability distributions representing two contexts. So that addresses the problem how to compute the vectors? Next, let's see how we can define similarity in this approach. Well, here we simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors. Now it's interesting to see that this similarity function actually has a nice interpretation. And there is this dot product  infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical. If they are very different then the chance of seeing identical words being picked from the two contexts would be small. So this intuitively makes sense for measuring similarity of contexts. Now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical. So if you just stay at the formula  to check what's inside this sum then you will see, basically in each case it gives us the probability that we'll see overlap on a particular word, wi and where xi gives us the probability that will pick this particular word from d1 and  yi gives us the probability of picking this word from d2 and when we pick the same word from the two contexts then we have identical  pick. Alright, so that's one possible approach. EOWC expected overlap of words in context. Now, as always, we would like to assess whether this approach it would work well. Now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations. But analytically, we can also analyze this formula little bit. So first, as I said, it does make sense right? because this formula will give a higher score if there is more overlap between the two contexts. So that's exactly what we want. But if you analyze the formula more carefully, then you also see there might be some potential problems. And specifically there are two potential problems. First it might favor matching one frequent term very well over matching more distinct terms, and that is because in the dot product, if one element has a high value and this element is shared by both context and it contributes a lot to the overall sum. And it might indeed make the score higher than in another case where the two vectors actually have a lot of overlap in different terms, but each term has a relatively low frequency. So this may not be desirable. Of course, this might be desirable in some other cases, but in our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context. If you only rely on one term and that's a little bit questionable. It may not be robust. The second problem is that it treats every word equally, so  if you match a word like the, and match was, it would be the same as matching on the word like eats. But intuitively we know matching the isn't really surprising because the occurs everywhere, so matching the is not as such  a strong evidence as matching a word like eats  which doesn't occur frequently. So this is another problem of this approach. In the next lecture, we're going to talk about how to address these problems.
410	c1e55739-56e6-4547-9bec-86797f401ea8	There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure. like MAP or nDCG. Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. One can imagine that why our prediction may not be too bad. Let's say both are around .5, so it's kinda of in the middle of zero and one for the two documents. But the ranking can be wrong, so we might have got a larger value for D2 and than D1. So that won't be good from retrieval perspective, even though by likelihood function is not bad. In contrast, we might have another case where we predicted values all around .9, let's say and by the objective functioning the error will be larger, but if we can get the order of the two documents correct, that's actually a better result. So these new, more advanced approaches will try to correct that problem. Of course, then the challenge is that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end to know more about the these approaches. Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem. So here I list some, for example recommender systems, computational advertising or summarization and there are many others that you can probably encounter in your applications. To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results. Actually the use of machine learning in information retrieval has started since many decades ago. So, for example, the Rocchio feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems, and first, it's mostly driven by the availability of a lot of training data in the form of click throughs. Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this. Secondly, it's also driven by the need for combining many features and this is not only just because there are more features available on the web that can be naturally used to improve scoring, it's also because by combining them we can improve the robustness of ranking, so this is desired for combating spams. Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing. The topic of learning to rank is still active research topic in the community and so you can expect to see new results being developed in the next few years, perhaps. Here are some additional readings that can give you more information about how learning to rank works and also some advanced methods.
410	c2e926e1-eba7-4512-9b8b-594be572e4df	this letter is about the link analysis for web search in this lecture we're going to talk about web search and particularly focusing on how to do link analysis and use the results to improve search the main topic of this lecture is to look at the ranking algorithms for web search in the previous after we talked about the how to create index now that we have got the index we want to see how we can improve ranking of pages the web i'll stand the IR models can be also applied here in fact that they are important the building blocks for improvement for supporting web search but they aren't sufficient an mainly for the following reasons first on the web we tend to have very different information needs for example people might search for a web page or entry page and this is different from the traditional library search people are primarily interested in collecting literature information so this kind of queries often called navigation operates the purposes or navigate into a particular target page so for such queries we might benefit from using linking permission secondly documents have additional information and on the web web pages are well format there are lots of other crews such as the layout of the title or link information again so this has provide the opportunity to use extra context information of a document to improve the scoring and finally information quality varies a lot that means we have to consider many factors to improve the ranking algorithm this would give us a more robust way through rank the pages making it harder for N S member to just manipulate the one signal to improve the ranking of page so as a result people have made a number of major extensions to the ranking algorithms one line is to explore links to improve scoring and that's the main topic of this active people have also proposed algorithms to exploit the largest scale implicit affair back information in the form of click throats that's of course in the category of feedback techniques and machine learning is often used there in general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features many of them are based on the standard visual models such as BM twenty five that we talked about query likely hold to score different parts of documents or to provide additional features based on content matching but link information is also very useful so they provide additional scoring signals so let's look at links in more detail on the web so this is snapshot of some part of the web so we can see there are many links link different pages together an in this case you can also look at the centre here there is a description of a link that's pointing to the document on the right side now this description text it's called anchor text if you think about this text it's actually quite useful becaus it provides some extra description of that page being pointed too so for example if someone wants to bookmark amazon dot com front page the person might say the biggest online bookstore and then with a link to amazon right so the description here actually is very similar to what the user would typing in the query box when they are looking for such a page and that's why it's very useful for ranking pages suppose someone types in the query like online bookstore or fixed online bookstore the query would match this anchor text in the page here and then this actually provides evidence for matching the page that's being pointed to that is the amazon entry page so if you match the anchor text that describes a link to a page actually that provides good evidence for the relevance of the page being pointed to it so any attacks are very useful and if you look at the bottom part of this picture you can also see there are some patterns of links and these links might indicate the utility of a document so for example on the right side you can see this page has received a major in english alarm is many other pages are pointing to this page and this shows that this page is quite useful on the left side you can see this is another page that points to many other pages so this is the directory page that would allow you to actually see a lot of other pages so we can call the first case authority page and the second is a hub page this means the link information can help in two ways one is to provide extra fast for matching and the other is to provide some additional scores for the web pages to characterize how likely are pages how likely pages authoritie so people then of course an proposed ideas to leverage it is this link information google 's page rank which was the main technique that they used in early days is a good example and that that is the algorithm to capture page popularity basically to score authoritie so the intuitions here are links i just like citations in the literature think about the one page pointing to another page this is very similar to one paper citing another paper so of course then if a PP cited off and then we consume this page to be more useful in general so that's a very good intuition now page rank is essentially to take advantage of this intuition to implement it with principle approach intuitively it's essentially doing citation counting or in link counting it just improves the simple idea in two ways one is it will consider in direct citations so that means you don't just look at how many in links you have you also look at what are those pages that are pointing to you if those pages themselves have a lot of england 's well that means alot in some sense you will get some credit from them but if those pages that are pointing to you are not being pointed to by other pages they themselves don't have many index then well you don't get that much credit so that's the idea of the directive citation i saw you can also understand this idea by looking at again the research papers if you're cited by and it's a ten papers and those ten papers are just workshop papers and that or some papers that are not very influential right so although you got ten A links and that's not as good as if you are side by ten papers that themselves have attracted a lot of other citations so in this is a case where we would like to consider in direct links an pager anger does that young idea it's going to small citations or assume that basically every page is having a non zero zero citation count essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone the reason why they want to do that is this would allow them to solve the problem elegantly with linear algebra technique so i think maybe the best way to understand the page rank is through think of this as to compute the probability of random server visiting every web page
410	c6213520-fc62-43c3-80cd-1e9b91389463	This lecture is a summary of this course. This map shows the major topics we have covered in this course. And here are some key high level takeaway messages. First we talked about the natural language content analysis. Here the main takeaway message is natural language processing is the foundation for text retrieval. But current NLP isn't robust enough, so the bag of words representation is generally the main method used in modern search engines. And it's often sufficient for the most of the search tasks, but obviously for more complex such tasks than we need a deeper natural language processing techniques. And we then talked about the high level strategies for text access and we talked about the push vs pull. In pull, we talked about the querying versus browsing. In general, in future search engines we should integrate all these techniques to provide multiple information access. And then we talked about a number of issues related to search engines we talked about the search problem, and we framed that as a ranking problem. And we talked about a number of retrieval methods. We started with the overview of vector space model and the probabilistic model, and then we talked about the vector Space Model in depth. and we also later talked about the language modeling approaches, and that's a probabilistic model and here The main takeaway messages is that modern retrieval functions tend to look similar, and they generally use various heuristics. Most important ones are TF-IDF weighting, document length normalization, and TF is often transformed through a sublinear Transformation function. And then we talked about how to implement a retrieval system. And here the main techniques that we talked about are how to construct the inverted index so that we can prepare the system to answer query quickly. And we talked about how to perform search by using the inverted index. And we then talked about how to evaluate the text retrieval system. Mainly introduced the Cranefield evaluation methodology. This is a very important evaluation methodology that can be applied to many tasks. We talked about the major evaluation measures, so the most important measures for search engine--map(mean average precision),  nDCG(normalized, discounted cumulative gain), an also precision and recall are the two basic measures. And we then talked about the feedback techniques and we talked about the Rocchio in the vector space model and the mixture model in the language modeling approach. Feedback is a very important technique, especially considering the opportunity of learning from a lot of clickthroughs on the web. We then talked about Web search, and here we talked about how to use parallel indexing to solve the scalability issue. In indexing we introduce the map reduce and then we talked about how to use linking information on the web to improve search. We talked about page rank and HITS as the major algorithms to analyze links on the web. We then talked about learning to rank. This is the use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach, but we can also improve the robustness of the ranking function so that it's not easy to spam the search engine with just. Some features to promote a page. And finally, we talked about the future of web search. We talked about some major directions that we might see in the future in improving the current generation of search engines. And then finally we talked about the recommender systems and these are systems to implement the push mode and we talked about the two approaches. One is content based, one is collaborative filtering and they can be combined together. Now. An obvious missing piece in this picture is the user, you can see, so user interface is also an important component in any search engine, even though the current searching interface is relatively simple, they actually have been a lot of studies of user interface related to visualization for example. And this is a topic that you can learn more by reading this book. It's an excellent book about all kinds of studies of search interface. If you want to know more about the topics that we talked about, you can also read some additional readings that are listed here in this short course. We only managed to cover some basic topics in text retrieval and search engines. And these resources provide additional information about the more advanced topics, and they gave a more thorough treatment of some of the topics that we talked about and a main source is synthesis digital library. Where you can see a lot of short textbook or textbooks or long tutorials, they tend to provide a lot of information to explain a topic and there are multiple serieses that are related to this course, and one is the information concepts retrieval and services and another is Human language technology and yet another is artificial intelligence and machine learning. There were also some major journals and conferences listed here that tend to have a lot of research papers related to the topic of this course, and finally, For more information about resources, including readings and toolkits, etc. You can check out this URL. So if you have not taken the text mining course in this data mining specialization series, then naturally the next step is to take that course as this picture shows to mine big text data we generally need two kinds of techniques, one is text retrieval, which is covered in this course and these techniques would help us convert the raw big text data into small relevant text data which are actually needed in the specific application. Human plays an important role in mining any text data becausw text data is written for humans to consume, so involving humans in the process of data mining is very important. And in this course we have covered various strategies to help users get access to the most relevant data. These techniques are also essential in any text mining system to help provide provenance, stand to help users interpret in the patterns that user would find through text data mining. So in general the user would have to go back to the original data to better understand the patterns. So the text mining course, or rather text mining analytics course will be dealing with what to do once the user has found the information. So this is the second step in this picture where we would convert the text data into actionable knowledge. And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge buried in text and such knowledge can then be used in application system to help decision making or to help user finish a task. So if you have not taken that course, the natural step in the natural next step would be to take that course. Thank you for taking this course. I hope you have found this course to be useful to you and I look forward to interacting with you at a future opportunity.
410	c78efb34-5da4-4f3b-aefa-9fa2f2f5337d	in this lecture we are going to talk about how to improve the instantiation of the vector space model this is a continual discussion of the vector space model we're going to focus on how to improve the instantiation of this model in the previous lecture you have seen that with simple since initiations over the vector space model we can come up with a simple scoring function that will give us basically account of how many unique query terms are matching the document we also have seen that this function has a problem as shown on this slide in particular if you look at these three documents they will all get the same score becaus they matched three unique query words but intuitively would like A T four to be ranked above the three and D two is really non relevant so the problem here is that this function couldn't capture at the following heuristics first we would like to give more credit to two D four becaus it matched the presidential more times than the three second intuitively matching presidential should be more important than matching about because about is a very common word that occurs everywhere it doesn't really carry that much content so in this lecture let's see how we can improve the model to solve these two problems it's worth thinking at this point about why do we have this role problems if we look back at assumptions we have made while instantiating the vector space model we will realize that the problem is really coming from some of the assumptions in particular it has to do with how we place the vectors in the practice space so then naturally in order to fix these problems we have to revisit those assumptions perhaps we will have to use different ways to instantiate the vectors based model in particular we have to place the vectors in the different way so let's see how can you prove this while natural thought is in order to consider multiple times of the term in the document we should consider the term frequency instead of just absence or presence in order to consider the difference between a document where a queer at home occured in multiple times and one where the query term occur just once we have to consider the term frequency the calendar of a term in the document in the simplest model with only model the presence and absence of the time we ignored the actual number of times that a term occurs in the document so let's add this back so we can do then represent document by a vector was term frequency as elements so that is to say now the elements of both the query about the and the document about the will not be zero or once but instead there will be the counts of award in the query or the document so this was bringing additional information about the document so this can be seen as more accurate representation of our documents so now let's see what the formula would look like if we change this representation so as you see on this slide we still use dot product and so the formula looks very similar in the form in fact it looks identical but inside the sum of course zion why are now different then out of the counts of words i in the query and in the document now at this point i also suggest you to pause the lecture for moment and just think about how we can interpret the score of this new function it's doing something very similar to what the simplest at V S M is doing but be cause of the change of the vector now the new score has a different interpretation can you see the difference and it has to do with the consideration of multiple occurrences of the same term in a document more important they would like to know whether this would fix the problems of the simplest of access based model so let's look at this example again so supposedly changed the vector appendage into term frequency vectors now let's look at these three documents again the query vector is the same becaus all these words occur exactly once in the query so the vector is zero zero one vector and in fact D two is also essentially representing the same way be cause none of these words has been repeated many times as a result of the score is also same steel three the same issue forty three and we still have a three but before would be different this is now presidential occured twice here so the element of for presidential in the talk with the vector would be two instead of one as a result now the score forty four is higher it's a fall now so this means by using term frequency we can now rank D four about D two and D three as we hope to so this solve the problem with default but we can also see that T two and T three R steer treated in the same way this they have identical scores so it did not fix the problem here so how can we fix this problem intuitively we would like to give more credit for matching presidential then matching about but how can we solve the problem in a general way is there any way to determine which words should be treated more importantly and which word can be basically ignored about is such a word at which does not really care that much content we can essentially ignore that we sometimes call such a word stop word those are generally very frequently occur everywhere match it doesn't really mean anything but computationally how can we capture that so again i encourage you to think a little bit about this can you come up with any statistical approaches to somehow distinguish impressed angel from about if you think about it for a moment you realize that what differences that award like about occurs everywhere so if you count the currents of the world in the whole collection then we would see that about as much higher frequency then presidential which tends to occur only in some documents so this idea suggests that we could somehow use the global statistics of terms or some other information to try to down weight the element for about in the vector representation of D two at the same time we hope to somehow increase the weight of presidential in the vector of these three if we can do that then we can expect that D tool will get the overall score to be less sensory Y A D three will get the school about three then we will be able to rank these on top of D two so how can we do this systematically again we can rely on some statistical counts and in this case the particular idea is called the inverse document frequency we have seen document frequency as one signal used in the modding retrieval functions we discussed this in previous vector so here's a specific way of using it document frequency is the calendar of documents that contain a particular term here we say inverse document frequency becaus we actually want to reward award that doesn't occur in many document and so the way to incorporate this into our vector repetition is to modify the frequency count by multiplying it by the IDF of the corresponding word assume here if we can do that then we can't analyze common words which generally have a lower idea and reward rail words which we have a high IDF so more specifically the IDF can be defined as a logarithm of N plus one divided by K wear em is the total number of documents in the collection K is the TF or document frequency the total number of documents containing the word W now if we plot this function by varying K then you want to see the curve would look like this in general you can see it would give higher value for low D F word a rare work you can also see the maximum value of this function is log of N plus one will be interesting for you to think about what's minimum value for this function this could be interesting exercise now this specific function may not be as important as the heuristic to simply penalize popular times but it turns out that this particular function form has also worked very well now whether there is a better form of function here is still open research question but it's also clear that if we use a linear panelization like what's shown here with this line then it may not be as reasonable as the standard IDF in particular you can see the difference in the standard IDF and we somehow have turning point here after this point that we're going to say these terms are essentially not very useful there can be essentially ignored and this makes sense when the term occurs so frequently and let's say a term occurs in more than fifty percent of the documents then the term is unlikely very important and it's basically a common term it's not very important match this water so with the standard idea you can see it's basically assuming that they all have low weights there's no difference but if you look at the linear penalization at this point there is there some difference so intuitively would want to focus more on the discrimination of low D F words rather than these common words well of course which one works better still has to be validated by using the empirically created data set and we have to use users to judge which results are better so now let's see how this can solve problem too so now let's look at the two documents again now without the idea of waiting before we just have term frequency vectors but with the idea of waiting we now can adjust the T F weight by multiplying with the idea of value for example here we can see is adjustment and in particular for about there is adjustment by using the idea of value of about which is smaller than the IDF value of presidential so if you look at this the idea where distinguish these two words as a result of adjustment here would be larger would make this weight larger so if we score with these new vectors then what would happen is that of course they share the same weights for news then the campaign but the matching of about an presidential will discriminate them so now as a result of IDF weighting we will have the three to be ranked above D two becaus it match the real world whereas D two match the common word so this shows that the idea of waiting can solve problem too so how effective is this model in general when we use TF IDF weighting well let's look at the all these documents that we have seen before these are the newest scores of the new documents but how effective is this new weighting method an new scoring function so now let's see overall how effective is this new ranking function with TF IDF weighting here we show all the five documents that we have seen before and these are their scores now we can see the scores for the first four documents here seem to be quite reasonable they are as we expected however we also see a new problem becaus now TI five here which did not have a very high score with our simplest vectors based model now actually has a very high score in fact it has the highest scope here so this creates a new problem this is actually a common phenomenon in designing retrieval functions basically when you try to fix one problem you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function is there open research question researchers are still working on that but in the next few lectures of urban doors or talk about some additional ideas to further improve this model and try to fix this problem so to summarize this lecture we've talked about how to improve the vectors based model an we have gotten improve the instantiation of the vectors based model based on TF IDF weighting so the improvement most of that is on the placement of the vector where we give higher weight to a term that occur them many times in a document but infrequently in the whole collection and we have seen that this improvement model indeed works better than the simplest vectors based model but it also still has some problems in the next structure we're going to look at the how do address these additional problems
410	c91df6aa-48b6-4ceb-a12e-f31953378688	this lecture is about how to evaluate the text retrieval system when we have multiple levels of judgments in this lecture we will continue the discussion of evaluation we're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments so so far we have talked about binary judgments that means a document is judged as being random in the owner adam but earlier we also talked about relevance as a matter of degree so we often can't distinguish it very highly relevant documents those are very useful documents from your moderator random in the documents they're OK they're used for perhaps and further from nonrandom documents those are not useful so imagine you can have ratings for these pages then you would have multiple levels of ratings for example here i show example of three levels through evil relevant sorry three four very relevant to for marginally relevant than one for non relevant now how do we evaluate the search engine system using these judgments obviously the map doesn't work average precision doesn't work precision and recall it doesn't involve becaus they rely on binary judgments so let's look at the some top ranked results when using these judgments right imagine the user would be most really care about the top ten results here right and we marked the rating levels or relevance levels for these documents as assume here three two one one three etc and we call these game and the reason why we call it again is be cause the measure that we're introducing is called endd normalized discounted cumulative gain so this game basically can measure how much gain of relevant information a user can obtain by looking at each document right so looking at the first document that the user can gain three points looking at the non random document user with only game one point by looking at the moderate are relevant or marginal relevant document the user would get two points it cetera so this gain intuitive the mesh is the utility of a document from the users perspective of course if you assume the user stops at the ten documents and we're looking at the color of the ten we can look at the total game of the user and what's that well that's simply the sum of this and we called the cumulative day so if the user stops at the position one that's just for three if the user looks at another document that's a three plus two if the user looks at more documents then the cumulative gain is more of course this is at the cost of spending more time to examine the list so cumulative gain gives us some idea about how much too low gain the user would have if the user examines all these documents now in a TCG we also have another letter here T discounted cumulative gain so why do we want to do discounting well if you look at this cumulative gain there is one deficiency which is did not consider the rank position of these documents so for example looking at some here and we only know there is one highly random document one marginally raring document to non relevant documents we don't really care where they are ranked ideally we want these two to be ranked on the top and which is the case here but how can we capture that intuition well we have to say well this is three here is not as good as this three on the top and that means the contribution of the gain from different positions has to be weighted by their position and this is the idea of discounting basically so we're going to say well the first one doesn't need to be discounted because the user can be assumed that you always see this document but the second while this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it so we divide this cane by the weight based on the position so log of two two is the rank position of this document and when we go to the third position we discount even more because the normalizes log of three and so on and so forth so when we take a such a sum then a lower rear end document will not contribute contribute that much as highly ranked document so that means if you for example switch the position of this let's say this position and this one and then you would get more discount if you put for example very relevant document here as opposed to tool here imagine if you put the three here then it would have to be discounted so it's not as good as if we put the three here so this is the idea of discounting OK so now at this point that we have got the discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments so are we happy with this where we can use this rank systems now we still need to do a little more in order to make this measure comfortable across different topics and this is the last step ann by the way here we just show the TCG at ten right so this is the total sum of this EG of all these ten documents so the last step is called N normalization an if we do that then we will get a normalized disease and so how do we do that well the idea here is within the normalized DCG by the ID or TCG at the same cut of what is the ideal DCG well this is any of ID or ranking so imagine if we have nine documents in the whole collection rated three here and that means in kudo we have nine documents rated three then our ideal rank the listener would have put all these nine documents on the very top so all these would have to be three and then this would be followed by a two here because that's the best we could do after we have run out of trees but all these positions would be series right so this would be ideal ranked list and then we can compute the TCG for this ideal rank list so this would be given by this formula that you see here and so this idea this would then be used as the normalizer TCG so here and this idea dizzy will be used as a normalizing so you can imagine now normalization essentially it will compare the actual dizzy with the best DVD you can possible to get for this topic now why do we want to do this where by doing this we're map the DCG values into a range of zero through one so the best value or the highest value for every query would be one that's when you're ranked list is in fact the idea list but otherwise in general you will be lower than one now what if we don't do that well you can see this transformation or this normalization doesn't really affect the relative comparison of systems for just one topic becausr this ideal DCG is the same for all the systems so the ranking of systems based on only DG would be exactly the same as if you rank them based on the normalized DCG the difference however is when we have much more topics becaus if we don't do normalization different topics will have different scales of this EG for a topical like this one we have nine highly relevant documents the DCG can get really high but imagine another case there are only two very relevant documents in total in the whole collection then the heist TCG that any system could achieve for such a topical will not be very high so again we face the problem of different scales of DCG values and we take an average it we don't want the average to be dominated by those high values those are again easy queries so by doing the normalization we can avoid the avoided the problem making all the queries contribute equal to the average so this is the idea of NDC G it's useful for measuring ranked list based on multiple level relevance judgments so more in a more general way this is basically a measure that can be applied to any rank the task with multiple level of judgment ann the scale of the judgments can be multiple can be more than binary not only more than binary there can be multiple levels and like one zero five or even more depending on your application and the main idea of this measure i just to summarize is to measure the total utility of the top K documents so you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower rank the document and then finally it would do normalization to ensure comparability across queries
410	c9288ad9-146c-43c5-a84c-95df47b082d0	In this lecture we continue the discussion of vector space model. In particular, we're going to talk about the TF transformation. In the previous lecture, we have derived a TF IDF weighting formula using the vector space model. And we have shown that this model actually works pretty well for these examples, as shown on this slide except for D5, which has received very high score. Indeed, it has received the highest score among all these documents, but this document is intuitively non relevant, so this is not desirable. In this lecture we're gonna talk about how we can use TF transformation to solve this problem. Before we discuss the details, let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high score. So this is the formula and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms. And inside the sum, each matching query term has a particular weight and this way it is TF IDF weighting. So it has an idea of component where we see two variables. One is the total number of documents in the collection. And that is M. The other is the document frequency. This is the number of documents that contain this word W. The other variables involved in the formula included the count of the query, term Term. W in the query and the count of the word in the document. If you look at this document again, now it's not hard to realize that the reason why it hasn't received the highest score is be cause it has a very high count of campaign. So the count of campaign in this document is a four which is much higher than the other documents and has contributed to the high score of this document. So intuitively, in order to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. And if you think about the matching of terms in a document carefully, you actually would realize. We probably shouldn't reward multiple occurrences. So generously. By that I mean the first occurrence of term says a lot about the matching of this term because it goes from zero count to a count of 1 and that increase means a lot. Once we see a word in the document, it's very likely that the document is talking about this word. If we see a extra occurrence on top of the first occurrence, that is to go from one to two. Then we also can say that was the 2nd occurrence. Kind of confirmed that it's not a accidental matching of the word. Now we are more sure that this document is talking about this word. But imagine we have seen, let's say, 50 times of the word in the document. Then adding one extra occurrence is not good to tell us more about the evidence 'cause we already sure that this document is about this word. So if you think in this way, it seems that we should restrict the contribution of high count of term. And that is the idea of TF transformation. So this transformation function is going to turn the raw count of word into a term frequency, wait for the word in the document. So here I show in X axis the raw count. In Y axis I showed term frequency weight. So in the previous ranking functions we actually have implicitly used some kind of transformation. For example, in the 01 bit vector representation, we actually use researcher transformation function as shown here. Basically, if the count is 0, then it has zero weight, otherwise it would have a weight of 1. It's a flat. Now, what about using term count as TF wait, that's the linear function, right? So it has just exactly the same way as the count. We have just seen that this is not desirable. So what we want is something like this. So for example, with the logarithm function, we can have a sub linear transformation that looks like this and this would control the influence of really high weight because it's going to lower its inference, yet it will retain the inference of small counts. Or we might want to even bend the curve more by applying logarithm twice. Now people have tried all these methods and they are indeed working better than the linear form of the transformation. But so far what works the best seems to be this special transformation called  BM25 transformation. BM stands for best matching. Now in this transformation you can see there's a parameter K here. And this K controls the upper bound of this function. It's easy to see this function has upper bound. Because if you look at the x / x + K where K is non negative number then the numerator will never be able to exceed the denominator, right? So it's upper bounded by K + 1. This is also difference between this transformation function and the logarithm transformation which doesn't have upper bound. Furthermore, 1 interesting property of this function is that as we vary K, we can actually simulate a different transformation functions, including the two extremes that I've shown here. That is a 01 bit transformation and the linear transformation. So for example, if we set K to 0 now you can see, the function value would be 1. So we precisely recover the 01 bit transformation. If you set K to a very large number, on the other hand is going to look more like the linear transformation function. So in this sense, this transformation is very flexible. It allows us to control the shape of the transformation. It also has a nice block of the upper bound. And this upper bound is useful to control the influence of a particular time. And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time. In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to computer score. As I said, this transformation function has worked well so far. So to summarize this lecture. The main point is that we need to do some linear TV TF transformation, and this is needed to capture the intuition of diminishing return from higher term counts. It's also to avoid dominance by one single term over all others. This BM25  transformation that we talked about is very interesting. It's so far one of the best performing TF transformation formulas. It has upper bound and social robust and effective. If you plug this function into our TF IDF weighting vector space model. then we would end up having the following ranking function which has a BM 25 TF component. Now this is already very close to 2. a state of the art ranking function called BM 25. And will discuss how we can further improve this formula in the next lecture.
410	cc9d1885-767f-4969-b091-62523657bef7	this lecture is about the feedback in the vector space model in this lecture we continue talking about the feedback in text retrieval particularly we're going to talk about feedback in the vector space model as we have discussed before in the case of feedback task of a text retrieval system is rhel learn from examples to improve retrieval accuracy we will have positive examples those are the documents that are assumed little bit relevant or judgment be relevant all of the document that are viewed by users we also have negative examples those are documents known vinaigrette and they can also be the document atascii why users the general method in the vectors based model for feedback is to modify our query about them and we want to place the query about the in a better position to make it accurate and what does that mean exactly well if we think about the query vector i would mean we have to do something to the vector elements and in general that would mean we might add new terms and we might adjust weights of all terms or assign weights to new terms and as a result in general the query we have more terms so we often call this query expansion the most effective method in the vector space model or feel bad news called rock your feedback which was after it proposed several decades ago so the idea is a quite simple we illustrate this idea by using a two dimensional display of all the documents in the collection and also the query vector so now we can see the query vector is here in the center and these are all the documents so when you sequera bakhtaran user similarity function will find the most similar documents we are basically join our so called here and then these documents would be basically the top rank the documents and this process are relevant documents and these are very LA kings for example this relevant etc and then these minus is our negative documents like this so our goal here is trying to move this query vector to sum position to improve the ritual accuracy by looking at this diagram what do you think where should we move the query vector so that we can improve the retrieval accuracy intuitively where do you want to move the query graph two if you want to think more you can post a video if you think about this picture you can realize that in order to work well in this case you want the query about that we as close to the positive factors as possible that means ideally you want to place the query back that somewhere here or you want to move the query vector closer to this point now so what exactly this point well if you want these relevant documents to rank the on the top you want this to be in the center of all these relevant documents right because then if you draw a circle around this one you get all these relevant documents so that means we can move the query vector toward the center of all the relevant document vectors and this is basically the idea of rock hill of course you can consider the centroid of negative documents and we want to move away from the negative log not joe match but they were talking about the moving closer to some other back then away from other vectors algebraically it just means we have this formula here you can see this is original query vector anne this average basically is the centroid vector of relevant documents when we take the average of these vectors there were computing the centroid of these vectors a similarly this is the average of that now relevant locking in the factors so it's essentially of now remember documents and we have these three parameters here awful beta and gamma there controlling the amount of movement when we add these two actors together we're moving the query that the closer to the central later i said well add them together when we subtract this part what kind of move the query vector away from that century so this is the main idea of rock hill feedback and after we have done this we work at a new query vector which can be used to score documents this new re nuclear about the will then reflect the move of this original query vector toward this relevant centroid vector and away from the noun relevant essentially about OK so let's take a look at the example this is the example that we have seen earlier only that i in the display of the actual documents i only showed the vector representation of these documents we have five documents here and we have two random in the documents here right they are displayed in red and these are the term vectors and i have just assumed some TF IDF weights a lot of terms we have zero weights of course and these are negative documents there are two here there is another one here now in this rock hill method we first compute the center of each category i saw let's see look at the center of the vector of the positive documents we simply just so it's very easy to see we just add this with this one the corresponding element and then that's down here and take the average and then we win the added the corresponding elements and then just take the average we do this for all these in the end that what we have is this one this is the average vector of these tool so it's a centroid of these tool that's also look at the central area of the negative documents this is basically the same we're going to take the average of three elements and these are the corresponding elements in the three vectors and so on so forth so in the end we have this one now in the rock hill feedback method we're going to combine all these with the original query about that which is this so now let's see how we combine them together well that's basically this so we have a parameter are for controlling the original query term wait that's one and then we've baydar to control the inference of the positive centroid vector way that's one point five that comes from here right so this girls kia and we also have this negative wait here controller by gamma here in this world has come from cause the negative essentially here and we do exactly the same for other terms each is for one term and this is our new vector and we're going to use this new query vector this one to rank documents you can imagine what would happen right becaus of the movement that this one with match these reader documents much better becaus we move that this vector closer to then and it's going to penalize these black documents these non rather the documents so this is precisely what we want a front feedback now of course if we applied this method in practice we will see one potential problem and that is the original query has only four terms that are non zero but after we do query expansion you can imagine and we have made it comes that would have nonzero weights so the calculation would have to involve more terms in practice we often truncate this vector an only retain the terms with heist awaits so let's talk about how we use this method in practice i just mentioned that we often truncated that can see the only a small number of words that have highest weights in the central factor this is for efficiency answered i also say that here that a negative examples or non relevant examples can not be very useful especially compared with positive examples now you can think about why one reason is because negative documents tend to distract the query in all directions so when i take the average it doesn't really tell you where exactly should be moving whereas positive documents tend to be clustered together and they were point you to consistently direction so that also means that sometimes we don't have to use those negative examples but note that in some cases in difficult queries where most programming results are negative negative feedback factor is very useful another thing is will avoid overfitting that means we have to keep relatively high weight on original query terms why be cause the sample that we see in feedback is a relatively small sample we don't want to over the trust of this more simple an the original query terms are still very important those terms of hyping by the user and the user has decided that those terms are most important so in knowledge to prevent us from over fitting or drifting topics with prevent talk with drifting you do the bias toward the feedback examples with general would have to keep up with a higher weight on the original terms so it is safe to do that and this is especially true for pseudo relevance feedback now this method can be used for both relevance feedback and pseudorandomness feedback in the case of pseudo feedback up the parameter beta should be set to a smaller value that cause the relevant examples are assumed to be random as reliable as he relevance feedback in the case of relevance feedback we love this could use a larger value so those parameters they have to be set an paracle and the root rock your method is usually robust and effective it's still very popular method the force feedback
410	ce32ec54-c313-4bab-8698-300a558c5790	this lecture is about the future of web search in this lecture we're going to talk about some possible future trends of web search and intending the information retrieval systems in general in order to further improve the accuracy of search engine it's important to consider special cases of information need so one particular trend could be to have more and more specialized than a customized search engines and they can be called a vertical search engines these vertical search engines can be expected to be more effective than the current general search engines becaus they could assume that the users are special group of users that might have a common implementing need and then the search engine can be customized search users and be cause of the customization it's also possible to do plus summarization so the search can be personalized he calls we have a better understanding of the users be cause of the restricting the domain we also have some advantage in handling the documents be cause we can have better understanding of documents for example particular words may not be ambiguous in such a domain so we can bypass the problem of ambiguity another trend that we can't expect the Z is the search engine will be able to learn all the time it's like life time learning or lifelong learning and this is of course very attractive because that means the search anywhere self improve itself as more people are using it search anywhere become better and better and this is already happening because the search engines that can learn from the implicit feedback and more users use it and the quality of the search results for the popular queries that are typing by many users will likely become better so this is another feature of that we would see the third of trained might be the integration of multi models of information access so search navigation and recommendation or filtering might be combined to form a full fledged information management system and in the beginning of this cause we talked about push versus port these are different modes of information access but these modes can be combined and similarly in the poor mode querying and browsing could also be combined them and in fact that we're doing that basically today with the current search engines we are querying sometimes browsing clicking on links sometimes we've got some information recommended although most of the cases information recommended becaus of advertising but in the future you can imagine similarly integrate the system with multimode for information access and that would be convenient for people another trend is that we might see systems that try to go beyond the surgical support user tasks after all the reason why people want to search is to solve a problem or to make a decision or perform a task for example consumers might search for opinions about products in order to purchase a product and choose a good product to buy so in this case it would be beneficial to support the whole workflow of purchasing our product or choosing a product in this area after the current search engines already provide good support for example you can sometimes look at the reviews and then if you want to buy it and you can just click on the button the code or shopping side of the direct will get it down but it does not provide good task support for many other tasks for example for researchers you might want to find the relevant literature or site to the literature and then there's no support for finishing a task such as writing a paper so in general i think there are many opportunities to innovate and so in the following a few slides i'll be talking a little bit more about some specific ideas or thoughts that hopefully can help you imagine new application possibilities some of them might be already relevant to what you are currently working on in general you can think about any intelligent system especially in technical information system as these specified by these three nodes and social connect these is really into a triangle that will be able to specify information system i call this data user service triangle so basically the three questions you ask would be a whole are you serving and what kind of data are you are managing and what kind of service you provide right there this would that help us basically specify your system and there are many different ways will connect them depending on how you connect them you have a different kind of system so they may give you some example on the top you can see different kinds of users on the left side you can see different types of data or information on the bottom you can see different service functions now imagine we can connect all these in different ways so for example if then connect everyone with web pages and the support that search and browsing what do you get well that's web search what if we connect the UI UC employees with organizacion documents or enterprise documents to support the search end drowsy well that's enterprise search if you connect the scientists with literature information to provide all kinds of service including search browsing or alert of new randomly documents or mining analyzing research trends or provide the task of support or decision support for example they might be able to provide support for automatically generating related work section for research paper and this would be closer to task support so then we can imagine this would be a literature assistant if we connect the online shoppers with block articles or product reviews then we can help these people to improve shopping experience so we can provide for example there are mining capabilities to analyze the reviews too compare products coming out sentiment of callouts anne to provide a test this folder with decision support to help them choose what product to buy or we can connect customer service people with emails from the customers an we can imagine a system that can provide analysis of these emails for finding the major complaints of the customers we can imagine the system could provide task support by automatically generating response to customer email maybe intending generally attach also promotion message if a property it can detect that's a positive message not a complaint and then you might to take this opportunity to attach some promotion information or else if it's a complaint that you might be able to automatically generate this some generic response first tell the customer that he or she can expect the detail response later etc all these are trying to help people to improve the productivity so this shows that the activity days are really a lot it's just only restricted by our imagination so this picture shows the trend of the technology and also it characterizes invented information system in three angles you can see in the center there is a triangle that connects keyword queries to search and bag of words representation that means the current the search engines basically provides search support to do this and mostly model users based on keyword queries and it sees the data through a bag of words representation so it's a very simple approximation of the actual information in the documents but that's what the current system does it connects these three nodes in such a simple way or it only provides basically search function and this and that really understanding the user and then doesn't really understand that matching from mission documents now i show the some trends to push each in herb toward more advanced functions so think about the user node here so we can go beyond the keyword queries look at the user search history and then further model the user completely to understand the user 's task environment task need context other information so this is pushing for personal ization and complete the user model and this is a major direction in research in in order to build intelligent information systems on the document aside we can also see we can go beyond bag of words retransition to have entity relation representation this means will recognize peoples names or relations locations etc and this is already feasible with today's natural language processing technique and google 's reason the initiative on the knowledge graph if you have heard of it is a good step toward this direction and once we can get to that level representation in robust manner at larger scale it can enable the search engine to provide a much better service in the future we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent so this cost for large scale semantic analysis and perhaps this is more visible for vertical search engines it's easier to make progress in a particular domain now the service side we see that we need to go beyond the search of support information access in general so search is only one way to get access to information recommender systems and push in port lights with different ways to get access to relevant information but going beyond access we also need to help people digest the information once the information is found and this step has to do with analysis of information or data mining where to find the patterns or convert the text information in your knowledge that can be used in the application more actionable knowledge that can be used for decision making and furthermore the knowledge it would be used to help user to improve productivity in finishing a task for example a decisionmaking task so this is a friend and so basically in this time edging we anticipate in the future in vanity information systems will provide intelligent an interactive tasks support now i should also emphasize interactive here be cause it's important to optimize the combined the intelligence of the users and the system so we can get some help from users in some natural way an we don't have to assume the system has to do everything when the human user and the machine can collaborate in the impending way inefficient way then combine the intelligence will be high and in general we can minimize the users overall effort in solving problem so this is the big picture of future intelligent information systems and this hopefully can provide us some insights about how to make further innovations on top of what we can do today
410	cec5fcba-eefa-4d09-8c3c-624c7a621e9b	this lecture is about the web search in this lecture we're going to talk about one of the most important applications of text retrieval web search engines so let's first look at the some general challenges an AC unit is in web search now many information retrieval algorithms had been developed before the web was born so when the web was born it created the best opportunity to apply those algorithms to measure application problem that everyone would care about so naturally there had to be some further extensions of the classical search algorithms to address some new challenges encountered in web search so here are some general challenges first of this cabinet the challenging how to handle the size of the weapon in your complete list of coverage of all the information how to solve many users quickly and by answering all their queries so that's one major challenge you an before the web was won't the scale of search was relatively small the second problem is that this low quality information and there are often slams the third challenges dynamics of the the new pages are constantly created and some pages may be updated very quickly so it makes it harder to keep the index of fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine on the other hand there also some interesting opportunities that we can leverage to improve the search results there are many additional heuristics for example using links that we can leverage to improve scoring now the algorithm that we talked about the such as the vector space model our general algorithms and they can be applied to any search applications so that's the advantage on the other hand they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search web pages are linked with each other so obviously the link information is something that we can also leverage so the cause of these challenges then opportunities there are new techniques that have been divided for web search or due to the need of a web search one is parallel indexing and searching and this is what has the issue of scalability in particular googles imagine of map reduce is very inferential and has been very helpful in that aspect second and there are techniques that not developed for addressing the problem of spans so spam detection we have to prevent those spam pages from being ranked high and there are also techniques to achieve robust ranking and we're going to use a lot of signals to rank pages so that it's not easy to spend the search engine with a particular trick and the third line of techniques is link analysis and these are techniques that can allow us to improve search results by leveraging extra information and in general in web search if we're going to use multiple features for ranking not just link analysis but also exploiting all kind of clothes like layout of web pages or anchor text that describes a link to another page so here's a picture showing the basic search engine technologies basically this is the web on the left and then use it on the right side and we're going to help this user hookah taxes for the web information and the first component is a crawler that would crawl pages and then the second component that is index that that would take these pages it will create a inverted index the third component there is a retriever that with the user inverted index to answer users query by talking to the users browser and then search results will be given to the user and then the browser will show those results and to allow the user to interact with the web so we're going to talk about each of these components first we couldn't talk about the crawler also called a spider or software robot that would do something like a crawling pages on the web to build a toy roller is relatively easy 'cause you just need to start with a set of see the pages and then fetch pages from the web and pause these pages or freak out the new links and then add them to the priority queue and then just explore those additional links but to build real crawler actually is a tricky and there are some complicated issues that you have to deal with so for example robustness what if the server doesn't respond what if there is a trap that generates dynamically generated web pages that might attract your crawler to keep crawling the same site then to fetch dynamically generated pages the resource with this issue of crawling curtis and you don't want to overload the one particular server with many crawling requests an you have to respect the robot exclusion protocol you also need to handle different types of virus there were images PDF files all kinds of formats on the web and you have to also consider UI or extension so sometimes those are CGI script an there are internal references etc and sometimes you have javascript on the page that they also create the challenges and you ideally should also recognize redundant pages 'cause you don't have to duplicate the those pages and finally you may be interesting to discover hidden urls those are urls that may not be linked to any page but if you truncate the URL to shorter path that you might be able to get some additional pages so one of the major crowding strategies in general breakfast first is most common becauses natural advantages balances the server load you would not keep approving a particular server with requests also parallel crawling is very natural because this task is very easy to parallelize and there are some variations of the crowding task and one interesting variations called folks to crawling in this case we're going to crawl just some pages about a particular topic for example all pages about the automobiles and this is typically a going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and and gradually chrome or so one challenger in crawling ISRO find the new pages that people have created and people probably are creating new pages all the time and this is very challenging if the new pages have not been actually linked to any old page if they are then you can probably find them by recording the old page so these are also some interesting challenges that have to be solved and finally we might face the scenario of incremental crawling to repeated crawling right let's say if you want to build web search engine and you're the first across a lot of data from the web and then but then once you have collected all the data and in the future we just need to crawl the update pages in general you don't have to recall everything right or it's necessary so in this case you will go is to minimize the resource overhead by using minimum resources to just still closed the updates the pages so this is after a very interesting research question here and opa research question in that there aren't many standard algorithms established yet for doing this this task but in general you can imagine you can learn from the past experience so the two major factors that you have to consider our first will this page be updated frequently and do i have to crawl this page again if the page is a static page that hasn't been changed for months is you probably don't have to record the everyday right because it's unlikely that it will be changed frequently on the other hand if it's sports score page that gets updated very frequently and you may need to re croydon maybe even multiple times on the same day and the other factor to consider is this page frequently accessed by users if it if it is that it means it's a high utility page and then that's it's more important to ensure such a page to be fresh compare with another page that has never been fetched by any users for a year then even though that page has been changed a lot then it's probably not necessary to crawl that page or at least it's not as urgent as to maintain the freshness of frequently access page by users so to summarize web search is one of the most important applications of text retrieval and there are some new challenges particularly scalability if you should say quality information there also new opportunities particularly rich linked information and layout etc a crawler is an essential component of web search applications an in general we can classify two scenarios one is initial crawling and here we want to have complete crawling of the web if you are doing a general search ending or focusing crawling if you want to just to target at a certain type of pages and then there is another scenario that's incremental updating of the crowd data or incremental crawling in this case you need to optimize resource try to use minimum resource forget fresh information
410	d1d123ff-d463-4af3-bd67-df56205a7e5c	This lecture is about the vector space retrieval model we're going to give a introduction to its basic idea. In the last lecture we talked about the different ways of designing a retrieval model. Which would give us a different a ranking function. In this lecture we're going to talk about this specific way of designing a ranking function called vector space retrieval model. And we're going to give a brief introduction to the basic idea. Vector space model is a special case of similarity based models, as we discussed before, which means we assume relevance is roughly similarity between the document and the query. Now, whether this assumption is true is actually a question. But in order to solve a search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the programming language. So in this process will have to make a number of assumptions. This is the first assumption that we make here. Basically, we assume that if a document is more similar to a query than another document then the first document will be assumed to be more relevant than the second one, and this is the basis for ranking documents in this approach. Again, it's questionable whether this is really the best definition for relevance. As we will see later, there are other ways to model relevance. The basic idea of vector space retrieval model is actually very easy to understand. Imagine a high dimensional space. Where each dimension corresponds to a term. So here I show a 3 dimensional space. With three words, programming, library and presidential. So each term here defines one dimension. Now we can consider vectors in this 3 dimensional space. And we're going to assume that all our documents and the query will be placed in this vector space. So for example, one document might be represented by this vector, D1. Now this means this document probably covers library, and presidential, but it doesn't really talk about programming. Alright, what does this mean terms of representation of document? That just means we're going to look at our document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only the vector representation of the document. Of course, the document has other information. For example, the orders of words are simply ignored, and that's because we assume that the bag of words with representation. So with this representation you can already see D1 seems to suggest a topic about presidential library. Now this is different from another document which might be represented as a different vector D2 here. So in this case, the document covers programming and library, but does not talk about the presidential. So what does this remind you? You can probably guess the topic is likely about programming language, and the library is software library. So this shows that by using this vector space representation we can actually capture the differences between topics of documents. Now you can also imagine there are other vectors, for example D3 is pointing to that direction. That might be about present in your program. And in fact that we can place all the documents in this vector space. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space as another vector. And then we're going to measure the similarity between the query vector and every document vector. So in this case, for example, we can easily see D2 seems to be the closest to this query vector, and therefore D2 will be ranked above others. So this is basically the main idea of the vector space model. So to be more precise. To be more precise. Vector space model is a framework. In this framework, we make the following assumptions. First, we represent a document and query via term vector. So here are term can be any basic concept, for example a word or a phrase. Or even N-gram of characters. Those are just sequence of characters Inside the word. Each term is assumed to define one dimension. Therefore, N terms in our vocabulary would define an N dimensional space. A query vector would consist of a number of elements. Corresponding to the weights on different terms. Each document vector is also similar. It has a number of elements and each value of each element is indicating that weight of the corresponding term. Here you can see we assume there are N dimensions. Therefore there are N elements. Each corresponding to the weight on a particular term. So the relevance in this case would be assumed to be the similarity between the two vectors. Therefore, our ranking function is also defined as the similarity between the query vector and document vector. Now, if I ask you to write a program to implement this approach in the search engine, you would realize that this is far from clear, right? We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this. That's why I said this is a framework. And this has to be refined in order to actually suggest a particular ranking function that you can implement on your computer. So what does this framework not say? It actually hasn't set up many things that would be required in order to implement this function. First, it did not say how we should define or select the basic concepts exactly. We clearly assume the concepts are orthogonal, otherwise there will be redundancy. For example, if two synonyms are somehow distinguished as two different concepts, then there would be defining two different dimensions and that would clearly cause redundancy here, or over emphasizing of matching this concept. Because it would be as if you match the two dimensions when you actually match one semantic concept. Secondly, it did not say how exactly should place documents and query in this space. Basically I showed you some examples of query and document vectors, but where exactly should the vector for a particular document point to? So this is equivalent to how to define the term weights. How do you compute those element values in those vectors? Now this is a very important question because term weight in the query vector indicates the importance of term. So depending on how you assign the weights, you might prefer some terms to be matched over others. Similarly to term weight in the document is also very meaningful. It indicates how well the term characterizes the document. If you got it wrong, then you clearly don't represent this document accurately. Finally, how to define the similarity measure is also not given. So these questions must be addressed before we can have a operational function that we can actually implement using a program language. So how do we solve these problems? Is the main topic of the next lecture.
410	d7c569a0-95fd-4876-9cbf-594a163cb8d4	This lecture is about the evaluation of text retrieval systems. In the previous lectures we have talked about a number of text retrieval methods, different kinds of ranking functions. But how do we know which one works the best? In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods. So this is the main topic of this lecture. First, let's think about the why do we have to do evaluation? I already give one reason and that is we have to use evaluation to figure out which retrieval method works better. Now this is very important for advancing our knowledge, otherwise we wouldn't know whether a new idea works better than old idea. In the beginning of this course, we talked about the problem of text retrieval. We compared it with database retrieval. There we mentioned that text retrieval is empirically defined problem. So evaluation must rely on users. Which system works better would have to be judged by our users So this becomes a very challenging problem. Because. How can we get users involved in the evaluation? How can we do a fair comparison of different methods? So just go back to the reasons for evaluation. I listed two reasons here. The second reason is basically what I just said, but there is also another reason which is to assess the actual utility of text retrieval system. Now imagine you're building your own search engine applications. It would be interested in knowing how well your search engine works for your users. So in this case, matches must reflect the utility to the actual users in a real application. And typically this has to be done by using user studies and using the real search engine. In the second case, or for the second reason. The measures actually only to be correlated with the utility to actual users. Thus they don't have to accurately reflect the exact utility to users. So the measure only needs to be good enough to tell which method works better. And this is usually done through a test collection, and this is the main idea that we'll be talking about in this course. This has been very important for comparing different algorithms and for improving search engine system in general. So next we talk about what to measure right? There are many aspects of a search engine that we can measure we can evaluate. And here I listed the three major aspects. One is effectiveness or accuracy. How accurate the other search results. In this case, we're measuring systems capability of ranking relevant documents on top of non random ones. The second is efficiency. How quickly can a user get some results? How much computing resources are needed to answer query? So in this case we need to measure the space and time overhead of the system. The third aspect is usability. Basically, the question is how useful is a system for real user tasks. Here, obviously interfaces and many other things are also important, and we typically would have to do user studies. Now in this course we are going to talk mostly about effectiveness and accuracy measures because the efficiency and usability dimensions are not really unique to search engines and so. They are needed for evaluating any other software systems, and there is also good coverage of such materials in other courses. But how to evaluate a search engines quality or accuracy is something unique to text retrieval, and we're going to talk a lot about this. The main idea that people have proposed for using a test set to evaluate text retrieval algorithm is called the Cranfield evaluation methodology. This one actually was developed a long time ago, developed in 1960s. It's a methodology for laboratory test. Of system components, it's actually methodology that has been very useful not just for search engine evaluation, but also for evaluating virtually all kinds of empirical tasks. And for example, in natural language processing or in other fields where the problem is empirically defined, we typically would need to use such a methodology. And today with the Big Data Challenge with use of machine learning everywhere, this methodology has been very popular, but it was first developed for search engine application in 1960s. So the basic idea of this approach is to build a reusable test collections and define measures. One such a test collection is build. It can be used again and again to test the different algorithms, and we're going to define measures that would allow you to quantify the performance of a system or an algorithm. So how exactly would this work? We're going to have a sample collection of documents and this is just to simulate the real document collection in search application. We can also have a sample set of queries or topics. This is to simulate users queries. Then we'll have to have relevance judgments. These are judgments of which documents should be returned for which queries. Ideally they have to be made by users who formulated the queries, 'cause those are the people that know exactly what documents would be useful, and then finally we have to have measures to quantify how well systems result  matches the ideal ranked list that would be constructed based on users relevance judgments. So this methodology is very useful for starting retrieval algorithms because the tested connection can be reused many times and it would also provide a fair comparison for all the methods. We have the same criteria, same data set to be used to compare different algorithms. This allows us to compare a new algorithm with an older algorithm that was developed many years ago by using the same standard. So this is an illustration of how this works. So as I said, we need the queries that are shown here. We have Q1Q2, etc. We also need the documents that's called a document collection and on the right side you see we need relevance judgments. These are basically. The binary judgments of documents with respect to a query. So, for example d1 is judged as being relevant to Q1, D2 is judged as being relevant as well. And d3 is judged as non relevant. The two, Q1, etc. These would be created by users. But once we have these and then we basically have a text collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results. Let's say if the query is Q1 and then we would have results. Here I show R sub A as results from system A. So this is remember we talked about. Task of computing approximation of the relevant document set R sub A  is system A's approximation here. And R sub B is system B's approximation of relevant documents. Now let's take a look at these results. So which is better now? Imagine for a user, which one would you like? Now let's take a look at the both  results. And there are some differences, and there are some documents that are returned by both systems. But if you look at the results, you would feel that well, maybe A is better in the sense that we don't have many non relevant documents and among the three documents returned, two of them are relevant, so that's good, it's precise. On the other hand, one can also say, maybe B is better because we've got more relevant documents. We've got 3 instead of two. So which one is better and how do we quantify this? Obviously this question highly depends on the users task and it depends on users as well. You might be able to imagine for some users may be system A is better. If the user is not interested in getting all the relevant document. But in this case, the user doesn't have to read many and the user would see most of the relevant documents. On the one hand,  one can also imagine the user might need to have as many relevant documents as possible. For example, if you are doing a literature survey, you might be in the segment category and you might find that system B is better. So in that case we will have to also define measures to quantify them. And we might need to define multiple measures. Because users have different perspectives of looking at the results.
410	d857a66b-1018-4ffb-821a-9d8acc6f5012	So now let's talk about the extension of PLSA to derive LDA and to motivate that we need to talk about some deficiency of PLSA. First, it's not really generating model because we cannot compute the probability of a new document. You can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data. So we cannot compute the pis for future document. And there was some heuristic. A work around though. And Secondly, it has many parameters and I've asked you to compute how many parameters exactly there are in PLSA and "you will see there are many  That means the model is very complex and that also means there are many "local  overfitting and that means it's very hard to also find a good local maximum. And that really represents global maximum. And in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model. The model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. This, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. We are not always interested in modeling future data, but in other cases or if we care about generality, we would worry about this over fitting. So LDA is proposed to improve that and it basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters. Dirichlet is just a special distribution that we can use to specify prior. So in this sense, LDA is just a Bayesian version of PLSA and the parameters are now much more regularized. You will see there are many fewer parameters. And you can achieve the same goal as PLSA for text mining. It means it can compute the topic coverage and topic word distributions as in PLSA. However, there is no free launch while the parameters for PLSA  is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. So the inference part. Again, face the local Maxima problem. So essentially they are doing something very similar, but theoretically LDA is more elegant way of looking at the topic modeling problem. So let's see how we can generalize PLSA to LDA or extend the PLSA to have LDA now a full treatment of LDA is beyond the scope of this course and we just don't have time to go in depth in talking about that. But here I just want to give you a brief idea about what's the extension and what it enables. So this is a picture of LDA. Now I remove the background model just for simplicity. Now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. So these word distributions. So here and the other set of parameters are pis and we present as a vector also. And this is for convenience to introduce LDA and we have one vector for each document. And in this case in theta we have one vector for each topic. Now that the difference between LDA and PLSA is that in LDA we're going to not allow them to free the change. Instead, we're going to force them to be drawn from another distribution. So more specifically they will be drawn from 2 Dirichlet distributions respectively. "The  vectors, so it gives us a probability for a particular choice of a vector. Take for example pis, right? So this Dirichlet distribution tell us which vector of pis is more likely, and this distribution itself is controlled by another vector of parameters of alpha's. "Depending on  characterize the distribution in different ways and with force certain choices of pi's. To be more likely than others. For example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by Alpha. And similar here. The topic word distributions are drawn from another Dirichlet distribution with beta parameters and note that here Alpha has K parameters corresponding to our inference on the k values of pis for a document, whereas here beta has N values corresponding to controlling the N words in our vocabulary. Now, once we impose these price than the generation process will be different an we all start with drawing pi's from this Dirichlet distribution and this pi will tell us these probabilities. And then we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model. A similar here we're not going to have these distributions free. Instead we can do draw one from the Dirichlet distribution, and then from this, then we're going to further sample a word and the rest is very similar to the PLSA. The likelihood function now is more complicated for LDA, but there's a close connection between the likelihood function of LDA and PLSA, so I'm going to illustrate the difference here. So in the top you see PLSA. Likelihood function that you have already seen before it's copied from previous slide only that I dropped the background for simplicity. So in the LDA formulas you see very similar things. First you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions. And this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multiplied by the probability of observing the world from that topic. So this is a very important formula as I have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of LDA or PLSA and they all rely on this. So it's very important to understand this. And this gives us the probability of getting a word from a mixture model. Now next in the probability of a document we see there is a PLSA component in the LDA formula. But the LDA formula would add some integral here, and that's to explain to account for the fact that the pis are not fixed, so they are drawn from Dirichlet distribution. And that's shown here. That's why we have to take the integral to consider all the possible pi's that we could possibly draw from this "Dirichlet And similarly, in the likelihood for the whole collection, we also see further components added. Another integral here. Right, so basically in the LDA we just added these integrals to account for the uncertainties and we added of "course the  govern the choice of these parameters, pi's and theta's. So this is a likelihood function for LDA. Now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for LDA. Now you might think about how many parameters are there in LDA versus PLSA. You will see there are fewer parameters in LDA because in this case the only parameters are alphas and betas. So we can use the maximum likelihood estimated to compute that. Of course it's more complicated because the form of likelihood functions more complicated. But what's also important is not set. Now. These parameters that we are interested in, namely the topics and the coverage, are no longer parameters in LDA. In this case we have to use Bayesian inference or posterior inference to compute them based on the parameters Alpha and beta. Unfortunately, this computation is intractable, so we generally have to resort to approximate. Influence. And there are many methods are available for and then. So you will see them when you use different toolkits for LDA, or you read the papers about that these different extensions of LDA. Now here we of course can't give in depth introduction to, but just know that they are computed based on Bayesian inference with. By using the parameters of alphas and beta. But algorithmically, actually in the end, in some algorithm at least, it's very similar to PLSA an, especially when we use algorithm called collapsed Gibbs sampling. Then the algorithm looks very similar to the EM algorithm. So in the end they're doing something very similar. So to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. The best basis test setup is to take tax data as input, and we're going to output the key topics. Each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. And PLSA is the basic topic model, and in fact the most basic topic Model. And this is also often adequate for most applications. That's why we spend a lot of time to explain PLSA in detail. Now LDA improves over PLSA by imposing priors. This has led to theoretically more appealing models. However, in practice, LDA and PLSA intended to give similar performance, so in practice, PLSA, an LDA, would work equally well for most tasks. Here are some suggested readings if you want to know more about the topic. First is a nice review of probabilistic topic models. The 2nd paper has a discussion about how to automatically label a topic model. Now I've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? Can we use phrases to label the topic to make it more easy to understand? And this paper is about the techniques for doing that. The third one is empirical comparison of LDA and PLSA for various tasks. The conclusion is that they tend to perform similarly.
410	da74c929-efc1-4b65-9635-684c7ebcab3f	This lecture is about evaluation of text cluster. So far we have talked about multiple ways of doing text clustering but how do we know which method works the best? So this has to do with evaluation. Now to talk about evaluation, one must go to go back to the clustering bias that we introduced at the beginning. Because two objects can be similar depending on how you look at them, we must clearly specify the perspective of similarity. Without that, the problem of clustering is not well defined. So this perspective is also very important for evaluation. If you look at this slide and you can see we have two different ways to cluster these shapes. An if you ask a question, which one is the best or which one is better you actually see there's no way to answer this question without knowing whether we'd like to cluster based on shapes or cluster based on sizes. And that's precisely why the perspective or clustering bias is crucial for evaluation. In general, we can evaluate text clusters in two ways. One is direct evaluation and the other is indirect evaluation. So in directl valuation, we want to answer the following question: How close are the system generated clusters to the ideal clusters that are generated by humans? So the closeness here can be assessed assessed from multiple perspectives and that would help us characterize the quality of clustering results in multiple angles. And this is sometimes desirable. Now. We also want to quantify the closeness because this would allow us to easily compare different methods based on their performance figures. And finally, you can see in this case we essentially inject the clustering bias by using humans. Basically, humans would bring the needed or desired clustering bias. Now how do we do that exactly? The general procedure would look like this. Given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. That is, we're going to ask humans to partition the objects to create the gold standard. And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results. And this would be then used to compare with the system generated clusters from the same test set. And ideally we want the system results to be the same as human generated results, but in general they are not going to be the same, so we would like to then qualify the similarity between the system generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results. How well can you predict the cluster of the object in the gold standard or vice versa. Mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. F measure is another possible measure. Now again, a thorough discussion of this evaluation, of these evaluations, would be beyond the scope of this course. I've suggested some reading in the end that you can take a look to know more about that. So here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications. The 2nd way to evaluate text clusters is to do indirect evaluation. So in this case the question to answer is how useful are the clustering results for the intended applications? Now this of course is application specific question, so usefulness is is going to depend on specific applications. In this case, the clustering bias is imposed by the intended application as well. So what counts as the best clustering result would be dependent on the application. Procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. In this case what we care about is the contribution of clustering to some application. So we often have a baseline system to compare with. This could be the current system for doing something and then you hope to add clustering to improve it or the baseline system could be using a different clustering method and you then what you are trying to experiment with and you hope to have a better idea for clustering. So in case you have a baseline system to work with and then you can add a clustering algorithm to the baseline system to produce a clustering system. And then we're going to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application. So in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. So to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. This is often needed to explore text data. And this is often the first step when you deal with a lot of text data. The second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful. There are many approaches that can be used for text clustering and we discussed them: Model based approaches and similarity based approaches. In general, strong clusters tend to show up no matter what method is used. Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. Deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data to guide us to select the best number of clusters. Now sometimes you may see some methods that can automatically determine the number of clusters. But in general, that has some implied application of clustering bias there, and that's just not specified. Without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what? So this is important to keep in mind. And I should also say sometimes we can use application to determine the number of clusters. For example, if you are clustering search results, then obviously you don't want to generate 100 clusters, right? So the number can be dictated by the interface design. In other situations, we might be able to use the fitness of data to assess whether we've got a good number of clusters to explain our data well and to do that, you can vary the number of clusters and watch how well you can fit the data. If it's in general, when you add more components to mixture model, you should fit the data better, because you can always set the probability of using the new component at 0, so you can't in general fit the data worse than before, but as the question is, as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters. And finally, evaluation of clustering results and can be done both directly and indirectly. And we also would like to do both in order to get good sense about how our method works. So here's some suggested reading, and this is particularly useful to better understand the how the measures are calculated and clustering in general.
410	db1d54dd-bb05-46c0-995b-5f7d5243e3c4	This lecture is about the text representation. In this lecture we're going to discuss text representation. And discuss how natural language processing can allow us to represent text in many different ways. Let's take a look at this example sentence again. We can represent this sentence in many different ways. 1st. We can always represent such a sentence as a string of characters. This is true for all the languages when we store them in the computer. When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. The reason is because we're not even recognizing words. So as a string we're going to keep all the spaces and these ASCII symbols. We can perhaps count how... what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics. Yet this is the most general way of representing text, because we can use this to represent any natural language text. If we try to do a little bit more natural language processing by doing word segmentation. Then we can obtain a representation of the same text, but in the form of a sequence of words. So here we see that we can identify words like: a, dog, is, chasing, etc. Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc. And these words can be used to form topics. When we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. So representing text data as a sequence of words opens up a lot of interesting analysis possibilities. However, this level of representation is slightly less general than string of characters, because in some languages such as Chinese, it's actually not that easy to identify all the word boundaries, because in such a language you see text as a sequence of characters with no space in between. So you have to rely on some special techniques to identify words. In such a language, of course, then we might make mistakes in segmenting words. So the sequence of words representation is not as robust as string of characters. But in English it's very easy to obtain this level of representation, so we can do that all the time. Now if we go further to do natural language processing, we can add a part of speech tags. Now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. So this opens up a little bit more interesting opportunities for further analysis. Note that I use the plus sign here, because by representing text as a sequence of part of speech tags. We don't necessarily replace the original word sequence representation Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. This enriches the representation of text data and thus, also,  enables a more interesting analysis. If we go further then we'll be parsing the sentence to obtain a syntactic structure. Now this of course further open up more interesting analysis of, for example, the writing styles, or correcting grammar mistakes. If we could go further for semantic analysis, then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location. And we can further analyze their relations, for example, dog is chasing the boy and the boy is on the playground. Now this is to add more entities and relations through entity-relation recognition. At this level, then we can do even more interesting things. For example, now we can count easily the most frequent person that's mentioned in this whole collection of news articles, or whenever you mention this person, you also tend to see mention of another person, etc. So this is very useful representation an it's also related to the Knowledge Graph that some of you may have heard of. That Google is doing as a more semantic way of representing text data. However, it's also less robust than sequence of words or even syntactic analysis, because it's not always easy to identify all the entities with the right types, and we might make mistakes, and relations are even harder to find and we might make mistakes. So this makes this level of representation less robust, yet it's very useful. Now if we move further to logical representation then we can have predicates and even inference rules. And with inference rules we can infer interesting, derived facts from the text. So that's very useful, but unfortunately at this level of representation it's even less robust and we can make mistakes, and we can't do that all the time for all kinds of sentences. And finally, speech acts with added yet another level of representation of the intent of saying this sentence. So in this case it might be a request. So knowing that would allow us to analyze more, even more interesting things about the observer order. Author of this sentence, what's the intention of saying that? What scenarios, what kind of actions will be made? So this is... Another level of analysis that would be very interesting. So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. And unfortunately, such techniques would require more human effort. And they are less accurate. That means there are mistakes. So if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors. So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words. On the right side you see the arrow points down, to indicate that as we go down with our representation of text, it's closer to knowledge representation in our mind, and need for solving a lot of problems. Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. That's the purpose of text mining. So there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust. But wouldn't actually give us the necessary deeper representation of knowledge. I should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. They are always in the loop. Meaning that we should optimize the collaboration of humans and computers. So in that sense, it's OK that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively.
410	dc61d7ef-1929-4c5a-95ac-e2c8d8e8c610	this lecture is about the feedback in the language modeling approach in this lecture we will continue the discussion of feedback in text retrieval in particular we're going to talk about the feedback in language modeling approaches so we derive the query likelihood ranking function by making various assumptions as a base for retrieval function that formula all those formulas work that well but if we think about the feedback information it's a little bit of all quarter to use query like hold too perform feedback because a lot of times the feedback information is additional information about the query but we assume that the query is generated by assembling words from language model in the query like hold method it's kind of a natural to assemble words that form feedback documents as a result then researchers proposed a way to generalize query like hold function and it's called quebec labor diverges retrieval model and this model is actually going to make the query likely hold retrieval function much closer to vertice based model yet this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query like holder as a special case and in this case then feedback can be achieved the throw simple query model estimation more updating this is very similar to rock hill which updates query vector so let's see what is this care divergent switcher model so on the top what you see is query like hold retrieval function right this one and that care divergens or also called cross entropy which is more model is basically to generalize the frequency part here into a lambda model so basically it's the difference given by the probabilistic model here to characterize what the user is looking for versus the count of query words there and this difference allows us to plug in various different ways to estimate this so this can be estimated in many different ways including using feedback information now this is called a care divergent becaus this can be interpreted as measuring the KL divergent SOV two distributions one is the query model in order by this distribution one is the document that language model here an SMS it with clashing language model of course and we're not going to talk about the detail of that and going to find it in some references it's also called cross entropy be cause in the fact we can ignore some terms in the care divergent function and we will end up having actually cross entropy and that both are terms in information theory but anyway for our purpose here you can just receive the two formulas look almost identical except that here we have a probability of award given by a query language model all this and here the sum is over all the words that are in the document and also with the number zero probability for the query model so it's kind of again a generalization of some over the match query words now you can also easy to see we can recover the query like code which will something by simplest setting this query model to the relative frequency of words in the query this is very easy to see once you plug this into here you can eliminate this query length that's a constant and then you'll get exactly like that so you can see the equipments and that's also why this K I virgins model can be regarded as a generalization of query like hold becaus we can cover query like rolled as a special case but it would also allow us to do much more than that so this is how we can use the care divergent 's model to feedback the picture shows that we first estimate document language model then we estimate the query named model and we compute the KL divergent is often denoted by A D here but this basically means this was exactly i convect this based model cause we come through the vector for the document are computer another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions and then we got the results and we can find some feedback documents let's assume they are most inactive sorry mostly positive documents although we could also consider both kinds of documents so what we could do is like in rock you'll ever know compute another language model code feedback language model here again this is going to be another vector just like a computing century about the in rock hill and then this model can be combined with the original query model using a linear interpolation and this would then give us a update model just like again in rock hill so here we can see the parameter alpha can control the amount of feedback if it's set to zero then you say here there's no feedback after set to one we got full feedback if we ignore the original query and this is generated not desirable so this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are important so of course the main question here is how do you compute this data F this is the big question here and once you can do that the rest is easy so here will talk about one of the approaches and there are many approaches of course this approach is based on generated model and i'm going to show you how it works this is to use a generator mixture ball so this picture shows that we have this model here the feedback model that we want to estimate and the basis is the feedback of documents let's say we are observing the positive documents these are the click the documents by users or random documents judging by users or simply top ranked document that we assumed to be relevant now imagine how we can compute ascentia weight for these documents by using language model one approach is simply to assume these documents our generator from this language model as we did before what we could do is do just normalize the water frequency here and then we get this water distribution now the question is whether this distribution is good for feedback or you can imagine the top ranking the words would be well what do you think well those words will be common was right as we always see in a language model the top rated words are actually common words like the etc so it's not very good for feedback because we would be adding a lot of such words to our query when we interpreted this with original query model so this is not good so we need to do something in particular we are trying to get rid of those common words and we have seen actually one way to do that by using background language model in the case of learning associations with words words that are related to the water computer we could do that and that would be another way to do this but here we are going to talk about another approach which is more principle approach in this case we're going to say well you say that there are common words here in this these documents that should not belong to this topic model right so now what we can do is to assume that well those was are generated from background language model so they were generated those words like the example and if we use maximum likely resume to note that if all the words here must be generated from this model then this model is forced to assign high probabilities to award like that because it occurs so frequently here note that in order to reduce its probability in this model we have to have another model which is this one to help explain the word the here and in this case it's not a property to use the background language model to achieve this goal becaus this model would assign high probabilities to these common words so in this approach then we assume this machine that would generate these words with work as follows we have a source controller here imagine we flip a coin here to decide what distribution to use was probability of lemma the coin shows up as head and we're going to use the background language model and we can do then simple word from that more with probability of one minus them now we do decide to use the unknown topic model here that we would like to estimate and we're going to then generate a reward if we make this assumption and this whole thing will be just one model and we call this mixture model becaus there are two distributions that are mixed together and we actually don't know when each distribution is used so again think of this whole thing as one model and we just go ask for words and it will still give us a war in a random manner right and of course which word would show up with depend on both this distribution and that is reaching in the teaching would also depend on this lambda becaus if you say lambda is very high and it's going to always use the background distribution you'll get different words and then if you say well i'm not very small we're going to use this right so all these are parameters in this model and then if you think in this way basically we can do exactly the same as what we did before we're going to use maximum likelihood estimator to adjust this model to estimate the parameters basically we're going to adjust well this parameter so that we can pass to explain all the data the difference now is that we are not asking this model alone to explain this but rather we're going to ask this whole model makes you more also explain the data becaus there has got some help from the background model it doesn't have to assign high probabilities towards mega the as a result it would then assign higher probabilities two other words that are common here but not having high probability here so those will be common here i and if they are common they would have to have a high probability this according to maximum like are estimated anne if they are rare here so if they are rare here then you don't get much help from this background model as a result this topic model must assign high probability days so the high probability words according to the topic model would be those that are common here but rare in the background OK so this is basically a little bit like a idea of waiting here but this would allow us to achieve the effect of removing these top awards that are meaningless in the feedback so mathematically what we have is to compute the like hold again local like hold of the feedback documents and and note that we also have another parameter lambda here but we assume that the lender denotes the noise in the feedback document so we are going to let's say set this to a parameter let's say fifty percent of the words are noise or nine example noise and this can be assumed it would be fixed if we assume this fixed then we only have these probabilities as parameters just like in the simplest unigram language model we have N parameters an is the number of words and then the likelihood of function would look like this it's very similar to the likelihood function dog likely hold function free see before except that inside of the logarithm there's a some here and this some isba cause we consider two distributions and which ones used would depend on lambert and that's why we have this form but mathematically this is the uh function with theater as unknown variables so this is just a function all the other values are known except for this guy so we can then choose this probability distribution to maximize this log likely code the same idea as the maximum like horace made it as a mathematical problem we just we just have to solve this optimization problem we send your word try all the theater values and until we find one that gives this whole thing the maximum probability so it's well defined math problem once we have done that we obtain this F that can be the interpreter with original query model to feedback so here are some examples of the feedback model learned from a web document collection and we do pseudo feedback are we just use the top ten documents and we use this mixture model so the queries airport the security what we do is we first retrieve ten documents from the web database and this is of course a pseudo feedback and then we're going to fit that mixture model to this ten welcome the set and these are the words learned using this approach this is the probability of award given by the feedback model in both cases so in both cases you can see the highest probability was include a very relevant words to the query so airport security for example these query words still show up as high probabilities in each case naturally becaus they occur frequently that operated documents but we also see beverage alcohol bomb terrorists etc so these are relevant to this topic and they if combined with the original query you can help us met more accurately documents and also they can help us bring up documents that only imagine the some of these other words and maybe for example just the airport and then form for example so this is how sudo works issues that this model really works and picks up some related words to the query what's also interesting is that if you look at the two tables here and you come here then and you see in this case when lembar is set to a small value and we still see some common words here and that means when we don't use the background model often remember lemler confuses the probability of using the background model to generate the text if we don't rely much on background model we still have to use this topic model to account for the common words whereas if we set lambda to a very high value we will use the background model very often to explain these words then there's no burden on explaining those common words in the feedback documents by the topic model so as a result of the topic model here is very discriminant if it contains all the relevant without common words so this can be added to the original query to achieve feedback so to summarize in this lecture will talk about the feedback in language model approach in general feedback is to learn from examples these examples can be assumed examples can be sued examples like assume top ten document that are assumed to be random there could be based on user interactions like a feedback based on clicks rules or implicit feedback we talked about the three major feedback scenarios random is feedback pseudo feedback and in principle feedback we talked about how to use rock you to do figure back in vector space model and how to use query model as machine for feedback in language model and we briefly talk about the mixture model the basic idea there are many other methods for example the relevance model is a very effective model for estimating query model so you can read more about these methods in the references are listed at the end of this lecture so there are two additional readings here the first one is book that has a systematic review and discussion of language models of all information retrieval and signal one is important research paper that's about relevance based on damage models and it's a very effective way of computing query model
410	dccc8a84-66da-47ce-ab88-28e8acf192b9	This lecture is a continued discussion of generative probabilistic models for text clustering. In this lecture we're going to finish the discussion of generative probabilistic models for text clustering. So this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. In this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. Now, as in most cases, the EM algorithm can be used to solve this problem for mixture models. So here's the detail of this EM algorithm for document clustering. Now, if you have understood how EML works for topic models, PLSA and I think here it will be very similar and you just need to adapt a little bit to  this new mixture model. So as you may recall, EM algorithm starts with initialization of all the parameters. So this is the same as what happened before for topic models. And then we're going to repeat until their likelihood converges. An in each step will do E step and M step. in M step. we're going to infer which distribution has been used to generate each document. And so we have to introduce a hidden variable ZD for each document and this value variable could take a value from the range of one through K representing K different distributions. And more specifically, basically we're going to apply Bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. Given the document. An we know it's proportional to the probability of selecting this distribution P of Theta I and the probability of generating this whole document from that distribution, which is a product of all the probabilities of words for this document, as you see here. Now, as in all cases, it's useful to kind of remember the normalizer or the OR the constraint on this probability. So in this case we know the constraint on this probability in the E step is that all the probabilities of Z equals I must sum to one 'cause the document must have been generated from precise or one of these K topics. So the probability of the generator from each of them should sum to one. And if this constraint then you can easy to compute this distribution as long as. what it is proportional to, right? So once you compute this product that you see here, then you simply normalize this these probabilities to make them some to 1 what over all the topics. So that's E step after E Step we would know  which distribution is more likely to have generated this document D, which is unlikely. And then in the M step, we're going to relist, made all the parameters based on the, infer the Z values, or in further knowledge about which district has been used to generate which document. So there estimation involves two kinds of parameters. One is P of Theta and this is the probability of selecting a particular distribution. Before we observe anything, we don't have any knowledge about which cluster is more likely, but after we have observed these documents, then we can collect the evidence. To infer which cluster is more likely, and so this is proportional to the sum of the probability of Z sub DJ is equal to I. And so this gives us all the evidence about using topic. I said I to generate a document and we put them together and again we normalize them into probabilities. And then so this is for P of Theta sub I. Now the other kind of parameters are the probabilities of words in each distribution, each cluster, and this is very similar to the case of PLSA. And here we just pulled the counts of words that are in documents that are inverted to have been generated from a particular topic Theta I here   and this would allow us to then estimate how many words have actually been generated from Theta I. And then we normalize again. These counts into probabilities so that the probabilities on all the words  some to one. Note that it's very important to understand these constraints as they are precisely the normalizers in all these formulas, and it's also important to know that distribution is over what? For example, the probability of Theta is overall the key topics and that's why these K probabilities sum to 1. well, whereas the probability of a word given Theta is a probability distribution over all the words. So there are many probabilities and they have to send one. So now let's take a look like this. Take a look at the simple example of two clusters. I have two clusters. I've shown some initializer values for the two distributions. And let's assume we randomly initialized to probabilities of selecting each cluster as .5. So equally likely. And then let's consider one document that you have seen here. There are two words, sorry, two occurrences of text and two occurrences of mining. So there are four words together. Medical and health did not occur in this document, so this first thing about the hidden variables. Now for each document we must use a hidden variable and before in PLSA we used 1 hidden variable for each word. Because that's the output from what mixture model. So in our case the output from a mixture model or the observation from mixture model is a document not a word. So now we have 1 hidden variable attached to the document. That hidden variable must tell us which distribution has been used to generate the document, so it's going to take two values, one and two to indicate the two topics. So now how do we infer which distribution has been used to generate the D? It's to use Bayes rule so it looks like this in order for the first topic is setup, want to generate the document. Two things must happen. First theater subway must have been selected, so it's given by P of 01 second. It must have also been generating the four words in the document, namely two occurrences of text and two occurrences of mining. That's why you see the numerator has the product of the probability of selecting Theta one and the probability of generating the document from Theta 1. So the denominator is just the sum of two possibilities of generating this document, and you can plug in the numerical values to verify. Indeed in this case the document is more likely to be generated from Theta 1,  much more likely than from than Theta 2. So once we have this problem that we can easily compute the probability of Z = 2 given this document, how we're going to use the constraint? Right now it's going to be 1 - 100 /101 1,000,000 one. So now it's important to note that in such a computation there is a potential problem of underflow, and that is because if you look at the numerator, the original numerator and denominator it involves the computation of a product of many small probabilities. Imagine if a document has many words and it's going to be a very small value here, as it can cause the problem of underflow. So to solve the problem. We can use a normalized. So here you see that we take average of all these two solutions to compute another average district called Theater Bar. And this does the average distribution will be comperable to each of these distributions. In terms of the quantities, the magnitude. So we can then divide the numerator and the denominator both by this normalizer. So basically this normalizes the probability of generating this document by using this average word distribution. So you can see the normalizer here. And since we have used exact the same normalizer for the numerator and denominator, the whole value of this expression is not changed. But by doing this normalization you can see we can make the numerators and denominators more manageable in that the overall value is not going to be very small for each, and thus we can avoid underflow problem. In some other times we sometimes also use logarithm of the product to convert this into a sum of log of probabilities. This can help preserve precision as well, but in this case we cannot use logarithms to solve the problem because there's sum in the denominator, But this kind of normalizes can be effective for solving this problem, so it's a technique that's sometimes useful in other situations as well. Now let's look at the M step. So from the E step we can see our estimate of which distribution is more likely to have generated a document, and you can see D1 is more likely from the first topic. Where is D2 is more like from the second topic, etc. Now let's think about what we need to compute in the M step. Basically we need to re estimate all the parameters. Let's first look at the P of Theta 1 and P of Theta 2. How do we estimate that? Intuitively, you can just pull together the Z probability Z probabilities from E Steps, right? So if all these documents say they're more likely from silouan, then we intuitively would give a high probability to see that one right? So in this case, so we can just take the average of these probabilities that you see here, and we obtain the .6 for Theta 1 so Theta 1 is more likely  than Theta 2. So you can see the probability of Theta 2 would be naturally .4. What about these world probabilities? What we do the same? And intuition is the same, so we're going to see in order to estimate the probabilities of words in Theta one, we're going to look at which documents have been generated from Scylla and we're going to pull together the words in those documents and normalize them. So this is basically what I just said. Most specifically, we're going to for example. Use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account. Instead, we can do that. Discount them by the probabilities that each document is likely be generated from Theta 1. So this gives us some fractional counts, and then these Council would be then normalized in order to get the probability. Now how do we normalize them? These probabilities of these words must sum to one. So to summarize, our discussion of generating models for clustering. We showed that a slight variation of Top Model can be used for clustering documents and this also shows the power of generating models in general by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. So in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. So here you can see the word distribution actually generates a term cluster as a byproduct. A document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. And then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster. And this probabilistic assignment that sometimes is useful for some applications. But if we want to achieve a harder clusters mainly to partition documents into disjoint clusters. Then we can just force the document into the cluster corresponding to the water distribution. That's most likely to have generated the document. We've also shown that the EM algorithm can be used with computer. The maximum lag or is made up an. In this case we need to use a special normalization technique to avoid underflow.
410	dda800a9-7b08-467f-b670-ebed5d4a639b	This lecture is about learning to rank. In this lecture we are going to continue talking about web search. In particular, we're going to talk about the using machine learning to combine different features to improve the ranking function. So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results. In the previous lectures we have talked about a number of ways to rank documents. We have talked about some retrieval models, like BM25 or query like code. They can generate the content based scores for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking. Now the question now is how can we combine all these features and potential many other features to do ranking and this will be very useful for ranking web pages. Not only just to improve accuracy, but also to improve the robustness of the ranking function so that it's not easy for a spammer to just perturb a one or a few features to promote the page. So the general idea of learning to rank is to use machine learning to combine these features to optimize the weights, different features to generate the optimal ranking function. So we will assume that the given a query document appear Q and D. We can define a number of features. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25, or query likelihood or pivot length normalization PL2 etc. It can also be linked based score like page rank score. It can be also application of retrieval models to the anchor text of the page, those are the text descriptions of links that point to this page. So these can all be clues about whether this document is relevant or not. We can even include a feature such as whether the URL has a tilde '~', because this might be indicator of homepage or engine page. So all these features can then be combined together to generate the ranking function. The question is of course, how can we combine them? In this approach we simply hypothesize that the probability that this document is relevant to this query is function of all these features. So we can hypothesize that the probability of relevance is related to these features through a particular form of the function that has some parameters. These parameters can control the influence of different features on the final relevance. This is of course just a assumption whether this assumption really makes sense is the big question and they have to empirically evaluate the function. But by hypothesising that relevance is related to these features in a particular way, we can then combine these features to generate the potentially more powerful ranking function and more robust ranking function. Naturally, the next question is how do we estimate those parameters and how do we know which features should have a higher weight than which features should have lower weight, so this is the task of training or learning, right? So in this approach, what we will do is to use some training data. Those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be ranked high for which queries. And this information can be based on real judgments by users, or this can also be approximated by just using click through information where we can assume the clicked documents are better than the skiped documents or clicked documents are relevant and skiped documents are non relevant. So in general, we would fit such a hypothesize the ranking function to the training data, meaning that we will try to optimize its retrieval accuracy on the training data, we can adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as MAP or nDCG. So the training data would look like a table of tuples. Each tuple has three elements. The query, the document and judgment. So it looks very much like our relevance judgments that we talked about in evaluation of retrieval systems.
410	de325d45-7bf3-451f-99c7-8949673e2f5e	this letter is about to some practical issues that you would have to address in evaluation of text retrieval systems in this lecture we will continue the discussion of evaluating will cover some practical issues that you have to solve in actual evaluation of text retrieval systems so in order to create the tester clashing we have to create a set of queries a set of documents and set of relevance judgments it turns out that each is actually challenging to create first the documents and queries must be representative they must represent the real queries anril documents that the users handle and we also have to use many queries and many documents in order to avoid a bias to conclusions for the matching of relevant documents with the queries we also need to ensure that there exists a lot of relevant documents for each query if a query has only one let's stay relevant document in the collection then you know it's not very informative to compare different methods using such a query 'cause there's not much room for us to see difference so ideally there should be more relevant documents in the collection but yet the query is also we should represent the real queries that we care about in terms of relevance judgments the challenge is to ensure complete judgments of all the documents for all the queries yet minimizing human fault be cause we have to use the human labor to label these documents it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries especially considering a giant data set micro the web so this is actually a major challenge it's a very difficult challenge for measures it's also challenging because we want to measure that would accurately reflect the perceived utility of users we have to consider carefully what the users care about and that design measures to measure that if you measure is not measuring the right thing then your conclusion would be misled so it's very important so we're going to talk about a couple of issues here one is the statistical significance test and this also is the reason why we have to use a lot of queries and the question here is how sure can you be that observed difference it doesn't simply result from the particular queries you choose so here are some sample results of average position for system and system be in two different experiments and you can see in the bottom we have mean average precision so the mean if you look at the meaning average precision the mean average precisions all exactly the same in both experiments so you can see this is point two this is point four four system V and again here it's also pointed to an point the fall so they are identical yet if you look at these exact average positions for different queries if you look at these numbers in detail you will realize that the in one case you would feel that you can trust the conclusion here given by the average in another case in the other case you will feel that well i'm not sure so why don't you take a look at all these numbers for a moment pause the video so if you look at the average the mean average precision we can easily say that well system B is better right so it's after all it's point forward and then this is twice as much as point two so that's a better performance but if you look at these two experiments i look at the detail results you will see that would be more confident that to say that in the case one in experiment one in this case be cause these numbers seem to be consistently better for system be whereas in experiment two we're not sure becaus looking at some results like this after a system a is better and this is another case system a is better but yet if we look at the only average system bees better so what do you think you know how reliable is our conclusion if we only look at the average now in this case intuitively we feel experiment one is more reliable but how can we quantitatively answer this question and this is why we need to do statistical significance test so the idea of statistical significance test is basically to assess the variance across these different queries if there is a a big variance that means the the results could fluctuate a lot according to different queries then we should believe that unless you have used a lot of queries the results might change if we will use another set of queries right so this is not so if you have C high variance then it's not very reliable so let's look at these results again in the second case so here we show two different ways to compare them one is assigned test where we just look at the sign if system B is better than system we have a plus sign when system is better we have a minus sign etc it is in this case if you see this well there are seven cases we have to have four cases where system B is better but three cases system is better you intuitively this is almost like a random results so if you just take a random sample flip seven coins and if you use plus to denote the head and minus your dinner detail and that could easy to be the results of just randomly flipping these seven coins so the fact that average is larger doesn't tell us anything and we can reliability conclude that and this can be quantitatively measured by P value and that basically means the probability that this result is in fact from random fluctuation in this case probabilities one it means it surely is random fluctuation now in wilcoxon test nonparametric test and we would be not only looking at the signs will be also looking at the magnitude of the difference but we can do a similar conclusion where you say well it's very likely to be from random so to illustrate this let's think about such a distribution and this is called allow distribution we assume that the mean is zero here let's say we start with something that there's no difference between the two systems but we assume that that be cause of random fluctuations depending on the queries or we might observe a difference so the actual difference might be on the left side here or on the right side here so this curve kind of shows the probability that we will actually observed values that are deviating from zero here now so if we look at this picture then we see that if a differences observed here then the chance is very high that this is in fact a random observation right we can define a reading of likely observation becaus of random fluctuation and this is ninety five percent of all the outcomes and in this info then the observed values may still be from random fluctuation but if you observe a value in this region or a difference on this side then the difference is unlikely from random fluctuation so there's a very small probability that you will observe such a difference just becaus of random fact eration so in that case we can then conclude the difference must be real so system B is indeed better so this is the idea of statistical significance test the takeaway message here is that you have to use many queries to avoid jumping into a conclusion as in this case to say system B is better there are many different ways of doing this statistical signaling task so let's talk about the other problem of making judgments and as we said earlier it's very hard to judge all the documents completely unless it's a very small data set so the question is if we can afford a judging all the documents in the collection which is subset should we judge and the solution here is pulling and this is a strategy that has been used in many cases to solve this problem so the idea of pulling is the following we would first choose a diverse set of ranking methods these are text retrieval systems and we hope these methods that can help us nominate likely relevant documents so the goal is to figure out the relevant documents we want to make judgments on relevant logins because those are the mostly useful documents from a user 's perspective so then we're going to have each to return top K documents the cake can vary from systems right but the point is to have asked them to suggest the most likely random in the documents and then we simply combine all these top K sets to form a poor of documents for humor assessors to judge so imagine you have many systems each will return K documents will take the top K documents and we formed the union now of course there are many documents that are duplicated 'cause mini systems might have retrieved the same random in the documents so there will be some duplicate documents there are also unique document that only returned by one system and so the idea of having diverse set of ranking methods is to ensure the PO is broad and can include as many possible relevant documents as possible and then the users would hugh masses would make completely judgments on this data set this poor and there are other unjudged documents are usually just assumed to be non relevant now for the poor is large enough this some machines are OK but if the poor is not very large in this actually has to be we considered and we might use other strategies to deal with them and there are indeed other methods to handle such cases and such a strategy is generally OK for comparing systems that contribute to the poor that means if you participate in contributing to the poor then it's unlikely that it will penalize your system because that operating with documents have all been judging however this is problematical for evaluating a new system that may have not contributed to the pool in this case a new system might be penalized because it might have nominated some relevant documents that have not been judged so those documents might be assumed to be non relevant that's unfair so to summarize the whole part of text retrieval evaluation it's extremely important be cause the problem is the empirical define the problem if we don't rely on users there's no way to tell whether method works better if we have in the property experiment design we might miss guide our research or applications and we might just do our own conclusions and we have seen this in some of our discussion so make sure you get it right for your research or application the main methodology is cream filled the virus and methodology and this is near the main paradigm used in all kinds of empirical evaluation task is not just a search engine marriage map an end DCG are the two main measures that should definitely know about and they are a property for comparing ranking algorithms you will see them often in research papers proceeding at the ten documents is easier to interpret from the users perspective so that's also often useful what's not covered is some other evaluation strategy like AB test where the system would mix to the results of two methods randomly and then will show the mixer results to users of course the users don't see which resulted from which method the users would charge those results or click on those documents in in a search engine application in this case then the search engine can keep track of the click the document and see if one method has contributed more through the click the documents if the user tends to click on one the results from one method then it suggests that that method may may be better so this is the leverage is the real users of a search engine to do evaluation it's called AB test and it's a strategy that's often used by modding search engines the commercial search engines another way to evaluate IR or text retrieval is user studies and we haven't covered that i've put some references here that you can look at if you want to know more about that so there are three additional readings here these are three many books about evaluation and they're all excellent in covering a broader review of information retrieval evaluation and discovered some of the things that we discussed but they also have a lot of others to offer
410	de9f09fc-7cf7-40cf-9073-816e9e1a5300	This lecture is about the text retrieval problem. This picture shows our overall plan for lectures. In the last lecture we talked about the high level strategies for text access. We talked about push versus pull. Search engines are the main tools for supporting the pull mode. Starting from this lecture, we're going to talk about how the search engines work in detail. So first it's about the text retrieval problem. We are going to talk about three things in this lecture. First, we'll define text retrieval. Second, we're going to make a comparison between text retrieval and the related task, database retrieval. Finally, we're going to talk about the document selecting versus document ranking as two strategies for responding to a users query. So what is text retrieval? It should be a task that's familiar to most of us because we're using web search engines all the time. So text retrieval is basically a task where the system would respond to a user's query with relevant documents. Basically, to support the query. As one way to implement the pull mode of information access. So the scenario is the following you have a collection of text documents. These documents could be all the web pages on the web. Or all the literature articles in the digital library. Or maybe all the text files in your computer. A user will typically give a query to the system to express information need and then the system would return relevant documents to users. Relevant documents refer to those documents that are useful to the user who is in typing the query. Now this task is often called information retrieval. But literally, information retrieval would broadly include retrieval of other non textual information as well. For example audio, video etc. It's worth noting that text retrieval is at the core of information retrieval, in the sense that other medias, such as video, can be retrieved by exploiting the companion text data. So for example. Current image search engines. Actually match the users query with the companion text data of the image. This problem is also called the search problem. And the technology is often called search technology in industry. If you have to take a course in databases. It will be useful to pause the lecture at this point. And think about. The differences between text retrieval and database retrieval. Are these two tasks are similar in many ways? But there are some important differences. So spend a moment to think about the differences between the two. Think about the data and information managed by search engine versus those that are managed by a database system. Think about the difference between the queries that you typically specify for a database system. Versus the queries that are typed in by users on the search engine. And then finally, think about the answers. What's the difference between the two? OK, so if we think about the information or data managed by the two systems, we will see that. In text retrieval, the data is unstructured free text, but in databases. They are structured data where there is a clear definer schema to tell you. This column is the names of people in that column is ages, etc. In unstructured text it's not obvious what are the names of people mentioned in the text. Because of this difference, we can also see that text information tends to be more ambiguous, and we talked about that in the natural language processing lecture. Whereas in databases that data tended to have well defined the semantics. There is also important difference in the queries. And this is partly due to the difference in the information. Or data. So text queries tend to be ambiguous. Where as in database search. The queries are typically well defined. Think about the SQL query that would clearly specify what records to be returned so it has very well defined semantics. Keyword queries or natural language queries tend to be incomplete also. In that it doesn't really fully specify what document should be retrieved. Where as in the database search. The SQL query can be regarded as a computer specification for what should be returned. And because of these differences, the answers will be also different. In the case of texy retrieval, we're looking for relevant documents. In the database search we are retrieving records or match records with. The SQL query more precisely. Now, in the case of tax retrieval, what should be the right answers to a query is not very well specified as we just discussed. So it's unclear what should be the right answers to a query, and this has very important consequences and that is text retrieval is an empirically defined problem. So this is. A problem because. If it's empirically defined. Then we cannot mathematically prove one method is better than another method. That also means we must rely on empirical evaluation involving users. To know which method works better. And that's why we have a lecture, actually more than one lectures to cover the issue of evaluation. Because this is a very important topic for search engines. Without knowing how to evaluate the algorithm appropriately, there's no way to tell whether we have got a better algorithm or whether one system is better than another. So now let's look at the problem in a formal way. So this slide shows a formal formulation of the text retrieval problem. First, we have our vocabulary set. Which is just a set of words in a language. Now here. We are considering just one language, but in reality on the web there might be multiple natural languages. We have text data in all kinds of languages. But here for simplicity, we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language. Although there is important difference, the principles and methods are. very similar. Next we have the query, which is a sequence of words. And so here. You can see. The. Query. Is defined as a sequence of words. Each q sub I is a word in the vocabulary. A document is defined in the same way, so it's also a sequence of words and here d sub ij. is also a word in the vocabulary. Now typically the documents are much longer than queries. But there are also cases where. The documents may be very short. So you can think about what might be an example of that case. I hope you can think of Twitter search right tweets are very short. But in general, documents are longer than the queries. Now then we have a collection of documents. And this collection can be very large, so think about the web. It could could be very large. And then the goal of text retrieval is to find the set of relevant documents which we denoted by R of Q because it depends on the query and this is in general a subset of all the documents in the collection. Unfortunately, this set of relevant documents is generally unknown. And user dependent in the sense that for the same query typed in by different users. The expected relevant documents may be different. The query given to us by the user is only a hint on which document should be in this set. And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search. Where collection is so large the user doesn't have complete knowledge about the whole collection. So the best a search system can do. Is to compute an approximation of this relevant document. set so we denote by r prime of Q. So formally, we can see the task is to compute this r prime of Q, an approximation of the relevant documents. So how can we do that? Imagine if you are now asked to write a program to do this. What would you do now? Think for a moment. Right, so these are your input. the query the documents. And then you are to compute the answers to this query. Which is a set of documents that would be useful to the user. So how would you solve the problem? Now, in general. There are two strategies that we can use. The first strategies will do document selection and that is we're going to have a binary classification function or binary classifier. That's a function that would take a document and query as input. And then give a zero or one as output to indicate whether this document is relevant to the query or not. So in this case you can see the document. The relevant document in the set is defined as follows, it basically. All the documents that. Have a value of 1 by this function. And so in this case you can see the system must decide if a document is relevant or not. Basically it has to say whether it's one or zero. And this is called absolute relevance. Basically it needs to know exactly whether it's going to be useful to the user. Alternatively, there's another strategy called document ranking. Now, in this case the system is not going to make a call whether a document is relevant or not, but rather the system is going to use the real value function F here. That would simply give us a value that would indicate which document is more likely relevant. So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant. So this function then can be used to rank the documents. And then we're going to let the user decide where to stop when the user looks at the documents. So we have a threshold. See down here. To determine what documents should be in this approximation, set. And we can assume that all the documents that are ranked above this threshold are in this set. Be cause in effect, these are the documents that we deliver to the user. And theta is a cut off determined by the user. So here we've got some collaboration from the user in some sense, because we don't really make a cut off and the user kind of helped the system make a cut off. So in this case the system only needs to decide if one document is more likely relevant than another, and that is it only needs to determine relative relevance. As opposed to absolute relevance. Now you can probably already sense that. Relevant relative relevance would be easier to determine their absolute relevance because in the first case, we have to say exactly whether a document is relevant or not. And it turns out that ranking is indeed generally preferred to document selection. So let's look at this. These two strategies in more detail. So this picture shows how it works. So on the left side we see these documents and we use the pluses. To indicate the relevant documents so we can see the true relevant documents here. This is. This set of two random documents consist of these pluses these documents. And with the document selection. Functioning we are going to do basically classify them into two groups, relevant documents and non relevant ones. Of course the classifier will not be perfect so it will make mistakes. So here we can see in the approximation of the relevant documents we have got some non relevant documents. And similarly there is a relevant document that's miss classified as non relevant. In the case of document ranking, we can see the system seems like simply ranks all the documents in the descending order of the scores. And then we're going to let the users stop wherever the user wants to stop. So if a user wants to examine more documents, then the user would go down the list to examine more and stop at the lower position. But if the user only wants to read a few relevant documents, the user might stop at the top position. So in this case the user stops at d4  so in effect we have delivered these four documents. To our user. So as I said, ranking is generally preferred. An one of the reasons is because the classifier. In the case of document selection is unlikely accurate. Why? Because the only clue is usually the query, but query may not be accurate in the sense that it could be overly constrained. For example, you might expect the relevant documents to talk about all these. Topics you by using specific vocabulary and as a result. You might. Match no random documents because in the collection no others have discussed the topic using these vocabularies. So in this case you will see there is this problem of. No relevant documents to return in the case of overly constraint query. On the other hand, if the query is underconstrained, for example. If the query does not have sufficient discriminating words to find the relevant documents, you may actually end up having over delivery. And this is when you thought these words might be sufficient to help you find the relevant documents. But it turns out that they are not sufficient, and there are many distraction documents using similar words. Right so. This is the case of over delivery. Unfortunately, it's very hard to find the right position between these two extremes. Why cause when the user is looking for the information? In general, the user does not have a good knowledge about the information to be found. And in that case the user does not have a good knowledge about what. Vocabularies will be used in those random documents. So it's very hard for user to pre specify the right level of constraints. Even if the classifier is accurate. We also still want to rank these red documents because. They are generally not equally relevant. Relevance is often a matter of degree. So we must prioritize. These documents for a user to examine. And this note that this prioritization is very important. Because a user cannot digest all the contents at once, the user general would have to look at each document sequentially. And therefore it would make sense to feed the users with the most relevant documents. And that's what ranking is doing. So For these reasons, ranking is generally preferred now. This preference also has a theoretical justification, and this is given by the probability ranking principle. In the end of this lecture there is reference for this. This principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions. First, the utility of the document to a user is independent of the utility of any other document. 2nd, a user would be assumed to browse the results sequentially. Now it's easy to understand why these two assumptions are needed in order to justify for the ranking strategy. because if the documents are independent then we can evaluate the utility of each document separately. And this would allow us to compute the score for each document independently, and then we're going to rank these documents based on those scores. The second assumption is to say that the user would indeed follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially, then obviously the ordering would not be optimal. So under these two assumptions. We can theoretically justify the rankings strategy is in fact the best you could do. Now I've put one question here. Do these two assumptions hold? Now I suggest you to pause the lecture for a moment to think about this. Now, can you think of some examples? That would suggest. These Assumptions aren't necessarily true. Now, if you think for a moment you may realize. None of the assumptions is actually true. For example, in the case of independence assumption. We might have identical documents that have similar content or exactly the same content. If you look at each of them alone. Each is relevant. But if the user has already seen one of them. We assume it's generally not very useful for the user to see another similar or duplicated one. So clearly the utility of document is dependent on other documents that user has seen. In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together, they provide answer to the users question. So this is a collective relevance and that also suggests that the value of the document might depend on other documents. Sequential browse in general would make sense if you have a ranked list there. But even if you have a ranked list. There was evidence showing that users don't always just go strictly sequentially through the entire list. There sometimes would look at the bottom, for example, or skip some. And if you think about the more complicated interface that we could possibly use, like 2 dimensional interface where you can put the additional information on the screen, then seek when you're browsing is a very restrictive assumption. So the point here is that. None of these assumptions is really true. But nevertheless. The probability ranking principle established some solid foundation for ranking as a primary task for search engines. This has actually been the basis for a lot of research work in information retrieval, and many algorithms have been designed based on this assumption Despite that. The assumptions aren't necessarilly true and we can address this problem by doing post processing of a ranked list, for example to remove redundancy. So to summarize this lecture, the main points. That you can take away are the following. First text retrieval is an empirical defined problem. And that means which algorithm is better must be judged by the users. Second, document ranking is generally preferred. And this will help users prioritize examination of search results. And this is also to bypass the difficulty in determining absolute relevance. Because we can get some help from users in determining where to make the cut off. It's more flexible. So this further suggests that the main technical challenge in designing search engine is redesigned effective ranking function. In other words, we need to define what is the value of this function F on. The query and document pair. The whole design. Such a function is the main topic in the following lectures. There are two suggested additional readings. The first is the classic paper on probability ranking principle. The second is a must read for anyone doing research information travel. It's a classic IR book. Which has excellent coverage of the main research results in early days. Up to the time when the book was written, Chapter 6 of this book has in depth discussion of the probability ranking principle and probabilistic retrieval models in general.
410	e6c92e3a-0169-4af1-96d7-3fe4a5303e3a	This lecture is a continued discussion of probabilistic topic models. In this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. So in this simple setup we are interested in analyzing one document and trying to discover just one topic. So this is the simplest case of topic modeling. The input now no longer has K, which is the number of topics because we know there is only one topic. And the collection has only one document also. In the output we also no longer have coverage because we assumed that the document covers this topic 100%. So the main goal is just to discover the word probabilities for this single topic, as shown here. As always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the data or data representation. And then we're going to design a specific model for the generation of the data from our perspective. Where our perspective just means we want to take a particular angle of looking at the data so that the model would have the right parameters for discovering the knowledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. And the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. Which means we'll take the estimated parameters as a knowledge that we discover from the text. So let's look at these steps for this very simple case. Later, we'll look at this procedure for some more complicated cases. So our data in this case is just the document which is a sequence of words. Each word here is denoted by X sub I. Our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. So we will have as many parameters as many words in our vocabulary, in this case M. And for convenience we're going to use theta sub I to denote the probability of word W sub I. And obviously these thetas of i's would sum to one. Now, what does the likelihood function look like? This is just the probability of generating this whole document given such a model. Because we assume the independence in generating each word, so the probability of the word the document would be just a product of the probability of each word. And since some word might have repeated occurrences, so we can also rewrite this product in a different form. So in this line we have rewritten the formula into a product over all the unique words in the vocabulary, W sub one through the W sub M. Now this is different from the previous line where the product is over different positions of words in the document. Now when we do this transformation, we then would need to introduce a count function here. This denotes the count of word one in document. And similarly, this is the count of words of M in the document. Because these words might have repeated occurrences. You can also see if a word did not occur in the document, it would have a zero count and therefore that corresponding term will disappear. So this is a very useful form of writing down the likelihood function that we will often use later. So I want you to pay attention to this. Just get familiar with this notation. It's just to change the product over all the different words in the vocabulary. So in the end, of course we'll use theta sub I to express this likelihood function and it would look like this. Next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. So now let's take a look at the maximum likelihood estimate problem more closely. This line is copied from the previous slide. It's just our likelihood function. So our goal is to maximize this likelihood function. We will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, because after the logarithm transformation, our function will become a sum instead of a product. And we also have constraints over these probabilities. The sum makes it easier to take derivative, which is often needed for finding the optimal solution of this function. So please take a look at this sum again here and this is a form of function that you often see later also in more general topic models. So it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document. And this is multiplied by the logarithm of the probability. So let's see how we can solve this problem. Now at this point the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. The objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. So one way to solve the problem is to use Lagrange multiplier approach. Now this content is beyond the scope of this course. But since Lagrange multiplier is very useful approach, I also would like to just give a brief introduction to this for those of you who are interested. So in this approach we will construct a Lagrange function here. And this function would combine our objective function with another term that encodes our constraints. And we introduce Lagrange multiplier here, Lambda. So it's additional parameter. Now the idea of this approach is to just turn the constrained optimization into, in some sense, unconstrained optimizing problem. So now we're just interested in optimizing this Lagrange function. As you may recall from calculus, an optimal point would be achieved when the derivative is set to 0. This is a necessary condition. It's not sufficient though, so. If we do that, you will see the partial derivative with respect to theta i here is equal to this. And this part comes from the derivative of the logarithm function. And this Lambda is simply taken from here. And when we set it to zero, we can easily see theta sub i is related to Lambda in this way. Since we know all the theta I's must sum to one, we can plug this into this constraint here, and this will allow us to solve for Lambda. And this is just negative sum of all the counts and this further allows us to then solve optimization problem. Eventually to find the optimal setting for Theta Sub I. And if you look at this formula, it turns out that it's actually very intuitive because this is just the normalized count of these words by the document length, which is also a sum of all the counts of words in the document. So after all this math, after all, we have just obtained something that's very intuitive, and this will be just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here. And you might also notice that this is the general result of maximum likelihood estimator. In general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. So this is basically an analytical solution to our optimization problem. In general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. Instead, we have to use some numerical algorithms, and we're going to see such cases later also. So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document D here, let's imagine this document is a text mining paper. Now what you might see is something that looks like this. On the top you will see the high probability words tend to be those very common words, often functional words in English, and this will be followed by some content words that really characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be externally mentioned in the document. As a topic representation, you will see this is not ideal, right? The because of the high probability words are functional words they are not really characterizing the topic. So one question is how can we get rid of such common words? "Now this is a topic of the next lecture. We're going to talk about how to use probabilistic models to somehow get rid of these common words.
410	ec28a239-f43c-4fb5-bda0-920395666f51	this letter is about inverted index construction in this lecture we will continue the discussion of the system implementation in particular we're going to discuss how to construct the inverted index the construction of the inverted index is actually very easy if the data set is very small it's very easy to construct dictionary and then install the postings in a fire the problem is that when our data is not able to fit to the memory then we have to use some special method which will deal with it an unfortunately in most retrieval applications that are set would be large and they generate cannot be loaded into memory at once and there are many approaches to solving the problem and sorting based method is quite common it works in four steps assume here first you collect the local term ID document ID and frequency tables basically you will look counterterms in a small set of documents and then once you crack those counts you have sought those counts based on terms so that you build a local a partial inverted index and these are called rounds and then you write them into temporary file on the disk and then you merge in step three you would do pairwise merging of these runs until you eventually merge all the runs we generate a single inverted index so this is an illustration of this method on the left you see some documents and on the right we have shown a complex him and a document ID lacks these lessons are too map a stream based repetitions of document IDS or terms into integer representations or adam that back from integers to the screen rotation and the reason why we are interested in using integers represent these i DS is becaus integers are often easier to handle for example integers can be used as index for array and there are also easy to compress so this is one reason why we tended to map these strings into integers so that so that we don't have to carry these strings around so how does this approach work well it's very simple look at the scan these documents sequentially and then pause the document an account of the frequencies of terms an in this stage we generally sort the frequencies by document IDS becaus we process each document sequentially so we first encounter all the terms in the first document therefore the document i DS one 's in this case and so and this would be followed by document IDS tool tools an their natural resource in this order just becaus we process the data in the sequential order at some point we will run out of memory and then we would have to write them into the disk before we do that we're going to sort them just to use whatever memory we have we can sort them and then this time we're going to sort based on term items noted out here we're using this term i DS as a peter thought so all the entries that share the same term would be grouped together in this case we can see all the or the IDS of document that match tom one with the groups together and we're going to write this into the disk as a temporary file and that would allow them to use the memory the plus as an extra batch of documents and we're going to do that for all the documents so we're going to write a lot of temporary files into the disk and then the next stages blue merge assault basically window merge them and then sort them eventually we will get single inverted index whether their entries are sorted based on term IDS and on the top we can see these are the order entries for the documents that match term ID one so this is basically how we can do the construction of inverted index even though the data cannot be or loaded into the memory now we mentioned earlier that the cost of postings are very large it's desirable to compress them so let's now talk a little bit about how we compress inverted index well the idea of compression in general is the leverage skewer distributions of values and we generally have to use variable length encoding instead of the fixed lens encoding as we use in by default in a programming language like C plus plus and so how can we leverage the skewed distributions of values to compress these values well in general would use fewer bits to encode those frequent awards at the cost of using longer bits to encode those rail values so in our case let's think about how we can compress the TF term frequency if you can picture what the inverted index would look like muc in postings there are a lot of confidence is those are the frequencies of terms in all those documents now we if you think about what kind of values are most frequently there your program will be able to guess that a small numbers tend to occur far more frequently than large numbers why well think about the distribution of words and this isn't due to the zipf 's law and many words occur just rarely so we see a lot of small numbers therefore we can use a few orbits for the small but highly frequent integers and at the cost of using more bits for large integers this is a trade off of course if the values are distributed in uniform then this one will save us any space but be cause we tend to see many small values they are very frequent we can save on average even though sometimes when we see a large number we have to use a lot of bits what about the document i DS that we also saw in postings well they are not distributed in the skilled away right so how can we deal with that where it turns out that really use a trick called the gap and that is restore the difference of these term IDS and we can imagine if a term has matched the many documents then there will be a long list of document IDS so when we take the gap i'm going to take the difference between adjacent document IDS those gaps will be small so again see a lot of small numbers whereas if atoma cutting only a few documents then the gap would be large the large numbers will not be frequent so this creates some security distribution that would allow us to compress this values and this is also possible becaus in order to uncover or uncompressed these document IDS we have the sequential process that data because we store the difference an in order to recover the exactly document ID we have to first recover the previous document ID and then we can add the difference to the previous document ID to restore the current document ID now this was possible becaus we only need to have sequential access to those document IDS once we look up a term we fetch all the document i DS that match the term then we sequentially process so it's very natural that's why this trick actually works and there are many different methods for encoding so binary cold is a common user called in just programming language where we use basically fixed lens encoding unary called gamma gold and yellow gold are awful spinedace and there are many other possible in this so let's look at some of them in more detail binary coding is really equal lansing coding that's a property for randomly distributed values the unary coding is variable lansing calling method in this case integer that's at least one would be encoded as X minus one one bit followed by zero so for example three would be encoded as two ones followed by zero whereas five will be encoded as four ones followed by zero etc so now you can imagine how many bits do we have to use for large number like one hundred so how many bits do have to use exactly four number like one hundred well exactly we have to use one hundred bits right so it's the same number of bits as the value of this number so this is very inefficient if you will likely see some large numbers imagine if you occasionally see a number like one thousand you have to use one thousand bits so this only works well if you are absolutely sure that there would be no large numbers mostly very very often you see very small numbers how do you decode it this code since these are variable length encoding methods and you can't just count how many bits and then they just stop why are you going to say eight bits or thirty two bit then you will start another code they are variable length so you have to rely on some mechanism in this case for unary you can see it's very easy to see the boundary now you can easily see zero with signal the end of encoding so you just count how many ones you have seen and here you hit zero you know you have finished one number you will start another number now we just saw that the universe coding is too aggressive in rewarding small numbers if you occasionally you can see very big number it will be a disaster so what about some other less aggressive method well demarco ding is one of them an in this method will can do use unary coding for transform form of the value so it's one plus the flow of log of X so the magnitude of this value is much lower than the original X so that's why we can't afford using unary code for that so and so first we have the unary code for coding this log of X and this will be followed by a uniform code or binary code and this is basically the same uniform code and binary code of the same and we're going to use this code to code the remaining part of the value of X and this is basically precisely X minus one two to the floor of log of X so the unary called basically called the flow of log of X well add one there i can hear but the remaining part will be using uniform code to actually code difference between the X and this two to the log of X and it's easy to show that for this this value this difference we only need to use up to this many bits and the floor of log of X bits an this is easy to understand if the difference is too large then we would have a higher flow of log of X so here are some examples for example three is encoded as one oh one the first two digits are the unary coder so this is before value two like one zero in codes two enumerate coding and so that means log of X the flow of log of X is one because we want actually use unary coded to encode one plus the flow of log of X since this is two then we know that the flow of log of X is actually one so but three is still larger than to do the one so the difference is one and that one is encoded here at the end so that's why we have one oh one four three of similarly at five the encoded as one one zero followed by zero one an in this case is a unary code encodes three and so this is the unary code one one zero and so the floor of log of X is two and that means we're going to compute the difference between five and two to the two and that's one and so we now have again one at the end but this time we're going to use two bits becaus with this level flow of log of X we could have more numbers five six seven they would all share the same prefix here one one zero so in order to differentiate them we have to use two bits in the end to differentiate them so you can imagine six would be one zero here in the end instead of zero one after one one zero it's also true that the form of a gamma code is always and the first odd number of bits and in the center there is a zero that's the end of the unary code and before that all on the left side of this zero there would be all once an on the right side of this zero it's binary coding or uniform code so how can you decode such a code well you again first do you know recording right once you hit zero you know you have got the unary code and this also would tell you how many bits you have to read further to decode the uniform code so this is how you can decode it there michael there is also terrible code that's basically the same as a gamecode except that you replace the unary prefix with the gamma code so that's even less conservative than comma code in terms of rewarding small integers so that means it's OK if you occasionally see a large number it's OK with the error code it's also fine with gamecode it's really a big loss for unary code and they are all operating of course at different degrees of favoring short faring small integers and that also means there would be a property for asserting distribution but none of them is perfect for all distributions and which method works the best would have to depend on the actual distribution in your data set for inverted index compression people have found that gamma coding seems to work well so how do i uncompressed invert index and we just talked about this first day would decode those encode integers an we just i think discussed how it is called the union recording and camera coding so i won't repeat what about the document i DS that might be compressed using the gap well we're going to do sequential decoding so suppose the encoded idealist is X one X two X three etc we first the codex one obtain the first document id id one then with the decode X two which is actually the difference between the second ID and the first one so we have to add the decoder value of X two two ID one to recover the value of the the ID at this second position so this is all well you can see the advantage of converting document IDS into integers and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position
410	eca11e7a-dae2-480b-9c0e-e1ac7a91dd57	This lecture is the overview of text retrieval methods. In the previous lecture we introduced the problem of text retrieval. We explained that the main problem is to design a ranking function to rank documents for a query in this lecture we will give a overview of different ways of designing this ranking function. So the problem is the following. We have a query that has a sequence of words and a document that's also a sequence of words and we hope to define a function F. That can compute a score based on the query and document. So the main challenge here is to design a good ranking function that can rank all the relevant documents on top of all the non relevant ones. Now clearly this means our function must be able to measure the likelihood that a document d, is relevant to a query Q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model which gives us a formalization of relevance. Now, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories. 1st. One thing many of the models are based on the similarity idea. Basically, we assume that if a document is more similar to the query, then another document is then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vectors space model, which we will cover more in detail later in the lecture. The second kind of models are called probabilistic models. In this family of models we follow a very different strategy, where we assume that. Queries and documents are all observations from random variables. And we assume there is a Binary Random variable called R here. To indicate whether a document is relevant to a query. We then define the score of document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query. There are different cases of such a general idea. One is classic probabilistic model. Another is language model, yet another is divergent from randomness model. In the later lecture we will talk more about one case which is language model. The third kind of models are based on probabilistic influence, so here the idea is to associate uncertainity to inference rules, and we can then quantify the probability that we can show that the query follows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here the idea is to define a set of constraints that we hope. A good retrieval function. To satisfy. So in this case, the problem is to seek A good ranking function that can satisfy all the desired constraints. Interestingly, although these different models are based on different thinking. In the end. The retrieval function. Tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the retrieval model. And to examine some of the common ideas used in all these models. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation. Used in all the search engines. So with this assumption, the score of a query, presidential campaign news. With respect to a document d here would be based on scores computed based on each individual word. And that means the score would depend on the score of each word. Such as presidential campaign and news. Here we can see. There are three different components, each corresponding to how well the document matches each of the query words. Inside these functions. We see a number of heuristics used. So for example, one factor that affects the. Function G Here is how many times does the world presidential occur in the document. This is called a term frequency or TF. We might also denote as c of presidential and d. In general, if. The word occurs more frequently in the document then the value of this function would be larger. Another factor is how long is the document. And This is to use the document length for scoring. In general. If a term occurs in a long document that many times. It's not as significant as. If it occurred the same number of times in a short document, because in a long document. Any term is expected to occur more frequently. Finally, there is this factor called the document frequency. That is, we also want to look at the how often presidential occurs in the entire collection. And we call this document frequency or DF of presidential. And in some other models we might also. Use a probability. To characterize this information. So here is show the probability of presidential in the collection. So all these are trying to characterize the popularity of the term in the collection in general, matching a rare term in the collection. Is contributing more to the overall score than matching a common term. So this captures some of the main ideas used in pretty much all the state of art retrieval models. So the natural question is which model works the best? Now it turns out that many models work equally well, so here are a list of the four major models that are generally regarded as a state of the art retrieval models. Pivoted length normalization. BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines and you will also often see this method discussed in research papers. And we'll talk more about this method later in. Some other letures. So to summarize. The main points made in this lecture are first the design of a good ranking function. Pre requires a computational definition of relevance and we achieve this goal by designing appropriate retrieval model. Second, many models are equally effective, but we don't have a single winner yet. Researchers are still actively working on this problem. Trying to find a truly optimal retrieval model. Finally, the state of art ranking functions tend to rely on the following ideas. First bag of words representation. 2nd. TF and document frequency of words, such information is used in. the weighting function to determine the overall contribution of matching a word. And document lengths. These are often combined in interesting ways and we'll discuss how exactly they are combined to rank documents in the lectures later. There are two suggested the additional readings. If you have time. The first is a paper where you can find a detailed discussion and comparison of multiple state of the art models. The second is a book with a chapter that gives a broad review of different retrieval models.
410	ee41ce3a-6c8d-4c3b-90fd-79f3e9190402	This lecture is a continued discussion of latent aspect rating analysis. Earlier we talked about how to solve the problem of Lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights. Now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. So given an entity, we can assume there are aspects that are described by word distributions. Topics and then we can use a topic model to model the generation of the review text. Our assumed the words in the review text are drawn from these distributions. In the same way as we assumed for a generative model like PSA. And then we can then plug in the latent regression model to use the text to further predict the Overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. So this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. So we don't have time to discuss this model in detail, as in many other cases in this part of the course where we discuss the cutting edge topics. But there is a reference site here where you can find more details. So now I'm going to show you some simple results that you can get by using this kind of generative models. First it's about rating decomposition. So here what you see are the decomposed ratings for three hotels that have the same overall rating. So if you just look at the overall rating you don't. You can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. Dimensions like value, but others might score better in other dimensions like location and so this can reveal detailed opinions at the aspect level. Now here, the ground truth is shown in the plans, so this also allows you to see whether the prediction is accurate. It's not always accurate, but it's mostly still reflecting some of the trends. The 2nd result is to compare different reviewers on the same hotel so the table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same. So if you just look at the overall ratings, you don't really get that much information about the difference between the two reviews. But after you decompose the ratings you can see clearly they have high scores on different dimensions. So this shows that the model can reveal differences in. Opinions of different reviewers and such a detailed understanding can help us understand better about reviews and also better about their feedback on the hotel. This is something very interesting because this is in some sense some byproduct in our problem formulation. We did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. And you can see the highly weighted words versus the negatively lower weighted words here for each of the four dimensions. Value, rooms, location and cleanliness. I added the top words, cleared it, makes sense, and the bottom words also makes sense. So this shows that with this apology, we can also learn sentiment information directly from the data. Now this kind of laxing is very useful becausw in general a word like long, let's say, may have different the sentiment polarities for different context. So if I say the battery life of this laptop is long, then that's positive. But if I say the rebooting time for the laptop is long, that's bad, right? So even for reviews about the same product laptop, the word long Is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about the hotels in social media like tweets. And, what's also interesting that since this is an almost computer and supervised, assuming that the reviews with overall ratings are available, and then this can allow us to learn from potentially a large amount of data on the Internet to reach sentiment lexicon. And here are some results to validate the preference weights. Remember, the model can infer whether a reviewer cares more about service or the price. Now, how do we know whether the inferred weights are correct and this poses a very difficult challenge for evaluation. Now here we show some interesting way of evaluating result. what you here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviews. The top ten other reviewers with the highest inferred value to other aspect ratio. So for example, value versus location value versus room etc. But the top ten are the reviewers that have the highest ratios by this measure. And that means these reviewers tend to put a lot of weight on value as compared with other dimensions. That means they really emphasize on value. The bottom ten, on the other hand, are the reviews that have the lowest ratio. What does that mean? Well, that means these reviewers have put higher weights on other aspects than value, so those are people that care about the another dimension and they didn't care so much about the value in some sense by this, less compared with the top ten group. Now these ratios are computer based on the inferred weights from the model. So now you can see the average prices of hotels are favored by toptenreviews are indeed and much cheaper than those that are favored by the bottom 10. And this provides some Indirect way of validating the infer wait. It just means the weights are not random and they are actually meaningful here and in comparison with the average price in these three cities, you can actually the top ten tends to have below average price, whereas the bottom time where they care a lot about other things like service or room condition tend to have hotels that have higher prices than average. So with these results we can build a lot of interesting applications. For example, direct application would be the generator rated aspect, the summary. An because of the decomposition, we can now generate the summaries for each aspect. The positive sentence is negative sentences about each aspect. It's more informative than original review that has just overall rating and review test. Here are also mother results about the aspects discovered from reviews with low ratings. These are MP3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects. Or they comment more on different aspects. So that can help us discover, for example, consumers trained in appreciating different features of product. For example, one might have discovered the trend that people tend to like large screens of cell phones or lightweight of laptop etc. And such knowledge can be useful for manufacturers to design their next generation of products. Here are some interesting results on analyzing users rating behavior. So what you see is average weights on different dimensions by different groups of reviewers. And on the left side you see the weights of reviews like the expensive hotels they give. The whole expensive hotels five stars and you can see their average weights tend to be more focused on service and that suggests that people might be expensive hotels because of good service. And that's not surprising as also another way to validate the inferred weights. But if you look at the right side where look at the column of five stars, these are the reviewers that like the cheaper hotels and they give cheaper hotels, five stars as we expected, and they put more weight on value and that's why they like the cheaper hotels. But if you look at the when they didn't like expensive hotels or cheaper hotels and you seal it tended to have more weights on the condition of the room cleanliness. So this shows that by using this model we can infer some information that's very hard to obtain, even if you read all the reviews. Even if you read all the reviews, it's very hard to infer such preferences or such emphasis. So this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. You can compare different hotels, compare the opinions from different consumer groups in different locations, and of course the model is general. It can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. Finally, there is also some result on applying this model for personalized ranking or recommendation of entities. So because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about. So, for example, if a query here that shows 90% of the way it should be on value and 10% on others. So that just means I don't care about other aspects, I just care about getting a cheap hotel. My emphasis is on the value dimension. Now what we can do is such a query is that we can use reviewers that we believe have a similar preference to recommend the hotels for you. How can we know that we can infer the weights of those reviewers on different aspects? We can find the reviewers whose weights or more precise whose inferred weights or similar to yours and then use those reviews to recommend the hotels for you. And this is what we call a personalized or rather query specific recommendation. The non personalized recommendation results are shown on the top. An you can see the top results generally have much higher price than the low Group, and that's because when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels. So this is yet another application of this technique. And shows that by doing text mining we can understand the users better. And once we can end users better, we can serve these users better. So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. And as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. And we also need to consider the order of those categories and we talk about the ordinal regression. For solving this problem. We have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of words in the model. As a result, we can learn those useful information when fitting the model to the data. Most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. So here are some suggested readings, the first 2. are small books that are excellent reviews of this topic where you can find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem. The next two papers are about the generative models for letting the aspect rating analysis. The first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model. To solve the problem using a unified model.
410	f1951cf2-4293-450b-8578-4d74c72f9862	This lecture is about opinion mining and sentiment analysis covering its motivation. In this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. In particular, we're going to talk about the opinion mining and sentiment analysis. As we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. In contrast, we have other devices such as video recorder that can report what's happening in the real world objectively to generate the video data, for example. Now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans. Now this is actually unique advantage of text data, as compared with other data because it offers us a great opportunity to understand the observers. We can mine the text data to understand the opinions, understand the people's preferences, how people think about something. So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data. So let's start with the concept of opinion that it's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something. Now I highlighted a quite a few words here, and that's because it was thinking a little more about these words and that would help us better understand what's in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining. So let's first look at the keyword subjective here. Now this is in contrast with objective statement or factual statement. Those statements can be proved right or wrong. And this is a key differentiating factor from opinion, which tends to be not easy to prove wrong or right because it reflects what a person thinks about something. So in contrast, objective statement can usually be proved wrong or correct. For example, you might say this computer has a screen and a battery. Now that's something you can check. It's either having a battery or not. But in contrast, if you think about the sentence such as this laptop has the best battery. Or this laptop has a nice screen, now these statements are more subjective and it's very hard to prove whether it's wrong or correct. So opinion is subjective statement. And next, let's look at the keyword person here and that indicates this opinion Holder 'cause when we talk about opinion, it's about the opinion held by someone and then we notice that there is something here. So that's the target of the opinion. The opinions is expressed on this something. And now, of course, believes or thinks implies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context. People from different background may also think in different ways. So this analysis shows that there are multiple elements that we need to include in order to characterize an opinion. So what's the basic opinion representation like, well,  it should include at least three measurements, right? First it has to specify what's the opinion Holder. So whose opinion is this, second must also specify the target. What's this opinion about? And 3rd, of course we want opinion content and So what exactly is the opinion? If you can identify this, we get a basic understanding of an opinion and can already be useful. Sometimes if you want to understand further, we want to enrich the opinion representation. And that means we also want to understand, for example, the context of the opinion and what situation was opinion expressed. For example, in what time was it expressed? We also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? Or perhaps the opinion holder was happy or sad. And so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. So let's take a simple example of a product review. In this case, this actually explicit opinion Holder and explicit target, so it's It's obviously what's opinion Holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. For example iPhone 6. When the review was posted, usually you can extract such information easily. Now the content of course is the review text that's in general also easy to obtain. So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation. But of course, if you want to get more information, we might want to know the context. For example, the review was written in 2015. Or we want to know that the sentiment of this review is positive, and so this additional understanding of course adds value to mining the opinions. Now you can see in this case the task is relatively easy, and that's because. The opinion Holder and opinion target that have already been identified. Now let's take a look at the sentence in the news. In this case, we have implicit holder and implicit target. And the task is in general harder so we can identify opinion holder here and that's governor of Connecticut. We can also identify the target. So one target is Hurricane Sandy. But there is also another target, which is a hurricane of 1938. So what's the opinion? Well, this negative sentiment here that's indicated by words like a bad and worst. And we can also then identify the context. New England in this case. Now unlike in the product review, all these elements must be extracted by using natural language processing techniques. So the task is much harder and we need a deeper natural language processing. And these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened. Analyzing sentiment in news is still quite difficult. It's more difficult than the analysis of opinions in product reviews. Now there are also some other interesting variations. In fact, here we're going to examine the variations of opinions more systematically. First, lets think about the opinion Holder. Now the Holder could be an individual or could be a group of people and sometimes opinion was from a committee or from a whole country of people. Opinion Target can also vary a lot. It can be about 1 entity, a particular person, a particular product, a particular policy, etc. But it could be about a group of products. Could be about the product from a company in general. Could also be very specific about one attribute, attribute of Entity for example. It's just about the battery of iPhone. It could be about someone else opinion and one person might comment on another persons opinion etc. So you can see there is a lot of variation here that will cause the problem to vary a lot. Now opinion content, of course, can also vary a lot on the surface. You can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article. And Furthermore, we can identify the variation in the sentiment or emotion dimension. That's about the feeling of the opinion Holder. So we can distinguish positive versus negative or neutral or happy versus sad, etc. Finally, the opinion context can also vary. We can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. So when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. From computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. First, the Observer might make a comment about the opinion target in the observed world. So in this case we have the author's opinion. For example, I don't like this phone at all, and that's opinion of this author. In contrast, the text might also report opinions about others so the person could also make observation about another person's opinion and report this opinion. So for example, I believe he loves the painting and that opinion is really about, is really expressed by another person. Here, so it doesn't mean this author loves that painting. So clearly the two kinds of opinions need to be analyzed in different ways and sometimes in product reviews you can see, although mostly the opinions are from this reviewer. Sometimes a reviewer might mention opinions of his friend or her friend, right? And another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion. For example, one statement might be this phone ran out of battery in just one hour. Now this is in a way, a factual statement, 'cause you know it's either true or false, right? You can even verify that. But from this statement one can also infer some negative opinions about the quality of the battery of this phone or the feeling of the opinion holder about the battery. In the opinion, Holder clearly wish the battery to last longer. So these are interesting variations that we need to pay attention to when we extract opinions. Also, for this reason about the indirect opinions. It's often also very useful to extract it or whatever the person had said about the product, and sometimes factual sentences like this are also very useful. So from practical viewpoint, sometimes we don't necessarily extract the subjective sentences. Instead, would you just get all the sentences that are about opinions that are useful for understanding the person or understanding the product that we are commenting on. So the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. In each representation we should identify opinion Holder, target content and context. Ideally we can also infer opinion sentiment from the content and context to better understand the opinion. Now often some elements of the representation are already known. I just gave a good example, in the case of product reviews where the opinion Holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. Now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. So now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. So here I identify three major reasons, 3 broad reasons. The first is it can help decision support. I can help us optimize our decisions. We often look at the other peoples opinions and look at the reader reviews in order to make a decision like buying, buying a product, or using the service. We also Would be interested in others opinions. When we decide whom to vote, for example. And policymakers may also want to know peoples opinions when designing a new policy. So that's one general kind of applications. And it's very broad, of course. The second application is to understand people, and this is also very important. For example, it can help understand peoples preferences. So and this could help us better serve people. For example, we can optimize the product search engine, optimize recommender system if we know what people are interested in, what people think about products. It can also help her with advertising, of course, and we can have targeted advertising if we know what kind of people tend to know to like what kind of product. Now the third kind of applications can be called voluntary survey. Now this is mostly to support research that used to be done by doing surveys, doing manual surveys, questioning answering. People need to fill in forms to answer some questions. Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans to kind of assess the general opinion. Now this is would be very useful for business intelligence, where product manufacturers want to know, where their products have advantages over others. What are the winning features of their product or winning features of competitive products? Market research has to do with understanding consumers opinions and this is clearly very useful, directed for that. Data Driven social science research can benefit from this because they can do text mining to understand the people's opinions. And if we can aggregate a lot of opinions from social media from a lot of public information, then you can actually do some study of some questions. For example, we can study the behavior of people on social media or in social networks, and these can be regarded as voluntary survey, but done by those people. And in general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of prediction.
410	f47410d8-2bc1-44d5-a37d-cbf2d2c917f8	This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems. In this lecture we will continue the discussion of evaluation we'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems. So, In order to create the test collection, we have to create a set of queries, a set of documents and a set of relevance judgments. It turns out that each is actually challenging to create. First, the documents and queries must be representative. They must represent the real queries, and real documents that the users handle, and we also have to use many queries and many documents in order to avoid biased conclusions. For the matching of relevant documents, with the queries we also need to ensure that there exists a lot of relevant documents for each query. If a query has only one, let's say rather than the document in the collection, then you know it's not very informative to compare different methods using such a query, because there's not that much room for us to see difference, so ideally there should be more relevant documents in the collection, but yet the queries also should represent the real queries that we care about. In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries, yet minimizing human effort because we have to use human labor to label these documents, it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries, especially considering a giant dataset like the web. So this is actually a major challenge. It's a very difficult challenge. For measures, It's also challenging because we want the measures that would accurately reflect the perceived utility of users. We have to consider carefully what the users care about. And then design measures to measure that. If your measure is not measuring the right thing, then your conclusion would be misled. So it's very important. So, we're going to talk about a couple of issues here. One is a statistical significance test and this also is the reason why we have to use a lot of queries. And the question here is how sure can you be that observed difference that doesn't simply result from the particular queries you choose, so here are some sample results of average position for system A and system B in two different experiments. And you can see in the bottom we have mean average precision. So the mean if you look at the mean average precision. The mean average precisions are exactly the same in both experiments. So you can see this is 0.2. This is 0.4 for system B and again here, It's also 0.2 and 0.4, so they are identical. Yet if you look at the these exact average precisions for different queries. If you look at these numbers in detail, You will realize that, in one case you would feel that you can trust the conclusion here given by the average. In another case, in the other case, you will feel that well, I'm not sure. So why don't you take a look at all these numbers for a moment? Pause the video. So if you look at the average, the mean average precision, we can easily say that, well, system B is better, right? So it's afterwards 0.4 and then this is twice as much as 0.2, so that's a better performance. But if you look at these two experiments. Look at the detail results, You will see that would be more confident to say that in the case one in experiment one. In this case, because these numbers seem to be consistently better for system B . Whereas in experiment 2, We're not sure because looking at some results like this. Actually System A is better and this is another case. System A is better. But yet if we look at the only average, System B is better. So,  What do you think? You know how reliable Is our conclusion, if we only look at the average? Now, in this case, intuitively we feel experiment one is more reliable. But how can we quantitatively answer this question? And, This is why we need to do statistical significance test. So the idea of statistical significance test is basically to assess the variance across these different queries. If there is a A big variance that means the results could fluctuate a lot according to different queries. Then we should believe that unless you have used a lot of queries, the results might change if we use another set of queries. Right, so this is not. So, If you have see high variance then it's not very reliable. So let's look at these results again in the second case, right? So here we show two different ways to compare them. One is signed test where we just look at the sign. If system B is better than system A, we have a plus sign when System  A is better, we have a minus sign, etc. Using this case, If you see this, well, there are seven cases. We actually have 4 cases where system B is better, but 3 cases System A is better. You intuitively. This is almost like a random result, right? So if you just take a random sample of two to flip 7 coins, and if you use plus to denote the head and then minus to denote the tail, and that could easily be the results of just randomly flipping these 7 coins. So the fact that the average is larger doesn't tell us anything and we can reliably conclude that, and this can be quantitatively measured by a P value and that basically,  means the probability that this result is infected from random fluctuation. In this case probability is 1. It means it surely is random fluctuation. Now in Wilcoxon test, the nonparametric test and we would be not only looking at the science will be also looking at the magnitude of the difference, but we can draw a similar conclusion where you say it's very likely to be from random. So to illustrate this, let's think about the such a distribution and this is called the null distribution. We assume that the mean is 0 here. This say we start with the assumption that there's no difference between the two systems. But we assume that because of random fluctuations depending on the queries. We might observe a difference, so the actual difference might be on the left side here or on the right side here, right, so? And this curve kind of shows the probability that we will actually observe values that are deviating from zero here. Now, so if we look at this picture, then we see that. If a difference is observed here, then the chance is very high that this is in fact a random observation, right? We can define a region of you know likely observation because of random fluctuation, and this is 95% of all the outcomes and in this interval then the observed values may still be from random fluctuation. But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation, right? So there's a very small probability that you will observe such a difference just because of random fluctuation. So in that case we can then conclude the difference must be real. So, System B is indeed better. So this is the idea of statistical significance test. The takeaway message here is that you have to use many queries to avoid jumping into a conclusion, as in this case to say System B is better. There are many different ways of doing this statistical significance test. So, now let's talk about the other problem of making judgments. And as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set. So the question is if we can afford judging all the documents in the collection, which is subset, should we judge? And the solution here is pooling and this is a strategy that has been used in many cases to solve this problem. So the idea of pulling is the following. We would first choose a diverse set of ranking methods. These are text retrieval systems. And we hope these methods can help us nominate likely relevant documents. So the goal is to figure out the relevant documents we want to make judgments on relevant documents, because those are the most useful documents from users perspective. So then we're going to have each to return top K documents. "The ""K"" can vary from systems right?" But the point is to ask them to suggest the most likely relevant documents. And then we simply combine all these top K sets to form a pool of documents for human assessors, to judge. So imagine you have many systems, each will return K documents, will take the top K documents and we formed the union. Now, of course there are many documents that are duplicated bcause many systems might have retrieved the same relevamnt documents. So there will be some duplicate documents and there are also unique documents that are only returned by one system and so the idea of having diverse set of ranking methods is to ensure the pool is broad and can include as many possible relevant documents as possible. And then the users would. Human assistance would make a completely judgment on this data set this pool. And the other unjudged documents are usually just assumed to be non relevant. Now if the pool is large enough, this assumption is OK. But the if the pool is not very large, this actually has to be reconsidered and we might use other strategies to deal with them, and there are indeed other methods to handle such cases, and such a strategy is generally ok for comparing systems that contributed to the pool. That means if you participated in contributing to the pool, then it's unlikely that it will penalize your system because the top ranked documents have all been judged. However, this is problematic for evaluating a new system that may have not contributed to the pool. In this case, a new system might be penalized because it might have nominated some relevant documents that have not been judged, so those documents might be assumed to be non relevant. That's unfair. So to summarize, the whole part of text retrieval evaluation, it's extremely important because the problem is empirically defined problem. If we don't rely on users, there's no way to tell whether one method works better. If we have inappropriate experiment design, we might misguide our research or applications and we might just draw wrong conclusions. And we have seen this in some of our discussion, so make sure to get it right for your research or application. The main methodology is Cranfield evaluation methodology and this is still the main paradigm used in all kinds of empirical evaluation tasks, not just the search engine evaluation. MAP and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms. You will see them often in research papers. Pricision at the 10 documents is easier to interpret from the user's perspective, so that's also often useful. What's not covered is Some other evaluation strategy like A-B Test where the system would mix 2, the results of two methods randomly and then will show the mixed results to users. Of course the users don't see and which result is from which method the users would judge those results or click on those documents in search engine application. In this case then the search engine can keep track of the, clicked documents and see if one method has contributed more. through the clicked documents, if the user tends to click on one, the results from one method, then it's just that the method may may be better, so this is the leverage the real users of a search engine to do evaluation. It's called A-B Test, and it's a strategy that's often used by the modern search engines. Commercial search engines. Another way to evaluate IR or Text retrieval is user studies, and we haven't covered that. I've put some references here that you can look at if you want to know more about that. So there are three additional readings here. These are three mini books about evaluation and they all excellent in covering a broad review of information retrieval, evaluation, and discovered some of the things that we discussed. But they also have a lot of others to offer.
410	f64adab4-578a-4868-8b2c-03fdd4ddf55d	This lecture is a continued discussion of generative probabilistic models for text clustering. In this lecture, we're going to continue talking about the tax capture text clustering, particularly "generative So this is a slide that you have seen earlier where we have written down the likelihood function for a document. With two distributions in two component mixture model for document clustering. Now in this lecture, we're going to generalize this to include the K clusters. Now if you look at the formula and think about the question how to generalize it, you will realize that all we need is to add more terms like what you have seen here. So you can just add more thetas and the probabilities of thetas and the probabilities of generating D from those thetas. So this is precisely what we're going to use. This is general presentation of the mixture model for document clustering. So as more cases we follow these steps using a generated model. First think about our data, right? So in this case our data is a collection of documents N documents denoted by the sub I. And then we talk about the model. Think about the model. In this case, we design a mixture of K unigram language models. It's a little bit different from the topic model. But we have similar parameters. We have a set of theta i's denote the word distributions corresponding to the K unigram language models. We have P of each theta I as the probability of selecting each of the K distributions to generate the document. Now note that, although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster. And this, as you see later, would allow us to assign a document to the. Cluster that has the highest probability of being able to generate the document. So as a result, we can also recover some other interesting. Properties. As you will see later. So the model basically would make the following assumption about the generation of the document. We first choose a theta I according to probability of theta I and then generate all the words in the document using this distribution. Note that it's important that we use this distributed generator. All the words in the document. This is very different from topic model, so the likelihood function would be like what you are seeing here. So the. You can take a look at the formula here. We have used the different. Notation here in the second line of this. Of this equation. But you can see now the. notation has been changed to use unique word in the vocabulary in the product instead of particular position in the document. So from X sub J to W is a change of notation, and this change allows us to show the estimation formulas more easily and you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words. I and so with the lack of functioning. Now we can talk about how to do parameter estimation. Here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. So after we have estimate the parameters, how can we then allocate clusters to the documents? Let's take a look at this situation more closely, so we just repeated the parameters here. For this mixture model. Now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right? So theta. I, for example, represents the content of class I. This is actually a byproduct. It helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution. And they will tell us what the cluster is about. An P of theta i can be interpreted as. Indicating the size of cluster because it tells us how likely cluster would be used to generate the document. The more likely a cluster is used to generate the document, we can assume the larger the cluster size is. Note that unlike in PLSA and this probability of theta I is not dependent on D. Now. You may recall that the topic choice in each document actually depends on D. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents. But of course, given a particular document that we still have to infer which topic is more likely. To generate the document so in that sense, we can still have a document dependent probability of clusters. So lets look at a key problem of assigning document to clusters or assigning clusters to documents Lets to compute the C sub D here and this will take one of the values in the range of one to k to indicate which cluster should be assigned to D. Let's first you might think about a way to use likelihood only, and that is to assign D to the cluster corresponding to the topic Theta I. That most likely has been used to generate D. So that means we're going to choose one of those distributions that gives D highest probability. In other words, we see which distribution has a content that matches our D best. Intuitively, that makes sense. However, this approach does not consider the size of clusters, which is also available to us. And so a better way is to use the likelihood together with the prior. In this case the prior is P of Theta I. And together, that is, we're going to use the base formula to compute the posterior probability of Theta given D. And if we choose theta based on this posterior probability and we would have the following formula that you see here. On the bottom of this slide, and in this case, we're going to choose the theta that has a large P of Theta I. That means a large cluster and also a high probability of generating D. So we're going to favor a cluster that's large and also consistent with the document. And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster. So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. So next we have to discuss how to actually compute the estimate of the model.
410	f736ee59-8c2e-495d-93c5-c1385c49a44a	this letter is a summary of this cause this map shows the major topics we have covered in this cause and here are some key high level takeaway messages first we talked about the metro media content analysis here the main takeaway messages natural language processing the foundation for text retrieval but currently NF he is in the robust enough so the bag of words representation is generally the main method used in modern search engines and it's often sufficient for most of the search tasks but obviously for more complex such tasks we need a deeper natural language processing techniques and we then talked about the high level strategies for text for access we talked about the push pull in poor we talked about the querying with this browsing now in general in future search engines we should integrate all these techniques to provide a multi bold information access and then we talked about a number of issues related to search engines without power to the search problem and we frame that as a ranking problem and we talked about a number of retrieval methods and started with the overview of vector space model and the probabilistic model and then we talked about the vector space model impacts we also later talked about language modeling approach that's probabilistic model and hear the main takeaway messages that modeling retrieval functions tend to look similar and they generally use where is heuristics most important ones are TF IDF weighting document length normalization in the TF is often transformed through seven linear transformation function and then we talked about how to implement retrieval system and here the main techniques that we talked about how to construct the inverted index so that we can prepare the system to answer query quickly and we talked about how to faster search by using the inverted index an we then talked about how to evaluate the text retrieval system main introduced the cran field evaluation methodology this was a very important evaluation methodology that can be applied to many tasks we talked about the major evaluation measures so the most important measures for search engine map mean average precision and end DCG normalized discounted cumulative gain an also precision and recorder two basic measures and we then talked about the feedback techniques an we talked about the rock you in the vector space model and the mixture model in the language modeling approach feedback is very important technique especially considering the opportunity of learning from a lot of clicks rules on the web we then talked about web search an here we talked about how to use parallel indexing to solve the scalability issue in indexing we introduce the mapreduce and then we talked about how to use linked information on the web to improve search we talk about page rank and hits as the major algorithms to analyze links on the web we then talked about learning to rank this is use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach but we can also improve the robustness of the ranking function so that it's not easy to spam or search engine with just some features through promote a page and finally we talked about the future of web search without about some major directions that we might have seen in the future improving currently generation of search engines and then finally we talked about recommended systems and these are systems to implement the push mode and we talked about the two approaches one is content based one is a collaborative filtering and they can be combined together now an obvious listen tastes in this picture is the user can see so user interface is also important component in any search engine even though the current are searching face is relatively simple they actually have been a lot of the studies of user interface is relative to visualization for example and this is a topic that you can learn more by reading this book so X men the book about all kinds of studies of search using the face if you want to know more about the topics that we talked about you can also read some additional ratings that are listed here in this short course we only manage a little cover some basically topics in text retrieval and search endings and these resources provide additional information about more advanced topics and they gave more feral treatment of some of the topics that we talked about and a main source is synthesis digital library where you can see a lot of short textbook oh textbooks or long tutorials they tend to provide a lot of information to explain a topic and there are multiple series is that are related to this calls one of information concepts retrieval and services another is human language technology and yet another 's artificial intelligence and machine learning there are also some major journals and conferences listed here that tend to have a lot of research papers related to the topic of this course and finally for more information about the resources including readings and toolkits etc you can check out this UI so if you have not taken the text mining calls in this data mining specialization series then naturally the next step is to take that cause as this picture shows to mind big text there are we generally need two kinds of techniques one is text retrieval which is covering these costs and these techniques will help us converter or big text there are into small relevant the text data which you are actually needed in the specific application human plays important role in mining any tax there are becaus text data is written for humans to consume so involving humans in the process of data mining is very important and in this cause we have covered various strategies to help users get access to the most relevant data these techniques also essentially text mining system to help provide provenance to help users interpret the independence that user defined through texture data mining so in general the user would have to go back to the original data to better understand the patterns so the text mining calls rather text mining an analytics course would be dealing with what to do once the user has found information so this is the second step in this picture where we would convert that extra data into actionable knowledge and this has to do with helping users it'll further digest the founding formation or find the patterns and to reveal knowledge buried in text an such knowledge can then be used in application system to help decision making all to help user finish your task so if you have not taken that cause the natural step in the natural makes the step would be to take that course thank you for taking this course i hope you have found this caused be useful to you and i look forward to interacting with you at a future opportunity
410	faaa4dbf-c3bb-44d1-ac49-b04834a20e2d	this lecture is about the evaluation of text retrieval systems in the previous lectures we have talked about a number of cattle retrieval methods different kinds of ranking functions but how do we know which one works the best in order to answer this question we have to compare them and that means we have to evaluate these retrieval methods so this is the main topic of this laptop first really think about the why do we have to do evaluation i already given one reason that is we have to use devaluation to figure out which retrieval method works better now this is very important for advancing our knowledge otherwise we wouldn't know whether a new idea works better than all the idea in the beginning of this cause we talked about the problem of text refillable we compare it with database retrieval there we mentioned that text retrieval is empirically defined the problem so evaluation must rely on users which is system works better with have to be judged by our users so this becomes a very challenging problem becaus how can we get users involved in the evaluation how can we do a fair comparison of different methods so just go back to the reasons for evaluation i just didn't two reasons here the second reason is basically what i just said but there is also another reason which is to assess the actual utility of attacks retrieval system not imagine you're building your own search any applications it would be interested in knowing how well your search engine works for your users so in this case measures must reflect the utility to the actual users in a real application and typically this has to be done by using user studies and using the real search engine being the signal case or for the second reason the mesh shoes actually own need to be correlated with the utility to actual users that they don't have to accurately reflect the exactly utility to users so the measure only needs to be good enough to tell which method works better and this is usually down through a test collection and this is the main idea that will be talking about this calls this has been very important for comparing different algorithms and for improving a search engine system in general so next we talk about what to measure right there are many aspects of a search engine that we can measure we can evaluate and here i listed the three major aspects one is effectiveness so accuracy how active the other search results in this case we're measuring systems capability of ranking relevant documents on top of mount read in the ones the second is the efficiency how quickly can i use are getting results how much are computing resources are needed to answer a query so in this case we need to measure the space and time overhead of the system the third aspect is usability basically the question is how useful is the system for real user tasks here obviously interfaces and many other things also important and we typically would have to do using studies now in this cause we're going to talk mostly about effectiveness an accuracy measures becaus the efficiency and usability dimensions are not really unique to search engines and so they are needed for evaluating any other software systems and there is also good coverage of such materials in other causes but how to evaluate a search engines quality accuracy is something unique to text retrieval and we're going to talk a lot about this the main idea that people have proposed before using attests attitude evaluate text retrieval algorithm is called the cranfield evaluation methodology this one actually was developed the long time ago in ninety six days its methodology for laboratory test of system components it's actually methodology that has been very useful not just before search engine evaluation but also for evaluating virtually all kinds of empirical tasks and for example in natural language processing or in other fields where the problems in paragraph defined with people you would need to use such a methodology and today with big data challenge with use of machine learning everywhere we this methodology has been very popular but it was first available for search engine application in nineteen sixties so the basic idea of this approach is repealed are reusable test collections and define measures once such a test class and is build it can be used again and again to test the different algorithms and we're going to define measures that would allow you to quantify the performance of system or an algorithm so how exactly would this work well we're going to have a sample clashing of documents and this is just to simulate the real document collection you know search application we're going to also have a sample set of queries or topics this is a simulated the users queries then we'll have to have relevance judgments these are judgments of which document that should be returned for which queries ideally they have to be made by users who formulated the queries 'cause those are the people that know exactly what documents would be useful and then finally we have to have measures to quantify how well a systems result that matches the ideal rent list that would be constructed based on users relevance judgments so this methodology is very useful for starting retrieval algorithms becaus the tester crashing can be reused many times and it would also provide a fair comparison for all the methods we have the same criteria same that are set to be used to compare different algorithms this allows us to compare a new algorithm with an older algorithm that was divided with many years ago by using the same standard so this is the illustration of how this works so as i said we need a queries assume here we have two one two two etc we also need a documents that's called the document clashing and on the right side you see we need relevance judgments these are basically the binary judgments of documents with respect to a query so for example D wise judge it as being relevant to Q R D two is judged as being rather than the as well and T three is judging as non relevant the two two one etc these would be created by users but we also have these and then we basically have attested collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results let's say if the query is Q one and then we would have results here here i show ask subway as results from system so this is remember we talked about task of computing approximation of relevant documents that are subway is system is approximation here anne also be is system bees approximation of relevant documents now let's take a look at these results so which is better now imagine for user which one would you like let's take a look at both results and there are some differences and there are some documents that are returned by both systems but if you look at the results you would feel that well maybe a is better in the sense that we don't have many non relevant documents an amount of three documents return the two of them are relevant so that's good it's precise on the other hand one council say maybe bees better becaus we've got more relevant documents we've got three instead of two so which one is better and how do we quantify this well obviously this question highly depends on a user 's task and it depends on users as well you might be able to imagine for some users may be system way is better if the user is not interested in getting all the random in the document like in this case this is the user doesn't have to read a million the user would see most of the relevant documents on the one hand and one can also imagine user might need to have as many relevant documents as possible for example if you're doing a literature survey you might be in the second category and you might find that system B is better so in that case we will have to also define measures that would qualify them and we might need to define multiple measures becaus users have different perspectives of looking at the results
410	fd7efa33-8e41-4bdc-9fd1-613a8f92bfd6	So now let's take a look at the specific  Method that's based on regression. Now this is one of the many different methods. In fact, it's one of the simplest methods and I choose this to explain the idea because it's simple. So in this approach, we simply assume that the relevance of document with respect to query is related to a linear combination of all the features. Here are used Xi to denote the feature, so Xi of Q and D is a feature. And we can have as many features as we would like. And we assume that these features can be combined in a linear manner. And each feature is controlled by a parameter here. And beta is a parameter that's a weighting parameter, a larger value would mean the feature would have a higher weight and would contribute more to the scoring function. The specific form of the function actually also involves a transformation of the probability of relevance. So this is the probability of relevance. We know that the probability of relevance is within the range from 0 to 1. And we could have just assumed the scoring function is related to this linear combination, right? So we can do a linear regression, but then the value of this linear combination could easily go beyond one, so this transformation here would map the zero to 1 range to the whole range of real values. You can verify it by yourself. So. This allows us then to connect the probability of relevance which is between zero and one to a linear combination of arbitrary features. And if we rewrite this into a probability function we would get the next one, so on this on this equation then we will have the probability of relevance. And on the right hand side we would have this form. Now this form is clearly non negative and it still involves the linear combination of features. And it's also clear that if this value is, this is actually negative of the linear combination in the equation above, if this, This value here. If. This value is large, then it would mean this value is small and therefore this probability this whole probability would be large and that's what we expect. Basically it would mean if this combination gives us a high value, then the documents more likely relevant. So this is our hypothesis. Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance. So now we have this combination function. The next task is to see how we to estimate the parameters so that the function can actually be applied without knowing the beta values, it's harder to apply this function, so let's see how we can estimate beta values. Let's take a look at a simple example. In this example we have 3 features. One is BM25 score of the document and query one is the page rank score of the document, which might or might not depend on the query. We might have a topic sensitive Pagerank that would depend on query. Otherwise the general page rank doesn't really depend on query and then we have BM25 score on the anchor text of the document. These are then the feature values for a particular Doc document query pair. An in this case, the document is D1, and the judgment that says that is relevant. Here's another training instance and with these feature values. But in this case it's not relevant. OK, this is overly simplified case where we just have two instances. But it is sufficient to illustrate the point. So what we can do is we use the maximum likelihood estimator to actually estimate the parameters. Basically, we are going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here. Can we predict the relevance? Yeah, and of course the prediction will be using this function. That you see here. And we hypothesize that the probability of relevance is related to features in this way, so we're going to see for what values of beta. We can predict the relevance well. What do we? What do we mean by predicting the relevance well? we just mean in the first case for D1. This expression here right here should give a high values. In fact, we hope this to give a value close to one, why? because this is a relevant document. On the other hand, in the second case for D2, we hope this value will be small. Why? Because It's a non relevant document. So now let's see how this can be mathematically expressed. This is similar to expressing the probability of document. Only that we're not talking about the probability of words, but talking about probability of relevance one or zero. So what's the probability of this document? The relevant if it has these feature values. This is just this expression, right? We just need to plug in the Xi(s). So that's what we will get. It's exactly like. What we have seen above, only that we replaced these Xi(s) with now specific values, right? So for example, this .7  goes to here and this .11. goes to here. And these are different feature values and we combine them in this particular way. The beta values are still unknown, but this gives us the probability that this document is relevant if we assume such a model. OK, we would want to maximize this probability, since this is a relevant document. What we do for the second document? Well, we want to compute the probability that the prediction is non relevant. So. This would mean we have to compute 1 minus. This expression. Since this expression. Is actually the probability of relevance. So to compute the non relevance from relevance we just. Do 1 minus the probability of relevance. OK. So this whole expression then just is our probability of predicting these two. Relevance values one is 1 here, one is zero and this whole equation is our probability. Of observing a 1 here. And observing a zero here. Of course, this probability depends on the beta values. Right so then our goal is to adjust the beta values to make this whole thing reach its maximum. Make it as large as possible. So that means we're going to compute this. The beta is just the parameter values that would maximize this whole likelihood expression. And what that means is if look at the function is we are going to choose betas to make this as large as possible and make this. Also, as large as possible, which is equivalent to, say, make this the part as small as possible. And this is precisely what we want. So once we do the training now we will know the beta values. So then this function would be well defined once beta values are known. Both this and this. Would be completely specified, so for any new query and new documents, we can simply compute the features. For that pair and then we just use this formula to generate the ranking score and this scoring function can be used to rank documents for a particular query. So that's the basic idea of learning to rank.
