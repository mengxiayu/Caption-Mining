[
  {
    "text": "word distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "all the x(i) will sum to one in this vector. so this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one. now the weight of bm25 for each word is defined here. and if you compare this with our old definition where we just ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "odel, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. but now we're going to us",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. but now we're going to use a word distribution to describe the topic. so here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabul",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "r one term or one phrase. but now we're going to use a word distribution to describe the topic. so here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary. so for example, the high probability words here are sports, game, basketball, football, pl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "st slightly. the slide is very similar to what you have seen before, except that we have added refinement for what the topic is. so now each topic is word distribution. and for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. so you see a constraint h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "imilar to what you have seen before, except that we have added refinement for what the topic is. so now each topic is word distribution. and for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. so you see a constraint here and we still have another co",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "for analysis, and that means each word is a unit. now the output would consist of as first a set of topics represented by theta i's each theta_i is a word distribution. and we also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before. so given a set of text da",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": ", of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. first, we have theta_i's each is a word distribution and then we have a set of pi's for each document. and since we have n documents so we have n sets of pis. and each set of the pi values will sum to o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ve n documents so we have n sets of pis. and each set of the pi values will sum to one. so this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. so how do we model the data in this way? and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ing the model, of course we can discover different knowledge. so to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic. it also allows us to assign weights on words so we can model subtle ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " as a distribution, so the input is a collection of text articles. the number of topics and vocabulary set and the output is a set of topics. each is word distribution. and also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "e have n words, so we have n probabilities, one for each word, and they sum to one. so now we can assume our text is a sample drawn according to this word distribution. that just means we're gonna draw a word each time and then eventually we'll get a text. so for example now again. we can try to sample words accordi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "e model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ty distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a par",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "r data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this case m. and for conve",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " controlled by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basically flip a coin a fair coin to decid",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the backgro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "sing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "en we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o use the background word distribution to generate the word. so in such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating text data and such a model is called a mixture model. so now let's see. in this case, what'",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ved from each of the two distributions, so we have to consider 2 cases. therefore it's a sum over these two cases. the first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of theta sub d, which is the probability of choosing the mode",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e same is all similar, right? so we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution. now later you will see this is actually general form, so you might want t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ut the text, so in this case, what do we discover? well, these are represented by our parameters, and we have two kinds of parameters. one is the two word distributions. those are two topics and the other is the coverage of each topic in each. the coverage of each topic and this is determined by probability of theta",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ers of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the ot",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're goin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "el as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our target. so this is the case of customizing a probalistic model so that we embed the unknown variables that we are intere",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this is a powerful way of customizing a model for a particular need. now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "point and you will be able to write it down. so the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. and it accounts for the two ways of generating text. an inside each case we have the probability of choosing the model which is .5 multiplied by the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "d more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. a large collection of english documents, by using just one distribution a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probability of the o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for examp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "o be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "xtremely simple, right? if you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and normalize them. so indeed this problem will be very easy to solve. if we had known which words are from which distribution precisely.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime and then all we need to do is just normali",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "o you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution. we are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so. we're ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ial values of z attached to all the words. and that's why we call these hidden variables. now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. the value of this hidden variable. and so. the algorithm, the em algorithm then ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "e e-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "rested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of em algo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " distribution. in this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? so this is our primary goal. we hope to have a more discriminating world distribution. but the last column is also by product and this actuall",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": ". those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. and the other is the word distributions that characterize all the topics. so the next line then is simply to plug this in to calculate the probability of document. this is again of the fam",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "us to what extent we should allocate this word to topic theta sub-j. but the normalization is different because in this case we are interested in the word distribution. so we simply normalize this over all the words. this is different. in contrast, here we normalized among all the topics. it would be useful to take ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " as the algorithm, we first initialize all the unknown parameters randomly. in our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. and we just randomly normalize them. this is the initialization step, and then we will repeat until likelihood converges. now how do we kno",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "h distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. and i said it's proportional to this becau",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " normalize among all the topics to get re estimate of pi the coverage. or we can renormalize based on the. for all the words and that would give us a word distribution. so it's useful to think of the algorithm in this way, because when you implement, you can just use. variables to keep track of these quantities in e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. and such detailed characterization of coverage of topics ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "s dirichlet prior dirichlet distribution based on this preference, then the only difference in the em algorithm is in the m step. when we re estimate word distributions, we are going to add. additional counts to reflect our prior right? so here you can see the pseudocounts are defined the based on the probability of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ver, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. so the inference part. again, face the l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " remove the background model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also. and this is for convenience to introduce lda and we have one vecto",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our inference on the k",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " you see very similar things. first you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions. and this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multip",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "many applications. the best basis test setup is to take tax data as input, and we're going to output the key topics. each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ction c and number of topics k and vocabulary v, and we hope to generate as output two things. one is a set of topics denoted by theta i's. each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. so this is a topic coverage and it's also visualized",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "m of mining. one topic that we discussed earlier. so here again it's a slide that you have seen before. and here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. but we can also consider some variations of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ke a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the k word distributions that we have. but this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "bution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the case of topic modeling, one distribution doesn't have to generate with ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on one document. so that's the connection that we discussed earlier. but now you can see more clearly. so as more cases of using a generative m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "he content of class i. this is actually a byproduct. it helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution. and they will tell us what the cluster is about. an p of theta i can be interpreted as. indicating the size of cluster because it tells us how likel",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "numerator and the denominator both by this normalizer. so basically this normalizes the probability of generating this document by using this average word distribution. so you can see the normalizer here. and since we have used exact the same normalizer for the numerator and denominator, the whole value of this expr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "n achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is ge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this questi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "a i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "en from this cluster, so we should favor such a cluster. the other is a likelihood part, that is this part. and this has to do with whether the topic word distribution can explain the content of this document well. and we want to pick a topic that's high by both values. so more specifically, we just multiply them to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " shown the details. below here you can see how the prior here is related to the posterior on the left hand side. and this is related to how well this word distribution explains the document here, and the two are related in this way. so to find the topic that has the highest posterior probability here, it's equivalen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ing that you have seen in document clustering. an we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. so this idea can be directly adapted to do categorization and this is precisely what naive b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nformation, except that we're looking at the categorization problem now, so we assume that if theta i represents category i accurately that means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely like what we did for text clustering. namely, we ar",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " now the question is, how can we make sure each theta i actually represents category i accurate? now, in clustering we learned this category i or the word distributions for category i from the data. but in our case what can we do to make sure this theta i represents indeed category i? if you think about the question",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lity of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. the other kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " moment and to think about how to solve this problem. so let me state the problem again, so let's just think about category one. we know there is one word distribution that has been used to generate documents. and we generated each word in the document independently and we know that we have observed the set of n sub",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "et of n sub one documents in the set of t1. these documents have been all generated from category one, namely have been all generated using this same word distribution. now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ords, we make this probability proportional to the size of training dataset in each category. that's the size of the set t sub i. now, what about the word distribution? well, we do the same again. this time we can do this for each category. so let's say we are considering category i or theta i. so which word has hig",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ust go back to the original estimate based on the observed training data to estimate the probability of each category. now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. so here you see we'll add pseudocounts to each wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the differenc",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "pen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "e generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "he context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " different views associated with the each of the topics. and these are shown as view one, view two and view three each view is a different version of word distributions. and these views are tide to some context variables. for example, type to the location texas or the time july 2005 or the occupation of the other be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " could be from any of these contexts. let's say we have taken this view. that depends on the time in the middle. so now we have a specific version of word distributions. now you can see some probabilities of words for each topic. now, once we have chosen a view, now the situation will be very similar to what happene",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ds for each topic. now, once we have chosen a view, now the situation will be very similar to what happened in standard plsa. we assume we have got a word distribution associated with each topic, right? and then next to the view we choose a coverage from the bottom. so we're going to choose particular coverage and t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " we might pick a particular coverage, let's say in this case. we pick we've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in plsa. so what it means we're going to use the coverage to choose a topic to choose one o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "until we generate all the words and this is basically the same process as in plsa. now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. so in other words, we have extra switches that are tied to this context that would control the choices of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "and this is. this topic is indeed very relevant, to both wars. if you look at the column further and what's interesting is that the next two cells of word distributions actually tell us collection specific variations of the topic of united nations. so it indicates that in iraq war, united nations was more involved i",
        "label": "use"
      }
    ]
  },
  {
    "text": "text mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "lem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially for decision making or for completi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ta as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows the process of generating text data in more detail. most specifically, human sensor or human observer would look at t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "anguage to express what he or she has observed. in that case, we might have text data of mixed languages for different languages. so the main goal of text mining is actually to revert this process of generating test data. and we hope to be able to uncover some aspect in this process. and so specifically we can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. finally, if you look at the picture to the left side of this picture, then you can see we can certainly also sa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "t the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? so indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. and we want to predict the value of certain interesting variables",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "t the observer or the authors of text data. we could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. in this course we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. first, w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the obser",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " any text data. but unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. the reason is because we're not even recognizing words. so as a string we're going to keep all the spaces and these ascii symbols. we can perhaps co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "oblems. now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. that's the purpose of text mining. so there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from tex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "of knowledge. i should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. they are always in the loop. meaning that we should optimize the collaboration of humans and computers. so in tha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "rd based representation. these techniques are general and robust and thus are more widely used in various applications. in fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level. but obviously all these other leve",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the picture that you have seen before in order to show the generation process. so the idea of this approach is actually to 1",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit the data as wel",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "vering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they are generated this way and inside the model,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "butions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the q",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "tion, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "n this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "aper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. assuming that no word has a zero probability in the distribution and that just means we can essen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ates a different topic and in this case it's probably about health. so if we sample words from such distribution, then the probability of observing a text mining paper would be very very small. on the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relative",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": ", in this case, we're going to assume that we have observed data. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we ask the q",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "t this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. so one way to address this problem is actually to use bayesian estimation, where we actually would look at both the data and all our prior knowledge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the high probability words tend to be those very common word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " or to revise our estimate of the parameters. so let me also illustrate we can group the words that are believed to have come from cedar sub d and as text mining algorithm for example and clustering. and we had group them together. to help us re estimate the parameters. that were interested in so these will he",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " the training data looks like, and then it doesn't allow us to generalize the model for using other data. this, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. we are not always interested in modeling future data, but i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "of plsa and the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "t might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "uster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say all the email messages from ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ther its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is often t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "o text data indirectly. once we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often inter",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " consumers opinions and this is clearly very useful, directed for that. data driven social science research can benefit from this because they can do text mining to understand the people's opinions. and if we can aggregate a lot of opinions from social media from a lot of public information, then you can actua",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-grams. you",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ven if you read all the reviews. even if you read all the reviews, it's very hard to infer such preferences or such emphasis. so this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. you can compare differ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "and of course the model is general. it can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. finally, there is also some result on applying this model for personalized ranking or recommendation of entities. so because we can inf",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ated by this query and they tend to really have favor low price hotels. so this is yet another application of this technique. and shows that by doing text mining we can understand the users better. and once we can end users better, we can serve these users better. so to summarize our discussion of opinion mini",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ctually very general and it's reflecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for predictio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "em and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " really help improving the accuracy of prediction. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer va",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ovide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "eference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining becaus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "d allow us to then compare such a subset with another set of papers written by authors in other countries. or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. topic. and note that these partitioning can be also intersect with each other to generate ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "aring topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "aign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "g contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with approp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ng to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to know mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "le to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see some sam",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ere are some suggested readings. the first is paper about simple extension of plsa to enable cross collection comparison. it's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. the second one is the main paper abo",
        "label": "intro"
      }
    ]
  },
  {
    "text": "mixture model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ainty associated with the use of a word distribution. but we can still think of this as a model for generating text data and such a model is called a mixture model. so now let's see. in this case, what's the probability of observing the word w? \"now here i showed some words like \"\"the\"\"\" \"and \"\"text\"\", so as in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ould convince yourself that this is indeed the probability of observing text. so to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. and in each case it's a product of the probability of selecting that component mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ing the data point from that component model, and this is something quite general and you will see this occurring often later. so the basic idea of a mixture model is just to treated these two distributions together as one model. so i use the box to bring all these components together. so if you view this whole ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "rd. but the way that determines this probability is quite different from when we have just one distribution. and this is basically a more complicated mixture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "one distribution. and this is basically a more complicated mixture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "b d. second, the probability of actually observing the word from this component model. and so this is a very general description of, in fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecial case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0. so in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "rence is that inside here now it's a sum instead of just one, so you might recall before we just had this one. but now we had this sum because of the mixture model and because of the mixed model we also have to introduce the probability of choosing that particular component distribution. and so this is just anot",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our target. so this is the case of customizing a probalistic model so that we embed the unk",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "mmon words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the behavior of a mixture model. so we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "to examine the behavior of a mixture model. so we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a ver",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here. so specifically in this case",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "hat's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model. now the interesting question now is, how can we then optimize this likelihood? well, you will notice that there were only two variables. they are pr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "he probability of text given by theta sub d somewhat larger so that the two sides can be balanced. so this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. would tend to do the opposite. bas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies. ok, so what you're seeing now is basically the likelihood function for the two",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ow you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective funct",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "r these frequent words. if you have a very small probability of being chosen, than the incentive is less. so to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can ex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ity of being chosen, than the incentive is less. so to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. and that would make the discovered\u00a0 topic more discriminative. t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ls. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "amily of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "roblem will be very easy to solve. if we had known which words are from which distribution precisely. and this is in fact making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the single word distrib",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "rd is from so let's assume that we actually know tentative probabilities for these words in theta sub d. so now all the parameters are known for this mixture model. and now let's consider word like\u00a0 text. so the question is, do you think text is more likely have been having been generated from theta sub d or fro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "r which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply bayes rule to infer which distribution is more likely to generate each word and this prediction essentially helped ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "right? so this one. and this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. but in the case of mixture model, we cannot easily find the analytical solution to the problem. so we have to resolve a numerical algorithm. an em algorithm is such an algorithm. it'",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "oint. to summarize, in this lecture we introduce the em algorithm. this is a general algorithm for computing. maximum regular is made of all kinds of mixture models. so not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial po",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "oduce the em algorithm. this is a general algorithm for computing. maximum regular is made of all kinds of mixture models. so not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. the general idea is that we will have",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "erate these as output because there are many useful applications if we can do that. so the idea of plsa is actually very similar to the two component mixture model that we have already introduced. the only difference is that we're going to have more than two topics. otherwise it's essentially the same. so here i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "odeling, would want to figure out the likelihood function. so we will also ask the question what's the probability of observing a world w from such a mixture model? now if you look at this picture and compare this with the picture that you have seen earlier, you will see the only difference is that we have added",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ess again, this is a very important formula to know because. this is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we have the likelihood function, we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "m algorithm you will see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating top",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "sions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of getting a word from a mixture model. now next in the probability of a document we see there is a plsa component in the lda formula. but the lda formula would add some integral here, and",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it looks similar. so here we see that when we generate a document. we generated each word independently. and we generat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "y one distribution has been used to generate. all the words in one document. so if you realize this problem, then we can naturally design alternative mixture model for doing clustering. so this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "t because for each word we can make a potential different decision and that's the key difference between the two models. but this is obviously also a mixture model, so we can just group them together as one box to show that this is. model that will give us a probability of a document. now inside this model there",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "probability of a document. now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. and of course, the main problem in document clustering is to infer. which distribution has been used to generator a document and that would allow us",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "w that just means we have no uncertainty now. we just stick with one particular distribution. now in that case, clearly we will see this is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data. i in this case, so it's going to be some over these k topics because everyone",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " looks very similar or in many ways they are similar. but there's also some difference. and in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. and that's corresponding to our assumption of 1st make a choice of choosing one di",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ate each each word, we have to make a decision regarding which distribution we use. so we have sum there for each word. but in general, ideas are all mixture models that we can estimate these models by using the em algorithm as we will discuss more later.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to include the k clusters. now if you look at the formula and think abou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " of thetas and the probabilities of generating d from those thetas. so this is precisely what we're going to use. this is general presentation of the mixture model for document clustering. so as more cases we follow these steps using a generated model. first think about our data, right? so in this case our data ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "w can we then allocate clusters to the documents? let's take a look at this situation more closely, so we just repeated the parameters here. for this mixture model. now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "he discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ut how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think her",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ou have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorithm starts with initialization of all the parameters. so this is the same as what happened before for topic models. a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": ". now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable atta",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "iable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "den variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must tell us which distribution has been us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "it can be also shown this process will converge to a local minimum. i think about this process for a moment. it might remind you the em algorithm for mixture model. indeed, this algorithm is very similar to the em algorithm for the mixture model for clustering. so more specifically, we also initialize these. pre",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this process for a moment. it might remind you the em algorithm for mixture model. indeed, this algorithm is very similar to the em algorithm for the mixture model for clustering. so more specifically, we also initialize these. predators in the em algorithm, so the random inner inner initialization is similar. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. so it's essentially similar to eastep. also, what's the difference? while the differences here, we don't make a probabilistic on location as in the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "d to convert converted local minimum. so to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "well and to do that, you can vary the number of clusters and watch how well you can fit the data. if it's in general, when you add more components to mixture model, you should fit the data better, because you can always set the probability of using the new component at 0, so you can't in general fit the data wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generativ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "d from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. there's no mixing of different ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "emi supervised machine learning techniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actual",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " and this means we will impose some preference on certain thetas over others. and by using bayes rule that i have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full explanation of bayes rule and some of these things related to bayes",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "nerative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of parameter values, this function can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ooking at the data so that the model would have the right parameters for discovering the knowledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. and the likelihood function wil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. and the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ence we're going to use theta sub i to denote the probability of word w sub i. and obviously these thetas of i's would sum to one. now, what does the likelihood function look like? this is just the probability of generating this whole document given such a model. because we assume the independence in generating each w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "r in the document, it would have a zero count and therefore that corresponding term will disappear. so this is a very useful form of writing down the likelihood function that we will often use later. so i want you to pay attention to this. just get familiar with this notation. it's just to change the product over all ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ation. it's just to change the product over all the different words in the vocabulary. so in the end, of course we'll use theta sub i to express this likelihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "likelihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelih",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "unction. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the log likelihood instead of the original likelihood an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "lihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, becaus",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. the objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. so one way to solve the problem is to use lagrange multiplier approach. now this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "n a particular way, as you will also see later. so this is basically an analytical solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. instead, we have to use some numer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e word w? \"now here i showed some words like \"\"the\"\"\" \"and \"\"text\"\", so as in all cases, once we\" set up the model, we're interested in computing the likelihood function. the basic question is, so what's the probability of observing a specific word here? now we know that the word can be observed from each of the two d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration that you have seen before, which is dimmer now is just the illustration of this generation model. so mathematically, this model. th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "erstand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? well, in general we can use ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o to think about the special cases, like when we set one of them to one. what would happen? well, the other would be 0, right? and if you look at the likelihood function. it will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o see the model index and to have a complete understanding of what's going on in this model and we have mixing weights of course also. so what is the likelihood function look like? it looks very similar to what we had before, so for the document, first, it's a product of all the words in the document exactly the same ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ting the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word probabilities in each topic must sum to one, the other is the choice of each topi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "xt point one. now, let's also assume that our data is extremely simple. the document has just the two words text and the. so now let's write down the likelihood function in such a case. first, what's the probability of text and what's the probability of the? i hope by this point and you will be able to write it down. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "f observing text from that model. similarly, the would have a probability of the same form, just with different exact probabilities. so naturally our likelihood function is just the product of the two, so it's very easy to see that. once you understand what's the probability of each word, which is also why it's so imp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "other behavior of mixture model and in this case let's look at their response to the data frequencies. ok, so what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1. now it's interestin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "o the document. so what would happen if we add many the's to the document? now this will change the game, right? so how? well, picture what would the likelihood function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihoo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "'s to the document? now this will change the game, right? so how? well, picture what would the likelihood function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihood function by additional terms to account for the addit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. since in this case all the additional terms are the, we're going to just multip",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "same term and so on, so forth until we add as many terms as the number of the's that we added to the document d prime. now this obviously changes the likelihood function, so what's interesting is now to think about how would that change our solution. so what's the optimal solution now? intuitively, you would know the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "f the data. so all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "rithm that would gradually improve the estimate of parameters and as i will explain later, there's some guarantee for reaching a local maximum of the likelihood function. so let's take a look at the computation for specific case. so these formulas are the em formulas that you see before, and you can also see there are",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "'t really covered yet. so here what you see is on the x dimension. we have set up value. this is the parameter that we left on the y axis. we see the likelihood function. so this curve is reaching or like roller function, right? so this one. and this is the one that we hope to maximize an we hope to find a set of valu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "r like recorder. so that's the idea of hill climbing. any in the mri was the way we achieve this is to do two things. first will fix a lower bound of likelihood function, so this is the lower bound you can see here. an once we fit the lower bound we can then maximise the lower bound and of course the reason why this w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "ented data, which would make it easier to estimate the distribution to improve the estimate of parameters. here improve is guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. ther",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "s guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. there are some properties that have to be satisfied in order for the parameters also too. convert it to some stable value. no",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "lustrate how we can generate the text that i was multiple topics. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ask the question what's the probability of observing a world w from such a mixture model? now if you look at this picture and compar",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "d a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we have the likelihood function, we would be interested in knowing the parameters right? so to estimate the parameters. but first let's put all these together to have the complete l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "n, we would be interested in knowing the parameters right? so to estimate the parameters. but first let's put all these together to have the complete likelihood function for plsa. now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as i said. and so ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "d be the output that we generate when we use plsa to analyze text data, and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constraine",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "draw one from the dirichlet distribution, and then from this, then we're going to further sample a word and the rest is very similar to the plsa. the likelihood function now is more complicated for lda, but there's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the diffe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " a word and the rest is very similar to the plsa. the likelihood function now is more complicated for lda, but there's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the difference here. so in the top you see plsa. likelihood function that you have already seen before it",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ere's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the difference here. so in the top you see plsa. likelihood function that you have already seen before it's copied from previous slide only that i dropped the background for simplicity. so in the lda formulas you see v",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "d these integrals to account for the uncertainties and we added of \"course the\u00a0 govern the choice of these parameters, pi's and theta's. so this is a likelihood function for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. now. these parameters that we are interested in, namely the topics and the coverage, are no ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "to solve a problem, we first look at theta and then think about how to design the model. but once we design model, the next step is to write down the likelihood function. and after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " step is to write down the likelihood function. and after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "od function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "tion. note that it's important that we use this distributed generator. all the words in the document. this is very different from topic model, so the likelihood function would be like what you are seeing here. so the. you can take a look at the formula here. we have used the different. notation here in the second line",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ls for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "oing to model this. note that this is a conditional probability of y given x. and this is also precisely what we want for classification. now, so the likelihood function would be just a product over all the training cases. and in each case, this is the modeled probability of observing this particular training case. so",
        "label": "use"
      }
    ]
  },
  {
    "text": "mutual information",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "and they have different upper bounds, so we cannot really compare them in this way. so how do we address this problem? later we'll discuss we can use mutual information to solve this problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ormation theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual information, denoted by i(x;y), measures the entropy reduction of x obtained from knowing y. more specifically the question we're interested in here, is how much",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "es h(x|y) and h(y|x) are not equal. \u00a0but interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by i here and this function has some interesting properties. first, it's also non negative. this is easy to understand becausw the original e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ways help us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero if and only if the two random variables are completely independent. that means knowing one",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "s when x&y are completely independent. now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ould give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intui",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the ques",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "w the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ver eats occurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "s question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "sically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ll see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cause major information",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ill tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than the mutual inf",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "nformation between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than the mutual information between eats and the. because knowing the doesn't really help us predict eats. similarly knowing eats doesn't help us predicting the as well. and you",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " knowing the doesn't really help us predict eats. similarly knowing eats doesn't help us predicting the as well. and you also can easily see that the mutual information between\u00a0 a word and itself is the largest which is equal to the mutual info. the entropy of this word. so because in this case the reduction is maxim",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "is case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "her completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ther words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "at word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "o compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called kl-divergences or callback labeler divergance. this",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ariables. in our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "riable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. the larger this divergence",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ions of the actual joint distribution from the expected distribution under the independence assumption. the larger this divergence is, the higher the mutual information would be. so now let's further look at the what are exactly the probabilities involved in this formula of mutual information. and here i listed all t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "gence is, the higher the mutual information would be. so now let's further look at the what are exactly the probabilities involved in this formula of mutual information. and here i listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "rs. so this is when the two variables taking a value of 0. and they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. here. once we know how to calculate these probabilities, we can easily calculate the mutual information. it's also interesting to note that there ar",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "probabilities involved in the calculation of mutual information. here. once we know how to calculate these probabilities, we can easily calculate the mutual information. it's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "unts, we can just normalize. these counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. now there is a small problem. when we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "otal the sum is actually one. so that's why in the denominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring cor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "rmation theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches the entropy reduction of x. due to knowing why or entropy reduction of why do too knowing eggs? they are the same, so these thre",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "at's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. in particular, mutual information is a principled way for discovering such a relation. it allows us to have values computer on different pairs of words that are comfortable, and so we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to re",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ions with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for all the words, then we can cl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results. how well can y",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "y of or the cluster of object in the system-generated results. how well can you predict the cluster of the object in the gold standard or vice versa. mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "the cluster of the object in the gold standard or vice versa. mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. f measure is another possible measure. now again, a thorough discussion of ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "training data",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " document. you can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data. so we cannot compute the pis for future document. and there was some heuristic. a work around though. and secondly, it has many parameters and i've ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "find a good local maximum. and that really represents global maximum. and in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model. the model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " we might find that it would overfit the training data because of the complexity of the model. the model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. this, however, is not necessary problem for text mining becaus",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data to guide us to select the best number of clusters. now sometimes you may see some methods that can automatically determine the number of clusters. bu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories. and this is called a training data. and then secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "r even syntactic structures. so once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. so soft rules just means we're going to still decide which category should be assigned to the document. but it's not going to be used using a rule t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " for separating different categories. and it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ent categories. and it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most lik",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "d we generate the value in y as output, and we hope the output y would be the right category for x, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of input and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "o be provided by humans. and they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different me",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different object function, which is",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different object function, which is often also called a loss function or cost function. they also tend to vary in their ways o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "onal. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines and the k nearest neighbors. we will cover some of these classifiers in deta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ata. but in the case of categorization, we are given the categories. so we kind of have predefined categories and. then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. but because of the similarity of the two problems",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "we do to make sure this theta i represents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "stion and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. in other words, these are the documents with known c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "her kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate these two probabilities. and in general we can do this separately for different categories. that's just because these documents are known",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " second probability depends on how likely that you will see documents in other categories. right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these k categories. to estimate all these parameters. now if you spend some time to think abou",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ch category. and we simply just normalize this count to make this a probability. in other words, we make this probability proportional to the size of training dataset in each category. that's the size of the set t sub i. now, what about the word distribution? well, we do the same again. this time we can do this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "th what would happen if you have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "no distinction between them, so it becomes just a uniform. what if delta is zero? well we just go back to the original estimate based on the observed training data to estimate the probability of each category. now we can do the same for the word distribution, but in this case we sometimes we find it useful to us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "eral, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known label for that x y, either one or zero. so in our cas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "o adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known label for that x y, either one or zero. so in our case we are interested",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " minimize this one. so you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta valu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interested in classifying and say we ha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "s there has to be set empirically and typically you can optimize such a parameter by using cross validation. basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose. the the parameter k here or some other parameters in other classifi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "rlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best. but in this vm, we're going to look at another criteria for determining which lines best and this time the crite",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ve 11 and this is purely for mathematical convenience, as you will see in a moment. so the goal of optimization first is to make sure the labeling on training data is all correct. so that just means if yi, the known label, for instance xi is one we would like this classify value to be large. and here we just cho",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "y of capturing these constraints. what's our second goal? that's true. maximizing margin, right? so we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin. now the margin ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "eneral method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorizati",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " they also optimize different objects and functions. but in order to achieve good performance, they all require effective features and also plenty of training data. so as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "effective features and also plenty of training data. so as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. so performance is often much more affected by the effectiveness of features and then by the choice of specific clas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " can actually do clustering on these text data to learn categories. and then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. so you can in fact use the em algorithm to actually combine both. that would allow you e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "labeled text documents. and then we're going to assume the high confidence classification results, or actually reliable. then you certainly have more training data. the cause from the unlabeled data we some are labeled as category ones and more labeled as category two. although the label is not completely reliab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "then we combine them with the true training examples. to improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data. so for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " 1) * (m + 1). that's alot alot of parameters. so when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model? so that's not the idea. the second problem is that these problems these k ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "llow us to have two positive benefit of one is it's going to reduce the number of parameters significantly. and the other is to allow us to share the training data, because all these parameters are assumed to be equal. so these training data for different classifiers. can then be shared to help us set the optima",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " of parameters significantly. and the other is to allow us to share the training data, because all these parameters are assumed to be equal. so these training data for different classifiers. can then be shared to help us set the optimal value for beta. so we have more data to help us choose a good beta value. so",
        "label": "use"
      }
    ]
  },
  {
    "text": "text categorization",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is r",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as sho",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "eneral categorize the observer based on the content. that they produce. finally, it's also related to text based prediction. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for tex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested in doing that in this lecture. and then we're going to talk about how to do text categorisation followed by how to evaluate",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "en the task is to classify any tax object into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as sho",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ld further help the system then learn how to recognize. the categories of new tax objects that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category. there is also another important kind of applications of routing emails to the right person to handle. so in helpdes",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "erent kinds of requests and in many cases a person will manually assign the messages to the right people. but you can imagine you can build automatic text categorization system to help routing a request. and this is to classify the incoming request in to one of the categories where each category actually corresponds t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "categories etc. so why is text categories important well, i already showed you several applications, but in general there are several reasons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with catego",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "eing some applications, text or categorization is called a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help cate",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "nce we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using text data as extra",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ple of using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorization problem.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the id",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design caref",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the catego",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. main difference is that ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we'v",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ore likely to appear and if you have not seen text. but this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. but you should know that this kind of model doesn't have to make this assumption. we could, for example, assume the words may be dependent on ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "represents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that altho",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ctor. since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very closed, closely rela",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": ". so this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. so the second classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability of label. given ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this lecture is a continued discussion of. discriminative classifiers for text categorization. so in this lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popular classificatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ve classifier called a support vector machine or vm, which is a very popular classification method, and there has been also shown to be effective for text categorization. so to introduce this classifier, let's also think about the simple case of two categories and we have two public categories, season one and season 2",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "ters of this book where you can find more discussion about evaluation measures. the second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "d disgusted. so as you can see, the task is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "cation. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated features that ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "hat, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated features that may be more appropriate for sentiment ta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " use ordinal regression to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is charac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "eature design is generally an art. that's perhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. and we also need to consider the order of thos",
        "label": "intro"
      }
    ]
  },
  {
    "text": "language model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these model",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "f statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statist",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "tatistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distribution that gives",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ese models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distribution that gives \"\"\"today is wednesday\"\" a pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ause it's impossible to enumerate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ll the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated indep",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "rd independently. now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model. basically now the probability of a sequence of words w_1 through w_n would be just a product of each. the probability of each word. so for such a mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ta, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we samp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated probabilities of these words as i've shown here. so what ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "guage model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated probabilities of these words as i've shown here. so what do you think? what would be your guess? would ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "er you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram lang",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data give",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "odel which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this fun",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "cated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language model",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "uage models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously be",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "us model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specific",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "wo unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "s also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. in fa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ntroduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum like",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ls representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of model set",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": ". in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "f documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions correspo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "fferent from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "del slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we ca",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "uct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "his single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "of model doesn't have to make this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ake this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rul",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "t's not obvious to you. there is another issue in naive bayes which is a smoothing. in fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. so smoothing is the important technique to address data spars",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "l can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "uage model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of backgrou",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrenc",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "he decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also see a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially rem",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "n mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categorie",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "pproach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "rther understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " an impact on the topics of research information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of mo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ubtropical retrieval, with another task introduced later by trec. on the bottom we see the variations that are correlated with the publication of the language model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language mode",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "anguage model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to und",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of event again, the technique is general so ",
        "label": "use"
      }
    ]
  },
  {
    "text": "conditional entropy",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "nd conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predict the presence or ab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "e of eats? would that also help us predict the presence or absence of meat. so these questions can be addressed by using. another concept, called the conditional entropy. so to explain this concept, let's first look at the scenario we had before where we know nothing about the segment. so we have these probabilities i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ontext. so as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. right? so you can see this is essentially the same",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. right? so you can see this is essentially the same entropy function as you have seen before, except that we all ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ave a condition. and this then tells us the entropy of meat after we have known eats occurring in the segment. and of course, we can also define this conditional entropy for the scenario where we don't see eats. so if we know eats did not occur in the segment, then this conditional entropy would capture the uncertaint",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "rse, we can also define this conditional entropy for the scenario where we don't see eats. so if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. so now putting different scenarios together, we have the complete definition",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ld capture the uncertainty of meat in that content in that condition. so now putting different scenarios together, we have the complete definition of conditional entropy as follows. basically. we're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " one, and this gives us the probability that eats is equal to 0 or 1. basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. so if you expand this entropy, then you have the following equation. where you see the involvement of those cond",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "lowing equation. where you see the involvement of those conditional probabilities. now in general, for any discrete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "crete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information about the segment, we won't be able to increase the uncertainty. we can only reduce uncertainty, and that int",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "rediction and it cannot hurt the prediction in any case. now what's interesting here is also to think about what's the minimum possible value of this conditional entropy. now we know that the maximum value is the entropy of x. but what about the minimum? so what do you think? i hope you can reach the conclusion that t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we can predict the one word given that we know ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ells us to what extent we can predict the one word given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itsel",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ook at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so what is the value of this? now. this means ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ions, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so what is the value of this? now. this means we know whether meat occurs in the sentence and we hope to predict whether",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ymore once we know whether the word occurs in the segment we will already know the answer for the prediction. so this is 0. and that's also when this conditional entropy reaches the minimum. so now let's look at some other cases. so this is a case of. knowing the and trying to predict the meat and this is the case of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ropy. and that means there is a stronger association between meat and eats. so we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0? and for what kind of words would it reach its maximum? well, that's when this w is not really related to meat. li",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " related to meat. like the, for example, it would be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a smal",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "rall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word w1, and then we can take the top ranked t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ith w1. note that we need to use a threshold to find these words. the threshold can be the number of top candidates to take or absolute value for the conditional entropy. now this would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the stro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "rget word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it is to predict w1. but if we think about the two p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "are the conditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it is to predict w1. but if we think about the two pairs where we share w2 in the same cond",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagma",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "utual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "s. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual information, denoted by i(x;y), measures the entropy reduction of x obtained",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "tion in the entropy of x can we obtain by knowing y. so mathematically, it can be defined as the difference between the original entropy of x and the conditional entropy of x given y. and you might see here you can see here. it can also be defined as a reduction of entropy of y, because of knowing x. normally the two ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ies. first, it's also non negative. this is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some information can always help us potentially, but won't ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second prope",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero if and only if the two random variables are complet",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "tell us anything about the other. and this last property can be verified by simply looking at the equation above. and it reaches 0 if and only if the conditional entropy of x given y is exactly the same as original entropy of x. so that means knowing why did not help at all, and that's when x&y are completely independ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "opy of x. so that means knowing why did not help at all, and that's when x&y are completely independent. now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": ", because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " the entropy of this word. so because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and anoth",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "s between occurrences of two words. we introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches the entropy reduction of x. due to knowing why or en",
        "label": "intro"
      }
    ]
  },
  {
    "text": "maximum likelihood",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "is question we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even sli",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data lik",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "erent ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here. where we define th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "d then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words whic",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "s posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "efine our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "edge, we have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplific",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "s point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it'",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "en a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "eters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a k",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ", we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likeli",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " theta by assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "er parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "obabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ing behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "deo a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than tex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from on",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "so the only thing unknown is this word probabilities are given by theta sub d update. and in this lecture were going to look into how to compute this maximum likelihood\u00a0 estimator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background mo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "nd then all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "meters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "o. this gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "nguage model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each rep",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "a, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ly favor certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "e in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is no",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "obabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm fo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "hat are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most pop",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "moothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a st",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "next question is, how can we estimate these parameters and so we collectively denote all the parameters by lambda here. now we can, as usual, use the maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed. observer ratings condition on their respective reviews. ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "text clustering",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "this lecture is the first one about the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions ab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "t technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 cluster",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have lear",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster documents in the whole text collection. so in this case documents are the units to be clustered. we may ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the go",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ther clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "g to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. we may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "rovide additional discrimination that might be used for texture classification as we will discuss later. so there are in general many applications of text clustering any. i just saw it with two very specific ones. one is to cluster search results for example and you can imagine a search engine can cluster the sear",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "lustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk abou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "c models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of app",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ious lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "abilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you ha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic wit",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ent is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. in text clustering, however, we only allow a document to cover one topic. if we assume one topic is a cluster. so. that means if we change the topic definition just sli",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "clustering problem. so because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e top. that means the words in the document could have been generated in general from multiple distributions. now this is not what we want to see for text clustering. for document clustering where we hope this document will be generated from precisely one topic. so now that means we need to modify the model, but h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have se",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "n of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function lo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "enerative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this lecture is about the similarity based approaches to text for clustering. in this lecture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "this lecture is about evaluation of text cluster. so far we have talked about multiple ways of doing text clustering but how do we know which method works the best? so this has to do with evaluation. now to talk about evaluation, one must go to go back to the cluste",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " discover interesting clustering structures in text data, and these structures can be very meaningful. there are many approaches that can be used for text clustering and we discussed them: model based approaches and similarity based approaches. in general, strong clusters tend to show up no matter what method is u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ". main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. in fact, that's the goal of text clustering. we want to find such clusters in the data. but in the case of categorization, we are given the categories. so we kind of have predefined categories ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "at means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely like what we did for text clustering. namely, we are going to assign document d to the category that has the highest probability of generating this document. in other words, we're going ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "hniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data t",
        "label": "use"
      }
    ]
  },
  {
    "text": "word distributions",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ve n documents so we have n sets of pis. and each set of the pi values will sum to one. so this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. so how do we model the data in this way? and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "e model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ut the text, so in this case, what do we discover? well, these are represented by our parameters, and we have two kinds of parameters. one is the two word distributions. those are two topics and the other is the coverage of each topic in each. the coverage of each topic and this is determined by probability of theta ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "rested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of em algor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": ". those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. and the other is the word distributions that characterize all the topics. so the next line then is simply to plug this in to calculate the probability of document. this is again of the fami",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " as the algorithm, we first initialize all the unknown parameters randomly. in our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. and we just randomly normalize them. this is the initialization step, and then we will repeat until likelihood converges. now how do we know",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. and such detailed characterization of coverage of topics i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "s dirichlet prior dirichlet distribution based on this preference, then the only difference in the em algorithm is in the m step. when we re estimate word distributions, we are going to add. additional counts to reflect our prior right? so here you can see the pseudocounts are defined the based on the probability of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ver, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. so the inference part. again, face the lo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " remove the background model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also. and this is for convenience to introduce lda and we have one vector",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our inference on the k ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " you see very similar things. first you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions. and this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multipl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ke a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the k word distributions that we have. but this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this questio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "a i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ing that you have seen in document clustering. an we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. so this idea can be directly adapted to do categorization and this is precisely what naive ba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " now the question is, how can we make sure each theta i actually represents category i accurate? now, in clustering we learned this category i or the word distributions for category i from the data. but in our case what can we do to make sure this theta i represents indeed category i? if you think about the question ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lity of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. the other kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate thes",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the difference",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "e generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "he context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " different views associated with the each of the topics. and these are shown as view one, view two and view three each view is a different version of word distributions. and these views are tide to some context variables. for example, type to the location texas or the time july 2005 or the occupation of the other bei",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " could be from any of these contexts. let's say we have taken this view. that depends on the time in the middle. so now we have a specific version of word distributions. now you can see some probabilities of words for each topic. now, once we have chosen a view, now the situation will be very similar to what happened",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " we might pick a particular coverage, let's say in this case. we pick we've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in plsa. so what it means we're going to use the coverage to choose a topic to choose one of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "until we generate all the words and this is basically the same process as in plsa. now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. so in other words, we have extra switches that are tied to this context that would control the choices of d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "and this is. this topic is indeed very relevant, to both wars. if you look at the column further and what's interesting is that the next two cells of word distributions actually tell us collection specific variations of the topic of united nations. so it indicates that in iraq war, united nations was more involved in",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabili",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuitively ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ecial cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " document. so in this simple setup we are interested in analyzing one document and trying to discover just one topic. so this is the simplest case of topic modeling. the input now no longer has k, which is the number of topics because we know there is only one topic. and the collection has only one document al",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " solution of this function. so please take a look at this sum again here and this is a form of function that you often see later also in more general topic models. so it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document. and this is multiplied by the loga",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " common words. this way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. this way our target is the topic theta here would be only generating the content words that characterize the content of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the paramete",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixtu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "analysis or p lsa. in this lecture we're going to introduce probabilistic latent semantic analysis, often called the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "oing to introduce probabilistic latent semantic analysis, often called the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "t useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. here i show a sample article which is a blog article about hurricane ka",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ll these cases. and i have to stress again, this is a very important formula to know because. this is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "this lecture is about the latent dirichlet allocation or lda. in this lecture, we're going to continue talking about topic models. in particular, we are going to talk about some extensions of plsa, and one of them is lda or latent dirichlet allocation. so the plan for this lect",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ain, face the local maxima problem. so essentially they are doing something very similar, but theoretically lda is more elegant way of looking at the topic modeling problem. so let's see how we can generalize plsa to lda or extend the plsa to have lda now a full treatment of lda is beyond the scope of this cou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " world from that topic. so this is a very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of gettin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "lgorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to tak",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "c is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic topic model. and this is also often adequate for most applications. that's why we spend a lot of time to explain plsa in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "n, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic topic model. and this is also often adequate for most applications. that's why we spend a lot of time to explain plsa in detail. now lda improves over plsa by im",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? can we use phrases to label the topic to ma",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " may be able to cluster terms in this case. terms are objects. and cluster of terms can be used to define the concept or theme or topic. in fact, the topic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world dis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "egments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "uss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have input of text collection c and number of topics k and vocabulary v, and we hope to generate as output two things. one is a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "cument covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in g",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ith the. problem of mining. one topic that we discussed earlier. so here again it's a slide that you have seen before. and here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. but we can also consid",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " one topic, and we hope to embed such such preferences in a generative model. but if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic ins",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "r and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it lo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "uirement is how can we force every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it looks similar. so here we see that w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ecisely one topic. that means all the words in the document must have been generated from precisely one distribution, and this is not true for such a topic model that we're seeing here, and that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to gener",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ee the desicion of. of using a particular distribution is made of just once for this document. in the case of document clustering. but in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "nerator a document and that would allow us to recover the cluster identity over document so it would be useful to think about the difference from the topic model, as i have also mentioned multiple times. there are many. two differences. one is the choice of. using a particular distribution is made just once fo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "here are many. two differences. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a documen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "mes. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. multiple distribution could have been used to generate the words in t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probabilit",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "o the probability of the whole document is just a product of the probability of each word in the document. so this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose i am also copying the probability of. topic model with two components here. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "uld be very similar to the topic model, but it's also useful to think about the difference and for that purpose i am also copying the probability of. topic model with two components here. so here you can see at the formula looks very similar or in many ways they are similar. but there's also some difference. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "h this distribution to generate all the words. and that's why we had the product inside the sum. the sum corresponds to the choice. right. now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently. and that's why we have the product outs",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " using this distribution. note that it's important that we use this distributed generator. all the words in the document. this is very different from topic model, so the likelihood function would be like what you are seeing here. so the. you can take a look at the formula here. we have used the different. nota",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "x sub j to w is a change of notation, and this change allows us to show the estimation formulas more easily and you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words. i and so with the lack of functioning. now we can talk a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "lve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorith",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "w mixture model. so as you may recall, em algorithm starts with initialization of all the parameters. so this is the same as what happened before for topic models. and then we're going to repeat until their likelihood converges. an in each step will do e step and m step. in m step. we're going to infer which d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ure different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "cs factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation, but the representation can be map",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "d words and that would allow us to segment the text into segments. discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, but anyway, that's the first stage where we would obtain the counts of words in each segment. in the segmentation stage, whi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "odeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "uld be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assume",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "r letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model. to solve the problem using a unified model.",
        "label": "use"
      }
    ]
  },
  {
    "text": "syntagmatic relations",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ow to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "y using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. so we'll learn the structure and wh",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews. such a syntagmatic relations would help us show the detailed opinions about the product. so how can we discover such associations automatically? now, here are some intuitions abo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "what words tend to occur to the left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "t, you tend to see the other words. and if you don't see eat, probably you don't see other words often either. so this intuition can help us discover syntagmatic relations. now again, consider example- how helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence woul",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ccurrences with their individual occurrences. we're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " so with this modification, then the new function will likely address those two problems. now interestingly we can also use this approach to discover syntagmatic relations. in general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have hi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "e candidate word, for example, cat. so for this reason, the highly weighted terms in this idf weighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering the two relations. and indeed they can be discussed, discovered in a joint manner by leveragin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "5 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "scovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": " we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definition, syntagmatic relations hold between words that have correlated co occurrences. that means when we see one word occurs in the context, we tend to see the occurrence of the o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. now we address the different scenario ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ld be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "dict the one word given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this cond",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " example, it would be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ll entropy, meaning that it helps us predict the target word w1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with w1. note that we need to use a threshold to find these words. the threshold can be the number of top candidates to take or absolute value for th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "is would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the strongest k syntagmatic relations from entire collection. because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": ". because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in this case of discovering syntagmatic relations for a target word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable right",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "very. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ful for other applications as well. that's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. in particular, mutual information is a principled way for discovering such a relation. it allows us to have values computer on different pairs of wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word lik",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "lation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic relations. these are fairly general. they can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "s you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different k",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "e on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledg",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic models",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuitively w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ecial cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in j",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " solution of this function. so please take a look at this sum again here and this is a form of function that you often see later also in more general topic models. so it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document. and this is multiplied by the logar",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixtur",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "oing to introduce probabilistic latent semantic analysis, often called the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "t useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. here i show a sample article which is a blog article about hurricane kat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ll these cases. and i have to stress again, this is a very important formula to know because. this is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "this lecture is about the latent dirichlet allocation or lda. in this lecture, we're going to continue talking about topic models. in particular, we are going to talk about some extensions of plsa, and one of them is lda or latent dirichlet allocation. so the plan for this lectu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " world from that topic. so this is a very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of getting",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "lgorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to take",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " may be able to cluster terms in this case. terms are objects. and cluster of terms can be used to define the concept or theme or topic. in fact, the topic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world dist",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "uss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "lve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorithm",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "w mixture model. so as you may recall, em algorithm starts with initialization of all the parameters. so this is the same as what happened before for topic models. and then we're going to repeat until their likelihood converges. an in each step will do e step and m step. in m step. we're going to infer which di",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "cs factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation, but the representation can be mapp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "d words and that would allow us to segment the text into segments. discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, but anyway, that's the first stage where we would obtain the counts of words in each segment. in the segmentation stage, whic",
        "label": "use"
      }
    ]
  },
  {
    "text": "generative model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "lem. indeed, you can write a heuristic program to solve this problem, but here we're going to introduce a general way of solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the pict",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "m text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ics that we are discovering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they are generated this way and i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "chanism for generating text. and that just means we can view text data as data observed from such a model. for this reason, we also call such a model generative model. so now given a model, we can then sample sequences of words. so for example, based on the distribution that i have shown here on this slide, we migh",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "owledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood fun",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " topic 100%. so the main goal is just to discover the word probabilities for this single topic, as shown here. as always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the dat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " together as one model. so i use the box to bring all these components together. so if you view this whole box as one model, it's just like any other generative model. it would just give us the probability of a word. but the way that determines this probability is quite different from when we have just one distribu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration that you have seen before, which is dimmer now is just the illustrat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "hich is dimmer now is just the illustration of this generation model. so mathematically, this model. this is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. the form you're seeing now is more general form than. what ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ome sense a user controlled plsa, so it doesn't blindly just listen to data but also would listen to our needs. the second is to extend the plsa as a generative model fully generated model. this has led to the development of latent dirichlet allocation or lda. so first let's talk about the plsa with prior knowledge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ther cases or if we care about generality, we would worry about this over fitting. so lda is proposed to improve that and it basically to make plsa a generative model by imposing a dirichlet prior on the model parameters. dirichlet is just a special distribution that we can use to specify prior. so in this sense, l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "el to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. so in this case it's a cluster",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "t clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. so in this case it's a clustering structure. the topics and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "del. so in this case it's a clustering structure. the topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. but if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requir",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ing which distribution should be used to generate the world, and then we're going to use this distribution to sample word. now. notice that in such a generative model. the decision on which distribution to use for each word is independent, so \"that means, for example, \"\"the\"\" here could\" have been generated from th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "distribution based on one document. so that's the connection that we discussed earlier. but now you can see more clearly. so as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model. but once we design model, the next step is to write down the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "cture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "y based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "e clustering structure is built into a generated model. that's why we can use potentially a different model to recover different instruction. complex generative models can be used to discover complex clustering structures. we did not talk about it, but we can easily design generated model to generate a hierarchical",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ion learning. and then we can certainly cluster terms based on actually their tax representations. of course, term clusters can be generated by using generative models as well as we have seen.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. deciding the optimal number of clusters is",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "sing machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to documen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ure model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generative model for documents in each category. can vary, and here we just talk about a very simple case. perhaps the simplest case. so now the question is, how can ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ssifying new objects. so that's the basic idea of sven. so to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "t aspects and then we use a little regression model to learn the aspect ratings and letting the weights. now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "l the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a generative model like psa. and then we can then plug in the latent regression model to use the text to further predict the overall rating and that means we first pred",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "hat means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. so this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. so we don't have time to discuss this model in detail, as in man",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ere is a reference site here where you can find more details. so now i'm going to show you some simple results that you can get by using this kind of generative models. first it's about rating decomposition. so here what you see are the decomposed ratings for three hotels that have the same overall rating. so if yo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ng very interesting because this is in some sense some byproduct in our problem formulation. we did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. and you can see the highly weighted words versus the negatively lowe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "uct laptop, the word long is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "nd we also need to consider the order of those categories and we talk about the ordinal regression. for solving this problem. we have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem. the next two papers are about the generative models for letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model wh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. one is to assume that depending on the context depending on dif",
        "label": "intro"
      }
    ]
  },
  {
    "text": "background model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "'t know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust theta sub d in order to maximize the probability of the observed document here and we ass",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ery naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. so we further assume that the background model gives probability of point nine to the word the and text point one. now, let's also assume that our data is extremely simple. the document has just t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ext is now much larger than probability of the. this is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. and if you look at the equation, you will see obviously some interaction of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ther distributions to do the same, and this is to balance them out so that we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probab",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "on to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. meaning that they have a very small probability from the background model, like a text here.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "probability mass on the content words that cannot be explained well by the background model. meaning that they have a very small probability from the background model, like a text here.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "hat gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the probability of choosing a background model. now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "um likelihood\u00a0 estimator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actuall",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " likely from\u00a0 theta sub d, why? and you will probably see that it's because. text has a much higher probability here. by the theta sub d, than by the background model, which has a very small probability. and by this we are going to say text is more likely from theta sub d. so you see our guess of which distribution",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": ", so we also need to compare these two priors why? because imagine if we adjust these probabilities, we're going to say the probability of choosing a background model is almost 100%. now if we have that kind of strong prior, then that would affect your guess. you might think well, wait a moment, maybe text could ha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ribution. so when we multiply the two together, we get the probability that text has in \"fact has been generated from theta sub d\u00a0 similarly, for the background model an. the probability of generating text is another product of similar form. we also introduced a latent variable z here to denote whether the word is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "e improved parameters from here to. here we have improvement. so in this setting we have assumed that the two models have equal probabilities and the background model is known. so what are the relevant statistics? well, these are the word counts. so assume we have just 4 words and their counts are like this and thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": ". so what are the relevant statistics? well, these are the word counts. so assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. an in the first iteration you can picture what would happen. well, we first we initialize a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "it's generally a sum over all the different possibilities of generating the world. so let's first look at the how the world can be generated from the background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " the world. so let's first look at the how the world can be generated from the background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the back",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the background model. and that's probability of lambda sub b and then the second we mus",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "odel is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the background model. and that's probability of lambda sub b and then the second we must have actually obtained the world w from the background, and that's probability of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "out that. but here i just want to give you a brief idea about what's the extension and what it enables. so this is a picture of lda. now i remove the background model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now r",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "'ll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ge model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the difference of the probability of such words smaller across categories. because every category has some help from their backg",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "o longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary. so each word has the same probability. the",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood estimator",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "e will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviousl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "r prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informative prior,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "y assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see la",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that look",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "g a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constrain",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ment and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "eters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ies than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "vior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on differ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "h a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat higher pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in the m step, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "odel to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ght have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have some expe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the prior calle",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": ". now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ies of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimate the para",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most popular, then",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard opt",
        "label": "use"
      }
    ]
  },
  {
    "text": "similarity function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "arity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent the context and then define the similarity function? so first we note that the context actually contains a lot of words. so they can be regarded as a pseudo document. an imaginary document. but there a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "sure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. and of course we can also assign weights to these different similarities to allow us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "in idea for discovering paradigmatically related words is to compute the similarity of their context. so next, let's see how we exactly compute these similarity functions. now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. now those of you who have bee",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "wo vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors. now it's interesting to see that this similarity function actually has a nice interpretation. and there is this dot product\u00a0 infact gives us the probability that two randomly picked words from the two contex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "e original data set, then it would still be frequently in the collected context documents. so how can we add these heuristics to improve our..... our similarity function. here's one way, and there are many other ways that are possible. but this is a reasonable way where we can adapt the bm25 retrieval model for paradi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "al idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "oaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "r, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. so once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups. anne, try to maximize the intragroup simi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "omerative clustering or agency, the other is k-means. so first let's look at the agglomerative hierarchical clustering. in this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together. and then was going to see which pair is. the next one to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity. we are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups. and there are also different ways to do tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "od for similarity based classroom in this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by just selecting kate randomly selected vectors as centroids of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ummarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "d approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is built into a generated model. that's why we can use potentially a different model",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "efinition of similarity into such a model. we also talked about the similarity based approaches. these approaches are more flexible. directly specify similarity functions. but one potential disadvantage is that their object function is not always very clear. the k means algorithm has a clearly defined the objective fu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "tured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "l given data that is p of y given x. now i'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "e proceed, let me emphasize that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "der for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our insights about features. basically, effective features are ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "n. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our insights about features. basically, effective features are those that would make the obje",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "hat would make the objects that are in the same category look more similar, but distinguishing objects in different categories. so the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how k-nn works. suppose we have a lo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "rning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could n",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "er this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could not capture that. obviously t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could not capture that. obviously the assumption would be a problem, and then the classifier would not be accurate. let's proceed with this assumptio",
        "label": "intro"
      }
    ]
  },
  {
    "text": "scoring function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "an it brings down the weight of really those really high counts. and this is what we want, because it prevents that kind of terms from dominating the scoring function. now there is also another interesting transformation called a bm25 transformation which has been shown to be very effective for retrieval and in thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " will now have a somewhat lower weights and this would help control the influence of these high frequency terms. now the idf can be added here in the scoring function. that means we'll introduce weight for matching each term. so you may recall this sum indicates all the possible words that can be a overlap between ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": " phrases. let's say the simplest solution is to just take each word as a term. these words then become candidate topics. then we're going to design a scoring function to measure how good each term is as a topic. so how can we design such a function? well, there are many things that we can consider. for example, we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ic. so how can we design such a function? well, there are many things that we can consider. for example, we can use pure statistics to design such as scoring function. intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. so that would mean we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "resent a lot of content in the collection. so that would mean we want to favor a frequent term. however, if we simply use the frequency to design the scoring function, then the highest scored terms would be \"general terms or functional terms, like \"\"the\"\", \"\"a\"\"\" etc. those terms are very frequent in english. so we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "s about the discovery of word associations. so these are statistical methods, meaning that the function is defined mostly based on statistics. so the scoring function would be very general. it can be applied to any language and any text. but when we apply such an approach to a particular problem, we might also be a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ashtags which are invented to denote topics. so naturally hashtags can be good candidates for representing topics. anyway, after we have designed the scoring function, then we can discover the k topical terms by simply picking k terms with the highest scores. now of course we might encounter a situation where the h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "at is to do a greedy algorithm, which is sometimes called maximal marginal relevance ranking. basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the k topical terms. the first term of course will be picked when we pick the next term. we're going to look at th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "a document and then choose the category with the highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "right, so it's suppose we have just two categories and we're going to score based on their ratio of probability, so this is ann let's say this is our scoring function for two categories. so this is a score of a document for these two categories. and we're going to score based on this probability ratio. so if the ra",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "d small probabilities, and this would then give us this formula in the second line. and here we see something really interesting, because this is our scoring function for deciding between the two categories. and if you look at this function, we'll see it has several parts. the first part here is actually log of pri",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ight. tells us. to what extent observing this word helps contributing to our decision to put this document in category one. i remember the higher the scoring function is more likely it's in category one. now if you look at this ratio basically or sorry this weight it's basically based on the ratio of the probabilit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "that we think are relevant for categorization. for example document length or the font size or counts of other patterns in the document. and then our scoring function can be defined as a sum of constant beta zero and sum of the feature weights over all the features. so if hf sub i is a feature value then we multipl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "parameters here. so what are the parameters? well these betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "on of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here. now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. of course the features don't have to be a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "re weights are the log of probability ratios of the word given by two distributions here. now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. of course the features don't have to be all the words and their features can be othe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "re are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined scoring function to help us classify a document right? so what's the function? well, it's this one. if we have all the betavalues already known, all we need is to com",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " least .5 above and now is equivalent to whether the score of the object that is. larger than or equal to negative of \u00a0alpha_j as shown here. now the scoring function now is just taking linear combination of all the features weighted by beta values. so this means now we can simply make a desicion of rating by looki",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ar combination of all the features weighted by beta values. so this means now we can simply make a desicion of rating by looking at the value of this scoring function and see which bracket it falls into. now you can see the general decision rule is thus when the score is in the particular range of our values, then ",
        "label": "use"
      }
    ]
  },
  {
    "text": "parameter values",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "del to our data, meaning that we can estimate the parameters or infer the parameters based on the data. in other words, we would like to adjust these parameter values until we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilitie",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": ". in other words, we would like to adjust these parameter values until we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is what parameter values will give our data set the highe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is what parameter values will give our data set the highest probability. so i also illustrate the problem with the picture that you see here. on the x axis, i just illustrate",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ested in, and then we model the data. we adjust the parameters to fit the data as well as we can. after we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " adjust the parameters to fit the data as well as we can. after we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. by varying the model, of cours",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "re generated this way and inside the model, we embed some parameters that were interested in denoted by lambda. and then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the lambda star as knowledge discovered from the text for our problem, and we can adjus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " compromise of the prior mode and the maximum likehood estimate. in general, in bayesian inference we are interested in the distribution of all these parameter values. as you see, here is there's a distribution over theta values that you can see here p of theta given x. so the problem of bayesian inference is to in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of parameter values, this function can tell us which x, which data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or inf",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " bayes rule. but this step is very crucial for understanding the em algorithm. because if we can do this, then we would be able to 1st initialize the parameter values somewhat randomly, and then we're going to take a guess of these z values and or which distribution has been used to generate which word and the init",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "t randomly, and then we're going to take a guess of these z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply bayes rule to infer which distribution is more",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "elp us re estimate the parameters. that were interested in so these will help us re estimate these parameters. but note that before we just set these parameter values randomly, but with this guess we will have a somewhat improved estimate of this. of course, we don't know exactly whether it's zero or one, so we're ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "e this lower bound. so in our example, the current gas is parameter value given by the current generation and then the next guest is the re estimated parameter values. from this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck ther",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "mate of parameters. here improve is guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. there are some properties that have to be satisfied in order for the parameters also too",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "el set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": ". otherwise it's going to say it's in the other category. so this is our assumption or setup. so in the linear is uvm, we're going to then seek these parameter values to optimize the margins and then the training error. the training laid out would be basically like a in other classifiers we have a set of training p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "e to be large. and here we just choose threshold one here. but if you use another threshold, you can see you can easily affect that constant into the parameter values b&w to make the right hand side. just one. now, if, on the other hand, why i is negative one that means it's in a different class then we want this c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " will assign the corresponding rating to that text object. so in sum, in this approach we're going to score the object. by using the features and the parameter values, beta values. and this score will then be compared with a set of training the other values to see which range the score is in, and then using the ran",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "and then we're going to set up a generation probability for the overall rating given the observed words. and then of course, then we can adjust these parameter values including betas, rs, alpha i. in order to maximize the probability of the data in this case, the conditional probability of the observed rating given",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "e word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "this lecture is\u00a0 about topic mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigm",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "bout mining another kind of knowledge, which is content mining and trying to discover knowledge about. the main topics. in the text. and we call that topic mining and analysis. in this lecture we're going to talk about its motivation and the task definition. so first, let's look at the concept of topic. so topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "ight know some insights about people's opinions in different locations. so that's why mining topics is very important. now let's look at the tasks of topic mining and analysis. in general, it would involve first discovering a lot of topics. in this case k topics. and then we also would like to know which topics",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "this lecture is about the topic mining and analysis. we are going to talk about using a term as topic. this is a slide that you have seen in the earlier lecture where we defined the task o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ht also discover document 2 does not cover sports at all, so the coverage is zero, etc. so now of course, as we discussed. in the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. so let's first think about how we can discover",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "nt values, it will give some data points higher probabilities than others. now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. first, we have theta_i's each is a word distribution and then we have a set of pi's for each document. and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "escribe a complicated topic. it also allows us to assign weights on words so we can model subtle variations of semantics. we talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. the number of topics and vocabulary set and the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "parameters. so this is a general illustration of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "this lecture is the first one about the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are int",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "chnique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e real world. that may not be directly related to the text, or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with char",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " can design for text data. it has also been addressed to some extent by talking about the other knowledge that we can mine from text. so for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. so topics can be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "on. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer values of real world variables. but in order to ac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "der to achieve the goal, we can do some other preparations and these are sub tasks. so one sub task could be mine, mine the content of text data like topic mining. and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. and both can help provide predictors for the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "lso add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. and such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of op",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "textual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinio",
        "label": "use"
      }
    ]
  },
  {
    "text": "paradigmatic relation",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "alk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have pa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic class or syntactic",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "on, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the other without affecting the understanding of the sen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "hout affecting the understanding of the sentence. that means we would still have a valid sentence. for example, cat and dog. and these two words have paradigmatic relation because they are in the same class of animal. and in general, if we replace cat with dog in a sentence, the sentence would still be a valid sentence ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "if we replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of. similarly, monday and tuesday have paradigmatic relation. the second kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ive in a sentence to still get a valid sentence. meaning that if we do that, the sentence will become somewhat meaningless. so this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n phrase. if you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. so they oc",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r synonyms, for example, and then you can help a lot of tasks. and grammar learning can be also done by using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for pu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "bout the product. so how can we discover such associations automatically? now, here are some intuitions about how to do that. let's first look at the paradigmatic relation. here we essentially can take advantage of similar context. so here you see some simple sentences about cat and dog. you can see they generally occur",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " here you see some simple sentences about cat and dog. you can see they generally occur in similar context. and that, after all, is the definition of paradigmatic relation. so on the right side you can see i extracted explicitly the context of cat and dog from this small sample of text data. so i have taken away cat and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "around this word. and even in the general context you also see some similarity between the two words. so this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. so for example, if we think about the following questions, how similar are context of cat and conte",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "etween the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship. and then imagine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic idea of discovering paradigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definiti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ome other different ways. sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much rela",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "orld, then you likely will capture words that are very much related by their syntactical categories and semantics. so the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ntext, so d2. and then we can measure the similarity of these two vectors. so by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. so the two questions that we have to address is first how to compute each ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ell. now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations. but analytically, we can also analyze this formula little bit. so first, as i said, it does make sense right? because this formula will give a high",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "in this lecture, we continue discussing paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "nction. here's one way, and there are many other ways that are possible. but this is a reasonable way where we can adapt the bm25 retrieval model for paradigmatic relation mining. so here we define in this case we define the document vector. as containing elements representing normalized bm 25 values. so in this normali",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discoverin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ions. and indeed they can be discussed, discovered in a joint manner by leveraging such associations. so to summarize, the main idea for discovering\u00a0 paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. and then compute the s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " similarity of the corresponding context documents of two candidate words. an then we can take the highly similar word pairs and treat them as having paradigmatic relations. these are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "s design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "o represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "arrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "c mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ated by representing each term with some text content. for example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. and then we can certainly cluster terms based on actually their tax representations. of course, term clusters can be generated by using gen",
        "label": "use"
      }
    ]
  },
  {
    "text": "sentiment analysis",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ry useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text based prediction problems w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "hese words can be used to form topics. when we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. so representing text data as a sequence of words opens up a lot of interesting analysis possibilities. however, this level of representation is slig",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "presentation. it's quite general and relatively robust. it can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. for example, thesaurus discovery has to do with discovering related wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. because we can categorize the authors, for example, based on th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ng design. the words. as features. but deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generall",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "this lecture is about opinion mining and sentiment analysis covering its motivation. in this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "owledge, namely knowledge about the observer or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. in contrast, we have other devices such ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "s talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-grams. you can just have a sequence of characters as a unit, and they can be mixed with d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "e words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. for example, we might see a sentence like it's not good or it's not as good as something else. so in such a case, if you just take a good an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "t of speech tags. for example, the word great might be followed by a noun and this could become a feature, a hybrid feature. that could be useful for sentiment analysis. so next we can also have word classes, so these classes can be syntactic like a part of speech tags. or could be semantic and they might represent c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "this lecture is about the ordinal logistic regression for sentiment analysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we have an opinionated text d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ut the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ers better. so to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature represen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflects what we know about t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other questio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "erve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer values of real world variables. but in order to achieve the goal, we can do some other preparations and these are s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "tasks. so one sub task could be mine, mine the content of text data like topic mining. and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. and both can help provide predictors for the prediction problem. and of course we can also add non text data directly to the pre",
        "label": "use"
      }
    ]
  },
  {
    "text": "logistic regression",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ere's a strong connection close connection between the two kinds of approaches, and this slide shows how naive bayes classifier can be connected to a logistic regression. and you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to solv",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "atures don't have to be all the words and their features can be other signals that we want to use. and we mentioned that this is precisely similar to logistic regression. so in this lecture we're going to introduce some discriminative classifiers. they try to model the conditional distribution of labels given the data",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very closed, closely related to the log or log odds that i introduced i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "now that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way. so this is a log odds ratio. here. and so in logistic regression, we basically assume that the probability of y = 1\u00a0 given x is dependent on this linear combination of all these features. so it's just one of the ma",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "you can see, and that's precisely what we want. since we have a probability here. and the function form looks like this. so this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "milar, but distinguishing objects in different categories. so the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how k-nn works. suppose we have a lot of training instances here. and i've colored them differently and t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ere. now the idea of this classifier is do design. also a linear separator. here that you see and it's very similar to what you have seen or just for logistic regression. and we're going to also say that if the sign of this function value is positive, then we're going to say the object is in category 1. otherwise, we'",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "this lecture is about the ordinal logistic regression for sentiment analysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we hav",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it posi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "fferent approaches, and here we are going to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to be a linear function of these features as shown here. so this would allow us to a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "t application of logistical regression for binary categorization. what if we have multiple categories, multiple levels? we actually use such a binary logistic regression program to solve this multi level rating prediction. and the idea is we can introduce multiple binary classifiers and each case we ask the classifier",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "e just seen on the previous slide. only that here we have more parameters because for each classify we need a different set of parameters. so now the logistic regression classifiers indexed by j, which corresponds to a reading level. and i have also used offer subject to replace beta 0. and this is to make the notatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ng level. and i have also used offer subject to replace beta 0. and this is to make the notation more consistent with what we can show in the ordinal logistic regression. so anyway, so here we now have basically k - 1 regular logistic regression classifiers. each has its own set of parameters. so now with this approac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "to make the notation more consistent with what we can show in the ordinal logistic regression. so anyway, so here we now have basically k - 1 regular logistic regression classifiers. each has its own set of parameters. so now with this approach we can now do rating prediction as follows. after we have trained these k ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ssifiers. each has its own set of parameters. so now with this approach we can now do rating prediction as follows. after we have trained these k - 1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision. so first let's loo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ng higher and for any of these classifiers, for all these classifiers. so we should be able to take advantage of this factor. now the idea of ordinal logistic regression is precisely that\u00a0 a key idea is just the improvement over the k -1 independent logistical regression classifiers, and that idea is to tie these beta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ave n + k - 1 because we have m beta values and plus k minus one alpha values. so that's just the basically that's basically the main idea of ordinal logistic regression. so now let's see how we can use such a method to actually assign ratings. it turns out that with this. idea of tying all the parameters, the beta va",
        "label": "intro"
      }
    ]
  },
  {
    "text": "data mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowled",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ecially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. so a more general picture would be to include non text data as well. and for this rea",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "give us different kinds of associations. these discovery associations can support them. any other applications in both information retrieval and text data mining. so here are some recommended readings. if you want to know more about the topic, the 1st is a book with a chapter on locations which is quite releva",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "c. or we are interested in knowing about the research topics. for example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago. now this involves discovery of topics in data mining, literatures and also we want to discover ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "ng what are the current research topics in data mining and how are they different from those five years ago. now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past. and then we can make a comparison. we might be also int",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are intereste",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "alk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have learned in some other courses. the idea is to discover natural structures in the data. in other words, we want to group similar ob",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analysis. and that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "echniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested in doing that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e get sophisticated patterns of text together with other kinds of\u00a0data? \" it would be useful to first take a look at the big picture of prediction in data mining in general and i call this\u00a0data mining loop. \" so the picture that you're seeing right now is that there are multiple sensors, including human sensor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "gether with other kinds of\u00a0data? \" it would be useful to first take a look at the big picture of prediction in data mining in general and i call this\u00a0data mining loop. \" so the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the re",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " that. we might want to make decisions based on that. so how can we get from the data to these predicted values? well, in general we first have to do data mining and analysis of the data. because we in general should treat all the data that we collected. in such a prediction problem set up, we are very much in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "n consuming or interpreting text data. but when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. \" sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "n controlling the sensors. and, this is so that we can adjust the sensors to collect the most useful data for prediction. so that's why i called this data mining loop because as we perturb the sensors to collect the new data and more useful data then we will obtain more data for prediction. this data generally",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " for machine learning programs if you can label them, right. so in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the world and then observe what happens. and then you can then decide what additional data. ha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "on of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "y of work in this direction, and we're going to highlight some of them in the next lectures. now the other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text in the context of time. so time is a context in this case. is there any diff",
        "label": "use"
      }
    ]
  },
  {
    "text": "syntagmatic relation",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ow to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "still be a valid sentence that you can make sense of. similarly, monday and tuesday have paradigmatic relation. the second kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so a&b have syntagmatic relation if they can be combined with e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ond kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so a&b have syntagmatic relation if they can be combined with each other in a sentence. that means these two words are semantically related. so for example, cat and sit are related b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r locations in a sentence or in a sequence of data elements in general. so they occur in similar locations relative to the neighbors in the sequence. syntagmatic relation on the other hand, is related to co-occurring elements that tend to show up in the same sequence. so these two are complementary and basically relati",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "y using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. so we'll learn the structure and w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews. such a syntagmatic relations would help us show the detailed opinions about the product. so how can we discover such associations automatically? now, here are some intuitions ab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n general they will be very different from what words occur after cat. so this is the basic idea of discovering paradigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of te",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "radigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of text. but here we are interested in knowing what other words are correlated with the verb eats. and what words can g",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "what words tend to occur to the left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "uld help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? so the question here has to do with whether there are some other words that tend to co-",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "t, you tend to see the other words. and if you don't see eat, probably you don't see other words often either. so this intuition can help us discover syntagmatic relations. now again, consider example- how helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence wou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "in the sentence. so this is in contrast to the question about eats and meat. this also helps explain the intuition behind the methods for discovering syntagmatic relation. mainly we need to capture the correlation between the occurrences of two words. so to summarize, the general ideas for discovering word associations",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. and we're going to compare",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ccurrences with their individual occurrences. we're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "rences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be assoc",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "lone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " so with this modification, then the new function will likely address those two problems. now interestingly we can also use this approach to discover syntagmatic relations. in general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "e candidate word, for example, cat. so for this reason, the highly weighted terms in this idf weighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering the two relations. and indeed they can be discussed, discovered in a joint manner by leveragi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "5 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discov",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "scovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by defini",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": " we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definition, syntagmatic relations hold between words that have correlated co occurrences. that means when we see one word occurs in the context, we tend to see the occurrence of the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to tal",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. now we address the different scenario",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ld be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "dict the one word given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this con",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " example, it would be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ll entropy, meaning that it helps us predict the target word w1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with w1. note that we need to use a threshold to find these words. the threshold can be the number of top candidates to take or absolute value for t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "is would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the strongest k syntagmatic relations from entire collection. because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": ". because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in this case of discovering syntagmatic relations for a target word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable righ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to dis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "very. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really com",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be f",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "why in the denominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ful for other applications as well. that's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. in particular, mutual information is a principled way for discovering such a relation. it allows us to have values computer on different pairs of w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word li",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "lation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic relations. these are fairly general. they can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entitie",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "s you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "e on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowled",
        "label": "use"
      }
    ]
  },
  {
    "text": "objective function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "int the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. the objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. so one way to solve the problem is to use lagrange mu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "tion to this for those of you who are interested. so in this approach we will construct a lagrange function here. and this function would combine our objective function with another term that encodes our constraints. and we introduce lagrange multiplier here, lambda. so it's additional parameter. now the idea of this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "et high probabilities on different words to avoid this competition in some sense. or to gain advantage in this competition. so again, looking at this objective function and we have a constraint. on the two probabilities. now. if you look at the formula intuitively, you might feel that you want to set the probability ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you can imagine, it would make sense to actually assign a smaller pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "rity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are simi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "sed on the allocated objects in each cluster. and this is. to adjust the centroid and then we had repeated this process until the similarity based on objective function. in this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective function. given k clusters. so it can be also shown this process will converge to a local minimum. i think about this process for a moment. it might remind yo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "nsider all the data points. based on probabilistic allocations. but in nature they are very similar and that's why it's also maximizing where defined objective function and it's guaranteed to convert converted local minimum. so to summarize our discussion of clustering methods, we first discussed the model based appr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ity functions. but one potential disadvantage is that their object function is not always very clear. the k means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach. the hierarchical clustering algorithm, on the other hand, is. it's harder to. to specify the o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "n, but it's also very similar to a model based approach. the hierarchical clustering algorithm, on the other hand, is. it's harder to. to specify the objective function so it's not clear what exactly is being optimized. both approaches can and generate the term clusters and document clusters. an term clusters can be ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "d matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they mig",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ant thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. so in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors. but if we can model the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " the problem. so these discriminative classifiers attempted to model the. conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "problem, right? we have some variables to optimize and these are the weights and b and we have some constraints. these are linear constraints and the objective function is a quadratic function of the weights. so this is a quadratic program with linear constraints and there are standard algorithms that are available f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this to happen. so we want to then also minimize this ci. so cassie, i needs to be minimized in order to control the error. and so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "t's not very good of course, so see should be set to a non 0 value and a positive value. but when she is settled very, very large value would see the objective function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role. so if that happens, what would hap",
        "label": "use"
      }
    ]
  },
  {
    "text": "bayes rule",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ok at the prior so the prior here is defined by p of theta. and this means we will impose some preference on certain thetas over others. and by using bayes rule that i have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "wn here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full explanation of bayes rule and some of these things related to bayesian reasoning would be outside the scope of this course, but i just give a brief introduction because this i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ld be outside the scope of this course, but i just give a brief introduction because this is a general knowledge that might be useful for you, so the bayes rule is basically defined here. and allows us to write down one conditional probability of x given y in terms of the conditional probability of y given x.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "updating formula is based on the combination of our prior here and the likelihood of observing this y if x is indeed true. so much for a detour about bayes rule. so in our case, what we're interested in is inferring the theta values so we have a prior here. that includes our prior knowledge about the paramete",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ere? while the evidence here is the word text. now that we are interested in the word text, so text can be regarded as evidence. and in the if we use bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basica",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "sub d or from theta sub b. and equivalently, the probability that z is equal to 0 given that the observed evidence is text. so this is application of bayes rule. but this step is very crucial for understanding the em algorithm. because if we can do this, then we would be able to 1st initialize the parameter v",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "hich word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply bayes rule to infer which distribution is more likely to generate each word and this prediction essentially helped us to separate words from the two distributio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ters are mainly the probability of a word given by status update. so this is the initialization stage. it is initialized values would allow us to use bayes rule to take a guess of these z values. so will guess these values we can say for sure whether taxes from background or not, but we can have our guesses. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " on the document. and that might give a different guess of word for word in different documents, and that's desirable. in both cases we are using the bayes rule as i explained, basically assessing the likelihood of generating word in from each distribution and is normalized. what about the m-step? well, we ma",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "able could take a value from the range of one through k representing k different distributions. and more specifically, basically we're going to apply bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. given the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ng to take two values, one and two to indicate the two topics. so now how do we infer which distribution has been used to generate the d? it's to use bayes rule so it looks like this in order for the first topic is setup, want to generate the document. two things must happen. first theater subway must have be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. that we need to consider if a topic or cluster has a higher prior then it's more",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e we observe any document. but this conditional probability here is the posterior probability of the topic after we have observed the document d. and bayes rule allows us to update this probability based on the prior and i shown the details. below here you can see how the prior here is related to the posterio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "as the highest score by this function. so this is called a naiyes bayes classifier. now the keyword bayes is understandable because we are applying a bayes rule here. when we go from the posterior probability of the topic to a product of the likelihood and the prior. now it's also called a naive because we've",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "age model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generative model for documents in each category. can vary, and here we just talk about a very simple case. perha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "arger \u00a0then it means it's more likely to be in category one, so the larger the score is, the more likely the document is in category one. so by using bayes rule we can write down this ratio as follows and you have seen this before. now, we generally take logarithm of this ratio and to avoid small probabilitie",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "sifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "oing to introduce some discriminative classifiers. they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary res",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "two categories that you have seen on the previous slide. so that this is what i meant, right? so in the case of naive bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this. that looks like this. but here we actually would assume explicitly that we would model ",
        "label": "use"
      }
    ]
  },
  {
    "text": "opinion holder",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "d to prove whether it's wrong or correct. so opinion is subjective statement. and next, let's look at the keyword person here and that indicates this opinion holder 'cause when we talk about opinion, it's about the opinion held by someone and then we notice that there is something here. so that's the target of th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ion. so what's the basic opinion representation like, well,\u00a0 it should include at least three measurements, right? first it has to specify what's the opinion holder. so whose opinion is this, second must also specify the target. what's this opinion about? and 3rd, of course we want opinion content and so what exa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " what time was it expressed? we also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? or perhaps the opinion holder was happy or sad. and so such understanding obviously goe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "his is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? or perhaps the opinion holder was happy or sad. and so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. so let's take a simple",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "just extracting the opinion content and needs some analysis. so let's take a simple example of a product review. in this case, this actually explicit opinion holder and explicit target, so it's it's obviously what's opinion holder, and that's just a reviewer, and it's also often very clear what's the opinion targ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "t's take a simple example of a product review. in this case, this actually explicit opinion holder and explicit target, so it's it's obviously what's opinion holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. for example iphone 6. w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ditional understanding of course adds value to mining the opinions. now you can see in this case the task is relatively easy, and that's because. the opinion holder and opinion target that have already been identified. now let's take a look at the sentence in the news. in this case, we have implicit holder and im",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "e a look at the sentence in the news. in this case, we have implicit holder and implicit target. and the task is in general harder so we can identify opinion holder here and that's governor of connecticut. we can also identify the target. so one target is hurricane sandy. but there is also another target, which i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "o some other interesting variations. in fact, here we're going to examine the variations of opinions more systematically. first, lets think about the opinion holder. now the holder could be an individual or could be a group of people and sometimes opinion was from a committee or from a whole country of people. op",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "e opinion like a whole article. and furthermore, we can identify the variation in the sentiment or emotion dimension. that's about the feeling of the opinion holder. so we can distinguish positive versus negative or neutral or happy versus sad, etc. finally, the opinion context can also vary. we can have simple c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "n verify that. but from this statement one can also infer some negative opinions about the quality of the battery of this phone or the feeling of the opinion holder about the battery. in the opinion, holder clearly wish the battery to last longer. so these are interesting variations that we need to pay attention ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "of opinion mining can be defined as taking text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, target content and context. ideally we can also infer opinion sentiment from the content and context to better understand the opinion. now often som",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "nd the opinion. now often some elements of the representation are already known. i just gave a good example, in the case of product reviews where the opinion holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. now it's intere",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ion representation are already known, then our only task maybe just the sentiment classification as shown in this case. so suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. then we mainly need to decide the opinion sentiment of the review",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " have categories such as positive, negative or neutral. the other is emotion analysis. that can go beyond polarity to characterize the feeling of the opinion holder. in the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web. five might denote the most ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " fitting the model to the data. most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "plications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. so here are some sugg",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "hich still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflects what we know about the opinion holder. but this only provides limited view of what we can predict. in this lecture and the following lectures, we're going to talk more about how we can pr",
        "label": "use"
      }
    ]
  },
  {
    "text": "naive bayes",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "directly captures the training errors. but if we can model the data in each category accurately, then we can also classify accurately. one example is naive bayes classifier. in this case. the other kind of approaches are called discriminative classifiers. these classifiers try to learn what features separate c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ibutions for these categories and the prior on these categories. so this idea can be directly adapted to do categorization and this is precisely what naive bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if thet",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ount would be 0, so this count would just disappear. so effectively we're just have the product over all the words in the document. so basically with naive bayes classifier, we're going to score each category for a document by this function. now you may notice that here it involves the product of a lot of smal",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "rs. and this is the problem that you have seen. also several times in this course. now, if you haven't thought about that this problem, haven't seen\u00a0 naive bayes classifier, it would be very useful for you to pause the video for a moment and to think about how to solve this problem. so let me state the problem",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "on and you will know how to normalize, this counts and so this is a good exercise to work on it if it's not obvious to you. there is another issue in naive bayes which is a smoothing. in fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ategory for a document and then choose the category with the highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio. here in naive bayes. and then finally we have this constant of bias here, so that formula actually is a formula that can be generalized to accommodate more features. and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier cal",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "es later, but here i want you to know that there's a strong connection close connection between the two kinds of approaches, and this slide shows how naive bayes classifier can be connected to a logistic regression. and you can also see that in discriminative classifiers that tend to use a more general form on",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "o continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring func",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "y to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary response variable y here, on some predictors. that are denoted",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "s theta 1 now the goal here is to model the conditional probability of y given x directly as opposed to model the generation of x&y as in the case of naive bayes. and another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. since we're not mode",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ssumed functional form of y depending on x is the following, and this is very closed, closely related to the log or log odds that i introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide. so that this is what i meant, right? so in the case of na",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "es or log of probability ratio of the two categories that you have seen on the previous slide. so that this is what i meant, right? so in the case of naive bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this. that looks like this. but here we actually would a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " allow you essentially to also pick up a useful words in the unlabeled data. you can think of this in another way. basically, we can use, let's say a naive bayes classifier to classify all the unlabeled text documents. and then we're going to assume the high confidence classification results, or actually relia",
        "label": "use"
      }
    ]
  },
  {
    "text": "background language",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ive. this is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "arize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "nguage models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of mod",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ne dominate. in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distributi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole se",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "l in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background lang",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ground language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of ba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essential",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "k about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these cat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "mate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think abo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "seful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and ",
        "label": "use"
      }
    ]
  },
  {
    "text": "common words",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "the word occurs in the context. but we can't just say any frequent term in the context that would be correlated with the candidate word. because many common words like 'the' will occur frequently in all the context. but if we apply idf weighting as you see here, we can then we weight these terms based on idf th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "text mining paper. now what you might see is something that looks like this. on the top you will see the high probability words tend to be those very common words, often functional words in english, and this will be followed by some content words that really characterized the topic well like text, mining etc an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ecause of the high probability words are functional words they are not really characterizing the topic. so one question is how can we get rid of such common words? \"now this is a topic of the next lecture. we're going to talk about how to use probabilistic models to somehow get rid of these common words.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "of such common words? \"now this is a topic of the next lecture. we're going to talk about how to use probabilistic models to somehow get rid of these common words.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ferent here. in particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution. so one natural way to solve the problem is to think about using another distribution to account for jus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "not be explained by this distribution. so one natural way to solve the problem is to think about using another distribution to account for just these common words. this way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. this way our target is the topic theta here would be only generating the content words that characterize the content of the document. so how does th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "eta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can set to attract the common words. because common words would be assigned high probabilities in this model. so the parameters can be collectively called a lambda, which i show here ag",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ended to denote the topic of document d and theta sub b which is representing a background topic that we can set to attract the common words. because common words would be assigned high probabilities in this model. so the parameters can be collectively called a lambda, which i show here again, and you can again",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "or out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "d when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the behavior of a mixture mo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. meaning that they have a very small pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "re the word counts. so assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. an in the first iteration you can picture what would happen. well, we first we initialize all the values. so here this probability that we'",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the oth",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the difference of the probability of such words smaller across categories.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "erstand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're going to score based on their ratio of probability, so this is ann let's say this is ou",
        "label": "use"
      }
    ]
  },
  {
    "text": "text objects",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "n. syntactical structure representation. we can also generate the structure based feature features and those are features that might help us classify text objects into different categories. by looking at the structures, sometimes the classification can be more accurate. for example, if you want to classify arti",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s, passages, sentences or websites. and then our goal is to group similar texture objects together. so let's see a example here. you don't really see text objects, but i just use some shapes to denote objects that can be grouped together. now, if i ask you what are some natural structures or natural groups well",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of comb",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "hieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cluster websites. each website is actually composed of multiple documents. similarly, we can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " what are some typical or representative document in the collection? and clustering help us achieve this goal. we sometimes also want to link similar text objects together and these. these objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a like",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " similarity based on the individual object similarity. so let's illustrate how can induce a structure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has the highe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " or desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. that is, we're going to ask humans to partition the objects to create the gold standard. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "on as well. so what counts as the best clustering result would be dependent on the application. procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. in this case what we care about is the contribution of clustering to some app",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ws. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ect into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and the catego",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "oblem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all the text objects. and y is all the categories a set of categories, so the classifier would take any value in x as input and we generate the value in y as output, and ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "hen try to be able to compute the values for future access that we have not seen. so in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. and they will also combi",
        "label": "intro"
      }
    ]
  },
  {
    "text": "categorization problem",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " of reviews about the product. and then these text data can help us infer properties of product or a restaurant. in that case, we can treat this as a categorization problem. we can categorize restaurants or categorize products based on their corresponding reviews. so this is example of external category. here are some sp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "tegorisation task can be actually performed by using binary categorization. and basically we can look at each category separately and then the binary categorization problem is whether object is in this category or not. meaning in other categories. and the hierarchical category categorisation can also be done by progressi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "f using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorization problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "zation in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. so, for example, if you want to do topical categorisation for news articles, you can say if the news article mentions word like game and sports thre",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ral problems with this approach. first, of course it's labor intensive. it requires a lot of manual work. obviously we can't do this for all kinds of categorization problems. we have to do it from scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well. \u00a0secondly, it canno",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "categorization and this is precisely what naive bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if theta i represents category i accurately that means the word distribution characterizes the content of documents in categor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "res. and each feature has a value x sub i here and our goal is model the dependency of this binary response variable on all these features. so in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the y value to denote the two categories. and when y is 1 it means the category ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "raining data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "? well, unfortunately this is very application specific, so there's no really much general thing to say here. but we can. and do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "ument basis? one example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. missing a legitimate email is all is 1 type of error. but letting us ma'am to come into your folder is another type of error. the two types of error",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "curacy, and this is basically similar to search again. and in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. so for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "mulated as a ranking problem instead of categorization problem. so for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. but typically we frame this as a ranking problem and we e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "inionated text document d as input an we want to generate as output already in the range of one through k, so it's discrete rating and thus this is a categorization problem. we have k categories here. now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ng to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. so the predictors are rep",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. so the predictors are represented as x and these are the features and there are m features altogether, which feature value is a real number, and thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "d y has two values, binary response variable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to b",
        "label": "use"
      }
    ]
  },
  {
    "text": "opinion mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text base",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ed to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. because we can categorize the authors, f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "this lecture is about opinion mining and sentiment analysis covering its motivation. in this lecture we are going to start talking about mining a different kind of knowledge, namely know",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ifferent kind of knowledge, namely knowledge about the observer or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. in contrast, we h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "stand what's in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining. so let's first look at the keyword subjective here. now this is in contrast with objective statement or factual statement. those statements can be p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "sentences that are about opinions that are useful for understanding the person or understanding the product that we are commenting on. so the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "duct reviews where the opinion holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "w it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ple, because those are the cases where you can easily build applications by using opinion mining techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. so here i id",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ing techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. so here i identify three major reasons, 3 broad reasons. the first is it can help decision support. i can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to intr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform detailed analysis ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " text mining we can understand the users better. and once we can end users better, we can serve these users better. so to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "text of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion ta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflect",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": ". as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with t",
        "label": "intro"
      }
    ]
  },
  {
    "text": "high probabilities",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ler probabilities for sports and science, which makes sense. \"and similarly you can see \"\"star\"\" also\" occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics. so with this representation it addresses the three problems that mentioned earlier. first,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "our belief about x values after we have observed y. given that we have observed y, now what do we believe about x now, do we believe some values have high probabilities than others? now, the two probabilities are related through this can be regarded as the probability of the observed evidence y here given a particula",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecause these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid of them, that would mean we have to do something different here. in parti",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "cument d and theta sub b which is representing a background topic that we can set to attract the common words. because common words would be assigned high probabilities in this model. so the parameters can be collectively called a lambda, which i show here again, and you can again think about the question about how m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on different words to avoid this competition in some sense. or to gain advantage in this competition. so again, looking at this objective function an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "is is to balance them out so that we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "act because it occurs just once. right, so this means there is another behavior that we observe here that is high frequency words generally will have high probabilities from all the distributions. and this is no surprise at all, because after all we are maximizing the likelihood of the data. so all the more word occu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st every component component model attempts to assign high probabilities to high frequency words in the data. and this is to collaboratively maximize likelihood. second, different component models tend to bet high probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "probabilities to high frequency words in the data. and this is to collaboratively maximize likelihood. second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "tistics? well, these are the word counts. so assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. an in the first iteration you can picture what would happen. well, we first we initialize all the values. so here this prob",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "user generated. document d will set to the pi value to 0 for that topic. we can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words. in this case, we're not going to say it's impossible, but we're going to just strongly favor certain kind of distributions.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "d in battery life of a laptop, and we're analyzing reviews. so the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life. so we could do say there's a distribution that's entirely concentrated on battery life and we all priors is that one of your di",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "r topic. in fact, the topic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world distribution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extrac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "egory is or how likely we would have observed the document in that category. the other kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate these two probabilities. and in general we can do this sepa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "the probability of such words smaller across categories. because every category has some help from their background for words, like the, a which have high probabilities. therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more ",
        "label": "use"
      }
    ]
  },
  {
    "text": "conditional probability",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "uction because this is a general knowledge that might be useful for you, so the bayes rule is basically defined here. and allows us to write down one conditional probability of x given y in terms of the conditional probability of y given x. and you can see the two probabilities are two conditional probabilities are differ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " be useful for you, so the bayes rule is basically defined here. and allows us to write down one conditional probability of x given y in terms of the conditional probability of y given x. and you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " any other data, that's our belief about x, what we believe some x values have higher probability than others. and this probability of x given y is a conditional probability, and this is our posterior belief about x, because this is our belief about x values after we have observed y. given that we have observed y, now wha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "of the data and the label x&y. and, this can then be factored out to a product of y. the distribution of labels and join the probability of sorry the conditional probability of x given y so it's y. so we first model distribution of labels and then we model how the data is generated given a particular label here. and once ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " of labels and then we model how the data is generated given a particular label here. and once we can estimate these models, then we can compute this conditional probability of label given data based on. the probability of data given label. and the label distribution here by using the base rule. now this is the most impor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "sed on. the probability of data given label. and the label distribution here by using the base rule. now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. so in such approaches, the objective function is actually likelihood, so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "et posterior becausw this one p of theta i is the prior, that's our belief about which topic is more likely. before we observe any document. but this conditional probability here is the posterior probability of the topic after we have observed the document d. and bayes rule allows us to update this probability based on th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "he y value to denote the two categories. and when y is 1 it means the category of the documents first class theta 1 now the goal here is to model the conditional probability of y given x directly as opposed to model the generation of x&y as in the case of naive bayes. and another advantage of this kind of approach is that",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " basically to model y given the observed x. so it's not like a. it's not like a modeling x, but rather we're going to model this. note that this is a conditional probability of y given x. and this is also precisely what we want for classification. now, so the likelihood function would be just a product over all the traini",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "t use them often for text categorization. so the second classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " categories of the neighbors. intuitively, this makes a lot of sense. but mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is p of y given x. now i'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "t works well on your training set would be actually the best for your future data. so as i mentioned that knn can be actually regarded as estimate of conditional probability of y given x, and that's why we put this in the category of discriminative approaches. so the key assumption that we made in this approach is that th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of course, then we can adjust these parameter values including betas, rs, alpha i. in order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document. and so we have seen such cases before in, for example, plsa, where we predict the text data. but here we p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of course it's zero if the world doesn't occur in the segment. now the model is going to predict the rating based on the. so we are interested in the conditional probability of r sub t given d. and this model is set up as follows. so all of this is assumed to follow a normal distribution with a mean that denotes actually ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " but the visualization shows that with this technique that we can have conditional distribution of time given a topic. so this allows us to plot this conditional probability. general curves like what you're seeing here. we see that initially the two curves tracked each other very well. but later we see the topic of new or",
        "label": "use"
      }
    ]
  },
  {
    "text": "unigram language",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "merate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "erved data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "nguage model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e complicated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated lang",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. sp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ure of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this backgr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ection of documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "e bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "g the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a bypro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "c model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each documen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nts that are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty ",
        "label": "use"
      }
    ]
  },
  {
    "text": "related words",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " count what are the most frequent words in this document or in the whole collection, etc. and these words can be used to form topics. when we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. so representing text data as a sequence of words",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "t analysis, and there are many applications that can be enabled by this kind of analysis. for example, thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covere",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "val and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effective. it's often called query expansion. or you can use related words to suggest related queries to the user t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "this can be used to introduce additional related words to a query to make the query more effective. it's often called query expansion. or you can use related words to suggest related queries to the user to explore the information space. another application is to use word associations to automatically construct t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. so next, let's see how we exactly compute these similarity functions. now to answer this question it's",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "r this approach it would work well. now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations. but analytically, we can also analyze this formula little bit. so first, as i said, it does make sense right",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "shold can be the number of top candidates to take or absolute value for the conditional entropy. now this would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the strongest k syntagmatic relations from entire collection. because in ord",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ble. so this simple example illustrates some problems of this approach. first, when we count what words belong to the topic, we also need to consider related words. we can't simply just count the topic. word sports. in this case, it did not occur at all, but there are many related words like\u00a0 basketball, game, e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": " we also need to consider related words. we can't simply just count the topic. word sports. in this case, it did not occur at all, but there are many related words like\u00a0 basketball, game, etc. so we need to count related words. also. the second problem is that a word like star can be actually ambiguous. so here ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ount the topic. word sports. in this case, it did not occur at all, but there are many related words like\u00a0 basketball, game, etc. so we need to count related words. also. the second problem is that a word like star can be actually ambiguous. so here it probably means a basketball star but we can imagine it might",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "second problem, we need to introduce weights of words. this would allow you to distinguish subtle differences in topics and to introduce semantically related words in the fuzzy manner. finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it tu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "to describe fairly complicated topics. second, it assigns weights to terms, so now we can model several differences of semantics and you can bring in related words together to model topic. third, because we have probabilities for the same word in different topics. we can disambiguate the sense of word in the tex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " we can also learn word clusters empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words. and these clusters can be features to supplement the word based representation. furthermore, we can also have frequent",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "rs empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words. and these clusters can be features to supplement the word based representation. furthermore, we can also have frequent pattern syntax and these coul",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "e done by using seed words like location and room. or price to retrieve the relevant the segments and then from those segments we can further mine correlated words. with these seed words and that would allow us to segment the text into segments. discussing different aspects, but of course later as we would see, ",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic word",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "his is controlled by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example wher",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "st to flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " observed from each of the two distributions, so we have to consider 2 cases. therefore it's a sum over these two cases. the first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of theta sub d, which is the probability of choo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "arameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ssume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follow",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "stimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probabi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "g the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would me",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "nown to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right?",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "e interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "u will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "iform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our infer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "h theta i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior infor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "has been from this cluster, so we should favor such a cluster. the other is a likelihood part, that is this part. and this has to do with whether the topic word distribution can explain the content of this document well. and we want to pick a topic that's high by both values. so more specifically, we just mul",
        "label": "use"
      }
    ]
  },
  {
    "text": "text object",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "n. syntactical structure representation. we can also generate the structure based feature features and those are features that might help us classify text objects into different categories. by looking at the structures, sometimes the classification can be more accurate. for example, if you want to classify art",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s, passages, sentences or websites. and then our goal is to group similar texture objects together. so let's see a example here. you don't really see text objects, but i just use some shapes to denote objects that can be grouped together. now, if i ask you what are some natural structures or natural groups wel",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of com",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "hieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cluster websites. each website is actually composed of multiple documents. similarly, we can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "pers or similar. furthermore, text clusters can also be further clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particul",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " what are some typical or representative document in the collection? and clustering help us achieve this goal. we sometimes also want to link similar text objects together and these. these objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a lik",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " similarity based on the individual object similarity. so let's illustrate how can induce a structure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has the high",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "tter. now let's look at another example of method for similarity based classroom in this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by just s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " or desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. that is, we're going to ask humans to partition the objects to create the gold standard.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "on as well. so what counts as the best clustering result would be dependent on the application. procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. in this case what we care about is the contribution of clustering to some ap",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ws. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. so",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ect into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and the categ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "an also vary, and we can generally distinguish two kinds of categories. one is internal categories. these are categories that characterize content of text object. for example, topic categories. or sentiment categories and they generally have to do with the content of the tax objects, direct characterization of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "f the tax objects, direct characterization of the content. the other kind is external categories that can characterize the entity associated with the text object. for example, authors or entities associated with the content that they produce. and so we can use their content, determine which author has written ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ve which categories. and this is called a training data. and then secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. so we need to provide some basic features for the computers to look into. and in the case of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to ma",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "r can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "oblem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all the text objects. and y is all the categories a set of categories, so the classifier would take any value in x as input and we generate the value in y as output, and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "hen try to be able to compute the values for future access that we have not seen. so in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. and they will also comb",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "he conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objec",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interested in clas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ll, since this is in, this object is in category of diamonds. let's say then we're going to say, well, we're going to assign the same category to our text object. but let's also look at the another possibility of finding a larger neighborhood. so let's think about the four neighbors. in this case, we're going ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "d with one wait for each feature we have m features and so have aim weights and are represented as a vector. an similarly the data instance. here the text object is represented by also a feature vector of the same number of elements. xi is future value. for example word count. i can you can verify when we mult",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ng sentiment classification for understanding opinion. sentiment classification can be defined more specifically as follows: the input is opinionated text object. the output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. one is polarity analysis where we have categories",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "n see the general decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object. so in sum, in this approach we're going to score the object. by using the features and the parameter values, beta values. and this score will then b",
        "label": "intro"
      }
    ]
  },
  {
    "text": "contextual text",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this directi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "detail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "cs and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we'r",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ic technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. reca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interes",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "urally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see s",
        "label": "intro"
      }
    ]
  },
  {
    "text": "paradigmatic relations",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r synonyms, for example, and then you can help a lot of tasks. and grammar learning can be also done by using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for put",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "etween the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship. and then imagine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic id",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ome other different ways. sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much relat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "orld, then you likely will capture words that are very much related by their syntactical categories and semantics. so the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ntext, so d2. and then we can measure the similarity of these two vectors. so by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. so the two questions that we have to address is first how to compute each v",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ell. now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations. but analytically, we can also analyze this formula little bit. so first, as i said, it does make sense right? because this formula will give a highe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ions. and indeed they can be discussed, discovered in a joint manner by leveraging such associations. so to summarize, the main idea for discovering\u00a0 paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. and then compute the si",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " similarity of the corresponding context documents of two candidate words. an then we can take the highly similar word pairs and treat them as having paradigmatic relations. these are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "o represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "arrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "c mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mi",
        "label": "use"
      }
    ]
  },
  {
    "text": "total number",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "we can use the\u00a0 idf weighting that's commonly used in retrieval. idf stands for inverse document frequency. document frequency means the count of the total number of documents that contain a particular word. so here we show that the idf measure is defined as a logarithm function of the number of documents that ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "mber of documents that match the term, or document frequency. so k is the number of documents containing word or document frequency and m here is the total number of documents in the collection. the idf function is giving a higher value for a lower k, meaning that it rewards a rare term. and the maximum value i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ros. and for estimating these probabilities, we simply need to collect the three counts. so the three counts of 1st, the count of w. 1 and that's the total number of segments that contain world w one. it's just the ones in the column of w one we can just count how many ones we have seen there. the second counte",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "nt how many ones we have seen there. the second counter is for word 2 and we just count the ones in the second column. and these this would give us a total number of segments that contain w2. the third account is when both words occurred, so this is time we're going to count the segments where both columns have",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "count is when both words occurred, so this is time we're going to count the segments where both columns have ones. and then so this would give us the total number of segments where we have seen both w and w2. once we have these counts, we can just normalize. these counts by n, which is the total number of segme",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "e us the total number of segments where we have seen both w and w2. once we have these counts, we can just normalize. these counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. now there is a small problem. when we have zero count",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "and similarly this .05 comes from one single pseudo segment that indicates the two words occur together. and of course, in the denominator we add the total number of pseudo segments that we added. in this case we added a 4th through the segments. each is weighted 1/4, so the total the sum is actually one. so th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "a. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we ask the question, what is the most likely language model that has been us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "imate this probability. and i have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region. so the numerator that you see here c of theta and r is a count of the documents in region r with category theta i. since ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "y're categories. we can simply count how many times we have seen sports here, how many times we have seen science etc. and then denominator is just a total number of documents training documents in this region, so this gives us a rough estimate of which category is most popular in this neighborhood, and we're g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": " two kinds of errors. so the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. so we know that the total number of decisions is. in multiplied by k. and the number of characters decisions obviously are",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "acy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. so we know that the total number of decisions is. in multiplied by k. and the number of characters decisions obviously are basically of two kinds. one is why pluses and the other is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "classifiers do we have? you can see the answer is that for each classifier we have n + 1 parameters. and we have k - 1 classifiers altogether, so the total number of premises (k - 1) * (m + 1). that's alot alot of parameters. so when the classifier has a lot of parameters would in general need a lot of data to ",
        "label": "use"
      }
    ]
  },
  {
    "text": "document clustering",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": ". the number of documents. so this document must share some topics. and if we have n documents for share k topics, then will again have precisely the document clustering problem. so because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clust",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " words in the document could have been generated in general from multiple distributions. now this is not what we want to see for text clustering. for document clustering where we hope this document will be generated from precisely one topic. so now that means we need to modify the model, but how well, let's first thin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "icture with the previous one, you will see the desicion of. of using a particular distribution is made of just once for this document. in the case of document clustering. but in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potentia",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "here's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. and of course, the main problem in document clustering is to infer. which distribution has been used to generator a document and that would allow us to recover the cluster identity over document so it wou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "as i have also mentioned multiple times. there are many. two differences. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to ge",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ilar or in many ways they are similar. but there's also some difference. and in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. and that's corresponding to our assumption of 1st make a choice of choosing one distribution and then s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "at you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to include the k clusters. now if you look at the formula and think about the question how to ge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " probabilities of generating d from those thetas. so this is precisely what we're going to use. this is general presentation of the mixture model for document clustering. so as more cases we follow these steps using a generated model. first think about our data, right? so in this case our data is a collection of docum",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " is generally higher than in a small cluster. so this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. so next we have to discuss how to actually compute the estimate of the model.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "stimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. main difference is that in clustering we don't really know what are the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ate a document to one of these categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and that's just because we've made the assumption about the independence in generating each word ok. so this is just something that you have seen in document clustering. an we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and t",
        "label": "intro"
      }
    ]
  },
  {
    "text": "aspect ratings",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ews, preferences and also we can understand better how reviewers view this hotel from different perspectives. now, not only do we want to. infer this aspect ratings. we also want to infer the aspect of weights, so some reviewers may care more about values as opposed to service, and that would be a case like what'",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ore detailed understanding of the opinion. so the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called latent aspect rating analysis. so the task in general is",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of the opinion. so the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called latent aspect rating analysis. so the task in general is given a set of review arti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " by beta sub i and w. in the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings. so we're going to assume we have aspect weights in order by of r sub of d. and this would be used to take a weighted average of the aspect ratings, ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "e aspect ratings. so we're going to assume we have aspect weights in order by of r sub of d. and this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the. and we can assume the overall rating is simply a weighted average of this aspect ratings. so this setup allo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "hted average of the aspect ratings, which are denoted by our supply of the. and we can assume the overall rating is simply a weighted average of this aspect ratings. so this setup allows us to predict the overall rating based on the observed word frequencies. so on the left side you will see all these observed in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ers of course are also very different. but if you can see if we can uncover these parameters, that would be nice because also r of d is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub id is precisely the aspect weights that we hope to get. as a bi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this model is set up as follows. so all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings. r sub of d as shown here is normal distribution has a variance of or square. now of course, this is just what our assumption in the actual reading i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ways when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. in this case, the aspect ratings and aspect of weights. now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just se",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "to be weighted sum of these weights where the weight is just sentiment wait. so. as i said, the overall rating is assumed to be a weighted average of aspect ratings. now this alpha values of a alpha sub of d together by our vector that depends on d is the document specific weights and we can assume this factor it",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of other values from this multivariate gaussian prior distribution and once we get these alpha values were going to use, then the weighted average of aspect ratings as the mean here to use the normal distribution. and to generate the overall rating. now the aspect rating as i just said is the sum of the sentiment",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " to solve the problem of lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights. now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "o if you just look at the overall rating you don't. you can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. dimensions like value, but others might score better in other dimensions like location and so th",
        "label": "intro"
      }
    ]
  },
  {
    "text": "logistical regression",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines and the k nearest neighbors. we will cover some of these classifiers in detail in the next few lectures.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification. and we are going to talk more about such ap",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ion of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary response variable y here, on some predictors. that are denoted as x. so here we have also changed the notati",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "in fact the function form here would vary depending on whether y sub i is one or zero. if it's one will be taking this form. and that's basically the logistical regression function. but what about this if it's 0? well, if it's zero then we have to use a different form and that's this one. now how do we get this one? tha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "for that document. and then plugging those values that will give us a estimate. the probability that the document is in category one. ok, so much for logistical regression. let's also introduce another discriminative classifier called k nearest neighbors. now in general, i should say there are many such approaches. and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "lines that can separate the both clauses, which line is the best? in fact, you can imagine there are many different ways of choosing the line. so the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ariable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to be a linear function of these feature",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " this probability to probability that y = 1 to the feature values. and of course, b_i is our parameters here. so this is just a direct application of logistical regression for binary categorization. what if we have multiple categories, multiple levels? we actually use such a binary logistic regression program to solve t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "fier to distinguish two and one so altogether we'll have k - 1 classifiers. now if we do that of course, then we can also solve this problem, and the logistical regression program would be also very straightforward as you have just seen on the previous slide. only that here we have more parameters because for each class",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "o level of rating k. so this classifier will tell us whether this object should have a rating of k or above. and if its probability according to this logistical regression classifier is larger than .5, we're going to say yes, the rating is k. now, what if it's not as large as .5? well, that means the reading is below k,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "advantage of this factor. now the idea of ordinal logistic regression is precisely that\u00a0 a key idea is just the improvement over the k -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the beta parameters these are the parameters that in",
        "label": "use"
      }
    ]
  },
  {
    "text": "language models",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "f statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statisti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ta, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sampl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "us model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifica",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "wo unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background la",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "f documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions correspon",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "fferent from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "his single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "t's not obvious to you. there is another issue in naive bayes which is a smoothing. in fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. so smoothing is the important technique to address data sparse",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood estimate",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating the proba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ion we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "'s talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplification of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it's the mos",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "a sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're interest",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood func",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "robabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the idea of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. no",
        "label": "use"
      }
    ]
  },
  {
    "text": "posterior probability",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ertain thetas over others. and by using bayes rule that i have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full explanation of bayes rule and some of these things related to bayesian reasoning would be outside the scope of this cour",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ior knowledge about the parameters. and then we have the data likelihood here that would tell us which parameter value can explain the data well. the posterior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "well. the posterior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estim",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "presents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood esti",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "likelihood together with the prior. in this case the prior is p of theta i. and together, that is, we're going to use the base formula to compute the posterior probability of theta given d. and if we choose theta based on this posterior probability and we would have the following formula that you see here. on the bottom",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "nd together, that is, we're going to use the base formula to compute the posterior probability of theta given d. and if we choose theta based on this posterior probability and we would have the following formula that you see here. on the bottom of this slide, and in this case, we're going to choose the theta that has a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ecifically, basically we're going to apply bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. given the document. an we know it's proportional to the probability of selecting this distribution p of theta i and the probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e which topic has the highest product. so more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document. get posterior becausw this one p of theta i is the prior, that's our belief about which topic is more likely. before",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "heta i is the prior, that's our belief about which topic is more likely. before we observe any document. but this conditional probability here is the posterior probability of the topic after we have observed the document d. and bayes rule allows us to update this probability based on the prior and i shown the details. b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "related to how well this word distribution explains the document here, and the two are related in this way. so to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course. an we can then change the probability of document ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "going to assign document d to the category that has the highest probability of generating this document. in other words, we're going to maximize this posterior probability as well. and this is related to the prior and the likelihood an as you have seen on the previous slide. and so naturally, we can then decompose this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ". so this is called a naiyes bayes classifier. now the keyword bayes is understandable because we are applying a bayes rule here. when we go from the posterior probability of the topic to a product of the likelihood and the prior. now it's also called a naive because we've made an assumption that every word in the docum",
        "label": "use"
      }
    ]
  },
  {
    "text": "machine learning",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "sults that are contradictory. so all these. problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning. so these machine learning methods are more automatic, but i still put automatic in quotation marks cause they're not really completely automatic bec",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "y. so all these. problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning. so these machine learning methods are more automatic, but i still put automatic in quotation marks cause they're not really completely automatic because it still require manua",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "sophisticated features like phrases or even policy feature tags or even syntactic structures. so once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. so soft rules just means we're going to still decide which category should be assigned",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "egory, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "put y would be the right category for x, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of input and output for function and then the computer is going to figure ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " previous slide we have used fi to represent the feature values. an here we use the notation of x vector, which is more common when we introduce such machine learning algorithms, so x is our input, it's a vector. and with m features. and each feature has a value x sub i here and our goal is model the dependency of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ication tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in gen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " should say there are many such approaches. and thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. here, just want to include the basic introduction to some of the most commonly used cl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " and thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " would be also similar, and so that's a very key assumption, and that that's. that's actually important assumption that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our simil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "presentation and to design effective feature set that we need domain knowledge and humans definitely play important role here. although there are new machine learning methods like representation learning that can help with learning features. an another common scene is that they might be. be performing similarly on ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "s that would combine different methods and tend to be more robust and can be useful in practice. most techniques that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "lly applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of text co",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "rror analysis very important in general, and that's where you can get the insights about your specific problem. and then finally we can leverage some machine learning techniques. so for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with tryin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "l space features to provide a multiresolution representation, which is often very useful. deep learning is a new technique that has been developed in machine learning. it's particularly useful for learning representations, so different learning refers to deep neural network. it's another kind of classifier where yo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " all of that high quality, but they can still be useful. another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "zation method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be differ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you. do that effectively. here's a suggestion reading an where you can find more details about some of the methods that we h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "ally, sometimes ranking may be more appropriate, so be careful. sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well. so here are two suggested readings are one is some chapters of this book where you can find more dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " match it and it says a lot and it's accurate. it's unlikely, very ambiguous. but it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories. an obviously that kind ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ven a part of parse tree. so in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. in general, i think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing fea",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " significantly, and it's a very important part of any machine learning application. in general, i think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features. so first you want to use domain knowledge and your understanding of the problem to design",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " and your understanding of the problem to design seed features. and you can also define a basic feature space with a lot of possible features for the machine learning program to work on. and machine learning can be applied to select the most effective features or construct the new features that feature learning. an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " design seed features. and you can also define a basic feature space with a lot of possible features for the machine learning program to work on. and machine learning can be applied to select the most effective features or construct the new features that feature learning. and these features can then be further anal",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ff between frequent versus infrequent features, and that's why feature design is generally an art. that's perhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "fy what data should be collected next. in general, we want to collect data that are most useful for learning. and this there is actually a subarea in machine learning called active learning that has to do with this. how do you identify data points? that would be most helpful for machine learning programs if you can",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "tually a subarea in machine learning called active learning that has to do with this. how do you identify data points? that would be most helpful for machine learning programs if you can label them, right. so in general, you can see there's a loop here from data acquisition to data analysis or data mining to predic",
        "label": "use"
      }
    ]
  },
  {
    "text": "clustering bias",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " in order to make the clustering problem well defined, a user must define the perspective. for assessing similarity. and we call this perspective the clustering bias. and when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be use",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "milarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ng methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is built into a generated model. th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is built into a generated model. that's why we can use potentially a different model to recover different instruction. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "tering but how do we know which method works the best? so this has to do with evaluation. now to talk about evaluation, one must go to go back to the clustering bias that we introduced at the beginning. because two objects can be similar depending on how you look at them, we must clearly specify the perspective of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "wer this question without knowing whether we'd like to cluster based on shapes or cluster based on sizes. and that's precisely why the perspective or clustering bias is crucial for evaluation. in general, we can evaluate text clusters in two ways. one is direct evaluation and the other is indirect evaluation. so i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ould allow us to easily compare different methods based on their performance figures. and finally, you can see in this case we essentially inject the clustering bias by using humans. basically, humans would bring the needed or desired clustering bias. now how do we do that exactly? the general procedure would look",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " and finally, you can see in this case we essentially inject the clustering bias by using humans. basically, humans would bring the needed or desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ed applications? now this of course is application specific question, so usefulness is is going to depend on specific applications. in this case, the clustering bias is imposed by the intended application as well. so what counts as the best clustering result would be dependent on the application. procedure wise we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": ". in general, strong clusters tend to show up no matter what method is used. also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " now sometimes you may see some methods that can automatically determine the number of clusters. but in general, that has some implied application of clustering bias there, and that's just not specified. without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what? ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "er of clusters. but in general, that has some implied application of clustering bias there, and that's just not specified. without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what? so this is important to keep in mind. and i should also say sometimes we can use ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "overall ratings",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ng examples can also be used so those can be called a pseudo training examples. for example, if you take a reviews from the internet, they might have overall ratings. so to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "nalysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings. first, motivation. here are two reviews that you often see on the internet about the hotel and you see some overall ratings. in this case, both revi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "analysis of reviews with overall ratings. first, motivation. here are two reviews that you often see on the internet about the hotel and you see some overall ratings. in this case, both reviewers have given five stars. and of course there are also reviews that are in text. now, if you just look at these reviews, i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "eights. when they are combined together, we can have a more detailed understanding of the opinion. so the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " as output. and this is a problem called latent aspect rating analysis. so the task in general is given a set of review articles about the topic with overall ratings. an we hope to generate the three things. one is the major aspects comment on in the reviews. the second is the ratings on each aspect, such as value",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "hese are the aspects of specifica sentiment, weights of words, so more formally. they thought we are modeling. here is a set of review documents with overall ratings. and each review documents denoted by at and overall rating is denoted by r sub d and these pre segmented into k as their segments and we're going to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "s to compare different reviewers on the same hotel so the table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same. so if you just look at the overall ratings, you don't really get that much information about the difference between the two reviews. bu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same. so if you just look at the overall ratings, you don't really get that much information about the difference between the two reviews. but after you decompose the ratings you can see clearly the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "hotels in social media like tweets. and, what's also interesting that since this is an almost computer and supervised, assuming that the reviews with overall ratings are available, and then this can allow us to learn from potentially a large amount of data on the internet to reach sentiment lexicon. and here are s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "rom reviews with low ratings. these are mp3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects. or they comment more on different aspects. so that can help us dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "e are mp3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects. or they comment more on different aspects. so that can help us discover, for example, consumers trai",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "compare the opinions from different consumer groups in different locations, and of course the model is general. it can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. finally, there is also some result on applying this model for p",
        "label": "use"
      }
    ]
  },
  {
    "text": "background words",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the problem. it will be useful to think about why we end up",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "bout how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background he",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "o the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust theta sub d in order to maximize the probability of th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " observed document here and we assume all the other parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "out so that we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mas",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "f terms to give us the probabilities of all these words. now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. and that would make the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the model param",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "g the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " this actually can also be very useful and you can think about that. and one use is to. for example, is made to what extent this document has covered background words. and this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explai",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "xample. see there's a criticism of government response, and this is followed by the discussion of flooding of the city and donation, etc. we also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. so segment of the topics to figure out which wo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "'s take a closer look at this. after that contains all the important parameters. so first we see lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and ty",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic coverage",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ll get k topical terms and those can be regarded as the topics that we discovered from the collection. next let's think about how we can \"compute the topic coverage i j so looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document how should we figure out",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ll the probabilities should sum to one over all the words in the vocabulary. so you see a constraint here and we still have another constraint on the topic coverage, namely pis. so all the pis of ij's must sum to one for the same document. so how do we solve this problem? well, let's look at this problem as a com",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the topic coverage in each document. a document is not allowed to cover a topic outside the set of topics that we are discovering. so the coverage of each of these k to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "text data right? and then the output is of two kinds. one is the topic category characterization seedies hci is a water distribution and 2nd it's the topic coverage for each document. these are pie some ideas and they tell us which document covers which topic to what extent. so we hope to generate these as output",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "straint, two kinds of constraints. one is awarded distributions. all the words must have probabilities that sum to 141 distribution. the other is the topic coverage distribution. anna document will have to cover precisely these k topics, so the probability of covering each topical would have to sum to one. so at ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "looking at the opinions about the laptop, because the user is particularly interested in these aspects. now, a user may also have knowledge about the topic coverage. and we may know which topic is definitely not covered in which document or is covered in the document. for example, we might have seen those tags to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ore regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ns as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. so the inference p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " theta i's. each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "rize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that we assume. the topic coverage also depends on the context. and that means depending on the time or location, we might cover topics differently. and then again this dependency woul",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "topic content in different context and this gives us different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. because in the. in the case of location like texas, people might want to cover the red topics more at the",
        "label": "intro"
      }
    ]
  },
  {
    "text": "optimization problem",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "constraint here, and this will allow us to solve for lambda. and this is just negative sum of all the counts and this further allows us to then solve optimization problem. eventually to find the optimal setting for theta sub i. and if you look at this formula, it turns out that it's actually very intuitive because this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "nd it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a close",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "l solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "hown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, diff",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " topical would have to sum to one. so at this point it's basically where they find applied math problem. you just need to figure out the solutions to optimization problem. there's a function with many variables and we need to just figure out the values of these variables to make the function which its maximum.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ultiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "tor basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard optimization problem. in this case, it can be also solved in many ways. newtons method is a popular way to solve this problem. there are other methods as well, but in the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "lent to finding values for w&b because they would determine where exactly the separator is. so in the simplest case, the linear osfm is just a simple optimization problem. so again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "and this is simply to maximize sorry to minimize w transpose multiplied by w and we often denote this by file w. so now you can see this is basically optimization problem, right? we have some variables to optimize and these are the weights and b and we have some constraints. these are linear constraints and the objecti",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ariables. cassie i an we in fact will have one for each data instance and this is going to model the error that will allow for each instance. but the optimization problem will be very similar. so specifically, you will see we have added something to the optimization problem. first we have added some. some error to the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "or that will allow for each instance. but the optimization problem will be very similar. so specifically, you will see we have added something to the optimization problem. first we have added some. some error to the constraint so that now we allow. allow the classifier to make some mistakes here, so this kci is allowed",
        "label": "use"
      }
    ]
  },
  {
    "text": "general idea",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ng to first talk about what is word association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "he methods for discovering syntagmatic relation. mainly we need to capture the correlation between the occurrences of two words. so to summarize, the general ideas for discovering word associations are the following. for paradigmatically relation we represent each word by its context, and then compute the conte",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. so these general ideas can be implemented in many different ways, and the course won't cover all of them, but we will cover at least some of the methods that effective for",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ft and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics. so the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. so here for example, we can measure the similarity of cat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " and treat them as having paradigmatic relations. these are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similar",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ram to solve this problem, but here we're going to introduce a general way of solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the picture that you have seen before in order ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "c outside the set of topics that we are discovering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "so this is indeed a general idea of the expectation maximization, or em algorithm. so in all the em algorithms, we introduce a hidden variable to help us solve the problem more easil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. the general idea is that we will have two steps to improve the estimate of parameters in the e step. we roughly all augmenting our data by predicting values of useful",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "lustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in con",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "at say come here to get four clusters. or we can use the threshold to cut or we can cut at this high level to get just the two clusters. so this is a general idea. now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group simi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "tional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary response variable y here, on some predictors. that are denoted as x. so here we hav",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ct, so we can have we can give such a neighbor more influence on the vote and we can take weighted sum of their votes based on the distances. but the general idea is to look at the neighborhood and then try to assess the category based on the categories of the neighbors. intuitively, this makes a lot of sense. ",
        "label": "use"
      }
    ]
  },
  {
    "text": "relation discovery",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "sed frequently for modeling documents and queries for search. but here we also find it convenient to model the context of a word for paradigmatically relation discovery. so the idea of this approach is to view each word in our vocabulary as defining one dimension in high dimensional space so we have n words in total ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "in this lecture, we continue discussing paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector that represe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagm",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover synt",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "enominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. we intro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "airs and discover the strongest cinematical relationship from collection of documents. now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. so we already discussed the possibility of using bm 25 to achieve waiting for terms in the context to potent",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "cal relationship from collection of documents. now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. so we already discussed the possibility of using bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "or computer similarity between these words based on their context similarity. so this provides yet another way to do term waiting for paradigmatic. a relation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic rela",
        "label": "intro"
      }
    ]
  },
  {
    "text": "probabilistic topic",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "pic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuit",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "oblem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "c models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution ove",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interest",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked abou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "his lecture is about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ng. then the algorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " an lda, would work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest",
        "label": "intro"
      }
    ]
  },
  {
    "text": "special case",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": ". you could think about in general\u00a0 here is for what kind of x? does the entropy reached maximum or minimum and we can in particular think about some special cases. for example, in one case we might have a random variable that always takes the value of one,\u00a0 the probability is one\u00a0 or there is a random variable",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "bsence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports. you can also see it as a very special case if the probability mass is concentrated entire of just one word. let's sports, and this basically degenerates to the simple representation of topic w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic mode",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " estimate the value of this function f as the expected value of f according to the posterior distribution of data given the observed evidence x. as a special case, we can assume f of theta is just equal to theta. in this case we get the expected value of theta. that's basically the posterior mean that gives us ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "etermined by probability of theta sub d. and probability of theta sub b. note that they sum to one. now what's interesting is also to think about the special cases, like when we set one of them to one. what would happen? well, the other would be 0, right? and if you look at the likelihood function. it will then",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " one of them to one. what would happen? well, the other would be 0, right? and if you look at the likelihood function. it will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0. so in this sense, the mixture mo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ther is 0. so in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the mod",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "o words: the and text. obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. so we further assume that the background model gives probability of point nine to the word the and text point one. now, let's also assume that our d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "odels. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. a large collect",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "enerate with all the words in a document. multiple distribution could have been used to generate the words in the document. it's also think about the special case when one of the one of the probability of choosing a particular distribution is equal to 1. now that just means we have no uncertainty now. we just s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " counts that we add for all the words. so mu is also non-negative constant and it's\u00a0 empirically set to control smoothing. there are some interesting special cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary. so ea",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "t the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and this is actually one of those",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ave a new object in the center that we want to classify. so according to this approach we're going to\u00a0 find the neighbors. and let's first think of a special case of finding just one neighbor, the closest neighbor. now in this case, if the, let's assume the closest neighbor is the box filled with diamonds and s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "gry, surpised and disgusted. so as you can see, the task is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do tha",
        "label": "intro"
      }
    ]
  },
  {
    "text": "information retrieval",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " this question it's useful to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. but here w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "how do you compute the similarity? now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. and they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the simil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "er text article and this would give us different kinds of associations. these discovery associations can support them. any other applications in both information retrieval and text data mining. so here are some recommended readings. if you want to know more about the topic, the 1st is a book with a chapter on locations ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": ", but sometimes a user might have some expectations about which topics to analyze. for example, we might expect to see retrieval models as a topic in information retrieval. we also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particula",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "on and so first we have the simplest case, which is a binary categorization where there are only two categories and there are many examples like that information retrieval or search engine applications would want to. distinguish it relevant documents from non relevant documents for a particular query. spam filter is int",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "ut the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called cranfield evaluation methodology. the basic idea is to help humans to create test collection. where we already every document is t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "st better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. and these are also proposed by information retrieval researchers in 19, six days for evaluating searching results. but now they have become a standard measure used everywhere. so when the system says ye",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "application of this kind of model where we look at the use of the model for event impact analysis. so here we are looking at the research articles in information retrieval, ir, particularly sigir papers. and the topic we focus on is about the retrieval models and you can see the top word top words with high probability ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ion effort sponsored by us government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "search information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "al paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divi",
        "label": "use"
      }
    ]
  },
  {
    "text": "probabilistic models",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. so in this simple setup we are int",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "the topic. so one question is how can we get rid of such common words? \"now this is a topic of the next lecture. we're going to talk about how to use probabilistic models to somehow get rid of these common words.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "tive probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "cture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a sl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the li",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "r. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of ev",
        "label": "use"
      }
    ]
  },
  {
    "text": "data point",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "rs in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the follo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "uld like to adjust these parameter values until we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is what parameter values will give our data set the highest probability. s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "here. so when we have a model, we generally have two problems that we can think about. one is given a model. how likely we'll observe certain kind of data points. that is, we're interested in the sampling process. the other is the estimation process and that is to figure out the parameters of the model given ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "and it's often very useful, and it seeks the parameters that best explain the data. but it has a problem when the data is too small, because when the data points are too small, there are very few data points. the sample is small, then if we trust data entirely and try to fit the data and then we will be biase",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "arameters that best explain the data. but it has a problem when the data is too small, because when the data points are too small, there are very few data points. the sample is small, then if we trust data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "y of data given some model. and this function is very important. given a particular set of parameter values, this function can tell us which x, which data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " given a particular set of parameter values, this function can tell us which x, which data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. and the likelihood function will have some parameters in the function and then we are usually interested in estimat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e word. and in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. so the basic idea of a mixture model is j",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "oration and competition between the component models. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data. i in this case, so it's going to be some over these k topi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "g to be. a product of two probabilities and one is the probability of choosing a distribution. the other is the probability of observing a particular data point from that distribution. so if you are map, this formula is kind of formula to our problem. here you will see the probability of observing a document ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "fferences here, we don't make a probabilistic on location as in the case of the step. but rather we make a choice. we're going to make a call if this data point is closest to cluster two that were going to say you are in class too. so there's no choice, and we're not going to say you are 70% belonging to clas",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " however, we do a probabilistic location, so we split the counts. and we're not going to say exactly which distribution has been used to generate the data point. now the next we're going to adjust the centroid, and this is very similar to m step where we re estimate the parameters. that's when we'll have a be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ties when we count the points in this case, for k means we're going to only count the objects allocated to this cluster, and this is only a subset of data points. but in the em algorithm, we in principle consider all the data points. based on probabilistic allocations. but in nature they are very similar and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "only count the objects allocated to this cluster, and this is only a subset of data points. but in the em algorithm, we in principle consider all the data points. based on probabilistic allocations. but in nature they are very similar and that's why it's also maximizing where defined objective function and it",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ion of classes. so sorry for the problem. so these discriminative classifiers attempted to model the. conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "document, we can try to estimate the probability of the category given that entire region. now this has the benefit of course, of bringing additional data points to help us estimate this probability. and so this is precise idea of knn. basically now we can use the known categories of all the documents in this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ther class. now one question is to take a point like this one and to ask the question what's the value of this expression or this classifier for this data point. so what do you think? basically working to evaluate its value by using this function. and as we said, if this value is positive we're gonna say this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " separator. to maximize the margin. so what is the margin? well, i choose. so i've shown some daughter lines here to indicate the boundaries of those data points in. in each class and the margin is simply the distance between the line, the separator and the closest points from each class. so you can see the m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "readings you might do. ok. so. when we maximize the margins of separate, it just means with the boundary of. the separate is only determined by a few data points, and these are the data points that we call support vectors. so here are illustrated to support vectors for one class and two for the other class. a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "when we maximize the margins of separate, it just means with the boundary of. the separate is only determined by a few data points, and these are the data points that we call support vectors. so here are illustrated to support vectors for one class and two for the other class. at this, porters define the marg",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "margin basically. and you can imagine once we know which are support vectors, then this center separate line will be determined by them so. the other data points actually don't really matter that much. and you can see if they you change other data points, it won't really affect the margin, so the separate wit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "r separate line will be determined by them so. the other data points actually don't really matter that much. and you can see if they you change other data points, it won't really affect the margin, so the separate with the stay the same mainly affected by the support vector machines. sorry it's mainly affecte",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "le can stay right, so we want to minimize the training error, but try to also maximize the margin. but in this case we have a soft margin because the data points may not be a completely separate bowl. so it turns out that we can easily modify it as vm to accommodate this. so what you see here is very similar ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "st useful for learning. and this there is actually a subarea in machine learning called active learning that has to do with this. how do you identify data points? that would be most helpful for machine learning programs if you can label them, right. so in general, you can see there's a loop here from data acq",
        "label": "use"
      }
    ]
  },
  {
    "text": "generative models",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "cture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measur",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "e clustering structure is built into a generated model. that's why we can use potentially a different model to recover different instruction. complex generative models can be used to discover complex clustering structures. we did not talk about it, but we can easily design generated model to generate a hierarchical ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ion learning. and then we can certainly cluster terms based on actually their tax representations. of course, term clusters can be generated by using generative models as well as we have seen.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "sing machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ssifying new objects. so that's the basic idea of sven. so to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ere is a reference site here where you can find more details. so now i'm going to show you some simple results that you can get by using this kind of generative models. first it's about rating decomposition. so here what you see are the decomposed ratings for three hotels that have the same overall rating. so if you",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "uct laptop, the word long is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "nd we also need to consider the order of those categories and we talk about the ordinal regression. for solving this problem. we have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem. the next two papers are about the generative models for letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model whe",
        "label": "intro"
      }
    ]
  },
  {
    "text": "text retrieval",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n component expressions. so we'll learn the structure and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional rel",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effectiv",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "l to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. but here we also find it con",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " 'eats'. so now we are going to talk about how to solve these problems. most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ntroduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. so to address the first problem, we can use a sub linear transformation of term frequency. that is",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " second problem, we can put more weight on rare terms. that is, we can reward matching a rare word and this heuristic is called idf term weighting in text retrieval. idf stands for inverse document frequency. so now we're going to talk about the two heuristics in more detail. first, let's talk about the tf transf",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " value, it would behave more like the linear transformation. so this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. so we just talk about how to solve the problem of over emphasizing a frequently frequent term. now l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "re are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weigh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d top words with high probability is about this model on the left. and then we hope to examine the impact of two events. one is the start of trec for text retrieval conference. this is a major evaluation effort sponsored by us government and was launched in 1992 or around that time and that is known to have made ",
        "label": "use"
      }
    ]
  },
  {
    "text": "multiple topics",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "that are shared by these topics. well, when i say shared, that just means even with some probability threshold you can still see one word to occur in multiple topics. in this case i marked them in black so \"you can see \"\"travel\"\",\" for example, occured in all the three topics here, but with different probabilities",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. here",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc. so the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e is that we're going to have more than two topics. otherwise it's essentially the same. so here i illustrate how we can generate the text that i was multiple topics. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ask the question what's the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. in text clustering, however, we only allow a docume",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " from one through k to indicate one of the k clusters. and basically tells us document di is in which cluster. as illustrated here, we no longer have multiple topics covered in each document is precisely one topic, although which topic is still uncertain. there is also a connection with the. problem of mining. one",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "y the model, but how well, let's first think about why this model cannot be used for clustering, and i just say the reason is because. it has allowed multiple topics to contribute the words to the document. and that causes confusion because we're not going to know which cluster this document is from an it's more i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ere are also many applications like that. there could be more than two categories, so topical categorisation is often such example where you can have multiple topics. email routing would be another example when you may have multiple folders, or if you route the email to the right person to handle it, then there ar",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "gorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the problem of deciding which cluster document d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "willing to explain the generation process of text data that has context associated in such a model. so as you see here, we can assume there are still multiple topics. for example, some topics might represent the themes like a government response donation or the city of new orleans. now this example is in the conte",
        "label": "use"
      }
    ]
  },
  {
    "text": "language processing",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "e we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. first, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "this lecture is about the text representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent text in many different ways. let's take a look at this example sentence again. we can represent this sentence in many diffe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but in the form of a sequence of words. so here we see that we can ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": " characters. but in english it's very easy to obtain this level of representation, so we can do that all the time. now if we go further to do natural language processing, we can add a part of speech tags. now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "... another level of analysis that would be very interesting. so this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. and unfortunately, such techniques would require more human effort. and they are less accurate. that means there are mistakes.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "re general and robust, so they are applicable to any natural language. that's a big advantage over other approaches that rely on more fragile natural language processing techniques. secondly, it does not require much manual effort or sometimes it does not require any manual effort. so that's again important benefit, b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "can also then identify the context. new england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened. analyzing sentiment",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "use they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. so in general, natural language processing is very important to derive complex features. they can enrich text representation. so for example, this is a simple sentence that i showed you long t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "t's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. so here are some suggested readings, the first 2. are small books that are excellent reviews of this topic whe",
        "label": "use"
      }
    ]
  },
  {
    "text": "training examples",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "of texture categorisation is defined as follows. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to class",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "m will in general assign categories to these documents as shown on the right. and the categorisation results. and we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can also be u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. for example, if you take a reviews from the internet, they might have overall rat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "t there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. for example, if you take a reviews from the internet, they might have overall ratings. so to train a sentiment categorizer meaning we want to distin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "distinguish positive from negative opinions and categorize reviews into these two categories then. we could assume five star reviews are all positive training examples. onstar negative but of course sometimes in five star reviews. we also mention negative opinions so that rain example is not all of that high quality",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "pletely reliable. but then they can still be useful. so let's assume they are actually training label examples and then we combine them with the true training examples. to improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "nt and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. that involves data that follow very different distributions from wha",
        "label": "intro"
      }
    ]
  },
  {
    "text": "bayesian inference",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate. in general, in bayesian inference we are interested in the distribution of all these parameter values. as you see, here is there's a distribution over theta values that you can see he",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " all these parameter values. as you see, here is there's a distribution over theta values that you can see here p of theta given x. so the problem of bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on theta. so i showed f of theta here as an ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "showed f of theta here as an interesting variable that we want to compute. but in order to compute this value, we need to know the value of theta. in bayesian inference, we treat data as uncertain variable. so we think about all the possible values of theta. therefore we can estimate the value of this function f as t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "de, but it's not always the same, so it gives us another way to estimate the parameters. so this is a general illustration of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "rom theta sub b? so in other words, we want to infer which distribution has been used to generate this text. now, this inference process is a typical bayesian inference situation where we have some prior about these two distributions so can you see what is our prior here? well the prior here is the probability of eac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " we don't know what word has been observed, our best guess is just say well. they are equally likely. alright, so it's just a flipping a coin. now in bayesian inference, we typically then would update our belief after we have observed evidence. so what is evidence here? while the evidence here is the word text. now t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "turns out that there's a. a very elegant way of doing that, and that's all incorporated such knowledge as priors on the models. and you may recall in bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen. so in this case we can use maximum a posteriori estim",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "et. now. these parameters that we are interested in, namely the topics and the coverage, are no longer parameters in lda. in this case we have to use bayesian inference or posterior inference to compute them based on the parameters alpha and beta. unfortunately, this computation is intractable, so we generally have t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "bout that these different extensions of lda. now here we of course can't give in depth introduction to, but just know that they are computed based on bayesian inference with. by using the parameters of alphas and beta. but algorithmically, actually in the end, in some algorithm at least, it's very similar to plsa an,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " over all the words in the vocabulary. now, what about the aspect weights? alpha sub i of d? it's not part of our parameter, right? so we have to use bayesian inference to compute it. and in this case we can use the maximum a posteriori. 2 computer this alpha value. basically we're going to maximize the product of th",
        "label": "use"
      }
    ]
  },
  {
    "text": "aspect rating",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in part",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ews, preferences and also we can understand better how reviewers view this hotel from different perspectives. now, not only do we want to. infer this aspect ratings. we also want to infer the aspect of weights, so some reviewers may care more about values as opposed to service, and that would be a case like what",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ore detailed understanding of the opinion. so the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called latent aspect rating analysis. so the task in general i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of the opinion. so the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called latent aspect rating analysis. so the task in general is given a set of review art",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "put and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. and this is a problem called latent aspect rating analysis. so the task in general is given a set of review articles about the topic with overall ratings. an we hope to generate the three things. one",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " and this prediction happens in two stages. in the first stage, we're going to use the sentiment weights of these words in each aspect to predict the aspect rating. so, for example, if in the discussion of location using a word like amazing mentioned many times and it has a high weight. for example, here is 3.9.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " discussion of location using a word like amazing mentioned many times and it has a high weight. for example, here is 3.9. then it would increase the aspect rating for location. but another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating. so the aspect ra",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ct rating for location. but another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating. so the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words. now of course these senti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " by beta sub i and w. in the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings. so we're going to assume we have aspect weights in order by of r sub of d. and this would be used to take a weighted average of the aspect ratings,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "e aspect ratings. so we're going to assume we have aspect weights in order by of r sub of d. and this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the. and we can assume the overall rating is simply a weighted average of this aspect ratings. so this setup all",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "hted average of the aspect ratings, which are denoted by our supply of the. and we can assume the overall rating is simply a weighted average of this aspect ratings. so this setup allows us to predict the overall rating based on the observed word frequencies. so on the left side you will see all these observed i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ers of course are also very different. but if you can see if we can uncover these parameters, that would be nice because also r of d is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub id is precisely the aspect weights that we hope to get. as a b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this model is set up as follows. so all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings. r sub of d as shown here is normal distribution has a variance of or square. now of course, this is just what our assumption in the actual reading ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ways when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. in this case, the aspect ratings and aspect of weights. now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "ormal way to model the problem, and that allows us to compute interesting quantities. in this case, the aspect ratings and aspect of weights. now the aspect rating as you see on the second line is assumed to be weighted sum of these weights where the weight is just sentiment wait. so. as i said, the overall rati",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "to be weighted sum of these weights where the weight is just sentiment wait. so. as i said, the overall rating is assumed to be a weighted average of aspect ratings. now this alpha values of a alpha sub of d together by our vector that depends on d is the document specific weights and we can assume this factor i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "of other values from this multivariate gaussian prior distribution and once we get these alpha values were going to use, then the weighted average of aspect ratings as the mean here to use the normal distribution. and to generate the overall rating. now the aspect rating as i just said is the sum of the sentimen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "oing to use, then the weighted average of aspect ratings as the mean here to use the normal distribution. and to generate the overall rating. now the aspect rating as i just said is the sum of the sentiment weights of words in their spectrum. note that here the sentiment weights are specifically to aspects, so b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "this lecture is a continued discussion of latent aspect rating analysis. earlier we talked about how to solve the problem of lara in two stages when we first do segmentation of different aspects and then we use a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " to solve the problem of lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights. now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "a. and then we can then plug in the latent regression model to use the text to further predict the overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. so this would give us a unified generative model where we model both the gen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "o if you just look at the overall rating you don't. you can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. dimensions like value, but others might score better in other dimensions like location and so t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " other variations of the problem and techniques proposal for solving the problem. the next two papers are about the generative models for letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated w",
        "label": "intro"
      }
    ]
  },
  {
    "text": "prior knowledge",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " of text mining. so one way to address this problem is actually to use bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. we assume that we have some prior belief about the parameters. now in this case, of course, so we are not going to look at just",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "much for a detour about bayes rule. so in our case, what we're interested in is inferring the theta values so we have a prior here. that includes our prior knowledge about the parameters. and then we have the data likelihood here that would tell us which parameter value can explain the data well. the posterior pro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ion of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "s of plsa, and one of them is lda or latent dirichlet allocation. so the plan for this lecture is to cover two things. one is to extend the plsa with prior knowledge that would allow us to have in some sense a user controlled plsa, so it doesn't blindly just listen to data but also would listen to our needs. the s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "enerative model fully generated model. this has led to the development of latent dirichlet allocation or lda. so first let's talk about the plsa with prior knowledge. in practice, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " to 0. and this would mean we don't allow that topic to participate in generating that document. and this is only reasonable, of course, when we have prior knowledge that strongly suggests this.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "erally not accurate. so we have to do smoothing to make sure it's not zero probability. the other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solv",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "y to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "or a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. there",
        "label": "use"
      }
    ]
  },
  {
    "text": "word associations",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ta, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ord association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effective. it's often called query ex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " query expansion. or you can use related words to suggest related queries to the user to explore the information space. another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. the user could navigate from one wor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "words as nodes and associations as edge. the user could navigate from one word to another to find information in the information space. finally, such word associations can also be used to compare and summarize opinions. for example, we might be interested in understanding positive and negative opinions about iphone ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "tagmatic relation. mainly we need to capture the correlation between the occurrences of two words. so to summarize, the general ideas for discovering word associations are the following. for paradigmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "f stands for term frequency\u00a0 idf stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. so these are statistical methods, meaning that the function is defined mostly based on statistics. so the scoring function would be very general. it",
        "label": "use"
      }
    ]
  },
  {
    "text": "observed data",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " why it varies as you change the value of lambda. what we're interested in here is to find the lambda star that would maximize the probability of the observed data. so this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "at is, we're interested in the sampling process. the other is the estimation process and that is to figure out the parameters of the model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram la",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "on, different text will have different probabilities. now let's look at the estimation problem. now, in this case, we're going to assume that we have observed data. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "s out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewh",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. but this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "is further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. ",
        "label": "use"
      }
    ]
  },
  {
    "text": "world distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "esented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the topic coverage in each docu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the ba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "lue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize that well we can simply take all these words that are known to be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "en though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities. so these words are believed to be more likely from the topic. these, on the other hand, are less likely proba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "be also useful because our main goal is to estimate these word distribution right? so this is our primary goal. we hope to have a more discriminating world distribution. but the last column is also by product and this actually can also be very useful and you can think about that. and one use is to. for example, is ma",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " add mu multiplied by the probability of w given our prior distribution to the connected counts. when we re estimate the when we re estimate the this world distribution right? so this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "pic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world distribution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "of just two clusters. and so in this case you can see it's a sum of two cases. in each case it's indeed the probability of choosing the. choosing the world distribution. is theta one or theta two right? and then it's this probability is multiplied by the probability of observing this document from this particular dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "o just model the text. and so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. because in the. in the case of loca",
        "label": "use"
      }
    ]
  },
  {
    "text": "sentiment classification",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "this lecture is about the sentiment classification. if we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "e sentiment classification. if we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. so suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " also know the content and context of the opinion. then we mainly need to decide the opinion sentiment of the review. so this is a case of just using sentiment classification for understanding opinion. sentiment classification can be defined more specifically as follows: the input is opinionated text object. the output is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "hen we mainly need to decide the opinion sentiment of the review. so this is a case of just using sentiment classification for understanding opinion. sentiment classification can be defined more specifically as follows: the input is opinionated text object. the output is typically, a sentiment label or sentiment tag, and t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "orisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "erhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "this lecture is about the ordinal logistic regression for sentiment analysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we have an opinionated text document d as input an we want to generate as output already in the ran",
        "label": "use"
      }
    ]
  },
  {
    "text": "classification accuracy",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "ive method to perform better than a less effective one, even though the measure is not perfect. so the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions. so here you show that here you see that there are k categories denoted by c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "our combinations in total and two of them are correct and when we have y plus or minus and then there are also two kinds of errors. so the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. so we know that the tot",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": " to come into your inbox, so the error of the first missing a legitimate email is very high cost. it's very serious mistake. and classification error classification accuracy does not address this issue. there's also another problem with imbalanced tests at the imagine there's a skew. the test set where most instances are ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "y. in this case, it's going to be appearing to be very effective, but in reality this is obviously not a good result. and so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced. and we wonder about equal number of instances. for example, in each class the minority",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "d we wonder about equal number of instances. for example, in each class the minority categories or classes tend to be overlooked in the evaluation of classification accuracy. how to address these problems? we of course would like to also evaluate the results in other ways and in different ways. as i said, it's beneficial ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "es. or we do that for each document and then aggregate over all the documents. but here we pulled them together. now this will be very similar to the classification accuracy that we introduced earlier, and one problem here of course, is to treat all the instances, all the decisions equally. and, this may not be desirable.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "te for some applications, especially if we associate, for example, the cost for each combination. then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision. so there could be variations of these methods that would be more useful",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": " of course has to be designed in application specific away. some commonly used measures for relative comparison of different methods or the following classification accuracy is very commonly used for especially balanced tester set. precision, recall, and f scores are commonly reported to characterize the performances in d",
        "label": "use"
      }
    ]
  },
  {
    "text": "maximum likelihood estimator",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "d then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviousl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "efine our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informative prior,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "eters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " theta by assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see la",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that look",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constrain",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "er parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "obabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ing behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on differ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "deo a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat higher pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "meters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "o. this gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in the m step, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "nguage model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "a, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have some expe",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ly favor certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the prior calle",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "obabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimate the para",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "hat are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most popular, then",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "moothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard opt",
        "label": "use"
      }
    ]
  },
  {
    "text": "background language model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ive. this is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. in fa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "arize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum like",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "nguage models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of model set",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ne dominate. in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "l in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ground language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of backgrou",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrenc",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also see a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially rem",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "k about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categorie",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "mate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "seful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're ",
        "label": "use"
      }
    ]
  },
  {
    "text": "contextual text mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "detail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining becaus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "cs and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ic technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with approp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "d we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "urally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to know mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see some sam",
        "label": "intro"
      }
    ]
  },
  {
    "text": "maximum likelihood estimate",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating the proba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "is question we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "s posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "edge, we have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplification of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "s point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it's the mos",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "en a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're interest",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ", we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood func",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "nd then all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the idea of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "e in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. no",
        "label": "use"
      }
    ]
  }
]