[
  {
    "text": "word distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "eights of\u00a0 all the words. this is to again ensure all the x(i) will sum to one in this vector. so this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one. now the weight of bm25 for each word is defined here. and if you compare this with our old definition where we just have a normalized count. on this one, right? so we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. but now we're going to use a word distribution to describe the topic. so he",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "representation of topic as a word distribution. so what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. but now we're going to use a word distribution to describe the topic. so here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary. so for example, the high probability words he",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "where we represent each topic with just one word or one term or one phrase. but now we're going to use a word distribution to describe the topic. so here you see that for sports, we're going to use a word distribution over theoretical speaking all the words in our vocabulary. so for example, the high probability words here are sports, game, basketball, football, play, star, etc. these are sports-related terms and ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "course, our problem definition has been refined just slightly. the slide is very similar to what you have seen before, except that we have added refinement for what the topic is. so now each topic is word distribution. and for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. so you see a constraint here and we still have another constraint on the to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "as been refined just slightly. the slide is very similar to what you have seen before, except that we have added refinement for what the topic is. so now each topic is word distribution. and for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. so you see a constraint here and we still have another constraint on the topic coverage, namely pis. so all",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "alysis. in most cases, we use words as the basis. for analysis, and that means each word is a unit. now the output would consist of as first a set of topics represented by theta i's each theta_i is a word distribution. and we also want to know the coverage of topics in each document so that that's the same pi_ij's that we have seen before. so given a set of text data, we would like to compute all these distributio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "higher probabilities than others. now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. first, we have theta_i's each is a word distribution and then we have a set of pi's for each document. and since we have n documents so we have n sets of pis. and each set of the pi values will sum to one. so this is to say that we first pretend we alr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "e a set of pi's for each document. and since we have n documents so we have n sets of pis. and each set of the pi values will sum to one. so this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. so how do we model the data in this way? and we assume that data are actually samples drawn fr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "y the discovered knowledge from text data. by varying the model, of course we can discover different knowledge. so to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic. it also allows us to assign weights on words so we can model subtle variations of semantics. we talked about the task ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "f topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. the number of topics and vocabulary set and the output is a set of topics. each is word distribution. and also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "so the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the topic coverage in each document. a document is n",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "er of words in our vocabulary. so here we assume we have n words, so we have n probabilities, one for each word, and they sum to one. so now we can assume our text is a sample drawn according to this word distribution. that just means we're gonna draw a word each time and then eventually we'll get a text. so for example now again. we can try to sample words according to a distribution. we might get wednesday often",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ess and that is to figure out the parameters of the model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a di",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ed the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of parameter values, this function can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "s procedure for some more complicated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this case m. and for convenience we're going to use theta sub i to denote th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "which of the two distributions to use, and this is controlled by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basically flip a coin",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "r probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basically flip a coin a fair coin to decide which one to use. but in general these probabili",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "rocess of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "o flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in such a case we have a model that has some uncert",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating text data an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ight be going through this path. and we're going to use the background word distribution to generate the word. so in such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating text data and such a model is called a mixture model. so now let's see. in this case, what's the probability of observing the word w? \"now he",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " word here? now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. therefore it's a sum over these two cases. the first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of theta sub d, which is the probability of choosing the model multiplied by the probability of actually observ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ckground. now obviously the probability of text the same is all similar, right? so we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution. now later you will see this is actually general form, so you might want to make sure that you have really understood this e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "allow us to discover the interesting knowledge about the text, so in this case, what do we discover? well, these are represented by our parameters, and we have two kinds of parameters. one is the two word distributions. those are two topics and the other is the coverage of each topic in each. the coverage of each topic and this is determined by probability of theta sub d. and probability of theta sub b. note that ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "re going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ut background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our target. so this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in. but we are going to simplify other things",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ing to assume we have knowledge about others. and this is a powerful way of customizing a model for a particular need. now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust theta sub d i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "bilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the behavior of a mixture model. so we're going to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "and what's the probability of the? i hope by this point and you will be able to write it down. so the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. and it accounts for the two ways of generating text. an inside each case we have the probability of choosing the model which is .5 multiplied by the probability of observing text from that model. si",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ls. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. a large collection of english documents, by using just one distribution and then we'll just have normalized frequencies of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": ". now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "d mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "r words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize that well we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "em of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and normalize them. so indeed this problem will be very easy to solve. if we had known which words are from which distribution precisely. and this is in fact making this model no longer m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " and this is in fact making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub i. and th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ing to say text is more likely from theta sub d. so you see our guess of which distribution has been used to generate the text would depend on how high the probability of the data the text is in each word distribution. we are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so. we're going to choose word that has a higher likelihood.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ose z values. we just imagine there are such a social values of z attached to all the words. and that's why we call these hidden variables. now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. the value of this hidden variable. and so. the algorithm, the em algorithm then would work as follows. first will initialize all t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " we're going to repeat this. we're going to use the e-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of em algorithm. start with initial values that are often ra",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ties that a word is believed to have come from one distribution. in this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? so this is our primary goal. we hope to have a more discriminating world distribution. but the last column is also by product and this actually can also be very useful and you can think about ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "se me, you see two interesting kinds of parameters. those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. and the other is the word distributions that characterize all the topics. so the next line then is simply to plug this in to calculate the probability of document. this is again of the familiar form where you have some and you have accoun",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "n see this is the same discounted count, it tells us to what extent we should allocate this word to topic theta sub-j. but the normalization is different because in this case we are interested in the word distribution. so we simply normalize this over all the words. this is different. in contrast, here we normalized among all the topics. it would be useful to take a comparison between the two. this gives us differ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "s. so i'm going to go over this in some detail. so as the algorithm, we first initialize all the unknown parameters randomly. in our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. and we just randomly normalize them. this is the initialization step, and then we will repeat until likelihood converges. now how do we know whether likelihood converges we're going to comp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "e are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. and i said it's proportional to this because in completing the implementation of em algorith",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "to obtain the re-estimate. so, for example, we can normalize among all the topics to get re estimate of pi the coverage. or we can renormalize based on the. for all the words and that would give us a word distribution. so it's useful to think of the algorithm in this way, because when you implement, you can just use. variables to keep track of these quantities in each case. and then you just normalize these variab",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "l can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. and such detailed characterization of coverage of topics in documents can enable a lot of further analysis.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "e map estimator with the conjugated prior, which is dirichlet prior dirichlet distribution based on this preference, then the only difference in the em algorithm is in the m step. when we re estimate word distributions, we are going to add. additional counts to reflect our prior right? so here you can see the pseudocounts are defined the based on the probability of words in our prior. so battery obviously will hav",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again fac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "rage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. so the inference part. again, face the local maxima problem. so essentially they are doing",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "hat it enables. so this is a picture of lda. now i remove the background model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also. and this is for convenience to introduce lda and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "ound model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distributions. so here and the other set of parameters are pis and we present as a vector also. and this is for convenience to introduce lda and we have one vector for each document. and in this case in theta we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "e, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our inference on the k values of pis for a document, whereas here beta h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " background for simplicity. so in the lda formulas you see very similar things. first you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions. and this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multiplied by the probability of observing the world fro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " way of mining and analyzing topics in texts with many applications. the best basis test setup is to take tax data as input, and we're going to output the key topics. each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic topic model. and this is also often adequate for mo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "del. here we show that we have input of text collection c and number of topics k and vocabulary v, and we hope to generate as output two things. one is a set of topics denoted by theta i's. each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "rtain. there is also a connection with the. problem of mining. one topic that we discussed earlier. so here again it's a slide that you have seen before. and here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. but we can also consider some variations of the problem. for example, we can consider there a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "t you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the k word distributions that we have. but this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the all the words in the document. and that means once",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "s. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. multiple distribution",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on one document. so that's the connection that we discussed earlier. but now you can see more clearly. so as more cases of using a generative model to solve a problem, we first look at theta an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "bout the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ing, right? so theta. i, for example, represents the content of class i. this is actually a byproduct. it helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution. and they will tell us what the cluster is about. an p of theta i can be interpreted as. indicating the size of cluster because it tells us how likely cluster would be used to generate the document. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ntities, the magnitude. so we can then divide the numerator and the denominator both by this normalizer. so basically this normalizes the probability of generating this document by using this average word distribution. so you can see the normalizer here. and since we have used exact the same normalizer for the numerator and denominator, the whole value of this expression is not changed. but by doing this normaliza",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "n assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "d we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single lang",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "els to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this question boils down to decide which theta i has been use",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and this question boils down to decide which theta i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. that we need to consider if a topic or cluster has",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ior then it's more likely that the document has been from this cluster, so we should favor such a cluster. the other is a likelihood part, that is this part. and this has to do with whether the topic word distribution can explain the content of this document well. and we want to pick a topic that's high by both values. so more specifically, we just multiply them together and then choose which topic has the highest",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "o update this probability based on the prior and i shown the details. below here you can see how the prior here is related to the posterior on the left hand side. and this is related to how well this word distribution explains the document here, and the two are related in this way. so to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also mu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "in generating each word ok. so this is just something that you have seen in document clustering. an we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. so this idea can be directly adapted to do categorization and this is precisely what naive bayes classifier is doing, so here it's mostly the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lassifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if theta i represents category i accurately that means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely like what we did for text clustering. namely, we are going to assign document d to the category that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " a very simple case. perhaps the simplest case. so now the question is, how can we make sure each theta i actually represents category i accurate? now, in clustering we learned this category i or the word distributions for category i from the data. but in our case what can we do to make sure this theta i represents indeed category i? if you think about the question and you're likely to come up with the idea of usi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "re are two kinds. so one is the prior. the probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. the other kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate these two probabilities. and in general we can do thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ld be very useful for you to pause the video for a moment and to think about how to solve this problem. so let me state the problem again, so let's just think about category one. we know there is one word distribution that has been used to generate documents. and we generated each word in the document independently and we know that we have observed the set of n sub one documents in the set of t1. these documents h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ependently and we know that we have observed the set of n sub one documents in the set of t1. these documents have been all generated from category one, namely have been all generated using this same word distribution. now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category? of course, this se",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " this count to make this a probability. in other words, we make this probability proportional to the size of training dataset in each category. that's the size of the set t sub i. now, what about the word distribution? well, we do the same again. this time we can do this for each category. so let's say we are considering category i or theta i. so which word has higher probability? well, we simply count the word oc",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s just a uniform. what if delta is zero? well we just go back to the original estimate based on the observed training data to estimate the probability of each category. now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the difference of the probability of such words smaller across ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "tion of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "s on context. one is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that we assume. the topic coverage also depends on the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " orleans. now, as you can see, we assume there are different views associated with the each of the topics. and these are shown as view one, view two and view three each view is a different version of word distributions. and these views are tide to some context variables. for example, type to the location texas or the time july 2005 or the occupation of the other being sociologist. now on the right side you see now",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "st also choose a view. and this view of course now could be from any of these contexts. let's say we have taken this view. that depends on the time in the middle. so now we have a specific version of word distributions. now you can see some probabilities of words for each topic. now, once we have chosen a view, now the situation will be very similar to what happened in standard plsa. we assume we have got a word d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "butions. now you can see some probabilities of words for each topic. now, once we have chosen a view, now the situation will be very similar to what happened in standard plsa. we assume we have got a word distribution associated with each topic, right? and then next to the view we choose a coverage from the bottom. so we're going to choose particular coverage and that coverage. before is fixed in plsa and it's har",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "that has influenced the coverage. so, for example, we might pick a particular coverage, let's say in this case. we pick we've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in plsa. so what it means we're going to use the coverage to choose a topic to choose one of these three topics. let's say we have picked up,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in plsa. now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. so in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " the background, of course this is not surprising and this is. this topic is indeed very relevant, to both wars. if you look at the column further and what's interesting is that the next two cells of word distributions actually tell us collection specific variations of the topic of united nations. so it indicates that in iraq war, united nations was more involved in weapon inspections, whereas in afghanistan war i",
        "label": "use"
      }
    ]
  },
  {
    "text": "mixture model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "n such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating text data and such a model is called a mixture model. so now let's see. in this case, what's the probability of observing the word w? \"now here i showed some words like \"\"the\"\"\" \"and \"\"text\"\", so as in all cases, once we\" set up the model, we're intere",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "really understood this expression here. and you should convince yourself that this is indeed the probability of observing text. so to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. and in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually obs",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": ". multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. so the basic idea of a mixture model is just to treated these two distributions together as one model. so i use the box to bring all these components together. so if you view this whole box as one model, it's just like any other generat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "del. it would just give us the probability of a word. but the way that determines this probability is quite different from when we have just one distribution. and this is basically a more complicated mixture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ability is quite different from when we have just one distribution. and this is basically a more complicated mixture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration that you have seen before, which is dimmer now is jus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "probability of selecting a component like theta sub d. second, the probability of actually observing the word from this component model. and so this is a very general description of, in fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likeli",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "lihood function. it will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0. so in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "on and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can se",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ocument exactly the same as before. the only difference is that inside here now it's a sum instead of just one, so you might recall before we just had this one. but now we had this sum because of the mixture model and because of the mixed model we also have to introduce the probability of choosing that particular component distribution. and so this is just another way of writing it again by using a product ove",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ground here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our targe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "e this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub d, which is our target. so this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in. but we a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "nd up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the behavior of a mixture model. so we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in gene",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ion. so to understand why this is so, it's useful to examine the behavior of a mixture model. so we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here. so spec",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "or of a mixture model. so we're going to look at a very, very simple case in order to understand some interesting behaviors of a mixture model the observed patterns here actually are generalizable to mixture model in general, but it's much easier to understand this behavior when we use a very simple case like what we're seeing here. so specifically in this case. let's assume that the probability of choosing ea",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " it's very easy to see that. once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model. now the interesting question now is, how can we then optimize this likelihood? well, you will notice that there were only two variables. they are precisely the two probabilities of the two words tex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " so in order to compensate for that we must make the probability of text given by theta sub d somewhat larger so that the two sides can be balanced. so this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. would tend to do the opposite. basically it would discourage other distributions to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies. ok, so what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solut",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ecrease it to less than\u00a0 0.1? what do you think? now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before eac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " it's more important than to have higher values for these frequent words. if you have a very small probability of being chosen, than the incentive is less. so to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " frequent words. if you have a very small probability of being chosen, than the incentive is less. so to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st every component component model attempts to assign ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ction of english documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. and that would make the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ontinue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estim",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. on the other ha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "n theta sub d and normalize them. so indeed this problem will be very easy to solve. if we had known which words are from which distribution precisely. and this is in fact making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the single word distribution problem, and in this case let's call these w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "he parameters can we infer the distribution the word is from so let's assume that we actually know tentative probabilities for these words in theta sub d. so now all the parameters are known for this mixture model. and now let's consider word like\u00a0 text. so the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub b? so in other words, we want to infer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e're going to take a guess of these z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply bayes rule to infer which distribution is more likely to generate each word and this prediction essentially helped us to separate words from the two distributions, a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "o this curve is reaching or like roller function, right? so this one. and this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. but in the case of mixture model, we cannot easily find the analytical solution to the problem. so we have to resolve a numerical algorithm. an em algorithm is such an algorithm. it's a hill climb algorithm that would mean you start",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "e other way to determine a good initial starting point. to summarize, in this lecture we introduce the em algorithm. this is a general algorithm for computing. maximum regular is made of all kinds of mixture models. so not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. the general idea is that we will have two st",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "rting point. to summarize, in this lecture we introduce the em algorithm. this is a general algorithm for computing. maximum regular is made of all kinds of mixture models. so not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. the general idea is that we will have two steps to improve the estimate of parameters i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "vers which topic to what extent. so we hope to generate these as output because there are many useful applications if we can do that. so the idea of plsa is actually very similar to the two component mixture model that we have already introduced. the only difference is that we're going to have more than two topics. otherwise it's essentially the same. so here i illustrate how we can generate the text that i wa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "cs. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ask the question what's the probability of observing a world w from such a mixture model? now if you look at this picture and compare this with the picture that you have seen earlier, you will see the only difference is that we have added more topics here. so before we have just one topi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "s just a sum of all these cases. and i have to stress again, this is a very important formula to know because. this is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we have the likelihood function, we would be interested in knowing the parameters righ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "his one. so in general, in the implementation of em algorithm you will see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can he",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "nd you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of getting a word from a mixture model. now next in the probability of a document we see there is a plsa component in the lda formula. but the lda formula would add some integral here, and that's to explain to account for the fact that th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it looks similar. so here we see that when we generate a document. we generated each word independently. and we generated each word first make a choice between these dis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " for clustering because it did not ensure that only one distribution has been used to generate. all the words in one document. so if you realize this problem, then we can naturally design alternative mixture model for doing clustering. so this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potent",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ny decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models. but this is obviously also a mixture model, so we can just group them together as one box to show that this is. model that will give us a probability of a document. now inside this model there's also this, which of choosing a different distri",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "x to show that this is. model that will give us a probability of a document. now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. and of course, the main problem in document clustering is to infer. which distribution has been used to generator a document and that would allow us to recover the cluster identity over document so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "oosing a particular distribution is equal to 1. now that just means we have no uncertainty now. we just stick with one particular distribution. now in that case, clearly we will see this is no longer mixture model 'cause there's no certainty here and we're going to just use precise one of the distributions for generating a document, and we're going back to the case of estimating one word distribution based on ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "re in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data. i in this case, so it's going to be some over these k topics because everyone can be used to generate the document and then ins",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "omponents here. so here you can see at the formula looks very similar or in many ways they are similar. but there's also some difference. and in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. and that's corresponding to our assumption of 1st make a choice of choosing one distribution and then stay with this distribution to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "why we have the product outside. but when we generate each each word, we have to make a decision regarding which distribution we use. so we have sum there for each word. but in general, ideas are all mixture models that we can estimate these models by using the em algorithm as we will discuss more later.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to include the k clusters. now if you look at the formula and think about the question how to generalize it, you will real",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "you can just add more thetas and the probabilities of thetas and the probabilities of generating d from those thetas. so this is precisely what we're going to use. this is general presentation of the mixture model for document clustering. so as more cases we follow these steps using a generated model. first think about our data, right? so in this case our data is a collection of documents n documents denoted b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "odel. so after we have estimate the parameters, how can we then allocate clusters to the documents? let's take a look at this situation more closely, so we just repeated the parameters here. for this mixture model. now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right? so theta. i, for example, represents the content",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "lustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ers. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to ada",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "is em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorithm starts with initialization of all the parameters. so this is the same as what happened before for topic models. and then we're going to repeat until their likeliho",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "nt, so this first thing about the hidden variables. now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must te",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "es. now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must tell us which distribution has been used to genera",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must tell us which distribution has been used to generate the document, so it's going to take",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "e define objective function. given k clusters. so it can be also shown this process will converge to a local minimum. i think about this process for a moment. it might remind you the em algorithm for mixture model. indeed, this algorithm is very similar to the em algorithm for the mixture model for clustering. so more specifically, we also initialize these. predators in the em algorithm, so the random inner in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "s will converge to a local minimum. i think about this process for a moment. it might remind you the em algorithm for mixture model. indeed, this algorithm is very similar to the em algorithm for the mixture model for clustering. so more specifically, we also initialize these. predators in the em algorithm, so the random inner inner initialization is similar. and then in the eml with them, you may recall that ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ithm. in that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. so it's essentially similar to eastep. also, what's the difference? while the differences here, we don't make a probabilistic on location as in the case of the step. but rather we make a choice. we'",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "here defined objective function and it's guaranteed to convert converted local minimum. so to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "got a good number of clusters to explain our data well and to do that, you can vary the number of clusters and watch how well you can fit the data. if it's in general, when you add more components to mixture model, you should fit the data better, because you can always set the probability of using the new component at 0, so you can't in general fit the data worse than before, but as the question is, as you add",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "make this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generative model for documents in each category. can vary, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "the documents that are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. there's no mixing of different categories here. so the estimation problem of cour",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "it unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categ",
        "label": "use"
      }
    ]
  },
  {
    "text": "text mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn tex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially for decision making or for completing whatever tasks that require text data to suppor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "re general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this sli",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " going to focus more on text mining. but we can also touch how to join the analysis of both text data and non-text data. with this problem definition we can now look at the landscape of the topics in text mining analytics. now this slide shows the process of generating text data in more detail. most specifically, human sensor or human observer would look at the world from some perspective. different people w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "f course, the person could have used a different language to express what he or she has observed. in that case, we might have text data of mixed languages for different languages. so the main goal of text mining is actually to revert this process of generating test data. and we hope to be able to uncover some aspect in this process. and so specifically we can think about the mining, for example, knowledge ab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? so indeed",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "owledge about the observer. finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? so indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. and we want to predict the value of certain interesting variables. so this picture basically covered multiple types",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "content or the language usage or the opinions about the observer or the authors of text data. we could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. in this course we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. first, we are going to cover natural language processing v",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "t of these general topics. first, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text bas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "since we can always use this approach to represent any text data. but unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. the reason is because we're not even recognizing words. so as a string we're going to keep all the spaces and these ascii symbols. we can perhaps count how... what's the most frequent character in e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "tion in our mind, and need for solving a lot of problems. now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. that's the purpose of text mining. so there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ually give us the necessary deeper representation of knowledge. i should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. they are always in the loop. meaning that we should optimize the collaboration of humans and computers. so in that sense, it's ok that computers may not be able to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "s course is covering techniques mainly based on word based representation. these techniques are general and robust and thus are more widely used in various applications. in fact, in virtually all the text mining applications you need this level of representation and the techniques that support analysis of texting this level. but obviously all these other levels can be combined and should be combined in order",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ut here we're going to introduce a general way of solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the picture that you have seen before in order to show the generation process. so the idea of this approach is actually to 1st design a model for our data. so we design a pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit the data as well as we can. after we have fitted data then we wil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " topic outside the set of topics that we are discovering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they are generated this way and inside the model, we embed some parameters that were interested in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "mpling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the question about what is the probability of generatin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "er probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ords that often occur in text mining context. so in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conferen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "kely will see text that looks like a text mining paper of course. ... the text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, inclu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ition and healthy, diet etc. so this clearly indicates a different topic and in this case it's probably about health. so if we sample words from such distribution, then the probability of observing a text mining paper would be very very small. on the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher. so that just means given a particular d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ies. now let's look at the estimation problem. now, in this case, we're going to assume that we have observed data. we know exactly what the text data looks like. in this case, let's assume we have a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we ask the question, what is the most likely language model th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "mple is small, then if we trust data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not op",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "maximizing the likelihood of the observed data. but this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. so one way to address this problem is actually to use bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. we assume that we have some",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ng to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the high probability words tend to be those very common words, often functional words in english, and this wil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " normalize the count to estimate the probabilities or to revise our estimate of the parameters. so let me also illustrate we can group the words that are believed to have come from cedar sub d and as text mining algorithm for example and clustering. and we had group them together. to help us re estimate the parameters. that were interested in so these will help us re estimate these parameters. but note that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "the model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. this, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. we are not always interested in modeling future data, but in other cases or if we care about generality, we w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " so in this sense, lda is just a bayesian version of plsa and the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "hat we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say all the email messages from customers in some time period, or all the literatu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "icit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is often the first step when you deal with a lot of text dat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " ages and other things can be actually connected to text data indirectly. once we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using text data as extra sensor data coll",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ucts? market research has to do with understanding consumers opinions and this is clearly very useful, directed for that. data driven social science research can benefit from this because they can do text mining to understand the people's opinions. and if we can aggregate a lot of opinions from social media from a lot of public information, then you can actually do some study of some questions. for example, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "der. for example, we could use ordinal regression to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-grams. you can just have a sequence of characters as a unit,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "fer some information that's very hard to obtain, even if you read all the reviews. even if you read all the reviews, it's very hard to infer such preferences or such emphasis. so this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. you can compare different hotels, compare the opinions from different co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "different consumer groups in different locations, and of course the model is general. it can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. finally, there is also some result on applying this model for personalized ranking or recommendation of entities. so because we can infer the reviewers weights on different dimensions, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels. so this is yet another application of this technique. and shows that by doing text mining we can understand the users better. and once we can end users better, we can serve these users better. so to summarize our discussion of opinion mining in general, this is a very important topic and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " accuracy of prediction. and this big picture is actually very general and it's reflecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for prediction about human be",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "big picture is actually very general and it's reflecting a lot of important applications of big data. so it's useful to keep that in mind while we're looking at some text mining techniques. \" so from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. and this is most useful for prediction about human behavior or human preferences or opi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "provides a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question that we have not addressed yet. so in this lectur",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e put together with many other predictors they can really help improving the accuracy of prediction. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer values of real world variables. but in order to achi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "e can mine from text. as we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. in one perspective, we can see non text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ee this reference listed here for more detail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "he reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and si",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": " based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "s additional context. of the authors and this would allow us to then compare such a subset with another set of papers written by authors in other countries. or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. topic. and note that these partitioning can be also intersect with each other to generate even more complicated partitions. and so in genera",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "what issues mattered in the 2012 presidential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent sema",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "e discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilistic lat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "is case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here. i just want to explain the high le",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "eters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see some sample results of comparing news articles about iraq ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "can use this to analyze the impact of any event. here are some suggested readings. the first is paper about simple extension of plsa to enable cross collection comparison. it's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. the second one is the main paper about the cplsa model with a discussion of a lot of a",
        "label": "intro"
      }
    ]
  },
  {
    "text": "topic model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you hav",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuitively we need to use more words to describe the topic an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "zzy manner. finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the old repr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, wh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "odels which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. so in thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e are interested in just mining one topic from one document. so in this simple setup we are interested in analyzing one document and trying to discover just one topic. so this is the simplest case of topic modeling. the input now no longer has k, which is the number of topics because we know there is only one topic. and the collection has only one document also. in the output we also no longer have coverage ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ive, which is often needed for finding the optimal solution of this function. so please take a look at this sum again here and this is a form of function that you often see later also in more general topic models. so it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document. and this is multiplied by the logarithm of the probability. so let's see how we can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ing another distribution to account for just these common words. this way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. this way our target is the topic theta here would be only generating the content words that characterize the content of the document. so how does this work? it's just a ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": ". and so this is a very general description of, in fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "this lecture is about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ecture is about the probabilistic latent semantic analysis or p lsa. in this lecture we're going to introduce probabilistic latent semantic analysis, often called the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "emantic analysis or p lsa. in this lecture we're going to introduce probabilistic latent semantic analysis, often called the plsa. say this is the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "s the most basic topic model. also, one of the most useful topic models. now, this kind of models can in general be used to mine multiple topics from text documents, and plsa is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. here i show a sample article which is a blog article about hurricane katrina. an i showed some sample topics, for example",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "bability of observing the world is just a sum of all these cases. and i have to stress again, this is a very important formula to know because. this is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we have the likelihood function, we would be intere",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "this lecture is about the latent dirichlet allocation or lda. in this lecture, we're going to continue talking about topic models. in particular, we are going to talk about some extensions of plsa, and one of them is lda or latent dirichlet allocation. so the plan for this lecture is to cover two things. one is to extend the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "parameters of the model. so the inference part. again, face the local maxima problem. so essentially they are doing something very similar, but theoretically lda is more elegant way of looking at the topic modeling problem. so let's see how we can generalize plsa to lda or extend the plsa to have lda now a full treatment of lda is beyond the scope of this course and we just don't have time to go in depth in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "pic multiplied by the probability of observing the world from that topic. so this is a very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of getting a word from",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "observing the world from that topic. so this is a very important formula as i have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of lda or plsa and they all rely on this. so it's very important to understand this. and this gives us the probability of getting a word from a mixture model. now next in the pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "orithm called collapsed gibbs sampling. then the algorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to take tax data as input, and we're going to output the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "nd we're going to output the key topics. each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic topic model. and this is also often adequate for most applications. that's why we spend a lot of time to explain plsa in detail. now lda improves over plsa by imposing pri",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. and plsa is the basic topic model, and in fact the most basic topic model. and this is also often adequate for most applications. that's why we spend a lot of time to explain plsa in detail. now lda improves over plsa by imposing priors. this has led to theoretically more ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "r performance, so in practice, plsa, an lda, would work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? can we use ph",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "sks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? can we use phrases to label the topic to make it more easy to understand? and this paper is a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s case documents are the units to be clustered. we may be able to cluster terms in this case. terms are objects. and cluster of terms can be used to define the concept or theme or topic. in fact, the topic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world distribution. another example is to just a cluster an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "he topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have input of text collection c and number of topics k a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have input of text collection c and number of topics k and vocabulary v, and we hope to generate as output two things. one is a set of topics denoted by theta i's. each is a wor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " ij's and these are the probabilities that each document covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one top",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "c is still uncertain. there is also a connection with the. problem of mining. one topic that we discussed earlier. so here again it's a slide that you have seen before. and here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. but we can also consider some variations of the problem. for example, we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "tructure. the topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. but if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "em and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it looks similar. so here we see that when we generate ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of k topics? as in the topic model. so let's revisit the topic model again in more detail. so this is a detailed view of two component mixture model and when we have k components it looks similar. so here we see that when we generate a document. we generated each word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "n we would have a document to be generated from precisely one topic. that means all the words in the document must have been generated from precisely one distribution, and this is not true for such a topic model that we're seeing here, and that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate. all the words in one document. so if you real",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "are this picture with the previous one, you will see the desicion of. of using a particular distribution is made of just once for this document. in the case of document clustering. but in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models. but thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "s to infer. which distribution has been used to generator a document and that would allow us to recover the cluster identity over document so it would be useful to think about the difference from the topic model, as i have also mentioned multiple times. there are many. two differences. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " model, as i have also mentioned multiple times. there are many. two differences. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the case of topic modeling, one distribu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": ", whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. multiple distribution could have been used to generate the words in the document. it's also think about the special cas",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ood function. and after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document. so this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose i am also copying the probability of. topic model with two components here. so here you can see at the formula looks very simi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ity of each word in the document. so this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose i am also copying the probability of. topic model with two components here. so here you can see at the formula looks very similar or in many ways they are similar. but there's also some difference. and in particular, the differences on the top you s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ice of choosing one distribution and then stay with this distribution to generate all the words. and that's why we had the product inside the sum. the sum corresponds to the choice. right. now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently. and that's why we have the product outside. but when we generate each each word, we have ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "cuments n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting eac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " i and then generate all the words in the document using this distribution. note that it's important that we use this distributed generator. all the words in the document. this is very different from topic model, so the likelihood function would be like what you are seeing here. so the. you can take a look at the formula here. we have used the different. notation here in the second line of this. of this equa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "d of particular position in the document. so from x sub j to w is a change of notation, and this change allows us to show the estimation formulas more easily and you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorithm starts with initialization of all the parameters",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "nd you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em algorithm starts with initialization of all the parameters. so this is the same as what happened before for topic models. and then we're going to repeat until their likelihood converges. an in each step will do e step and m step. in m step. we're going to infer which distribution has been used to generate each documen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "tly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": ". a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "a lower dimensional space. typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation, but the representation can be mapped to the topi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. lda can actually help us reduce the dimension of features. imagine the words are original feature representation, but the representation can be mapped to the topic space representation. let's say w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " can further mine correlated words. with these seed words and that would allow us to segment the text into segments. discussing different aspects, but of course later as we would see, we can also use topic models to do the segmentation, but anyway, that's the first stage where we would obtain the counts of words in each segment. in the segmentation stage, which is called latent rating regression, we're going",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "or solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "he generation of text and so a natural solution would be to use topic model. so given an entity, we can assume there are aspects that are described by word distributions. topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a generative model like psa. and then we can",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "next two papers are about the generative models for letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model. to solve the problem using a unified model.",
        "label": "use"
      }
    ]
  },
  {
    "text": "language model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distribution that gives \"\"\"today is wednesday\"\" a probability of 0.001\" i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ive an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distribution that gives \"\"\"today is wednesday\"\" a probability of 0.001\" it might give \"\"\"today wednesda",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ds. obviously it's impossible to specify that, because it's impossible to enumerate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ecify that, because it's impossible to enumerate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but after we make this assumption, we ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ssume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model. basically now the probability of a sequence of words w_1 through w_n would be just a product of each. the probability of each word. so for such a model we have as many parameters as the number of wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "the parameters of the model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ave a text mining paper. in fact, it's abstract of the paper, so the total number of words is 100, and i've shown some counts of individual words here. if we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": " shown some counts of individual words here. if we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated probabilities of these words as i've shown here. so what do you think? what would be your guess? would you ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "f we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? ok, so the problem now is just the estimated probabilities of these words as i've shown here. so what do you think? what would be your guess? would you guess text that has a very very small probabil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "imation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "or knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " we'll look at this procedure for some more complicated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this case m. and for convenience we're going to use the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier wher",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can set to attract the common words. because common wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. in fact, if it's not consistent, we're going to say the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water distribution. now next in the rest of this formula.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "e implementation of em algorithm you will see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ch is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ly 1 background the topic. now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of model setting would be 0 according to our prior. so now we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "mu is infinity, we basically let this one dominate. in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. so in this case we can even force the distribution t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " right? so in this case our data is a collection of documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is to find the clusters and we actually have used a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ging the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "d distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using mult",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "tion actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "erating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster. and this probabilistic assignment that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ization tasks. but you should know that this kind of model doesn't have to make this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "uld know that this kind of model doesn't have to make this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generative ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "category one, and t2 represents the documents that are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. there's no mixing of different categories here. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and so this is a good exercise to work on it if it's not obvious to you. there is another issue in naive bayes which is a smoothing. in fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be sm",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " we find it useful to use a non-uniform pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "l. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "uage model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "uenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categori",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also see another smoothing parameter mu here, which controls",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "there are some interesting special cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obvi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set the t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ore as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're going to score based on their ratio of probability",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "or around that time and that is known to have made an impact on the topics of research information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simpl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "mail was used in the enterprise search tasks and subtropical retrieval, with another task introduced later by trec. on the bottom we see the variations that are correlated with the publication of the language model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " that are correlated with the publication of the language model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of event again, the technique i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of event again, the technique is general so you can use this to analyze the impact of any even",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " prior so the prior here is defined by p of theta. and this means we will impose some preference on certain thetas over others. and by using bayes rule that i have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full explanation of bayes rule and some of these things related to bayesian reasoning would be outside the scope of this c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "lity distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of parameter values, this function can tell us which x, which data point has a higher li",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "just means we want to take a particular angle of looking at the data so that the model would have the right parameters for discovering the knowledge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. and the likelihood function will have some parameters in the function and then we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ge that we want, and then we'll be thinking about the likelihood function or write down the library function to capture more formally how likely a data point will be obtained from this model. and the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "in our vocabulary, in this case m. and for convenience we're going to use theta sub i to denote the probability of word w sub i. and obviously these thetas of i's would sum to one. now, what does the likelihood function look like? this is just the probability of generating this whole document given such a model. because we assume the independence in generating each word, so the probability of the word the document w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "currences. you can also see if a word did not occur in the document, it would have a zero count and therefore that corresponding term will disappear. so this is a very useful form of writing down the likelihood function that we will often use later. so i want you to pay attention to this. just get familiar with this notation. it's just to change the product over all the different words in the vocabulary. so in the e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "attention to this. just get familiar with this notation. it's just to change the product over all the different words in the vocabulary. so in the end, of course we'll use theta sub i to express this likelihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likeliho",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ", of course we'll use theta sub i to express this likelihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this like",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, bec",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "tion. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the log likelihood instead of the original likelihood and this is purely for mathematical convenience, because after the logarithm transformation, our function",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " problem. now at this point the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. the objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. so one way to solve the problem is to use lagrange multiplier approach. now this content is beyond the scope of this course. but s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "d it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " this case, what's the probability of observing the word w? \"now here i showed some words like \"\"the\"\"\" \"and \"\"text\"\", so as in all cases, once we\" set up the model, we're interested in computing the likelihood function. the basic question is, so what's the probability of observing a specific word here? now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. ther",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": ". sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration that you have seen before, which is dimmer now is just the illustration of this generation model. so mathematically, this model. this is nothing but to just define the following gen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of topic models. so now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? well, in general we can use some observed text data to estimate the model para",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "hat they sum to one. now what's interesting is also to think about the special cases, like when we set one of them to one. what would happen? well, the other would be 0, right? and if you look at the likelihood function. it will then degenerate to the special case of just one distribution right so you can easily verify that by assuming one of these two is 1.0 and the other is 0. so in this sense, the mixture model i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "sually good exercise to do because it allows you to see the model index and to have a complete understanding of what's going on in this model and we have mixing weights of course also. so what is the likelihood function look like? it looks very similar to what we had before, so for the document, first, it's a product of all the words in the document exactly the same as before. the only difference is that inside here",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ferent unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word probabilities in each topic must sum to one, the other is the choice of each topic must sum to one.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "s probability of point nine to the word the and text point one. now, let's also assume that our data is extremely simple. the document has just the two words text and the. so now let's write down the likelihood function in such a case. first, what's the probability of text and what's the probability of the? i hope by this point and you will be able to write it down. so the probability of text is basically the sum ov",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " model which is .5 multiplied by the probability of observing text from that model. similarly, the would have a probability of the same form, just with different exact probabilities. so naturally our likelihood function is just the product of the two, so it's very easy to see that. once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "now let's look at the another behavior of mixture model and in this case let's look at their response to the data frequencies. ok, so what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0.9 and the probability of 0.1. now it's interesting to think about a scenario where we start adding ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "bout a scenario where we start adding more words to the document. so what would happen if we add many the's to the document? now this will change the game, right? so how? well, picture what would the likelihood function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihood function by additional terms to account for the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " document. so what would happen if we add many the's to the document? now this will change the game, right? so how? well, picture what would the likelihood function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. since in this case all t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "t? so how? well, picture what would the likelihood function look like now? it started with the likelihood function for the two words. as we add more words, we know that,\u00a0 we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. since in this case all the additional terms are the, we're going to just multiply by this term for the probability of the. an if ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "other occurrence of the, we multiply again by the same term and so on, so forth until we add as many terms as the number of the's that we added to the document d prime. now this obviously changes the likelihood function, so what's interesting is now to think about how would that change our solution. so what's the optimal solution now? intuitively, you would know the original solution. 0.9 and\u00a0 0.1 will no longer be ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "cause after all we are maximizing the likelihood of the data. so all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "en we repeat this. so this is a hill climbing algorithm that would gradually improve the estimate of parameters and as i will explain later, there's some guarantee for reaching a local maximum of the likelihood function. so let's take a look at the computation for specific case. so these formulas are the em formulas that you see before, and you can also see there are superscripts here n to indicate the generation of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "ledge about some of the inequalities that we haven't really covered yet. so here what you see is on the x dimension. we have set up value. this is the parameter that we left on the y axis. we see the likelihood function. so this curve is reaching or like roller function, right? so this one. and this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. but in the case ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "g this to another point where you can have a higher like recorder. so that's the idea of hill climbing. any in the mri was the way we achieve this is to do two things. first will fix a lower bound of likelihood function, so this is the lower bound you can see here. an once we fit the lower bound we can then maximise the lower bound and of course the reason why this works is because the lower bound is much easier to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "rld. in the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters. here improve is guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. there are some properties that have to be satisfied in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "improve the estimate of parameters. here improve is guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. there are some properties that have to be satisfied in order for the parameters also too. convert it to some stable value. now he thought data augmentation is done probabilist",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " otherwise it's essentially the same. so here i illustrate how we can generate the text that i was multiple topics. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ask the question what's the probability of observing a world w from such a mixture model? now if you look at this picture and compare this with the picture that you have seen earlier",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "o for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. of w is indeed the some of these terms. so next, once we have the likelihood function, we would be interested in knowing the parameters right? so to estimate the parameters. but first let's put all these together to have the complete likelihood function for plsa. now the first line sh",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "erms. so next, once we have the likelihood function, we would be interested in knowing the parameters right? so to estimate the parameters. but first let's put all these together to have the complete likelihood function for plsa. now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as i said. and so let's take a closer look at this. after that conta",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "nd it would also allow you to understand what would be the output that we generate when we use plsa to analyze text data, and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen befo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " have these distributions free. instead we can do draw one from the dirichlet distribution, and then from this, then we're going to further sample a word and the rest is very similar to the plsa. the likelihood function now is more complicated for lda, but there's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the difference here. so in the top you see plsa. likelihood",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "then from this, then we're going to further sample a word and the rest is very similar to the plsa. the likelihood function now is more complicated for lda, but there's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the difference here. so in the top you see plsa. likelihood function that you have already seen before it's copied from previous slide only that i dropped ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "d function now is more complicated for lda, but there's a close connection between the likelihood function of lda and plsa, so i'm going to illustrate the difference here. so in the top you see plsa. likelihood function that you have already seen before it's copied from previous slide only that i dropped the background for simplicity. so in the lda formulas you see very similar things. first you see the first equati",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " here. right, so basically in the lda we just added these integrals to account for the uncertainties and we added of \"course the\u00a0 govern the choice of these parameters, pi's and theta's. so this is a likelihood function for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you migh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "er parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. now. these parameters that we are interested in, namely the topics and the coverage, are no longer parameters in lda. in this case we have to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "rly. so as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model. but once we design model, the next step is to write down the likelihood function. and after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "sign the model. but once we design model, the next step is to write down the likelihood function. and after that we can do is to look at the how to estimate the parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "he parameters. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. if you still recall what the likelihood function looks like in plsa, then you realize that in general the probability of observing a data point from mixture model is going to be a sum over all the possibilities of generating the data. i in this cas",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to include the k clusters. now if you look at the fo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " all the words in the document using this distribution. note that it's important that we use this distributed generator. all the words in the document. this is very different from topic model, so the likelihood function would be like what you are seeing here. so the. you can take a look at the formula here. we have used the different. notation here in the second line of this. of this equation. but you can see now th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "sh the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "on to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any pa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " a. it's not like a modeling x, but rather we're going to model this. note that this is a conditional probability of y given x. and this is also precisely what we want for classification. now, so the likelihood function would be just a product over all the training cases. and in each case, this is the modeled probability of observing this particular training case. so given a particular xi, how likely we are going to",
        "label": "use"
      }
    ]
  },
  {
    "text": "mutual information",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "recisely the entropy of w1 and the entropy of w3. and they have different upper bounds, so we cannot really compare them in this way. so how do we address this problem? later we'll discuss we can use mutual information to solve this problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ry and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not rea",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual informatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "utual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual information, denoted by i(x;y), measures the entropy reduction of x obtained from knowing y. more specifically the question we're interested in here, is how much reduction in the entropy of x can we obtain by kn",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "of knowing x. normally the two conditional entropies h(x|y) and h(y|x) are not equal. \u00a0but interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by i here and this function has some interesting properties. first, it's also non negative. this is easy to understand becausw the original entropy is always not going to be lower than the po",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " original entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero if and only if the two random variables are completely independent. that means knowing one of them doesn't tell us anything about the other.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "t means knowing why did not help at all, and that's when x&y are completely independent. now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ormation, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic r",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whene",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be framed as a mutual informa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "al information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "tion we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ccurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ts and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cau",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "he same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than the mu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "hat words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than the mutual information between eats and the. because knowing the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "on, so this i give some example here. the mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than the mutual information between eats and the. because knowing the doesn't really help us predict eats. similarly knowing eats doesn't help us predicting the as well. and you also can easily see that the mutual information b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "e mutual information between eats and the. because knowing the doesn't really help us predict eats. similarly knowing eats doesn't help us predicting the as well. and you also can easily see that the mutual information between\u00a0 a word and itself is the largest which is equal to the mutual info. the entropy of this word. so because in this case the reduction is maximum because knowing one would allow us to predict t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "l info. the entropy of this word. so because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between ea",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "cause knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual informa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "refore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in or",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual informat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "al information between eats and another word. in other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically writ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "d computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a f",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ual information larger than the mutual information between eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called kl-divergences or callback labeler divergance",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "en eats and itself. so now let's think about how to compute the mutual information. now, in order to do that, we often. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called kl-divergences or callback labeler divergance. this is another term in information theory that measur",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "e combinations of the values of these two random variables. in our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. the larger this divergence is, the high",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " two random variables. in our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. the larger this divergence is, the higher the mutual information would be. s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ws that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. the larger this divergence is, the higher the mutual information would be. so now let's further look at the what are exactly the probabilities involved in this formula of mutual information. and here i listed all the probabilities involved and it's easy for you to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "the independence assumption. the larger this divergence is, the higher the mutual information would be. so now let's further look at the what are exactly the probabilities involved in this formula of mutual information. and here i listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word. so for w1, we hav",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "inally we have the scenario when none of them occurs. so this is when the two variables taking a value of 0. and they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. here. once we know how to calculate these probabilities, we can easily calculate the mutual information. it's also interesting to note that there are some relations or constraints among these probab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " 0. and they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. here. once we know how to calculate these probabilities, we can easily calculate the mutual information. it's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the margina",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " we have seen both w and w2. once we have these counts, we can just normalize. these counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. now there is a small problem. when we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sample and in general we would believe that it's pot",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "rough the segments. each is weighted 1/4, so the total the sum is actually one. so that's why in the denominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. we int",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "o words. we introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches the entropy reduction of x. due to knowing why or entropy reduction of why do too knowing eggs? they are the same, so these three concepts are actually very useful for other appl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "lly very useful for other applications as well. that's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. in particular, mutual information is a principled way for discovering such a relation. it allows us to have values computer on different pairs of words that are comfortable, and so we can rank these pairs and discover the strongest c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " possibility of using bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "t the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "luate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results. how well can you predict the cluster of the object in the gold s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "sure which basically measures based on the identity of or the cluster of object in the system-generated results. how well can you predict the cluster of the object in the gold standard or vice versa. mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. f measure is another possible measure",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ystem-generated results. how well can you predict the cluster of the object in the gold standard or vice versa. mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. f measure is another possible measure. now again, a thorough discussion of this evaluation, of these evaluations, would be be",
        "label": "intro"
      }
    ]
  },
  {
    "text": "syntagmatic relation",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ce cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of. similarly, monday and tuesday have paradigmatic relation. the second kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so a&b have syntagmatic relation if they can be combined with each other in a sentence. that means these two word",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ay and tuesday have paradigmatic relation. the second kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so a&b have syntagmatic relation if they can be combined with each other in a sentence. that means these two words are semantically related. so for example, cat and sit are related because a cat can sit somewhere. similarly, car and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. so they occur in similar locations relative to the neighbors in the sequence. syntagmatic relation on the other hand, is related to co-occurring elements that tend to show up in the same sequence. so these two are complementary and basically relations of words, and we're interested in discovering ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " of tasks. and grammar learning can be also done by using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. so we'll learn the structure and what can go with what else. word relations can be a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "tive and negative opinions about iphone 6. in order to do that, we can look at what words are most strongly associated with a feature word like the battery in positive versus negative reviews. such a syntagmatic relations would help us show the detailed opinions about the product. so how can we discover such associations automatically? now, here are some intuitions about how to do that. let's first look at the paradi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "nd then imagine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic idea of discovering paradigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of text. but here we are interested in knowing what oth",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "r cat. so this is the basic idea of discovering paradigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of text. but here we are interested in knowing what other words are correlated with the verb eats. and what words can go with eat? and if you look at the right side of t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n each sentence. and then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? so the q",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "n ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? so the question here has to do with whe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? so the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever yo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "gether with eats, meaning that whenever you see eat, you tend to see the other words. and if you don't see eat, probably you don't see other words often either. so this intuition can help us discover syntagmatic relations. now again, consider example- how helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence would generally help us predict the whether meat also",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "t really help us predict whether text also occurs in the sentence. so this is in contrast to the question about eats and meat. this also helps explain the intuition behind the methods for discovering syntagmatic relation. mainly we need to capture the correlation between the occurrences of two words. so to summarize, the general ideas for discovering word associations are the following. for paradigmatically relation ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "igmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. and we're going to compare their co occurrences with their individual occurr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "cument even. and we're going to compare their co occurrences with their individual occurrences. we're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. so these general ideas can be implemented in many",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": ", so we emphasize more on matching rare words now. so with this modification, then the new function will likely address those two problems. now interestingly we can also use this approach to discover syntagmatic relations. in general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " the words that tend to occur in the context of the candidate word, for example, cat. so for this reason, the highly weighted terms in this idf weighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering the two relations. and indeed they can be discussed, discovered in a joint manner by leveraging such associations. so to summarize, the main id",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "elations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definition, syntagmatic relations hold between words tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "k about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definition, syntagmatic relations hold between words that have correlated co occurrences. that means when we see one word occurs in the context, we tend to see the occurrence of the other word. so take a more specific example, here ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful f",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. now we address the different scenario where we assume that we know something about the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we can predict the one word given that we know the pres",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "rds. because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "his w is not really related to meat. like the, for example, it would be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "py, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word w1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with w1. note that we need to use a threshold to find these words. the threshold can be the number of top candidates to take or absolute value for the conditional entropy. now this would allow us to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "absolute value for the conditional entropy. now this would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the strongest k syntagmatic relations from entire collection. because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in this case of discovering syntagmatic relations for ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "est k syntagmatic relations from entire collection. because in order to do that, we have to ensure that these conditional entropies are\u00a0 comparable across different words. in this case of discovering syntagmatic relations for a target word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked abou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "s? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "l information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of using mutual information for syntagmatic relation mining. now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? so this question can be framed as a mutual information question, that is, w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": ", so the total the sum is actually one. so that's why in the denominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. we introduce the three concepts from information",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ame, so these three concepts are actually very useful for other applications as well. that's why we spend some time to explain this in detail, but in particular there also very useful for discovering syntagmatic relations. in particular, mutual information is a principled way for discovering such a relation. it allows us to have values computer on different pairs of words that are comfortable, and so we can rank thes",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for all the words,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ther way to do term waiting for paradigmatic. a relation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic relations. these are fairly general. they can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities. are we introduced multiple statistical approach",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. these discovery association",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "s\u00a0 about topic mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. the main topics. in the text. and we cal",
        "label": "use"
      }
    ]
  },
  {
    "text": "conditional entropy",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relatio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. now we address the different scen",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "so ask the question, what if we know of the absence of eats? would that also help us predict the presence or absence of meat. so these questions can be addressed by using. another concept, called the conditional entropy. so to explain this concept, let's first look at the scenario we had before where we know nothing about the segment. so we have these probabilities indicating whether a word like meat occurs or does ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " of meat. given that we know eats occured in the context. so as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. right? so you can see this is essentially the same entropy function as you have seen before, except ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "s a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. so this equation now here. would be. the conditional entropy conditioned on the presence of eats. right? so you can see this is essentially the same entropy function as you have seen before, except that we all the probabilities now have a condition. and this t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "before, except that we all the probabilities now have a condition. and this then tells us the entropy of meat after we have known eats occurring in the segment. and of course, we can also define this conditional entropy for the scenario where we don't see eats. so if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. so no",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ve known eats occurring in the segment. and of course, we can also define this conditional entropy for the scenario where we don't see eats. so if we know eats did not occur in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. so now putting different scenarios together, we have the complete definition of conditional entropy as follows. basically. we'",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " in the segment, then this conditional entropy would capture the uncertainty of meat in that content in that condition. so now putting different scenarios together, we have the complete definition of conditional entropy as follows. basically. we're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1. basically, whether eats is present or a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "nsider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1. basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. so if you expand this entropy, then you have the following equation. where you see the involvement of those conditional probabilities. now in general, for any dis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " if you expand this entropy, then you have the following equation. where you see the involvement of those conditional probabilities. now in general, for any discrete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information about the segment, we won't be able to increase t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "itional probabilities. now in general, for any discrete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information about the segment, we won't be able to increase the uncertainty. we can only reduce uncertainty, and that intuitively makes sense because as we know more infor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " information, it should always help us. make the prediction and it cannot hurt the prediction in any case. now what's interesting here is also to think about what's the minimum possible value of this conditional entropy. now we know that the maximum value is the entropy of x. but what about the minimum? so what do you think? i hope you can reach the conclusion that the minimum possible value would be 0 and it will b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ou think? i hope you can reach the conclusion that the minimum possible value would be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we can predict the one ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "lue would be 0 and it will be interesting to think about and in what situation will achieve this. so let's see how we can use conditional entropy to capture syntagmatic relations. now, of course this conditional entropy gives us directly one way to measure the association of two words. because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. now befor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "measure the association of two words. because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "esence or absence of another word. now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so what is the value of this? now. this means we know whether meat occurs in the sentence and we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. so, here we listed the this conditional entropy in the middle. so it's here. so what is the value of this? now. this means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence. now of course th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this is zero because there's no uncertain there anymore once we know whether the word occurs in the segment we will already know the answer for the prediction. so this is 0. and that's also when this conditional entropy reaches the minimum. so now let's look at some other cases. so this is a case of. knowing the and trying to predict the meat and this is the case of knowing eats and trying to predict the meat. which",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "econd term, namely, this one to have a smaller entropy. and that means there is a stronger association between meat and eats. so we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0? and for what kind of words would it reach its maximum? well, that's when this w is not really related to meat. like the, for example, it would be very close to the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ts maximum? well, that's when this w is not really related to meat. like the, for example, it would be very close to the maximum, which is the entropy of meat itself. so this suggests that we can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 giv",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "e can use conditional entropy for mining syntagmatic relations. the algorithm would look as follows. for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the ta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "for each word w1, we're going to enumerate the overall other words w2, and then we can compute the conditional entropy of w1 given w2. and we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word w1, and then we can take the top ranked the candidate words as words that have potential sy",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": " words that have potential syntagmatic relations with w1. note that we need to use a threshold to find these words. the threshold can be the number of top candidates to take or absolute value for the conditional entropy. now this would allow us to mine the most strongly correlated words with a particular word w1 here. but this algorithm does not help us mine the strongest k syntagmatic relations from entire collecti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "case of discovering syntagmatic relations for a target word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it is to predict w1. but if we think about the two pairs where we share w2 in the same condition and w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "ns for a target word like w1, we only need to compare the conditional entropies for w1 given different words. and in this case they all comparable right? so the conditional entropy of w1 given w2 and conditional entropy of w1 given\u00a0 w3 are comparable. they all measure how hard it is to predict w1. but if we think about the two pairs where we share w2 in the same condition and we try to predict the w1&w3, then the co",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're g",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "r strong syntagmatic relations globally from corpus. so now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. a more comparable across different pairs. in particular, mutual information, denoted by i(x;y), measures the entropy reduction of x obtained from knowing y. more specifically the question we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "estion we're interested in here, is how much reduction in the entropy of x can we obtain by knowing y. so mathematically, it can be defined as the difference between the original entropy of x and the conditional entropy of x given y. and you might see here you can see here. it can also be defined as a reduction of entropy of y, because of knowing x. normally the two conditional entropies h(x|y) and h(y|x) are not eq",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ere and this function has some interesting properties. first, it's also non negative. this is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second property is th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "g properties. first, it's also non negative. this is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. in other words, the conditional entropy would never exceed the original entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "nditional entropy would never exceed the original entropy. knowing some information can always help us potentially, but won't hurt us in predicting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero if and only if the two random variables are completely independent. that means knowing one of them do",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "dependent. that means knowing one of them doesn't tell us anything about the other. and this last property can be verified by simply looking at the equation above. and it reaches 0 if and only if the conditional entropy of x given y is exactly the same as original entropy of x. so that means knowing why did not help at all, and that's when x&y are completely independent. now when we fix x to rank different ys using ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " of x given y is exactly the same as original entropy of x. so that means knowing why did not help at all, and that's when x&y are completely independent. now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as rankin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " same order as ranking based on mutual information, because in the function here h of x is fixed because x is fixed. so ranking based on mutual information is exactly the same as ranking based on the conditional entropy of x given y. but the mutual information allows us to compare different pairs of x&y, so that's why mutual information is more general and in general more useful. so let's examine them intuition of u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " was have higher mutual information with eats. so we're going to compute the mutual information between eats and other words. and if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. we have lower mutual information, so this i give some exa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " is the largest which is equal to the mutual info. the entropy of this word. so because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero. therefore the mutual information reaches its maximum. it's going to be larger than or equal to the mutual information between eats and another word. in other words, picking any other word, a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "n generally be discovered by measuring correlations between occurrences of two words. we introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches the entropy reduction of x. due to knowing why or entropy reduction of why do too knowing eggs? they a",
        "label": "intro"
      }
    ]
  },
  {
    "text": "text categorization",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analysis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "f the articles that they have written. we can in general categorize the observer based on the content. that they produce. finally, it's also related to text based prediction. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for coveri",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "d that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested in doing that in this lecture. and then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. the problem of tex",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and the categorisation results. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "arn patterns in different categories, and this would further help the system then learn how to recognize. the categories of new tax objects that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clusteri",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ies assigned. to text content. another application is automatically email routing or sorting, so you might want to automatically sort your emails into different folders, and that's one application of text categorization, where each folder is a category. there is also another important kind of applications of routing emails to the right person to handle. so in helpdesk email messages generally routed to a particular ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " to handle different people attempt to handle different kinds of requests and in many cases a person will manually assign the messages to the right people. but you can imagine you can build automatic text categorization system to help routing a request. and this is to classify the incoming request in to one of the categories where each category actually corresponds to a person to handle the request. and finally, aut",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "each category. we can further categorize into sub categories etc. so why is text categories important well, i already showed you several applications, but in general there are several reasons. one is text categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. so now with categorisation, text can be represented in multiple leve",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "based on our vocabulary. and sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary. the second kind of reasons is to use text categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to discove",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": " is to use text categorization to infer properties of entities. and text categorisation allows us to infer the properties of such entities that are associated with text data. so this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. so it's useful ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "n be actually connected to text data indirectly. once we can make the connection, then we can make predictions about those values. so this is a general way to allow us to use text mining tool. sorry, text categorization to discover knowledge about the world. very useful, especially in big text data. analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certai",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ased on the political speech at this is again example of using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorization problem.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rule",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the cat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "this lecture is about the methods for text categorization. so in this lecture were going to discuss how to do text categorization. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. so, for example, if you want to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "o predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all the text objects. and y",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ", the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. main difference is that in clustering we don't really know what are the pr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "would like to allocate a document to one of these categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "le categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ", and then it makes categorization or clustering more likely to appear and if you have not seen text. but this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. but you should know that this kind of model doesn't have to make this assumption. we could, for example, assume the words may be dependent on each other, so that would make it a bigram languag",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "our case what can we do to make sure this theta i represents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. in other words, these are the documents with known c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the gene",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ny other features than words to be used in this vector. since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very closed, closely related to the log or log odds that i introduced in th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "bility here. and the function form looks like this. so this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ning course or read more about machine learning to know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. so the second classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this lecture is a continued discussion of. discriminative classifiers for text categorization. so in this lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popular classification method, and there has been also shown to be effe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "is lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popular classification method, and there has been also shown to be effective for text categorization. so to introduce this classifier, let's also think about the simple case of two categories and we have two public categories, season one and season 2 here. an we want to classify documents into these",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ust and can be useful in practice. most techniques that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to so",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "re are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures. the second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "gories are happy, sad, fearful, angry, surpised and disgusted. so as you can see, the task is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does req",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "rpised and disgusted. so as you can see, the task is essentially a classification task or categorisation task. as we've seen before, so it's a special case of text categorization. this also means any text categorization method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "zation method can be used to do sentiment classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated features that may be more appropriate for sentiment tagging, as ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "t classification. now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. in particular, it needs two kinds of improvements. one is to use more sophisticated features that may be more appropriate for sentiment tagging, as i will discuss more in a moment. the oth",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "seful to consider the order. for example, we could use ordinal regression to do, and that's something that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-grams. you can just have a sequence of chara",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "quent versus infrequent features, and that's why feature design is generally an art. that's perhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": " better. so to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. and we also need to consider the order of those categories and we talk about the ordinal regress",
        "label": "intro"
      }
    ]
  },
  {
    "text": "training data",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "because we cannot compute the probability of a new document. you can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data. so we cannot compute the pis for future document. and there was some heuristic. a work around though. and secondly, it has many parameters and i've asked you to compute how many parameters exactly t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "overfitting and that means it's very hard to also find a good local maximum. and that really represents global maximum. and in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model. the model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. thi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "l maximum. and in terms of explaining future data, we might find that it would overfit the training data because of the complexity of the model. the model is so flexible to fit the precisely what the training data looks like, and then it doesn't allow us to generalize the model for using other data. this, however, is not necessary problem for text mining because here we are often only interested in fitting the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "rity function to explicitly define bias. deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data to guide us to select the best number of clusters. now sometimes you may see some methods that can automatically determine the number of clusters. but in general, that has some implied application of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "man experts to help in two ways. first, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories. and this is called a training data. and then secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. so we need to provide some basic feat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "eatures like phrases or even policy feature tags or even syntactic structures. so once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. so soft rules just means we're going to still decide which category should be assigned to the document. but it's not going to be used using a rule that is deterministic, so we might use something si",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "going to figure out which features are most useful for separating different categories. and it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ich features are most useful for separating different categories. and it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the predictio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "e classifier would take any value in x as input and we generate the value in y as output, and we hope the output y would be the right category for x, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of input and output for function and then the computer is goin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " these features are very important and they have to be provided by humans. and they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "tter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different object function, which is often also called a loss function or cost functio",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "y, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different object function, which is often also called a loss function or cost function. they also tend to vary in their ways of combining the features, so linear combination fo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "native classifiers attempted to model the. conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines and the k nearest neighbors. we will cover some of these classifiers in detail in the next few lectures.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "clustering. we want to find such clusters in the data. but in the case of categorization, we are given the categories. so we kind of have predefined categories and. then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ategory i from the data. but in our case what can we do to make sure this theta i represents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. in other words",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ents indeed category i? if you think about the question and you're likely to come up with the idea of using the training data right. indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. in other words, these are the documents with known categories assigned, and of course human experts mu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ave observed the document in that category. the other kind is word distributions and we want to know what words have high probabilities for each category. so the idea then is to just use the observed training data to estimate these two probabilities. and in general we can do this separately for different categories. that's just because these documents are known to be generated from a specific category, so once",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "rior probability of this category? of course, this second probability depends on how likely that you will see documents in other categories. right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these k categories. to estimate all these parameters. now if you spend some time to think about this and it would help you understand the follow",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " see n sub i denotes the number of documents in each category. and we simply just normalize this count to make this a probability. in other words, we make this probability proportional to the size of training dataset in each category. that's the size of the set t sub i. now, what about the word distribution? well, we do the same again. this time we can do this for each category. so let's say we are considering",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " estimate of language models and this has to do with what would happen if you have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probabi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s. infinity amount of documents, and then there's no distinction between them, so it becomes just a uniform. what if delta is zero? well we just go back to the original estimate based on the observed training data to estimate the probability of each category. now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. so he",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ctive function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known label for that x y, either one or zero. so in our c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "xt step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known label for that x y, either one or zero. so in our case we are interested in maximizing this conditional",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "te the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x and a known label for that x y, either one or zero. so in our case we are interested in maximizing this conditional likelihood. the co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " one. when i maximize this one. it's equivalent to minimize this one. so you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ject that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interested in classifying and say we have found the k nearest neighbors. that's why this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "not very similar to the object. so the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation. basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose. the the parameter k here or some other parameters in other classifiers, and then you're going to assume this number t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "stical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best. but in this vm, we're going to look at another criteria for determining which lines best and this time the criteria is more tide to the classification error. as y",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "them as zero and one, but instead of having negative 11 and this is purely for mathematical convenience, as you will see in a moment. so the goal of optimization first is to make sure the labeling on training data is all correct. so that just means if yi, the known label, for instance xi is one we would like this classify value to be large. and here we just choose threshold one here. but if you use another thr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "aints in a unified way, and that's a convenient way of capturing these constraints. what's our second goal? that's true. maximizing margin, right? so we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin. now the margin can be shown to be related to the magnitude of the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "here are many different ways of combining them and they also optimize different objects and functions. but in order to achieve good performance, they all require effective features and also plenty of training data. so as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. so performance is often much more affected by the eff",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "der to achieve good performance, they all require effective features and also plenty of training data. so as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. so performance is often much more affected by the effectiveness of features and then by the choice of specific classifiers. so feature design tends to be more import",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ot of unable text data for categorization then you can actually do clustering on these text data to learn categories. and then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. so you can in fact use the em algorithm to actually combine both. that would allow you essentially to also pick up a useful words in the u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ay a naive bayes classifier to classify all the unlabeled text documents. and then we're going to assume the high confidence classification results, or actually reliable. then you certainly have more training data. the cause from the unlabeled data we some are labeled as category ones and more labeled as category two. although the label is not completely reliable. but then they can still be useful. so let's as",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ume they are actually training label examples and then we combine them with the true training examples. to improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " we are working on. but basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data. so for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categoriz",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "s altogether, so the total number of premises (k - 1) * (m + 1). that's alot alot of parameters. so when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model? so that's not the idea. the second problem is that these problems these k - 1 classifiers are not really independent. these ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "er in these categories. now, in fact, this would allow us to have two positive benefit of one is it's going to reduce the number of parameters significantly. and the other is to allow us to share the training data, because all these parameters are assumed to be equal. so these training data for different classifiers. can then be shared to help us set the optimal value for beta. so we have more data to help us ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " benefit of one is it's going to reduce the number of parameters significantly. and the other is to allow us to share the training data, because all these parameters are assumed to be equal. so these training data for different classifiers. can then be shared to help us set the optimal value for beta. so we have more data to help us choose a good beta value. so what's the consequence? the formula would look ve",
        "label": "use"
      }
    ]
  },
  {
    "text": "generative model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "re may be many different ways of solving this problem. indeed, you can write a heuristic program to solve this problem, but here we're going to introduce a general way of solving this problem called\u00a0 generative model, and this is in fact very general idea, and it's a principle way of using statistical modeling to solve text mining problems, and here i dim the picture that you have seen before in order to show the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "te that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit the data as well as we can. after we have fitted ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ot allowed to cover a topic outside the set of topics that we are discovering. so the coverage of each of these k topics would sum to one for a document. we also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. we simply assume that they are generated this way and inside the model, we embed some parameters that wer",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "a model can also be regarded as a probabilistic mechanism for generating text. and that just means we can view text data as data observed from such a model. for this reason, we also call such a model generative model. so now given a model, we can then sample sequences of words. so for example, based on the distribution that i have shown here on this slide, we might, let's say, sample \"a sequence like \"\"today is w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e because we assumed that the document covers this topic 100%. so the main goal is just to discover the word probabilities for this single topic, as shown here. as always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the data or data representation. and then we're going to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e model is just to treated these two distributions together as one model. so i use the box to bring all these components together. so if you view this whole box as one model, it's just like any other generative model. it would just give us the probability of a word. but the way that determines this probability is quite different from when we have just one distribution. and this is basically a more complicated mix",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "tion. and this is basically a more complicated mixture model. sorry, more complicated model than just one distribution, and it's called a mixture model. so as i just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. the illustration that you have seen before, which is dimmer now is just the illustration of this generation model. so mathematically, t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ion. the illustration that you have seen before, which is dimmer now is just the illustration of this generation model. so mathematically, this model. this is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. the form you're seeing now is more general form than. what you have seen in the calculation earlier. i just u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "h prior knowledge that would allow us to have in some sense a user controlled plsa, so it doesn't blindly just listen to data but also would listen to our needs. the second is to extend the plsa as a generative model fully generated model. this has led to the development of latent dirichlet allocation or lda. so first let's talk about the plsa with prior knowledge. in practice, when we apply plsa to analyze text ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "lways interested in modeling future data, but in other cases or if we care about generality, we would worry about this over fitting. so lda is proposed to improve that and it basically to make plsa a generative model by imposing a dirichlet prior on the model parameters. dirichlet is just a special distribution that we can use to specify prior. so in this sense, lda is just a bayesian version of plsa and the para",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lectur",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "nk about how to use a probabilistic generating model to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. so in this case it's a clustering structure. the topics and each document that c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "istic generating model to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. so in this case it's a clustering structure. the topics and each document that covers one topic, and we hope t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "e to generate, or the structure that we hope to model. so in this case it's a clustering structure. the topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. but if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be gen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "on the top. now we first make this decision regarding which distribution should be used to generate the world, and then we're going to use this distribution to sample word. now. notice that in such a generative model. the decision on which distribution to use for each word is independent, so \"that means, for example, \"\"the\"\" here could\" have been generated from the second distribution. theta two, whereas text is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "'re going back to the case of estimating one word distribution based on one document. so that's the connection that we discussed earlier. but now you can see more clearly. so as more cases of using a generative model to solve a problem, we first look at theta and then think about how to design the model. but once we design model, the next step is to write down the likelihood function. and after that we can do is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "ased approaches to text for clustering. in this lecture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "unction. the model defines clustering bias. and the clustering structure is built into a generated model. that's why we can use potentially a different model to recover different instruction. complex generative models can be used to discover complex clustering structures. we did not talk about it, but we can easily design generated model to generate a hierarchical clusters. we can also use prior to further custom",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "of each term as we have done in paradigmatic relation learning. and then we can certainly cluster terms based on actually their tax representations. of course, term clusters can be generated by using generative models as well as we have seen.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": " no matter what method is used. also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. deciding the optimal number of clusters is very difficult problem for all the classroom meth",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "wo kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ies, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by wo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "guage model. and of course you can even use a mixture model to model what the document looks like in each category. so in nature they will be all using bayes rule to do classification, but the actual generative model for documents in each category. can vary, and here we just talk about a very simple case. perhaps the simplest case. so now the question is, how can we make sure each theta i actually represents cate",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " then we can have classified. that's ready for classifying new objects. so that's the basic idea of sven. so to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pros and cons, and the performance might also very ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "o stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights. now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to us",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": ". topics and then we can use a topic model to model the generation of the review text. our assumed the words in the review text are drawn from these distributions. in the same way as we assumed for a generative model like psa. and then we can then plug in the latent regression model to use the text to further predict the overall rating and that means we first predict the aspect rating and then combine them with a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "e text to further predict the overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. so this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. so we don't have time to discuss this model in detail, as in many other cases in this part of the course where we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "e where we discuss the cutting edge topics. but there is a reference site here where you can find more details. so now i'm going to show you some simple results that you can get by using this kind of generative models. first it's about rating decomposition. so here what you see are the decomposed ratings for three hotels that have the same overall rating. so if you just look at the overall rating you don't. you c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "about their feedback on the hotel. this is something very interesting because this is in some sense some byproduct in our problem formulation. we did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. and you can see the highly weighted words versus the negatively lower weighted words here for each of the four dimensi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ad, right? so even for reviews about the same product laptop, the word long is ambiguous, it could mean positive or could be negative, but this kind of lexicon that we can learn by using this kind of generative models can show whether a word is positive for a particular aspect, so this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "we need to have enriched feature representation. and we also need to consider the order of those categories and we talk about the ordinal regression. for solving this problem. we have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "are excellent reviews of this topic where you can find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem. the next two papers are about the generative models for letting the aspect rating analysis. the first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": ". firstly it would model the conditional likelihood of text given context. that clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. secondly, it makes 2 specific assumptions about the dependency of topics on context. one is to assume that depending on the context depending on different time periods or different locations, we ass",
        "label": "intro"
      }
    ]
  },
  {
    "text": "maximum likelihood",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in se",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "f the parameters? of course, in order to answer this question we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "so now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "re and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here. where we define the estimate as\u00a0 arg max of the probability of x giv",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " we trust data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "robability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informative prior, some bias towards certain values, then ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "d if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplification of course. so we're interested in which valu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "some interesting point estimates of theta. now this point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it's the most likely value of theta given by the post",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're interested in computing the posterior distributio",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ". and the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge that we discover from the text. so let's ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "elihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maxim",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "mization problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": ". so if you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid of them, that would mean we have to do so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "r all the unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "stead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word probabilities in each topic must ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "e observed document here and we assume all the other parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "on words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ook into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on different words to avoid this competition in s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "t do you think? now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ore word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. now it's also in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interest",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e assume that all the other parameters are known. so the only thing unknown is this word probabilities are given by theta sub d update. and in this lecture were going to look into how to compute this maximum likelihood\u00a0 estimator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unk",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the idea of perhaps we can guess which word is from w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ext data, and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ould be useful to take a comparison between the two. this gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in the m step, you generate computed expected count of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "s. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "n practice, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have some expectations about which topics to analyze. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ay it's impossible, but we're going to just strongly favor certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the prior called conjugate prior, then the functional f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "pi's and theta's. so this is a likelihood function for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. now. these parameters that we are intereste",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " but it's basically still just a product of the probabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimate the parameters, how can we then allocate cluster",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ikelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most popular, then we can simply normalize the count of do",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " if you have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we have not seen a word in the training doc",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "n see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard optimization problem. in this case, it can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "a values that delta and then the mu and sigma. so next question is, how can we estimate these parameters and so we collectively denote all the parameters by lambda here. now we can, as usual, use the maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed. observer ratings condition on their respective reviews. and of course, this would then give us all the use",
        "label": "intro"
      }
    ]
  },
  {
    "text": "paradigmatic relation",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " relations is useful and finally we are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "eral ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "e quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substituted for each other. that means, the two words that have paradigmatic relation would be in the same semantic class or syntactic class, and we can in general replace one by the other without affecting the understanding of the sentence. that means we would still have a valid sent",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "and we can in general replace one by the other without affecting the understanding of the sentence. that means we would still have a valid sentence. for example, cat and dog. and these two words have paradigmatic relation because they are in the same class of animal. and in general, if we replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of. similarly, monday and ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " are in the same class of animal. and in general, if we replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of. similarly, monday and tuesday have paradigmatic relation. the second kind of relation is called syntagmatic relation. in this case, the two words that have this relation can be combined with each other. so a&b have syntagmatic relation if they can be combi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " replace cat with sit in a sentence or car with drive in a sentence to still get a valid sentence. meaning that if we do that, the sentence will become somewhat meaningless. so this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. and definitely they can be generalized to describe re",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "y can even be more complex phrases than just a noun phrase. if you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. so they occur in similar locations relative to the neighbors",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "out the language. so if you know these two words or synonyms, for example, and then you can help a lot of tasks. and grammar learning can be also done by using such techniques because if we can learn paradigmatic relations, then we form classes of words. syntactic classes for example. and if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on compon",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "lations would help us show the detailed opinions about the product. so how can we discover such associations automatically? now, here are some intuitions about how to do that. let's first look at the paradigmatic relation. here we essentially can take advantage of similar context. so here you see some simple sentences about cat and dog. you can see they generally occur in similar context. and that, after all, is the d",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "entially can take advantage of similar context. so here you see some simple sentences about cat and dog. you can see they generally occur in similar context. and that, after all, is the definition of paradigmatic relation. so on the right side you can see i extracted explicitly the context of cat and dog from this small sample of text data. so i have taken away cat and dog from the corresponding sentences so that you ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ove the all words in the sentence or in sentences around this word. and even in the general context you also see some similarity between the two words. so this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. so for example, if we think about the following questions, how similar are context of cat and context of dog? in contrast, how similar are context of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " first case, the similarity value would be high. between the context of cat and dog, whereas in the second the similarity between contexts of cat and computer would be low because they are not having paradigmatic relationship. and then imagine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic idea of discovering paradigmatic relation. what abo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ot having paradigmatic relationship. and then imagine what words occur after computer. in general they will be very different from what words occur after cat. so this is the basic idea of discovering paradigmatic relation. what about the syntagmatic relation? here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. here you see the same sample of text. but here ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "re the following. for paradigmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to have paradigmatic relation. for syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. and we're going to compare their co occurrences wi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. note that the paradigmatic relation and syntagmatic relation, are actually closely related. in that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, our idea for discovering such relation is to look ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "llows us to measure the similarity similarity in some other different ways. sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics. so the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine all kind",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": ". an another word dog might give us a different context, so d2. and then we can measure the similarity of these two vectors. so by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. so the two questions that we have to address is first how to compute each vector, that is, how to compute the xi or yi? and ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ke to assess whether this approach it would work well. now, of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words really give us a paradigmatic relations. but analytically, we can also analyze this formula little bit. so first, as i said, it does make sense right? because this formula will give a higher score if there is more overlap between the two ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "in this lecture, we continue discussing paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector that represents the probability of word in the conte",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "e heuristics to improve our..... our similarity function. here's one way, and there are many other ways that are possible. but this is a reasonable way where we can adapt the bm25 retrieval model for paradigmatic relation mining. so here we define in this case we define the document vector. as containing elements representing normalized bm 25 values. so in this normalization function we see we take sum over some of al",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "s reason, the highly weighted terms in this idf weighted vector can also be assumed to be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering the two relations. and indeed they can be discus",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ows the relation between discovering the two relations. and indeed they can be discussed, discovered in a joint manner by leveraging such associations. so to summarize, the main idea for discovering\u00a0 paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. and then compute the similarity of the corresponding context documents o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "epresented as a bag of words. and then compute the similarity of the corresponding context documents of two candidate words. an then we can take the highly similar word pairs and treat them as having paradigmatic relations. these are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked abou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ment this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagm",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ver paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "plications. for example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text win",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "this lecture is\u00a0 about topic mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations. now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "clusters. an term clusters can be in general generated by representing each term with some text content. for example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. and then we can certainly cluster terms based on actually their tax representations. of course, term clusters can be generated by using generative models as well as we have seen.",
        "label": "use"
      }
    ]
  },
  {
    "text": "text clustering",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "this lecture is the first one about the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and wh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ut the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "t that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ure organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have learned in some other courses. the idea is to discover",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "early tells us that in order to evaluate the clustering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster documents in the whole text collection. so in this case documents are the units to be clustered. we may be able to cluster terms in this case. terms are o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "odel. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "imilar. furthermore, text clusters can also be further clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical scenario is that you are getting a lot of text data. let's say a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "e of the topic. we may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. we may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the fea",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "er, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later. so there are in general many applications of text clustering any. i just saw it with two very specific ones. one is to cluster search results for example and you can imagine a search engine can cluster the search results so that user can see overall structure ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering te",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "out the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to ta",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ng. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you se",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "d we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "rall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based appr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "re two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. her",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ment covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. in text clustering, howe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "s and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. in text clustering, however, we only allow a document to cover one topic. if we assume one topic is a cluster. so. that means if we change the topic definition just slightly by assuming that each document can only be g",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "pics, then will again have precisely the document clustering problem. so because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. so the question now is what generating model can be used to do clustering. as in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to genera",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " is more likely generated from the first one on the top. that means the words in the document could have been generated in general from multiple distributions. now this is not what we want to see for text clustering. for document clustering where we hope this document will be generated from precisely one topic. so now that means we need to modify the model, but how well, let's first think about why this model ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likeliho",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for do",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum likli",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this lecture is about the similarity based approaches to text for clustering. in this lecture, we're going to continue the discussion of how to do a text clustering. in particular, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "this lecture is about evaluation of text cluster. so far we have talked about multiple ways of doing text clustering but how do we know which method works the best? so this has to do with evaluation. now to talk about evaluation, one must go to go back to the clustering bias that we introduced at the beginning. bec",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "t indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. so to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. this is often needed to explore text data. and this is ofte",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "nd application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful. there are many approaches that can be used for text clustering and we discussed them: model based approaches and similarity based approaches. in general, strong clusters tend to show up no matter what method is used. also the effectiveness of a method highly dep",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ch document belongs to one category or one cluster. main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. in fact, that's the goal of text clustering. we want to find such clusters in the data. but in the case of categorization, we are given the categories. so we kind of have predefined categories and. then based on these categories and training d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "or text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. each topic is 1 cluster. so once we estimate such model, we faced the problem of deciding which cluster document d should",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "hat if theta i represents category i accurately that means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely like what we did for text clustering. namely, we are going to assign document d to the category that has the highest probability of generating this document. in other words, we're going to maximize this posterior probability as well. an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. and then try to somehow align ",
        "label": "use"
      }
    ]
  },
  {
    "text": "text object",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "lysis generally requires syntactical representation. syntactical structure representation. we can also generate the structure based feature features and those are features that might help us classify text objects into different categories. by looking at the structures, sometimes the classification can be more accurate. for example, if you want to classify articles into different categories corresponding to d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": " objects. for example, they can be documents, turns, passages, sentences or websites. and then our goal is to group similar texture objects together. so let's see a example here. you don't really see text objects, but i just use some shapes to denote objects that can be grouped together. now, if i ask you what are some natural structures or natural groups well you, if you look at it, you might agree that we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "e terms with high probabilities from world distribution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to di",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "es or any segments that you can extract the from a large text objects. for example, we might extract all the text segments about the topic, let's say by using a topic model. now, once we've got those text objects, then we can cluster. the segments that we've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "s can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean text objects may contain a lot of documents. so for example we might cluster websites. each website is actually composed of multiple documents. similarly, we can also cluster articles written by the same author,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "rs together based on whether they are published papers or similar. furthermore, text clusters can also be further clustered. regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. so more generally, why is text clustering interesting? well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. and so a typical s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "sted in getting. a sense about the major topics or what are some typical or representative document in the collection? and clustering help us achieve this goal. we sometimes also want to link similar text objects together and these. these objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents. sometimes they are about th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "an generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " there mainly differ in the ways to computer group similarity based on the individual object similarity. so let's illustrate how can induce a structure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together. ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "thods for your application to know which one is better. now let's look at another example of method for similarity based classroom in this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by just selecting kate randomly selected vectors as centroi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "g humans. basically, humans would bring the needed or desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. that is, we're going to ask humans to partition the objects to create the gold standard. and they will use their judgments based on the ne",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ustering bias is imposed by the intended application as well. so what counts as the best clustering result would be dependent on the application. procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. in this case what we care about is the contribution of clustering to some application. so we often have a baseline system to c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "blem of texture categorisation is defined as follows. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. so the picture on the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "is defined as follows. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these predefined categories. so the picture on the slide shows what happens. when",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "ries, and then the task is to classify any tax object into one or more of these predefined categories. so the picture on the slide shows what happens. when we do text categorization, we have a lot of text objects to be processed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and the categorisation results. and we often assume the availab",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "how to recognize. the categories of new tax objects that it has not seen. so here are some specific examples of text categorization and in fact, there are many examples. here are just a few. so first text objects can vary, so we can categorize a document. or a passage or sentence or collections of text, as in the case of clustering the units to be analyzed can vary a lot, so this creates a lot of possibiliti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "tes a lot of possibilities. secondly, categories can also vary, and we can generally distinguish two kinds of categories. one is internal categories. these are categories that characterize content of text object. for example, topic categories. or sentiment categories and they generally have to do with the content of the tax objects, direct characterization of the content. the other kind is external categorie",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "s and they generally have to do with the content of the tax objects, direct characterization of the content. the other kind is external categories that can characterize the entity associated with the text object. for example, authors or entities associated with the content that they produce. and so we can use their content, determine which author has written which part, for example, and that's called author ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "tell the computer which documents should not receive which categories. and this is called a training data. and then secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. so we need to provide some basic features for the computers to look into. and in the case of text, natural choice would be the words. so using ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "tures to minimize errors of categorisation on the training data, so the training data as you can see very important. it's the basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning fo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "e basis for learning. and then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all the text objects. and y is all the categories a set of categories, so the classifier would take any value in x as input and we generate the value in y as output, and we hope the output y would be the right category ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "unction behaves like based on these examples and then try to be able to compute the values for future access that we have not seen. so in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. and they will also combine multiple features in a weighted matter with we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": ". in this approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the trai",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interes",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ce we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neighbors of this text object in the training data set. so once we found we found the neighborhood and found the objects that are close to the. the object we're interested in classifying and say we have found the k nearest neighb",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "d with diamonds and so then we're going to say. well, since this is in, this object is in category of diamonds. let's say then we're going to say, well, we're going to assign the same category to our text object. but let's also look at the another possibility of finding a larger neighborhood. so let's think about the four neighbors. in this case, we're going to include a lot of other solid field boxes in red",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " is a biased constant and w is a set of weights and with one wait for each feature we have m features and so have aim weights and are represented as a vector. an similarly the data instance. here the text object is represented by also a feature vector of the same number of elements. xi is future value. for example word count. i can you can verify when we multiply these two vectors together, take the dot prod",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "iment of the review. so this is a case of just using sentiment classification for understanding opinion. sentiment classification can be defined more specifically as follows: the input is opinionated text object. the output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. one is polarity analysis where we have categories such as positive, negative or neutral. the other ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "on and see which bracket it falls into. now you can see the general decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object. so in sum, in this approach we're going to score the object. by using the features and the parameter values, beta values. and this score will then be compared with a set of training the other values",
        "label": "intro"
      }
    ]
  },
  {
    "text": "random variable",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "e segment more accurately. but it may also not occur in the segment. so now let's start this problem more formally. alright, so the problem can be formally defined as predicting the value of a binary random variable. here we denoted by x sub w, w denotes a word. so this random variable is associated with precisely one word. when the value of the variable is 1, it means this word is present. when it's zero, it me",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": " now let's start this problem more formally. alright, so the problem can be formally defined as predicting the value of a binary random variable. here we denoted by x sub w, w denotes a word. so this random variable is associated with precisely one word. when the value of the variable is 1, it means this word is present. when it's zero, it means the word is absent, and naturally the probabilities for one and zer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "zero should sum to 1. because a word is either present or absent in the segment. there's no other choice. so the intuition we discussed earlier can be formally stated as follows. the more random this random variable is, the more difficult the prediction would be. now the question is, how does one quantitatively measure the randomness of a random variable like x sub w, how in general, can we quantify the randomne",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "er can be formally stated as follows. the more random this random variable is, the more difficult the prediction would be. now the question is, how does one quantitatively measure the randomness of a random variable like x sub w, how in general, can we quantify the randomness of a variable? and that's why we need a measure called entropy. and this is a measure introduced in information theory to measure the rand",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "randomness of x. there is also some connection with the information here, but that's beyond the scope of this course. so for our purpose we just treat the entropy function as a function defined\u00a0 on a random variable. in this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. now the function form looks like this. there's a sum over al",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "on with the information here, but that's beyond the scope of this course. so for our purpose we just treat the entropy function as a function defined\u00a0 on a random variable. in this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. now the function form looks like this. there's a sum over all the possible values for this random variab",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": " so for our purpose we just treat the entropy function as a function defined\u00a0 on a random variable. in this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. now the function form looks like this. there's a sum over all the possible values for this random variable inside the sum,\u00a0 for each value we have a product of the probability t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "ndom variable, although the definition can be easily generalized for a random variable with multiple values. now the function form looks like this. there's a sum over all the possible values for this random variable inside the sum,\u00a0 for each value we have a product of the probability that the random variable equals this value and log of this probability. and note that there is also an negative sign there. now, e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "ltiple values. now the function form looks like this. there's a sum over all the possible values for this random variable inside the sum,\u00a0 for each value we have a product of the probability that the random variable equals this value and log of this probability. and note that there is also an negative sign there. now, entropy in general is not negative and that can be mathematically proved. so if we expand this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "have 0 log of 0,\u00a0 we would generally find that as zero because log of 0 is undefined. so this is the entropy function and this function will give a different value for different distributions of this random variable. and this clear it clearly depends on the probability that the random variable taking a value of one or zero. if we plotted his function against the probability that the random variable is equal to 1",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "fined. so this is the entropy function and this function will give a different value for different distributions of this random variable. and this clear it clearly depends on the probability that the random variable taking a value of one or zero. if we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this. at the two ends,\u00a0 that means when the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "tions of this random variable. and this clear it clearly depends on the probability that the random variable taking a value of one or zero. if we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this. at the two ends,\u00a0 that means when the probability of x = 1 is very small or very large, then the entropy function has a lower value when it's .5 i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "could think about in general\u00a0 here is for what kind of x? does the entropy reached maximum or minimum and we can in particular think about some special cases. for example, in one case we might have a random variable that always takes the value of one,\u00a0 the probability is one\u00a0 or there is a random variable that is equally likely taking a value of 1 or 0. in this case, the probability that x = 1 is .5. now, which ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "or minimum and we can in particular think about some special cases. for example, in one case we might have a random variable that always takes the value of one,\u00a0 the probability is one\u00a0 or there is a random variable that is equally likely taking a value of 1 or 0. in this case, the probability that x = 1 is .5. now, which one has a higher entropy? it's easier to look at the problem by thinking of simple example.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "hich one has a higher entropy? it's easier to look at the problem by thinking of simple example. using coin tossing,\u00a0 so when we think about the random experiment like a tossing a coin, it gives us a random variable that\u00a0 can represent the result. it can be head or tail, so we can define a random variable x sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. so now",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "mple. using coin tossing,\u00a0 so when we think about the random experiment like a tossing a coin, it gives us a random variable that\u00a0 can represent the result. it can be head or tail, so we can define a random variable x sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. so now we can compute the entropy of this random variable, and this entropy indicates how difficu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "can be head or tail, so we can define a random variable x sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. so now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing. so we can think about the two cases. one is a fair coin, it's completely fair. the coin shows up as ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "nce of meat. and if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat. we can also ask the question, what if we know of the absence of eats? would that also help us predict the presence or absence of meat. so these quest",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "word like meat occurs or does not occur in the segment, and we have the entropy function that looks like what you see on the slide. i suppose we know eats is present, so now know the value of another random variable that denotes eats. now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat. given that we know eats occured in the context. so as a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "at in that particular scenario. so if you expand this entropy, then you have the following equation. where you see the involvement of those conditional probabilities. now in general, for any discrete random variables x&y we have. the conditional entropy is no larger than the entropy of the variable x, so basically this is upper bound for the conditional entropy. that means by knowing more information about the s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "icting x. the second property is that it's symmetric while conditional entropy is not symmetrical. mutual information is. the third property is that it reaches its minimum zero if and only if the two random variables are completely independent. that means knowing one of them doesn't tell us anything about the other. and this last property can be verified by simply looking at the equation above. and it reaches 0 ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": " this is another term in information theory that measures the divergance between two distributions. now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions. the numerator has the joint actual observed. join the distribution of the two random variables. the bottom part of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ent values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions. the numerator has the joint actual observed. join the distribution of the two random variables. the bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. if there were independent. because when two random variables are independent, t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ns. the numerator has the joint actual observed. join the distribution of the two random variables. the bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. if there were independent. because when two random variables are independent, they joined distribution is equal to the product of the two probabilities. so this comparison would tell us whether the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "ribution of the two random variables. the bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. if there were independent. because when two random variables are independent, they joined distribution is equal to the product of the two probabilities. so this comparison would tell us whether the two variables are indeed independent if there indeed independ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "minator, that would mean the two variables are not independent, and that helps measure the association. the sum is simply to take into consideration of all the combinations of the values of these two random variables. in our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "iables are not independent, and that helps measure the association. the sum is simply to take into consideration of all the combinations of the values of these two random variables. in our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here. so if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "only have these four possible scenarios. either they both occur. so in that case both variables will have a value of one or one of them occurs. there are two scenarios. in these two cases, one of the random variables will be equal to 1 and the other would be 0. and finally we have the scenario when none of them occurs. so this is when the two variables taking a value of 0. and they're summing up to 1, so these a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ematic relation can generally be discovered by measuring correlations between occurrences of two words. we introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable x conditional entropy, which measures the entropy of x. given we know why. and mutual information of x&y which matches the entropy reduction of x. due to knowing why or entropy reduction of why do to",
        "label": "intro"
      }
    ]
  },
  {
    "text": "similarity function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "re such a context and then try to assess the similarity of the context of cat and the context of a word like dog. so now the question is, how can we formally represent the context and then define the similarity function? so first we note that the context actually contains a lot of words. so they can be regarded as a pseudo document. an imaginary document. but there are also different ways of looking at the context. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "text of two words. so here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. in general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. and of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "rally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. so next, let's see how we exactly compute these similarity functions. now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors. now it's interesting to see that this similarity function actually has a nice interpretation. and there is this dot product\u00a0 infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a wo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "but in the end, if a term is very frequently in the original data set, then it would still be frequently in the collected context documents. so how can we add these heuristics to improve our..... our similarity function. here's one way, and there are many other ways that are possible. but this is a reasonable way where we can adapt the bm25 retrieval model for paradigmatic relation mining. so here we define in this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "re are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text r",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "lar, we're going to cover a different kind of approaches than generative models. and that is similarity based approaches. so the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a li",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "citly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. so once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups. anne, try to maximize the intragroup similarity and minimize the intergroup similarity. tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "hods and. in some detail. one is hierarchical agglomerative clustering or agency, the other is k-means. so first let's look at the agglomerative hierarchical clustering. in this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "arity. so let's illustrate how can induce a structure based on just similarity. so start with all the text objects and we can then measure the similarity between them. of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together. and then was going to see which pair is. the next one to group. maybe these two now have the highest simil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "is is a general idea. now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity. we are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups. and there are also different ways to do that, and there's the three popular methods are singl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": " better. now let's look at another example of method for similarity based classroom in this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by just selecting kate randomly selected vectors as centroids of k clusters and treat them as sentence as they rep",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "anteed to convert converted local minimum. so to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is built into a generated model. that's why we can use",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "stering methods, we first discussed the model based approaches, mainly the mixture model. and here we use is implicitly similarity function. to define the clustering bias, there's no explicit definer similarity function. the model defines clustering bias. and the clustering structure is built into a generated model. that's why we can use potentially a different model to recover different instruction. complex generat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "but it's very hard to inject the such a explicit definition of similarity into such a model. we also talked about the similarity based approaches. these approaches are more flexible. directly specify similarity functions. but one potential disadvantage is that their object function is not always very clear. the k means algorithm has a clearly defined the objective function, but it's also very similar to a model base",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. deciding the optimal number of clusters is very difficult problem for all the classroom methods, and that's because it's unsupervised algorithm and there's no training data",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ectly estimate the conditional probability of label given data that is p of y given x. now i'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "e that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our insights about features. basically, effective f",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our insights about features. basically, effective features are those that would make the objects that are in the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "se classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our insights about features. basically, effective features are those that would make the objects that are in the same category look more simila",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "eatures. basically, effective features are those that would make the objects that are in the same category look more similar, but distinguishing objects in different categories. so the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how k-nn works. suppose we have a lot of training instances here. and i've colored the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ion that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could not capture that. obviously the assumption would be",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "o a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could not capture that. obviously the assumption would be a problem, and then the cla",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ty, because the neighborhood is largely determined by our similarity function. if our similarity function captures objects that do follow similar distributions, then this assumption is ok. but if our similarity function could not capture that. obviously the assumption would be a problem, and then the classifier would not be accurate. let's proceed with this assumption. then what we are saying is that in order to est",
        "label": "intro"
      }
    ]
  },
  {
    "text": "background model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust theta sub d in order to maximize the probability of the observed document here and we assume all the other parameters are known. now, altho",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ely two words: the and text. obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. so we further assume that the background model gives probability of point nine to the word the and text point one. now, let's also assume that our data is extremely simple. the document has just the two words text and the. so now let's write down",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "e. and as you can see, indeed the probability of text is now much larger than probability of the. this is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. and if you look at the equation, you will see obviously some interaction of the two distributions here. in particular, you wi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "o do the opposite. basically it would discourage other distributions to do the same, and this is to balance them out so that we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ndeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. meaning that they have a very small probability from the background model, like a text here.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "abilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. meaning that they have a very small probability from the background model, like a text here.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": ", assuming that each model is equally likely and that gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the probability of choosing a background model. now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficien",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " were going to look into how to compute this maximum likelihood\u00a0 estimator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ll guess text is probably from theta sub d is more likely from\u00a0 theta sub d, why? and you will probably see that it's because. text has a much higher probability here. by the theta sub d, than by the background model, which has a very small probability. and by this we are going to say text is more likely from theta sub d. so you see our guess of which distribution has been used to generate the text would depend o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": ". but our guess must also be affected by the prior, so we also need to compare these two priors why? because imagine if we adjust these probabilities, we're going to say the probability of choosing a background model is almost 100%. now if we have that kind of strong prior, then that would affect your guess. you might think well, wait a moment, maybe text could have been from the background as well, although the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " have to actually have observed text from the distribution. so when we multiply the two together, we get the probability that text has in \"fact has been generated from theta sub d\u00a0 similarly, for the background model an. the probability of generating text is another product of similar form. we also introduced a latent variable z here to denote whether the word is from the background or the topic. when z is zero, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ere. for example, we have n + 1. that means we have improved parameters from here to. here we have improvement. so in this setting we have assumed that the two models have equal probabilities and the background model is known. so what are the relevant statistics? well, these are the word counts. so assume we have just 4 words and their counts are like this and this is our background model that assigns high probab",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "al probabilities and the background model is known. so what are the relevant statistics? well, these are the word counts. so assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. an in the first iteration you can picture what would happen. well, we first we initialize all the values. so here this probability that we're",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "n this problem many times now, and if you recall, it's generally a sum over all the different possibilities of generating the world. so let's first look at the how the world can be generated from the background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "over all the different possibilities of generating the world. so let's first look at the how the world can be generated from the background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the background model. and that's probability of lambda sub",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ok at the how the world can be generated from the background model. the probability that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the background model. and that's probability of lambda sub b and then the second we must have actually obtained the world w from the back",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " that the world is generated from the background model is lambda multiplied by the probability of the world from the background model, right? two things must happen. first, we have to have chosen the background model. and that's probability of lambda sub b and then the second we must have actually obtained the world w from the background, and that's probability of w given sit out submit. ok, so similarly we can f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " just don't have time to go in depth in talking about that. but here i just want to give you a brief idea about what's the extension and what it enables. so this is a picture of lda. now i remove the background model just for simplicity. now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. so these word distr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "rm pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ey get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ts. now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. now this helps make the difference of the probability of such words smaller across categories. because every category has some help from their background for words, like the, a which have high proba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " a which have high probabilities. therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words do",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "istribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set the two uniform distribution and let's say one over the size of the vocabulary. so each word has the same probability. then this smoothing formula is going to be very simil",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood estimator",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "t data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likeli",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informative prior, some bias towards certain values, then map estima",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge that we discover from the text. so let's look at th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "ur intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical solution t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the high prob",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word probabilities in each topic must sum to one",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ed document here and we assume all the other parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the beha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on different words to avoid this competition in some sense.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " think? now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you can imagi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. now it's also interesting ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": ", and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, dif",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "useful to take a comparison between the two. this gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in the m step, you generate computed expected count of event base",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "e also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of each to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ce, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have some expectations about which topics to analyze. for exampl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "impossible, but we're going to just strongly favor certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " theta's. so this is a likelihood function for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "s basically still just a product of the probabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimate the parameters, how can we then allocate clusters to the d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "d function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most popular, then we can simply normalize the count of documents in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we have not seen a word in the training documents for",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "sically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard optimization problem. in this case, it can be also so",
        "label": "use"
      }
    ]
  },
  {
    "text": "parameter value",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "nce we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data. in other words, we would like to adjust these parameter values until we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "rameters or infer the parameters based on the data. in other words, we would like to adjust these parameter values until we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is what parameter values will give our data set the highest probability. so i also illustrate the problem ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "til we give our data set the maximum probability. i just say that depending on the parameter values, some data points will have higher probabilities than others. what we're interested in here is what parameter values will give our data set the highest probability. so i also illustrate the problem with the picture that you see here. on the x axis, i just illustrate the lambda, the parameters as one dimensional va",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ign a model with some parameters that we are interested in, and then we model the data. we adjust the parameters to fit the data as well as we can. after we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. by varying the model, of course we can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " are interested in, and then we model the data. we adjust the parameters to fit the data as well as we can. after we have fitted data then we will recover some parameter values will get this specific parameter values and those would be the output of the algorithm and we treat those as actually the discovered knowledge from text data. by varying the model, of course we can discover different knowledge. so to summ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "e generation of data. we simply assume that they are generated this way and inside the model, we embed some parameters that were interested in denoted by lambda. and then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "t we're interested in is inferring the theta values so we have a prior here. that includes our prior knowledge about the parameters. and then we have the data likelihood here that would tell us which parameter value can explain the data well. the posterior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to fin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "omise of the two. it would say that it's somewhere in between, so we can now look at some interesting point estimates of theta. now this point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "e posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate. in general, in bayesian inference we are interested in the distribution of all these parameter values. as you see, here is there's a distribution over theta values that you can see here p of theta given x. so the problem of bayesian inference is to infer this posterior distribution and also to infer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of parameter values, this function can tell us which x, which data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parame",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " values, this function can tell us which x, which data point has a higher likelihood, higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "served evidence is text. so this is application of bayes rule. but this step is very crucial for understanding the em algorithm. because if we can do this, then we would be able to 1st initialize the parameter values somewhat randomly, and then we're going to take a guess of these z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "ble to 1st initialize the parameter values somewhat randomly, and then we're going to take a guess of these z values and or which distribution has been used to generate which word and the initialized parameter values would allow us to have a complete specification of the mixture model, which further allows us to apply bayes rule to infer which distribution is more likely to generate each word and this prediction",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "d clustering. and we had group them together. to help us re estimate the parameters. that were interested in so these will help us re estimate these parameters. but note that before we just set these parameter values randomly, but with this guess we will have a somewhat improved estimate of this. of course, we don't know exactly whether it's zero or one, so we're not going to really do the split in hardware, but",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "proved as well. i so we already know it's improving the lower bound, so we definitely improve this original like record function which is above this lower bound. so in our example, the current gas is parameter value given by the current generation and then the next guest is the re estimated parameter values. from this illustration you can see the next gas is always better than the current gas unless it has reach",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "e this original like record function which is above this lower bound. so in our example, the current gas is parameter value given by the current generation and then the next guest is the re estimated parameter values. from this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck there. so the two would be equal. so the e step is ba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "r to estimate the distribution to improve the estimate of parameters. here improve is guaranteed in terms of the likelihood function. note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. there are some properties that have to be satisfied in order for the parameters also too. convert it to some stable value. now he thought",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. so in our case, let's assume we have training data. the training data here, x i and y i and each pair is basically feature vector of x a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ier will say x is in category one if it's positive. otherwise it's going to say it's in the other category. so this is our assumption or setup. so in the linear is uvm, we're going to then seek these parameter values to optimize the margins and then the training error. the training laid out would be basically like a in other classifiers we have a set of training points where we know the x vector and then we also",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "nstance xi is one we would like this classify value to be large. and here we just choose threshold one here. but if you use another threshold, you can see you can easily affect that constant into the parameter values b&w to make the right hand side. just one. now, if, on the other hand, why i is negative one that means it's in a different class then we want this classifier to give us a very small value. in fact ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " is in the particular range of our values, then we will assign the corresponding rating to that text object. so in sum, in this approach we're going to score the object. by using the features and the parameter values, beta values. and this score will then be compared with a set of training the other values to see which range the score is in, and then using the range we can then decide which rating the object sho",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "he interesting variables in the generating model. and then we're going to set up a generation probability for the overall rating given the observed words. and then of course, then we can adjust these parameter values including betas, rs, alpha i. in order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document. and so we have seen such cases bef",
        "label": "use"
      }
    ]
  },
  {
    "text": "scoring function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "le bit right? so the curve is a sub linear curve, an it brings down the weight of really those really high counts. and this is what we want, because it prevents that kind of terms from dominating the scoring function. now there is also another interesting transformation called a bm25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that. looks like t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "y. the difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms. now the idf can be added here in the scoring function. that means we'll introduce weight for matching each term. so you may recall this sum indicates all the possible words that can be a overlap between the two contexts. and the xi and yi probabilities ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "idate terms. here, candidate terms can be words or phrases. let's say the simplest solution is to just take each word as a term. these words then become candidate topics. then we're going to design a scoring function to measure how good each term is as a topic. so how can we design such a function? well, there are many things that we can consider. for example, we can use pure statistics to design such as scoring ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "function to measure how good each term is as a topic. so how can we design such a function? well, there are many things that we can consider. for example, we can use pure statistics to design such as scoring function. intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. so that would mean we want to favor a frequent term. however, if we sim",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "r representative terms, meaning terms that can represent a lot of content in the collection. so that would mean we want to favor a frequent term. however, if we simply use the frequency to design the scoring function, then the highest scored terms would be \"general terms or functional terms, like \"\"the\"\", \"\"a\"\"\" etc. those terms are very frequent in english. so we also want to avoid having such words on the top, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "we talked about some of these ideas in the lectures about the discovery of word associations. so these are statistical methods, meaning that the function is defined mostly based on statistics. so the scoring function would be very general. it can be applied to any language and any text. but when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. f",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "f we're dealing with tweets, we could also favor hashtags which are invented to denote topics. so naturally hashtags can be good candidates for representing topics. anyway, after we have designed the scoring function, then we can discover the k topical terms by simply picking k terms with the highest scores. now of course we might encounter a situation where the highest scored terms are all very similar. they are",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "uld like to remove redundancy and one way to do that is to do a greedy algorithm, which is sometimes called maximal marginal relevance ranking. basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the k topical terms. the first term of course will be picked when we pick the next term. we're going to look at the what terms have already been picked and try to a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "en we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. righ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're going to score based on their ratio of probability, so this is ann let's say this is our scoring function for two categories. so this is a score of a document for these two categories. and we're going to score based on this probability ratio. so if the ratio is larger \u00a0then it means it's more likely to b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "generally take logarithm of this ratio and to avoid small probabilities, and this would then give us this formula in the second line. and here we see something really interesting, because this is our scoring function for deciding between the two categories. and if you look at this function, we'll see it has several parts. the first part here is actually log of prior probability ratio and so this is the category b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ure. here it's the weight on each word and this weight. tells us. to what extent observing this word helps contributing to our decision to put this document in category one. i remember the higher the scoring function is more likely it's in category one. now if you look at this ratio basically or sorry this weight it's basically based on the ratio of the probability of the word from of the two distributions. essen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "of a word, but in general we can put any features that we think are relevant for categorization. for example document length or the font size or counts of other patterns in the document. and then our scoring function can be defined as a sum of constant beta zero and sum of the feature weights over all the features. so if hf sub i is a feature value then we multiply value by the corresponding weight beta sub i and",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " from all these features. and of course there are parameters here. so what are the parameters? well these betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very cl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probabi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "d combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here. now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. of course the features don't have to be all the words and their features can be other signa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "re the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here. now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. of course the features don't have to be all the words and their features can be other signals that we want to use. and we mentioned th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "method is a popular way to solve this problem. there are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined scoring function to help us classify a document right? so what's the function? well, it's this one. if we have all the betavalues already known, all we need is to compute the xi's for that document. and then plugging",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "whether the predicted probabilities above is or at least .5 above and now is equivalent to whether the score of the object that is. larger than or equal to negative of \u00a0alpha_j as shown here. now the scoring function now is just taking linear combination of all the features weighted by beta values. so this means now we can simply make a desicion of rating by looking at the value of this scoring function and see w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": ". now the scoring function now is just taking linear combination of all the features weighted by beta values. so this means now we can simply make a desicion of rating by looking at the value of this scoring function and see which bracket it falls into. now you can see the general decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that te",
        "label": "use"
      }
    ]
  },
  {
    "text": "conditional probability",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ope of this course, but i just give a brief introduction because this is a general knowledge that might be useful for you, so the bayes rule is basically defined here. and allows us to write down one conditional probability of x given y in terms of the conditional probability of y given x. and you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ion because this is a general knowledge that might be useful for you, so the bayes rule is basically defined here. and allows us to write down one conditional probability of x given y in terms of the conditional probability of y given x. and you can see the two probabilities are two conditional probabilities are different in the order of the two variables, but often the rule is used for making inferences of a variable. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "r belief about the x. that means before we observe any other data, that's our belief about x, what we believe some x values have higher probability than others. and this probability of x given y is a conditional probability, and this is our posterior belief about x, because this is our belief about x values after we have observed y. given that we have observed y, now what do we believe about x now, do we believe some va",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "so it attempts to model the join the distribution of the data and the label x&y. and, this can then be factored out to a product of y. the distribution of labels and join the probability of sorry the conditional probability of x given y so it's y. so we first model distribution of labels and then we model how the data is generated given a particular label here. and once we can estimate these models, then we can compute ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": " given y so it's y. so we first model distribution of labels and then we model how the data is generated given a particular label here. and once we can estimate these models, then we can compute this conditional probability of label given data based on. the probability of data given label. and the label distribution here by using the base rule. now this is the most important thing 'cause this conditional probability of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "his conditional probability of label given data based on. the probability of data given label. and the label distribution here by using the base rule. now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. so in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ior probability of the topic given the document. get posterior becausw this one p of theta i is the prior, that's our belief about which topic is more likely. before we observe any document. but this conditional probability here is the posterior probability of the topic after we have observed the document d. and bayes rule allows us to update this probability based on the prior and i shown the details. below here you ca",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "es, lets say theta 1 and theta 2, and we can use the y value to denote the two categories. and when y is 1 it means the category of the documents first class theta 1 now the goal here is to model the conditional probability of y given x directly as opposed to model the generation of x&y as in the case of naive bayes. and another advantage of this kind of approach is that it would allow many other features than words to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ional likelihood. the condition likelihood here is basically to model y given the observed x. so it's not like a. it's not like a modeling x, but rather we're going to model this. note that this is a conditional probability of y given x. and this is also precisely what we want for classification. now, so the likelihood function would be just a product over all the training cases. and in each case, this is the modeled pr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "the most commonly used classifiers, since you might use them often for text categorization. so the second classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "d and then try to assess the category based on the categories of the neighbors. intuitively, this makes a lot of sense. but mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is p of y given x. now i'm going to explain this intuition in the moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "s, and then you're going to assume this number that works well on your training set would be actually the best for your future data. so as i mentioned that knn can be actually regarded as estimate of conditional probability of y given x, and that's why we put this in the category of discriminative approaches. so the key assumption that we made in this approach is that the distribution of the label given the document or ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "overall rating given the observed words. and then of course, then we can adjust these parameter values including betas, rs, alpha i. in order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document. and so we have seen such cases before in, for example, plsa, where we predict the text data. but here we predicting the rating and the parameters of course ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": " denote the count of world w in aspect segment i. of course it's zero if the world doesn't occur in the segment. now the model is going to predict the rating based on the. so we are interested in the conditional probability of r sub t given d. and this model is set up as follows. so all of this is assumed to follow a normal distribution with a mean that denotes actually await the average of the aspect ratings. r sub of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "hese topics. and in addition to some other topics. but the visualization shows that with this technique that we can have conditional distribution of time given a topic. so this allows us to plot this conditional probability. general curves like what you're seeing here. we see that initially the two curves tracked each other very well. but later we see the topic of new orleans was mentioned again but oil price was not an",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ning. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "this lecture is\u00a0 about topic mining and analysis. \"we \"as you see on this roadmap, we have just \"we have just \"about the language namely discovery of word\u00a0 associations such as paradigmatic relations relations and syntagmatic relations",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": " starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. the main topics. in the text. and we call that topic mining and analysis. in this lecture we're going to talk about its motivation and the task definition. so first, let's look at the concept of topic. so topic is something that we all understand, i think, bu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "looking at the topics in different locations, we might know some insights about people's opinions in different locations. so that's why mining topics is very important. now let's look at the tasks of topic mining and analysis. in general, it would involve first discovering a lot of topics. in this case k topics. and then we also would like to know which topics are covered in which documents, to what extent. s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "this lecture is about the topic mining and analysis. we are going to talk about using a term as topic. this is a slide that you have seen in the earlier lecture where we defined the task of top mining and analysis. we also raised the ques",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "ut sports. and 12% is about the travel etc. we might also discover document 2 does not cover sports at all, so the coverage is zero, etc. so now of course, as we discussed. in the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. so let's first think about how we can discover topics if we represent each topic by a term. so t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier whe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "aning that if you set these parameters for different values, it will give some data points higher probabilities than others. now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. first, we have theta_i's each is a word distribution and then we have a set of pi's for each document. and since we have n documents so we have n sets of pi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "nd this has advantage of using multiple words to describe a complicated topic. it also allows us to assign weights on words so we can model subtle variations of semantics. we talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. the number of topics and vocabulary set and the output is a set of topics. each is word distribut",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " same, so it gives us another way to estimate the parameters. so this is a general illustration of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "this lecture is the first one about the text clustering. this is very important that technique for doing topic mining an analysis. in particular, in this lecture organ to start with some basic questions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "bilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. because the two problems are very similar, so this is a slide that you have seen earlier in the lecture on topic model. here we show that we have input of text collection ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discover",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "data to infer values of some other variables in the real world. that may not be directly related to the text, or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "res where we talked about what kind of features we can design for text data. it has also been addressed to some extent by talking about the other knowledge that we can mine from text. so for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. so topics can be intermediate representation of text. that would a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "can really help improving the accuracy of prediction. basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer values of real world variables. but in order to achieve the goal, we can do some other preparations ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "to infer values of real world variables. but in order to achieve the goal, we can do some other preparations and these are sub tasks. so one sub task could be mine, mine the content of text data like topic mining. and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. and both can help provide predictors for the prediction problem. and of course we can also add ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "for the prediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. and such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. as we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text base",
        "label": "use"
      }
    ]
  },
  {
    "text": "sentiment analysis",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text based prediction problems where we try to predict some real world variable ba",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "is document or in the whole collection, etc. and these words can be used to form topics. when we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. so representing text data as a sequence of words opens up a lot of interesting analysis possibilities. however, this level of representation is slightly less general than string of characters, becau",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "based representation is very important level of representation. it's quite general and relatively robust. it can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. for example, thesaurus discovery has to do with discovering related words and topic and opinion related applications are",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "e as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. because we can categorize the authors, for example, based on the content of the articles that they have written. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ecognition, where there aren't corresponding wedding design. the words. as features. but deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generally hard to get a lot of training examples because i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "this lecture is about opinion mining and sentiment analysis covering its motivation. in this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. in part",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. in contrast, we have other devices such as video recorder that can report what's happening",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "thing that will talk more about later. so now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. so let's start from the simplest one, which is character n-grams. you can just have a sequence of characters as a unit, and they can be mixed with different n(s),\u00a0 different lengths. and this is a v",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. for example, we might see a sentence like it's not good or it's not as good as something else. so in such a case, if you just take a good and that would suggest positive, it's not good, so i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "e can also mix n grams of words and n grams of part of speech tags. for example, the word great might be followed by a noun and this could become a feature, a hybrid feature. that could be useful for sentiment analysis. so next we can also have word classes, so these classes can be syntactic like a part of speech tags. or could be semantic and they might represent concepts in the\u00a0thesaurus or ontology like word net",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "this lecture is about the ordinal logistic regression for sentiment analysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we have an opinionated text document d as input an we want to generate as outpu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings. first, motivation. here are two reviews ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "nce we can end users better, we can serve these users better. so to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. and we also need to consider the order of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflects what we know about the opinion holder. but this only provides limited ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "n of some other variable. it maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ally you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. the goal here is mainly to infer values of real world variables. but in order to achieve the goal, we can do some other preparations and these are sub tasks. so one sub task could be mine, mine the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " can do some other preparations and these are sub tasks. so one sub task could be mine, mine the content of text data like topic mining. and the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. and both can help provide predictors for the prediction problem. and of course we can also add non text data directly to the predictive model, but then non text data also helps p",
        "label": "use"
      }
    ]
  },
  {
    "text": "logistic regression",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "roaches later, but here i want you to know that there's a strong connection close connection between the two kinds of approaches, and this slide shows how naive bayes classifier can be connected to a logistic regression. and you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to solve the problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "nt text data as a feature vector. of course the features don't have to be all the words and their features can be other signals that we want to use. and we mentioned that this is precisely similar to logistic regression. so in this lecture we're going to introduce some discriminative classifiers. they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "s vector. since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very closed, closely related to the log or log odds that i introduced in the naive bayes or log of probability ratio of t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " values, controlled by beta values. and since we know that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way. so this is a log odds ratio. here. and so in logistic regression, we basically assume that the probability of y = 1\u00a0 given x is dependent on this linear combination of all these features. so it's just one of the many possible ways of assuming that the dependency, ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "e mapped into a range of values from zero to 1.0. you can see, and that's precisely what we want. since we have a probability here. and the function form looks like this. so this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameter",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "objects that are in the same category look more similar, but distinguishing objects in different categories. so the design of this similarity function is closely tied to the design of the features in logistic regression. and other classifiers, so let's illustrate how k-nn works. suppose we have a lot of training instances here. and i've colored them differently and to show just different categories. now suppose we h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "represent again a document by a feature vector x here. now the idea of this classifier is do design. also a linear separator. here that you see and it's very similar to what you have seen or just for logistic regression. and we're going to also say that if the sign of this function value is positive, then we're going to say the object is in category 1. otherwise, we're going to say it's in category 2, so that makes ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "this lecture is about the ordinal logistic regression for sentiment analysis. so this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. we have an opinionated text document d as input an we wa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "would not capture this. so what's the solution? in general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two catego",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. so the predictors are represented",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "se variable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to be a linear function of these features as shown here. so this would allow us to also write the probability of y = 1 given x in this",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "_i is our parameters here. so this is just a direct application of logistical regression for binary categorization. what if we have multiple categories, multiple levels? we actually use such a binary logistic regression program to solve this multi level rating prediction. and the idea is we can introduce multiple binary classifiers and each case we ask the classifier to predict whether the rating is j or above all t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "gram would be also very straightforward as you have just seen on the previous slide. only that here we have more parameters because for each classify we need a different set of parameters. so now the logistic regression classifiers indexed by j, which corresponds to a reading level. and i have also used offer subject to replace beta 0. and this is to make the notation more consistent with what we can show in the ord",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "sifiers indexed by j, which corresponds to a reading level. and i have also used offer subject to replace beta 0. and this is to make the notation more consistent with what we can show in the ordinal logistic regression. so anyway, so here we now have basically k - 1 regular logistic regression classifiers. each has its own set of parameters. so now with this approach we can now do rating prediction as follows. afte",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "used offer subject to replace beta 0. and this is to make the notation more consistent with what we can show in the ordinal logistic regression. so anyway, so here we now have basically k - 1 regular logistic regression classifiers. each has its own set of parameters. so now with this approach we can now do rating prediction as follows. after we have trained these k - 1 logistic regression classifiers, separately of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ve basically k - 1 regular logistic regression classifiers. each has its own set of parameters. so now with this approach we can now do rating prediction as follows. after we have trained these k - 1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision. so first let's look at the classifier that corresponds to level of r",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "neral, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers. so we should be able to take advantage of this factor. now the idea of ordinal logistic regression is precisely that\u00a0 a key idea is just the improvement over the k -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "t we have far fewer parameters. specifically, we have n + k - 1 because we have m beta values and plus k minus one alpha values. so that's just the basically that's basically the main idea of ordinal logistic regression. so now let's see how we can use such a method to actually assign ratings. it turns out that with this. idea of tying all the parameters, the beta values, we also end up having a simpler way to make ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "opinion holder",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "e statements are more subjective and it's very hard to prove whether it's wrong or correct. so opinion is subjective statement. and next, let's look at the keyword person here and that indicates this opinion holder 'cause when we talk about opinion, it's about the opinion held by someone and then we notice that there is something here. so that's the target of the opinion. the opinions is expressed on this somet",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "e need to include in order to characterize an opinion. so what's the basic opinion representation like, well,\u00a0 it should include at least three measurements, right? first it has to specify what's the opinion holder. so whose opinion is this, second must also specify the target. what's this opinion about? and 3rd, of course we want opinion content and so what exactly is the opinion? if you can identify this, we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "t situation was opinion expressed. for example, in what time was it expressed? we also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? or perhaps the opinion holder was happy or sad. and so such understanding obviously goes beyond just extracting the opinion content and n",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? or perhaps the opinion holder was happy or sad. and so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. so let's take a simple example of a product review. in this case, this a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": ". and so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. so let's take a simple example of a product review. in this case, this actually explicit opinion holder and explicit target, so it's it's obviously what's opinion holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. for exa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "the opinion content and needs some analysis. so let's take a simple example of a product review. in this case, this actually explicit opinion holder and explicit target, so it's it's obviously what's opinion holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. for example iphone 6. when the review was posted, usually you can extract",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ntiment of this review is positive, and so this additional understanding of course adds value to mining the opinions. now you can see in this case the task is relatively easy, and that's because. the opinion holder and opinion target that have already been identified. now let's take a look at the sentence in the news. in this case, we have implicit holder and implicit target. and the task is in general harder s",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "t that have already been identified. now let's take a look at the sentence in the news. in this case, we have implicit holder and implicit target. and the task is in general harder so we can identify opinion holder here and that's governor of connecticut. we can also identify the target. so one target is hurricane sandy. but there is also another target, which is a hurricane of 1938. so what's the opinion? well",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " of opinions in product reviews. now there are also some other interesting variations. in fact, here we're going to examine the variations of opinions more systematically. first, lets think about the opinion holder. now the holder could be an individual or could be a group of people and sometimes opinion was from a committee or from a whole country of people. opinion target can also vary a lot. it can be about ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "n, but you can also have longer text to express the opinion like a whole article. and furthermore, we can identify the variation in the sentiment or emotion dimension. that's about the feeling of the opinion holder. so we can distinguish positive versus negative or neutral or happy versus sad, etc. finally, the opinion context can also vary. we can have simple context, like different time or different locations",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "know it's either true or false, right? you can even verify that. but from this statement one can also infer some negative opinions about the quality of the battery of this phone or the feeling of the opinion holder about the battery. in the opinion, holder clearly wish the battery to last longer. so these are interesting variations that we need to pay attention to when we extract opinions. also, for this reason",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "he product that we are commenting on. so the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, target content and context. ideally we can also infer opinion sentiment from the content and context to better understand the opinion. now often some elements of the representation are already known",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "nt from the content and context to better understand the opinion. now often some elements of the representation are already known. i just gave a good example, in the case of product reviews where the opinion holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. now it's interesting to think about other tasks that might be als",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "if we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. so suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. then we mainly need to decide the opinion sentiment of the review. so this is a case of just using sentiment classi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "ned in two ways. one is polarity analysis where we have categories such as positive, negative or neutral. the other is emotion analysis. that can go beyond polarity to characterize the feeling of the opinion holder. in the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web. five might denote the most positive and one maybe at most negative, for examp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "result, we can learn those useful information when fitting the model to the data. most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "nd there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. so here are some suggested readings, the first 2. are small books that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "erent from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflects what we know about the opinion holder. but this only provides limited view of what we can predict. in this lecture and the following lectures, we're going to talk more about how we can predict more information about the world. how can we",
        "label": "use"
      }
    ]
  },
  {
    "text": "objective function",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " see how we can solve this problem. now at this point the problem is purely a mathematical problem, because we're going to just to find the optimal solution of a constrained maximization problem. the objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. so one way to solve the problem is to use lagrange multiplier approach. now this content is beyond the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "h, i also would like to just give a brief introduction to this for those of you who are interested. so in this approach we will construct a lagrange function here. and this function would combine our objective function with another term that encodes our constraints. and we introduce lagrange multiplier here, lambda. so it's additional parameter. now the idea of this approach is to just turn the constrained optimiza",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "be competing on the words. and they will tend to bet high probabilities on different words to avoid this competition in some sense. or to gain advantage in this competition. so again, looking at this objective function and we have a constraint. on the two probabilities. now. if you look at the formula intuitively, you might feel that you want to set the probability of text to be somewhat larger than the. and this i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "pecify a similarity function to measure the similarity between 2:00 text objects. now this is in contrast with a generative model where we implicitly define the clustering bias. by using a particular objective function like a likelihood function. the whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "n we're going to recovery, compute the centroid based on the allocated objects in each cluster. and this is. to adjust the centroid and then we had repeated this process until the similarity based on objective function. in this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective functi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "jective function. in this case it's within cluster sum of squares converges an theoretically we can show that this process actually is going to minimize the within cluster sum of squares where define objective function. given k clusters. so it can be also shown this process will converge to a local minimum. i think about this process for a moment. it might remind you the em algorithm for mixture model. indeed, this",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "oints. but in the em algorithm, we in principle consider all the data points. based on probabilistic allocations. but in nature they are very similar and that's why it's also maximizing where defined objective function and it's guaranteed to convert converted local minimum. so to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. and here we use ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "oaches are more flexible. directly specify similarity functions. but one potential disadvantage is that their object function is not always very clear. the k means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach. the hierarchical clustering algorithm, on the other hand, is. it's harder to. to specify the objective function so it's not clear what exactly i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "orithm has a clearly defined the objective function, but it's also very similar to a model based approach. the hierarchical clustering algorithm, on the other hand, is. it's harder to. to specify the objective function so it's not clear what exactly is being optimized. both approaches can and generate the term clusters and document clusters. an term clusters can be in general generated by representing each term wit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "y will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different object function, which is ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "y using the base rule. now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. so in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors. but if we can model the data in each category accurately, then we can als",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "egorisation or separation of classes. so sorry for the problem. so these discriminative classifiers attempted to model the. conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines and the k nearest neighbors. we will cover some of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "sks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values, optimize the performance of classifier on the training data.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "so now you can see this is basically optimization problem, right? we have some variables to optimize and these are the weights and b and we have some constraints. these are linear constraints and the objective function is a quadratic function of the weights. so this is a quadratic program with linear constraints and there are standard algorithms that are available for solving this problem. and once we solve, the pr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "n be very, very large, so naturally we don't want this to happen. so we want to then also minimize this ci. so cassie, i needs to be minimized in order to control the error. and so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the errors as you see here, we simply take a sum over a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "value to make the constraints easy to satisfy. that's not very good of course, so see should be set to a non 0 value and a positive value. but when she is settled very, very large value would see the objective function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role. so if that happens, what would happen? what would happen is then we will try to do o",
        "label": "use"
      }
    ]
  },
  {
    "text": "categorization problem",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "lect a lot of reviews about a restaurant. or a lot of reviews about the product. and then these text data can help us infer properties of product or a restaurant. in that case, we can treat this as a categorization problem. we can categorize restaurants or categorize products based on their corresponding reviews. so this is example of external category. here are some specific examples of applications. news categorizati",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "r categorization tasks. for example, k category categorisation task can be actually performed by using binary categorization. and basically we can look at each category separately and then the binary categorization problem is whether object is in this category or not. meaning in other categories. and the hierarchical category categorisation can also be done by progressively doing flat categorisation at each level. so w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "on the political speech at this is again example of using text data to infer some knowledge about real world. in nature this all the problems are all the same and that's as we defined and it's a text categorization problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ion. 1st. there are many methods for text categorization in such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. so, for example, if you want to do topical categorisation for news articles, you can say if the news article mentions word like game and sports three times that we're going to say it's about sports.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "s and sometimes. however in general there are several problems with this approach. first, of course it's labor intensive. it requires a lot of manual work. obviously we can't do this for all kinds of categorization problems. we have to do it from scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well. \u00a0secondly, it cannot handle uncertainties in rules. often the rules a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ories. so this idea can be directly adapted to do categorization and this is precisely what naive bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if theta i represents category i accurately that means the word distribution characterizes the content of documents in category i accurately. then what we can do is precisely l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "so x is our input, it's a vector. and with m features. and each feature has a value x sub i here and our goal is model the dependency of this binary response variable on all these features. so in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the y value to denote the two categories. and when y is 1 it means the category of the documents first class theta 1 now the goal ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "nd can be useful in practice. most techniques that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. the computers of course here are t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "classifier. so how do we design effective features? well, unfortunately this is very application specific, so there's no really much general thing to say here. but we can. and do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us design features. an another way to figure out effe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "hese methods in detail. in apa category or per document basis? one example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. missing a legitimate email is all is 1 type of error. but letting us ma'am to come into your folder is another type of error. the two types of errors are clearly very different because it's very imp",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "uch a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again. and in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. so for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not usefu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "n such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. so for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. but typically we frame this as a ranking problem and we evaluated as a ranked list. that's be cause people ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ore specifically, rating prediction. we have an opinionated text document d as input an we want to generate as output already in the range of one through k, so it's discrete rating and thus this is a categorization problem. we have k categories here. now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories. intuit",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression. now let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. so the predictors are represented as x and these are the features and there",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "ow let's first think about how we use logistic regression for binary setting categorization problem. so suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. so the predictors are represented as x and these are the features and there are m features altogether, which feature value is a real number, and this can be representation of a text document. and y ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " this can be representation of a text document. and y has two values, binary response variable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to be a linear function of these features as shown her",
        "label": "use"
      }
    ]
  },
  {
    "text": "baye rule",
    "contexts": []
  },
  {
    "text": "naive baye",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ow the data are generated, so only thus it only indirectly captures the training errors. but if we can model the data in each category accurately, then we can also classify accurately. one example is naive bayes classifier. in this case. the other kind of approaches are called discriminative classifiers. these classifiers try to learn what features separate categories, so they directly tackle the problem of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "category based on the information about word distributions for these categories and the prior on these categories. so this idea can be directly adapted to do categorization and this is precisely what naive bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if theta i represents category i accurately that means t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ". when a word doesn't occur in the document. the count would be 0, so this count would just disappear. so effectively we're just have the product over all the words in the document. so basically with naive bayes classifier, we're going to score each category for a document by this function. now you may notice that here it involves the product of a lot of small probabilities and this can cause underflow prob",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "el. we want to take our best guess of the parameters. and this is the problem that you have seen. also several times in this course. now, if you haven't thought about that this problem, haven't seen\u00a0 naive bayes classifier, it would be very useful for you to pause the video for a moment and to think about how to solve this problem. so let me state the problem again, so let's just think about category one. w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": ". so once you figure out the answer to this question and you will know how to normalize, this counts and so this is a good exercise to work on it if it's not obvious to you. there is another issue in naive bayes which is a smoothing. in fact the smoothing is a general problem in all the estimate of language models and this has to do with what would happen if you have observed a small amount of data. so smoo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "bilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "on and that each feature has a weight that tells us how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio. here in naive bayes. and then finally we have this constant of bias here, so that formula actually is a formula that can be generalized to accommodate more features. and that's why i've introduced some other symbols he",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " the parameters? well these betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and this is actually o",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "as are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " and we are going to talk more about such approaches later, but here i want you to know that there's a strong connection close connection between the two kinds of approaches, and this slide shows how naive bayes classifier can be connected to a logistic regression. and you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ext categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the sco",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discussion of naive bayes classifier, where we have shown that although naive bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring fu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "introduce some discriminative classifiers. they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary response variable y here, on some predictors. that are denoted as x. so here we have also changed the notation ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " it means the category of the documents first class theta 1 now the goal here is to model the conditional probability of y given x directly as opposed to model the generation of x&y as in the case of naive bayes. and another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. since we're not modeling the generation of this vector and we can plu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "so most specifically, in logistic regression the assumed functional form of y depending on x is the following, and this is very closed, closely related to the log or log odds that i introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide. so that this is what i meant, right? so in the case of naive bayes, we compute this by using bayes rule an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "log or log odds that i introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide. so that this is what i meant, right? so in the case of naive bayes, we compute this by using bayes rule and eventually we have reached a formula that look like this. that looks like this. but here we actually would assume explicitly that we would model our probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " em algorithm to actually combine both. that would allow you essentially to also pick up a useful words in the unlabeled data. you can think of this in another way. basically, we can use, let's say a naive bayes classifier to classify all the unlabeled text documents. and then we're going to assume the high confidence classification results, or actually reliable. then you certainly have more training data. ",
        "label": "use"
      }
    ]
  },
  {
    "text": "natural language",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ral, this is basically what a person has in mind about the world, and we don't really know what exactly it looks like, of course. but then the human would express what the person has observed using a natural language such as english, and the result is text data. of course, the person could have used a different language to express what he or she has observed. in that case, we might have text data of mixed languag",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " of the topics in text mining and analytics. in this course we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. first, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "this lecture is about the text representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent text in many different ways. let's take a look at this example sentence again. we can represent this sentence in many different ways. 1st. we can always represent",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "nt this sentence in many different ways. 1st. we can always represent such a sentence as a string of characters. this is true for all the languages when we store them in the computer. when we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. but unfortunately, using such a repres",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "er in english text, or the correlation between those characters, but we can't really analyze semantics. yet this is the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but in the form of a sequence of words. so here w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "acters, but we can't really analyze semantics. yet this is the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but in the form of a sequence of words. so here we see that we can identify words like: a, dog, is, chasin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "entify words like: a, dog, is, chasing, etc. now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. by identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc. and these words can be used to fo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ence of words representation is not as robust as string of characters. but in english it's very easy to obtain this level of representation, so we can do that all the time. now if we go further to do natural language processing, we can add a part of speech tags. now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. so this opens ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "t scenarios, what kind of actions will be made? so this is... another level of analysis that would be very interesting. so this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. and unfortunately, such techniques would require more human effort. and they are less accurate. that means there are mistakes. so if we analyze text data at the leve",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "is. more powerful analysis. this course, however, focuses on word based representation. such techniques have also several advantages. first, they are general and robust, so they are applicable to any natural language. that's a big advantage over other approaches that rely on more fragile natural language processing techniques. secondly, it does not require much manual effort or sometimes it does not require any m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "uch techniques have also several advantages. first, they are general and robust, so they are applicable to any natural language. that's a big advantage over other approaches that rely on more fragile natural language processing techniques. secondly, it does not require much manual effort or sometimes it does not require any manual effort. so that's again important benefit, because that means you can apply directl",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "this lecture is about the word association mining and analysis. in this lecture we're going to talk about how to mine associations of words from text. this is an example of knowledge about natural language that we can mine from text data. here's the outline. we are gooing to first talk about what is word association and then explain why discovering such relations is useful and finally we are going to t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "re that's indicated by words like a bad and worst. and we can also then identify the context. new england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be easily done for product reviews, and tha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "text. new england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened. analyzing sentiment in news is still quite difficult. it's",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. so in general, natural language processing is very important to derive complex features. they can enrich text representation. so for example, this is a simple sentence that i showed you long time ago, and in another lecture. so fro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ning from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. so here are some suggested readings, the first 2. are small books that are excellent reviews of this topic where you can find a lot of discussion abo",
        "label": "use"
      }
    ]
  },
  {
    "text": "background word",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously bec",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "d by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basically flip a coin a fair coin to decide which one to use. but in general th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "hich means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in such a case we have a model that has some uncertainty associated with the use of a word distribution. but we can still think of this as a model for generating",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": ". we are going to assume we have knowledge about others. and this is a powerful way of customizing a model for a particular need. now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "del for a particular need. now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. so we assume the background model is already fixed. and the problem here is how can we adjust theta sub d in order to maximize the probability of the observed document here and we assume all the ot",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": " sub d in order to maximize the probability of the observed document here and we assume all the other parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "tions to do the same, and this is to balance them out so that we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ponent models. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. a large collection of english documents, by using just one distribution and then we'll just have normalized fr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. and that would make the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the model parameters and the prior here basically means one mode",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "hich is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probability of the observed document",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue wo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "bution. but the last column is also by product and this actually can also be very useful and you can think about that. and one use is to. for example, is made to what extent this document has covered background words. and this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explained well by the background.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ds from all these distributions. so we first for example. see there's a criticism of government response, and this is followed by the discussion of flooding of the city and donation, etc. we also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. so segment of the topics to figure out which words are from which distribution and to figure out",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "this is an important formula as i said. and so let's take a closer look at this. after that contains all the important parameters. so first we see lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use ",
        "label": "use"
      }
    ]
  },
  {
    "text": "background language",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " would make the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. in fact, if it's not consistent, we're going to s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "first we see lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water distribution. now next in the rest of this fo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ous counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical kno",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ing precisely 1 background the topic. now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of model setting would be 0 according to our prior. so n",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " why? when mu is infinity, we basically let this one dominate. in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. so in this case we can even force the distribu",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e sometimes we find it useful to use a non-uniform pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this backgr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nguage model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background lang",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ground language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different ca",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also see another smoothing parameter mu here, which co",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "smoothing. there are some interesting special cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "al cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're going to score based on their ratio of proba",
        "label": "use"
      }
    ]
  },
  {
    "text": "opinion mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "s, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. it's also one of the most useful techniques in text mining. and then we're going to talk about opinion mining and sentiment analysis. so this can be regarded as one example of mining knowledge about the observer. and finally, we are going to cover a text based prediction problems where we try to predict some",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "t kinds of knowledge as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data based on some predefined topics. secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. because we can categorize the authors, for example, based on the content of the articles t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "this lecture is about opinion mining and sentiment analysis covering its motivation. in this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have gener",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ure we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. in particular, we're going to talk about the opinion mining and sentiment analysis. as we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. in contrast, we have other devices such as video recorder that can ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ut these words and that would help us better understand what's in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining. so let's first look at the keyword subjective here. now this is in contrast with objective statement or factual statement. those statements can be proved right or wrong. and this is a key differenti",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "ve sentences. instead, would you just get all the sentences that are about opinions that are useful for understanding the person or understanding the product that we are commenting on. so the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. in each representation we should identify opinion holder, target content and context. ideally we can also inf",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "wn. i just gave a good example, in the case of product reviews where the opinion holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. so now that w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "to be one of the simplest opinion mining tasks. now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very us",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": " to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. so here i identify three major reasons, 3 broad reasons. the f",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "can easily build applications by using opinion mining techniques. so now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. so here i identify three major reasons, 3 broad reasons. the first is it can help decision support. i can help us optimize our decisions. we often look at ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, whic",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "this lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. in this lecture, we're going to continue discussing opinion mining and sentiment analysis. in particular, we're going to introduce. late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings. first, motivation",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ication of this technique. and shows that by doing text mining we can understand the users better. and once we can end users better, we can serve these users better. so to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. and as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "duct reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ated to the text, or only remotely related to text data. so this is very different from content analysis or topic mining where we directly characterize the content of text. \" it's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjective\u00a0content\u00a0 which reflects what we know about the opinion holder. but this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. it's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for th",
        "label": "intro"
      }
    ]
  },
  {
    "text": "zero probability",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "umber of segments and this will give us the probabilities that we need to compute mutual information. now there is a small problem. when we have zero counts sometimes and in this case we don't want a zero probability because our data maybe a small sample and in general we would believe that it's potentially possible for award to occur in any context. so to address this problem we can use a technique called smooth",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "ible for award to occur in any context. so to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we don't get a zero probability in any case. now, the best way to understand the smoothing is imagine that we actually. observed more data than we actually have. we will pretend we observe some pseudo segments that are illustrated ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "d we observe some pseudo segments that are illustrated on the top on the right side of the slide and these pseudo segments would contribute additional counts of these words so that no event will have zero probability probability. now, in particular, we introduce the four pseudo segments. each is weighted 1/4. and these represent the four different combinations of occurrences of these words. so now each event, eac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "words in our vocabulary. so for example, the high probability words here are sports, game, basketball, football, play, star, etc. these are sports-related terms and of course it would also give a non zero probability to some other words like \"\"\"travel\"\" which might be related to\" sports. but in general not so much related to the topic. in general, we can imagine a non zero probability for all the words and some w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "rse it would also give a non zero probability to some other words like \"\"\"travel\"\" which might be related to\" sports. but in general not so much related to the topic. in general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one. so that it forms a distribution of all the words. now intuitiv",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents. the second distribution show on the bottom has different wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "d then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. but this ze",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "e of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. but this zero probability for all the unseen words",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ty. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. but this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. so one way to address this problem is actually to use bayesi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we have not seen a word in the training documents for, let's say, category i, then our estimate wou",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "uments for, let's say, category i, then our estimate would be 0 for the probability of this word in this category. and this is generally not accurate. so we have to do smoothing to make sure it's not zero probability. the other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. there is also a third reason, which is sometimes not very obvious, but we'll explain that in a moment an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "or knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. there is also a third reason, which is sometimes not very obvious, but we'll explain that in a moment and that is to help achieve discriminative weighting of terms. and this is also called idf weigh",
        "label": "use"
      }
    ]
  },
  {
    "text": "unigram language",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "le to specify that, because it's impossible to enumerate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but after we make this assumption,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ure out the parameters of the model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ". later, we'll look at this procedure for some more complicated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this case m. and for convenience we're going to u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlie",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to sol",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "s sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denot",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ase. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can set to attract the common words. because com",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "l, in the implementation of em algorithm you will see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common te",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ur data, right? so in this case our data is a collection of documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is to find the clusters and we actually have u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "l or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by usin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster. and this probabilistic assignmen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ed from category one, and t2 represents the documents that are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. there's no mixing of different categories he",
        "label": "use"
      }
    ]
  },
  {
    "text": "word association",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "ural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to analyze conte",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": " because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful wa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "this lecture is about the word association mining and analysis. in this lecture we're going to talk about how to mine associations of words from text. this is an example of knowledge about natural language that we can mine from text data. her",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "bout how to mine associations of words from text. this is an example of knowledge about natural language that we can mine from text data. here's the outline. we are gooing to first talk about what is word association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "tline. we are gooing to first talk about what is word association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. in general there are two word relations, and these are quite basic. one is called a paradigmatic relation, the other is syntagmatic relations. a&b have paradigmatic relation if they can be substitu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "e'll learn the structure and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effective. it's often called query expansion. or you can use related words to suggest ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "o make the query more effective. it's often called query expansion. or you can use related words to suggest related queries to the user to explore the information space. another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. the user could navigate from one word to another to find information in the informati",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "ruct the topic map for browsing where we can have words as nodes and associations as edge. the user could navigate from one word to another to find information in the information space. finally, such word associations can also be used to compare and summarize opinions. for example, we might be interested in understanding positive and negative opinions about iphone 6. in order to do that, we can look at what words",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "e intuition behind the methods for discovering syntagmatic relation. mainly we need to capture the correlation between the occurrences of two words. so to summarize, the general ideas for discovering word associations are the following. for paradigmatically relation we represent each word by its context, and then compute the context similarity. we are gonna assume the words that have high context similarity to ha",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share similar contexts. namely, they occur in similar positions in text. so naturally, our idea for discover",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discoveri",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. earlier we talked about using entropy to capture how easy it is to predic",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "similarity between these words based on their context similarity. so this provides yet another way to do term waiting for paradigmatic. a relation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic relations. these are fairly general. they can be applied to any items in any language, so the units don't have to ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "e based on, tf-idf weighting from retrieval. and tf stands for term frequency\u00a0 idf stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. so these are statistical methods, meaning that the function is defined mostly based on statistics. so the scoring function would be very general. it can be applied to any language and any text. but",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic word",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecide which of the two distributions to use, and this is controlled by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basicall",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecific word here? now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. therefore it's a sum over these two cases. the first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of theta sub d, which is the probability of choosing the model multiplied by the probability of ac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "r, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "m the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we as",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "imator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are kno",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "t would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "he parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of em algorithm. start with initial values that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "d the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our inference on the k values of pis for a document, wherea",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ong to and this question boils down to decide which theta i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. that we need to consider if a topic o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "her prior then it's more likely that the document has been from this cluster, so we should favor such a cluster. the other is a likelihood part, that is this part. and this has to do with whether the topic word distribution can explain the content of this document well. and we want to pick a topic that's high by both values. so more specifically, we just multiply them together and then choose which topic ha",
        "label": "use"
      }
    ]
  },
  {
    "text": "contextual text",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": ", you can see this reference listed here for more detail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location,",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ted to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the met",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "mparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyz",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "s context. what issues mattered in the 2012 presidential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic late",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analy",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilis",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": ". and in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here. i just want to explain the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see some sample results of comparing news articles abou",
        "label": "intro"
      }
    ]
  },
  {
    "text": "probabilistic model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": "sense ambiguation, a topical term or related term can be ambiguous. for example, basketball star versus star in the sky. so in the next lecture we're going to talk about how to solve the problem with probabilistic modeling of the topic.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ing problems, and here i dim the picture that you have seen before in order to show the generation process. so the idea of this approach is actually to 1st design a model for our data. so we design a probabilistic model to model how the data are generated. of course this is based on our assumption. the actual data aren't necessary generated this way, so that would give us a probability distribution of the data that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "l and parameters that are denoted by lambda. so this capital lambda actually consists of all the parameters that we're interested in. and these parameters in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. now in this case, of course, for our tax mining problem, or more precis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. so in this simple setup we are interested in analyzing one document and trying to d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "nctional words they are not really characterizing the topic. so one question is how can we get rid of such common words? \"now this is a topic of the next lecture. we're going to talk about how to use probabilistic models to somehow get rid of these common words.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "nce is that we're going to have more than two topics. otherwise it's essentially the same. so here i illustrate how we can generate the text that i was multiple topics. and naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. so we will also ask the question what's the probability of observing a world w from such a mixture model? now if you look at this picture and com",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "this lecture is about the generative probabilistic models for text clustering. in this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering so this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. i",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "and why text clustering is interesting. in this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. one is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. so to talk about generative models for text clustering, it would be useful to revisit the topic mining p",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture, we're going to continue talking about the tax capture text clustering, particularly \"generative so this is a slide that you have seen earlier where we have writ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "this lecture is a continued discussion of generative probabilistic models for text clustering. in this lecture we're going to finish the discussion of generative probabilistic models for text clustering. so this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also comp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approa",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "cture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ropical retrieval, with another task introduced later by trec. on the bottom we see the variations that are correlated with the publication of the language model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this tech",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "ed with the publication of the language model paper. before we have those classical probabilistic model logic model, boolean model etc. but after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. so this technique here can use event as context. to understand the impact of event again, the technique is general so you can us",
        "label": "use"
      }
    ]
  },
  {
    "text": "clustering result",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ions about the clustering: what is text clustering and why we are interested in text clustering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have learned in some other courses. the idea is to discover natural structures in the data. in other ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": ", so that would give us a different way to cluster the data. if we look at the size and look at the similarity in size. so as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ize and look at the similarity in size. so as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. without perspective, it's very hard to define what is the best clustering result. so there are many examples of text clustering. set up. and so, for example, we can cluster documents in the whole text collection. so in this case documents are the units to be clustered. we may be ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "this case. which is called k means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. now we're going to start with some tentative clustering result by just selecting kate randomly selected vectors as centroids of k clusters and treat them as sentence as they represent each cluster. so this is. this gives us the initial tentative classroom. and t",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "er initialization is similar. and then in the eml with them, you may recall that we're going to repeat eastep and m step to improved our primary destinations. in this case, we're going to improve the clustering result iteratively by also doing 2 steps, and in fact the two steps are very similar to em algorithm. in that when we allocate vector into one of the clusters based on our tentative clustering, it's very si",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5bb813bd-6b7d-4f77-8156-21995f5944ad",
        "lecture": "Lecture 34 \u2014 Text Clustering Similarity based Approaches | UIUC",
        "lecture_num": 34,
        "context": "he next we're going to adjust the centroid, and this is very similar to m step where we re estimate the parameters. that's when we'll have a better estimate of the parameter. so here we have a better clustering result by adjusting the centroid. and note that the central is adjusted based on the average of the vectors in the. a cluster, so this is also similar to the m step, where we do counts pull together counter",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "tem generated clusters to the ideal clusters that are generated by humans? so the closeness here can be assessed assessed from multiple perspectives and that would help us characterize the quality of clustering results in multiple angles. and this is sometimes desirable. now. we also want to quantify the closeness because this would allow us to easily compare different methods based on their performance figures. a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "desired clustering bias. now how do we do that exactly? the general procedure would look like this. given the test set which consists of a lot of text objects, we can have humans who create the ideal clustering result. that is, we're going to ask humans to partition the objects to create the gold standard. and they will use their judgments based on the need of a particular application to generate what they think a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "e're going to ask humans to partition the objects to create the gold standard. and they will use their judgments based on the need of a particular application to generate what they think are the best clustering results. and this would be then used to compare with the system generated clusters from the same test set. and ideally we want the system results to be the same as human generated results, but in general th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "stem generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "ld allow you to think about how to do evaluation in your applications. the 2nd way to evaluate text clusters is to do indirect evaluation. so in this case the question to answer is how useful are the clustering results for the intended applications? now this of course is application specific question, so usefulness is is going to depend on specific applications. in this case, the clustering bias is imposed by the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "plication specific question, so usefulness is is going to depend on specific applications. in this case, the clustering bias is imposed by the intended application as well. so what counts as the best clustering result would be dependent on the application. procedure wise we also would create the test set with text objects for the intended application to quantify the performance of the system. in this case what we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "da74c929-efc1-4b65-9635-684c7ebcab3f",
        "lecture": "Lecture 35 \u2014 Text Clustering  Evaluation | UIUC",
        "lecture_num": 35,
        "context": "e question is, as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters. and finally, evaluation of clustering results and can be done both directly and indirectly. and we also would like to do both in order to get good sense about how our method works. so here's some suggested reading, and this is particularly usef",
        "label": "intro"
      }
    ]
  },
  {
    "text": "data mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "knowledge that we can use in (the) real world. especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. so a more general picture would be to include non text data as well. and for this reason, we might be concerned with joint mining of te",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "text window or longer text article and this would give us different kinds of associations. these discovery associations can support them. any other applications in both information retrieval and text data mining. so here are some recommended readings. if you want to know more about the topic, the 1st is a book with a chapter on locations which is quite relevant to the topic of these lectures. the second is t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "rts or talking about some international events, etc. or we are interested in knowing about the research topics. for example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago. now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "380a7417-6702-4df8-9818-5aceba7cde2b",
        "lecture": "Lecture 14 \u2014 Topic Mining and Analysis  Motivation and Task Definition | UIUC",
        "lecture_num": 14,
        "context": "ics. for example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago. now this involves discovery of topics in data mining, literatures and also we want to discover topics in today's literature and those in the past. and then we can make a comparison. we might be also interested in knowing what do people like about some ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the pa",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ering? in the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. \"so what is text\u00a0 clustering actually is a very general technique for data mining. as you might have learned in some other courses. the idea is to discover natural structures in the data. in other words, we want to group similar objects together. in our case, these objects are of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "this lecture is about the text categorization. in this lecture we're going to talk about the text categorization. this is a very important technique for a text, data mining and analytics. it is relevant to discovery of various different kinds of knowledge as shown here. first is related to topic mining analysis. and that's because it has to do with analyzing text data b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "on. because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. and so this is a very important technique for text data mining. this is the overall plan for covering the topic. first we're going to talk about what is text categorization and why we are interested in doing that in this lecture. and then we're going to talk abo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "redict more information about the world. how can we get sophisticated patterns of text together with other kinds of\u00a0data? \" it would be useful to first take a look at the big picture of prediction in data mining in general and i call this\u00a0data mining loop. \" so the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the real world in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": ". how can we get sophisticated patterns of text together with other kinds of\u00a0data? \" it would be useful to first take a look at the big picture of prediction in data mining in general and i call this\u00a0data mining loop. \" so the picture that you're seeing right now is that there are multiple sensors, including human sensors to report what we have seen in the real world in the\u00a0form of data. \" and of course the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "would be important because we might want to act on that. we might want to make decisions based on that. so how can we get from the data to these predicted values? well, in general we first have to do data mining and analysis of the data. because we in general should treat all the data that we collected. in such a prediction problem set up, we are very much interested in joint mining of non text and text data",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ed to be consumed by humans. humans are the best in consuming or interpreting text data. but when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. \" sometimes machines can see patterns in a lot of data that humans may not see, but in general human would play an important role in analyzing text data in all applications. next human also must be ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "'s interesting that human could be also involved in controlling the sensors. and, this is so that we can adjust the sensors to collect the most useful data for prediction. so that's why i called this data mining loop because as we perturb the sensors to collect the new data and more useful data then we will obtain more data for prediction. this data generally will help us improve the prediction accuracy and ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "u identify data points? that would be most helpful for machine learning programs if you can label them, right. so in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the world and then observe what happens. and then you can then decide what additional data. have to be collected by adjusting the sensor. or fro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " text data, it provides a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question that we have not addressed yet. so",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "nd you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next lectures. now the other perspective is text data can help non text data mining as well. and this is because text data can help interpret patterns discovered from non text data. this helps discover some frequent patterns from non text data. now we can use the text data that are ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text in the context of time. so time is a context in this case. is there any difference in the responses of people in different reg",
        "label": "use"
      }
    ]
  },
  {
    "text": "document clustering",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "that we are going to assume there are fewer topics. the number of documents. so this document must share some topics. and if we have n documents for share k topics, then will again have precisely the document clustering problem. so because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. so the question now is what generating mode",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ated from the first one on the top. that means the words in the document could have been generated in general from multiple distributions. now this is not what we want to see for text clustering. for document clustering where we hope this document will be generated from precisely one topic. so now that means we need to modify the model, but how well, let's first think about why this model cannot be used for clusteri",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " the entire document d. now, if you compare this picture with the previous one, you will see the desicion of. of using a particular distribution is made of just once for this document. in the case of document clustering. but in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "probability of a document. now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. and of course, the main problem in document clustering is to infer. which distribution has been used to generator a document and that would allow us to recover the cluster identity over document so it would be useful to think about the difference from th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " think about the difference from the topic model, as i have also mentioned multiple times. there are many. two differences. one is the choice of. using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. four different words. the second is that word distribution here is going to be used to generate all the words for a document. but in the ca",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": " so here you can see at the formula looks very similar or in many ways they are similar. but there's also some difference. and in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. and that's corresponding to our assumption of 1st make a choice of choosing one distribution and then stay with this distribution to generate all the wor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ng, particularly \"generative so this is a slide that you have seen earlier where we have written down the likelihood function for a document. with two distributions in two component mixture model for document clustering. now in this lecture, we're going to generalize this to include the k clusters. now if you look at the formula and think about the question how to generalize it, you will realize that all we need is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ore thetas and the probabilities of thetas and the probabilities of generating d from those thetas. so this is precisely what we're going to use. this is general presentation of the mixture model for document clustering. so as more cases we follow these steps using a generated model. first think about our data, right? so in this case our data is a collection of documents n documents denoted by the sub i. and then we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "use the chance of a document being a large cluster is generally higher than in a small cluster. so this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. so next we have to discuss how to actually compute the estimate of the model.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml works for topic models, plsa and i think here it will be very similar and you just need to adapt a little bit to \u00a0this new mixture model. so as you may recall, em",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discriminative approaches. so the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. in",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "tegories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. but because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. or we can understand how we can use generative models to do text categorization from the perspective of clustering. and so this is a slide that we've talked about befo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nt in your product of the probability of each word and that's just because we've made the assumption about the independence in generating each word ok. so this is just something that you have seen in document clustering. an we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. so this idea can be ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "relation discovery",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "this lecture is about the paradigmatic relation discovery. in this lecture we're going to talk about how to discover a particular kind of word association called paradigmatic relations. by definition, 2 words are paradigmatically related if they share simil",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "s would realize that vector space model has been used frequently for modeling documents and queries for search. but here we also find it convenient to model the context of a word for paradigmatically relation discovery. so the idea of this approach is to view each word in our vocabulary as defining one dimension in high dimensional space so we have n words in total in the vocabulary. then we have n dimensions as il",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "in this lecture, we continue discussing paradigmatic relation discovery. earlier, we introduced a method called expected overlap of words in context. in this method, we represent each context by a word vector that represents the probability of word in the context and we ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "51be74e8-eb10-47c2-a768-b688605de1e0",
        "lecture": "Lecture 11 \u2014 Syntagmatic Relation Discovery  Conditional Entropy | UIUC",
        "lecture_num": 11,
        "context": "this lecture is about the syntagmatic relation discovery and conditional entropy. in this lecture, we're going to continue the discussion of word association mining an analysis. we're going to talk about the conditional entropy, which is useful for discove",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mu",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "this lecture is about the syntagmatic relation discovery and mutual information. in this lecture, we're going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a proble",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "al the sum is actually one. so that's why in the denominator you still want there. so this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. no. so, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. we introduce the three concepts from information theory, e",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "s that are comfortable, and so we can rank these pairs and discover the strongest cinematical relationship from collection of documents. now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. so we already discussed the possibility of using bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "nk these pairs and discover the strongest cinematical relationship from collection of documents. now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. so we already discussed the possibility of using bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "or all the words, then we can cluster these words or computer similarity between these words based on their context similarity. so this provides yet another way to do term waiting for paradigmatic. a relation discovery an. so to summarize, this whole part about word association mining, we introduce the two basic associations, called paradigmatic and syntagmatic relations. these are fairly general. they can be appli",
        "label": "intro"
      }
    ]
  },
  {
    "text": "logistical regression",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": ". conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. some examples include the logistical regression support vector machines and the k nearest neighbors. we will cover some of these classifiers in detail in the next few lectures.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ocuments. just like in the case of naive bayes we can clearly see naive bayes classifier is a special case of this general classifier. actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification. and we are going to talk more about such approaches later, but here i want you to know that t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "fiers. they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logistical regression is to model the dependency of the binary response variable y here, on some predictors. that are denoted as x. so here we have also changed the notation to x for feature values you may recall in the p",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "nding y i of course, y i could be one or zero and in fact the function form here would vary depending on whether y sub i is one or zero. if it's one will be taking this form. and that's basically the logistical regression function. but what about this if it's 0? well, if it's zero then we have to use a different form and that's this one. now how do we get this one? that's just one minus the probability of y = 1, right",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "already known, all we need is to compute the xi's for that document. and then plugging those values that will give us a estimate. the probability that the document is in category one. ok, so much for logistical regression. let's also introduce another discriminative classifier called k nearest neighbors. now in general, i should say there are many such approaches. and thorough introduction to all of them is clearly be",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "he moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this work. i note that in naive base classifier we did not need a similarity function. an in logistical regression, we did not talk about the similarity function either. but here we explicitly requires a similarity function. now this similarity function. actually is a good opportunity for us to inject any of our ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " lines. but the question is when we have multiple lines that can separate the both clauses, which line is the best? in fact, you can imagine there are many different ways of choosing the line. so the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": " document. and y has two values, binary response variable {0,1}. 1 means x is positive, 0 means x is negative. and then of course, this is a standard two category categorization problem. we can apply logistical regression. you may recall that in logistic regression we assume the log of probability that y is equal to 1 is assumed to be a linear function of these features as shown here. so this would allow us to also wr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "t's logistical function and you can see it relates this probability to probability that y = 1 to the feature values. and of course, b_i is our parameters here. so this is just a direct application of logistical regression for binary categorization. what if we have multiple categories, multiple levels? we actually use such a binary logistic regression program to solve this multi level rating prediction. and the idea is",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "'s classifier two, and in the end we need a classifier to distinguish two and one so altogether we'll have k - 1 classifiers. now if we do that of course, then we can also solve this problem, and the logistical regression program would be also very straightforward as you have just seen on the previous slide. only that here we have more parameters because for each classify we need a different set of parameters. so now ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "st let's look at the classifier that corresponds to level of rating k. so this classifier will tell us whether this object should have a rating of k or above. and if its probability according to this logistical regression classifier is larger than .5, we're going to say yes, the rating is k. now, what if it's not as large as .5? well, that means the reading is below k, right? so now we need to invoke the next class fi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2d0e46c7-df4e-48b3-9550-dac3fec3062d",
        "lecture": "Lecture 45 \u2014 Opinion Mining and Sentiment Analysis  Ordinal Logistic Regression | UIUC",
        "lecture_num": 45,
        "context": "l these classifiers. so we should be able to take advantage of this factor. now the idea of ordinal logistic regression is precisely that\u00a0 a key idea is just the improvement over the k -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the beta parameters these are the parameters that indicate the influence of those weights. and we're g",
        "label": "use"
      }
    ]
  },
  {
    "text": "likelihood estimate",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "rameters? of course, in order to answer this question we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "rved data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "so now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ty to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplification of course. so we're interested in which value of data",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "eresting point estimates of theta. now this point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it's the most likely value of theta given by the posterior dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "robability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're interested in computing the posterior distribution of the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid of them, that would mean we have to do something d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "cture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in com",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the idea of perhaps we can guess which word is from which. dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "t think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. now. these parameters that we are interested in, nam",
        "label": "use"
      }
    ]
  },
  {
    "text": "posterior probability",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "and this means we will impose some preference on certain thetas over others. and by using bayes rule that i have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. now a full explanation of bayes rule and some of these things related to bayesian reasoning would be outside the scope of this course, but i just give a brief introduction because t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "lues so we have a prior here. that includes our prior knowledge about the parameters. and then we have the data likelihood here that would tell us which parameter value can explain the data well. the posterior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ell us which parameter value can explain the data well. the posterior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum li",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "terior probability combines both of them. so it represents a compromise of the two preferences. and in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a non",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "vailable to us. and so a better way is to use the likelihood together with the prior. in this case the prior is p of theta i. and together, that is, we're going to use the base formula to compute the posterior probability of theta given d. and if we choose theta based on this posterior probability and we would have the following formula that you see here. on the bottom of this slide, and in this case, we're going to c",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "e prior. in this case the prior is p of theta i. and together, that is, we're going to use the base formula to compute the posterior probability of theta given d. and if we choose theta based on this posterior probability and we would have the following formula that you see here. on the bottom of this slide, and in this case, we're going to choose the theta that has a large p of theta i. that means a large cluster and",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "epresenting k different distributions. and more specifically, basically we're going to apply bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. given the document. an we know it's proportional to the probability of selecting this distribution p of theta i and the probability of generating this whole document from that di",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "lly, we just multiply them together and then choose which topic has the highest product. so more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document. get posterior becausw this one p of theta i is the prior, that's our belief about which topic is more likely. before we observe any document. but this conditional pro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "he document. get posterior becausw this one p of theta i is the prior, that's our belief about which topic is more likely. before we observe any document. but this conditional probability here is the posterior probability of the topic after we have observed the document d. and bayes rule allows us to update this probability based on the prior and i shown the details. below here you can see how the prior here is relate",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " the posterior on the left hand side. and this is related to how well this word distribution explains the document here, and the two are related in this way. so to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course. an we can then change the probability of document in your product of the probability of each word an",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e what we did for text clustering. namely, we are going to assign document d to the category that has the highest probability of generating this document. in other words, we're going to maximize this posterior probability as well. and this is related to the prior and the likelihood an as you have seen on the previous slide. and so naturally, we can then decompose this likelihood into a product. as you see here now her",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "tegory that has the highest score by this function. so this is called a naiyes bayes classifier. now the keyword bayes is understandable because we are applying a bayes rule here. when we go from the posterior probability of the topic to a product of the likelihood and the prior. now it's also called a naive because we've made an assumption that every word in the document is generated independently, and this is indeed",
        "label": "use"
      }
    ]
  },
  {
    "text": "machine learning",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "e order of applying the rules or combination of results that are contradictory. so all these. problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning. so these machine learning methods are more automatic, but i still put automatic in quotation marks cause they're not really completely automatic because it still require manual work. more specifical",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "es or combination of results that are contradictory. so all these. problems with this approach, and it turns out that the both problems can be solved or alleviated by using machine learning. so these machine learning methods are more automatic, but i still put automatic in quotation marks cause they're not really completely automatic because it still require manual work. more specifically, we have to use human ex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "oice to start with. but of course there are other sophisticated features like phrases or even policy feature tags or even syntactic structures. so once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. so soft rules just means we're going to still decide which category should be assigned to the document. but it's not going to be used us",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "o a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. if the human would to make a judgement. so when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. so the setup is. to learn a classifier to map a value of x into a map of y. so here x is all",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "rate the value in y as output, and we hope the output y would be the right category for x, and here correct of course is judged based on the training data, so that's the general goal, like in all the machine learning problems or supervised learning problems where you are given some examples of input and output for function and then the computer is going to figure out how the function behaves like based on these e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "this lecture is about how to use generative probabilistic models for text categorization. there are in general are two kinds of approaches to text categorization by using machine learning. one is generative probabilistic models, the other is discriminative approaches. in this lecture, we're going to talk about the generative models. in the next lecture, we're going to talk about discr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "tion to x for feature values you may recall in the previous slide we have used fi to represent the feature values. an here we use the notation of x vector, which is more common when we introduce such machine learning algorithms, so x is our input, it's a vector. and with m features. and each feature has a value x sub i here and our goal is model the dependency of this binary response variable on all these feature",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "classifier that can be used to do a lot of classification tasks, including text categorization. so as in all cases of model, we would be interested in estimating the parameters and in fact in all the machine learning programs. once you set up the model set of objective function. to model the classifier, then the next step is to compute the parameter values. in general, we're going to adjust these parameter values",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "fier called k nearest neighbors. now in general, i should say there are many such approaches. and thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "eral, i should say there are many such approaches. and thorough introduction to all of them is clearly beyond the scope of this course and you should take a machine learning course or read more about machine learning to know about them. here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. so the second classifier, ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "ng to assume that the probability of theta given d would be also similar, and so that's a very key assumption, and that that's. that's actually important assumption that would allow us to do a lot of machine learning, but in reality, whether this is true of course would depend on how we define similarity, because the neighborhood is largely determined by our similarity function. if our similarity function capture",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "hat these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here. although there are new machine learning methods like representation learning that can help with learning features. an another common scene is that they might be. be performing similarly on the data set but with different mistakes and so th",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "to make the same mistakes so. and symbol approaches that would combine different methods and tend to be more robust and can be useful in practice. most techniques that we introduce the use supervised machine learning and which is a very general method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "l method. so that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. problems to solve the categorization problem. to allow us to characterize content of text concisely with categories or the predictor, some pro",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " to obtain. insights for design new features. so error analysis very important in general, and that's where you can get the insights about your specific problem. and then finally we can leverage some machine learning techniques. so for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you ac",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "h such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful. deep learning is a new technique that has been developed in machine learning. it's particularly useful for learning representations, so different learning refers to deep neural network. it's another kind of classifier where you can have intermediate features embedded in the m",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "tion negative opinions so that rain example is not all of that high quality, but they can still be useful. another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. so in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even im",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "th the true training examples. to improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. that invol",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "g categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you. do that effectively. here's a suggestion reading an where you can find more details about some of the methods that we have covered.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6382e23f-d54e-4ece-a231-8df819983fb5",
        "lecture": "Lecture 42 \u2014 Text Categorization  Evaluation - Part 2 | UIUC",
        "lecture_num": 42,
        "context": "ou can obtain insights for improving a method. finally, sometimes ranking may be more appropriate, so be careful. sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well. so here are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures. the second is a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " discriminative and they are more specific. if you match it and it says a lot and it's accurate. it's unlikely, very ambiguous. but it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories. an obviously that kind of classifier won't generalize well to future data",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " mixed grams of word and part of speech tags. or even a part of parse tree. so in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. in general, i think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features. so first you want to use domain knowledge a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "re design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. in general, i think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features. so first you want to use domain knowledge and your understanding of the problem to design seed features. and you can also define a basic fe",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "eatures. so first you want to use domain knowledge and your understanding of the problem to design seed features. and you can also define a basic feature space with a lot of possible features for the machine learning program to work on. and machine learning can be applied to select the most effective features or construct the new features that feature learning. and these features can then be further analyzed by h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "knowledge and your understanding of the problem to design seed features. and you can also define a basic feature space with a lot of possible features for the machine learning program to work on. and machine learning can be applied to select the most effective features or construct the new features that feature learning. and these features can then be further analyzed by humans through error analysis. and you can",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": " more discriminating, so this really caused tradeoff between frequent versus infrequent features, and that's why feature design is generally an art. that's perhaps the most important part in applying machine learning to any problem in particular. in our case, for text categorization, or more specifically, sentiment classification.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "ed and machines would of course help humans identify what data should be collected next. in general, we want to collect data that are most useful for learning. and this there is actually a subarea in machine learning called active learning that has to do with this. how do you identify data points? that would be most helpful for machine learning programs if you can label them, right. so in general, you can see the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "are most useful for learning. and this there is actually a subarea in machine learning called active learning that has to do with this. how do you identify data points? that would be most helpful for machine learning programs if you can label them, right. so in general, you can see there's a loop here from data acquisition to data analysis or data mining to prediction of values, and to take actions to change the ",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic coverage",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b75d0d2e-a1b7-4f6a-b699-f81ec4b720c7",
        "lecture": "Lecture 15 \u2014 Topic Mining and Analysis  Term as Topic | UIUC",
        "lecture_num": 15,
        "context": " high score over term. ok so after this then we will get k topical terms and those can be regarded as the topics that we discovered from the collection. next let's think about how we can \"compute the topic coverage i j so looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document how should we figure out the coverage of each topic in the document? one a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "on. and for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. so you see a constraint here and we still have another constraint on the topic coverage, namely pis. so all the pis of ij's must sum to one for the same document. so how do we solve this problem? well, let's look at this problem as a computation problem now. so we clearly specify the in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the topic coverage in each document. a document is not allowed to cover a topic outside the set of topics that we are discovering. so the coverage of each of these k topics would sum to one for a document. we also intr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": " of topics and vocabulary set. and of course, the text data right? and then the output is of two kinds. one is the topic category characterization seedies hci is a water distribution and 2nd it's the topic coverage for each document. these are pie some ideas and they tell us which document covers which topic to what extent. so we hope to generate these as output because there are many useful applications if we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "e and we still have two constraints, different constraint, two kinds of constraints. one is awarded distributions. all the words must have probabilities that sum to 141 distribution. the other is the topic coverage distribution. anna document will have to cover precisely these k topics, so the probability of covering each topical would have to sum to one. so at this point it's basically where they find applied ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "n certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects. now, a user may also have knowledge about the topic coverage. and we may know which topic is definitely not covered in which document or is covered in the document. for example, we might have seen those tags topic tags assigned to documents. and those tag coul",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": " version of plsa and the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "pute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again face the problem of influence of these variables because they're not the parameters of the model. so the inference part. again, face the local maxima problem. so esse",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "tput two things. one is a set of topics denoted by theta i's. each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. so this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. now a main difference between this and text clustering problem is that here a document is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "erent versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. the other is that we assume. the topic coverage also depends on the context. and that means depending on the time or location, we might cover topics differently. and then again this dependency would then allow us to capture the association of topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "nd so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. because in the. in the case of location like texas, people might want to cover the red topics more at the new audience, as visualized here. but in a certai",
        "label": "intro"
      }
    ]
  },
  {
    "text": "optimization problem",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "a i's must sum to one, we can plug this into this constraint here, and this will allow us to solve for lambda. and this is just negative sum of all the counts and this further allows us to then solve optimization problem. eventually to find the optimal setting for theta sub i. and if you look at this formula, it turns out that it's actually very intuitive because this is just the normalized count of these words by th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "eneral, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. instead, we have to use some numer",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": " also see later. so this is basically an analytical solution to our optimization problem. in general, though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likeliho",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "o after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. one is",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "hese k topics, so the probability of covering each topical would have to sum to one. so at this point it's basically where they find applied math problem. you just need to figure out the solutions to optimization problem. there's a function with many variables and we need to just figure out the values of these variables to make the function which its maximum.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "e provided by humans. and they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. so ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. different methods tend to vary in their ways of measuring the errors on the training data. they might optimize a different",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard optimization problem. in this case, it can be also solved in many ways. newtons method is a popular way to solve this problem. there are other methods as well, but in the end will we're going to get the set of beta value",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ind the line? or the separator. now this is equivalent to finding values for w&b because they would determine where exactly the separator is. so in the simplest case, the linear osfm is just a simple optimization problem. so again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights w&b. and the classifier will say x is i",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "e objective that's tide to maximization of margin and this is simply to maximize sorry to minimize w transpose multiplied by w and we often denote this by file w. so now you can see this is basically optimization problem, right? we have some variables to optimize and these are the weights and b and we have some constraints. these are linear constraints and the objective function is a quadratic function of the weights",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ve seen before, but we have introduced the extra variables. cassie i an we in fact will have one for each data instance and this is going to model the error that will allow for each instance. but the optimization problem will be very similar. so specifically, you will see we have added something to the optimization problem. first we have added some. some error to the constraint so that now we allow. allow the classif",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "h data instance and this is going to model the error that will allow for each instance. but the optimization problem will be very similar. so specifically, you will see we have added something to the optimization problem. first we have added some. some error to the constraint so that now we allow. allow the classifier to make some mistakes here, so this kci is allowed error if we set kci to 0, then we go back to the ",
        "label": "use"
      }
    ]
  },
  {
    "text": "probabilistic topic",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that y",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuitively we need to use more words to describe the to",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ords in the fuzzy manner. finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the ol",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so fir",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "cal language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. so ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we h",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the backg",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "this lecture is about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scena",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "hen we use algorithm called collapsed gibbs sampling. then the algorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to take tax data as input, and we're going to outp",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "to give similar performance, so in practice, plsa, an lda, would work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? can we ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "generate model",
    "contexts": []
  },
  {
    "text": "information retrieval",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " compute these similarity functions. now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. but here we also find it convenient to model the context of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " compute the xi or yi? and the other question is, how do you compute the similarity? now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. and they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "nt can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. these discovery associations can support them. any other applications in both information retrieval and text data mining. so here are some recommended readings. if you want to know more about the topic, the 1st is a book with a chapter on locations which is quite relevant to the topic of these lect",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " some insight about data. this is also very useful, but sometimes a user might have some expectations about which topics to analyze. for example, we might expect to see retrieval models as a topic in information retrieval. we also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects. now, a user may a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "re are also many variants of the problem formulation and so first we have the simplest case, which is a binary categorization where there are only two categories and there are many examples like that information retrieval or search engine applications would want to. distinguish it relevant documents from non relevant documents for a particular query. spam filter is interesting. distinguishing spams from non spam. so a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "sation results? so first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called cranfield evaluation methodology. the basic idea is to help humans to create test collection. where we already every document is tagged with the desired categories, or in the case ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3956403f-f159-448a-9514-5dc69f314c5a",
        "lecture": "Lecture 41 \u2014 Text Categorization  Evaluation - Part 1 | UIUC",
        "lecture_num": 41,
        "context": "es. alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. and these are also proposed by information retrieval researchers in 19, six days for evaluating searching results. but now they have become a standard measure used everywhere. so when the system says yes, we can ask the question how many are correct? w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " reveal spatial temporal patterns. is yet another application of this kind of model where we look at the use of the model for event impact analysis. so here we are looking at the research articles in information retrieval, ir, particularly sigir papers. and the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left. and then we hope ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "text retrieval conference. this is a major evaluation effort sponsored by us government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on informati",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "s known to have made an impact on the topics of research information retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "retrieval. the other is the publication of a seminal paper by croft and ponte, and this is about the language modeling approach to information retrieval. it's also known to have made a high impact on information retrieval research, so we hope to use this kind of model, understand impact, and the idea here is simply to use the time as context an use these events to divide the time periods into a period before the event",
        "label": "use"
      }
    ]
  },
  {
    "text": "text retrieval",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "s for putting together a larger expression based on component expressions. so we'll learn the structure and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effec",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "omponent expressions. so we'll learn the structure and what can go with what else. word relations can be also very useful for many applications in text retrieval and mining. for example, in search in text retrieval we can use word associations to modify a query. and this can be used to introduce additional related words to a query to make the query more effective. it's often called query expansion. or you can u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": " functions. now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. but here we also find it convenient to model the context of a word for paradig",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "the' would contribute equally as content word like 'eats'. so now we are going to talk about how to solve these problems. most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. so to address the first problem, we can u",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "hese problems. most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. so to address the first problem, we can use a sub linear transformation of term frequency. that is, we don't have to use the raw frequency count of ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "asize so much on the raw frequency. to address the second problem, we can put more weight on rare terms. that is, we can reward matching a rare word and this heuristic is called idf term weighting in text retrieval. idf stands for inverse document frequency. so now we're going to talk about the two heuristics in more detail. first, let's talk about the tf transformation. that is, to convert the raw count of wor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "ector. whereas when we set the key to a very large value, it would behave more like the linear transformation. so this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. so we just talk about how to solve the problem of over emphasizing a frequently frequent term. now let's look at the second problem, and that is how w",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": " are the words that share similar context. and there are many different ways to implement this general idea and we just talk about some of the approaches. and more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these a",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "nction to compute the paradigmatic relations. more specifically, we have used the\u00a0 bm25 and idf weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. finally, syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "t the retrieval models and you can see the top word top words with high probability is about this model on the left. and then we hope to examine the impact of two events. one is the start of trec for text retrieval conference. this is a major evaluation effort sponsored by us government and was launched in 1992 or around that time and that is known to have made an impact on the topics of research information re",
        "label": "use"
      }
    ]
  },
  {
    "text": "language processing",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "topics in text mining and analytics. in this course we're going to selectively cover some of those topics. we actually hope to cover most of these general topics. first, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. second, we're going to talk about how to mine word associations from t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "this lecture is about the text representation. in this lecture we're going to discuss text representation. and discuss how natural language processing can allow us to represent text in many different ways. let's take a look at this example sentence again. we can represent this sentence in many different ways. 1st. we can always represent such a sen",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "but we can't really analyze semantics. yet this is the most general way of representing text, because we can use this to represent any natural language text. if we try to do a little bit more natural language processing by doing word segmentation. then we can obtain a representation of the same text, but in the form of a sequence of words. so here we see that we can identify words like: a, dog, is, chasing, etc. now",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "words representation is not as robust as string of characters. but in english it's very easy to obtain this level of representation, so we can do that all the time. now if we go further to do natural language processing, we can add a part of speech tags. now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. so this opens up a little",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "db1d54dd-bb05-46c0-995b-5f7d5243e3c4",
        "lecture": "Lecture 5 \u2014 Text Representation - Part 1 | UIUC",
        "lecture_num": 5,
        "context": "ios, what kind of actions will be made? so this is... another level of analysis that would be very interesting. so this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. and unfortunately, such techniques would require more human effort. and they are less accurate. that means there are mistakes. so if we analyze text data at the levels that are",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "niques have also several advantages. first, they are general and robust, so they are applicable to any natural language. that's a big advantage over other approaches that rely on more fragile natural language processing techniques. secondly, it does not require much manual effort or sometimes it does not require any manual effort. so that's again important benefit, because that means you can apply directly to any ap",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "s indicated by words like a bad and worst. and we can also then identify the context. new england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "f1951cf2-4293-450b-8578-4d74c72f9862",
        "lecture": "Lecture 43 \u2014 Opinion Mining and Sentiment Analysis  Motivation | UIUC",
        "lecture_num": 43,
        "context": "w england in this case. now unlike in the product review, all these elements must be extracted by using natural language processing techniques. so the task is much harder and we need a deeper natural language processing. and these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened. analyzing sentiment in news is still quite difficult. it's more diffi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4a54f790-991c-44bb-ab62-713cbef84ad1",
        "lecture": "Lecture 44 \u2014 Opinion Mining, Sentiment Analysis  and  Sentiment Classification | UIUC",
        "lecture_num": 44,
        "context": "hms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. so in general, natural language processing is very important to derive complex features. they can enrich text representation. so for example, this is a simple sentence that i showed you long time ago, and in another lecture. so from these wor",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "m news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. implicit and so that calls for natural language processing techniques to uncover them accurately. so here are some suggested readings, the first 2. are small books that are excellent reviews of this topic where you can find a lot of discussion about the othe",
        "label": "use"
      }
    ]
  },
  {
    "text": "training example",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "aluate the categorisation results so. the problem of texture categorisation is defined as follows. we're given a set of predefined categories. possibly forming a hierarchy so. and often also a set of training examples or training set of labeled text objects. which means that text objects have already been labeled with known categories, and then the task is to classify any tax object into one or more of these pred",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "39d13817-de51-4195-a33a-985b0b54e64d",
        "lecture": "Lecture 36 \u2014 Text Categorization Motivation | UIUC",
        "lecture_num": 36,
        "context": "rocessed by a categorisation system. and the system will in general assign categories to these documents as shown on the right. and the categorisation results. and we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in different categories, and this would further hel",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "classifier, is called k nearest neighbors. in this approach, we're going to also estimate the conditional probability of label. given data, but in a very different way. so the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the k examples in the training set and that are most similar to this text object. basically this is to find the neigh",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ing effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can als",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "s like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training exam",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ding the training examples, it's generally hard to get a lot of training examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. for example, if you take a reviews from the internet, they might have overall ratings. so to train a sentiment categorizer meaning",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "ining examples because it involves human labor. but there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. for example, if you take a reviews from the internet, they might have overall ratings. so to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categor",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": " train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then. we could assume five star reviews are all positive training examples. onstar negative but of course sometimes in five star reviews. we also mention negative opinions so that rain example is not all of that high quality, but they can still be useful. another idea is r",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "led as category two. although the label is not completely reliable. but then they can still be useful. so let's assume they are actually training label examples and then we combine them with the true training examples. to improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. that involves data that follow very different distributions from what we are working on. but basically when the two d",
        "label": "intro"
      }
    ]
  },
  {
    "text": "bayesian inference",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "terior mode, it's the. it's the most likely value of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate. in general, in bayesian inference we are interested in the distribution of all these parameter values. as you see, here is there's a distribution over theta values that you can see here p of theta given x. so the problem of bayesian ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "inference we are interested in the distribution of all these parameter values. as you see, here is there's a distribution over theta values that you can see here p of theta given x. so the problem of bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on theta. so i showed f of theta here as an interesting variable that we want to compute. but ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "sting quantities that might depend on theta. so i showed f of theta here as an interesting variable that we want to compute. but in order to compute this value, we need to know the value of theta. in bayesian inference, we treat data as uncertain variable. so we think about all the possible values of theta. therefore we can estimate the value of this function f as the expected value of f according to the posterior ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "heta.\u00a0 and it's sometimes the same as posterior mode, but it's not always the same, so it gives us another way to estimate the parameters. so this is a general illustration of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basica",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e been having been generated from theta sub d or from theta sub b? so in other words, we want to infer which distribution has been used to generate this text. now, this inference process is a typical bayesian inference situation where we have some prior about these two distributions so can you see what is our prior here? well the prior here is the probability of each distribution, right? so the prior is given by th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " we call it prior. if we don't observe the word or we don't know what word has been observed, our best guess is just say well. they are equally likely. alright, so it's just a flipping a coin. now in bayesian inference, we typically then would update our belief after we have observed evidence. so what is evidence here? while the evidence here is the word text. now that we are interested in the word text, so text ca",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "w can we incorporate such knowledge into plsa? it turns out that there's a. a very elegant way of doing that, and that's all incorporated such knowledge as priors on the models. and you may recall in bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen. so in this case we can use maximum a posteriori estimate, also called map estimate, and the formula is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "re complicated. but what's also important is not set. now. these parameters that we are interested in, namely the topics and the coverage, are no longer parameters in lda. in this case we have to use bayesian inference or posterior inference to compute them based on the parameters alpha and beta. unfortunately, this computation is intractable, so we generally have to resort to approximate. influence. and there are ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "fferent toolkits for lda, or you read the papers about that these different extensions of lda. now here we of course can't give in depth introduction to, but just know that they are computed based on bayesian inference with. by using the parameters of alphas and beta. but algorithmically, actually in the end, in some algorithm at least, it's very similar to plsa an, especially when we use algorithm called collapsed",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b1854d1c-3199-4c42-ab7d-f219f70259a3",
        "lecture": "Lecture 46 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 1 | UIUC",
        "lecture_num": 46,
        "context": "g in the aspect i, and that's why we can take some over all the words in the vocabulary. now, what about the aspect weights? alpha sub i of d? it's not part of our parameter, right? so we have to use bayesian inference to compute it. and in this case we can use the maximum a posteriori. 2 computer this alpha value. basically we're going to maximize the product of the prior of our according to our assumed market val",
        "label": "use"
      }
    ]
  },
  {
    "text": "hidden variable",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "so this is indeed a general idea of the expectation maximization, or em algorithm. so in all the em algorithms, we introduce a hidden variable to help us solve the problem more easily. in our case, the hidden variable is a binary variable for each occurrence of word. and this binary variable would indicate whether the world has been generat",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": " this is indeed a general idea of the expectation maximization, or em algorithm. so in all the em algorithms, we introduce a hidden variable to help us solve the problem more easily. in our case, the hidden variable is a binary variable for each occurrence of word. and this binary variable would indicate whether the world has been generated from theta on sunday or theater super b. and here we show some possible ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "d is from the topic. then it's 0 for z etc. now, of course we don't observe those z values. we just imagine there are such a social values of z attached to all the words. and that's why we call these hidden variables. now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. the value of this hidden variable. and so. the alg",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "hy we call these hidden variables. now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. the value of this hidden variable. and so. the algorithm, the em algorithm then would work as follows. first will initialize all the parameters with random values. in our case the parameters are mainly the probability of a word given",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "the right data counts. re estimate our parameters. and then once we have a new generation of parameters, we're going to repeat this. we're going to use the e-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "es, and then that would lead to another generation of re estimate the parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captur",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "e believed to have come from the topic. we have high probability like this one text. and of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values. so we have a new generation of values because of the e step based on the new generation of parameters. and this these new in further values of these will give us then another generation of th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": " would depend on initial points. the general idea is that we will have two steps to improve the estimate of parameters in the e step. we roughly all augmenting our data by predicting values of useful hidden variables that we would use to simplify the estimation. in our case, this is the distribution that has been used to generate the world. in the end step, then would exploit such augmented data, which would mak",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "in order for the parameters also too. convert it to some stable value. now he thought data augmentation is done probabilistically. that means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the wo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "n is done probabilistically. that means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the world counts between the two distributions.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "we can compute this maximum regular estimated by using the em algorithm. so in the e-step, we now have to introduce more hidden variables because we have more topics. so our hidden variable z now, which is a topic indicator, can take more than two values. specifically, will take a k plus one values with b denoting the background and o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "we can compute this maximum regular estimated by using the em algorithm. so in the e-step, we now have to introduce more hidden variables because we have more topics. so our hidden variable z now, which is a topic indicator, can take more than two values. specifically, will take a k plus one values with b denoting the background and one through k to denote all the k topics. so now the e",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "y, will take a k plus one values with b denoting the background and one through k to denote all the k topics. so now the e step as you can recall is augmented data and by predicting the values of the hidden variable. so we're going to predict for word whether the word has come from one of these k+1 distributions. this equation allows us to predict the probability that the word w in document \"d is generated from ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "d with the previous likelihood if it doesn't change much and we're going to say stop right? so in each step we can do e step and m step in the e step we're going to augment the data by predicting the hidden variable values. in this case the hidden variable z sub dw indicates whether word in w in d is from topic or background, an if it's from a topic which topic? so if you look at the e step formulas essentially ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "n't change much and we're going to say stop right? so in each step we can do e step and m step in the e step we're going to augment the data by predicting the hidden variable values. in this case the hidden variable z sub dw indicates whether word in w in d is from topic or background, an if it's from a topic which topic? so if you look at the e step formulas essentially we're actually normalizing these counts. ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " until their likelihood converges. an in each step will do e step and m step. in m step. we're going to infer which distribution has been used to generate each document. and so we have to introduce a hidden variable zd for each document and this value variable could take a value from the range of one through k representing k different distributions. and more specifically, basically we're going to apply bayes rul",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": ". there are two words, sorry, two occurrences of text and two occurrences of mining. so there are four words together. medical and health did not occur in this document, so this first thing about the hidden variables. now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "and two occurrences of mining. so there are four words together. medical and health did not occur in this document, so this first thing about the hidden variables. now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "our words together. medical and health did not occur in this document, so this first thing about the hidden variables. now for each document we must use a hidden variable and before in plsa we used 1 hidden variable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidde",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "iable for each word. because that's the output from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must tell us which distribution has been used to generate the document, so it's going to take two values, one and two to indicate the two topics. so now",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "from what mixture model. so in our case the output from a mixture model or the observation from mixture model is a document not a word. so now we have 1 hidden variable attached to the document. that hidden variable must tell us which distribution has been used to generate the document, so it's going to take two values, one and two to indicate the two topics. so now how do we infer which distribution has been us",
        "label": "use"
      }
    ]
  },
  {
    "text": "component model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "e observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. and in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. so the basic idea of a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "fferent ways of generating the word. and in each case it's a product of the probability of selecting that component model. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. so the basic idea of a mixture model is just to treated these two distributions together as one model. so i use the b",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "m, each term is a product again of two terms. and the two terms are ,first, the probability of selecting a component like theta sub d. second, the probability of actually observing the word from this component model. and so this is a very general description of, in fact, all the mixture models. and i just want to make sure that you understand this, because this is really the basis for understanding all kinds of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "and to see what intuitively what we do in order to set these probabilities to maximize the value of this function. ok, if we look into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "tribution theta sub d to assign somewhat higher probability to this word. now it's also interesting to think about the impact of probability of theta sub b. the probability of choosing one of the two component models. now, we've being so far, assuming that each model is equally likely and that gives us 0.5, but you can again look at this like your function and try to picture what would happen if we increase the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. 1st every component component model attempts to assign high probabilities to high frequency words in the data. and this is to collaboratively maximize likelihood. second, different component models tend to bet high probabilities on dif",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " these intuitions. 1st every component component model attempts to assign high probabilities to high frequency words in the data. and this is to collaboratively maximize likelihood. second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize the likelihood. 3rd",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "bability, and this would allow them to collaborate more efficiently to maximize the likelihood. 3rd, the probability of choosing each component regulates the collaboration and competition between the component models. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " collaborate more efficiently to maximize the likelihood. 3rd, the probability of choosing each component regulates the collaboration and competition between the component models. it would allow some component models to respond more to the change, for example of frequency of data point in the data. we also talk about the special case of fixing one component to a background word distribution, and this distributio",
        "label": "use"
      }
    ]
  },
  {
    "text": "world distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "opics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. the first is the constraint on the word distributions. in each world distribution, the probabilities on all the words must sum to one over all the words in the vocabulary. the second constraint is on the topic coverage in each document. a document is not allowed to cover a topic o",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "t we can account for all kinds of words. and this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. meaning that they have a very smal",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize that well we can simply take all these words that are known to be from this word distribution theta sub d and norma",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "e different probabilities in the background. so even though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities. so these words are believed to be more likely from the topic. these, on the other hand, are less likely probably from background. so once we have the z values,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "ribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? so this is our primary goal. we hope to have a more discriminating world distribution. but the last column is also by product and this actually can also be very useful and you can think about that. and one use is to. for example, is made to what extent this document has covered backgr",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "so controlled by a parameter mu and we're going to add mu multiplied by the probability of w given our prior distribution to the connected counts. when we re estimate the when we re estimate the this world distribution right? so this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been generated from this topic. but now we force this di",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "ine the concept or theme or topic. in fact, the topic models that you have seen some previous lectures. can give you cluster of terms in some sense. if you take the terms with high probabilities from world distribution. another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. for example, we might extract all the tex",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8717e27a-33fb-4d06-ae68-2e0d915b1568",
        "lecture": "Lecture 31 \u2014 Text Clustering Generative Probabilistic Models - Part 1 | UIUC",
        "lecture_num": 31,
        "context": "ions. because we have a very simplified situation of just two clusters. and so in this case you can see it's a sum of two cases. in each case it's indeed the probability of choosing the. choosing the world distribution. is theta one or theta two right? and then it's this probability is multiplied by the probability of observing this document from this particular distribution. and if you further expand this probabil",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "hat we hope to model as well. so we're not going to just model the text. and so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. because in the. in the case of location like texas, people might want to cover the re",
        "label": "use"
      }
    ]
  },
  {
    "text": "mining algorithm",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aca8d826-412b-4134-8d2c-87537fdc4a76",
        "lecture": "Lecture 2 \u2014 Overview Text Mining and Analytics - Part 2 | UIUC",
        "lecture_num": 2,
        "context": "so looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. and we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. especially for decision making or for completing whatever tasks that require text data to support, now bec",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "entation at this level. now moving down, we'll see we can gradually add additional representations. by adding syntactic structures we can enable, of course, syntactic graph analysis. we can use graph mining algorithms to analyze syntactic graphs. and some applications are related to this kind of representation. for example, stylistic analysis generally requires syntactical representation. syntactical structure re",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "2997c717-2552-411d-9dc4-7e648e16bbf0",
        "lecture": "Lecture 6 \u2014 Text Representation - Part 2 | UIUC",
        "lecture_num": 6,
        "context": "se other levels can be combined and should be combined in order to support sophisticated applications. so to summarize, here are the major takeaway points. text representation determines what kind of mining algorithms can be applied. and there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc. \"and these different representations should in ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": " our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. so this is a general idea of using a generative model for text mining. first, we design a model with some parameters that we are interested in, and then we model the data. we adjust the parameters t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge that we discover from the text. so let's look at these steps for this very simple case. later, we'll look at this procedure for some mo",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "alize the count to estimate the probabilities or to revise our estimate of the parameters. so let me also illustrate we can group the words that are believed to have come from cedar sub d and as text mining algorithm for example and clustering. and we had group them together. to help us re estimate the parameters. that were interested in so these will help us re estimate these parameters. but note that before we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9b1334-5f53-407b-8864-2cff7edbc603",
        "lecture": "Lecture 30 \u2014 Text Clustering  Motivation | UIUC",
        "lecture_num": 30,
        "context": "e've got to discover interesting clusters that might also represent the subtopics. so this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. the goal of doing more sophisticated mining and analysis of text data. we can also cluster fairly large text law gets, and by that i just mean ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "ee41ce3a-6c8d-4c3b-90fd-79f3e9190402",
        "lecture": "Lecture 47 \u2014 Opinion Mining and Sentiment Analysis  Latent Aspect Rating Analysis - Part 2 | UIUC",
        "lecture_num": 47,
        "context": "ome information that's very hard to obtain, even if you read all the reviews. even if you read all the reviews, it's very hard to infer such preferences or such emphasis. so this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. you can compare different hotels, compare the opinions from different consumer gro",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": "des a much better representation of the problem and it serves as more effective predictors. and similarly, sentiment analysis can lead to such predictors as well. so those are the data mining or text mining algorithms can be used to generate the predictors. the other question is how can we join mine text and non text data together? now this is a question that we have not addressed yet. so in this lecture and the ",
        "label": "use"
      }
    ]
  },
  {
    "text": "prior knowledge",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "we want the distribution to characterize the topic of text mining. so one way to address this problem is actually to use bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. we assume that we have some prior belief about the parameters. now in this case, of course, so we are not going to look at just the data, but also look at the prior so the prior",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ihood of observing this y if x is indeed true. so much for a detour about bayes rule. so in our case, what we're interested in is inferring the theta values so we have a prior here. that includes our prior knowledge about the parameters. and then we have the data likelihood here that would tell us which parameter value can explain the data well. the posterior probability combines both of them. so it represents a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ate the parameters. so this is a general illustration of bayesian estimation and bayesian inference. inference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language mod",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ticular, we are going to talk about some extensions of plsa, and one of them is lda or latent dirichlet allocation. so the plan for this lecture is to cover two things. one is to extend the plsa with prior knowledge that would allow us to have in some sense a user controlled plsa, so it doesn't blindly just listen to data but also would listen to our needs. the second is to extend the plsa as a generative model ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "our needs. the second is to extend the plsa as a generative model fully generated model. this has led to the development of latent dirichlet allocation or lda. so first let's talk about the plsa with prior knowledge. in practice, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by usi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ed. for example, we may want to set one of the pis to 0. and this would mean we don't allow that topic to participate in generating that document. and this is only reasonable, of course, when we have prior knowledge that strongly suggests this.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ity of this word in this category. and this is generally not accurate. so we have to do smoothing to make sure it's not zero probability. the other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "he other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probabi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "prior knowledge, and this is also generally true for a lot of situations of smoothing. when the data set is small, we tend to rely on some prior knowledge to to solve the problem. so in this case our prior knowledge\u00a0 says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. there is also a third reason, which is sometimes not ve",
        "label": "use"
      }
    ]
  },
  {
    "text": "discriminative classifier",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "nt variations of. many variations of these learning methods. so in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers. the other is called discriminative classifiers. the generative classifiers try to learn what the data looks like in each category. so it attempts to model the join the distribution of the data and the label x&y. and, this can then be factored ou",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ng errors. but if we can model the data in each category accurately, then we can also classify accurately. one example is naive bayes classifier. in this case. the other kind of approaches are called discriminative classifiers. these classifiers try to learn what features separate categories, so they directly tackle the problem of categorisation or separation of classes. so sorry for the problem. so these discriminative c",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "aac4e33c-97bb-46c8-a108-3e3e3322a85c",
        "lecture": "Lecture 37 \u2014 Text Categorization  Methods | UIUC",
        "lecture_num": 37,
        "context": "ative classifiers. these classifiers try to learn what features separate categories, so they directly tackle the problem of categorisation or separation of classes. so sorry for the problem. so these discriminative classifiers attempted to model the. conditional. probability of the label given the data point directly. so the objective function tends to directly measure the errors of categorisation on the training data. so",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ere's a strong connection close connection between the two kinds of approaches, and this slide shows how naive bayes classifier can be connected to a logistic regression. and you can also see that in discriminative classifiers that tend to use a more general form on the bottom, we can accommodate more features to solve the problem.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "this lecture is about the discriminative classifiers for text categorization. in this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. this is a slide that you have seen from the discuss",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": " be all the words and their features can be other signals that we want to use. and we mentioned that this is precisely similar to logistic regression. so in this lecture we're going to introduce some discriminative classifiers. they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that indirectly. as we have seen in naive bayes. so the general idea of logi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "r that document. and then plugging those values that will give us a estimate. the probability that the document is in category one. ok, so much for logistical regression. let's also introduce another discriminative classifier called k nearest neighbors. now in general, i should say there are many such approaches. and thorough introduction to all of them is clearly beyond the scope of this course and you should take a mach",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this lecture is a continued discussion of. discriminative classifiers for text categorization. so in this lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popular classification method, and there has b",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "7f9f9b1e-4527-4875-b6f7-4dc0d57ab719",
        "lecture": "Lecture 40 \u2014 Text Categorization  Discriminative Classifier - Part 2 | UIUC",
        "lecture_num": 40,
        "context": "this lecture is a continued discussion of. discriminative classifiers for text categorization. so in this lecture will introduce yet another discriminative classifier called a support vector machine or vm, which is a very popular classification method, and there has been also shown to be effective for text categorization. so to introduce this classifier, let's als",
        "label": "intro"
      }
    ]
  },
  {
    "text": "probability distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "ity that you would actually pick this word from d1 if you randomly pick the word. now of course these xi's will sum to 1 because they are normalized frequencies. and this means the vector is actually probability distribution over words. so, the vector d2 can be also computed in the same way. and this would give us then two probability distributions representing two contexts. so that addresses the problem how to compute t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "b9eb5a35-5bad-4cf5-9d94-b0b0c816c8cf",
        "lecture": "Lecture 8 \u2014 Paradigmatic Relation Discovery - Part 1 | UIUC",
        "lecture_num": 8,
        "context": "use they are normalized frequencies. and this means the vector is actually probability distribution over words. so, the vector d2 can be also computed in the same way. and this would give us then two probability distributions representing two contexts. so that addresses the problem how to compute the vectors? next, let's see how we can define similarity in this approach. well, here we simply define the similarity as a do",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "r data. so we design a probabilistic model to model how the data are generated. of course this is based on our assumption. the actual data aren't necessary generated this way, so that would give us a probability distribution of the data that you are seeing on this slide given a particular model and parameters that are denoted by lambda. so this capital lambda actually consists of all the parameters that we're interested ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "anguage models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distribution that gives \"\"\"today is wednesday\"\" a probability of 0.001\" it might give \"\"\"today wednesday is\"\" which is a non\" grammatical sentenc",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "here and we might then multiply these numbers together to get the probability \"of \"\"today is wednesday\"\".\" so as you can see, with n probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model. ignore the word order, so it may not be effective for some problems such as speech recognition, where you may care about the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "ference. and later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood functi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "20703c3c-ced6-4410-ace1-139baa46505c",
        "lecture": "Lecture 25 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 3 | UIUC",
        "lecture_num": 25,
        "context": "rt it to some stable value. now he thought data augmentation is done probabilistically. that means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the world counts between the two distributions.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "that distribution is over what? for example, the probability of theta is overall the key topics and that's why these k probabilities sum to 1. well, whereas the probability of a word given theta is a probability distribution over all the words. so there are many probabilities and they have to send one. so now let's take a look like this. take a look at the simple example of two clusters. i have two clusters. i've shown s",
        "label": "use"
      }
    ]
  },
  {
    "text": "maximum likelihood estimator",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " we trust data entirely and try to fit the data and then we will be biased. so in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likeli",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. the same as here. ok, but if we have some informative prior, some bias towards certain values, then map estima",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ". and the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. which means we'll take the estimated parameters as a knowledge that we discover from the text. so let's look at th",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "e just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here. and you might also notice that this is the general result of maximum likelihood estimator. in general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. so this is basically an analytical solution t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "mization problem by having a closed form formula. instead, we have to use some numerical algorithms, and we're going to see such cases later also. so if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here, let's imagine this document is a text mining paper. now what you might see is something that looks like this. on the top you will see the high prob",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "r all the unique words in our vocabulary, instead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "stead of having a product of all the positions in the document and this form where we look at different unique words is a convenient form for computing the maximum likelihood estimator later. and the maximum likelihood estimator\u00a0 is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. one is the word probabilities in each topic must sum to one",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "e observed document here and we assume all the other parameters are known. now, although we designed the model heuristically to try to factor out this background words. it's unclear whether if we use maximum likelihood estimator we will actually end up having order distribution where the common words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "on words like the will be indeed having smaller probabilities than before. so now. in this case, it turns out that the answer is yes, and when we set up the probalistic model in\u00a0 this way when we use maximum likelihood estimator we will end up having a word distribution that where the common words will be factored out via the use of the background distribution. so to understand why this is so, it's useful to examine the beha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "ook into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. but there are also competing in someway an in particular they will be competing on the words. and they will tend to bet high probabilities on different words to avoid this competition in some sense.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "t do you think? now you might want to pause the video a moment to think more about this question, because this has to do with understanding of important behavior of a mixture model and indeed all the maximum likelihood estimator. now if you look at the formula for a moment then you will see. it seems that now the objective function is more influenced by the than text before each contributed one turn. so now, as you can imagi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": "ore word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. this is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. it also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. now it's also interesting ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "ext data, and these are precisely the unknown parameters. so after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. and we can do the usual thing. maximum likelihood estimator. so again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, dif",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ould be useful to take a comparison between the two. this gives us different distributions and these tells us how to improve the parameters? and as i just explained in both e step formulas, we have a maximum likelihood estimator based on the allocated word \"counts to \"topic theta sub-j. now this phenomena is actually general phenomenon in all the em algorithms in the m step, you generate computed expected count of event base",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "s. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-word distributions, each representing a topic and the other is the proportion of each to",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "n practice, when we apply plsa to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. the standard plsa is going to blindly listen to the data by using maximum likelihood estimator. we are going to just fit data as much as we can and get some insight about data. this is also very useful, but sometimes a user might have some expectations about which topics to analyze. for exampl",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ay it's impossible, but we're going to just strongly favor certain kind of distributions. and you will see example later. the map can be computed using a similar em algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. and in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "pi's and theta's. so this is a likelihood function for lda. now let's next let's talk about parameter is making an inference is now the parameters can be now estimated using exactly the same approach maximum likelihood estimator for lda. now you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": " but it's basically still just a product of the probabilities of all the words. i and so with the lack of functioning. now we can talk about how to do parameter estimation. here we can simply use the maximum likelihood estimator, so that's just a standard way of doing things, so all should be familiar to you now, it's just a different model. so after we have estimate the parameters, how can we then allocate clusters to the d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "ikelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. in this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. now, as in most cases, the em algorithm can be used to solve this problem for mixture models. so here's the detail of this em algorithm for document clustering. now, if you have understood how eml w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. so to estimate the probability of each category. and to answer the question which category is most popular, then we can simply normalize the count of documents in",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " if you have observed a small amount of data. so smoothing is the important technique to address data sparseness. in our case the training data set can be small and one data set is small. when we use maximum likelihood estimator we often face the problem of zero probability. that means if the event is not observed. then the estimated probability would be 0 in this case if we have not seen a word in the training documents for",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4da6283d-6903-4be9-8bfc-ad5d330343c6",
        "lecture": "Lecture 39 \u2014 Text Categorization  Discriminative Classifier - Part 1 | UIUC",
        "lecture_num": 39,
        "context": "n see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. so, as in other cases, when compute the maximum likelihood estimator basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. and this again then gives us a standard optimization problem. in this case, it can be also so",
        "label": "use"
      }
    ]
  },
  {
    "text": "background language model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "5350ccd0-beab-48fc-8484-d8e6a38c4cbf",
        "lecture": "Lecture 22 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 2 | UIUC",
        "lecture_num": 22,
        "context": " would make the discovered\u00a0 topic more discriminative. this is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. in fact, if it's not consistent, we're going to say the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "1cc2d7fa-3d11-49fa-b979-ef5e9442466f",
        "lecture": "Lecture 26 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 1 | UIUC",
        "lecture_num": 26,
        "context": "first we see lambda sub b here. this represents the percentage of background words. that would believe exist in the text data and this can be unknown value that we set empirically. second, we see the background language model and typically we also assume this is known. we can use a large collection of text or use all the tests that we have available to estimate the water distribution. now next in the rest of this formula.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "ous counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": " model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. and, we show that with maximum likelihood estimator we can discover topical knowledge from text data. in this case plsa allows us to discover two things. one is k-w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": "ing precisely 1 background the topic. now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. in other words, in other cases if it's not like that, we're going to say supplier says it's impossible. so the probability of that kind of model setting would be 0 according to our prior. so now we ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "5190e288-54f7-4021-9083-8e8ceac11345",
        "lecture": "Lecture 28 \u2014 Latent Dirichlet Allocation LDA - Part 1 | UIUC",
        "lecture_num": 28,
        "context": " why? when mu is infinity, we basically let this one dominate. in fact, we are going to set this one. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. so in this case we can even force the distribution t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "e sometimes we find it useful to use a non-uniform pseudo counts for the words. so here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background l",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nguage model. theta sub b now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "set of all the training data to estimate this background language model. but if we don't have to use this one, we can use larger text data that are available from somewhere else. now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language m",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ground language model to add pseudocounts, we find that some words will receive more pseudocounts. so what are those words? well those are the common words. because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. now, this addition of background model would cause nonuniform smoothing of this ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "s more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categori",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. from the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. you also see another smoothing parameter mu here, which controls",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "smoothing. there are some interesting special cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obvi",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "al cases to think about as well. first, let's think about when mu approaches infinity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other s",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "nity. what would happen? or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. and that essentially removes the difference between these categories. obviously we don't want to do that. the other special cases we think about the background model an suppose we actually set the t",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": " highest score as we discussed earlier. now it's useful to further understand whether the naive bayes scoring function actually makes sense, so to understand that. and also to understand why adding a background language model will actually achieve the effect of idea of idf weighting and to penalize common words. right, so it's suppose we have just two categories and we're going to score based on their ratio of probability",
        "label": "use"
      }
    ]
  },
  {
    "text": "unigram language model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "le to specify that, because it's impossible to enumerate all the possible sequences of words. so in practice we will have to simplify the model in some way. so the simplest language model is called a unigram language model. in such a case, we simply assume that text is generated by generating each word independently. now, in general, the words may not be generated independently, but after we make this assumption, we ca",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "ure out the parameters of the model given some observed data, and we're going to talk about that in a moment. let's first talk about the sampling. so here i show two examples of word distributions or unigram language models. the first one has higher probabilities for words,\u00a0 text, mining, association, etc. now this signals a topic about text mining, because when we sample words from such a distribution we tend to see w",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": " the topics. so to summarize, we introduced the language model which is basically probability distribution over text. it's also called a generative model for text data. the simplest language model is unigram language model. it's basically a word distribution. we introduced the concept of likelihood function which is the probability of data given some model. and this function is very important. given a particular set of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": ". later, we'll look at this procedure for some more complicated cases. so our data in this case is just the document which is a sequence of words. each word here is denoted by x sub i. our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. so we will have as many parameters as many words in our vocabulary, in this case m. and for convenience we're going to use the",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier wher",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. so if you want to solve the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "s sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ase. so to summarize, and we talked about the mixture of two unigram language models. and the data we consider here is just still 1 document. and the model is a mixture model with two components: two unigram language models. specifically, theta sub d which is intended to denote the topic of document d and theta sub b which is representing a background topic that we can set to attract the common words. because common wo",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "4453a049-7597-4df4-9b9b-67c2d124a116",
        "lecture": "Lecture 27 \u2014 Probabilistic Latent Semantic Analysis PLSA - Part 2 | UIUC",
        "lecture_num": 27,
        "context": "l, in the implementation of em algorithm you will see you accumulated counts various counts and then you normalize them. so to summarize, we introduced the plsa model, which is a mixture model with k unigram language models representing k topics. and we also added a predetermined background language model to help discover discriminating topics. because this background language model can help attract the common terms. a",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "ur data, right? so in this case our data is a collection of documents n documents denoted by the sub i. and then we talk about the model. think about the model. in this case, we design a mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "f64adab4-578a-4868-8b2c-03fdd4ddf55d",
        "lecture": "Lecture 32 \u2014 Text Clustering Generative Probabilistic Models - Part 2 | UIUC",
        "lecture_num": 32,
        "context": "mixture of k unigram language models. it's a little bit different from the topic model. but we have similar parameters. we have a set of theta i's denote the word distributions corresponding to the k unigram language models. we have p of each theta i as the probability of selecting each of the k distributions to generate the document. now note that, although our goal is to find the clusters and we actually have used a ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": " by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. so in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a u",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "l or word distribution, and that's similar to topic model. so here you can see the word distribution actually generates a term cluster as a byproduct. a document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using mult",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "dccc8a84-66da-47ce-ab88-28e8acf192b9",
        "lecture": "Lecture 33 \u2014 Text Clustering Generative Probabilistic Models - Part 3 | UIUC",
        "lecture_num": 33,
        "context": "then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. and then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster. and this probabilistic assignment that",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ed from category one, and t2 represents the documents that are known to have been generated from category two, etc. now if you look at this picture, you see that the model here is really a simplified unigram language model. it is no longer mixture model. why? because already know which distribution has been used to generate which documents. there's no uncertainty here. there's no mixing of different categories here. so",
        "label": "use"
      }
    ]
  },
  {
    "text": "topic word distribution",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecide which of the two distributions to use, and this is controlled by another probability: probability of theta sub d and probability of theta sub b here. so this is the probability of selecting the topic word distribution. this is the probability of selecting the background word distribution denoted by theta sub b. now in this case i just give example where we can set both to .5. so if you can do basically flip a coin",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": " the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. let's say the coin shows up as head, which means we're going to use the topic word distribution. then we're going to use this word distribution to generate a word. otherwise we might be going through this path. and we're going to use the background word distribution to generate the word. so in ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "ecific word here? now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. therefore it's a sum over these two cases. the first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of theta sub d, which is the probability of choosing the model multiplied by the probability of actually observ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "r, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "m the topic word distribution. so the idea is to assume that the text data actually contain two kinds of words. one kind is from the background here. so the is away etc and the other kind is from our topic word distribution that we're interested in. so in order to solve this problem of factoring out background words, we can set up our mixture model as follows. we're going to assume that we already know the parameters of",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in computing this estimate. and we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "imator. now let's start with the idea of separating the words in the text data into two groups. one group would be explained by the background model, the other group would be explained by the unknown topic word distribution after all, this is the basic idea of mixture model. but suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "t would mean, for example these words: the is and we are known to be from this background word distribution. on the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "e other words, text, mining, clustering, etc are known to be from the topic word distribution. if you can see the color, then these are shown in blue. these blue words are then assumed to be from the topic word distribution. if we already know how to separate these words, then the problem of estimating the world distribution would be extremely simple, right? if you think about this for a moment, you realize that well we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "4c3d9363-8e43-44fe-ab21-8740cfd1a8c5",
        "lecture": "Lecture 24 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 2 | UIUC",
        "lecture_num": 24,
        "context": "he parameters. for the word distribution that we're interested in. ok, so as i said, the bridge between the two is really variable z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. so this slide has a lot of content and you may need to pause the video to digest it, but this basically captured the essence of em algorithm. start with initial values that are often ra",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "d the parameters are now much more regularized. you will see there are many fewer parameters. and you can achieve the same goal as plsa for text mining. it means it can compute the topic coverage and topic word distributions as in plsa. however, there is no free launch while the parameters for plsa\u00a0 is much fewer, there were fewer parameters and in order to compute the topic coverage and word distributions, we again fac",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by alpha. and similar here. the topic word distributions are drawn from another dirichlet distribution with beta parameters and note that here alpha has k parameters corresponding to our inference on the k values of pis for a document, whereas here beta h",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "ong to and this question boils down to decide which theta i has been used to generate d. suppose d has l words represent represent as xi here. now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? in general, we use bayes rule to make this inference. and you can see this prior information here. that we need to consider if a topic or cluster has",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "6962b043-7dd8-4050-bad0-bbdb13e2c302",
        "lecture": "Lecture 38 \u2014 Text Categorization Generative Probabilistic Models | UIUC",
        "lecture_num": 38,
        "context": "her prior then it's more likely that the document has been from this cluster, so we should favor such a cluster. the other is a likelihood part, that is this part. and this has to do with whether the topic word distribution can explain the content of this document well. and we want to pick a topic that's high by both values. so more specifically, we just multiply them together and then choose which topic has the highest",
        "label": "use"
      }
    ]
  },
  {
    "text": "contextual text mining",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " text data can help text mining. because non text data can provide a context for mining text data. provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. and that's to mine text in the context defined by non text data. and you can see this reference here for a large body of work in this direction, and we're going to highlight some of them in the next",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": ", you can see this reference listed here for more detail. so here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "a97a9d5e-48b7-4f4e-9754-4c5b30a31424",
        "lecture": "Lecture 48 \u2014 Text Based Prediction | UIUC",
        "lecture_num": 48,
        "context": " here are the reference that i just mentioned. the first is reference for pattern\u00a0annotation. \" the second is a qiaozhu mei dissertation on contextual text mining. it contains a large body of work on contextual text mining\u00a0techniques. \"",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "this lecture is about the contextual text mining. contextual text mining is related to multiple kinds of knowledge that we mine from text data. as i'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and si",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "ted to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. so more specifically, why are we interested in contextual text mining? well that's, first, because text often has rich context information and this can include direct context such as meta data. and also indirect context, so the direct context can include the meta-data ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "mparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. so there are many interesting questions that require contextual text mining here, i list some very specific ones. for example, what topics have been gaining increasing attention recently in data mining research? now to answer this question, obviously we need to analyze text ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "95f92696-1963-4307-83c6-a8370ff03b30",
        "lecture": "Lecture 49 \u2014 Contextual Text Mining  Motivation | UIUC",
        "lecture_num": 49,
        "context": "s context. what issues mattered in the 2012 presidential campaign or presidential election? now in this case, time series again as context. df so, as you can see, the list can go on and on, basically contextual text mining can have many applications.",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent sema",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "this lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " analysis. in this lecture, we're going to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": " to continue discussing contextual text mining. and we're going to introduce contextual probabilistic latent semantic analysis as an extension of plsa for doing contextual text mining. recall that in contextual text mining we hope to analyze topics in text. in consideration of context so that we can associate the topics with appropriate context that we're interested in. so in this approach contextual\u00a0 probabilistic lat",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": ". and in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. and this would allow us to do contextual text mining. so this is the basic idea. now we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail here. i just want to explain the high le",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "3103be2f-681e-41cf-b0f7-21cf6ba56616",
        "lecture": "Lecture 50 \u2014 Contextual Text Mining  Contextual Probabilistic Latent Semantic Analysis | UIUC",
        "lecture_num": 50,
        "context": "those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. and this is precisely what we want in contextual text mining. so here are some sample results from using such a model. not necessary exactly the same model, but similar models. so on this slide you see some sample results of comparing news articles about iraq ",
        "label": "intro"
      }
    ]
  },
  {
    "text": "discover syntagmatic relation",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": " right, in each sentence. and then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? now thinking about this question would help us discover syntagmatic relations. because syntagmatic relation essentially captures such correlations. so the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? so the q",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "0d005f7e-0c3f-465b-b7fe-08f45355e3de",
        "lecture": "Lecture 7 \u2014 Word Association Mining and Analysis | UIUC",
        "lecture_num": 7,
        "context": "-occur together with eats, meaning that whenever you see eat, you tend to see the other words. and if you don't see eat, probably you don't see other words often either. so this intuition can help us discover syntagmatic relations. now again, consider example- how helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence would generally help us predict the whether meat also",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "rare word, so we emphasize more on matching rare words now. so with this modification, then the new function will likely address those two problems. now interestingly we can also use this approach to discover syntagmatic relations. in general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9a443634-7f2e-4d3a-9ccd-0f1b6604c939",
        "lecture": "Lecture 9 \u2014 Paradigmatic Relation Discovery - Part 2 | UIUC",
        "lecture_num": 9,
        "context": "be candidate for syntagmatic relations. now of course, this is only a bi-product of our approach for discovering paradigmatic relations. and in the next lecture, we're going to talk more about how to discover syntagmatic relations. but it clearly shows the relation between discovering the two relations. and indeed they can be discussed, discovered in a joint manner by leveraging such associations. so to summarize, the main id",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "2736e0b3-cd3e-4760-b07e-e9aadcc588e2",
        "lecture": "Lecture 10 \u2014 Syntagmatic Relation Discovery  Entropy | UIUC",
        "lecture_num": 10,
        "context": "this lecture is about the syntagmatic relation discovery. an entropy. in this lecture, we're going to continue talking about word association mining. in particular, we can talk about how to discover syntagmatic relations. and we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. by definition, syntagmatic relations hold between words tha",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "9e5a0c5f-ff4a-42a6-b37b-f43628632860",
        "lecture": "Lecture 12 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 1 | UIUC",
        "lecture_num": 12,
        "context": "'re going to continue discussing syntagmatic relation discovery. in particular, we're going to talk about another concept, the information theory, called mutual information. and how it can be used to discover syntagmatic relations? before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "bm 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. but here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. so this would give us another way to represent the context. of a word like a cat, and if we do the same for all the words,",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": " maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. and similarly, counting co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. these discovery association",
        "label": "use"
      }
    ]
  },
  {
    "text": "maximum likelihood estimate",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "44df41bc-04d3-41ca-ac51-dbd22dc98305",
        "lecture": "Lecture 13 \u2014 Syntagmatic Relation Discovery  Mutual Information - Part 2 | UIUC",
        "lecture_num": 13,
        "context": "in general, we can use the empirical counts of events in the observed data to estimate probabilities. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. so if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "f the parameters? of course, in order to answer this question we have to define what we mean by best. in this case, it turns out that our guesses are indeed the best in some sense, and this is called maximum likelihood estimate. and it's the best in that it would give our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "our observed data the maximum probability. meaning that if you change the estimate somehow even slightly, then the probability of the observed text data will be somewhat smaller. and this is called a maximum likelihood estimate.",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "so now let's talk about the problem a little bit more and specifically, let's talk about the two different ways of estimating parameters. one is called maximum likelihood estimate that i already just mentioned. the other is bayesian estimation. so in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "robability to find a theta that would maximize this posterior probability. and this estimator is called the maximum a posteriori or map estimate. and this estimate is a more general estimate than the maximum likelihood estimate. because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "d if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. now let's look at the bayesian estimation in more detail. ok, so i show the theta values as just one dimension value and that's a simplification of course. so we're interested in which value of data",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "some interesting point estimates of theta. now this point represents the mode of prior. that means the most likely parameter value according to our prior before we observe any data. this point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. now this point is interesting. it's the posterior mode, it's the. it's the most likely value of theta given by the posterior dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "48b37a2f-5ca3-4b7b-9bfc-d841da37c566",
        "lecture": "Lecture 18 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 2 | UIUC",
        "lecture_num": 18,
        "context": "higher probability. given a data point, sorry, given a data sample x, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. we also talked about the bayesian estimation or influence. in this case we must define a prior on the parameters p of theta, and then we're interested in computing the posterior distribution of the ",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "elihood function and it would look like this. next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. so now let's take a look at the maximum likelihood estimate problem more closely. this line is copied from the previous slide. it's just our likelihood function. so our goal is to maximize this likelihood function. we will find it often easy to maximize the l",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": ". so if you want to solve the problem. it will be useful to think about why we end up having this problem. well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. so in order to get rid of them, that would mean we have to do something d",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": " this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. so we are interested in com",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub i. and that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. this idea, however, doesn't work, because we in practice don't really know which word is from which distribution. but this gives us the idea of perhaps we can guess which word is from which. dis",
        "label": "use"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "you might think about how many parameters are there in lda versus plsa. you will see there are fewer parameters in lda because in this case the only parameters are alphas and betas. so we can use the maximum likelihood estimated to compute that. of course it's more complicated because the form of likelihood functions more complicated. but what's also important is not set. now. these parameters that we are interested in, nam",
        "label": "use"
      }
    ]
  },
  {
    "text": "probabilistic topic model",
    "contexts": [
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you hav",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "this lecture is about the probabilistic topic models for topic mining and analysis. in this lecture we're going to continue talking about the top mining and analysis. we're going to introduce probabilistic topic models. so this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. so to solve these problems intuitively we need to use more words to describe the topic an",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "6a9537c1-62fc-4f31-a8a7-bdfcad2f50eb",
        "lecture": "Lecture 16 \u2014 Topic Mining and Analysis  Probabilistic Topic Models | UIUC",
        "lecture_num": 16,
        "context": "ords in the fuzzy manner. finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. it turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. so the basic idea here is improved representation of topic as a word distribution. so what you see now is the old repr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "this lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, wh",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "54ab232c-85cb-4829-abd0-6cbaed5f3fc8",
        "lecture": "Lecture 17 \u2014 Probabilistic Topic Models  Overview of Statistical Language Models - Part 1 | UIUC",
        "lecture_num": 17,
        "context": "cal language models which cover probabilistic topic models as special cases. in this lecture we're going to give an overview of statistical language models. these models are general models that cover probabilistic topic models as special cases. so first, what is the statistical language model? a statistical language model is basically the probability distribution over word sequences. so, for example, we might have a distr",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "e6c92e3a-0169-4af1-96d7-3fe4a5303e3a",
        "lecture": "Lecture 19 \u2014 Probabilistic Topic Models  Mining One Topic | UIUC",
        "lecture_num": 19,
        "context": "this lecture is a continued discussion of probabilistic topic models. in this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. so in thi",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "8eaf2971-31ff-40b7-9fc6-b91c7637f916",
        "lecture": "Lecture 20 \u2014 Probabilistic Topic Models  Mixture of Unigram Language Models | UIUC",
        "lecture_num": 20,
        "context": "this lecture is about a mixture of unigram language models. in this lecture we will continue discussing probabilistic topic models. in particular, we're going to introduce a mixture of unigram language models. this is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "27d06808-2624-4922-a079-04dccb301dde",
        "lecture": "Lecture 21 \u2014 Probabilistic Topic Models  Mixture Model Estimation - Part 1 | UIUC",
        "lecture_num": 21,
        "context": "this lecture is about mixture model estimation. in this lecture, we're going to continue discussing probabilistic topic models. in particular, we're going to talk about how to estimate the parameters of a mixture model. so let's first look at our motivation for using a mixture model and we hope to factor out the background ",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "93efd70b-16b1-45f1-9c81-c0f2aeb7c9ee",
        "lecture": "Lecture 23 \u2014 Probabilistic Topic Models  Expectation Maximization Algorithm - Part 1 | UIUC",
        "lecture_num": 23,
        "context": "this lecture is about the expectation maximization algorithm, also called the em algorithm. in this lecture, we're going to continue the discussion of probabilistic topic models. in particular, we're going to introduce the em algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. so this is now familiar scenario of",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "hen we use algorithm called collapsed gibbs sampling. then the algorithm looks very similar to the em algorithm. so in the end they're doing something very similar. so to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. the best basis test setup is to take tax data as input, and we're going to output the",
        "label": "intro"
      },
      {
        "course": "CS_410",
        "transcription": "d857a66b-1018-4ffb-821a-9d8acc6f5012",
        "lecture": "Lecture 29 \u2014 Latent Dirichlet Allocation LDA - Part 2 | UIUC",
        "lecture_num": 29,
        "context": "to give similar performance, so in practice, plsa, an lda, would work equally well for most tasks. here are some suggested readings if you want to know more about the topic. first is a nice review of probabilistic topic models. the 2nd paper has a discussion about how to automatically label a topic model. now i've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? can we use ph",
        "label": "intro"
      }
    ]
  }
]