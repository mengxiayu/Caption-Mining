{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/177.xml", "intro": "In 1966, Eliza simulated dialogue as a Rogerian psychotherapist [47]. Fast forward to 2016, the MIT Technology Review heralded chatbots as one of the year's breakthrough technologies [33]. Chatbots have made much headway since Eliza's introduction. However, it has become apparent that current conversational technologies are still inadequate at handling all of the complexities of natural language interactions, as manifested by a number of high-profile chatbot failures [2,34]. Breakdowns in understanding user input happen often, and they can have profound impact on how people perceive and interact with a chatbot. In the worst case, they may abandon the chatbot or the current task. Or, they may need to endure a haphazard trial-and-error process to recover from the breakdown. Both breakdowns and current recovery processes decrease peoples' satisfaction, trust, and willingness to continue using a chatbot [19,20,28].A universal challenge faced by chatbot developers is how to design appropriate strategies that mitigate the negative impact of breakdowns. Previous work [19,24,42,48] studied strategies that aim to alleviate peoples' negative emotional response from agent or robot breakdowns, such as showing politeness and apologetic behaviors. However, in taskoriented settings, such as a chatbot performing information assistance, these strategies may be ineffective if the user still fails to accomplish the task. In this paper, we focus on strategies that support repair -recovering from the breakdown and accomplishing the task goal.Repair is a ubiquitous phenomenon in human communication. When a breakdown happens in a conversation, people take a variety of actions such as repeating, rephrasing, or clarifying, to repair it. Although chatbot users should be skillful in using similar actions as the speaker, the repair task becomes challenging as the listener is no longer a fellow human. Two problems often impede the repair process with chatbots: 1) there may be a lack of evidence that a breakdown has occurred, which may either be a limitation of the underlying technology (i.e., unable to recognize a breakdown) or a failure in design to communicate the breakdown; 2) the system's model is unfamiliar for the user to choose an effective way to repair. When talking to another person, repairs are almost subconscious acts, which may include a combination of speech, gesture, and facial expression [6]. Chatbots rely on machine learning algorithms to process a user's input, which are \"black boxes\" for the user. Though these interfaces are deemed \"conversational,\" they may not be repaired in the same way as talking to another person [32].In this work, we study repair strategies that a chatbot (listener) could adopt to tackle the above problems -providing evidence for the breakdown and supporting repair towards a desirable direction for the system model. We note that many commercial chatbot products are already adopting repair designs to serve these goals. One example is to ask for confirmation when the system has low confidence, which gives a clear signal of a potential breakdown and allows the user to initiate repair without the system mistakenly executing a task. Another example is to provide options of tasks that the chatbot can handle based on their proximity to the user's input, which not only indicates that a breakdown occurred, but also drives the interaction to the scope of the system model's capabilities.This paper makes two contributions. First, we identify a set of repair strategies, informed by communication theories and prior work on conversational agents. In addition, we introduce a group of novel repair strategies that aim to expose the system model, as inspired by recent work in explainable machine learning [35,41,46]. These strategies explain why a breakdown occurred, such as showing which keywords the system was able/unable to understand, in order to assist a user in effective self-repair. These strategies contrast with system-repair strategies such as directly providing options. Second, we conducted a scenario-based study with Mechanical Turk workers (N=203) to systematically understand people's preferences for different repair strategies. Our study focuses on text-based chatbots, which are widely used and growing in popularity [21], although some of the repair strategies we examined can be applied to voice-based agents as well.", "relwork": "Our study is informed by communication theories relevant to conversational breakdown and repair, prior work on repair in human-agent interaction, as well as transparency and explanation of machine learning systems.", "rq": "The results of this study are promising in delineating the best repair strategies for human-agent repairs. However, we acknowledge some limitations. First, for a lack of statistical significance, we could not make strong conclusions for how some of the lesser-ranked repairs fare against each other (Top, Repeat, Confirmation, Defer) given their larger p-values. However, by answering research questions guided by the theoretical framework, we believe that we paint an accurate high-level picture of preferred repairs in humanagent breakdowns. Second, limited by using a scenario-based experimental study, our work could not account for how user preferences for repair strategies are affected by nuances in system performance, such as confidence level and performance of the explanation methods. Future work should explore these questions with a real chatbot system. The study was limited by the fact that we only tested scenarios with a one-turn request-response task. Future studies can benefit from evaluating different kinds of user tasks, such as multi-turn conversations. Our study is also limited by our sample of Mechanical Turk workers. Due to the linguistic nature of our task, we desired to have fluent English speakers participate. However, our final sample was biased toward college educated males. Future work is needed to understand how repair strategy preferences differ across languages and cultures, which may have different expectations or norms for how humans ought to interact with conversational agents. ."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/48.xml", "intro": "A survey published in January 2018 found that 81 percent of women and 43 percent of men experienced sexual harassment [7]. However, the U.S. Equal Employment Opportunity Commission reported that 75 percent of sexual harassment and 64 percent of sexual assault cases are not reported [5,21]. Although it can be helpful for victims to receive support from others by disclosing their problems [17], they still hesitate to get counseling or share their story because it can cause unwanted situations [18]. However, keeping their problem alone and an absence of supporters (e.g., confidant) can lead to depression, suicide and maladaptation to job [17,18].Conversational agents (CAs) have gained much attention from researchers and practitioners hoping that they might be able to give victims information of follow-up action and emotional support. Especially, the design and evaluation of CAs intended to support recovery in relation to chronic health conditions has become a significant strand of HCI research [11]. Recent studies have shown that use of counseling chatbots such as 'Woebot' and 'Wysa' have lowered users' depression and stress level [3,6].However, despite the optimistic view of CAs, little research has focused on what burdens users would have when disclosing a sensitive story to a machine. In order to receive help either from humans or CAs, it is necessary for victims to tell their case without hesitation. Identifying victims' burdens is important because by doing so, we can design better CAs after considering how to alleviate the burdens.In this paper, we aim to identify what burden victims feel when deciding to disclose their case to the police and CAs. Three research questions are as follows:What type of burden do victims have when reporting their case to the police and CAs? Do the degrees of burdens differ depending on whom they are talking to (Police vs. CAs)?", "relwork": "Previous studies have shown a possibility that machines could draw out more sensitive information than humans could do. A recent study by [14] found that users trust machine agents over human agents when it comes to credit card information. Kim et al. [8] showed that participants were more comfortable revealing secrets to a CA due to machines inherently having no feelings. Also, CAs' expression of empathy has been observed to increase likeability and feeling supported when patients tell CAs they suffer from sexually transmitted disease [9].", "rq": "In this paper, we aim to identify what burden victims feel when deciding to disclose their case to the police and CAs. Three research questions are as follows:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/216.xml", "intro": "Many national organizations recognize the essential role of early literacy in a child's later educational and life opportunities [28], [29], [30]. \"The richness of nouns, modifiers, and past-tense verbs in their parents' utterances, their parents' high propensity to ask yes/no questions, especially auxiliary-fronted yes/no questions; and their parents' low propensity to initiate and use imperatives and prohibitions were more strongly predictive of the children's performance on the Stanford-Binet IQ test battery.\" Hart and Risley [18] note that early literacy is an enormous challenge and will require lengthy and regular language experiences for the child. The greatest impact on child literacy will arguably come from intervention at pre-school ages [18]. Moreover, a growing body of research is also suggesting that pre-school children are voracious inquisitors. One recent study found that preschoolers ask approximately 80 questions/hour [8]. These questions are an essential part of language development: they provide primary experience with question construction, statement construction, explanation construction, complex tenses etc. At the preschool level, most questions are fact-based, e.g. \"do fish fly?\" although around age 3 there is a sharp increase in explanation-oriented questioning. Fact-based questions are readily answered by short statement responses. Children may ask follow-up questions, but in general the chain of conversation is short. Explanation-oriented questions seek richer answers with causal links or chains [13]. They will often be met with additional requests for more information, or a universal \"why.\" Children have strong preferences for the form of the answer (a causal explanation vs. fact-based answer), although less so for content. Question-asking, not surprisingly, goes beyond literacy and is an integral part of children's cognitive development [7]. So attention to this activity may have benefits in cognitive development as well.Children evidently need some form of linguistic engagement for many hours a week, with a language-able partner who can engage with them in age-appropriate language-learning activities. Since research in early child development suggests that for pre-school children question-answering serves as a frequent and heavily-utilized medium, this linguistic engagement can come in the form of interactive questionanswering systems. Since children spend a significant amount of time playing alone, or out of home, there might be instances when they don't find an adult around to answer their questions. There might also be times, when the adult doesn't have sufficient information at hand to answer a child's question. This explains the need for expert interactive systems that can work as engaging question-answering agents.rendered over a wide area using a projection system, will engage children in language games that use questionanswering as the primary dialogue structure. For example, the character will show the child several objects, then hide one and ask the child to guess what it is by asking questions about it. The game engages children in language use, and also in concrete questions about things in the world and their properties. The envisaged solution is shown in Figure 1. In this paper we describe a two-phase study, one phase using a human language partner, and the second using a system which approximates figure 1. Rather than relying on speech recognition and dialog interpretation, we used a Wizard-Of-Oz system. The goal of the studies was to explore the feasibility of the envisaged solution: whether students would ask \"on-topic\" questions, whether the questions matched some templates, and whether they would be engaged by the game. Phase 1 involved 12 children studying at the same preschool, playing a 20-questions game with a familiar researcher. It contributed to answering the following research questions:1. Are children's questions predictable and deterministic, when grounded in an activity like 20 questions?2. Is the repair required in such a dialogue limited and feasible?3. Is it possible to effectively \"nudge\" preschoolers to solve problems without disengaging?Phase 2 involved the same participants as phase 1. Half of them played the game with the same researcher. The other half played the same with (wizard-of-Oz) Spot, an agent that we designed and implemented. Effectively, 6 children played just with the familiar human in both the phases and the remaining 6 first played with the human and then the agent. In phase 3, all the participants played with (non-wizard, fully automated) Spot. Phases 2 and 3 built on phase 1, and answered the following research question:1. Using commonly used parenting styles in dialogues, how can we design an agent that can engage preschoolers in a familiar question-answering activity as effectively as a familiar human?Effectiveness in this case is primarily restricted to questionanswering efficiency, flow of communication and affect/engagement. Moreover, literature suggests that vocabulary accrues from language use [18]. It is not feasible to measure vocabulary richness, especially in constrained activities like question answering. Therefore, in this round of research priority was given to the volume of language use. We see the following contributions for this work:\uf0b7 Testing the feasibility of question-asking/answering behavior in preschoolers, and if technology can be designed to support such behavior. \uf0b7 Investigation of engaging preschoolers in context of language usage, using technology. \uf0b7 Using common and effective parenting styles to design and develop a system for preschoolers.", "relwork": "With the growth of conversational technologies, the possibilities for integrating conversation and discourse in elearning are receiving greater attention in both research and commercial settings. Conversational agents have been produced to meet a wide range of applications, including tutoring (e.g. [16], [19], [2]), question-answering (e.g. [10], [12], [34]), conversation practice for language learners, (e.g. [14], [1]), pedagogical agents and learning companions (e.g. [23], [33], [4], [11]), and dialogues to promote reflection and metacognitive skills (e.g. [17], [20]). Conversational agents build on traditional education systems, providing a natural and practical interface for the learner. They are capable of offering support for each individual, and recognizing and building upon the strengths, interests and abilities of individuals in order to create engaged and independent learners. However, the current interactive conversational tutors are geared more towards older children, who have a larger set of knowledge or skills than pre-school children and are easier to understand, and also focus on specific skills or domains.The key difficulty in developing an agent for such a younger audience is maintaining children in their ZPD (Zone of Proximal Developent) [37]. The project CACHET examines the responses young children have to interactive conversational agents using electronic stuffed toys [25]. These toys are designed to speak, respond to touch via sensors, gesture with motors, and be linked to a PC wirelessly to provide support and feedback while a child plays games encouraging number and language learning [25].Children were able to skillfully navigate through the games, however, and were adept at asking for help when they were aware of and were not irritated with the toy [25]. This technological adroitness suggests high potential for interactive agents for younger children.", "rq": "Phase 2 involved the same participants as phase 1. Half of them played the game with the same researcher. The other half played the same with (wizard-of-Oz) Spot, an agent that we designed and implemented. Effectively, 6 children played just with the familiar human in both the phases and the remaining 6 first played with the human and then the agent. In phase 3, all the participants played with (non-wizard, fully automated) Spot. Phases 2 and 3 built on phase 1, and answered the following research question:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/164.xml", "intro": "Persuasive technologies developed to date have, for the most part, implemented persuasive techniques inspired by human-human interaction [16]. Rather than investigating a single persuasive technique, we are interested in modeling all the communicative activities that take place during a session with a human health counselor who is trained to change the health behavior and attitudes of her clients. Since physical activity is a behavior with many well-known long term health benefits [33], we focus on the promotion of attitudes that are known to be predictive of changes in exercise behavior [29].In the current study, we investigate the efficacy of using an embodied conversational agent to simulate this face-to-face counseling with as much fidelity as possible, and compare this to a more conventional interface which delivers the same intervention via menus and text messages. We are also interested in the efficacy of the social affordances of embodied conversational agents for building trust and rapport with their users. An effective counselor will attempt to form a strong working relationship with the client, and this counselor-client relationship is predictive of outcomes [22]. Accordingly, we also investigate the use of social dialogue as a relationship-building tactic, and examine how this may impact the ability of the agent to change user attitudes compared to an agent that does not use social dialogue.", "relwork": "The persuasiveness of a message may be affected by the interpersonal relationship between speaker and listener. A message is more persuasive if it comes from a source with whom the listener perceives a greater affiliation [12]. A relationship need not be extensive or long-lasting to cause a significant effect. Burger et. al. showed that participants were more likely to comply with a request from a confederate they had interacted with previously -even if the previous interaction consisted solely of sitting quietly in a room together for a short period of time [7].Social dialogue may be an effective tactic for eliciting compliance with a request. Howard [18] showed that asking someone how they were feeling, and acknowledging the response, led to greater compliance with a charitable request. Similarly, Dolinski et. al. showed that a person approached with a charitable request was more likely to comply if the requester first engaged in casual social dialogue; they argue that the social dialogue provides situational cues which the listener associates with a relationship [15]. However, relational cues may not be persuasive for all kinds of decisions. The Elaboration Likelihood Model (ELM) of persuasion theorizes that people react differently to persuasive messages based on the personal relevance of the decisions involved. Decisions of high personal relevance are processed by the ``central route'', while decisions of lower relevance are processed by the ``peripheral route''. When the peripheral route is used, then source characteristics, such as the perceived trustworthiness of the source of a persuasive message, have a greater influence on the decision-making process [28].Within the domain of exercise, Jones et. al. [19] examined the effects of source credibility and message framing on the persuasiveness of messages that promoted physical activity. Participants reported more positive exercise intentions and behaviors when messages were attributed to a credible source and were positively framed -focusing on the benefits of exercise rather than the costs of not exercising.", "rq": "\u2022 \"I know just from experience you have a better feel, a better self-esteem.\" The remaining research questions addressed the effect of the manipulated factors on subjective perceptions of the argument, message, and agent. We expected both the use of an ECA (hypothesis 2b) and the use of social dialogue (hypothesis 3b) to lead to more positive perceptions; the results of the study indicate partial and qualified support for these hypotheses. Social dialogue increased positive perceptions of the argument and message, but only when used by an ECA; social dialogue delivered by a textonly agent appears to be counterproductive."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/213.xml", "intro": "", "relwork": "According to Gordon Pask, social systems are formed through a shared interpretation of language [10]. To apply it in the context of chatbots, social systems can be formed with chatbots if it can engage a human in interpersonal conversation -recognize and respond to user input, understand conversational attributes like turn-taking, and contribute to new conversations [4]. Intelligent chatbots, which could learn from conversations, are developed with a personality in mind [13]. Attributes like age, gender, emotional response and personality traits assigned to a bot play a critical role in both how they interact with the user as well as how users respond to them [16,17]. People often treated chatbots as fellow humans, and develop a personal connection with them [18]. ELIZA, a bot developed by Weizenbaum at Massachusetts Institute of Technology was one such case [19]. Having a personality profile further brings users developer a deeper bond with their virtual assistants, discuss \" ...sensitive content that is too embarrassing to ask another human\" [21], and volunteer a lot of content without inhibitions. This creates problems in protecting information disclosure. Implications for privacy in interpersonal communication are already widely discussed in the research community [15].", "rq": "Opinions: One of our key research questions was to understand if users would behave differently while conversing with a chatbot, compared to what they would with another person in a usual social setting. It is socially uncommon for people to express such strong personal preferences in person, especially to a person they have just met. However, it appeared that users were more comfortable venting their political, and religious beliefs to a bot. This led to the creation of an 'informational wasteland' where people used a chatbot to express their beliefs which might otherwise be disruptive in a social setting."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/64.xml", "intro": "Collaboration between people and conversational artificial intelligence (AI) agents-AI systems that communicate through natural language [30]-is now prevalent. As a result, there is increasing interest in designing these agents and studying how users interact with them [1,14,23,30,49,66]. While the technical underpinnings of these systems continue to improve, we still lack fundamental understanding of the mechanisms that influence our experience of them. What mechanisms cause some conversational AI agents to succeed at their goals, while others are discarded? Why would Xiaoice [68] amass millions of monthly users, while the same techniques powering Tay [35] led to the agent being discontinued for eliciting anti-social troll interactions? Many AI agents have received polarized receptions despite offering very similar functionality: for example, Woebot [55] and Replika [43] continue to evoke positive user behavior, while Mitsuku [79] is often subjected to dehumanization. Even with millions of similar AI systems available online [11,46], only a handful are not abandoned [30,83]. The emergence of social robots and human-AI collaborations has driven home a need to understand the mechanisms that inform users' evaluations of such systems.In HCI, experiences of a system are typically understood as being mediated by a person's mental model of that system [54]. Conveying an effective understanding of the system's behavior can enable users to build mental models that increase their desire to cooperate with the system [5,10,37,41]. However, a mental model explanation is insufficient to answer the present question: in the case of Xiaoice and Tay, both agents were based on the same underlying technology from Microsoft, but they resulted in very different reactions by users. Likewise, other agents such as Replika and Mitsuku elicit very different evaluations while existing even within the same cultural context. While theories of mental models and culture each help us understand how users experience conversational AI agents, we require additional theoretical scaffolding to understand the phenomenon.An important and unexamined difference between these otherwise similar agents are the different metaphors that they project. Conceptual metaphors are short descriptions attached to a system that are suggestive of its functionality and intentions [15,50]. For instance, Microsoft described Tay as an \"AI that's got no chill\" [62], while it markets Xiaoice as an \"empathetic ear\"-two very different metaphors. Metaphors are a central mechanism in the designer's toolkit. Unlike mental models, they offer more than just functional understandings of the system-they shape users' expectations from the system. And while most existing expectation-shaping mechanisms depend on the functionality of the specific AI system or task [41], metaphors are agnostic to specificities of a system and can be used to shape expectations for nearly any AI system. Prior theory suggests that pre-use expectations of AI systems influence both initial behaviors [31,40,76] and long-term behaviors [42], even if the system itself remains unchanged while varying user expectations [58].We propose that these metaphors are a powerful mechanism to shape expectations and mediate experiences of AI systems. If, for example, the metaphor primes people to expect an AI that is highly competent and capable of understanding complex commands, they will evaluate the same interaction with the system differently than if users expect their AI to be less competent and only comprehend simple commands (Figure 1). Similarly, if users expect a warm, welcoming experience, they will evaluate an AI agent differently than if they expect a colder, professional experienceeven if the interaction with the agent is identical in both cases.In this paper, we test the effect of metaphors on evaluations of AI agents. We draw on the Stereotype Content Model (SCM) from psychology [16,24], which demonstrates that the two dimensions of warmth and competence are the principal axes of human social perception. Judgements along these dimensions provoke systematic cognitive, emotional, and behavioral reactions [16]. The SCM suggests that user expectations and therefore evaluations, are mediated by judgements of warmth and competence. We crowdsource the labeling of a set of metaphors along these axes to identify a set of metaphors that appear in different quadrants of the SCM -e.g., a toddler, who is high warmth and low competence, and a shrewd executive, who is low warmth and high competence.We perform an experiment (\ud835\udc41 = 260) that manipulates the metaphor associated with an AI agent and measures how it invokes expectations of competence and warmth and how those two dimensions affect ratings of usability, intention to adopt, and desire to cooperate. We draw on an established method from prior experiments [12,34,71,74] to instantiate the agent itself as a remote Wizard-of-Oz who is blind to the condition and randomized across conditions for each participant. Participants are first exposed to the agent's metaphor, then converse with the agent to complete a travel planning task [4].Our results suggest that, contrary to how designers typically describe their AI agents, low competence metaphors lead to increases in perceived usability, intention to adopt, and desire to cooperate relative to high competence metaphors. These results persist despite both the low competence and high competence agents operating at full human-level performance levels via a wizard, suggesting that no matter how competent the agent actually is, people will view it negatively if it projects a high level of competence. Participants perceive the wizards to possess lower competence than the expectations implied by high competence metaphors. These results align with Contrast Theory [67], which states that users' evaluations are defined by the difference between their experiences and expectations. Finally, we find that the warmth axis operates conversely to competence: users viewed the AI with higher warmth more positively, interacted with it longer, and were more willing to cooperate with it. This result aligns with Assimilation Theory [67]: users recolor warmth experiences in light of their initial expectation.Previous work has sought explanations for user behavior and evaluations of AI by profiling users [5,18] or by making the AI more interpretable [9,44,60]. However, these approaches fail to explain why otherwise functionally similar systems elicited vastly different user responses. Our analysis suggests that designers should carefully analyze the effects of metaphors that they associate with the AI systems they create, especially whether they are communicating expectations of high competence. In discussion, we consider implications for design by retrospectively analyzing the metaphors used to describe existing and past AI agents, such as Xiaoice, Tay, and Mitzuku, and show that our results are consistent with the adoption and user cooperation with these products. The connection between our conclusions and the outcomes experienced by Xiaoice and Tay cannot explain the whole story; however, the pattern is striking and motivates the need for exploration of mechanisms to shape expectations and elicit prosocial user behavior.We begin by laying out related work, deriving our research question and hypotheses from prior theories. We then describe our procedure for sampling metaphors. In Study 1, we study the effects of metaphor warmth and competence. In Study 2, we sample additional metaphors along the competence axis in order to understand the effects of competence at a more fine-grained level. In Study 3, we test the negative effects of portraying a low competence metaphor by studying the effect that warmth and competence have on participants' interest in using the system in the first place. Finally, we discuss the implications of our findings for the choice of metaphors when designers deal with the dual objective of attracting more users and ensuring a positive user experience.", "relwork": "Pre-use expectations play a critical role in users' initial usage of a system or design [32,40,76]. Setting positive or negative expectations colors users' evaluation of what would otherwise be identical experiences [58]. The effects of these pre-use expectations can have effects on evaluations even after weeks of interaction with a service [42].In the case of AI systems, which are often data-driven and probabilistic, there exists no simple method of setting user expectations. Providing users with performance metrics does not establish an accurate expectation for how the system behaves [41]. In the absence of effective mental models of AI systems, users instead develop folk theories -intuitive, informal theories -as expansive guiding beliefs about the system and its goals [26,29,45,64].Prior work has shown how subjective evaluations of interface agents are strongly influenced by the face, voice, and other design aspects of the agent [53,82], beyond just the actual capabilities of the agent. These results motivate our study of how metaphors set expectations that affect how users view and interact with conversational AI systems. Inaccurate expectations can be consequential. Previously, interviews have established that expectations from conversational agents such as Siri, Google Assistant, and Alexa are out of sync with the actual capabilities and performance of the systems [49,83]. So, after repeatedly hitting the agent's capability limits, users retreat to using the agents only for menial, low-level tasks [49]. While these prior interview-based studies have demonstrated that a mismatch between user expectations and system operation are detrimental to user experiences [49], they haven't been able to establish causality and quantify the magnitude of this effect. This gap motivates our inquiry into understanding mechanisms that might shape these expectations and measuring the effect of expectations on user experiences and attitudes. We are guided by the following research question:Research Question: How do metaphors impact evaluations of interactions with conversational AI systems?", "rq": "Research Question: How do metaphors impact evaluations of interactions with conversational AI systems?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/165.xml", "intro": "", "relwork": "Text-to-Speech (TTS) automatically converts text into synthetic speech [46] and has diverse applications such as assistive communication [50], screen readers [17], or spoken interfaces for products like Apple Siri [6]. A TTS system must generate synthetic speech for any input text. It should usually sound as similar as possible to natural speech ('natural'), and be as comprehensible as possible ('intelligible'): cf. [46,Chapter 17.2].", "rq": "Our study provided us with qualitative and quantitative feedback to answer our research questions: RQ1: What is the difference in perception between a persuasive synthetic voice and an expressive but non-persuasive synthetic voice? and RQ2: Do users more often follow the recommendations of the persuasive voice, in an interactive goal-oriented scenario? We found that synthetic speech generated from models trained on a persuasive speech dataset was generally perceived as more persuasive than the baseline (RQ1). This was particularly the case for speaker's Truthfulness and Involvement. However, this perception of persuasiveness did not translate into changes in user behaviour, at least in the goal-oriented task we employed (RQ2)."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/171.xml", "intro": "Conversation is an essential design component of a chatbot. As a conversational user interface (CUI), in a chatbot the user and the agent interact with a series of chat bubbles in a conversational manner. When designing conversations for chatbots, designers often employ an iterative design process: designing a conversation flow, testing with users, reviewing user data, and improving the design. Many designers use existing chatbot prototyping tools such as Landbot 1 , Botmock 2 , and Chatfuel 3 . Designers rely on visual aids like flow diagrams offered by the tools to create a conversation scenario and generate a working prototype. Normally, an interactive prototype is exported as a web link, which gets distributed to potential users for their testing and feedback. After the testing, designers analyze the collected data manually and revise their conversation design.Although iterative conversation design is possible with existing prototyping tools, we observed several challenges around the design process in our formative interview with conversation designers. When designers try to verify their design, it is difficult to recruit participants quickly whenever they want to in order to get feedback on the design. Even though the tools mentioned above support testing chatbots, for testers, there is no way to provide feedback on specific components (e.g., whether the sequence of the conversation is natural, whether a specific utterance is awkward, whether a branch is needed, whether additional topics should be included) due to limited ways to express detailed suggestions. This results in user feedback that is often abstract and not actionable, which in turn presents challenges to designers in making informed design decisions. There have been several methods around collecting granular feedback on design via crowdsourcing in domains such as UI [27] and poster design [23,32,35]. However, those approaches mainly support visual design tasks, which might not directly apply to conversational user interfaces. The design of CUI involves 'conversation' that mainly uses free-form responses whereas in GUI, the user interaction is gathered through button clicks, menu selections, etc. As follows, CUI designers cannot easily predict and limit the range of user interactions. Here, we try to collect granular feedback on the unique and specific domain of 'chatbot conversation design', and explore design considerations for getting granular feedback on conversational user interfaces.In this paper, we explore the idea of engaging an online crowd in the design process to support conversation design. First, we increase the availability of test participants by making it possible for designers to recruit crowd workers on demand within a chatbot design tool. Second, we guide the crowd to provide concrete and clear feedback on specific components during a testing session. Finally, we provide multiple types of interactive visualizations to help designers effectively interpret the collected data and make design revisions.To investigate the feasibility of the three directions we suggest, we introduce ProtoChat, a crowdpowered system built to support the iterative process of conversation design. Designers can create a conversation flow with branching to support conditional flows. After crowd-testing, designers can review and inspect crowd data with interactive visualizations, such as an overview of conversation flows and an utterance-level review of crowd conversations. As a tester, a crowd worker can perform three kinds of tasks within the crowd-testing interface-conversing with the chatbot to follow the conversation flow, adding an appropriate utterance on the chatbot's side, and adding a branch in the conversation.To evaluate how crowd workers and designers use ProtoChat in a conversation design scenario, we conducted a three-day study with eight designers. They went through a design iteration each day and performed four main design tasks (Design, Crowd-test, Review, and Interview) with ProtoChat.Participants chose different domains for their conversation design, which varied from ice cream order to YouTube channel recommendation to talking behind significant other's back. Each day, we recruited a new batch of crowd workers whose number was determined by the designer. We found that ProtoChat could provide an agile design experience to create, test, analyze, and improve the conversation. Designers were able to improve their design with evidence collected from the crowd, by modifying the overall structure of the conversation or fixing a specific part of the conversation. Designers also diversified the options provided to the user, modified the response format (e.g., natural input, button choice) of topics, or gathered insights of UI design implications for the final version of chatbot. Beyond the conversation itself, some designers set a persona (e.g., proactive, good listener) for the chatbot by editing chatbot utterances with crowd input as hints. The conversation design increased in complexity over time through iterations by 33% after the first iteration, and 11% after the second iteration.The contributions of this work include:\u2022 Insights from the formative interview that identify challenges in conversation design and the required support for a more agile iterative design process; \u2022 ProtoChat, an interactive chatbot design tool that supports designers to make informed decisions by collecting design feedback from crowd workers and visualizing the crowdsourced data; \u2022 Empirical findings from a user study that shows how our system could help designers to utilize the crowd feedback and provide the crowd workers the methods to suggest concrete feedback.", "relwork": "We review previous work related to conversation design and crowdsourcing applications. We first investigate what kind of methods are currently being used for conversation design of chatbots.Then, as we propose a system empowered by the crowd, we discuss how crowdsourcing is utilized in chatbot design and how the crowd is invited to work on usability testing in general.", "rq": "To answer the second research question (\"How does the designer utilize the crowd outcome into their design with our system?\"), we relied on qualitative responses from participants during the study. We analyzed both what participants said as they were thinking aloud during the design process, as well as what they said in the pre/post/final interviews. To analyze qualitative data from think-aloud sessions and interviews, we first scripted all the audio recordings into text and two researchers conducted a content analysis on it."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/173.xml", "intro": "A great deal of learning involves factual knowledge (e.g., numerous topics in medicine, language, and law). Further, such information is often learned outside of a formal classroom setting. Developing more effective automated methods for accelerating or improving factual learning therefore has the potential to benefit a multitude of students on a broad scale.Traditional electronic tools for practicing factual knowledge tend to be flashcard based [15,29,30]. Flashcard apps are simple and can be easily designed to provide personalized adaptive practice based on well-studied models of human memory [14,50,55]. However, to optimize for speed, flashcards typically involve passive learning (i.e., the user is asked to visualize the answer and then check for correctness). This may not fully take advantage of the testing effect (retrieval through testing with proper feedback) [48]. As shown in many previous studies, retrieval practices like testing lead to higher retention than purely studying via even multiple passive means of self-evaluation [8,34,47]. Feedback received from test results further improves retention [3,37].Moreover, flashcards are not typically designed to be engaging, making their effectiveness heavily dependent on people's desire to learn. Research confirms engagement can mediate learning effectiveness [7,27], especially for technologybased learning [26]. A more engaging way to learn factual knowledge could therefore lead to better learning outcomes.One possible path towards boosted engagement is using Natural Language Processing (NLP) powered chatbots, which are becoming increasingly sophisticated [21,52]. For example, such systems enable students to speak or type out their answers during a two-way dialogue and receive targeted feedback from NLP techniques interpreting the spoken or written words. This new interaction for learning factual knowledge may be much more motivating and engaging, and may also be more effective at providing adaptive feedback and promoting deeper learning [11].Given this potential for conversational approaches to enhance learning, we designed and built QuizBot, a dialoguebased adaptive learning system for students to learn and memorize factual knowledge in science, safety, and advanced English vocabulary. These three subjects were chosen because they cover diverse topics in medicine, language, and rules. They can represent important subclasses of factual knowledge that are usually learned outside the classroom setting.On the technical side, QuizBot leverages the supervised Smooth Inverse Frequency (SIF) algorithm [2] for automatic answer grading and the DASH model [39] for adaptive question sequencing. On the design side, we created Frosty, an encouraging tutoring agent that provides targeted feedback to learners based on their inputs (see Figure 2). The design of QuizBot was inspired by previous studies [9,13,20] to leverage the persona effect, the strong positive impact of animated agents on learning experience [38].To determine the impact of QuizBot on learning, we evaluated it against a carefully designed flashcard app, the typical medium for learning factual knowledge, through two controlled within-subject studies. We aimed to closely match the flashcard app to QuizBot in order to target assessment at the impacts of the conversational components. Specifically, the flashcard app used the same DASH algorithm for adaptive question selection, and a single pool of questions and answers was subdivided for the flashcard app and QuizBot.In the first within-subject study with 40 students, when the number of practice items was held constant for both flashcards and QuizBot, students scored substantially better on recall (fill-in-the-blank) and recognition (multiple-choice) with QuizBot than for items trained using flashcards (66.3% vs. 45.2% for recall and 87.2% vs. 65.8% for recognition). However, the time taken was longer with QuizBot than flashcards. In the second within-subject study with 36 students, we allowed learners to voluntarily allocate their time between the two apps. We found students spent 2.6x more time on QuizBot, and that students performed equivalently on recognizing items but significantly better with QuizBot at recall (with an effect size of .45). These results suggest that QuizBot is more engaging to use and more effective at recall and equally effective at recognition in typical user-driven scenarios. In normal use, QuizBot may be less efficient per unit time, but still yields improved learning on recall due to users voluntarily choosing to use it substantially more.This work has three chief contributions. First, QuizBot is the first chat-based learning system for factual knowledge memorization outside of classroom settings. Moreover, we show its effectiveness and engagement through rigorous comparison studies with a traditional learning tool for knowledge memorization, and our results demonstrate benefits of using chatbots to learn factual knowledge, especially for casual learning. Lastly, our results also reveal inefficiencies of chat-based learning systems, and we offer design suggestions for building improved future educational chatbot systems.", "relwork": "Our work was built upon previous studies on natural language tutoring systems, semantic similarity algorithms, and memory models.", "rq": "In this section, we discuss and answer our research questions based on evaluation results, and we offer suggestions for designing further improved chat-based learning systems."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/72.xml", "intro": "Recommender systems have become critically important for helping users quickly find ideal items among a large number of products [25]. However, personalized recommendations may lead users to increasingly narrower space of items over time (called \"filterbubble\" effects) [20,30]. To mitigate this issue, several attempts have been made to encourage users to explore diverse sets of items, such as diversity-driven algorithms [41,42] and visualizing recommendations [16,32]. On the other hand, dialogue-based conversational recommender systems enable users to freely give feedback on recommendations through natural language [3,11,12], which show considerable potential for promoting users' exploratory activities. However, so far little work has studied supporting user exploration through conversational interaction.Recently, there is a work [12] that studied a chatbot for accommodating user control in music recommendations with two critiquing techniques (i.e., user-initiated critiquing (UC) and system-suggested critiquing (SC) [7]). The user study of this work reveals that users tend to feel receiving more diverse recommendations when using the system with both UC and SC. Inspired by this observation, we consider to stimulate users' exploration of recommendations by strengthening the critiquing technique in conversational interaction.Therefore, in the current work, we have designed two kinds of system-suggested critiquing technique: Progressive systemsuggested critiquing (Progressive SC) and cascading systemsuggested critiquing (Cascading SC) for facilitating users' exploration of music with two different directions: The former is preference-oriented, which provides critiques based on users' current preferences and incremental critiquing feedback [24], while the latter is diversity-oriented, which suggests critiques to steer users into a cascade of diverse types of music using a strategical approach with the assumption of the cascading user behavior as inspired by [19]. Then, we have developed a music chatbot with three system variants, which are respectively featured with UC (i.e., users can make critiques on the recommended songs to explore songs they want), Progressive SC and Cascading SC. To investigate how these critiquing techniques influence users' music exploration with conversational interaction, we conducted a between-subject user study (involving 107 participants) to compare the three system variants in terms of both user perception of and user interaction with recommendations. We also examined how these critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations.In a short summary, we have mainly focused on answering two research questions as follows (see Figure 1): RQ1: How do critiquing techniques influence users' exploration of music in a conversational recommender?RQ2: How do critiquing techniques moderate the relationship between user interaction behavior and user perception of music recommendations?Our main contributions of this work are four-fold:(1) We have proposed two kinds of system-suggested critiquing technique, in order to encourage users' exploration of music recommendations, and compared three variants of the system supported with different critiquing methods (i.e., UC, Progressive SC, and Cascading SC) in terms of users' perception of and interaction with recommendations. The experimental results show that users perceive higher diversity of recommendations with the system that offers Cascading SC and feel more serendipitous recommendations with the system that offers Progressive SC.(2) We have investigated the moderation effects of critiquing techniques, and find that the critiquing techniques significantly moderate some relationships between interaction metrics (such as number of listened songs and number of dialogue turns) and user perception metrics (such as perceived helpfulness and serendipity). (3) We have analyzed users' interaction flow towards UC and SC, and find that users tend to use UC when they have gradually established their new preferences during the interaction with conversational recommendations, while users may be stimulated to request SC when they have benefited from the SC proactively offered by the system. (4) We have discussed our findings and provided practical implications for designing a critiquing-based conversational recommender system for supporting users' music exploration.", "relwork": "Prior work has shown various strategies to support user exploration by diversity-driven algorithms or visualizing recommendations. Diversity-driven algorithms typically generate recommendations that maintain the balance between accuracy and diversity [9,18,[40][41][42]. For instance, some researchers proposed to increase the recommendation diversity based on items' attributes, such as book topics [42], movie genres, and social tags [33]. In [30], the authors proposed a way to help users take a gradual path towards the desired new music preference by traversing user preference graphs and generating a sequence of artists as guided transition. Most of the related studies have attempted to increase the diversity for a ranked list, but there are some limitations [6,29], such that users tend to pay less attention to the bottom of the list when exploring recommendations, which is a position bias. Besides, some works have visualized recommendations to support user exploration in recommender systems. For example, to raise users' awareness of exploration, [16,32] highlighted the regions of the underrepresented recommendation space, so-called blind spots, which could help users to identify what is known and what is unknown in their profile. Some visualization systems, such as TalkExplorer, [34], and Moodplay [1], allow users to explore diverse items during the recommendation process. In [26,27], the authors introduced a shortlist as a short-term memory to reduce users' cognitive efforts and help users make better decisions when exploring diverse movies. The recommender systems discussed so far support user exploration by presenting diverse recommendations or visualizations. To the best of our knowledge, little work has been done to support user exploration with conversational interaction.", "rq": "Figure 1: Our research questions."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/73.xml", "intro": "Learning by teaching is a popular and well studied pedagogical technique. Research has shown that this technique produces the prot\u00e9g\u00e9 effect [7]-students who are asked to teach others learn through the generation of explanations and questions, which requires a form of knowledge building that is particularly conducive to learning-students synthesize and structure materials, become aware of their own learning process, and expend more effort to learn.Despite extensive research, our understanding of the exact conditions that make learning by teaching effective is limited, mostly because there can be high variability in the tutor and tutee behaviour. One recommendation, put forth by Roscoe in a large survey on tutor learning [19], is to \"develop teachable agents to test hypotheses about specific tutor behaviours\" by systematically manipulating the teachable agent's question asking behavior (e.g., shallow vs deep questions), accuracy (e.g., few vs frequent mistakes), and level of prior knowledge (e.g., high vs low).In this work, we introduce a learning by teaching web application called the Curiosity Notebook that supports learning by teaching through dialogue. We conducted a 4-week exploratory study with 12 fourth and fifth grade elementary school students teaching a humanoid robot to classify objects, to understand how to design a platform that can support research on learning by teaching. In summary, our work contributes:\u2022 a configurable learning by teaching platform that enables students to teach a virtual, voice-only, or physical robot agent, individually or in groups, and on different topics, \u2022 an outline of design goals that are important for the development of group-based learning by teaching platforms, \u2022 insights from an exploratory study on potential refinements of the design goals.", "relwork": "In education research, learning by teaching is closely related or synonymous to other terms, such as peer tutoring (PT), cooperative learning (CL), and peer-assisted learning (PAL). It is hypothesized that many of the activities demanded by teaching-e.g., explaining [23], questioning [9], assessment and feedback [15]-require reflective knowledge building, where students synthesize, structure and reflect [19]. Roscoe and Chi [19] proposed that teachable agents can serve as an infrastructure for testing different hypotheses about tutor behaviour. In computermediated learning applications, agents have mostly served as peers [20,14] or tutors [12,17], with only a handful of systems positioning the agent as a less knowledgeable peer that students teach [2,4]. SimStudent [16] is a simulated learner used to study student-tutor learning in mathematics problem solving. In Betty's Brain [3,2], students read articles, then teach and quiz a virtual agent (i.e., Betty) about causal relationships (e.g., burning fossil fuels increases CO 2 ) in science by manipulating concept maps.Other teachable agent research involves physical robots. In Tanaka and Matsuzoe [22], young children (age 3-6) taught a humanoid robot English words, while simultaneously interacting with a human teacher. Later, they [21] also investigated how preschool children learn English by teaching Pepper, an adult-size humanoid robot, while receiving guidance from a human teacher demonstrating vocabularyrelated gestures on a small screen. Yadollahi et al. [24] developed a collaborative story reading environment, where children (aged 6-7) can correct the robot's mistakes as it reads aloud. In other works [13,6], children-working individually, in pairs and in groups-corrected a robot's handwriting. Finally, Chaffey et al. [5] had older students (mean age=20) teach a robot to solve math problems, and explored how dyadic stance affects student attitudes. ", "rq": "Our goal is to create a platform that facilitates student learning through teaching a conversational agent. Several design goals guided our development-To facilitate testing of hypotheses around learning by teaching, the platform should enable systematic modulation of the tutee characteristics hypothesized to be relevant to learning (e.g., types of question asked, accuracy), and provide students with choices of teaching tasks in order to allow for a quantitative characterization of their teaching strategies. To be feasible in real-world learning environments, the platform should support students teaching individually, in pairs or in larger groups, while providing equal access to teaching opportunities. Finally, to produce generalizable findings, the platform should support teaching conversations with agents with different embodiments (e.g., virtual, voice or physical agents) and learning tasks that can scale in complexity to different age groups (e.g., usable by elementary school and college students). These platform features, many of which are beyond what is provided by existing learning-by-teaching systems, enable researchers to ask a wide range of research questions about conversational agents within the context of diverse collaborative learning scenarios."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/115.xml", "intro": "In recent years, conversational agents or chatbots have received great attention. Since 2016, major tech companies started providing open APIs for building chatbots. This resulted in a massive growth of chatbots especially the chatbots on text messaging platforms. Currently, more than 300,000 bots exist on Facebook messenger [49] cutting across a variety of use-cases-online shopping [44], casual chatting [5], travel arrangements [45], etc. So, far, the focus has been largely commercial, e.g. customer support, since the service providers see chatbots as a cost-e ective and e cient way to supplement human-based customer service. However, chatbots also o er opportunities for creating positive social impact [35]. With the ability to reach millions in a ordable ways, chatbots o er unique opportunities to empower those living in the constrained environments. Further, we discuss the ethical implications of introducing breastfeeding education chatbot in the study context and potential societal impact.Recently, HCI and CSCW communities have started exploring use of chatbots for variety of problem domains [26,102,116]. In India, the access to doctor is limited given the poor ratio of almost 4.8 doctors to 10,000 people [89]. To ll this gap, India uses a large network of community health workers for critical areas of public health such as maternal and child health. We want to explore the use of chatbots in assisting community health worker and mothers in the exclusive context of breastfeeding. Promoting exclusive breastfeeding is a global priority of maternal and child health programs because of its importance in child survival [86]. Timely initiation of breastfeeding and exclusive breastfeeding for the rst six months of child's life helps in preventing 20% newborn death and 13% under-ve deaths [55]. However, despite this knowledge, breastfeeding rates fall short of recommendation [51].Our users, community health workers and mothers residing in under-developed (slums) regions of India, have limited or no access to technological advancements in chatbot research, such as Alexa or Google Home, because of poor economic conditions and limited literacy. While they now possess smart-phone, it is often a low-end smart-phone (< $100) [42]. Moreover, the socio-economical conditions dictate that the phone may be shared within the family, often with the husband or other male members of the family [96]. This makes our users, the rst-time users of chatbots.These rst time users of chatbots, in their unique contexts, represent an important class to study and design for. So far, the research around chatbots has been majorly technology driven with limited investigation on user perceptions and contexts of use [88].In this study, we explore the feasibility of using chatbots for breastfeeding education of community health workers and mothers in urbal slum areas of India and understand how they react and perceive chatbot based intervention. We aim to contribute to the HCI and CSCW community by informing design recommendations for chatbot implementation for causes like breastfeeding education in under-developed areas, with users who are newcomers to personal device use. We guide our exploration with the following research questions:(1) What is the perception of users towards chatbot based breastfeeding assistance? (2) What kind of information users -CHWs and Mothers -seek from a chatbot? (3) What are the contexts of chatbot use for mothers and community health workers residing in slum areas of India?We study users' interaction with the chatbot through a Wizard-of-Oz experiment with 22 participants (12 ASHAs and 10 mothers). We prototyped our chatbot as an interactive questionanswering application and analyzed users' interaction patterns, perceptions, and contexts of use. Our ndings highlight the role of familial unit in breastfeeding practices. We found that the majority of questions sought by users are answerable by a chatbot and value of chatbots for mothers as a rst point of contact. This gives us the con dence that breastfeeding education is a potential application for chatbot intervention. We also discuss implications for the design for future bots and the characteristics they should embody to address users' concern in constrained settings.", "relwork": "Breastfeeding is considered to be the best way of providing optimal food to infants. Ideally, breastfeeding should be initiated within 1 hr of childbirth, practiced exclusively for the rst six months and then continued with the addition of adequate complementary foods up to two years of life [51]. Good breastfeeding practice is directly linked with improved survival rates of under-5 and child's health that includes optimal cognitive development and lower risk of obesity and diabetes later in life. Mothers get positive bene ts in terms of reduced risk of haemorrhage and breast cancer [51].According to the study [80] that examines trends in the young child feeding practices in India in the period of 2006-2016, breastfeeding practices have shown signi cant improvements: Early initiation of breastfeeding nearly doubled from 23% to 42%, and Exclusive breastfeeding increased from 46% to 55%. However, still, a major cohort of children are missing out and the current rates, at best can be described as modest [80]. Several factors play role in dis-continuation of breastfeeding.While it is known that barriers to breastfeeding occur at the social, cultural and political level that are outside of maternal control [24], there are di erences in contexts of these factors. Prevalence of traditional beliefs and wrong practices among women (both rural and urban areas) are a major deterrent to exclusive breastfeeding [12,13,57]. Many families commonly practice prelacteal feeding as a ritual and discard colostrum [71]. For example, in some of the Hindu communities, after birth, a child is welcomed through a ceremony in which a family member writes the word \"OM\" onto the tongue of the child with honey in the hope to wish good qualities to the infant. In some castes, breastfeeding is delayed until fth day due to their belief in ancient medical science. Perception of insu cient milk supply among mothers and caregivers is also a common barrier to exclusive breastfeeding. The study [12] found that nearly 33% women believed their milk supply would be low for their child and introduced animal milk and external food. Another challenge for new mothers is con icting advice from a health practitioner and a family member. Female relatives in the house have a strong in uence on breastfeeding practices.From the country's perspective, to support optimal infant and young child feeding practices (IYCF), various strategies have been established [91]. However, gaps remain at the implementation level. For instance, the Maternity Bene t Act 1961 was amended only recently in 2017 for extending the paid maternity leaves for working women from 12 weeks to 26 weeks [84]. Further, currently, the act is applicable to certain establishments e.g. government and other organized sectors and does not protect women working in unorganized sectors such as household sta , contractual labors etc. [91]. This directly hampers the women belonging to lower socio-economic strata which hold a greater percentage of the country's population [15]. To improve awareness of women on IYCF, there is an emphasis on providing counseling at health centers and through community outreach. ASHA workers, who play a crucial role in connecting to marginalized communities, are seen as an important vehicle of behavioral change and thereby also trained on IYCF to promote exclusive breastfeeding in their post-natal home visitations [74,99]. However, a range of challenges exists in translating the knowledge into actual practice. Due to over-burdened duties, high engagement in paperwork and sub-optimal training, ASHAs' awareness of being health educators has been found to be low [41,43]. Studies assessing ASHAs knowledge on breastfeeding practices report that though their knowledge is good, signi cant improvement is required on addressing prelacteal feeds, mothers perception on milk supply, and complementary food [99]. Further, there is a scarcity of studies to report on how mothers are supported beyond the six home visits of the ASHAs that is after 42 days of delivery.", "rq": "At the same time, an another aspect on usage of bots must be explored. And this happens to be in the area of trust and ethics. Could a bot be completely trusted and replace a human? Who pays for a mistake made by a bot by giving faulty or inappropriate advice. How do we check that a bot continues to provide authentic answers is an interesting research question and as designers, we need to provide solutions that can help navigate a user towards the correct approach e.g. veri cation by an expert or relying on the bot itself."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/28.xml", "intro": "In many research areas like human-computer interaction (HCI), interview is one of the key methods used to obtain information from target respondents. While traditional interviews by people or paper-and-pencil do a good job in gathering and mining information from target audience, they still present several limitations. First, they are difficult to scale due to their limitations in temporal and spatial accessibility. Second, interview fatigue [10] is challenging to combat due to the rigid question-answer formation in traditional interviews. Third, respondents may not feel comfortable disclosing sensitive information because of social desirability biases [6] when interacting with human interviewers. Moreover, traditional ways to conduct interviews require extra efforts to process the results, which might lead to higher chance of errors. To overcome these limitations, AI-powered chatbots embodied with interactive features, such as Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).  providing response feedback [2] and probing responses [7], have been proposed to conduct conversational interviews and proved to be effective in terms of elicitation [9,10]. Despite the promising future of applying chatbots to conduct interviews, how to design an interview chatbot that can elicit high response quality still remains unanswered.In addition to elicitation capabilities, ethics and safety issues are no less important. For example, privacy is a significant problem as interviews sometimes probe unexpected areas [1]. In practice, participants might feel privacy violation if the interview probes were designed too aggressive, which would greatly influence the further response quality and even break the trust bond in between. Considering this, we want to ask the following reserach questions:RQ1: How to design interview chatbots which can elicit highquality response data without breaking any ethical rules?RQ2: How can we efficiently evaluate interview chatbots in terms of their elicitation capabilities and ethical levels? However, very few research efforts have been put into the evaluation of such an interview chatbot's information elicitation capabilities and ethics level, let alone the design implications and assistive tools. In our study, we aim to develop an automatic evaluation framework, which can efficiently measure the quantitative effect of chatbots in terms of elicitation capabilities and ethical levels. Furthermore, a set of implications as well as assistive prototypes specifically for interview chatbots design will be proposed based on our findings.", "relwork": "Though recent research have revealed the effectiveness of interview chatbots in eliciting significantly better quality responses (compared to traditional online survey), the need of systematic set of evaluation metrics and more efficient evaluation process is still rising with the increasingly prevalent use of interview chatbots. In [10], researchers proposed a set of content-based metrics to measure response quality based on Gricean Maxims [3]; researchers in [9] and [4] have used behavior-based metrics such as interaction duration, click-through and sharing action to measure participants' engagement level. However, these metrics have not been wellorganized into a framework and some of them cannot be measured automatically but only manually, which would require tremendous efforts to annotate and analyze. Moreover, there have not been any in-depth studies examining the ethical aspects of interview chatbots to our knowledge, which makes our work necessary.", "rq": "RQ1: How to design interview chatbots which can elicit highquality response data without breaking any ethical rules?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/29.xml", "intro": "", "relwork": "Most of the related research on conversational style matching is based upon the work of Tannen, who described several markers that define conversational style [36]. These markers include: Topic (e.g., personal pronoun usage, persistence), Pace (e.g., speech rate, pauses), Expressive Paralinguistics (e.g., pitch and loudness shifts) and Genre (e.g., story framing). Based on the usage of these stylistic markers, people can be placed on an axis ranging from HC to HI.In more recent work, Shamekhi et al. looked at conversational style in human-agent interactions [33]. They examined whether there was a particular conversational style (HI or HC) that was preferred for a conversational agent in a structured conversation. However, rather than finding one specific style that worked best, they found that participants liked whichever agent matched their own conversational style.Thomas et al. looked at the conversational style of participants while performing a simple information seeking task [37,38]. This task was similarly structured to how a person might use an intelligent agent such as Cortana or Siri. They found that even in these tasks, participants aligned their conversational styles over time. It took less effort to complete tasks for participants who aligned their style compared to those who did not.Conversational style matching can be seen as a type of entrainment, as it relates to people synchronizing their behavior within an interaction [2,15]. Entrainment has been used in related work to generate more accurate and realistic reactions [20] or backchannels [18]. Levitan et al. implemented acoustic-prosodic entrainment in a conversational avatar and observed an increase in perceived reliability and likability of the system [22]. In a text-chat interface, Scissors et al. [31,32] found that lexical entrainment was associated with trust between partners. Specifically, pairs with high trust exhibited greater repetition of shared terms than did pairs with lower trust [32]. Subsequent work found that this was driven primarily by certain types of terms (e.g., those with greater positive emotional valence), and that not all similarity increased trust. The similarity in negative terms was associated with decreased trust [31]. We were inspired by this work and wanted to build upon it using an automated agent capable of voice-based interactions.", "rq": "In order to investigate these research questions, we constructed an intelligent agent that is capable of conducting open-ended conversations. This agent does not require any intervention of a human in order to converse with people, it responds to user input fully automatically. By using speech recognition and paralinguistic parameter recognition, we can apply conversational style matching in real time. We designed an experiment where participants spoke to the agent for about 15 minutes. Participants would either interact with an agent that applied conversational style matching or an agent that did not. Results from this study enabled us to formulate design guidelines for conversational agents, based on our findings."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/116.xml", "intro": "As we are living in an era of artificial intelligence, we are getting more comfortable with the idea of interacting with machines in our everyday life [42]. Artifacts such as Embodied Conversational Agents (ECAs) [10] are specifically designed to facilitate these interactions, providing a natural, human-like user interface capable of reproducing the different modalities of human communication such as speech, gestures, facial expressions or gaze.Proc. of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018), M. Dastani, G. Sukthankar, E. Andr\u00e9, S. Koenig (eds.), July 10-15, 2018, Stockholm, Sweden. \u00a9 2018 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved. While Bledsoe's dream [7] of seeing humans befriend machines is still a long way off, many efforts are being made to make these ECAs capable of building and maintaining a relationship in the long run with their users. Indeed, this social bond also known as rapport has been shown to increase ECA performance during tasks such as tutoring [33], health coaching [21], or museum guidance [6].One particular role that would greatly benefit from rapport building mechanisms is that of personal assistant [2]. Indeed, companies large and small are now moving forward with the vision of intelligent virtual personal assistants such as Apple's Siri, Microsoft's Cortana or Amazon's Alexa. However, these current personal assistants only provide a vocal interface and do not yet allow multimodal input, or provide embodied output to their users. Increasingly, these same companies have begun to investigate adding social chitchat, but their approach is not grounded in human behavior. They lack the social awareness and reasoning that would allow them to sense and generate relevant social language leading to increase rapport with their user.In this paper, we report results of a field trial with a semiautomatic socially aware personal assistant that helped participants of a large conference to find relevant sessions to attend and interesting people they should meet. Rather than simply delivering information through a textual interface, or a plain dialogue, we designed our personal assistant to build a relationship with the conference attendees through a multimodal rapport-building dialogue. Conference attendees interacted with our personal assistant during the conference, getting recommendations about sessions to attend and/or people to meet. Our main contribution here is to analyze these interactions and (1) investigate the relationship between the rapport dynamics and the task performance of our personal assistant and (2) propose design guidelines for personal assistants and socially aware ECAs in general.", "relwork": "Since 2011, the Siri's debut, major tech companies have released a number of voice-based intelligent personal assistants, such as Microsoft's Cortana, and Amazon's Alexa. One of the origins of such intelligent personal assistants might be CALO, the Cognitive Agent that Learns and Organizes, a foundation technology of Siri. CALO is able to handle major cognitive tasks, such as task and schedule management [5] [25], and human communication mediation [39]. Similarly, the RADAR project developed a software-based personal assistant to help users cope with email overload as effectively as a human assistant. The system analyzes text messages received by the user to distill out task-relevant information including new tasks elicited by a message [16]. The major purpose of these research prototypes and commercial products is to support users' cognitive tasks. They only focus on the task aspect of the interaction without taking into account the social cues delivered by their users. Some other work, however, has started to investigate the positive influence of rapport on their agents' task-performance. Rea is a real-estate virtual agent who talks about apartments to rent while building trust and rapport with her users through the use of social language and small-talk [11]. The authors found that extroverted users trusted the social version of Rea more than the version that only focused on the task itself. However, there was no evidence to suggest that users who engaged with social Rea would pay more for a recommended apartment than users who interacted with Rea's task-only counterpart. In [6], the authors deployed Tinker, a virtual museum guide designed to describe various exhibits to guests and then help them find their way out. Tinker was able to build rapport with its users through a short dialogue using predetermined strategies. The authors reported that Tinker's use of relational behavior improved users' engagement, which consequently improved the amount of information retained by the users about museum exhibits. Ellie [14] is a virtual agent designed to have engaging interactions in which users would feel comfortable sharing and disclosing information. Ellie uses non-verbal behavior and a set of dialogue policies to build rapport with its users. In terms of rapport, the authors discovered that people who interacted with a fully autonomous version of Ellie reported feelings comparable to people who had a face-to-face interview with a human semi-expert. Furthermore, most of the participants (75.8%) who interacted with autonomous Ellie agreed that they were comfortable sharing information with it. Most of these works rely on self-rated rapport score to investigate the relationship between rapport and the agent task-performance. None, however, try to capture the evolution of rapport during the interactions nor do they investigate whether an agent's task performance could affect the dynamics of rapport over time. We try to address these gaps by shedding the light on the following research question:RQ: \"How does the task performance of a personal assistant affect the dynamics of rapport over the course of an interaction?\"In this paper, we hypothesize that rapport between a personal assistant and its user is likely to increase if the former achieves high task-performance. On the other hand rapport is likely to decrease if the personal assistant fails to achieve its task.", "rq": "To investigate the influence of SARA's task performance on the rapport dynamics over time and thus answer to our research question, we define the following hypotheses. H1.a -The likelihood that rapport increases during an interaction (Rpt_Utopy) is negatively and monotonically correlated with attendee's bad recommendations rate (Bad_RecoOver + Bad_DiffDom), meaning that rapport was more likely to decrease during an interaction when SARA delivered bad recommendations. H1.b -The likelihood that rapport increases during an interaction (Rpt_Utopy) is positively and monotonically correlated with good recommendations rate (Messenger_Yes + Pic_Pro + Pic_Asked), meaning that rapport was more likely to increase during an interaction if SARA delivered good recommendations."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/17.xml", "intro": "Healthy eating implies complex decision making processes [6], including being aware of healthy options and choosing among them [24]. One solution to overcome this issue and help people to make healthier choices is to develop health-aware food recommender systems [31]. While significant effort has been put recently into optimizing the food selection algorithms [30], many other factors can also influence users' overall experience when interacting with a recommender system [14]. Indeed, the way the recommendation is presented [18], the system's response time [33], or even the length of the system's utterances [20] can have an influence on users' perception of the system.One trend to improve users' experience is to make the interaction more natural by designing the recommendation process as a conversation [23]. Besides helping users to achieve task-oriented goals, conversations can also fulfill interpersonal functions, such as building rapport [29]. Rapport can be described as a dynamic process that can be achieved when people \"click\" with each other or feel the interaction is due to \"chemistry\" [27]. Human-human studies have found that rapport between two people can influence task performance in situations as diverse as peer-tutoring [25] and negotiation [7]. Based on these findings, it becomes important to endow recommender systems with social conversational infrastructure that would allow them to build rapport with their users to improve task effectiveness.In this paper, we present a conversational system able to recommend recipes matching users' needs while building rapport with them. More specifically, our work focuses on investigating how the conversational skills of a recipe recommender system and the interaction modes it offers to its users would influence users' perception and their intention to cook. First, we describe the design of our system and its architecture before we explain how the recommendation process works. Then, we evaluate our system through an experiment in which we study the impact of our system's conversational skills and interaction mode on its persuasiveness. Our main contributions are (1) a rapport-building conversational approach to deliver recipe recommendations adapted to users' needs and habits and (2) a subjective evaluation investigating the influence of a recommender system's conversational skills and interaction mode on users' perception of the system, users' perception of the interaction and users' intention to cook the recommended recipes.", "relwork": "Food Recommender Systems. A common approach for food recommender systems is to recommend a recipe based on its ingredients. In [8], for example, the authors developed a system that relies on recipes that people like to infer their preferred ingredients. The system then recommends new recipes containing the previously inferred ingredients. In [9], the authors developed a system that collects users' preferences by asking them to rate and tag the recipes they usually cook at home. The system then relies on user's preferences to rank recipes and deliver recommendations with the highest scores. This Matrix Factorization algorithm outperformed the content-based approach proposed by [8]. Other approaches only rely on dietary information to recommend recipes that would match users' needs. YumMe, the recommender system developed in [36], automatically extracts dietary information from pictures of recipes to form a user profile. The system then relies on this user profile to deliver subsequent recommendations. In [11], authors analyzed people's eating behavior and clustered people in two categories: those interested in getting healthy recipes, and those who did not care about that. They found that two of the main recipe rating predictors for the first group were the fat and calorific content of the recipe, and decided to incorporate these features in their recommendation process.All these works focus on improving recommendation algorithms. They do not investigate how the modality of the interaction between the system and its users can improve users' experience which, according to [15], should not be neglected.", "rq": "To answer to our research questions RQ1 and RQ2, we designed an experiment investigating how our system's conversational skills and interaction mode influenced the perceived quality of both the system and the interaction, in addition to users' intention to cook recommended recipes."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/12.xml", "intro": "", "relwork": "Many existing recommendation systems already generate explanations, and several attempts have been made to classify these explanations [21,22,37]. In item-based explanations, the system relies on the previous recommendation's outcome to justify the current recommendation: \"I have recommended X because you previously liked/bought Y. \" Feature-based explanations use preferences that were specified by the user during the preference-elicitation process: \"Your interest in Z suggests that you would like X. \" These two different types can be combined: an example of item and feature based explanations can be found in [25], where the system displays the features of previously liked movies to justify the current recommendation. In the domain of movie recommendations, a system can justify its decision by emphasizing a plot similarity [25] or an overlapping cast [30]. An evaluation comparing featurebased explanations, item-based explanations, and a combination of both shows that the hybrid explanation type was significantly more appreciated by users [30].Both item-based and feature-based explanations are machinecentered and thus essentially reveal the system's decision-making process. Although they have a great impact on transparency, these explanations are tightly coupled with the types of features that the recommendation engine is relying on and may lack the persuasiveness and richness that humans often express when they recommend a specific item. Another important question regarding feature-based explanations is whether they should be personalized to match users' preferences. Research indicates that while personalization generally increases satisfaction, it can be detrimental to effective decision-making [33]. This shows how effectiveness and satisfaction aims can be discordant.Human-based explanations take an alternative approach; here, the system relies on collaborative filtering to reference similar products: \"People who liked X also liked Y.\" One such example is [13], in which the system recommends social software items such as social groups or communities and justifies its choice by showing the names of people in the group/community, as well as their relationship to the user. This relation could be \"familiar\" if the user was friends with the person, or \"similar\" if both shared similar interests. The authors' experiment shows that when these people were \"familiar, \" users were more satisfied with the recommendations.Human-based explanations can be merged with feature-based explanations by combining existing reviews with users' preferences [7,14] to generate explanations: \"You might want to watch X because Bob says that the storyline is amazing and I know that you are highly interested in plot. Here is his review: (...). \" This approach thus uses third-party opinions to justify choices. However, reviews are sometimes extremely long, making them difficult to integrate when conversing with a user.As recently demonstrated by [17], researchers would benefit from taking a more human-centered approach for the design of their recommendation systems, i.e., building systems able to express their \"own\" opinions. The authors' recommendation system, which used social conversational strategies such as self-disclosures and reciprocity in its recommendation process, significantly increased users' satisfaction and intention to seek future recommendations.In this paper, we aim to build a conversational recommendation system that recommends movies by expressing its \"own\" opinions and experience through social explanations. We thus focus on the following research questions: RQ-1: What are the types of social conversational strategies that humans use when they describe a movie they watched to someone? RQ-2: Do social explanations used by a conversational recommendation agent to justify its recommendations influence the perceived quality of both the recommendations and the interaction?", "rq": "In this paper, we aim to build a conversational recommendation system that recommends movies by expressing its \"own\" opinions and experience through social explanations. We thus focus on the following research questions: RQ-1: What are the types of social conversational strategies that humans use when they describe a movie they watched to someone? RQ-2: Do social explanations used by a conversational recommendation agent to justify its recommendations influence the perceived quality of both the recommendations and the interaction?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/10.xml", "intro": "", "relwork": "With the recent developments in many of the sub fields of conversational AI, including machine learning, dialog management and NLU, many different conversational AI systems have emerged [22].In industry, this technology has been incorporated into search engines, mobile devices, and personal computers. In search engines such as Google and Bing, conversational AI is used to create the feeling of having a conversation with the search engine, enhancing the experience. In mobile devices and personal computers, one use of conversational AI is to create virtual assistants. Some of the biggest virtual assistants on the market today are Apple's Siri, Google Assistant, Amazon Alexa and Microsoft Cortana [14]. These assistants also have the capability of acting as chatbots where they keep a turn-based dialog (a dialog where the user and the bot take turns in asking and responding to queries) with the user. There also exist conversational interfaces that only focus on this type-dialogbased conversation such as XiaoIce [11] and Replika [11]. These dialogs use what is known within conversational AI as intents and entities to understand the user's goal behind the query. In other words, an intent is what the user wants to achieve with the query, and an entity is the key information for answering the intent.Recently a number of different platforms have been made available to simplify the creation and integration of conversational interfaces for developers. The most popular ones are: Google's Di-alogFlow (formerly api.ai) 1 , IBM's cloud-based bot service Watson Conversation 2 , Amazon Lex 3 and the Microsoft Bot Framework 4 . These platforms come equipped with several different technologies used for NLU, dialog management, response generation and other aspects [5,11].Since conversational AI is a new field, systematic approaches to overview and categorize it are still in their infancy. Patil et al. [24] makes a general comparison of features and functionalities between some of the commercial platforms, giving an overview of what platform one might choose for developing a conversational AI system. There have also been more specific studies conducted which compare the NLU and conversational abilities of these types of platforms. Canonico and De Russis [18] compare the NLU performance of these platforms have in terms of usability, pre-built intents (a number intents already existing in the NLU tool) context etc. McTear [19] describe the two main conversation models \"oneshot queries\" and \"slot-filling dialogues\". He compares different platforms' ability to handle follow up questions in one-shot query scenarios and their mechanisms for slot-filling (a type of conversation where the bot asks specific questions to fill certain slots to fulfil a user intent). McTear also presents a number of problems that developers may face when creating conversational interfaces with these platforms. One of the main issues is that it might be difficult to know what functionalities a specific platform offers. There is also a difficulty in interpreting what functionalities might be common between platforms since there is no standard terminology. Venkatesh et al. [31] describe a number of metrics that can be used to evaluate the overall performance of a conversational agent based on the annual competition Alexa Prize [25] made for furthering conversational AI. They propose metrics such as conversational user experience, engagement, and conversational depth to measure the conversational abilities of entire conversational AI systems or chatbots [31]. Shawar and Atwell [26] describe metrics to specifically evaluate chatbot systems, a type of conversational AI interface. They argue that metrics for evaluating the abilities of these systems should be done based on the application and its domain and not solely on a standard.One of the main issues with creating the metrics described above is the understanding of what a good conversation is. Clark et al. [6] discuss that people generally describe conversations with conversational interfaces in terms of their performance and perceive them more as a device to be controlled. Indicating that people have a previous notion of how these systems will behave coming from a perception that infrastructure to support proper human-to-human dialogs do not exist.The maturity assessment framework presented in this paper takes inspiration from three language proficiency frameworks: Common European Framework of Reference (CEFR, [7]), American Council on the Teaching of Foreign Languages (ACTFL, [27]), and the Interagency Language Roundtable (ILR, [1]). The goal of these frameworks is to assess the language competency of an individual for a particular language. All of these frameworks have a similar structure, distinguishing different, successive levels (e.g., in case of CEFR, a sixitem scale A1-C2), language-relevant skills (e.g., for CEFR, reading, listening, speaking, and writing), and a number of hints for assigning an individual to a level. While the contents of the framework differ, they all share this same basic structure, which we also found useful for inspiring the design of our framework. A number of papers have scientifically investigated these frameworks, studying their validity and the possibility to use them in an automated way [12,15,30].", "rq": "We created a framework that can be used to evaluate the conversational maturity (intuitively, how \"smart\" an agent is in understanding questions and formulating responses) offered by the platforms. To this end, we considered existing frameworks that evaluate the language proficiency of humans, and previous discussions on how to evaluate different conversational AI development platforms. We then devised a framework based on the features identified in the first research questions and their effect on the human-like performance of a conversational AI development platform."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/111.xml", "intro": "Chatbots are regarded as one of the most promising technologies and are increasingly applied in many domains. Because chatbots provide a fast, convenient, and low-cost communication channel, both scholars and practitioners are keen to develop effective chatbots to address the challenges of providing healthcare services. For example, a growing body of research demonstrates how chatbots can be useful for helping people maintain good lifestyles [29,37], collecting daily health information to share with healthcare providers [26,34], and guiding people to improve their general well-being [28,32,63]. For instance, Wang et al. [61] proposed a conversational agent to coach people to relieve their public-speaking anxiety through cognitive reconstruction exercises, and Fitzpatrick et al. 's [17] Woebot system gives step-by-step guidance for users to think through their situation with cognitive behavioral therapy and was found to relieve users' depression. Other recent studies have applied a variety of conversational strategies and structures to promote behavioral change and to persuade chatbot users to act differently [27,29,57]. Some of these systems have even been found to outperform human-human interaction in some scenarios. For example, Lucas et al. [36] found that utilizing a virtual agent as an interviewer could promote users' depth of self-disclosure, and Xu et al. [67] concluded that the use of interactive robot agents would probably enhance physical-therapy outcomes. Therefore, these prior works have demonstrated that chatbots can serve as an effective platform for delivering guidance and tutoring people.Despite the success of utilizing chatbots to deliver guidance, there are still a number of challenges to overcome. For example, research points out that people easily become disengaged from using a chatbot [48,58], hampering them from long-term interventions. Moreover, people may overtrust solutions suggested by chatbots which could be inappropriate [24,67,68]. In another study, Luria et al. [38] found that people felt uncomfortable interacting with a chatbot which used the same personality to handle both low-risk (e.g., social chat) and high-risk (e.g., medical purpose) contexts. Thus, the authors suggested to design a chatbot that embodies multiple personalities, each of which are displayed in a unique social presence and have the expertise to focus on a single task.Prior studies inspire us to overcome challenges by integrating human support into a chatbot system. More specifically, we may be able to make the best use of both human-based and chatbotbased approaches by co-embodying them into a single system. Indeed, studies have suggested that the integration of human support with chatbot interactions could promote user engagement [48] and efficacy of using self-guided systems. For example, a recent study [34] proposed a mediator chatbot that promotes deep self-disclosure from users and delivers the information to a human expert. More research is clearly needed on how individuals might respond differently to interaction with a chatbot alone vs. one incorporating human support. We are also interested in understanding how such differences affect user experience in the long run. To help fill the gap, we conducted a mixed-methods study with 35 participants. We deployed two chatbot designs, both of which delivered training in journaling skills [22,25]. The first version of the chatbot guided participants in the journaling skills itself, while the second version integrated a human expert (coach) into its interaction when guiding the participants in the journaling skills. Over a period of four weeks, we tracked changes and differences in how each version impacted users' responses to and perceptions of the chatbot system, as well as their level of compliance with the guidance to practice journaling skills.Our work makes several contributions to the CSCW and human-computer interaction (HCI) communities. It is among the first that investigated the effects of integrating a human expert to deliver guidance for practicing journaling skills. Our unique three-phase design of an experimental study with 35 participants contributes novel findings of how chatbot interactions with and without expert guidance elicited user interaction differently over time. More specifically, during the Training phase, participants' actual and perceived engagement with the chatbot providing expert guidance (HC) was significantly higher than that of the participants who interacted with their chatbot alone (OC); however, during the Free-will phase, the OC participants chose to continue practicing journaling significantly more than the HC participants. Second, triangulating system log analysis with interviews and surveys, we provide new insights into how the design of chatbot systems with and without human support affected user experience of such systems both objectively (e.g., in terms of the length and depth of journaling content) and subjectively (e.g., participants' perceived trust and intimacy with the chatbots). Third, our work also presented empirical evidence of using chatbots to practice journaling on improving participants' self-reflection. Since prior work shows better self-reflection could enhance people's awareness of their well-being, our work further provides Exploring the Effects of Incorporating Human Experts to Deliver Journaling Guidance through a Chatbot 122:3 design implications for applying chatbots in the healthcare domain and to support diverse training purposes.", "relwork": "2.1 Chatbot for Delivering Guidance Conversational agents (e.g., chatbots) are gaining considerable attention in many fields including healthcare [69] and education [64]. Research has shown that chatbots can assist users in tracking and monitoring their behavior (e.g., [37]) and feelings (e.g., [17]), which could further be used to solicit social support and self-reflection [29]. Also, many studies designed chatbots to guide healthier habits or ways of thinking [46], such as better eating habits [37], exercise [29], ways of coping with stress [46], and self-compassion [32]. For example, Park et al. [46] incorporated a motivational interview technique into chatbot conversation to help users cope with stress, and found that their design facilitated conversations that improved self-reflection as well as stress management. Lee et al. [32] designed a dialogue aimed at inspiring users to take care of a chatbot that was portrayed as having had a negative experience, and found that after doing this for two weeks, users' self-compassion increased significantly. Another line of research has shown that chatbots have the potential to help people improve their mental well-being by training their thoughts and behavior [36,61,65]. For instance, Wang et al [61] designed a public-speaking tutor using a chatbot system to coach users and reduce their public speaking anxiety. Hence, these studies have shown that chatbots could not only help track users' behavior but could also play a proactive role in training users to learn skills.Recent advancements in artificial intelligence (AI) are also enabling chatbots and other virtual agents to act more credibly like human beings, including during the provision of self-help information [10,14]. Prior studies [5,33] indicated that conversational interaction can increase trust and affect users' acceptance of recommendations from a conversational agent. Thus, the design of the interaction between them is important in enhancing users' willingness to adopt chatbot suggestions. Gabrielli et al. [18] proposed a chatbot-based coaching intervention that successfully helped adolescents learn life skills, such as strategies for coping with bullying, and previous research [24,67] found that their participants' trust and compliance with physical therapeutic suggestions were both higher when interacting with robot therapy partners than with a human expert. Moreover, research has shown that people tend to apply the social norms of human relationships to their interactions with computer agents. This tendency, known as the Computers Are Social Actors (CASA) paradigm [42], has informed the design of many computer agents [35,50,57]. People may perceive intimacy and companionship with a computer agent [27,35,40], inducing changes in behavior change. For example, Ravichander et al. [50] found that reciprocity occurred in human-chatbot interactions and that a chatbot's self-disclosure encouraged people's self-disclosure. Similarly, recent work by Lee et al. [35] showed that a chatbot's self-disclosure improved participants' perceived intimacy with the chatbot and facilitated their self-disclosures in response to the chatbot's sensitive questions (e.g., failure experiences).However, several limitations of chatbot-based approaches remain, and in certain situations, chatbot-based approaches may be less beneficial than those provided by humans [40,41,48]. For example, Howard et al. [24] has pointed out that some people may trust robots too much, due to over-optimism about the viability of the solutions they suggest, and that this trust becomes a source of risk if robots make clinically suboptimal or inappropriate suggestions. In addition, for healthcare interventions that require long-term engagement [8], people may easily become disengaged from the use of self-guided systems, due to loss of motivation and/or failure to incorporate those systems' recommendations into their daily lives [48]. Furthermore, an investment model shows that purely computer-based interventions are often much less effective than hybrid ones with some professional human input [16], in part because the latter tends to inspire their users to execute a higher proportion of their intervention requests.", "rq": "To answer the three research questions, we present the results following their order. First, RQ1 is answered by analyzing conversational logs in the Suggestion session. Second, RQ2 is answered by the pre-and post-survey of users' perception, and the interview results are included to explore the reasons for causing the experience. Finally, RQ3 is answered by counting the number of participants that voluntarily practiced the journaling suggestions to understand the lasting effect. The interview is also involved in extending understanding. Fig.  shows two participants' sample dialogues with the chatbot system."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/134.xml", "intro": "Self-disclosure is a process in which a person reveals personal or sensitive information to others [23,1] and is crucial for developing a strong interpersonal relationship [1]. The advancement of computing technologies has enabled new ways for people to self-disclose [47,33]. The value and importance of self-disclosure through these technologies have been widely manifested. For example, people's self-disclosure on social media helps them release their stress, depression, and anxiety through these technologies [10,3]. Interviewees may disclose themselves more openly in an interview session when using virtual agents [33,12]. The challenge is that people naturally avoid revealing their vulnerabilities to others [8,29].Chatbots (also called conversational agents) have great potential to create breakthroughs in self-disclosure research [33,41], and the HCI community has dedicated an increasing amount of work to this. For example, people are found to provide more high-quality self-disclosure data when using chatbots than through web surveys [26]. Fitzpatrick et al. further utilized a therapy chatbot \"Woebot\" in their study to explore its feasibility to help release students' mental illness and showed the chatbot could help relieve symptoms of anxiety and depression [17]. Similarly, several works demonstrated the potential benefits of using chatbots for mental wellbeing [48,25,6]. Recently, Ravichander et al. also shared their findings that reciprocity could occur in human-machine dialog [41]. However, most of the existing research reported one-shot experiments; how chatbots can promote deep self-disclosure (conversing with machines about sensitive topics) over time is under-explored. This is an important question because many application domains, e.g., for mental well-being, [39,27], require sustained self-disclosure of sensitive topics over a period of time.In this work, we design, implement and evaluate a chatbot that has self-disclosure features when it performs small talk with people. We ran a study with 47 participants and divided them into three groups to use different chatting styles of the chatbot for journaling and answering sensitive questions. Each participant used the chatbot for three weeks, and each group experienced the chatbot's self-disclosure at varied levels (i.e., none, low and high). We found that chatbot's deep self-disclosure had a reciprocal effect on promoting participants' deep self-disclosure that lasted over the study period. In addition, chatbot's self-disclosure also had a positive impact on participants' perceived intimacy and enjoyment with the chatbot. The chatbot without self-disclosure, on the contrary, failed to have the same effect.Our work makes the following contributions to the HCI community: 1) we explore how varied levels of a chatbot's selfdisclosure influence the depth of people's self-disclosure, 2) we contribute new understandings of how time plays a role in chatbot and people's self-disclosure interactions, and 3) our findings also provide new implications into designing and using chatbots where deep self-disclosure is needed. http://dx.doi.org/10.1145/3313831.3376175", "relwork": "Self-disclosure -the gradual unveiling of personal information, thoughts, feelings, goals, and even failures-is key to individuals' formation of interpersonal relationships and achievement of intimacy [23,1]. A leading explanation of the selfdisclosure process is social penetration theory (SPT) [1], which categorizes four stages of self-disclosure, i.e., orientation, exploratory, affect-exchange, and stable-exchange. Together, these stages delineate a journey from the disclosure of shallow and general to deep and intimate information. As such, self-disclosure can be evaluated from two main dimensions, breadth and depth [38]. The breadth dimension denotes wide-ranging discussion of multiple topics such as music preferences and food, whereas the depth dimension comprises more personal details and intimate topics such as sexual relationship and self-perceived failures.Self-disclosure plays an important role in a wide range of settings, including mental well-being [39], customer service [36], and employment [33,12]; thus extensive research has been conducted on self-disclosure's relationships to various constructs including trust [49], intimacy [9], gender [19], and personality [9]. A considerable body of prior research has identified self-disclosure as a potential path to mental wellness, and its benefits during psychotherapy are also well attested [11]. The Substance Abuse and Mental Health Services Administration (SAMHSA) 1 reported that people who disclosed their mental illnesses felt relief and experienced improved relationships with friends and family members.However, disclosing personal mental health information is not easy for most people, and this is also one of the major practical difficulties in counseling sessions [18]. People naturally avoid revealing their vulnerabilities to others; this tendency is even more prevalent among those with mental illnesses, because those people who seek mental health care worry about social stigma and discrimination related to mental health problems. Previous studies have found that when people were interviewed face-to-face by a human interviewer, they may tend to disclose fewer symptoms of depression than when interviewed by a virtual agent [33]. It is not clear how people disclose when facing a different conversational agent design. For example, Clark et al. [7] found that there may be a fundamental barrier to developing relationships with conversational agents because people value different aspects in conversation with agents -some people may treat a chatbot as a tool, but users with mental health issues or social difficulties may benefit from social capabilities in a chatbot system.", "rq": "Though scholars have made significant progress with selfdisclosure research using chatbots, major research questions, such as how chatbots can promote deep self-disclosure over time, are still under-studied. Promising application domains, e.g., mental health  often need support tools to acquire people's sustained self-disclosure of sensitive topics over a period of time. Thus, in our work, we are interested in exploring: RQ1: How do different chatting styles influence people's self-disclosure? and RQ2: How do different chatting styles influence people's self-disclosure over time? Specifically, literature on reciprocity  suggests that when people make deep self-disclosures, their interlocutor will feel pressure to share information at a similar level. Therefore, we hypothesize that: H1: People self-disclose more deeply with a more self-disclosing chatbot over time."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/109.xml", "intro": "Rapid progress in technology is changing the way we interact with the information [8]. Improvements in speech recognition and natural language processing have allowed people to build voice-controlled personal assistants, such as Apple's Siri, Amazon's Alexa and Google Home. These technologies are increasingly popular, and people are integrating them in everyday life, e.g., for simple tasks like setting up a timer, checking the calendar, requesting the latest news, a song, etc. 1 . The popularity of text-based chatbots is also on the rise in many areas of the web [7]. Most of them are template-based and are designed to fulfill a single, often monotonous, job [5,6]. At the same time, a growing proportion of web search queries is formulated as natural language questions [12,9,2], which is partially explained by the increasing usage of voice interfaces [15]. Alas, for information seeking scenarios, existing chatbots and intelligent assistants are usually implemented as simply a \"proxy\" to existing web search engines, even though question-answering technology has made dramatic progress handling such question-like queries [14]. Furthermore, conversation provides additional opportunities to improve search quality. For example, a conversational system should be able to ask clarification questions [3] to better identify searcher's intent, and incorporate explicit user feedback [13] -something that is not normally available in a traditional web search scenario. However, before jumping into implementing additional features for conversational search systems, it is important to gain a better understanding what the users' expectations are when interacting with a truly intelligent conversational search agent. It is equally important to anticipate how users might behave when faced with a conversational search sys-1 https://arc.applause.com/2016/09/26/amazon-echo-alexa-use-cases/ tem since behavioural feedback is critical for system evaluation and improvements. To this end, we explore the following research questions:\u2022 RQ1: What are the main expectations from a conversational search system?\u2022 RQ2: What are the differences between human-tohuman and human-to-computer conversations?\u2022 RQ3: What characteristics prevent existing conversational agents from becoming effective tools for complex information seeking?As no truly intelligent conversational search systems exist yet, we explore these research questions with a mixture of survey methods and user studies. In the user study, the participants are faced with 3 complex information search tasks, derived from TREC Session track tasks [4]. To eliminate the voice recognition quality variable, we chose to use text messaging as the interface between a participant and conversational systems. We use three different conversational systems answering user requests: an existing commercial intelligent assistant, a human expert and a human disguised as an automatic system. The results of our exploration suggest: (1) people do not have biases against automatic conversational systems, as long as their performance is acceptable; (2) existing conversational assistants are not yet up to task, i.e., they cannot be effectively used for complex information search tasks; (3) by addressing a few requests from users that we identified, even current search systems might be able to improve their effectiveness and usability, with feasible modifications.", "relwork": "The topic of chatbots and conversational answer seeking has recently become quite popular.  [10], where 14 people were interviewed about their experience with an intelligent assistant that they use in their daily life. The authors report on people's experiences, expectations, discuss scenarios of successes and failures of conversational agents. They report that the most frequent types of tasks are relatively simple -weather updates and checking reminders. Our study, on the other hand, focuses on studying similar aspects of user behaviour for a different type of task -for complex search tasks. However, some of their findings overlap with ours. Much work has been done in the area of comparing user interactions with a human and a computer. There are varying opinions on the subject. Edwards et al. [6] found no significant differences in how Twitter users treated a social bot, whether it was perceived as a human or not. In turn, Cl\u00e9ment and Guitton [5] report that the way bots are perceived varies with the role they play. They found that \"invasive\" Wikipedia bots received more \"polarizing\" feedback -both positive and negative -compared to the bots that carried out \"silent helper\" functions. The similar result is reported by Murgia et al. [11] -Stackoverflow bot receives more negative feedback for false answers when its identity as an automatic program is revealed. Another work by Aharoni and Fridlund [1] reports mixed results from participants who underwent a mock interview with a human and an automatic system. The authors report that there were no explicit differences in the interviewer perception described by the participants, although the authors noticed significant differences in people's behaviour -when talking to a human interviewer they made greater effort to speak, smiled more, and were more affected by a rejection.", "rq": "To answer our second research question, about the differences between human-to-human and human-to-computer communication, we devised our second conversational agent -the Human agent. In this case, the Wizard from the previous setup was still serving as a backend, but the participants were explicitly informed that they were talking to a live person. Another difference was that the Human agent was not restricted to the pre-retrieved set of passages but was free to slightly reformulate or revise the passages to better respond to the question. By including both the Human and Wizard agents in the study, we were able to maintain a constant level of intelligence for both agents, thus comparing not the accuracy of each agent, but rather the participants' attitude and expectations towards a perceived automatic agent compared to a known human."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/135.xml", "intro": "During the past few years, chatbots, which engage users in a one-on-one, text-based conversation, have been adopted for a wide variety of applications [8,13,21,26]. Among various chatbot applications, a promising one is information elicitation (e.g., [51,63,64,69]). For example, Tallyn et al. use a chatbot to elicit user input in an ethnographic study [51]. Li et al. build a chatbot to interview job candidates and aid in talent selection [32]. Recent studies also show several benefits of chatbots for information elicitation, such as eliciting higher quality information than using traditional form-based methods (e.g., [29,64]).Inspired by these efforts, we are building interview chatbots to conduct user interviews and facilitate user research. To conduct effective interviews, interview chatbots should have skills similar to that of effective human interviewers [32,41]. One of such important skills is active listening-the abilities to understand and respond to a conversation partner properly [19,45]. Active listening is shown to facilitate interviews, e.g., eliciting higher quality responses [19,35,45] and making an interviewer more socially attractive [59]. In addition, studies find that active listening helps not only oral communication, but also online text communication, including text messaging [2,3]. Inspired by those findings, we hypothesize that interview chatbots with active listening would be more effective at conducting interviews and engaging interviewees. Figure 1 shows an example of such a chatbot, which can understand the user's input and summarize it in its response, making the user feel heard.Despite recent advances in Artificial Intelligence (AI), it is still challenging to build capable chatbots [22], let alone create chatbots with active listening skills. Below we highlight three main challenges specific to building effective interview chatbots with active listening skills.First, it is challenging to build interview chatbots that can effectively grasp and respond to user input to open-ended interview questions, which is the core of active listening. For example, in one of our user surveys, a chatbot asked an openended question \"what's the top challenge you're facing\".One user responded:\"The biggest challenge I've faced is finding a since of purpose. Being around like minded individuals who are constantly wanting more out of life through countless jobs I've never found something I was proud of\u2026\"Another user answered the same question very differently:\"With a new baby I have a lot of additional expenses. So I have to try to obtain additional income. I try to earn extra income by working on mturk, but the pay is low and I don't like the additional time taken away from my\u2026\"Given such user input, an effective chatbot should respond to each user empathetically to make them feel heard. Few chatbot platforms, however, enable chatbots to handle such complex and diverse user input. For example, popular chatbot platforms like Chatfuel [8] and Manychat [37] hardly handle user free-text input. More advanced platforms like Google Dialogflow [13] and IBM Watson Assistant [25] support Natural Language Processing (NLP), but they often require that a chatbot designer enumerate all user intents to be handled. With such a method, it would be very challenging to build an interview chatbot, since it is difficult to anticipate diverse user responses to open-ended questions and enumerate all possible user intents.Second, it is difficult to build interview chatbots that can effectively handle complex conversation situations to complete an interview task. As indicated by a recent report, natural language conversations are nonlinear and often go back and forth [22]. In an interview, a user may digress from a planned agenda for various reasons. For example, some users may not understand an interview question and want clarifications (e.g., \"What do you mean\"), while others might dodge a question by responding with \"Why do you want to know?\" or \"I don't know.\" Users might also misunderstand a question or simply do not know how to answer it. For example, one user offered an ambiguous response to the question mentioned above:\"Most challenges are met as an opportunity to grow. Hardest part is losing friends.\"Users may also be \"uncooperative\" and intentionally provide gibberish or irrelevant responses, such as those observed in crowd-sourced user studies [16].To complete an interview task, a chatbot must \"remember\" and stick to an interview agenda no matter how many times or how far a conversation has digressed from the agenda. However, most chatbots support scripted dialog trees instead of dynamic, graph-like conversations required by effective interview chatbots.Third, it is difficult for chatbot designers to take advantage of AI advances due to a lack of AI expertise or resources. For example, deep learning has enabled powerful conversational AI [36,62,65,66,1] and might help address the first challenge mentioned above. However, these models require large amounts of training data (i.e., interview data), which are hard to acquire.Given the three challenges mentioned above, we explore new ways to build effective interview chatbots. As the first step, we are investigating the feasibility and effectiveness of using existing AI technologies to build effective interview chatbots with active listening skills.Our investigation aims at answering two research questions:RQ1: Whether and how can we employ publicly available AI technologies to build effective interview chatbots with active listening skills?", "relwork": "Our work is related to research in four areas listed below.", "rq": "RQ1:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/127.xml", "intro": "In the contemporary Internet of Things (IoT) era, people can interact with a multitude of smart devices, always connected to the Internet, in the majority of the today's environments [6]. Smart lamps, thermostats, and many other Internet-enabled appliances are becoming popular in homes and workplaces. Furthermore, by using PCs and smartphones, users can access a variety of online services, ranging from social networks to news and messaging apps. In this complex scenario, the End-User Development (EUD) vision aims at putting personalization mechanisms in the hands of end users, i.e., the subjects who are most familiar with the actual needs to be met [13]. Through visual trigger-action programming platforms such as IFTTT [3] and Zapier [4], users can \"program\" the joint behaviors of their own connected entities, i.e., smart devices and online service, by defining trigger-action (IF-THEN) rules such as \"if I publish a photo on Facebook, then upload it to my Google Drive\", or \"if the security camera detects a movement, then blink the kitchen lamp. \"Despite apparent simplicity, previous studies [8,15,19,20] highlighted many interoperability, scalability, and understandability challenges suffered by contemporary trigger-action programming platforms. In such environments, smart devices and online services are typically modeled on the basis of the underlying brand or manufacturer [8]: as the number of supported technologies grows, so do the design space, i.e., the combinations between different triggers (if s) and actions (thens), and users often experience difficulties in discovering rules and related functionality [20]. As a result, trigger-action programming becomes a complex task for people without any previous programming experience [16]. Some previous works, e.g., [8,13], tackled the identified issues by proposing to move towards a new bread of trigger-action programming platforms supporting a higher level of abstraction, with abstract and technology-independent rules that can be adapted to different contextual situations. With triggers such as \"when user is sleeping\" and actions such as \"illuminate the room\", users can personalize their connected entities by saving time and reducing errors, without the need of explicitly programming every single involved technology. While this vision seems promising, however, it is yet unclear how to effectively move from abstract users' needs to the real devices and services needed for implementing them. How can a system decide how to \"illuminate\" a room? Is turning the lights on the right choice for the user? Does the user prefer to open the blinds, e.g., because she is interested in saving energy?  First, it allows users to communicate their personalization intentions and preferences (a). Then, it analyzes users' inputs, along with contextual and semantic information related to the available connected entities, to recommend a set of IF-THEN rules able to map the abstract users' needs to real connected entities (b).In this paper, we present HeyTAP, a conversational and semanticpowered platform able to map abstract users' needs to executable IF-THEN rules. By exploiting a multimodal interface, the user can interact with a conversational agent, either by typing or by voice, to communicate her personalization intentions for different contexts, e.g., to personalize her room's temperature when she is near home (Figure 1a). By interacting with the agent, the user can also specify her preferences on how to reach the goal of her personalization intention, e.g., convenience and preserving security in Figure 1a. To model such concepts, we extended the EUPont model [7], a semantic representation for End-User Development in the IoT. We exploited the OWL 1 classes and individuals of EUPont to categorize triggers and actions offered by user's connected entities in terms of provided functionality, and to model contextual information, e.g., the devices and services owned by the user and the relative position. Furthermore, we added classes and restrictions to automatically characterize triggers and actions on the basis of the user's preferences, e.g., to discriminate between energy demanding and privacy invasive behaviors. All these semantic information are used to suggest a set of IF-THEN rules that satisfies the user's needs, i.e., intentions and preferences. The user can finally inspect the recommended rules in the multimodal interface and select one or more of them to personalize her connected entities (Figure 1b).To understand to what extent HeyTAP is able to successfully guide participants from abstract needs to actual IF-THEN rules, 1 https://www.w3.org/OWL/, last visited on January 18, 2020 we ran an exploratory experiment with 8 users. In the study, we challenged participants in freely personalizing a set of connected entities in different contexts. Results confirm the effectiveness of the approach, and show that HeyTAP can successfully \"translate\" abstract users' needs into IF-THEN rules that can be instantiated and executed by contemporary trigger-action programming platforms. Despite participants expressed their personalization intentions with different level of abstractions, in particular, the tool was able to address the 90.63% of the collected needs, by providing IF-THEN recommendations that satisfied the participants. The collected participants' feedback also highlights possible improvements that could inform future works that aim at assisting users in personalizing their smart devices and online services.", "relwork": "One of the most popular paradigm to empower end users in directly programming their connected entities is trigger-action [11,19]. By defining trigger-action (IF-THEN) rules, users can connect a pair of devices or online services in such a way that, when an event (the trigger) is detected on one of them, an action is automatically executed on the latter. Trigger-action programming offers a very simple and easy to learn solution for creating end-user applications [5], and trigger-action programming platforms such as IFTTT and Zapier are becoming popular [10,15].Recently, researchers started to investigate different aspects of these solutions, e.g., through empirical characterization of usage perfomances [18] and large-scale analysis of publicly shared rules [20]. Despite apparent simplicity, indeed, the process of composing IF-THEN rules in trigger-action programming platforms has been found to be a complex task for non programmers [16], and the expressiveness and understandability of solutions like IFTTT have been criticized since they are rather limited [15,19,20]. Barricelli and Valtolina [5] analyzed the most popular end-user tools for personalizing connected entities, including IFTTT, and found that some of them \"offers a too complex solution for supporting end users in expressing their preferences.\" By evaluating thousands of trigger-action rules publicly shared on IFTTT, Ur et al. [19] found that the trigger-action approach can be both useful and usable for end-user development in IoT settings like smart homes, but they also found that the level of abstraction end users employ to express triggers needs to be better explored: many users, indeed, express triggers one level of abstraction higher, e.g., \"when I am in the room\" instead of \"when motion is detected by the motion sensor. \" In another study, Ur et al. [20] found that a large number of users is using IFTTT to create a diverse set of IF-THEN rules, which represents a very broad array of connections for filling gaps in devices and services functionality. According to the authors, however, the continuous growth of supported entities and connections highlights the need to provide users with more support for discovering functionality and managing collections of IF-THEN rules. The analysis emphasizes also the future need of making \"IFTTT rules more expressive. \" Similarly, Huang and Cakmak [15] conducted two user studies to systematically study how different types of triggers and actions, e.g., states vs. events, influence the understandability of trigger-action artifacts. They found users' inconsistencies in interpreting the behavior of IF-THEN rules and some errors in creating programs with a desired behavior.", "rq": "Results are organized across our 3 research questions. First, we report on how participants interacted with HeyTAP (RQ1), i.e., which level of abstraction they adopted in their personalization intentions and which preferences they expressed. Then, we investigate the ability of HeyTAP in addressing users' needs (RQ2), and we analyze the participants' satisfaction in using our conversational agent (RQ3).  Participants rarely included technologies, e.g., \"Philips Hue\" or \"Gmail,\" to express action (4.44%) and trigger (2.22%) intentions, thus confirming the limitations of platforms like IFTTT and Zapier . In line with previous works , in particular, trigger intentions were generally expressed in a more abstract way than action intentions. Indeed, while the 86.67% of action intentions specified an entity such as a door or a window, only the 17.78% of trigger intentions referred to a type of device or online service. On the contrary, trigger intentions were more likely to refer to a generic category, e.g., \"temperature\" or \"communication,\" with respect to action intentions (20.00% vs. 6.67%, respectively), while action intentions included a specific functionality, e.g., \"turn on\" or \"send, \" more often that trigger intentions (80.00% vs. 51.11%, respectively). Not surprisingly, a similar number of action and trigger intentions included a where (35.56% vs. 31.11%, respectively), while the when element was more common for trigger intentions than action intentions (35.56% vs. 2.22%, respectively).  reports the distribution of the preferences expressed by the participants during the study (RQ1). In the majority of cases, participants expressed their preference towards sustainability (37.93%) and convenience (24.14%). In the 20.69% of cases, instead, participants did not declared any particular preference, while security and privacy were mentioned in a limited number of cases (10.34% and 6.90%, respectively)."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/131.xml", "intro": "As the intelligence and robustness of dialogue systems and personal digital assistants continue to grow, so too does their potential usefulness in a wider variety of applications and their ability to integrate seamlessly into the daily lives of their users. Dialogue systems in particular show a great deal of promise in a number of medical and health-related applications, and have the potential to reach and support a wide variety of users due to their simplicity and natural interfaces [7]. However, potential uses occur in a wide variety of contexts which can have any number of disparate characteristics and require careful consideration of design choices [9].Many of the most highly impactful long-term uses of dialogue systems are particularly nuanced, especially when considering the potential use of intelligent technology as part of health or emotional support systems [7]  [12]. Dialogue agents in support roles may be required to interact with users in a variety of physical and emotional states, including situations with users under varying amounts of stress. Care must therefore be put into every aspect of the design for such systems, which requires an understanding of different use cases that goes beyond the surface level. Every use case, from setting reminders to keeping a calen-dar to talking about health, should be considered in detail. This becomes especially important when discussing the application of dialogue systems to more health and social support oriented domains, such as systems designed for use by the elderly aging-in-place or more broadly technologically underserved groups, since many of these populations may be vulnerable or otherwise at a disadvantage [10].In this paper we focus on the specific use-case of dialogue systems as daily planning and calendaring assistants, and investigate the characteristics of these interactions from a psychological and linguistic standpoint. More specifically, we seek to answer the following questions: What are the psycholinguistic characteristics of user interactions with a dialogue system designed to act as a scheduling assistant? How does a system's ability to learn about a user and maintain a user model affect these interactions? Are changes in interaction styles uniquely attributable to user modeling ability rather than simply user familiarity or acclimation with the system? To answer these questions, we performed a user study using a highly realistic Wizard of Oz approach. We then performed quantitative linguistic analysis of the transcribed interactions using Linguistic Inquiry and Word Count (LIWC 2015), a well-studied psycholinguistic text analysis tool, to gain a better understanding of the psychological dynamics that exist between user and conversational agent (CA) in this context [11]. This was also an initial exploration of whether users would be willing to discuss stress levels, and by extension affective content, with a CA.", "relwork": "Understanding user satisfaction with, and acceptability of, CAs and voice systems is complex [1]. Factors influencing long-term adoption (or more realistically, lack thereof) are also key concerns [2]. Several important interrelated design aspects stand out in relation to user satisfaction, though CS30, Page 2  some are less well understood than others. Conversational style and the presentation of information are critical, as is an agent's ability to learn about and adapt to a user -in other words, how a user model is formed and implemented [6].Most of the current literature relating to issues of user modeling explores integration of a user model and the response from users as relates to proactive intelligent agents that provide information to users based on knowledge of past behavior, such as when a smartphone alerts a user to current commute time on a weekday morning. Despite users indicating that they want a system that learns about themand despite the many advantages that such adapted or personalized learning can bring -this style of interaction also can be worrisome to users due to concerns such as loss of control or reductions in autonomy, lack of trust, and questions over data privacy. These are issues that have long plagued commercial systems [1,2,6]. Yet these concerns may be particularly foregrounded in the case of unimodal systems, such as a CA, where natural language is the only real interface to interact with the system, and where the longer-term goal is to incorporate the CA as a key interface within a larger health or emotional support system. In such a system, it will be necessary to balance incorporation of detailed and sensitive user information into a solid user model with the aims of developing a CA that invites further use, including disclosure of stress or other affective states.The method by which we evaluate interactions is also important. Since language provides us with key psychological and sociological insights, a robust linguistic analysis can provide key insights into user attitudes. For this study, the tool we use is Linguistic Inquiry Word Count (LIWC), one of the most extensively studied text analysis tools for both written and spoken communication [11]. Standard LIWC analyzes text for dozens of characteristics and types of language use, including categories like pronoun usage and social language. The most recent version, LIWC2015, also provides four major summary measures for a higher-level overview of text, which are the primary focus of this study. These include authenticity, tone, clout, and analytical thinking [8].Measurement of LIWC categorization and summary variables is done in slightly different ways. LIWC standard categories are reported as the percentage of words in a given text that fall into each category [11]. Linguistic summary variables, on the other hand, represent linguistic dimensions, encompassing multiple categories, that are predictive of different types of characteristics for the writer or speaker [8]. Summary variables are not reported as percentages of words in categories, but rather as scores on linguistic dimensions -in short, measures of the relationships between key linguistic categories. A more detailed discussion of the scales for each summary variable can be found in Findings. The word categories related to each summary variable are described briefly as follows [8]. Emotional tone is a measure of the overall positivity or negativity of a text, as captured by a difference between the number of word occurrences in positively valenced categories and those in negatively valenced categories. Authenticity measures the openness or honesty of a speaker or writer, and encompasses categories such as emotional valence, pronoun usage, and cognitive processing. Clout measures speaker authority, social status, or power, as captured by a focus on the self versus a more social focus or focus on the other. Analytical thinking measures cognitive style and complexity, as indicated mainly by syntactical and grammatical categories such as parts of speech and article usage.  ", "rq": "We performed an ex situ Wizard of Oz study of young adults interacting with an idealized digital personal assistant to discuss daily scheduling concerns and stress levels. We further varied rates of \"learning\" and personalization with the system to test user preferences and changes in participants' linguistic and psychological interactions with an unadapted versus adapted user model, and to determine whether those changes were attributable to acclimatization with the system or to the modeling capabilities, seeking to address 3 research questions: What are the psycholinguistic characteristics of user interactions with a dialogue system designed to act as a scheduling assistant? How does a system's ability to learn about a user and maintain a user model affect these interactions? Are changes in interaction styles uniquely attributable to user modeling ability rather than simply user familiarity or acclimation with the system? We present a linguistic analysis of the results using summary measures generated by a widely used psycholinguistic text analysis tool. Some of the measures seem to present the slightly paradoxical effect of reduced user engagement when a conversational agent explicitly discloses information about its user model to the user. These results suggest that future studies should take care to consider the degree to which the user model is directly exposed to the user. That is, being overly forthcoming about what has been CS30, Page 1 learned about a user may undermine attempts to tailor conversational agents to actively engage and relate to users.  "}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/24.xml", "intro": "", "relwork": "There is general agreement that context is of major importance in all information behavior activities (see e.g. [12,13]). The variety of contextual factors on which the information needs of users depend is large (see [1] for an overview of different notions and parts of context). Situation is one of these influencing factors -in particular if the situation is not well-known to users and, at the same time, requires users to adhere to a particular order of sequence of steps within this process. Information needs arising in these situations may result in the discomfort of users. A mobile, proactive information system, which is based on the process model of the particular task, may comfort users by fulfilling their information needs throughout this process. In this poster, we use passenger services at an airport as an example to investigate this problem. The reasons to choose this scenario were threefold: First, a passenger survey conducted world wide by IATA in 2016 reveals the need for personal information systems at airports [4]. Second, recent work suggests that specific user groups may not benefit from signage in public spaces, e.g. small people in crowded or information-overloaded settings like airports (see e.g. [7,2,6]). Thirdly, aircraft ground handling is a good example for a situation a larger number of people is not subjected to on a very regular basis. It is, yet, complex enough to pose several information needs -which is evident, for example, by the large number of information kiosks and terminals that can be found in airport buildings.", "rq": "Based on this scenario we were interested in two research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/.ipynb_checkpoints/173-checkpoint.xml", "intro": "A great deal of learning involves factual knowledge (e.g., numerous topics in medicine, language, and law). Further, such information is often learned outside of a formal classroom setting. Developing more effective automated methods for accelerating or improving factual learning therefore has the potential to benefit a multitude of students on a broad scale.Traditional electronic tools for practicing factual knowledge tend to be flashcard based [15,29,30]. Flashcard apps are simple and can be easily designed to provide personalized adaptive practice based on well-studied models of human memory [14,50,55]. However, to optimize for speed, flashcards typically involve passive learning (i.e., the user is asked to visualize the answer and then check for correctness). This may not fully take advantage of the testing effect (retrieval through testing with proper feedback) [48]. As shown in many previous studies, retrieval practices like testing lead to higher retention than purely studying via even multiple passive means of self-evaluation [8,34,47]. Feedback received from test results further improves retention [3,37].Moreover, flashcards are not typically designed to be engaging, making their effectiveness heavily dependent on people's desire to learn. Research confirms engagement can mediate learning effectiveness [7,27], especially for technologybased learning [26]. A more engaging way to learn factual knowledge could therefore lead to better learning outcomes.One possible path towards boosted engagement is using Natural Language Processing (NLP) powered chatbots, which are becoming increasingly sophisticated [21,52]. For example, such systems enable students to speak or type out their answers during a two-way dialogue and receive targeted feedback from NLP techniques interpreting the spoken or written words. This new interaction for learning factual knowledge may be much more motivating and engaging, and may also be more effective at providing adaptive feedback and promoting deeper learning [11].Given this potential for conversational approaches to enhance learning, we designed and built QuizBot, a dialoguebased adaptive learning system for students to learn and memorize factual knowledge in science, safety, and advanced English vocabulary. These three subjects were chosen because they cover diverse topics in medicine, language, and rules. They can represent important subclasses of factual knowledge that are usually learned outside the classroom setting.On the technical side, QuizBot leverages the supervised Smooth Inverse Frequency (SIF) algorithm [2] for automatic answer grading and the DASH model [39] for adaptive question sequencing. On the design side, we created Frosty, an encouraging tutoring agent that provides targeted feedback to learners based on their inputs (see Figure 2). The design of QuizBot was inspired by previous studies [9,13,20] to leverage the persona effect, the strong positive impact of animated agents on learning experience [38].To determine the impact of QuizBot on learning, we evaluated it against a carefully designed flashcard app, the typical medium for learning factual knowledge, through two controlled within-subject studies. We aimed to closely match the flashcard app to QuizBot in order to target assessment at the impacts of the conversational components. Specifically, the flashcard app used the same DASH algorithm for adaptive question selection, and a single pool of questions and answers was subdivided for the flashcard app and QuizBot.In the first within-subject study with 40 students, when the number of practice items was held constant for both flashcards and QuizBot, students scored substantially better on recall (fill-in-the-blank) and recognition (multiple-choice) with QuizBot than for items trained using flashcards (66.3% vs. 45.2% for recall and 87.2% vs. 65.8% for recognition). However, the time taken was longer with QuizBot than flashcards. In the second within-subject study with 36 students, we allowed learners to voluntarily allocate their time between the two apps. We found students spent 2.6x more time on QuizBot, and that students performed equivalently on recognizing items but significantly better with QuizBot at recall (with an effect size of .45). These results suggest that QuizBot is more engaging to use and more effective at recall and equally effective at recognition in typical user-driven scenarios. In normal use, QuizBot may be less efficient per unit time, but still yields improved learning on recall due to users voluntarily choosing to use it substantially more.This work has three chief contributions. First, QuizBot is the first chat-based learning system for factual knowledge memorization outside of classroom settings. Moreover, we show its effectiveness and engagement through rigorous comparison studies with a traditional learning tool for knowledge memorization, and our results demonstrate benefits of using chatbots to learn factual knowledge, especially for casual learning. Lastly, our results also reveal inefficiencies of chat-based learning systems, and we offer design suggestions for building improved future educational chatbot systems.", "relwork": "Our work was built upon previous studies on natural language tutoring systems, semantic similarity algorithms, and memory models.", "rq": "In this section, we discuss and answer our research questions based on evaluation results, and we offer suggestions for designing further improved chat-based learning systems."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/.ipynb_checkpoints/111-checkpoint.xml", "intro": "Chatbots are regarded as one of the most promising technologies and are increasingly applied in many domains. Because chatbots provide a fast, convenient, and low-cost communication channel, both scholars and practitioners are keen to develop effective chatbots to address the challenges of providing healthcare services. For example, a growing body of research demonstrates how chatbots can be useful for helping people maintain good lifestyles [29,37], collecting daily health information to share with healthcare providers [26,34], and guiding people to improve their general well-being [28,32,63]. For instance, Wang et al. [61] proposed a conversational agent to coach people to relieve their public-speaking anxiety through cognitive reconstruction exercises, and Fitzpatrick et al. 's [17] Woebot system gives step-by-step guidance for users to think through their situation with cognitive behavioral therapy and was found to relieve users' depression. Other recent studies have applied a variety of conversational strategies and structures to promote behavioral change and to persuade chatbot users to act differently [27,29,57]. Some of these systems have even been found to outperform human-human interaction in some scenarios. For example, Lucas et al. [36] found that utilizing a virtual agent as an interviewer could promote users' depth of self-disclosure, and Xu et al. [67] concluded that the use of interactive robot agents would probably enhance physical-therapy outcomes. Therefore, these prior works have demonstrated that chatbots can serve as an effective platform for delivering guidance and tutoring people.Despite the success of utilizing chatbots to deliver guidance, there are still a number of challenges to overcome. For example, research points out that people easily become disengaged from using a chatbot [48,58], hampering them from long-term interventions. Moreover, people may overtrust solutions suggested by chatbots which could be inappropriate [24,67,68]. In another study, Luria et al. [38] found that people felt uncomfortable interacting with a chatbot which used the same personality to handle both low-risk (e.g., social chat) and high-risk (e.g., medical purpose) contexts. Thus, the authors suggested to design a chatbot that embodies multiple personalities, each of which are displayed in a unique social presence and have the expertise to focus on a single task.Prior studies inspire us to overcome challenges by integrating human support into a chatbot system. More specifically, we may be able to make the best use of both human-based and chatbotbased approaches by co-embodying them into a single system. Indeed, studies have suggested that the integration of human support with chatbot interactions could promote user engagement [48] and efficacy of using self-guided systems. For example, a recent study [34] proposed a mediator chatbot that promotes deep self-disclosure from users and delivers the information to a human expert. More research is clearly needed on how individuals might respond differently to interaction with a chatbot alone vs. one incorporating human support. We are also interested in understanding how such differences affect user experience in the long run. To help fill the gap, we conducted a mixed-methods study with 35 participants. We deployed two chatbot designs, both of which delivered training in journaling skills [22,25]. The first version of the chatbot guided participants in the journaling skills itself, while the second version integrated a human expert (coach) into its interaction when guiding the participants in the journaling skills. Over a period of four weeks, we tracked changes and differences in how each version impacted users' responses to and perceptions of the chatbot system, as well as their level of compliance with the guidance to practice journaling skills.Our work makes several contributions to the CSCW and human-computer interaction (HCI) communities. It is among the first that investigated the effects of integrating a human expert to deliver guidance for practicing journaling skills. Our unique three-phase design of an experimental study with 35 participants contributes novel findings of how chatbot interactions with and without expert guidance elicited user interaction differently over time. More specifically, during the Training phase, participants' actual and perceived engagement with the chatbot providing expert guidance (HC) was significantly higher than that of the participants who interacted with their chatbot alone (OC); however, during the Free-will phase, the OC participants chose to continue practicing journaling significantly more than the HC participants. Second, triangulating system log analysis with interviews and surveys, we provide new insights into how the design of chatbot systems with and without human support affected user experience of such systems both objectively (e.g., in terms of the length and depth of journaling content) and subjectively (e.g., participants' perceived trust and intimacy with the chatbots). Third, our work also presented empirical evidence of using chatbots to practice journaling on improving participants' self-reflection. Since prior work shows better self-reflection could enhance people's awareness of their well-being, our work further provides Exploring the Effects of Incorporating Human Experts to Deliver Journaling Guidance through a Chatbot 122:3 design implications for applying chatbots in the healthcare domain and to support diverse training purposes.", "relwork": "2.1 Chatbot for Delivering Guidance Conversational agents (e.g., chatbots) are gaining considerable attention in many fields including healthcare [69] and education [64]. Research has shown that chatbots can assist users in tracking and monitoring their behavior (e.g., [37]) and feelings (e.g., [17]), which could further be used to solicit social support and self-reflection [29]. Also, many studies designed chatbots to guide healthier habits or ways of thinking [46], such as better eating habits [37], exercise [29], ways of coping with stress [46], and self-compassion [32]. For example, Park et al. [46] incorporated a motivational interview technique into chatbot conversation to help users cope with stress, and found that their design facilitated conversations that improved self-reflection as well as stress management. Lee et al. [32] designed a dialogue aimed at inspiring users to take care of a chatbot that was portrayed as having had a negative experience, and found that after doing this for two weeks, users' self-compassion increased significantly. Another line of research has shown that chatbots have the potential to help people improve their mental well-being by training their thoughts and behavior [36,61,65]. For instance, Wang et al [61] designed a public-speaking tutor using a chatbot system to coach users and reduce their public speaking anxiety. Hence, these studies have shown that chatbots could not only help track users' behavior but could also play a proactive role in training users to learn skills.Recent advancements in artificial intelligence (AI) are also enabling chatbots and other virtual agents to act more credibly like human beings, including during the provision of self-help information [10,14]. Prior studies [5,33] indicated that conversational interaction can increase trust and affect users' acceptance of recommendations from a conversational agent. Thus, the design of the interaction between them is important in enhancing users' willingness to adopt chatbot suggestions. Gabrielli et al. [18] proposed a chatbot-based coaching intervention that successfully helped adolescents learn life skills, such as strategies for coping with bullying, and previous research [24,67] found that their participants' trust and compliance with physical therapeutic suggestions were both higher when interacting with robot therapy partners than with a human expert. Moreover, research has shown that people tend to apply the social norms of human relationships to their interactions with computer agents. This tendency, known as the Computers Are Social Actors (CASA) paradigm [42], has informed the design of many computer agents [35,50,57]. People may perceive intimacy and companionship with a computer agent [27,35,40], inducing changes in behavior change. For example, Ravichander et al. [50] found that reciprocity occurred in human-chatbot interactions and that a chatbot's self-disclosure encouraged people's self-disclosure. Similarly, recent work by Lee et al. [35] showed that a chatbot's self-disclosure improved participants' perceived intimacy with the chatbot and facilitated their self-disclosures in response to the chatbot's sensitive questions (e.g., failure experiences).However, several limitations of chatbot-based approaches remain, and in certain situations, chatbot-based approaches may be less beneficial than those provided by humans [40,41,48]. For example, Howard et al. [24] has pointed out that some people may trust robots too much, due to over-optimism about the viability of the solutions they suggest, and that this trust becomes a source of risk if robots make clinically suboptimal or inappropriate suggestions. In addition, for healthcare interventions that require long-term engagement [8], people may easily become disengaged from the use of self-guided systems, due to loss of motivation and/or failure to incorporate those systems' recommendations into their daily lives [48]. Furthermore, an investment model shows that purely computer-based interventions are often much less effective than hybrid ones with some professional human input [16], in part because the latter tends to inspire their users to execute a higher proportion of their intervention requests.", "rq": "To answer the three research questions, we present the results following their order. First, RQ1 is answered by analyzing conversational logs in the Suggestion session. Second, RQ2 is answered by the pre-and post-survey of users' perception, and the interview results are included to explore the reasons for causing the experience. Finally, RQ3 is answered by counting the number of participants that voluntarily practiced the journaling suggestions to understand the lasting effect. The interview is also involved in extending understanding. Fig.  shows two participants' sample dialogues with the chatbot system."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/195.xml", "intro": "Conversational machines are being increasingly employed in physical spaces for both private and public usage. Examples include hotel lobbies and store showrooms [15], car dashboards [24], and home devices [40]. With such machines, or chatbots, human interactions may happen in the presence of an audience, be it friends, family (e.g., while on a road trip), or simply strangers and bystanders (e.g., in a hotel lobby).Previous studies have found that humans tend to change their normal conversation behavior when in front of others. In such contexts, people sometimes resort to using long and complicated words, uttering jokes, quoting from obscure authors, and, in general, pretending to be smarter, wittier, or funnier than in private conversations [4,35]. Also, when in the presence of others, some people may enhance the emission of dominant responses [8], according to the status of those in the audience [8,10]. However, some people react in the opposite way, becoming more shy than normal, failing to complete sentences, getting nervous, or even stuttering.Understanding such changes in behavior are important because it may be necessary to design the machine conversation systems to handle those situations where people change their usual behavior to accommodate the presence of an audience. We were motivated to study this kind of behavior change by some initial observations we made of visitors experiencing an art exhibit where they interacted with a group of chatbots either alone or in front of acquaintances and/or strangers. For example, we observed some people trying to amuse their friends by trying to \"break\" the machine with impossible questions; asking questions related to local politics and sports to provoke the other visitors; and uttering deep and complicated questions to show off to others their knowledge about the subject of the artwork. In any of those cases, we found that the art exhibit could have been designed to better handle the presence of an audience. For example, the system could have a higher threshold of guessing the right answer to complex questions when an audience is present. It could, for instance, assume that a visitor asking a complex question in front of an audience is less a situation where she is looking for knowledge and more like the system is being made fun of. While in the former case the appropriate response could be trying to find a good answer as hard as it could, whereas in the group situation it could be simply deflecting the question.Beyond art exhibits, as conversational systems become more ubiquitous, similar situations will be common in more down-to-earth scenarios. For instance, a conversation speaker (like Amazon's Echo or Google's Home) could benefit to adjust its behavior to handle audience effects. It could be less prune to making jokes to avoid making anyone in the audience uncomfortable, or, even worse, feeling ridiculed in front of acquaintances. In other words, by recognizing the audience context, the conversational system may be designed to answer in a more appropriate form for a situation of group social interaction, adapting to and enhancing the overall experience of users and their audiences.However, such considerations and strategies only make sense if we understand whether and how users change their behavior when conversing with machines in front of other people. Do they feel more embarrassed, powerful, or wittier by an audience when dialogging with a machine instead of a person? Are the changes different if the audience is comprised of acquaintances or strangers? To shed a light on such questions, we went further and performed two studies on the art exhibit and its visitors. This was a setting where single or multiple visitors freely conversed in a physical space with three text-based chatbots representing characters from a well-known 19th century book in Brazil. No control on how visitors interacted with the space or the chatbots was in place, with the exception that they had to do it through a single tablet. Images from the exhibit are depicted in Figure 1.To explore changes in conversational behavior of people due to the presence of an audience, we investigated the visitor perceptions of the three agents' social skills and the user's engagement with agents with the artwork. In the majority of the situations the interaction happened in front of other visitors, some of them known to the users, but also often in front of strangers. In our first study, we conducted 92 semi-structured interviews with visitors, after observing their behavior at the exhibit. Analyzing this data, we were able to determine that, in some specific situations, it was very likely that the audience presence was affecting the user experience of the visitors. In a second study, we analyzed the conversation logs of more than 5,000 sessions. Coupled with a silent video of the audience interaction, which we used to manually determine the occurrence and type of audience, we were able to explore changes in conversation patterns which could be related to the presence of other people around the visitor. The two studies seem to provide evidence of audience effects, and that designers should be taken into account audience effects in conversational systems in physical spaces. Moreover, our findings seem to indicate that those effects are modulated by many factors, including gender, knowledge about the content of the exhibit, and whether there were strangers in the audience.The next sections describe in detail the related work, the experimental setup, the two studies, their findings, and our main conclusions. Finally, we discuss some design implications, indicating how our findings may guide the design of conversational systems in physical spaces.", "relwork": "In this session, we describe the previous work as a background for our study, both in the scope of social interaction with chatbots and in the context of audience effects.Social Interaction with Chatbots: With the recent advances in conversational and natural language technologies, interest has increased on how humans interact with conversational systems, here referred generically as chatbots, and on how social presence and context may play a key role in understanding the dynamics of the interaction [29,37].Social presence is described as the social connection and involvement between two or more people in an interaction often developing and maintaining some sort of personal relationship [41]. The perception of social presence is sometimes connected to the anthropomorphism of physical robots, chatbots, and avatars. In particular, anthropomorphism is a prevailing topic of Embodied Conversational Agents (ECAs), a special case of embodied agents in which the agents provide human-like capabilities of face-to-face dialogue.Studies with ECAs have provided evidence that they can induce social-emotional effects comparable to those in humanto-human interactions [38,43]. Previous work found that people conversing with ECAs or interacting with robots show social reactions such as social facilitation or inhibition [3,38,50], a tendency to socially desirable behavior [20,39,43], and increased cooperation [32]. For example, analyses of users' utterances while interacting with a museum agent [19,20] showed semblance with human-to-human communication, with similarities in the amount of greetings and farewells, common phrases (such as \"How are you?\"), and human-likeness questions (e.g., \"Do you have a girlfriend?\").In general, system which exhibit human-like traits tend to improve the quality of the user experience with them., Cafaro et al. [6] found that smile, gaze and proxemics are important for conversational museum guide agents, implying that those agent influenced user's interpretation of agent's extraversion and affiliation and impacted on the user's decisions about further encounters.Although the degree of veracity in the dialogue often improves the quality of the interaction, it might have the opposite effect: the uncanny valley effect [30,44] where people are averse to a high degree of human similarity has also been observed. Experiments, such as [18,32], have validated this hypothesis by observing the user's emotion engagement strategies towards agents of varying human likeness.In this study, we contextualize our study object, the art exhibit, as containing three embodied chatbots. Even though the chatbots did not have a physical body they have a clear physical presence provide by scenographic elements (see Figure 1): female and male hats hanging above chairs around a table unmistakably embodied the chatbots.Audience Effects:. Seminal work on drive-producing effects of the presence of an audience [8] uncovered specific group interaction behaviors, which led to theories and design frameworks for spectatorship (e.g. [4,35]). Among the implications and findings of audience effects are the impact of behavior and views of bystanders on the response to an interaction, which has been known to influence engagement, either being related to attention, interest, or affective feelings [4,8,35].One of the early studies of audience effects concluded that proximity and presence of audience enhance the emission of dominant responses [8], i.e. responses governed by strong verbal habits at the expense of responses governed by weaker ones. Active audiences who looked and interacted with the subjects directly affected individual performance measured by the average number of responses in a word recognition task. In 1982, Michaels et al. performed a classical study on social facilitation showing that the performance of good pool players improved 14% in front an audience while bad pool players had a dramatic decrease of 30% [28].Love and Perry [23] studied the behavior and views of bystanders in response to a proximal mobile telephone conversation held by a third party. In their experiments, subjects demonstrated noticeable changes in body posture when viewing and listening to a confederate attending a call. The influence of audience has been also studied in video gaming, where researches explored audience aspects including age [49], size and distance of the interactor [18], typologies of spectatorship [27], player performance and perceived game difficulty [49], co-located/remote and virtual/real audience [11], cheering [16], supportiveness [5], activeness [21], and social aspects [9]. Overall, the findings report that different characteristics and behaviors of audience have positive and negative impacts, sometimes affecting the entire gameplay experience.Spectator experience design has been proposed by Reeves et al. [36], which produced a taxonomy that uncovers design strategies based on interface manipulations and their resulting outcomes. Audience participation in public spaces has also been studied from the point of view of interaction and engagement in many domains, such as education [47], sports [7], and arts [21]. One common observed practice which directly affects the experience is the honey-pot effect, where interaction with a screen in public can drive social clustering and further engagement [4]. Furthermore, Group interaction helps to explain how users understand and react to displays in public settings.Although previous works explored audience and spectatorship effects in games, sports, arts and other domains, to the best of our knowledge no research efforts have been made to study the experience of audience effects in scenarios where the main interaction is conversing with chatbots in a physical space. Finally, given that our setting is an art exhibit, we use the terms visitor and user interchangeably. In our study, a person is both a visitor of the exhibit as well as the user of the physical chatbot architecture described next.", "rq": "In summary, the two studies seem to have gathered enough evidence to support positive answers to both our research questions RQ1 and RQ2, and point towards the need of considering audience effects when designing conversational experiences in physical spaces. As shown in the findings, those audience effects can be modulated by many factors, including the audience being composed of acquaintances or strangers; the existence of a waiting queue; the knowledge of the context of the interaction; gender of the users; and use of direct address."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/80.xml", "intro": "The importance of self-disclosure-revealing personal or sensitive information to others [1,38]-for mental well-being has been proved in substantial literature [45,61]. For example, through selfdisclosure, people can release their stress [7,18], analyze themselves [44], gain social support [48] and receive professional services [15]. But it is always challenging for mental health professionals by bringing the unique insight that trust may be transferable from human-AI to human-human through AIs.", "relwork": "In this section, we first present literature on how chatbots are a promising technological solution for promoting people's self-disclosure; then, we discuss related work about how people self-disclose with peers through ICTs, e.g., social media platforms. Finally, we provide background work on the remaining challenges of self-disclosure with medical health professionals (MHPs), often without ICTs. With the literature review, we propose our research questions.2.1 Promoting Self-Disclosure to a Chatbot Chatbots have been broadly used in different areas [11,20,26,78]. They can not only help people complete various tasks [70] but can also improve mental well-being (e.g., self-compassion [49]). For example, chatbots are utilized in the workplace to assist team collaboration [70], to improve workers' quality of life and work productivity [78], and to reduce caregivers' workloads [25]. Park et al. [60] adopted Motivational Interview in the chatbot conversation to help users cope with stress and found that their design can facilitate a conversation for stress management and self-reflection. Lee et al. [49] designed a dialog to make the users take care of a chatbot's negative experience. After a two-week interaction with the chatbot, the user's self-compassion significantly increased. These studies have demonstrated the potential benefits of using a chatbot in different purposes, and our research aims at understanding how to use a chatbot to mediate sensitive information. The Computers Are Social Actors (CASA) paradigm indicated that people may apply social norms of human relationships when interacting with computer agents [58]. Thus, research has been focusing on advancing technological contributions for making computer agents to naturally chat and understand people; thus, some studies examined different strategies [6,36] to enhance users' experience when talking with a chatbot. For example, Hu et al. found that the tone-aware chatbot could be perceived as being more empathetic than a human agent [36].People's self-disclosure to chatbots can be used to detect symptoms, identify possible causes, and recommend actions to improve their symptoms by promoting people's self-disclosing, as well as to encourage interviewees to disclose themselves more openly in an interview session [53]. Scholars compared web surveys against chatbots and found that respondents tended to provide more highquality data when using the latter [41]. Fitzpatrick et al. utilized a therapy chatbot \"Woebot\" in their study to explore its feasibility to help reveal people's mental illness; their results showed that the chatbot helped relieve symptoms of anxiety and depression [26]. Additionally, chatbots can be deployed to various platforms using both speech and text; chatbots provide cost-effective [11] solutions for self-disclosure [53,64,80] or deliver education materials for self assessment (e.g., alcohol risks [21]). However, most of the works focus on human-chatbot interactions, and little work studied:RQ1: Do people self-disclose to a medical professional through a chatbot differently from selfdisclosing with a chatbot alone? 2.2 Self-Disclosure with Peers through ICTs Self-disclosure behavior on social network sites has gained the attention of HCI scholars. For example, people freely disclose stress, depression, and anxiety through online social media platforms [3,18,53,84]. It was found that such anonymous self-disclosure with their peers could help users maintain their mental well-being, as they may receive social support from their peers [4]. Similarly, Yang et al. [81] investigated the self-disclosure behaviors of online health support communities, and the study found the members' self-disclosure in private and public channels affected how they reciprocated with other and reached out for social support. Although self-disclosure on social media could help each other seek social support, people naturally avoid revealing their vulnerabilities to others [66], as it might also cause social risks [2,23]. Thus, Andalibi et al. [2] explored how people used throwaway accounts on Reddit to disclose their stigmatized issues (e.g., sexual abuse) and found that people using anonymous means engaged more in seeking support.Through interacting with the chatbot, people can search useful resources, i.e., self-help information, before reaching out for face-to-face counselling [11,20]. Therefore, chatbots have become popular in response to the demand of mental health care in modern society [69]. A recent work shows that chatbots can play a role to inquiry users answering questions and convincing them to share diet information with their family members so as to support each other [54]. Also, coaching apps have been developed, not only for boosting users' awareness of their own mental well-being, but also for helping mental-health professionals gain more knowledge about their clients [40]. In this study, we investigate using a chatbot to facilitate self-disclosure to a MHP:RQ2: What is an effective chatbot design as a mediator for eliciting self-disclosure to a medical professional?", "rq": "RQ1: Do people self-disclose to a medical professional through a chatbot differently from selfdisclosing with a chatbot alone? 2.2 Self-Disclosure with Peers through ICTs Self-disclosure behavior on social network sites has gained the attention of HCI scholars. For example, people freely disclose stress, depression, and anxiety through online social media platforms . It was found that such anonymous self-disclosure with their peers could help users maintain their mental well-being, as they may receive social support from their peers . Similarly, Yang et al.  investigated the self-disclosure behaviors of online health support communities, and the study found the members' self-disclosure in private and public channels affected how they reciprocated with other and reached out for social support. Although self-disclosure on social media could help each other seek social support, people naturally avoid revealing their vulnerabilities to others , as it might also cause social risks . Thus, Andalibi et al.  explored how people used throwaway accounts on Reddit to disclose their stigmatized issues (e.g., sexual abuse) and found that people using anonymous means engaged more in seeking support."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/154.xml", "intro": "A recent report by Grand View Research 1 predicts that the global chatbot market will reach $1.25 billion by 2025. Chatbots have utilized the power of artificial intelligence (AI) for various application domains ranging from customer services [12,40] and healthcare [2,23] to product recommendations [14,43]. In the domain of recommender systems, there are several cases where product recommendations are delivered to customers through chatbots [14,40] with an aim to improve customer engagement. At the same time, a number of research work [18][19][20]22] have emphasized the importance of user control in recommender system.Various studies with critiquing-based recommender systems (CBRS) [7,10,26] have shown the positive effects of increased interactivity on the effectiveness of recommendations. Critiquing is an iterative approach of evaluating the outputs of a recommender system, which allows the system to continuously update the settings and provide users with recommendations that better represent desired outcomes [10]. Figure 1 shows a typical interaction flow of CBRS. CBRS simulate an artificial salesperson who first recommends products based on a user's initial preferences and then shows a new set of products based on the user's feedback (aka critiques), e.g., \"something cheaper\", \"larger screen\", etc. Thus, CBRS are well suited to accommodate user control during the recommendation process.Most existing research studies [9,10] have compared different critiquing techniques with graphical user interfaces (GUIs). However, little work has studied different critiquing techniques with conversational user interfaces (CUIs) that mimic a conversation with a real human either by text or voice. Moreover, it has been shown that personal characteristics such as musical sophistication affect user perception of controls for music recommenders [20]; however, the effects of personal characteristics have not been validated on critiquing techniques yet. To fill these research gaps, this paper compares two typical critiquing techniques with CUIs and investigates how personal characteristics influences user perception and interaction of recommended items (see the dashed lines in figure 1). To achieve these objectives, we implemented a hybrid critiquing-based music recommender MusicBot, which uses a chatbot to enable users to interact with recommendations through both text and voice. The system offers two major critiquing techniques, user-initiated critiquing (UC) and systemsuggested critiquing (SC) to refine the recommendation. UC enables users to construct critiques according to their own needs, while SC generates a set of critiquing candidates for users to choose a desired critique. We then conducted an evaluation with 45 participants using MusicBot in a within-subject design. We raise three research questions for evaluating critiquing-based Music recommenders particularly with a conversational user interface (CUI).RQ1: Which critiquing setting, UC versus HC, is better suited for controlling music recommendations?RQ2: Which personal characteristics (e.g. musical sophistication, desire for control, chatbot experience, and tech savviness) might influence user's perception and interaction of recommendations?RQ3: Are critiquing techniques perceived as useful in low-involvement product domains as in high-involvement product domains?Our main contributions are four-fold:(1) We demonstrate a multi-modal (text and voice) conversational music recommender that incorporates both a userinitiated critiquing technique (UC) and a system-suggested critiquing technique (SC). We then employ a mixed qualitative and quantitative research method to compare UC with a hybrid critiquing technique (HC) in terms of subjective user experience (UX) with recommendations. Overall, recommendations generated by UC and HC were perceived at the same level, while users tend to need more effort to find a song using HC. (2) We find that two personal characteristics, desire for control and musical sophistication, positively influence several key UX metrics of recommendations such as interest matching, intent to give feedback, and perceived controllability.(3) Our study also verified the usefulness of critiquing techniques in a low-involvement domain of music recommendations. (4) Based on the findings in this study, we proposed specific design suggestions for critiquing-based recommender system with conversational interaction. This paper is organized as follows: We first introduce related work, followed by the design and implementation of MusicBot.We then present the quantitative and qualitative results of a user study. Finally, we conclude with a discussion of study findings and limitations.", "relwork": "In the following sub-sections, we review previous work that are closely related to our research.", "rq": "Overall, the subjective responses suggest that both UC and HC are useful for users to find good quality songs, but HC may increase user engagement and possibility to find diverse music. Thus, we answer the research question RQ1: Which critiquing setting, UC versus HC, is better suited for controlling music recommendations?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/54.xml", "intro": "Motivating people to change their attitudes and engage in healthy behavior is a difficult and important task. The pervasive use of computers and mobile devices paves the way for using software applications as a scalable means to help individuals follow health guidelines and attain their goals. Accomplishing this successfully requires creating interventions that are tailored to the individuals' characteristics and circumstances [1,2]. A common method for tailoring is identifying a user's stage of change using the transtheoretical model of health behavior change [3]. Stage-specific techniques can then be used to motivate people towards behavior change or behavior continuity.Motivational interviewing (MI) is a counseling method used to enhance a person's motivation for change [4] and has a variety of techniques that are used at different stages of change. The main goal of MI is for counselors to help their clients resolve their ambivalence about their current behavior and get them to consider behavior change. The counselor elicits this change-talk from clients using particular strategies, such as listing the pros and cons of their behavior, talking about their current level of motivation and confidence to change, and encouraging them to speak freely without being judged.Given the effectiveness of MI [5], creating systems capable of conducting MI sessions automatically would be beneficial. Embodied conversational agents (ECAs) are virtual agents that simulate face-to-face interactions by having a human embodiment and exhibiting non-verbal behaviors [6], and thus can be a vehicle for driving digital counseling interventions for health behavior change. However, since MI relies on open elicitation from clients, designing automated conversational interventions that implement MI faithfully is challenging [7]. Some researchers have implemented an ECA system that uses a few techniques from MI that work with constrained input, e.g., only allowing users to select what to say from a multiple-choice menu [8], but these are very limited in functionality.In our current effort, we developed an ECA that promotes two health behaviors-physical activity and fruit and vegetable consumption-using MI to increase motivation and confidence to change. We address these behaviors due to their wide applicability and because most American adults struggle to meet the Center for Disease Control's general recommendations, e.g., 23% of American adults meet the recommendations for physical activity [9] and more than half of Americans consume less than the recommended daily servings of fruits and vegetables [10].The ECA makes use of fully-constrained user input to the MI counseling conversation. Rather than being a limitation, constrained interfaces provide opportunities to explore novel methods for bringing about attitude change, for example, by only allowing users to express change-talk at key moments of the session. This approach is based on the phenomenon of opinion change following forced compliance, a derivative of cognitive dissonance theory [11] studied in the field of behavioral psychology. Researchers found that people who were forced to advocate for an opinion they did not hold (e.g., to come up with and rehearse counter-arguments) shifted their private views towards the position they advocated for, particularly when offered low extrinsic reward [12]. In our case, we evaluate the effect of constraining users to only being able to express motivation and confidence in their ability to change or maintain the target health behaviors, compared to a less constrained condition in which they are provided with both positive and negative statements.To explore these ideas and evaluate the interventions, we conducted a study to address the following research questions: RQ1: Can we implement an ECA that uses MI techniques to bring about attitude change towards physical activity and fruit and vegetable consumption? RQ2: Is the agent accepted by users and will they endorse its future use? RQ3: Does the interaction lead to increased confidence or motivation to change? RQ4: If we force them to endorse change-talk at key moments in the dialog, does that boost this effect?", "relwork": "Digital health interventions are increasingly used to promote health behavior change and a growing number of these interventions are implemented based on theoretical foundations. In a review of digital online interventions that promote health behavior change, Webb et al. found that interventions based on theory and those that incorporated more behavior change techniques tended to have larger effect sizes than ones that did not [13]. Among the top theories and behavior change techniques, with respect to effect size impact, were the transtheoretical model and barrier identification coupled with problem solving, an important MI technique.There have been several attempts to build automated health behavior change interventions that incorporate elements of MI, are deployed using ECAs, or both. One such example is the MAPIT program, which is a web-based intervention to increase motivation for substance abuse treatment among clients in the criminal justice system using illicit substances. The program uses the extended parallel process model, MI, and social cognitive theory in two sessions. The first session aims to motivate clients to complete probation, change their substance use, and obtain HIV care. The second session, 30 days later, focuses on goal setting, coping strategies, and social support. Participants in a pilot test were generally positive about the program's features, felt that it would help them be more successful on probation and in treatment. They appreciated that the system tailored content to them, it could display personal and population statistics, and give them insight into other people's reasons for completing probation [14].Another research-driven digital intervention combines MI and cognitive behavioral tools to create a self-guided supportive coaching experience for improving the mental health and wellness of its users. The system is a texting platform that leverages artificial intelligence and natural language technologies to conduct its interactions with users. In a feasibility study, 95% of users reported improvements in their mental wellbeing, though the particulars of how the system uses the counseling methods is not reported [15].With recent advances in speech recognition and machine learning technologies, using conversational agents for healthcare that allow natural language input has increased; however, their effectiveness remains unclear. In a review of 14 systems that allow natural language input (written or spoken), only one system was found to have a significant effect on participants' health [16]. The same review found that approximately 70% of the conversational agents suffered from problems with language understanding and/or dialog management [16], threatening user safety [17].Several researchers have evaluated the use of constrained input ECAs and social robots to drive automated MI sessions. Schulman et al. (2011) implemented a model of MI dialog into the dialog manager of an intelligent conversational agent to promote exercise and healthy eating behavior. The model contained adjacency pairs (agent utterance and user options) for each type of MI-specific dialog act that can be enacted depending on the context at any given time in the conversation. In a formative evaluation, users rated the system highly on satisfaction [8].In an ECA-based intervention delivering a brief MI for reducing alcohol consumption, Lisetti et al. (2013) found that an empathic ECA was rated significantly higher than a text-only system on several measures of usability and user experience, while a nonempathic ECA showed fewer significant results [18]. This finding is congruent with the emphasis that is placed on using empathy in MI [4]. These findings on using MI to promote behavior change through constrained interactions with ECAs and social robots [19] have shown that MI can increase satisfaction and usability of these systems. Our current effort focuses on measuring the effects of an interaction with ECAs on behavior change attitudes and explores the additional effects of being coerced into change-talk.", "rq": "To address our research questions, we conducted a one-factor counterbalanced within-subjects pretest-posttest experiment in which participants conducted a single brief counseling session with an ECA. For all conditions, we assessed change in attitudes about the target behavior before and after the intervention. The withinsubjects manipulation had two conditions: one restricting participants to choose only positive options when they are asked to express their confidence and motivation levels (Coerced), and the other providing a range of both positive and negative options (NotCoerced). Figure  shows an example of the user being asked about their level of motivation towards a particular behavior, with a user menu as it appeared in the Coerced condition (left) and the NotCoerced (right). The treatment order and the association of manipulation with ECA and specific health behavior (exercise or nutrition) were counterbalanced."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/155.xml", "intro": "", "relwork": "Early research in creating believable conversational agents were largely motivated by the Imitation Game (also known as the Turing Test), proposed by Alan Turing in 1950. To maintain grammatical correctness and produce logical responses, early conversational agents, known as chatterbots, relied mostly on programmer-defined pattern matching, conversational networks, and activation networks, which sometimes were able to obtain limited success in restricted Turing Test evaluations [22]. ALICE (Artificial Linguistic Internet Computer Entity) introduced a new markup language for programmer-defined conversation knowledge and responses called AIML (Artifical Intelligence Markup Language) supporting XML definitions of categories, patterns, and templates [28]. Current commercially-available chatbot software such as Cleverbot utilize similar approaches to rule-based conversation generation, augmented with mechanisms for learning new response patterns from conversations. More recently, the focus of conversational research has shifted away from solely generating convincing dialog and towards the creation of functional natural language interfaces and conversational modeling.Recent work has shown that neural language models and recurrent sequence-to-sequence models are able to encode limited conversational context a viable approach to language modeling with a large, unstructured corpus [27].To facilitate more realistic responses, other cues such as affect and contextual understanding can be used during response generation. Conversational models that encode affective signals such as user satisfaction and emotional state have shown to be effective for more believable conversational agents and low-perplexity language models [12,14].", "rq": "Within this work we evaluated our Neurally Animated Dialog Agent NADiA and compared its performance to both a human reference and a state of the art baseline both using subjective as well as objective evaluation criteria. We identified (RQ1) how well NADiA can understand its human interlocutor, (RQ2) how anthrompomorphic NADiA is perceived, and (RQ3) how pleasant the conversation with NADiA is perceived. For all three research questions we found encouraging results and could show that the here proposed NA-DiA architecture has potential to act as an enjoyable and empathic conversation partner. For future work we seek to improve its longterm memory capabilities by complementing the neural language generation module with a dedicated memory network and seek to improve its listening behavior by training a neural network that does not simply mimic the facial expressions of the human user. Overall, we believe that artificial conversational agents still have a long way to go to replace interpersonal human contact. However, we show that artificial neural architectures have the ability to uphold the illusion of understanding and some anthropomorphic characteristics during brief conversations."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/93.xml", "intro": "After an American actress Alyssa Milano tweeted a request to those who have been sexually harassed or assaulted to reply with \"me too\", 1.7 million tweets were made within ten days in at least 85 countries [44]. According to a survey in 2018, 81% of women and 43% of men in the US reported experiencing sexual harassment or assault in their lifetime [29]. Incidents of SH continue to be reported on a regular basis and many organizations are seeking ways to build a proper anti-harassment culture such as offering repeated and mandatory SH prevention training. A typical training program is comprised of presenting information and resources online with case studies and evaluating learner's comprehension using surveys and quizzes. In-person classes are sometimes held to supplement the online training and discuss SH issues [53].Despite prevalent training, recurring SH incidents are raising doubts about the effectiveness of existing SH prevention programs [21,46]. Some researchers assert that current methods have little impact on changing one's actual behavior [21,47] and may even have the opposite effect [9,12,52]. As our results will later show, the existing designs are perceived to be tedious, overwhelming, uncomfortable to express honest opinions, and un-motivating. Our work envisions a new class of interactive training delivered by an intelligent conversational agent. This paper progresses toward that vision by providing a realistic experience of having a conversation with an intelligent agent representing a person who has been sexually harassed.In this paper, we explored the design of a text-based conversational interface (CI) to incorporate design principles that underlie effective SH training. We derived three key principles from the literature about how to design effective SH prevention training [16,45,48,49,53]: 1) Foster empathy towards SH targets through the use of first-person narratives, 2) use interactive and experiential methods (e.g., role-play scenarios), and 3) utilize synchronous delivery methods (e.g., online chat). While prior studies in the design and education research communities have tested different subsets of these principles [16,20,32,37,58,63], we designed and implemented a CI that demonstrates a novel synthesis of all three principles for SH prevention training. Our proof-of-concept interface was designed to have a persona of a woman, named Jane, who has been sexually harassed in the workplace and engages the learner in a conversation about her experience from a first-person perspective.We conducted a mixed-methods study (N=32) to explore the benefits and limitations of the CI design for the purpose of SH prevention training. Participants were randomly divided into two groups, either interact with our interface (CI group) or read the same vignette on a web page (Control group). In both groups, we measured empathy using an 8-item scale [5] and Inclusion of the Other in the Self (IOS) scale [4], and SH attitude using Sexual Harassment Myth Acceptance (SHMA) scale [39] to evaluate how experiencing the vignette through the interfaces affects learners' empathy towards the target and attitude towards SH. We interviewed participants and extracted the themes that emerged from their responses.We compared the themes between the CI and the Control group and identified the themes that appear in the CI group only. The participants in the CI group reported feeling engaged due to the designed interactivity (N C I =14). Participants also reported that reading individual messages was less overwhelming than being presented with an entire article and created suspense of how the conversation would unfold (N C I =9). Participants appreciated the CI being a realistic simulation because this allowed them to feel comfortable discussing a sensitive topic (N C I =8). They also felt immersed in the situation and motivated to help the target (N C I =15). These benefits favorably contrasted with the limitations of the traditional SH training that participants had previously experienced. In contrast to our expectation that the virtual interaction with a SH target would increase empathy, the quantitative results revealed that there was no significant difference between the conditions on empathy.Our study makes three contributions. First, we identify design principles from prior literature for effective training and demonstrate how to implement a subset of these principles within an intelligent CI. Second, our results provide a deeper empirical understanding of how our interface design affects a learner's experience relative to the status quo approach for the purpose of SH training. Lastly, we provide design implications for building intelligent interfaces that aims to arouse empathy and support experiential learning. Our work is original because we reveal insights on how our interface can complement the current practices through systematic analysis, and initiate thought-provoking discussions on how to improve the proposed design and the training. We anticipate that the results and the implications generalize to other training programs (e.g., ethics, inclusiveness, and security training), contexts dealing with sensitive issues (e.g., stigmatic diseases), and domains that value empathetic responses from users (e.g., medical crowdfunding).", "relwork": "SH refers to unwelcome sexual advances, requests for sexual favors, and other verbal or physical harassment of a sexual nature [15]. Effective interventions are critical for reducing the prevalence and severity of SH. Our research focuses on advancing the use of training as an intervention for SH prevention [15]. Our work complements other HCI research and interventions that encompass sexual misconduct problems including dating and domestic violence [18], stalking [11], and online harassment [17].", "rq": "For the second research question, we did linear mixed-effects analyses using lmerTest package in R  with survey results. We used abbreviated LSH scale  to measure pre-existing attitude towards SH before the experiment. After the experiment, we measured attitude towards SH and empathy towards Jane. We used Sexual Harassment Myth Acceptance (SHMA)  (\u03b1=0.87) to measure attitude towards SH. SHMA is a comprehensive scale that contains subtle misconceptions about SH and used to measure attitudes and beliefs that serve to excuse sexually harassing behaviors. The reason we used LSH and SHMA scales is that they are the most frequently employed attitudinal measures of SH , and were also used in the most related prior work . We measured empathy using an 8-item scale of empathy-related reactions  (\u03b1=0.81). We also measured empathy during the interview using the Inclusion of the Other in the Self (IOS) scale  which we show seven Venn diagrams of two circles, each circle representing Jane and the self, in different degrees of overlap. We asked respondents to select a diagram that best describes their relationship with Jane and why."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/78.xml", "intro": "The productivity of information workers can be significantly influenced by information overload and stress. Due to the ubiquity of multiple devices, including desktops, laptops, phones, surfaces, smart watches and speakers, notifications, messages and other kinds of disruptions have become a serious problem for keeping focused on high priority tasks at work. A large body of work in human-computer interaction (HCI) research has concentrated on better understanding how workers manage their tasks, attempt to stay focused, and deal with distractions and interruptions throughout the day. In any given day, information workers are typically faced with multiple tasks that they need to complete, and often they devise unique strategies to remember and make note of these tasks [12,15,25]. A diary study of information workers found that workers had an average of 50 task-shifts over an entire work week [9]. Furthermore, information workers are usually faced with numerous interruptions throughout their day.At work, people average 40 seconds on a computer screen before switching [23], and it can take several minutes for an office worker to return to their original task after interruptions. The deleterious effects of notifications, email and face-to-face interruptions has been very well documented [9,20] in terms of lowered productivity at work. Different factors like the type and duration of the interruption, the complexity of the task prior to interruption, and even the exact moment of the interruption can have a negative effect on workers' ability to resume a task, their perceived productivity and their satisfaction with their performance [2,5,25,30,38]. For instance, past work from Iqbal and Horvitz [14] found that when information workers were interrupted by conversation, they were more likely to return to work on more peripheral tasks like email and web searches, rather than resume their previous task.Research suggests that more distractions can lead to higher reported stress and lower productivity in the workplace [20][21][22]. While there have been many attempts to design software to assist with this problem in order to reduce stress and improve focus (e.g., Freedom, Windows FocusAssist [29], Tracktime [4], see [16] for a review), few of these products have yet to be widely adopted in the Figure 1: We present two productivity agent prototypes which help users schedule and block time on their calendar to focus on important tasks, monitor and intervene with distractions, and reflect on their daily mood and goals. Snapshots of the sample morning dialogue interactions for the text-based (left) and virtual agent (right) prototypes are shown. The face images are blurred to respect privacy.workplace. Therefore, people continue to design their own methods and workarounds, but report having trouble nonetheless [16].These findings suggests that maintaining focus in the face of constant shifting priorities and interruptions in the workplace is a complex and important problem. Yet, to our knowledge, only one system, from Kimani et al. [16], has yet been designed that helps knowledge workers with prioritizing their work, providing reminders on when to switch tasks or get back on task, to take breaks, as well as to reflect on how much they accomplished at the end of the day. While this effort was notable for its complex system design, the user interface was quite simple in its design, similar to a standard chatbot. In our work, we design two different agent prototypes which build upon this past work from Kimani et al. [16] (described further in the related work). Our paper offers three main contributions: 1) The design of two different conversational agents, one text-based (TB), similar to a chatbot, and one virtual, embodied, conversational agent that responds to the user's emotion (VA), 2) a longitudinal evaluation of these two agents against a shipping product that tries to help users schedule focus time in Microsoft Outlook, and 3) actionable insights from qualitative analysis of user feedback on agent design around anthropomorphism, user task scheduling, and the need for better back and forth negotiation and control with the user.", "relwork": "For task and time management, various different applications have been developed, most notably, MeTime [35], RADAR [10], TaskBot [34], and Calendar.help [8] that each aim to assist users be more efficient in managing their work through different approaches. MeTime aims to provide real-time awareness of how users are spending their time through graphic visualization of their application usage in order to promote efficiency of time use achieving their task goals [35]. The authors showed that exposure to meTime decreased time spent on distractions (e.g., social media), and increased their feelings of productivity [35]. RADAR is an AI system that was designed to help reduce email overload by automatically producing tasks and to-do lists directly from users incoming emails [10]. Through an experimental setup, users that were aided by RADAR that were confronted with overload performed better at completing email intitiated tasks than without RADAR. [10] TaskBot, a chatbot agent was designed to mediate the creation and management of tasks within project teams [34]. Users found TaskBot useful for naturally transferring conversations into actionable tasks, but found that the system struggled when having to deal with multi-threaded conversations [34]. Finally, Calendar.help used an AI interface integrated with the user's email to automatically schedule user's meetings, and allowed the user or another human assistant to make adjustments manually when needed [8].Applications developed within this domain have also aimed at helping block distractions to help users focus. One such example is Microsoft Focus Assist, which allows users to define \"whitelisted\" work related sites and applications and \"blacklisted\" non-work related sites and applications [29]. Focus Assist also then blocks access to non-work related sites and applications for a period of time determined by the user [29].Finally, other applications have aimed at promoting workers to reflect on their emotions and goals throughout the workday, as well as to promote saving time for healthy and appropriate breaks. For instance, Robata was developed to be voice mediated agent that helped its users plan and organize their tasks, reflect on their motivation and satisfaction throughout the day, and also promote and reflect on their self-learning [18]. Users of Robata reported generally appreciating a way of reflecting on their planning and goal setting [18]. BreakSense is another application that was developed to help promote mobility for its users during their breaks in order to encourage healthy activity and lifestyle in office environments that can be often very sedentary [7].Although each of these applications attends to a specific problem area within the broader domain of promoting workplace productivity and well-being, there has been little research on the development and evaluation of AI systems that incorporate task scheduling and management, distraction blocking and monitoring, and mood and goal reflection in a single, standalone application. To our knowledge, the only application that has attempted to integrate all these different functionalities into a single system is the work of Kimani et al. [16]. In their work, they presented Amber, a conversational desktop assistant whose purpose was to help workers in four main areas: (1) scheduling high priority tasks, (2) aiding workers in transitioning from one task to the next, (3) avoiding and intervening with distractions, and (4) reflecting on their work through a conversational AI interface [16]. In our work, we present our productivity agent, which builds upon the infrastructure and capabilities of this previous agent developed by Kimani et al. [16]. Similar to the work from [16], we gave our agent a female gender.Our work extends this previous work in a number of ways. First, as Kimani et al. [16] found in their analysis that users desired an agent that took more control over the flow of interaction, we designed our agents to provide a series of dialogues that started at the beginning of each day with scheduling their tasks, helping users progress through these tasks until the end of the day. These dialogues were initiated automatically when appropriate without any extra user input. Second, in addition to incorporating the four major functionalities from Kimani et al. [16], we also wanted to investigate the impact that a more human-like agent with more emotional intelligence would have on user perceptions, as well as their focus and productivity. In past studies of virtual agents, research has found that emotionally appropriate responses from an agent contribute to a more positive and satisfying experience during interaction [11,36]. Furthermore, emotional intelligence in agents has been shown to help alleviate negative emotions, like frustration [17]. In a study of the effectiveness of different agent characteristics in an organizational setting, anthropomorphic appearance in agents was also shown to increase users perceptions of the agents usefulness [31]. As little research has investigated the effect of more human-like agents in the context of workplace focus and productivity, a primary goal of our study was to evaluate and compare its effectiveness to a typical chatbot agent. Therefore, in this work, we build two different prototypes of our agent: a text-based conversational interface prototype (TB) that employs a similar UI to the previous version developed by Kimani et al. [16], and a prototype virtual agent conversational interface (VA) version with a video avatar that speaks to the user. The VA prototype incorporates the ability to detect user emotions through video input and adapt its responses to be appropriate and congruent with the users emotional state. We conduct a three week long within-subjects user study to evaluate and compare the VA prototype, the TB prototype, and a simple, non-conversational task scheduling tool integrated into users' email, similar in part to Calendar.help [8], as a control. To our knowledge, our work presents the first multi-week user study evaluating different intelligent agent prototypes that aim to increase focus and productivity at work.", "rq": "We used a combination of behavioral logs from our sensing software and survey responses to address our research questions: RQ1: To determine if users were more productive and less distracted during the time periods they scheduled tasks with the agents (which we refer to as focused tasks), we use windowing activity data collected from our sensing software to determine the proportion of time where productivity and distraction applications (apps) and websites (sites) were the active foreground window during users focused tasks, and in time periods outside of focused tasks (which we refer to as outside focused tasks). In addition, we also used the face detection data from our sensing software to determine the proportion of time that the user was detected to be present at their workspace during their focused tasks and outside focused tasks."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/92.xml", "intro": "Developers of conversational systems are diverting an increasing amount of attention towards designing conversational agents that are more human-like. This includes incorporating and maintaining consistent personalities as is done for most commercial chatbots [19,42,64,65], mimicking nuances and quirks of human communication (notably the now-famous Google Duplex \"uh-hmm\") [62], but more broadly, retrofitting behavioral intricacies of human-human communication to human-agent communication. However, exploring the entire space of behaviors that emerge in human conversations is prohibitively large and consequently, research, with this thrust towards anthropomorphization, focuses on iteratively incorporating more of such nuances into agent design.A major challenge in making text-based agents more human-like is that humans are adaptive and flexible in conversations, and conversational behavior is largely dictated by the interests of people involved, their shared context, their social distance, and the cultural contexts. While humans are accustomed to adapting to people and contexts, agents need special provisions for such adaptations. Along these lines, work has looked at how responses of chatbots can be conditioned on information about individual users [35,40], how often agents should switch topics and ask questions [57], and how more diverse responses can be generated [7]. The overarching theme, in most previous work, is that content in human conversations is dynamic and we need to explore how to build agents that can adapt and navigate through a range of topics, subject to the user and context. While these approaches try to capture variations in \"what\" a chatbot should say at a particular turn in the conversation, they leave unexplored the question of \"how\" a chatbot should phrase the message.In order to move towards the goal of building human-like chatbots, we argue that it is insufficient to look at content alone. The fact that style in human conversations is dynamic too needs to be addressed. Human languages are rich and provide us several ways (styles) to express the same thought-we can choose to be more formal and ask, \"Can you give me the directions to the restaurant, please?\" or assume a more casual tone-\"uh, directions?\". We can choose to be more or less verbose, personal or impersonal, introduce varying amount of humour and possibly make infinitely many such choices in how we want to communicate our thoughts. Context, both interpersonal and cultural, finds its way into not just the content of conversations but also the style of utterances. Should we then design chatbots that are mindful of style and not just content, in a conversation? Would this make our chatbots more human-like?A form of stylistic variation that has assumed special importance is that of language mixing. Speakers in multilingual societies have been known to fluently and frequently alternate between languages they are proficient in-a phenomenon known as code-mixing (CM) [30,44]. Despite an increasing number of users of conversational systems coming from multilingual societies [2], conversational systems still remain predominantly monolingual, even as they are being developed in new languages [58]. In such multilingual markets, does a monolingual conversational system, capable of handling interaction in one language that the user speaks, suffice? It becomes important to understand the need for conversational systems to move beyond monolingual interactions towards such mixed language interactions. Backed by our reasoning about the importance of style, we seek to understand if chatbots that can handle code-mixing will be perceived as more human-like by such users. This paper presents a user study with the following question:Research Question: Do multilingual users prefer chatbots that code-mix systematically, and if so, what is the optimal way to code-mix systematically?A prominent theory that seeks to explain the dynamics of style in human conversations is the Communication Accommodation Theory (CAT) [28,29] which posits that speakers, in co-operative communication, end up reciprocating each others' styles, attempting to converge to a common style -a process called accommodation. A computational study of style-accommodation [20] shows that style-accommodation is highly prevalent and exhibits complexity in Twitter conversations. In this paper, we explore whether chatbots should accommodate for the CM of the individual users they interact with. We evaluate if users express a marked preference for a chatbot that reciprocates their CM over an otherwise similar chatbot that does not accommodate for their CM, the difference being only in the surface form realizations of the response. We choose to focus on users in India for pragmatic reasons including proximity and the presence of a large population of multilingual users. 10.6% of the population of India is English literate with 6.8% reporting it as their second language and 3.8% reporting it as their third language [27]. Given a smartphone penetration rate of over 25% [1], there is bound to be a sizeable population of multilingual users [36]. Thus the Indian market has been the focus of other research in multilingual interfaces as well [10,36]. Since Hindi is the most widely spoken language in India, with English being second, we choose to work with the frequently-mixed English-Hindi (En-Hi) language pair while studying preferences of participants from India.We devise two prototype CM policies-one that always code-mixes and another, rooted in the CAT, that does so intelligently by nudgingimplemented as algorithms that take into account the user's CM. These CM policies run in parallel with the conversational system's response generation system and when both the response and the CM policy output are ready, a paraphrasing system introduces CM in the response in accordance to the policy's output. Using a human-in-the-loop prototype of our conversational system, we carry out a study with 91 participants from India to understand the impact of CM accommodation on user evaluations of the conversational system.We find that participants rate bots that can code-mix higher than a monolingual English bot, in terms of human-likeness and conversational abilities. We observe that individual differences between participants are a strong predictor of their evaluations. Participants with higher fluency in the two languages, tend to reciprocate the chatbots' CM more often and evaluate the CM chatbots more favorably than participants that don't reciprocate the chatbots' CM. Similarly, when participants perceived that the chatbot was reciprocating their CM, they evaluated the chatbots' CM as more natural. Between the Always CM and the Nudge policy, we find that the Always CM policy is a high risk-high gain choice. On the other hand, the Nudge policy is a lower risk design choice with slightly lower but more consistent ratings across users.The primary implication of our findings, for the design of chatbots that code-mix, is that in the absence of knowledge about the users' fluency and attitudes towards CM, it is better to adopt the Nudge policy while the Always CM and monolingual English systems are a good fit when it is, a priori, known whether or not the users are fluent in the constituent languages and have a positive predisposition towards CM. Our results galvanize the need to divert increasing attention towards developing language understanding and generation systems for CM language due to the unique utility these serve in a conversational setting. The accommodation policies devised in this work also extend to style dimensions beyond just CM and can be incorporated into systems that accommodate for politeness, formality and similar style choices.", "relwork": "Our work is situated in the context of previous efforts that seek to understand how humans communicate with technological systems, how language technologies have evolved, how norms from human-human communication extend to human-computer interaction, and how to develop systems that are informed by all these in the specific domain of mixed language technologies.We give an overview of themes most relevant to our work, from literature in linguistics, humancomputer interaction and natural language processing.", "rq": "The pipeline for all the three variants has a human in-the-loop, but the complexity of work that a wizard performs per turn varies between the mixing and baseline policies. The average response time is thus prone to vary between the three variants. However, since we are interested in investigating if there is utility in chatbots code-mixing and if there is merit in directing efforts towards development of fully-automated code-mixing chatbots, we want to isolate out the effect of the various language mixing policies on user evaluations. Thus response time becomes a confounding factor to be accounted for. During our pilots, we recorded the average response times of the wizards in the CM policy settings, and used the larger of the two (the Nudge policy is slower to implement in real-time) as a minimum delay to be introduced. This approximate threshold delay (20 seconds) is then applied to each turn in the faster variants, so even if the system is ready with a response before this minimum threshold, the system waits for the remainder of the time before spitting out a response. While response time does affect user experience, we want to be able to take a peak into a future where all these variants are automated, in order to study our research question-to determine if such a future is worth developing by investing resources and efforts. So, we want to minimize the effect of response time, and only ascertain the merits/demerits of the language mixing itself. Thresholding the delay provides us a glimpse into that future at the cost of sub-optimal user experience which is uniform across the variants being studied. We further discuss the implications of thresholding the delay in the discussion."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/47.xml", "intro": "", "relwork": "The problem of reducing the complexity of performance analysis has been tackled previously. Solutions have been proposed for abstracting the process of problem stating from choosing, configuring, and running an appropriate performance analysis approach [28,34]. Especially in the field of application performance management [12], automatic analysis of the results is becoming an important requirement for tools that aim to be market leaders. The issue here is that these approaches still require significant prior knowledge in the field, as well as experience to analyze the results. Some works have been proposed for integrating performance-awareness into development environments, e.g., to assess and display the performance (impact) on code changes using model-based and measurementbased approaches (e.g., performance tests and production monitoring) [3,7,9,14]. However, these approaches do not focus on configuring the performance evaluation and do not include the use of conversational interfaces such as chatbots. Generally, also visualizations make performance analysis more accessible; various specialized approaches have been suggested [15], but none for visual reporting of load tests or similar scenarios. Chatbots (also called conversational interfaces or natural language interfaces) are gaining popularity in many applications and have already been applied for data analysis. Not a full-fledged conversational interface, but a popular example that answers data-related questions phrased as keywords or a sentence is Wolfram|Alpha; it often provides a mix of visualizations, tables, and lists as a reply, but actual conversations are not possible. Also, general-purpose search engines like Google answer more and more queries directly or by providing visualized data-snippets (e.g., for \"How many people live in California?\"). Srinivasan and Stasko [32] provide a short overview of natural-language interfaces that are combined with visualizations for data analysis-systems mostly focus on specifying a visualization and trigger visualization-related interactions. A recent example that covers both aspects is FlowSense [37] where users can both generate visualizations (e.g., \"Show a scatterplot of 'mpg' and 'horsepower'. \") and control interactions (e.g., \"Highlight the selected cars in a parallel coordinates plot. \"). Such systems are different from our approach that specifies a data-generating test scenario but does not immediately control the visualizations.Also, software engineering research has started to investigate the application of conversational interfaces in the context of the development process. The BotSE 2019 workshop (co-locacted with ICSE 2019) [30] discussed the usage of bots (chatbots being a subtype of these) in software engineering. Chatbots have been tested or discussed, for instance, to find experts for a certain code artefact [6], to help avoid potentially conflicting code changes [24], or to support project meetings with background information on the development [16]. Regarding the visualization of software dependencies, chatbots can help to select and filter elements [5]; a natural-language interface (here, also involving speech recognition) is particularly useful when software visualizations are presented in virtual reality, and hence, text input gets harder [29].In addition to the chat interface, our approach provides the analysis results as a detailed interactive report. It is inspired by previous work that uses natural language generation (NLG) [10,27] to generate documentation of software or to report data analysis results. For instance, there are approaches that textually summarize source code [17,18,31], commits and releases [8,19], or characteristics of executed tests [25]. Some approaches combine the generated texts with interactive visualizations similar to our reports, for instance, to describe runtime information of a method [4] or to summarize different aspects of code quality [20]. In an earlier work-in-progress publication [23], we already sketched a reporting framework (Vizard), which we now integrate with the chatbot interface.", "rq": "RQ1: How does PerformoBot help participants to create and execute a load test?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/185.xml", "intro": "", "relwork": "Although research showed that longer queries could produce better results for information seeking tasks, e.g. [6,7], people usually tend to use short search queries [3]. There are many approaches to support users in finding relevant information, e.g. through facets, recommendations, implicit and explicit user feedback. However, only few works have tried to motivate users to type in more query terms and thus provide more detailed information about their initial information need. Belkin at al., for example, showed that a queryentry box with several lines led to longer queries than a line mode search bar [6] and that query lengths were significantly longer when the query box was labelled with \"Information problem description (the more you say, the better the results are likely to be)\" than when it was labelled with \"Query terms\" [7]. Furthermore, they found that longer queries significantly increase searchers' satisfaction [7]. In contrast, Agapie et al. [1] found that telling users that longer queries deliver better search results does not influence query length. However, they showed that using a coloured halo around the search bar motivates searchers to provide significantly more query terms in a complex Web search scenario. Hiemstra et al. [10] evaluated the proposed halo effect in a website search system in a 50-day A/B test (N = 3506) but could not confirm the positive impact on query length. They conclude that this approach might be sensitive to the search task and search context. Kelly and Fu [12] show that additional information (domain knowledge, the information need, and search motivation) help to increase the retrieval performance. Likewise, Bendersky, Croft and Bruce [8] propose a machine learning method to extract the key facts from long queries. Their system performs better on longer natural language queries as compared to shorter, keyword-like queries.A reliable information need elicitation is getting more critical with the increasing use of voice assistant systems. Without a graphical user interface, refining the search via facets and exploring the results and recommendation lists becomes cumbersome. Research in the context of conversational search has explored asking clarifying questions [2] or coached conversational preference elicitation [15]. With one good question, Aliannejadi et al. [2] improved the retrieval performance by over 150%. Focusing on conversations, however, requires processing natural language (with challenges such as vague language and ambiguity [4]), which is, so far, not supported by common product search engines.", "rq": "To answer the research questions, we design interfaces based on cues from prior literature and evaluate them in an online study with a between-subjects design. The interface designs, questionnaire, and annotation guidelines are available online 1 ."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/191.xml", "intro": "In many domains, including human-computer interaction (HCI) research [62], conducting surveys is a key method to collect data. With the widespread use of the internet, self-administered 15:2 Z. Xiao et al. online surveys have replaced old-fashioned paper-and-pencil surveys and have become one of the most widely used methods to collect information from a target audience [27,30]. Compared to paper-and-pencil surveys, online surveys offer several distinct advantages. First, an online survey is available 24x7 for a target audience to access and complete at their own pace. Second, it can reach a wide audience online regardless of their geographic locations. Third, online survey tools automatically tally survey results, which minimizes the effort and errors in processing the results.Due to the extensive use of online surveys, survey fatigue is now a challenge faced by anyone who wishes to collect data. Research indicates two typical types of survey fatigue [69]. One is survey response fatigue. Since people are inundated with survey requests, they are unwilling to take any surveys [70]. The other is survey-taking fatigue. Evidence shows that as a survey grows in length, participants spend less time on each question and the completion rate also drops significantly [8,71]. For example, one of the biggest survey platforms, SurveyMonkey, shows that on average, participants spend 5 minutes to complete a 10-question survey but 10 minutes to finish a 30-question survey. 1  The problem is exacerbated with open-ended questions because of the extra time and effort required for formulating and typing responses to such questions [8,17]. Open-ended questions are an important method to collect valuable data and are widely used in self-administered online surveys [62].In particular, open-ended questions allow respondents to phrase their answers freely when the options of responses cannot be pre-defined or the pre-defined responses may introduce biases [17,47]. Moreover, open-ended questions help collect deeper insights, such as the background and rationales behind the answers [17,80]. However, open-ended questions often induce more cognitive burdens and survey-taking fatigue, and respondents are more likely to skip such questions or provide low-quality or even irrelevant answers [8,17,73]. Consequently, surveytaking fatigue adversely affects the quality and reliability of the data collected especially when open-ended questions are involved [17,18,64].To combat survey taking fatigue especially to motivate and guide survey participants to provide quality answers to open-ended questions, several approaches have been proposed. One set of proposals is to inject interactive features into an online static survey, such as providing response feedback [18] and probing responses [64], to improve response quality and encourage participant engagement. However, no existing survey platforms support such interactive features nor do they automatically motivate and guide survey participants to provide quality answers to open-ended questions during a survey.A lack of support of such interaction features on existing platforms may be due to two main reasons. First, it is difficult to automatically interpret participants' natural language responses to an open-ended question due to the diversity and complexity of such responses [17]. For example, when asked \"What do you think of the product,\" participants' responses could be: \"N/A,\" \"I don't know,\" or \"Although I've heard of the product, I've never used it so I don't know what to say.\" Interpreting such highly diverse or complex free-text input requires sophisticated natural language processing algorithms, which is a non-trivial task [34]. Second, even if a system can interpret participants' free-text responses to open-ended questions, it is difficult to manage potentially complex interactions based on participant responses. Using the above example, a participant may be unwilling to answer the open-ended question and may even provide a gibberish response such as \"afasf asfasf afyiasfaf asf\" [31]. In another example, a participant is willing to answer the question, but provides a very terse answer such as \"not bad\" as opposed to detailed, rich information. Yet in another example, instead of answering the question, a participant asks a clarification question \"Which aspects of the product do you want me to comment on.\" Handling all these situations Using an AI-Powered Chatbot to Conduct Conversational Surveys 15:3 or their compositions requires that a system not only understands a participant's input but also automatically handles diverse interaction situations, which is very challenging to implement [34].On the other hand, the advent of chatbots with their increasingly more powerful conversational capabilities can offer an alternative approach to static online surveys. Specifically, an artificial intelligence (AI)-powered chatbot can conduct a conversational survey. As shown in Figure 1, in a conversational survey, a chatbot asks open-ended questions, probes answers, and handles social dialogues.Intuitively, a chatbot-powered conversational survey retains the advantages of online surveys and offers several additional benefits especially facilitating gathering participant responses to open-ended questions. First, a chatbot can frame survey questions in more personalized, conversational messages, which might help improve participant engagement and response quality [17,36,46]. Second, the perceived anthropomorphic characteristics of a chatbot can potentially deliver human-like social interactions that encourage survey participants to reveal personal insights [82]. Third, it is natural for a chatbot to interactively encourage information exchange in the course of a survey, such as providing response feedback and probing responses, which in turn helps reduce survey-taking fatigue and improve response quality. Moreover, it is the inherent functions of chatbots that interpret diverse user natural language input and handle complex conversations. As a result, chatbots can potentially serve as a moderator and proactively manage a survey process, such as dealing with \"uncooperative\" participants, clarifying the meaning of a question per a participant's request, and guiding a participant to provide richer and more authentic responses [50,81].Despite their benefits, chatbots bear several risks for their use in conducting surveys. First, a turn-by-turn chat requires participants to take extra time and effort to complete a survey. It is unclear whether people would be willing to take the time to chat and complete a survey, let alone providing quality responses. The risk is even higher for surveys with paid participants, who would not be rewarded for taking a longer survey. Second, current chatbots are far from perfect and their limited conversation capabilities may lead to user disappointment and frustration [34]. It is unknown whether the limited capabilities would deter participants from offering quality responses or completing a survey. Moreover, it is difficult for a chatbot to accurately interpret and properly respond to humans' diverse free-text input to open-ended questions [22]. Once participants realize that a chatbot cannot fully understand or assess their responses, it is unknown whether they would do mischief by intentionally feeding the chatbot with bogus responses, which would adversely affect the overall response quality. Finally, the use of a personified conversational system may lead to user behaviors that affect survey quality. For example, studies show that people have positivity bias when giving opinions to an agent [84], producing potentially biased survey results.To our knowledge, there have not been any in-depth studies examining the effectiveness and limitations of AI-powered chatbot surveys in contrast to typical online surveys. We, therefore, ask two research questions:\u2022 RQ1: How would user response quality differ, especially the quality of user free-text responses to open-ended questions in an AI-powered chatbot-driven survey vs. a traditional online survey? \u2022 RQ2: How would participant engagement differ in an AI-powered chatbot-driven survey vs. a traditional online survey?To answer the above research questions, we designed and conducted a field study that compared the use of an AI-powered chatbot vs. a typical online survey with the focus on eliciting user answers to open-ended questions. As mentioned above, there are potential benefits and risks of using chatbots to conduct surveys, especially when involving open-ended questions. However, none of the benefits or risks have been examined. In this first study, we thus decided to focus on examining the holistic effect of a chatbot instead of investigating the effect of separate chatbot features.Additionally, to ensure that our study is based on real-world survey practices and offers practical value, we collaborated with a global-leading market research firm that specializes in discovering customer insights for the game and entertainment industry. Per the firm's request, our field study was to learn how gamers think and feel about two newly released game trailers. The study involved about 600 gamers, half of whom took a chatbot survey and the other half filled out a typical online survey. Through detailed analyses of over 5,000 collected responses, we addressed our two research questions. We also discussed the design implications for creating effective chatbots to conduct engaging surveys and beyond.To the best of our knowledge, our work is the first that systematically compared the holistic effect of an AI-powered conversational survey with that of a typical online survey on response quality and participant engagement. As a result, our work reported here provides three unique contributions.(", "relwork": "To ensure that our findings have ecological validity and practical value, we teamed up with a global leading market research firm that specializes in discovering customer insights for the entertainment industry, including game companies and movie studios. Per the request of the firm, we set up the field study to accomplish two goals. First, the firm wanted to gauge gamers' opinions of two video game trailers recently released at the Electronic Entertainment Expo (E3) 2018, the premier trade event for the video game industry. Second, they wanted to compare the effect of a chatbot survey with that of a typical online survey which they frequently use to collect such information.", "rq": "RQ1: How Would the Quality of Responses Differ?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HC_paper_all/xml/218.xml", "intro": "", "relwork": "Our work is related to several areas of work on understanding factors that impact teaming as well as the use and evaluation of conversational agents for various tasks.", "rq": "evaluate INDIGO and answer our research questions, we have examined multiple sources of data from INDIGO's field deployment, including the chat transcripts between each student and INDIGO and INDIGO-inferred personality traits of the students. Here we report the findings to answer our three research questions, respectively: (a) users' interaction with INDIGO, (b) the effectiveness of INDIGO in eliciting information from the students, and (c) the effect of INDIGO-derived personality traits on team performance and team perception.5.1 RQ1: Students' Interaction with INDIGOWe first examined students' engagement with INDIGO. Our results showed that on average each student spent about 60 minutes (SD = 26 minutes) with INDIGO in their pre-course interview and 26 minutes (SD = 7 minutes) in the post-course interview. In addition to engagement duration, we also computed the response length, defined by the number of words in each student's responses, to gauge the amount of information that the students were willing to provide during their interaction with INDIGO. On average, each student provided 620 words (SD = 291 words) in their pre-course interview and 289 words (SD = 184 words) in their post-course interview. These measurements demonstrated that the students were willing to spend a considerable amount of time with INDIGO and offer information during their interaction with INDIGO.5.1.1 Perceived Characteristics of INDIGO. We examined students' impression of INDIGO by examining their description and ratings of INDIGO."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HH-reorganized/xml/8.xml", "intro": "", "relwork": "Our work builds on two areas of prior research: (1) collaborative search and (2) dynamic help systems and interruptions.Collaborative Search: Collaborative search happens when multiple people work together on an information-seeking task. Collaborative search is often investigated with two dimensions in mind: time and space. The time dimension focuses on whether the collaboration happens synchronously or asynchronously, while the space dimension focuses on whether the collaborators are colocated or remote. A large body of prior work has focused on understanding collaborative search practices along these two dimensions [24,25,34,35]. In this paper, we focus on synchronous collaborative search in situations where the collaborators can only communicate via the Slack messaging platform.A number of different systems have been developed to support collaborative search, including SearchTogether [26], Co-Sense [28], Coagmento [32], CollabSearch [37], Querium [12], and ResultsSpace [4]. These systems have been designed with the traditional search engine as the centerpiece component, but include additional features for collaborators to communicate, share information, and become aware of each other's search activities. The goal of these additional features is to allow collaborators to coordinate, learn from each other's search paths, avoid duplicating work, and to assist with collaborative sensemaking-becoming aware of collaborators' motivations, actions, and state of knowledge [20,26]. Systems have also been designed to algorithmically alter the ranking of documents based on collaborators' activities, for example, by using documents shared between collaborators as a form of relevance feedback [30].Studies have found that these specialized systems provide different benefits during collaborative search, for example, by improving the collaborative experience compared to non-integrated tools [26], by raising the awareness of collaborators' activities [28]; by supporting different strategies adopted by the group (e.g., agreeing on a few relevant items vs. being as exhaustive as possible) [4]; and by reducing communication and coordination efforts [33].While many different systems have been developed to support collaborative search, these systems have not enjoyed wide-spread use [14]. A survey by Morris [25] found that while collaborative search has become increasingly common, most people use a combination of everyday search and communication technologies to collaborate on search tasks. Morris concluded by suggesting that integrating lightweight search tools into existing communication channels may be a more promising approach than developing dedicated systems for collaborative search.Prior research has found that people often use social networks such as Facebook and Twitter to engage in asynchronous collaborative search, an activity referred to as social search [11,27]. Efron and Winget [9] developed a taxonomy of questions posted on Twitter, and found that a large proportion request factual information that is likely to exist on the Web. This result suggests the possibility of developing search systems that can automatically respond to questions posted on social media and partly motivated the development of the SearchBuddies system [15]. SearchBuddies was designed to embed search results in response to questions posted on Facebook. The embedded search results appeared as a new post in the Facebook thread. A qualitative analysis of people's perceptions found interesting challenges and opportunities for \"socially-embedded search engines\". For example, users only reacted positively to the embedded search results when they were extremely relevant and non-obvious, or when they complemented another user's answer to the question. To our knowledge, no prior work has investigated how people perceive search systems that intervene in synchronous instant messaging conversations.Dynamic Help Systems and Interruptions: Prior research has investigated the reasons why people avoid systems that intervene to provide assistance. Users avoid help systems due to the cost of cognitively disengaging with the primary task, due to the fear of unproductive help-seeking, due to a failure to admit defeat, or because they are unaware of how the help system can provide support [8,18].An unwanted intervention can be viewed as an interruption. A large body of research has also focused on understanding how people respond to interruptions while engaged in a task (see Li et al. [22] for a review). Studies have found that interruptions can negatively affect task performance [2], cognitive load [16], and emotional state [1]. Research on interruptions has focused on three dimensions: the interruption protocol, timing, and relevance. Early work by McFarlane [23] investigated four interruption protocols: immediate, negotiated, mediated, and scheduled. Negotiated interruptions, which provide mechanisms for easily ignoring the interruption, were the most effective. A wide range of studies have focused on the timing of an interruption. Results consistently show that interruptions during periods of low mental workload are less disruptive. In this respect, studies have found that interruptions are less disruptive when they occur early in the task (before the user is deeply engaged) [7] and during sub-task transitions [1,16,17]. Finally, studies have found that interruptions that are more relevant to the primary task are less disruptive [7,17].Most research on interruptions has focused on interrupting individuals, rather than collaborators working on a common task. As one exception, Peters et al. [29] investigated interruptions aimed at one individual while collaborating with another. This study compared interruptions sent at random intervals versus interruptions sent by a human \"wizard\" monitoring the communication channel. The wizard's interruptions were less disruptive, suggesting that a system with access to the communication channel might be able to predict when to intervene.", "rq": "In terms of our third research question (RQ3), participants reported different motivations for engaging with the searchbot, different gains obtained from the searchbot, and different reasons for avoiding the searchbot. Our results suggest that the point of intervention is key. Participants reported avoiding the searchbot when the intervention was too soon (before understanding the task), too late (after solving the task), or during periods when they were deeply engaged with other tasks. This finding is consistent with prior work on dynamic help systems and interruptions."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HH-reorganized/xml/27.xml", "intro": "Computer technologies have changed collaborative work in profound ways. Recent advances in Artificial Intelligence (AI) and Conversational Agents (CAs) spark new excitement for bringing technologies endowed with human roles and humanlike behaviors into collaborative processes. By enabling interactions in a more natural form-conversations-CAs can potentially dissolve human-human and human-machine interaction boundaries by sensing, listening to and taking active roles in group activities. One of the roles a CA may take is to act as a group facilitator. Even if CAs cannot behave realistically like a human facilitator, many key functions of group facilitation (e.g., multi-party conversation monitoring [34], agenda setting [36], and preference elicitation [14]) are the targets of current research and technology development. We think the time is ripe to consider the key design issues for group facilitation agents. In this paper, we revisit a fundamental question that the HCI and CA communities have asked before: \"Does a conversational agent need a face?\" Many pioneers in the research communities are advocates of embodiment. Some of the pro-arguments were documented in the 2000 book Embodied Conversational Agents edited by Justine Cassell et al. [9], where she argued that having multiple modalities including gaze, face and gesture is the only way to attain human-like intelligence. Otherwise users would have trouble locating both task-related capabilities and social intelligence, because we are wired to exhibit social behavior such as turn-taking and affect. However, the CAs entering the mainstream market in recent years-the most popular being Apple's Siri, Amazon's Alexa, Microsoft's Cortana, and IBM's Watson-do not have embodiment beyond simple icons or inanimate objects.Many argue that, even putting costs aside, embodiment may not be necessary for CAs [57,25,26]. Empirical evidence about the necessity of the embodiment is mixed. Some literature [23,4,54] suggests that embodiment could improve subjective impression of the agents such as trustworthiness, and thus interaction engagement, but not necessarily objective performance of the tasks that the agent assists in [18,62]. There seems to be a pattern that for tasks that require continuous engagement (e.g., tutoring), embodiment of the agent improves task performance [1,40]; whereas for more \"sporadic\" interactions where agents only occasionally respond or prompt, embodiment may not benefit task effectiveness and it is not necessary to have an agent continuously \"being there\" [25,56]. These arguments may underly the design of popular text-or voice-only personal assistant type of CAs.What should we expect for a group facilitation agent? On the one hand, if the main function of a facilitation agent is to enforce a structured and balanced process, the system would continuously sense the context, but only interact sporadically. In this case, embodiment may not be necessary. On the other hand, if embodiment leads to more positive social perception of the agent, the benefit of embodiment may go beyond that observed in individual interaction settings. First of all, be-cause enforcing structure implies attention and compliance, a more socially favorable agent may be more effective in improving group processes and outcomes. Secondly, affective benefit is often emphasized in technologies supporting collaborative work because it can improve social and collaborative process [37], and positive affect brought by CAs has been observed in casual social settings [50]. Furthermore, we postulate that embodiment may create a stronger sense of presence as in continuously \"being there\", especially in a group setting. Perceiving an additional entity \"being there\" may impact both group interaction and users interaction with the agent. Whether such impact is positive or negative for collaborative tasks invites empirical inquiry.These questions motivated our study to explore impacts of the agent's embodiment on group facilitation tasks. We designed and developed an embodied conversational agent that facilitates a group decision task by enforcing a structured discussion process. We conducted a between-subject, Wizard-of-Oz style experiment with 20 user groups, in which half of the groups experienced the embodied agent, and the other half experienced a voice-only version of the agent. Through survey responses, we first examined whether the positive effect of embodiment on social perceptions of CAs in individual user settings still holds in a group setting, and then studied how the embodiment impacted the collaborative task. We complemented our quantitative results with qualitative focus group interview data. Our work was guided by the following research questions:\u2022 RQ1: How do different designs of the agent's embodiment (voice vs. embodied) influence subjective social perceptions of the agent (rapport, trustworthiness, intelligence and power) in a group setting? \u2022 RQ2: How do different designs of the agent's embodiment (voice vs. embodied) impact: a) the group decision outcome, b) participants' interaction with other group members, and c) participants' interaction with the agent in a group setting?In the remainder of the paper, we will first review related work that motivated our study. We then present how we designed the group facilitation agent, the Wizard of Oz experiment, as well as the experiment methodology. The result section starts with examining participants' subjective perceptions from survey responses to answer RQ1; then we report analyses on the process and outcomes of group decision making activities related to RQ2. This work contributes to the field by providing a comprehensive account of the impact of CA's embodiment in a group collaboration setting, and design considerations for conversational agents that go beyond supporting individual interactions to supporting group collaborations.", "relwork": "Our study is mainly informed by two sets of literature: the work on designing computer-supported systems to facilitate group decision-making tasks, and the work on conversational agent design, in particular, the design of agents' embodiment and its impact on user perceptions and behaviors.", "rq": "\u2022 RQ1: How do different designs of the agent's embodiment (voice vs. embodied) influence subjective social perceptions of the agent (rapport, trustworthiness, intelligence and power) in a group setting? \u2022 RQ2: How do different designs of the agent's embodiment (voice vs. embodied) impact: a) the group decision outcome, b) participants' interaction with other group members, and c) participants' interaction with the agent in a group setting?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HH-reorganized/xml/6.xml", "intro": "Conversational Agents (CAs) 1 are becoming increasingly integrated into various aspects of our lives, providing services across healthcare, entertainment, retail, and education. While CAs are relatively successful in task-oriented interactions [82,96], the initial promise of building CAs that can carry out natural and coherent conversations with users has largely remained unfulfilled due to both design and technical challenges [3,18,87]. This \"gulf\" between user expectation and experience with CAs [61] has led to constant user frustration, frequent conversation breakdowns, and eventual abandonment of CAs [3,61,98].Conducting smooth conversations with users becomes even more crucial when CAs are deployed in online communities, especially those catering to vulnerable populations such as online health support groups [71] and student communities [95]. These community-facing CAs often serve as a critical part of the community to ensure smooth interactions among community members and provide long-term informational and emotional support. However, these community-facing CAs face two unique challenges: the need to carry out smooth dyadic interactions with individual community members, and the need to respond accordingly based on the community's shifting perceptions [53,86]. In fact, the community-facing nature of the CA adds new complexity-each dyadic interaction with individual members is visible to other community members, which can not only change the community's perception of the CA, but can also impact other community members, i.e., unsatisfactory interaction with one individual might also frustrate others [42]. However, humans are able to gracefully conduct smooth interactions with each other and behave according to a community's expectations and norms at the same time. This process is based on a uniquely humane characteristic called \"Theory of Mind\" [7,12,78].Scholars posit that the Theory of Mind (ToM) is a basic cognitive and social characteristic that enables us to make conjectures about each others' minds through observable or latent behavioral and verbal cues [6,12,37,38,94]. This characteristic spontaneously drives our understanding of how we perceive each other during social interactions. This enables us to employ social techniques such as adjusting our appearances and behaviors to align others' perceptions about us based on our self-presentation [36]. In typical human-human interactions, having a Mutual Theory of Mind (MToM), meaning all parties involved in the interactions possess the ToM, builds a shared expectation of each other through behavioral feedback, helping us to maintain constructive and coherent conversations [36,75]. MToM is increasingly used as a theoretical framework for the design of human-centered AI, such as robots, that can be perceived as more \"natural\" and intelligent during collaborations with human partners [26,57,59,75].While MToM is influencing the design of human-centered AI in task-oriented interactions, its role in informing the design of human-AI communicative interactions remains unexplored. Existing approaches to designing human-AI interactions also lack a theoretical framework and a unified design guideline to design human-centered CAs, especially in communicative interactions. Consequently, researchers and designers turn to traditional HCI design guidelines intended for Graphical User Interfaces, which is not always the optimal perspective to look at designing the interactions between humans and often anthropomorphized CAs [89]-researchers and designers face major obstacles in balancing unrealistically high user expectations [61] while providing an adequate amount of social cues to facilitate long-term natural interactions [56].In analogy to human-human interactions, we propose designing towards MToM as a theoretical framework to guide the design of adaptive community-facing CAs that can cater to users' changing perceptions and needs. The first step towards building MToM in human-CA communications is thus equipping the CAs with an analog of ToM that can automatically identify user perceptions about the CAs. With this ability, CAs would be able to monitor users' changing perceptions and provide subtle behavioral cues accordingly to help users build a better mental model about CA's capability. This would also help alleviate the current one-sided communication burden on users, who had to constantly adjust their mental model of the CA through an arbitrary trial-and-error process to elicit desired CA responses [4,9].Research has explored ways along the realm of identifying user perceptions of CAs to facilitate dyadic human-AI interactions, including examining an individual's mental model of CAs in a variety of contexts [31,54,61]. These studies, most of which are qualitative in nature, are not only difficult to scale, but also lack directly feasible algorithmic outcomes that could be integrated into CA architecture to automatically recognize user perception about the CA. For community-facing CAs that are known to have fluid social roles in online communities [87], we presently lack a clear understanding of how community perception of CAs evolve over time, and whether the very dyadic interactions between humans and CAs in community settings reveal any signal related to user perceptions.We thus note a gap in theory and practice in automatically and scalably understanding human perceptions of a community-facing CAs at both individual and collective level. Drawing on the dynamics of human-human interactions, this paper explores a first step towards designing for MToM in long-term human-CA interactions by examining the feasibility of building community-facing CAs' ToM. Specifically, we target two research questions: RQ 1: How does a community's perception of a community-facing CA change over time? RQ 2: How do linguistic markers of human-AI interaction reflect perception about the community-facing CA?We examine these research questions within the context of online learning, where community-facing CAs are commonly seen to provide informational and social support to student communities [1,92,95]. We deployed a community-facing question-answering (QA)CA named Jill Watson [24,34,35] (JW for short) in an online class discussion forum to answer students' questions for 10 weeks over the course of a semester. We collected students' bi-weekly self-reported perceptions and conversations with JW for further analysis. We discuss changes in the student community's long-term perception of JW and examine the relationship between self-reported student perceptions of JW and linguistic attributes of student-JW conversations such as verbosity, adaptability, diversity, and readability. Regression analyses between linguistic attributes and student perceptions of JW reveal insightful findings such as readability, sentiment, diversity and adaptability positively vary with desirable perceptions, whereas verbosity varies negatively.Our contributions are three-fold: First, we propose MToM as the theoretical framework to design prolonged human-AI interaction within online communities. Second, our work provides a deeper understanding of how a community's perception of a communityfacing (QA)CA fluctuates longitudinally. Third, we provide empirical evidence on the potential of leveraging computational linguistic approach to infer community perception of a community-facing CA through accumulated public dyadic interactions within the community context. We discuss the implications of our work in designing adaptive community-facing (QA)CAs through theory-driven computational linguistic approaches, where our ultimate goal concerns building natural, long-term human-AI interactions.Privacy, Ethics, and Disclosure. We are committed to ensuring the privacy of students' data used in this study. This study was approved by the Institutional Review Board (IRB) at Georgia Tech. We collected the survey and discussion forum data (limited to only student-JW interactions) by seeking student consent and the data was anonymized. We offered extra credits to students for filling out each survey, and bonus extra credits if they completed at least five out of the six surveys. This work was in collaboration with the class instructor and we took measures to avoid coercion. The maximum number of extra credits students could earn by participation was less than 1% of the total grade, and these extra credits could also be earned in other ways as part of the standard class structure. We clarified to the students that survey responses would not be shared with the instructor, and would not have any impact on grades.", "relwork": "In this section, we provide an overview of ToM and its application in existing human-AI interaction research. We then discuss related work that explores user perception of the CA to facilitate human-AI interactions and highlight the potential of leveraging language analysis to improve human-AI interaction.", "rq": "RQ1: EXAMINING CHANGES IN STUDENT PERCEPTIONS ABOUT JW"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/conversationalAgent/HH-reorganized/xml/3.xml", "intro": "The Internet promotes a public sphere where people gather to exchange ideas, form opinions, and mobilize social movements [62]. Discussions in online chat spaces like Messenger, Telegram, and WhatsApp allow people to share different perspectives and opinions, free from time and place constraints. In certain online chat spaces, the guarantee of anonymity can facilitate greater openness about opinions and experiences [77]. Because of these advantages, online chats have emerged as a channel for discussing diverse social issues and driving social change [28].However, the fact that these spaces can host discussions does not guarantee that they will properly function as a segment of the public sphere [56]. A long history of empirical work has shown that rational debate and deliberation do not always occur in online discussions. Many people do not actively participate in discussions [29,65]. People join groups and seek information consistent with their own perspectives, which can make it difficult for them to understand or respect others' contrasting viewpoints [53]. Due to these problems, consensus-reaching can be difficult in online discussion [30]. Despite the above, consensus reaching is highly important for situations in which a society is required to make a decision regarding an issue with major social consequences (e.g., How should self-driving cars make decisions in complicated situations?, Who owns the copyrights for AI created art?, What kind of harmful online content should be moderated?) [54]. It benefits both community members who have a stake in the outcomes of these decisions and society as a whole if a consensus is reached through iterative and deliberative discussions that are perceived as legitimate and fair [24], and attempts at such a discussion are referred to as 'society in the loop' [60].Existing studies tend to pay attention primarily to discussion results, which are measured on the basis of whether or not a consensus has been reached. This perspective leads discussions toward being regarded as a means of obtaining a majority consent [63]. However, rather than the mere results of a given consensus, there are significant elements which constitute deliberative discussion including authenticity, substantive balance, diversity, and reasoning processes [21,25,72]. Thus, in this work we do not assess the success or failure of a discussion based on whether an agreement has been arrived at, instead distinguishing between deliberative consensus and mere agreement. We investigate whether the discussion includes both a deliberation process that matches the above criteria and an outcome where discussants actually agree with or concede to a consensus (authenticity) [25].The HCI and CSCW community has explored methods for prompting constructive and balanced discussion. Previous studies have developed systems to enable reasoned argumentation [18,52,64] and a balanced and valid perspective [40,45] and to help human moderation [47]. Furthermore, a multi-turn argumentation system for crowd workers has been shown to improve data accuracy [11] along with worker engagement [59]. Our work draws inspiration from this prior work, building on findings related to effective discussion facilitation, but translates these findings into the integration of a computerized \"facilitator\"-a conversational agent-into a discussion platform rather than transforming or adding elements of the platform's front-end interface. We treat this chatbot as a member of the host community [68]. In line with recent work [69], we argue that chatbot agents can foster positive group dynamics by playing specific social roles that human agents may not want to perform or may be naturally disadvantaged in performing relative to a chatbot.What role can chatbot agents play to promote deliberative discussion? Unlike official discussions managed by professional moderators [75], many informal discussions between people with common interests in online spaces take place without moderators, i.e., group chats or chatrooms. In situations where a moderator might have been able to manage a heated conversation, the absence of such a moderator can intensify the natural drawbacks of unstructured, unthreaded discussions [23,41]. Moreover, absent a moderator monitoring a discussion, the right or power to speak may not be evenly distributed among the participants [41], potentially leading to a \"spiral of silence\" [46]. Moderators distributing the right to speak and structuring discussion may induce more even and active participation [14], given a shared group goal of achieving consensus, enabling more effective deliberative discussion and allowing groups to reach a more authentic consensus.In this paper we present findings from the process of designing and testing a chatbot to facilitate deliberative discussion. We propose \"DebateBot\", which is designed to (1) structure discussion and (2) request opinions from reticent discussants. DebateBot structures discussion based on the thinkpair-share framework, which helps to maintain opinion independence and strengthen reasoned arguments (Figure 1: S1-5) [4,54]. It also encourages participation from lurkers and thus can solicit a broader variety of opinions (Figure 1: F1-3).In our tests we focused on discussion topics related to ethical dilemmas (i.e., the trolley problem of self-driving cars and the rights of AI), in which consensus-reaching and deliberative discussion are requisite. We predicted that the chatbot agent could facilitate deliberative discussion by encouraging more active and more balanced participation, greater opinion diversity, and clearer arrival at a mutually agreed-upon consensus. To evaluate the feasibility of the chatbot agent, we conducted a 2 (discussion structure: unstructured vs. structured) \u00d7 2 (discussant facilitation: unfacilitated vs. facilitated) experiment. In the structured condition, the chatbot agent structured discussion to encourage independent thinking and facilitate members' understanding of different perspectives using methods based on prior research [54,64] including a think-pair-share strategy [4]. Participants in the unstructured condition engaged in free discussion without a predefined format. In the facilitated condition, DebateBot encouraged participants who had been less involved in the discussion to express their opinions; this intervention did not occur in the unfacilitated condition. We ran experiments with 12 groups of five or six members each (N = 64). We measured deliberative discussion based on authentic consensus reaching (discrepancy between group's and individual's opinions), group behavior (active participation, even participation, lexicon diversity), and discussants' attitudes (opinion alignment, opinion authenticity, communication quality, and usefulness). We also collected and analyzed users' qualitative feedback.We found the following:\u2022 In general, a chatbot-moderated discussion structure positively affects the quality of the discussion. Facilitating lurkers to speak drives increased opinion alignment, equality of contribution, and group members' perceived satisfaction. \u2022 There was no difference in the overall magnitude of participation across the four conditions, but the distribution pattern of participation was different. Participants in the facilitated group participated more equally in the discussion.\u2022 Participants in structured discussions produced more diverse opinions (i.e., lexicons), generating a breadth of opinions. However, discussant facilitation did not accelerate this effect. This might be because one group, under the facilitated and structured condition, exhibited a unanimous prior opinion; this may have prevented the emergence of diversity. \u2022 In the facilitated and structured discussion condition, the highest proportion of participants reported that the group's consensus matched their personal opinions, resulting in authentic consensus reaching.Based on these findings, we discuss the design implications of the online chat system for deliberative discussion. The main contributions of this work are as follows:(1) We present a chatbot that we designed and built to enable deliberative discussion by structuring discussion and facilitating even participation. We demonstrate that the agent can perform the role of moderator in the group discussion process. (2) We present findings from an evaluation of deliberative discussion in terms of active and even participation, opinion diversity, and authentic consensus reaching based on behavioral log data, finding significant impact from the use of the chatbot agent. (3) We discuss the implications of a chatbot agent that can facilitate online discussion and present considerations for future work.It should be noted that the work we present here may not be appropriate for certain sensitive and divisive issues such as racial, sexual, religious, or political topics, as the power dynamics and emotional intensity of these topics could be beyond the facilitation capabilities of the system we present here [10]; for some topics within these categories, it is unclear whether a negotiated consensus is even the desired outcome [7]. For these topics, a more specialized intervention may be required.", "relwork": "This study aims to explore the feasibility of a text-based chatbot agent as a moderator in online discussions. We first look at how and where chatbots have been applied, then identify their advantages over other systems. Next, we explore the factors that enable deliberative discussion and their effects in face-to-face and computer-mediated contexts, and discuss how these may be integrated into the design of the chatbot agent.", "rq": "Although uneven participation among the users has been criticized as an obstacle to positive group dynamics, far too little attention has been paid to solving this problem using technology. Our design aims to overcome these challenges by encouraging members who are less involved in discussion to express their opinions. We incorporate this principle into the design of the chatbot, allowing it to identify members in real time who are passive in expressing their opinions and encourage them to participate, potentially leading to a greater diversity of opinions and making arrival at a representative understanding more likely. Thus, we focus on the following research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/88.xml", "intro": "Live video streaming platforms such as Periscope, Facebook Live & Youtube Live have gained popularity in the last few years, with particularly high numbers of new users in developing countries [11]. This popularity is enabled by decreasing costs and widespread availability of smartphones and cellular bandwidth. Content creators on live video streaming platforms have found engaged audiences for game playing, remote viewing of events (e.g., music concerts), disaster response coordination, and collective witnessing of injustice or corruption [17]. As mobile video streams, low cost 360 degree cameras, and augmented reality video streams become mainstream, we expect that the scope of use-cases for live video streaming will continue expanding.We see an opportunity for mobile video streaming to enable a new class of experiences: task-oriented cooperative work and play. We envision use cases such as remote shopping [16], where parents of young children or persons with disabilities who need to stay at home can interactively shop for items that require browsing (e.g., clothing) by interactively instructing a worker with a mobile phone to browse on their behalf; or spot checking and auditing, where donors and funders who are far away from a site (e.g., a construction area or NGO's premises) employ locals to visit and verify through a live video stream that work is completed as expected.These use cases have three distinguishing traits in common:(1) Participants of the stream (both creators and viewers) are collaborating in real-time towards a goal and may communicate in both directions to achieve that goal. (2) Participants on either side of the stream may not know each other.(3) Depending on the nature of the interaction, there may be an asymmetric power relationship between the creator and viewer of the video stream, e.g., when the viewer of the stream is paying the creator to complete a task.These unique characteristics present opportunities as well as challenges. For example, there is a need to ensure ethical behavior on the platform, and provide an environment where all participants are comfortable and engaged with the experience.In this paper, we explore one example of a task-oriented experience: enriched collaborative mobile video streaming in a physicalworld 'escape-the-room' puzzle game. Escape rooms involve collaborative puzzle solving where collocated team members try to escape from a series of locked rooms within a defined time frame. We tweak this setup to include remote team members who need to coordinate with other team members who are physically present in the escape room in order to solve puzzles that have been designed to require remote collaboration. We chose this particular technology probe as an exercise in understanding the types of interactions that will take place between participants, the most appropriate UX features for smooth game-play experience, and how games such as escape-the-room can be redesigned to enable technology-mediated puzzles. Similar alternate reality games (ARGs) have been studied in previous literature to explore research questions around trust, privacy and ability to collaborate efficiently [13,14].We conducted our technology probe with 26 study participants. We report findings from our study, including observations about appropriate communication modalities for remote collaborative game playing, as well as unexpected interactions and points of friction between participants. We also describe the iterative design of the mobile video streaming system, which we call Avatar, that we built and deployed in our study. We experiment with various UX elements as part of Avatar, such as text messaging versus voice messaging, and control modalities such as a virtual joystick. Our results leave us optimistic that Avatar and other systems like it can enable new and exciting experiences.In the long run we ultimately aim to demonstrate that real-time mobile video streaming can enable new and sustainable employment opportunities, especially for people in emerging regions like India & Brazil. Several important research questions on the path towards this goal remain unanswered, but we view this study as an important first step.", "relwork": "There is extensive literature from the HCI community on remote collaboration experiences. These include one-to-one video streams for immersive telepresence [3,6], interactions with multiple video streams [5], attempts to guide viewer attention, asynchronous video collaborations [1,4,8] and sharing media during a video call. Among these studies, some focus specifically on games, social play and distributed collaboration between people.At a high level, our study is unique in that we focus on collaborative, task-oriented experiences, where the participants may be anonymous and there may be asymmetric relationships between the participants. While others have studied use cases that have one of these characteristics (e.g., anonymity), the conjunction of these characteristics bring about unique opportunities and challenges.", "rq": "In the long run we ultimately aim to demonstrate that real-time mobile video streaming can enable new and sustainable employment opportunities, especially for people in emerging regions like India & Brazil. Several important research questions on the path towards this goal remain unanswered, but we view this study as an important first step."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/216.xml", "intro": "Real-life escape rooms are a new genre of game based on live-action role-playing games, treasure hunts, and online escape rooms [47,52]. Players are locked in a series of rooms and must solve puzzles in order to escape. Some include programmatically controlled doors or lights, computers, or sensors [47,52]. Escape rooms have been found to be a potential avenue for co-workers or friends to explore and enhance their collaboration skills, socialize with others, and build team morale [47,52]. Thus, they could be used by researchers or designers to explore a range of social and technical questions, depending on how they are constructed [52]. This is akin to the way alternate reality games (ARGs) have been used to explore social questions around trust, learning, and relationship building [1,6,9,10] and technical questions around connectivity and device usage [2,5]. For example, escape rooms could be used to study or improve relationship dynamics such as between co-workers, family or friends [52]. They could also augment educational programs for teaching workplace skills [52]. However, given their relative infancy, we know little in terms of how to effectively design escape rooms.Real-life escape rooms typically involve collocated play where groups of players must be in the same physical location together in order to participate [52]. Yet, in reality, co-workers may be distributed around the world. This distance-separation means that they do not have many opportunities to practice their collaboration skills or participate in team building activities with remote colleagues, though it could be highly valuable [50]. Similarly, many family members or friends live apart and do not have opportunities to perform shared activities together over distance [46]. Distributed escape rooms offer one potential solution to these problems. By distributed escape room, we are referring to an escape room that connects two or more rooms over distance where players participate as a team, despite being in different locations.The challenge, however, is that it is not clear how to best design distributed escape rooms to produce an enjoyable experience. We do not know: 1) what kinds of video and audio connectivity should be used to join the rooms; 2) how artifacts should be connected across the rooms; and, 3) if and how puzzles should be designed to support independent and joint work. To explore these points, we iteratively designed a distributed escape room called Escaping Together that connects two distance-separated rooms. We specifically explored three ways of connecting the rooms: video/audio connections, shared artifacts, and puzzle design. Next we conducted an exploratory study of Escaping Together with pairs who played in the distributed escape room-one partner in each of the rooms. Our goal was to understand if and why participants felt a sense of social presence with their partners; how play manifested in the rooms; how partners worked together or apart from each other; and, to what extent the design of the rooms affected these behaviors. By social presence, we refer to the feeling that one's partner is present and with you in the room [7].Our results show that participants strongly valued the audio link to connect with their partner, while stationary and mobile video links periodically augmented this connection. In contrast to prior research on workplace collaboration (e.g., [22,32]), seeing only part of the remote room over video was valuable as it created curiosity in players who wondered what the remote room was like. Players' expectations of collaboration affected their approaches to puzzles. Participants imagined the escape room environment to offer a parallel setup between rooms; in contrast, the escape room did not always require joint work and artifacts were sometimes different. These results suggest that distributed escape rooms require ways of providing social presence through audio that is decoupled from video; video feeds for sharing artifacts where it can be valuable to show limited viewpoints in order to increase curiosity; and, a mixture of puzzles and artifacts where some are similar and others are not. Similar artifacts and shared puzzles may help create feelings of social presence, while dissimilar ones may help create feelings of intrigue.", "relwork": "Real life escape rooms are a new genre of game that have evolved from live-action role playing games, treasure hunts, and online escape rooms where players try to free an avatar by solving puzzles [47,52]. Many different types of escape rooms exist in various facilities around the world where themes vary [47]. Depending on the escape room, players solve logic puzzles, spatial or mechanical puzzles, and word or math puzzles [47]. A survey of proprietors of 175 escape room facilities around the world found that 13% have an open model where players solve puzzles in no particular order, 37% have a sequential model where puzzles are designed in a linear sequence, and 45% have a path-based model where there are multiple sequential paths of puzzles [47]. Escape rooms are played by corporate groups, adult friends or family, and young adult friends [47]. A study of collocated players in an escape room showed that escape rooms provide players with opportunities to practice collaboration skills such as communication, situation awareness, and conflict resolution; however, the design of escape rooms can restrict such behaviors [52]. We build on this work to explore distributed escape rooms.", "rq": "The goal of our study was to understand what factors are important for the design of distributed real-life escape rooms to create an enjoyable collaborative experience for players where they feel like they are participating in a shared experience with remote partners. We now describe the design factors that we feel are most important where the factors raise important questions for researchers and designers who may create distributed escape rooms or other technologysupported distributed social activities for team building exercises, entertainment, or as research tools to explore social or technical research questions."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/59.xml", "intro": "Collaborative work allows to combine knowledge and shape collective solutions that incorporates different perspectives. This can benefit various application areas ranging from problem-solving and content layouting, to architecture and manufacturing tasks [26,31,54]. A promising technology to enable collaboration across distance is VR. One can imagine a workspace where a user is collaborating at a table. The table and its objects are physical, spatially registered with and visualized in the virtual scene. This provides the advantage of physicality, the intuitive familiarity of working on a table and the tactile sensation of grabbing and manipulating objects. At the same time, the benefits of virtuality can be exploited -one can perceive the remote collaborators' presence and the interaction is synchronized over the network in a unified VR experience.Lots of future collaborative work can shift to VR. With technical advancements, future VR systems could integrate physical objects and provide them as haptic props that are ubiquitously applicable during collaboration. This could enable remote users to create a collective solution by using physical objects present in their location.Here a fundamental challenge is interaction with remote physical objects. One can manipulate the own local object as well as the virtual representation of a remote object but not manipulate the actual remote physical object.Yet, collaborators may be of distinct expertise, where an ability to manipulate remote objects directly can become helpful. Related work suggests augmenting objects with motors [12,44] or using teleoperated robot arms for remote control [15]. Or, to indicate the manipulation virtually, which the partner can then physically recreate in their location [20,23]. However, as these approaches either require extensive hardware augmentation or user effort, it is desirable to seek alternative ways to tackle this challenge.We investigate a new approach that engages users more actively and closely preserves the notion of physical manipulation across local and remote spaces. Our idea is to use passive haptic props in each physical location for the interaction with virtual objects. These haptic props have a variable representation in the virtual environment and can be used to control the available virtual objects. We explore this in two ways of ownership of the assigned virtual objects: SingleOwnership and SharedOwnership. SingleOwnership restricts collaborators to manipulate only the virtual objects that are associated with their local haptic props, whereas SharedOwnership allows transferring virtual objects between remote locations by taking over ownership with haptic props.We implemented a distributed multi-user VR system that allows remote collaborators to interact with haptic props to solve a spatial arrangement task. The system incorporates haptic props registered at two locations by optical tracking. The spatial information is shared live across the network. Collaborators experience virtual objects assigned to remotely located haptic props at the correct 3D location and orientation in the virtual room. We conducted a user study to gather insights about the performance, experience, and trade-offs of the collaboration with different ownership strategies. We implemented a puzzle task that required the collaborators to create a certain arrangement of puzzle pieces using haptic props. To fulfill the task, the collaborators had to exchange knowledge with the given ownership techniques.For SingleOwnership we employed two conditions. (1) collaborators could either use haptic props to arrange their own puzzle piece and then rely on verbal communication and gestures to communicate the solution of the task to each other. (2) Collaborators could create virtual instructions that indicate the correct arrangement of puzzle pieces using blank haptic props (Instruct). Therefore, we provided an additional set of haptic props. These haptic props were 'blank' and could be assigned to a puzzle piece by the user. For SharedOwnership, we employed two transfer techniques namely copy and cut. Copy allowed collaborators to use blank haptic props to retrieve a copy of a virtual object that is assigned to a remotely located haptic prop. Cut allowed collaborators to reassign virtual objects from remote haptic props to blank local haptic props. In this case, the remote haptic prop turned blank.By having the ability to transfer remote objects using local haptic props, collaborators perceived that they communicated less using speech or gestures. Our results indicate that which strategy to handle ownership works best -SingleOwnership or SharedOwnership -is depending on the underlying scenario. We found that collaborators were significantly slower when using virtual instructions compared to verbal communication or transferring ownership via copy or cut. By having the ability to transfer remote objects using local haptic props, collaborators perceived that they communicated less using voice or gestures. Overall, we found that SingleOwnership techniques are more useful if awareness between collaborator's actions is needed (e.g., novice/expert scenario), while SharedOwnership techniques provide benefits when collaborators want to use their own expertise to solve a task with fewer dependencies on each other. For example, creating a collaborative solution to which collaborator contributes with their own knowledge to shape the best result.The contributions of this work are as follows:(1) The design of SingleOwnership and SharedOwnership collaboration techniques. SingleOwnership techniques rely on instructions either verbally or by using haptic props. Share-dOwnership techniques allow transferring virtual objects between remote locations using haptic props. This allows the collaborators to work more independently. ( 2) An open-sourced VR system for multi-user remote collaboration with tracked physical objects at both locations. (3) A user study comparing our techniques and revealing insights on the unique trade-offs between spatial task efficiency and communication engagement of the users.", "relwork": "In the following, we review previous work on video-based remote guidance, 3D-based collaboration, as well as tangibles and haptics with a focus on their use in collaboration.", "rq": "After our participants gave their informed consent we recorded demographic data. Then we situated them at the table and provided the VR-HMD and tracking gloves. We established a communication channel between the two locations using Skype 2 . The two collaborators could briefly introduce themselves. Each group of collaborators consisted of one participant and one confederate. The confederate was instructed to act as a newly instructed participant and had no knowledge of the research questions. The confederate adjusted to the working pace of the participants. Further, the confederates were not instructed to make mistakes intentionally. In total, two different persons acted as a collaborator -one self-identified as male and one as female. We did not tell our participants that they were collaborating with a confederate. After both collaborators were situated at the desk and were provided with a VR-HMD, we introduced them to the collaboration task. Each participant completed the task in four conditions, with each condition involving two trials. To account for learning effects, we only took the second trial into account in the analysis. In the first trial, we made sure that the participants understand how to collaborate using the provided collaboration technique. For each condition, we measured task completion time, the number of actions needed to fulfill the task, and the user experience (UEQ) . For the study, the order of the conditions was counterbalanced. After each condition, the participants also filled out a questionnaire about helpfulness, verbal communication, and quality of collaboration. The study concluded with a brief interview session. Each participant took on average one hour for the study. We used a screen capturing tool to record the virtual setting during the study for later analysis."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/173.xml", "intro": "Since the late 1960s, 9-1-1 services in North America have offered people a means to place telephone calls asking for emergency assistance in cases requiring an ambulance, the police, or fire fighters [58]. Audio-based calls can be made from landline phones, IP-based phones, or (most) mobile phones and they are received by a call taker who assesses the situation and dispatches the appropriate responders [15,16].While technology has long evolved past the telephone, it remains the main communication technology for information exchange between those in emergency situations and 9-1-1 services. Within Canada, we are seeing efforts to move towards'Next Generation 9-1-1' where citizens could use text messaging, video streaming, or the sharing of photos or videos during a 9-1-1 call [15,41]. Policy efforts suggest that infrastructure for such services should be in place by the year 2020 [15,16,41].We focus on this topic by exploring the idea of video calling for 9-1-1 calls-technology akin to a Skype or FaceTime call between a 9-1-1 call taker and a person experiencing an emergency.While the ability to show a call taker an emergency situation via a video call seems to hold promise and'keep pace' with current communication technology advancements, many open questions remain. In what ways might video calls enhance 9-1-1 services and when might they inhibit them? How should video calling technologies be designed such that they meet the needs of the caller, call taker, and dispatcher to ensure emergency calls are efficient and effective? What effects might there be on the work practices of 9-1-1 call takers if video calls are introduced?We explored these research questions by studying the 9-1-1 call taking practices within three 9-1-1 call centres in Canada. Our overarching goal was to understand what factors would be important for the design of such video calling systems if 9-1-1 services were expanded to include them. We observed police, fire, and ambulance call takers and dispatchers during their normal work practices and conducted contextual interviews with them about their work. We probed about a possible future with video calling technologies incorporated into 9-1-1 services.Our results show that 9-1-1 video calling-and the sharing of images or video more broadly during calls-creates the potential for many benefits, including the acquisition of additional contextual information of scenes, the ability to overcome information inaccuracies, and a means to acquire information from those who cannot easily speak (e.g., children, elderly, those injured). Yet video calling also raises many challenges and concerns, including the possibility of additional workplace stress, information overload, and privacy challenges related to the autonomy of the caller and call taker. Compared to commercial video chat software (e.g., Skype, FaceTime), video calling for 9-1-1 should be thought of as a continuum where audio is the primary communication medium and the sharing of video-based media varies from being turned off and not utilized, to the use of images or video clips as part of an audio call, to live video of a scene. This needs to be coupled with features that focus on supporting intricate camera work and decision making around when and how to show the emergency situation and how to easily transition between different visual modalities within the call.", "relwork": "In emergency call centres, call takers receive calls and get details about the incident in order to classify and prioritize it [20,35,45,57]. Textual information is then recorded in a Computer-Aided Dispatch (CAD) system [56]. Dispatchers review call information and send an appropriate response team, either police, fire, or ambulance [20,35]. It can be hard to figure out where a caller is because incoming location data may not be accurate from mobile phones, or people's descriptions of a location may be ambiguous [20,45]. Call centres can easily become chaotic if call volume is high [35]. Many callers are frantic, desperate, or hysterical so it can be hard to get accurate or enough information [4,58]. Call takers are trained to take control of the call so they can ask specific questions; however, hostile callers may not want to give up control [52].Situation awareness is critical in emergency call centres [10]. Situation awareness on the part of a caller taker is a momentto-moment understanding of what is happening during an incident and how this information should be acted upon [2,17]. It can be gained by listening to others, by purposefully looking around, or by noticing information in one's visual periphery [8,10,24,26,55]. Call takers use situation awareness to maintain an understanding of incoming calls to ensure that multiple calls about the same incident are known [4,35,45]. They can also maintain awareness by scanning call lists in the CAD system [35,45].Call takers can face a great deal of distress from dealing with traumatic situations and have had to rely on coping mechanisms such as counseling [36] and peer support [1,49]. Stress results from the complex nature of incoming calls, ambiguous information from the caller, multifaceted medical needs, and communication difficulties (e.g., poor English) [1,22]. Stress can be exacerbated because callers often imagine what the caller is telling them and call takers can have lasting memories of calls [1]. Life and death calls [7] and calls involving children [22] are particularly stressful. Sometimes call takers form personal connections with callers [1]. Feelings of helplessness can contribute to stress [29] as can shift work and a lack of sleep [49].Many people who are deaf or hard of hearing find emergency situations challenging and rely on friends or family to call 9-1-1 operators [51]. Teletypewriters have been used in the past, but they are not usually with a caller when mobile [51]. Text-to-911 services are available in most areas within North America, though some regions require special permission or signup to use the service [16,54]. Researchers have even designed special-purpose apps for deaf users [51].In emergency situations involving ambulance response, studies have shown the value in timely communication between emergency responders and trauma teams [1]. Hospitals would value more visual information from incident scenes to understand the severity of the situation in order to better prepare for the arriving ambulance [1]. Studies and technology designs have shown the value in providing hospital staff with video of patients who are on their way to the hospital [5,6,44]. Video can be used to show patient trauma, body details, and display screens of medical equipment [5]. In natural disaster response, decentralized uses of media have been shown to be critical [9,34], if information can be deemed credible [48].Our research expands on the related work by exploring the prospective benefits and challenges of using video calling within 9-1-1 call centres, which has not been done before. We explore scene assessment, situation awareness, software and hardware usage, and workplace stress, from the perspective of how they may be affected by video calling.", "rq": "We explored these research questions by studying the 9-1-1 call taking practices within three 9-1-1 call centres in Canada. Our overarching goal was to understand what factors would be important for the design of such video calling systems if 9-1-1 services were expanded to include them. We observed police, fire, and ambulance call takers and dispatchers during their normal work practices and conducted contextual interviews with them about their work. We probed about a possible future with video calling technologies incorporated into 9-1-1 services."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/239.xml", "intro": "Interactive software development in the 1970s and 1980s involved traditional software engineering processes, with limited interdisciplinary collaboration between developers and graphic designers. By the 1990s, the advent of full color, high-resolution displays enabled high-quality interactive graphics, with a corresponding need for professional designers. Today, graphic designers, interaction designers, and user experience specialists are routinely part of the teams creating interactive software, together with software developers. However, integrating designers' and developers' 19:2 Fig. 1. An example of a custom interaction in the Paper mobile app from FiftyThree. In the first screen, the user has selected the scissor tool and draws a circular area with one finger. In the second screen, the user drags this area to move the content to a new position and taps outside the circular area with another finger to create a copy of the selected area. In the third screen, the user drags the selected area to reveal the copied shape.Addressing the gap between designers and developers, and more precisely the gap between the tools that they use to create interactive systems thus remains an open research question. In this work, we identify problems faced by designers and developers as they collaborate, with a particular emphasis on the representation, communication, and interpretation of custom interactions. Our goal is to inform the design of new collaborative prototyping tools that reduce these problems, without forcing professionals to abandon their preferred representations. We want to facilitate the transition from design to implementation, as well as the transition from implementation back to the design when changes occur. More specifically, we address the following research questions:-What are the most common and critical problems that impair designer-developer collaboration when creating interactive software that involves custom interactions? -How can prototyping tools help mitigate these problems?We start by describing the motivation and methodology of the project, and review relevant related work. We then describe three studies that we conducted to better understand designerdeveloper practices, leading to a set of design principles to reduce the breakdowns they encounter. We introduce Enact, a prototyping tool for touch-based mobile interaction based on these principles, and report on two studies that we conducted to assess Enact.", "relwork": "We grouped the related work on designer-developer collaboration in two areas. First, we review descriptive research focused on the processes and artifacts currently used during the designerdeveloper collaboration. Second, we review research on novel prototyping tools for creating interactive systems.", "rq": "Addressing the gap between designers and developers, and more precisely the gap between the tools that they use to create interactive systems thus remains an open research question. In this work, we identify problems faced by designers and developers as they collaborate, with a particular emphasis on the representation, communication, and interpretation of custom interactions. Our goal is to inform the design of new collaborative prototyping tools that reduce these problems, without forcing professionals to abandon their preferred representations. We want to facilitate the transition from design to implementation, as well as the transition from implementation back to the design when changes occur. More specifically, we address the following research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/211.xml", "intro": "Since many organizations operate in distributed and international ways, multilingual communication is becoming more common, where people choose a common language (lingual franca), generally English, for collaboration [2,19]. However, due to the lack of language proficiency, non-native speakers (NNS) often face significant difficulties during multilingual communication. For example, they are often overwhelmed with such multiple parallel processes as phonetic analysis, parsing ongoing conversations, and intensive thinking, all of which are typically accompanied by internal speech in their native language [6,20,22]. In conversations dominated by native speakers (NS) as a majority, NNS are left behind as discussions advance rapidly [15,27].Researchers in the HCI/CSCW field have developed technologies to assist NNS in multilingual communication. Among them, transcripts generated by automated speech recognition (ASR) technologies have been proven to be useful for improving the comprehension of NNS during audio conferencing [18]. However, research also shows that ASR transcripts impose extra burdens on NNS. These burdens increase as the transcript error rate increases or during delays in showing the transcripts. When the error rate and delay exceed a certain threshold, the ASR transcripts become simply a source of burden with little value to the NNS [28].To avoid distracting NNS by ASR transcript errors/delays and to more efficiently use ASR transcripts, we propose adding another channel (i.e., highlighting) through which NS can provide help to NNS by highlighting the important parts of ASR transcripts. This idea frees NNS from following every word of a transcript; NNS do not have to give attention to the entire transcript if highlighting emphasizes its critical parts. Since NS do the highlighting, we assume the information is reliable and useful for NNS. It is important to note that the task loads of NS and NNS in conventional multilingual group communication are strongly unbalanced [4]. Even though NNS often suffer from difficulties due to language barriers, NS effortlessly handle the conversations [19,20]. Thus, ideally, our additional channel will reduce the communication work of NNS by exploiting the underutilized cognitive resources of NS while improving the overall group communication quality.We conducted a laboratory experiment to examine this idea. 14 triads of two NS and one NNS engaged in two collaborative discussions under different conditions: audio conferencing plus ASR transcripts with and without the highlighting function. In the with-highlighting condition, the two NS individually highlighted key points of ongoing conversations, simultaneously allowing NNS to see the highlighted sentences.From a combination of quantitative and qualitative analyses of the experiment's data, we determined that the highlighting interface indeed increased the burden on NS, but at the same time it improved the task performance (remembering the agreements reached during the discussion) of the NS themselves. As expected, highlighting by the NS helped the NNS in multilingual communication by clarifying and simplifying the messages, and it also created a more relaxing cross-lingual interaction.", "relwork": "In this section, we first introduce previous literature on the difficulties faced by NNS when interacting with NS as well as technologies intended to support NNS in multilingual conversations. We then focus on task rebalancing in the context of multilingual communication and discuss our design decisions (i.e., highlighting key discussion parts in automated transcripts by NS). Finally, we describe how the transcripts highlighted by NS can improve overall group communication quality.", "rq": "To answer the research question and test the hypotheses, we conducted a within-subject experiment that compared ASRtranscript-based multilingual communication support with and without the highlighting feature. 14 triads participated in a commonality-finding task. Each triad consisted of two native English speakers and one Japanese non-native English speaker. The participants were asked to use English as the common language in their audio conferencing discussions."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/210.xml", "intro": "Hand gestures have been shown to have an important role in remote collaboration, especially for physical tasks where an expert guides a worker in machinery assembly and maintenance [22]. Visual presentation of hand gestures on the remote side helps collaborators better establish common grounds, shorten verbal communication [10,11], improve task performances [10,22] and promote awareness [21,35]. Interactive solutions can convey hand gestures in various mediated ways, such as expressed through pointing cursor or sketches, or in unmediated ways, such as showing direct/natural images of the hands. Compared to mediated communication, unmediated hand gestures are perceived as more intuitive as they do not require the viewer to decode abstract representations of gestures [21]. Therefore, several research works have implemented remote collaborative systems using mixed reality approaches overlaying unmediated images of collaborators' hands on the shared workspace [7,20,22]. However, most of those systems require stationary or heavily instrumented devices.Thanks to the rapid development of mobile technology, adhoc remote collaboration through portable devices such as phones, tablets, and smartwatches in a mobile setting (i.e. at any time at any place), is a growing groupware segment, including physical tasks. For example, a technical consultant can help troubleshoot a broken machine using a mobile phone while on the move (e.g. walking, taking bus/taxi/train, etc.). Equipped with touch input, mobile devices allow the users to express certain gestures by pointing or sketching. While (in-air) unmediated hand gestures are a winning complement to (touch) pointing and sketching, mobile builtin cameras' narrow camera field of view (FOV) requires users to hold their hands relatively far from the screen (green area in Figure 1) for such gestures to be captured. Such hand poses can lead to arm fatigue [34], especially when unmediated gesture input is frequently combined with touch input. In spite of this ergonomic cost, we believe a dynamic combination of near-screen in-air gestures and touch input can benefit remote collaboration. For example, when guiding a remote worker to identify an object, the helper tends to hover a hand above the screen to skim the shared workspace shown on their device's screen while searching for the object before pointing on it [21]. We have observed that such gestures usually happen near the device screen outside the FOV of the front-facing camera. While touch-hover input techniques on smartwatch [15] and mobile devices [16] have been explored, the hand itself has not been captured. Using active acoustic sensing, touch-hover input techniques have been further refined [36], however, such solutions require specialized hardware not yet available, even in the most advanced ultrasonic mobile solutions (e.g. by Elliptic Labs 1 ).We suggest leveraging this gestural information by making it visible to the mobile camera remote collaboration. Past efforts have used cameras mounted at see-through headmounted displays (HMDs) to capture unmediated hand gestures in mobile settings. However, untethered HMDs such as Microsoft Hololens are not yet affordable for a majority of users. Aiming to leverage widely owned devices, we suggest using the built-in camera of mobile devices more effectively to make unmediated hand gestures better support remote collaboration in mobile contexts. Hence, our driving research question is: \"Is it feasible to provide a low-cost minimally instrumented solution capturing unmediated nearscreen gesture on mobile devices to effectively support remote collaboration on physical tasks?\"In this paper we present MirrorTablet; a low-cost system for capturing hand interactions on and above mobile devices, including near-screen gestures, with no need of extra sensors. While previous systems for remotely collaborating on physical tasks have required complex hardware setups, including multiple favorably positioned sensors, our approach requires only a single commonplace mirror mounted on a tablet and facing its screen, thereby capturing 1 http://www.ellipticlabs.com/technology/ user hand interactions and displaying them on another's device. While our approach addresses the needs of nomadic work, most previous works focus on fixed interactive surfaces. This paper contributes: \uf0b7 A prototype for a low-cost and minimally instrumented mobile system that captures hand interactions, supporting remote collaboration. \uf0b7 A user study investigating the effectiveness of our proposed system on improving remote collaboration on physical tasks.", "relwork": "We consider hand interactions on the tablet screen as the foreground and the current screenshot of the tablet as the background (see Figure 3 a, b). To achieve the foreground, we subtract the extracted screen area to the current screenshot of the tablet and threshold the subtracted image.To decrease the influence of luminance, we carry out background removal on YCbCr images rather than RGB images. During the informal prototyping process, we observed that when using the Cr channel of the subtraction image we could best separate the hand from the background. However, the result of the background removal is still a pixelated and flickering image of the hand, potentially distracting to users (Figure 3c). Therefore, we apply the following refinement step, mainly using skin color segmentation on the background removal result, to improve the appearance of the hand.", "rq": "We suggest leveraging this gestural information by making it visible to the mobile camera remote collaboration. Past efforts have used cameras mounted at see-through headmounted displays (HMDs) to capture unmediated hand gestures in mobile settings. However, untethered HMDs such as Microsoft Hololens are not yet affordable for a majority of users. Aiming to leverage widely owned devices, we suggest using the built-in camera of mobile devices more effectively to make unmediated hand gestures better support remote collaboration in mobile contexts. Hence, our driving research question is: \"Is it feasible to provide a low-cost minimally instrumented solution capturing unmediated nearscreen gesture on mobile devices to effectively support remote collaboration on physical tasks?\""}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/166.xml", "intro": "In recent years, interactive crowdsourcing systems have been designed that allow end users to send requests directly to crowd workers, and to continuously collaborate with them in real time [32,35]. In such systems, requesters can verbally communicate with workers to request various tasksranging from accessibility assistance to writing software-and researchers have developed a number of crowd-powered systems to enable this novel collaborative model [3,7,8,38,40,45,49]. This form of direct and natural interaction between a requester and crowd workers in real time minimizes the limitations of microtask crowdsourcing workflows, where a task needs to be broken down into context-free microtasks and the responses aggregated afterward [46]. Real-time collaboration between requesters and workers is useful in expanding the application of crowdsourcing to broader domains, allowing workers to help solve open-ended tasks whose steps are unknown in advance and may change during the process.While this mode of end users directly collaborating with crowd workers unlocks new opportunities for crowdsourcing, it introduces new set of challenges for requesters beyond mere unfamiliarity with such collaboration. This is because the characteristics unique to crowdsourcing-ubiquity, scale, transiency, and anonymity-make using crowd-powered systems different from simply collaborating with other users via real-time groupware. Specifically, designing crowdsourcing systems that leverage direct interaction entails an inherently asymmetrical structure between a requester and crowd workers in multiple ways: scale, roles, expertise, communication channels, and user interfaces. Such differences and imbalances are embedded in interactive crowd-powered systems and pose new design challenges beyond those addressed in real-time groupware design [20]. However, the current body of research in real-time crowd-powered systems has focused on developing interactive crowdsourcing systems for specific domains, in which the system that enables workers to resolve certain types of tasks is the main contribution [3,7,8,38,40,45,49]. Furthermore, the validation process of such systems is often focused on the crowd's ability to complete the central task, holding the requester constant throughout each trial in order to control for variance among requesters [11,38,45]. As a result, we lack sufficient understanding of the challenges that requesters have as end users of interactive crowdsourcing systems in general. This work focuses on examining the requester side of crowdsourcing.The goal of this paper is to better understand the challenges that are present when a requester directly collaborates and verbally communicates with crowd workers in real-time collaborative crowdsourcing systems, and to formulate design recommendations from the findings for future interactive crowdsourcing systems. In crowdsourcing systems, requesters are those who specify tasks for crowd workers. Crowd workers then choose and complete the tasks that are available through crowdsourcing platforms, such as Amazon Mechanical Turk [1]. Requesters generally have less control over who will do the tasks, and less direct interaction with the workers, than the traditional recruitment processes. This inherently transient nature of crowdsourcing poses a set of challenges that are not present in an existing real-time groupware. In addition, designing a single system for two disparate groups-requesters and workers-may require different approaches for each group, which can create unforeseen challenges. In particular, we are initially motivated by the following questions: Q1) How do requesters find and understand the benefits of real-time collaboration in crowdsourcing and engage with workers (5.1)? Q2) How do potential asymmetry of expertise, roles, or communication channels between a requester and crowd workers impact their communication and collaboration (5.2,5.3)? Q3) What particular challenges do requesters and workers face in collaboration (5.4,5.5)? Our findings can contribute to improving existing systems and informing the design of future interactive crowdsourcing and mixed-initiative systems (6).To answer these questions, we conducted a user study investigating how requesters collaborate and communicate with workers via an interactive crowdsourcing system. We chose to use an existing crowd-powered system, Apparition, which allows requesters to create sketch prototypes of graphical user interfaces with crowd workers [38,45]. In this study, an end user (requester) is asked to verbally describe and draw a GUI sketch; crowd workers behind the scene listen to the description and view the sketch, and create a refined GUI in real time. While the authors of previous works focused on building and validating systems to enable crowd workers to effectively create an interactive graphical user interface prototype [38,45], in this study we focus on how various requesters use the system and collaborate with workers.Our core contributions are valuable beyond the scope of crowd-powered design tools, derived from a qualitative analysis using questionnaires, interviews, and observations. The system and task in this study involve common components of real-time collaboration in crowdsourcing: a requester, crowd workers, an artifact (sketch), visual context (canvas) that is shared remotely, and communication channels (voice/chat). This structure is found in many crowdsourcing systems that involve real-time collaboration between a requester and crowd workers, allowing us to generalize our design recommendations. A summary of our findings is as follows:\u2022 Requesters actively collaborate with workers and understand the benefits of working with them in real time (5.1). \u2022 Expert requesters may speak in a style that makes it difficult for crowd workers to comprehend their requests (i.e., be less descriptive, use more jargon) (5.2). \u2022 The asymmetry in communication modalities (speech versus text) causes confusion (5.3).\u2022 Sharing the visual context was not enough to effectively coordinate collaboration (5.4).\u2022 Speaking rates vary among individuals; rapid speech may result in a backlog of requests (5.5).The design recommendations drawn from the study contribute to the broader goal of enhancing the design of crowdsourcing systems that can facilitate effective real-time communication between requesters and crowd workers.", "relwork": "Crowdsourcing has been effective in resolving complex tasks that computers alone cannot automate. The most established approach to crowdsourcing involves breaking the task down into microtasks that non-expert crowd workers can solve independently [46]. This method is effective in settings where the problem solving process is known ahead of time and immediate feedback is not necessary, but has limitations in addressing \"complex\" tasks, where tasks cannot be as clearly and rigidly defined [33]. One way researchers have addressed this problem is by using real-time, continuous crowdsourcing to enable on-going interaction between a requester and crowd workers [35,45].", "rq": "In order to capture the complete speech and canvas activity in Apparition, the audio and computer screens of T1, T2, and the follow-up interview were recorded. Both the audio extracted from the videos and the interviews were transcribed for data analysis. The transcribed speech from T1 and T2 was segmented into a series of speech utterances with a starting timestamp. The boundaries of each speech segment were drawn at pauses, at the end of sentences, or at a change in topic. Then, we developed and refined coding labels and interview questions over the course of pilot studies (eight runs) in the context of the initial research questions that we had (listed in the Introduction). Conversation coding has been used in previous studies that investigated shared visual contexts in collaborative design . We developed a set of coding labels as we observed the study, conducted interviews, and transcribed the recordings. In addition, through the process, we sought to identify emerging themes in relation to expertise, engagement, and temporal patterns. One of the authors coded all participant trials, and these coding results are what we present and analyze in sections 5 and 6. We also had another author code 10% of the data to report the inter-rater reliability of our coding. We calculated intraclass correlation coefficients (ICC) for the two coders' coding, and we report these scores and their interpretation (poor, fair, good, excellent) according to , following each of the label definitions below."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/115.xml", "intro": "Mobile VR (Virtual Reality) HMDs (Head-mounted Displays) are currently mostly based on smartphones and a case outfitted with lenses (e.g. Samsung GearVR, Google Daydream). A recent development focuses on mobile VR HMDs which are not based on smartphones but offer an untethered headset with embedded hardware, inside-out tracking and some form of input capabilities (e.g. Intel Alloy). Both these device types enable the interaction scenario of Nomadic VR [18,29], where users can immerse themselves inside a virtual world wherever and whenever they wish. This nomadic interaction scenario comes with several challenges such as the unknown and uninstrumented environment [18]. Since current mobile VR HMDs are designed exclusively for the wearing user (HMD User), every other person in this environment (Non-HMD User) is excluded and reduced to be a sole bystander [20]. This further leads to a complete isolation of the HMD User and could potentially lead to less social acceptance of the technology [40]. We identified two main challenges for this specific problem: (1) How can we visualize parts of the virtual environment to Non-HMD users and (2) how can we enable a form of interaction between HMD and Non-HMD user inside the uninstrumented environment. The overarching goal is to reduce exclusion for the Non-HMD User and reduce the isolation of the HMD User and enable a cohesive and enjoyable experience for both.We propose FaceDisplay, a concept for a mobile VR HMD that is designed having the HMD User and the environment with all other people (e.g. friends, family and strangers) in mind. FaceDisplay consists of three displays arranged around the backside of the HMD to function as a visualization for the Non-HMD User (Fig. 1). To further enable a form of interaction, we attached a Leap Motion facing outwards allowing for gestural interaction. Additionally, we used capacitive touch displays to enable a second form of interaction by actually touching the HMD. We implemented three example applications (FruitSlicer, SpaceFace and Conductor) to show how different visualization and interaction metaphors can be used inside each application.To investigate what specific interaction implications and social dynamics arise from such a concept, we conducted an exploratory user study (n=16). We recruited participants in pairs and let them interact with two applications (SpaceFace and Conductor) each as the HMD User and Non-HMD User, focusing on enjoyment, presence, social interaction and discomfort. We found that FaceDisplay enables the Non-HMD User to understand what the HMD User is doing and results in an equally enjoyable experience for HMD User and Non-HMD User. Additionally, we found a strong imbalance of the power level, putting the Non-HMD User in a more dominant position and derive design considerations based on our insights for co-located asymmetric virtual reality. We conclude by proposing a change in design perspective for future mobile VR HMDs. We argue that mobile VR HMDs should be designed having not only the wearer in mind (HMD User) but also the surrounding and everyone part of it. To truly overcome the future challenges for mobile VR, the negative aspect of isolation of the HMD User should be reduced.The main contributions of this work are:\u2022 The concept of FaceDisplay and the broader vision of designing mobile VR HMDs not only for the wearer, but also including people in the surrounding.\u2022 A prototypical implementation of such a VR HMD and three example applications -each presenting multiple aspects of this novel design space.\u2022 Results of an exploratory evaluation (n=16) explaining the implications such a design has on enjoyment, presence, social interaction and discomfort and deriving design considerations from these findings.", "relwork": "Our work is strongly influenced by the fields of Mobile/Nomadic VR, Asymmetric Interaction/Collaboration for VR/AR and Asymmetric Co-located Gaming.Mobile/Nomadic VR Since 90s' VR technology was not mature enough, the field of mobile and nomadic VR only became relevant in the more recent rise of VR around the 2010s. By combining a piece of cardboard, two lenses and a smartphone a simple VR viewer can be realised [4]. Google created Cardboard VR, one of the currently most spread mobile VR HMDs [17]. Following this trend, more smartphone-based (e.g. Samsung GearVR, Google Daydream) and self-contained (e.g. VIVE Focus) mobile VR HMDs were presented as consumer devices. This spread of VR technology into everyday consumer devices created the demand for HCI researchers to understand and design interaction concepts suitable for the nomadic VR usage scenario [18].Several projects explored different input techniques designed for uninstrumented environments that work solely by modifying the HMD and without additional accessories [45,28,19,31]. Smus et al. presented in [45] the original implementation of the magnetic input concept used throughout most first generation Cardboard VR viewers. Kent Lyons further enhanced this approach by extending the input from a binary selection to 2D input capabilities by applying magnetic field sensing to track the magnet on the side of the enclosure [31]. Instead of enhancing the magnet based interaction on the Google Cardboard, Kato et al. presented an modified Cardboard viewer that uses capacitive stripes attached to the case and running onto the normally unreachable touchscreen of the smartphone. This allowed users to create custom interaction interfaces and further extended the input space from the side of the HMD onto the backside of the HMD [28]. This form of back-of-device interaction for mobile VR was further explored and presented by Gugenheimer et al. [19].A variety of research on mobile VR is conducted within the field of haptic feedback. Having the constraints of an uninstrumented environment and no accessories, researchers focused either on ungrounded haptic feedback systems [21,41] or tried to leverage the feedback in the environment [23,34]. FaceDisplay on the other hand focuses more on exploring the design space of the interaction and uncover the underlying social dynamics occurring from this co-located asymmetric scenario. Misawa et al. presented a similar technical setup having a display attached to an HMD [35]. However, the focus was on enhancing telepresence and not in the field of virtual reality. To the best of our knowledge, FaceDisplay is the first concept enabling co-located asymmetric interaction for mobile virtual reality.enable collaboration and interaction between people using AR technology and further incorporates work with asymmetric setups (e.g. different visualization and different input capabilities [7,46]).The Studierstube [43]  Similar approaches for asymmetric collaboration were also explored in the field of virtual reality [12,36,11,25,20]. Duval et al. presented an asymmetric 2D/3D interaction approach which allowed Non-HMD Users to interact with users sitting at a PC [12], leveraging the advantage of each individual representation (2D and 3D). Oda et al. presented another asymmetric interaction between a remote user and a local user wearing an AR HMD [36].In a user study, the remote user had to explain a specific task to the local user either through a 2D interface or a VR HMD. The results show that local users understood faster when the remote users actually demonstrated the task wearing a VR HMD in comparison to writing annotations with a 2D interface. Also closely relevant to our work were projects exploring an asymmetric \"god-like interaction\" with the goal to enable people build worlds together [11,24]. HMD Users could collaboratively create virtual environments with users at a PC. A similar approach was shown by Ibayashi et al. with DollhouseVR [25]. Most recently Gugenheimer et al. presented ShareVR, a projection-based concept that enables asymmetric co-located collaboration between a HMD User and Non-HMD Users. FaceDisplay follows a similar motivation but needs a different solution to satisfy the restrictions (e.g. no instrumentation, no accessories) of the nomadic interaction scenario. Additionally, we expected different social dynamics than with ShareVR due to our on body touch interaction and more extreme level of asymmetry.", "rq": "Our main research questions were: (1) What social and interaction dynamics arise from FaceDisplay, (2) how do people perceive the physical interaction as HMD User and Non-HMD User and (3) how do the roles (HMD User and Non-HMD User) and interaction concepts (touch and gesture) impact enjoyment, presence and emotional state."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/28.xml", "intro": "Meetings are a primary mode of work [58], but many employees find them frustrating and even counter-productive when good meeting practices are lacking or violated [1,46]. The violations of general meeting norms and disrespectful behaviors have been shown to be negatively correlated with meeting effectiveness and satisfaction [45]. In 2020, the \"stay at home\" orders and travel restrictions of the COVID-19 pandemic dramatically accelerated the use of the video-conferencing meetings for work. By March 2020, daily usage of video-conferencing services such as Zoom and Microsoft Teams had increased by 300% and 775% respectively, and video-conferencing apps jumped to the top of the Apple app store. 1 Although video-conferencing has the potential to reduce the cost and effort behind organizing travel, space, and scheduling of in-person meetings 2 , the \"fractured ecologies\" [37] of videoconferencing can aggravate negative outcomes and marginalize some members of the meeting [38]. Video-conferencing has consistently presented communicative challenges [19,52] and introduced distractions [61] which can lead to ineffective and non-inclusive meetings [23,30]. A primary goal of remote collaboration tools should be to support the most effective meetings possible for all participants. Cutler et al. [9] conducted a large scale e-mail survey on remote meeting effectiveness (N=4,425) at a technology company (pre-COVID- 19) and showed that online meeting effectiveness correlates with meeting inclusiveness, participation, and comfort in contributing. They identify a large potential financial benefit to companies that can achieve these goals, and an opportunity to establish and maintain a workplace culture in which everyone feels free to contribute. There are clearly opportunities for technological solutions that assist attendees in feeling more included and improving meeting effectiveness by helping them understand their own and others' core meeting dynamics.This paper reports on an exploratory study with in situ work teams to identify the challenges they face in video-conferencing based meetings, and proposes a post-meeting feedback system to address the issues. In particular, we aimed to provide insights on the following research questions: (1) What aspects of meetings do videoconferencing attendees need help with?; (2) How can we leverage AI systems to make video-conferencing meetings more inclusive and effective?; (3) How should AI-extracted meeting features (including content, behavioral measurements, and sentiment) be categorized and visualized in a feedback dashboard?; and (4) What concerns exist regarding data privacy and accuracy for such systems?Our work addresses these research questions through a series of user studies and design iterations. Via an initial exploratory requirement analysis survey of 120 information workers, we identified the communicative signals (e.g., participation, facial sentiment) which are important in improving meeting effectiveness and inclusion.We conducted a longitudinal user study to record in situ videoconferencing meetings from eight teams over a four-week period. We used the insights from the requirement analysis survey to create a wireframe prototype of a post-meeting dashboard and 16 participants from the user study teams evaluated the design and helped us further refine the components. Finally, we developed an AI sensing system to quantify these meeting dynamics features and created personalized, interactive, post-meeting dashboards for the participants. Through surveys (N=23) and interviews (N=9), we found that participants were able to become more aware of the group dynamics by reviewing the dashboard. Our study shed light on the privacy concerns participants had regarding such insights within the dashboard. The dashboard also helped participants identify and recollect important events of the past meetings. Our findings also showed that participants perceived that the dashboard would improve meeting effectiveness and inclusivity. In addition, participants expressed that actionable suggestions were more helpful than data visualizations alone. The main contributions of this work are as follows:\u2022 We developed MeetingCoach, an AI-driven dashboard that provides both contextual and actionable insights based on meeting behaviors. \u2022 We implemented both behavioral (e.g., participation) and topical (e.g., questions) meeting dynamics features in our feedback system using state-of-the-art AI. \u2022 We identified and implemented shared and private design approaches for different feature components based on users' preferences from two design iterations and evaluations. \u2022 We demonstrated that MeetingCoach helped improve behavioral awareness and recollection of past meeting events, and bears the potential to improve perceived effectiveness and inclusivity. \u2022 We proposed design guidelines explaining the need for actionable suggestions, reminders or highlights based on timing, and multi-modal feature implementations to be adopted for future video-conferencing feedback systems.", "relwork": "Meeting effectiveness includes both task processing and interaction efficiency by a team [10,14,31,47,53]. Dickinson and McIntyre [10] emphasized the importance of goal specification, entailing identification and prioritization of tasks and sub-tasks, in agendas and other meeting resources. Even with clear goals, though, interaction efficiency has a clear impact on both outcomes and satisfaction. Balanced, active, and equal participation have been found to improve team performance [13,32]. Depending on the type of meeting, equal participation may not always be applicable or feasible, but in a collaborative decision-making discussion, participation from all members ensures at least the exchange of opinions and a sense of \"being heard\", which ultimately improves team satisfaction [32,50]. Turn-taking patterns also influence team performance and satisfaction, as some members may dominate the discussion without realizing they are doing so, reducing time for other members to voice their opinion or expertise [14]. Lawford [31] found that rapport building through verbal and non-verbal signals was an important factor in effective and inclusive discussions. To ensure coordination and rapport, affect management has been found to play an important role in a team's success [4,8]. Barsade [4] has shown that a member's positivity can improve the mood of the whole team, making it more inclusive and improving the quality of decision-making. Cannon-Bowers et al. [8] discussed the importance of effective strategy formulation to consider alternative courses of action in case of disagreement or task failure. Non-verbal gestures, through head nodding and shaking, indicate signs of agreement or disagreement, and levels of interest, acknowledgement, or understanding [22,34].While the face-to-face views of video-conferencing intuitively seem to support the above, they have been found to constrain attention to the non-verbal signals and the overall progress of the meeting [19,23]. Sellen [51] showed that having video did not improve the interruption or the turn-taking rate for video-conferencing meeting participants compared to audio-only ones. This implies that even though video is important in online meetings, it cannot fully resemble in-person meeting dynamics. Especially during long meetings, additional support for monitoring meeting progress and participation may be needed. Taking into consideration these concerns, we designed and developed an automated feedback system to summarize meeting content and attendee behaviors, with the goal toward improving meeting dynamics over time.", "rq": "Our work addresses these research questions through a series of user studies and design iterations. Via an initial exploratory requirement analysis survey of 120 information workers, we identified the communicative signals (e.g., participation, facial sentiment) which are important in improving meeting effectiveness and inclusion."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/262.xml", "intro": "Video chat has become a popular mode of computermediated communication between geographicallydistributed people -for example, it connects grandparents to their remote grandchildren [20], lets distributed friends chat with each other [7,41], fosters intimacy between romantic partners [39], and keeps families connected [1]. In addition to cultivating personal relationships, video chat is used in a range of professional contexts, such as in distributed work teams [23], to conduct job interviews [52], to support online test-taking [60], and in remote psychotherapy [11].The predominant interface design of video chat systems provides people with feedback from their own camerausually presented as a small picture-in-picture window. However, previous work has suggested that people may not necessarily want video feedback during a conversation, but would prefer to see feedback only if their face left the frame [37] or at the beginning of a call to adjust their position in the frame [40]. It can be distracting to see yourself in video chat [13], and even more distracting to be in a conversation in which the other person is clearly watching themselves rather than paying attention to the feed of you.It is not surprising that seeing oneself in video chat can be disconcerting or distracting -although it is the status quo in video-based communication, seeing oneself is not the status quo in face-to-face communication. Researchers have shown that allowing people to see their reflection in a mirror can increase sensitivity to negative feedback in a social interaction [18]. These negative effects of seeing yourself are attributed to an increase in a participant's selfawareness [18], which can facilitate aggressive behaviour in angered people [9], thwart intrinsic motivation [44], and decrease self-esteem [25]. The research using mirror manipulations suggests that seeing oneself in a video chat interface could induce self-awareness and affect resulting communication. The end result of increased self-awareness could be beneficial in some contexts and harmful in others; for example, seeing oneself can increase spontaneous selfdisclosure [27], which could be beneficial in a remote therapy application, but harmful in a remote job interview.Given the prevalence of video chat in both the personal and professional aspects of our lives, and given that the dominant paradigm in video chat interfaces is to have visual feedback of oneself, in this paper we investigated whether seeing oneself affected self-awareness in a video chat. Furthermore, we investigated how seeing oneself affected both the interaction between pairs of participants and the resulting conversation. We created a custom browser-based video chat system that displayed no feedback of the participant or picture-in-picture feedback. We connected pairs of strangers online and presented them with a personal information exchange task in one of the two feedback interfaces. We gathered subjective measures on participants' perceived relational communication and transcribed the audio to perform semantic analysis on the conversations themselves.Our results show that video feedback increased selfawareness and perceived relational affection and depth. However, it also increased the use of anxiety-related words and decreased the use of words expressing certainty. In addition, mixed-gender dyads rated themselves as having more social orientation with feedback than without. This was reflected in their conversations as an increase in inclusive pronouns and words expressing affiliation, and a decrease in words expressing discrepancy. The samegender dyads rated themselves as being more task oriented than the mixed-gender dyads when feedback was provided. This task focus of the same-gender dyads was reflected in an increased use of interrogative terms (e.g., 'what', 'how') and 'you'-centric words with feedback than without -their task was to engage in an information exchange, thus the increased use of these words suggests greater task focus (i.e., asking the other participant questions about themselves).We make several contributions. First, we show that visual feedback in video chat interfaces increases self-awareness and affects a person's perceived ability to relationally communicate. Second, we show that visual feedback increases social accommodation in conversationparticularly for mixed-gender dyads. Reduced expressions of conviction and discrepancy and increased expressions of social affiliation suggest that participants were more concerned with how others perceived them when they could see their own video feed. Third, we discuss how the increased conversational accommodation when feedback was provided -particularly for mixed-gender dyads -has implications for the design of video chat interfaces in contexts from remote therapy to online dating.A simple choice in interface design -whether or not to show visual feedback -influences how we view ourselves in a social interaction and how we engage in a social conversation. Video-based communication is becoming a common way for people separated by distance to communicate in both personal and professional contexts; our interface design choices have the power to influence how online relationships are formed and fostered.", "relwork": "We present research on feedback in video chat and on how feedback may affect self-awareness for different people.", "rq": "We created a custom browser-based video chat system and connected pairs on strangers in an online experiment either with feedback of themselves or without. Our work was guided by our five main research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/128.xml", "intro": "A pivotal point that can popularize Virtual Reality(VR) is natural communication; even in VR spaces, the facilitation of natural communication will not only provide unusual experiences such as space traveling and exploring space but also encourage the transformation of lifestyle and social structure. Natural communication in VR spaces would efects several positive efects to attain a sustainable society, such as improving work-life balance by eliminating movement such as for commuting and business trips and reducing energy consumption for passenger transportation. An essential factor for natural communication in VR spaces include methods that help express nonverbal behavior. However, these methods to express nonverbal behavior of intentions and feelings that includes expressions, hand gestures, and gaze are still in the rudimentary stages of development.The need for remote communication has increased dramatically with the impact of the corona virus(COVID- 19). The current mainstream for remote communication are video conference systems such as Zoom and Skype and usage of them is growing rapidly. However, they have not completely replaced face-to-face communication, and there are still some situations where face-to-face communication is preferred. A reason for preferring face-to-face interaction is that video conference system does not express nonverbal behavior compare to face-to-face communication. Therefore, it is difcult to apply the video conference system in a creative problem-solving discussion such as brainstorming.Avatar communication in VR space would communicate nonverbal behavior better than video conference systems, even though current afordable standalone VR devices are restricted with regard to expressions, gestures, and gaze.A video conference system such as Skype provides symbolized icons to express feelings and intent, as shown in Fig. 1. In online games with numerous players, known as MMO(Massively Multiplayer Online), a function called \"Emote\" is provided to make players' character express symbolized emotions. Social VR platforms such as VRChat and Sansar also implements a function called Emote to provide participants' avatar certain gestures [17]. Although the symbolic gestures using Emote are useful to express intent in VR spaces, very little research has been done to study the efectiveness of an avatar's symbolized gestures in expressing participants' sentiments and intent.This study examines brainstorming in VR space to determine the efectiveness of symbolic gestures through avatars for expressing the participants' behavior and sentiments under three experimental conditions: with and without avatar's symbolic gesture compared to face-to-face interaction communication.", "relwork": "Related work in the feld of computer-mediated communication (CMC) investigates the merits of diferent communication mediums, such as audio, video, and face-to-face, from the perspective of collaboration. Issacs and Tang [6] compared three diferent media-audio, video, and face-to-face-to understand how videos are efective in remote collaboration. They note that video communication is more efective compared to an audio-only communication, as it provides nonverbal information such as nodding and facial expressions. However, they also note that video communication is not as efective as face-to-face communication because nonverbal communication cues such as body positions and eye gaze are comparatively less. Therefore, to investigate the benefts of video communication over face-to-face communication, advanced studies have been conducted to improve communication and creativity by deforming visual information.To provide an example, consider two studies on improving video communication. One study increases number of answers in brainstorming by increasing artifcial smiles on the display using image processing [10]. The other study increases closeness by deforming a partner's face expression by mimicking the expression of a user [16]. These studies suggest that artifcial facial deformation can evoke positive psychological efect. Deforming facial expressions is efective not only in physical humans but also on avatars [11]. The study also suggested that not only the expressions but also the symbolic gestures by an avatar have a positive psychological efect in users [12]. As mentioned above, even in video communications with limited nonverbal communication cues, deforming nonverbal information improves the quality of communication. Riva et al. [13] suggested that a VR environment with a sense of presence tends to exploit emotion more than other mediums. Therefore, research results in improving video communication, which deform the appearance and behavior of a user's avatar, can be more efective in a VR environment.A recently developed VR technology provides nonverbal information that are richer compared to video communications. The VR technology can be useful as an interface to support group discussions. Recent research in VR and psychology has shown that a sense of ownership of an avatar's body afects self-perception and behavior [2,[7][8][9]19] . For example, Banakou reports that an avatar that imitates Einstein improves the cognitive capacity of a user owing to a sense of ownership of the avatar's body [3].Many of these previous studies have examined the efect of an avatar's appearance and other expressions on a single user's task performance and psychology.Previous studies investigating the efect of avatar expressions on the communication between multiple users in a VR space include research by Smith et al. [15] and Yoon et al. [20]. Smith et al. [15] investigated the efect of avatar expressions on social presence and communication through an experiment using two workers in a VR space under three conditions: face-to-face communication, and with and without avatars.Yoon et al. [20] explained that avatars can provide a rich collaboration experience for participants, even though the cartoon-style upper body has environmental restrictions and is subject to collaboration between participants.These previous studies have examined the efect of diferences in avatar appearance on communication.Tanenbaum et al. [17] compared the design of nonverbal information across 10 social VR platforms such as VRChat and Sansar, They described that an avatar's preset animation, called \"Emote, \" is the most common expressive nonverbal communication in virtual reality. Although the symbolic gestures made using Emote are useful to express intent in a VR space, little research has been done to study the efectiveness of an avatar's symbolized gestures in expressing participants' sentiments and intent. Therefore, our study aims to investigate the psychological efects of symbolic gestures expressed by avatars.", "rq": "We collected several data with regard to behavioral and psychological points to discuss a research question. The collected data is the number of created ideas for each session, total utterance duration, utterance frequency, balance of participation, usage count of symbolic gestures, and questionnaires."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/17.xml", "intro": "Real-time captioning provides vital spoken conversation access for many d/Deaf and hard of hearing (DHH) people. Both human-generated and automatic captions have received substantial attention from HCI and CSCW researchers, with a focus on captioning in classrooms or other formal environments (e.g., [13,37,43]). While human transcription (e.g., CART) is most common in these settings, researchers have also examined automatic captioning, particularly for constrained environments like classroom lectures; commonly there is a single dominant speaker in these settings and the vocabulary used is more predictable [1,37,71]. In contrast, captioning for more informal small-group and one-on-one interactions has received less attention, despite the fact that human captioning's high cost ($60-$200 an hour in the US) and need for advance scheduling crea te significant barriers in t his context [73,75].While automatic captioning is an increasingly viable alternative to human captioning [38,61], its accuracy varies from 9-37% word error rates across tools [38]. Unlike human captioners, automated techniques cannot convey non-speech context (i.e., visual references, emotion, emphasis) nor can it intervene to improve communication. Moreover, small-group conversation's interactive nature, flexible social dynamics, and high level of spontaneity further limit existing captioning services. Ultimately, captioning and other access tools (e.g., paper and pen, texting, notes apps) all come with limitations and do not fully support DHH people during small-group conversations [20].Despite the sociotechnical nature of small group captioning, most prior work has only examined technical considerations, such as how to convey uncertainty in automatic captioning through the use of simulated conversation in controlled experiments [7,59,60]. Seita et al. offer exceptions that explore how social interactions and behaviors impact captioning [65][66][67]. They first found that hearing people speak more loudly, clearly, quickly and with non-standard articulation when they are being captioned in small-group conversations [65]. In a preliminary [67] and follow up study [66], Seita et al. had a hearing actor modulate their conversation behaviors in various ways as part of a controlled experiment (e.g., speech rate, voice intensity, eye contact), and measured what behavior variants DHH participants preferred (e.g., fast, medium, or slow speech), and which behaviors were most important. They provide quantitative evidence that hearing people's behaviors impact DHH people's experiences of one-on-one captioned and interpreted conversations. These findings motivate the need to more deeply understand DHH people's small group captioning experiences through a sociotechnical lens. In this paper we address the questions: What social, environmental, and technical factors impact the use and usefulness of captioning in small groups? What opportunities exist to design captions and caption d isplays in w ays that support more accessible group com munication p ractices?To begin addressing these questions and to ground future small group captioning technologies in the needs and desires of DHH people, we conducted an interview and design probe study with 15 DHH participants. Each session began with an interview covering the participant's experiences with real-time captioning in small-group conversation and their perspectives on the role of hearing conversation partners in creating or obstructing accessibility. Participants then completed a design probe activity, building on methods outlined in [24,26,33,51]. In this activity, we presented a series of potential future captioning features (e.g., displaying speech rate, flagging overlapping speakers, supporting error correction by hearing people) to provoke discussion around what new designs participants desire and how that technology could be integrated into small-group conversations.Our findings highlight the myriad social (e.g., group norms, preferred communication modes), environmental (e.g., furniture configuration, online availability of a text chat), and technical (e.g., caption lag, built-in speaker identification) factors that shape real-time captioning, contributing an understanding of the context that surrounds captioned conversation. Particularly, we find that: (1) captioning's efficacy is highly determined by the group being captioned, (2) current captioning tools are often insufficient during interactive Social, Environmental, and Technical: Factors at Play in the Current Use and Future Design of Small-Group Captioning 434:3 conversation, and (3) while the lack of visual and spatial information online create barriers, features of video conferencing also provide new opportunities to increase access. Participants' responses to the design probe activity also highlight the potential to create more captioningfriendly environments, both online and in person, and suggest that providing conversation feedback and warnings to guide captioning-friendly group norms is a promising direction for future development. Based on these findings, we discuss the need to consider the intersection of social, environmental, and technical factors in captioning research, propose a reframing of captioning as a group technology, and put forth future design guidelines that center DHH peoples' needs.More broadly, we contribute (1) an empirical account of DHH participants' experiences of small group captioning which highlights how social, environmental, and technical factors impact its use and efficacy, (2) an exploration of design opportunities to support small group captioned conversations and future design guidelines, (3) an understanding for both (1) and (2) of how online environments-a historically little-studied captioning context-shape captioning experiences and preferences, and (4) reflections on reframing captioning as a group technology.", "relwork": "To contextualize our study, we analyze current captioning methods, caption use and design, and provide a Deaf and disability studies framing.", "rq": "Recognizing that these factors must be considered together to fully contextualize the use of captioning technology has implications for how we as HCI and CSCW researchers work. When formulating research questions, designing studies, analyzing data, and reviewing papers, researchers should consider and seek to account for social, environmental, and technical influences on captioning technology. Many proposed captioning designs have been evaluated out of context (e.g., ) or in terms of a narrowly defined outcome (e.g., improved comprehension  or performance ), and future work could complement these analyses with a focus on their social, environmental, and technical contexts. As researchers move to consider the role that hearing people play in conversational accessibility, findings from controlled experiments, such as work done by Seita et al. , could be contextualized by qualitative work focused on social relationships (e.g., ) and the environments in which technology is u sed (e.g., ). "}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/259.xml", "intro": "Mobile devices such as smartphones, tablets and headmounted displays present new opportunities for sharing experiences and communicating with others through mobile video chat (e.g. [5,38,24,16]). They are now being used to share experiences such as treasure hunts [38], activities such as walks [46] or bike rides [35], and as tools for supporting ad hoc assistance such as repair (e.g. [12,18,9]). Researchers have also explored novel camera and devices for sharing activities with others [24,28,16,21,22].The problem with conventional mobile video chat tools is that the remote participant has little autonomy over the video scene-their view is strictly controlled by the other participant in the chat. This creates asymmetries in how the two participants can view and explore the environment: namely, the remote participant's access to the environment is mediated by the local participant (i.e. they can only see what the local participant points the camera at) [20]. While the remote participant is at a noted disadvantage, in addition to participating in the video chat, the local participant is now also responsible with performing camerawork: framing subjects or objects in the video stream, providing effective overviews, and steady camerawork [20,30].Researchers have tried to address this asymmetry through novel hardware and software. For example, several researchers have explored novel hardware configurations of cameras such as head-mounted (or similar) approaches (e.g. [18,24]), telepresence robots [39], and even drones [19]. Other researchers rely on different types of form of scene reconstruction (e.g. using depth cameras), thereby allowing the remote participant to independently control his/her view into cached portions of the scene (e.g. [12,21]).In this paper, we study the use of a fixed, streaming 360\u00b0 camera on a monopod (affixed to a backpack carried by the local participation), where the video is streamed to the remote participant. This prototype gives the remote participant the freedom to visually explore the environment independently of the local user, while giving the remote participant a view of the local user's hands and head orientation. We were focused on three research questions: Collaboration DIS 2017, June 10-14, 2017, Edinburgh, UK \uf0b7 How do remote participants explore the environment, and contribute to collaboration with a 360\u00b0 video chat? \uf0b7 How do collaboration challenges manifest themselves with a 360\u00b0 video chat?To address these questions, we designed a study to simulate a guided tour (e.g. a remote tour of a factory), where participants would explore a part of our university campus together. We see this scenario as a common future scenario, where remote participants will need to rely on the local participants to guide them around or use 360\u00b0 video panning feature to explore the environment (e.g. remote assistance). 16 pairs of participants used our prototype to perform a photo walk tour, where the remote participant directed the local participant to take photos of certain landmarks at specific angles. Our findings show that the prototype gave remote participants agency in the interaction: they could comfortably and confidently explore the environment independently, enough so that they could assist the other participant in visual search tasks. On the other hand, because participants could not easily understand what the other could see in the environment, we observed small communication break-downs between pairs. Thus, some of the benefits of having independent views for collaboration were offset by the absence of gaze awareness between collaborators.We make four contributions in this paper: first, we contribute the one of the first studies exploring the impact of view independence in 360\u00b0 video chat; second, we describe findings from our study that outline opportunities for distributing collaboration with this approach; third, we identify challenges that arise from the view independence afforded by this configuration, and finally, we provide concrete design guidance by outlining how opportunities to address these challenges in future 360\u00b0 video chat designs.", "relwork": "Video media space research has long focused on bridging the gap between fixed, remote spaces (e.g. [13]). More recent efforts explore how video chat from mobile devices can connect people with others in remote environments, rather than fixed spaces (e.g. [30,20,16,24]). As it turns out, many communicative challenges that researchers are encountering with mobile video chat prototypes are reminiscent of challenges encountered decades ago: the problem of joint reference and fractured ecologies. We outline these problems to contextualize our work.", "rq": "In this paper, we study the use of a fixed, streaming 360\u00b0 camera on a monopod (affixed to a backpack carried by the local participation), where the video is streamed to the remote participant. This prototype gives the remote participant the freedom to visually explore the environment independently of the local user, while giving the remote participant a view of the local user's hands and head orientation. We were focused on three research questions: Collaboration DIS 2017, June 10-14, 2017, Edinburgh, UK \uf0b7 How do remote participants explore the environment, and contribute to collaboration with a 360\u00b0 video chat? \uf0b7 How do collaboration challenges manifest themselves with a 360\u00b0 video chat?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/258.xml", "intro": "Consider the following scenario: \"Beth, the only woman on a team of developers, always looks for an appropriate moment to interject during a staff meeting. But whenever an opening arises, louder voices quickly seize the opportunity to fill the silence. Beth's voice is drowned away, and she is rarely, if ever, heard. Even though Beth is by nature a confident and assertive person, her group members label her as shy and unassuming. \"The above scenario reveals how slight differences in preference for conversational mechanics affect which ideas are heard and how people are judged [50]. These behavioral dynamics are common, and if not addressed properly, can impede team productivity. Thus, analyzing and reflecting on these dynamics may positively impact productivity and foster positive and respectful relationships toward other group members. In this paper, we explore that possibility using a video conferencing platform. Despite the importance of understanding group Equal contribution as the first author 160:2 \u2022 S. Samrose et al. dynamics in a video-conferencing environment, it has not been studied extensively in the past. No existing videoconferencing platforms-including Skype, Google Hangouts, GoToMeeting, WebEx, ooVoo, and others-allow easy access to an API to automatically capture the audio and video feeds.What if, with participants' consent, we could automatically analyze their audial and visual data and offer intelligent analytics on conversation dynamics? Imagine the questions we might answer. Did everyone participate equally, or was it a one-sided conversation? What was the overall emotional valence? Did people talk over one another? Did they share smiles? How did participants act toward one another?Due to the proliferation of smart phones and tablets, video conferencing is gradually becoming a popular alternative to traditional face-to-face meetings, which require travel, space, and other advance planning. One survey reports that 76 percent of decision-makers use and recommend video conferencing without hesitation [3]. Ninety-six percent of participants surveyed agree that video conferencing increases average attention span from 23 minutes (on regular calls) to 35 minutes. It can also, however, reduce attention devoted to nonverbal cues [57]. Virtual meetings have introduced new challenges. But they have also opened up opportunities for computer-mediated feedback to address their shortcomings.Another challenge is translating the raw, sensed data into insights that are helpful to participants. The effectiveness of a group discussion depends on many parameters, such as those related to group type (one-time vs. ongoing groups), those specific to individual participants (personality, background, etc.), those related to the topic(s) of discussion (open-vs. set-agenda plans), and more. Some groups may consider the outcome of the discussion the most important factor-in a business meeting, for instance-while other groups may focus on strengthening cohesion among team members. Optimal participant behavior may be difficult to establish, as it hinges on both group dynamics and the desired result. In this paper, we present a fully-automated, online collaboration platform: Collaboration Coach (CoCo). CoCo allows multiple users to video chat and receive feedback on group dynamics as well as individual communication skills. The CoCo platform includes both a video conferencing system and a virtual feedback assistant. The conferencing system allows us to capture participants' audio and video feeds. Before a video conferencing session, participants agree to enable their webcam and to have the conversation's audio and video data transmitted to our server. Using the video feed, we analyze facial features to determine the aggregate emotional valence of participants, participants' attitudes toward one another, and how often users shared a smile with other members. Using the audio feed, we analyze how much participants talk (participation), how much each participant interrupts and gets interrupted (overlap), and who spoke after whom (turn-taking). The virtual feedback assistant, a chat-based interface, relays complex performance metrics CoCo: Collaboration Coach for Understanding Team Dynamics during Video Conferencing \u2022 160:3 in an easy-to-understand format and provides feedback and conversational insights to participants in a positive and constructive way. To validate our framework, we conducted a pilot study with 39 participants. We randomly split the participants into groups of four-and one group of three-resulting in 10 groups. Each group participated in two discussion sessions with two popular team-building tasks called Lost at Sea [1] and Survival on the Moon [24]. The ordering of the tasks are counterbalanced across all groups. In Lost at Sea, participants imagine themselves escaping a sinking ship, ranking in order of most useful for survival a pre-defined set of items. (The actual best ranking of the items is annotated by the U.S. Coastguard.) The Survival on the Moon task follows a similar pattern in which participants, as crew members of a malfunctioning space ship, have to again rank the available survival items. After receiving their scenario, each participant orders their own list. Then the participants uses the CoCo system to devise a unified, final ranking of the top five items, together representing the team's decision. CoCo records audio and video of the discussion using participants' webcams and microphones. From the audial and visual data, we extracted the facial and prosodic features of the participants. These features include: participation, interruption, turn-taking, emotional valence, attitude (engagement, attention, surprise, anger), and shared smiles. After each team made its final decision, our chat-based virtual feedback assistant debriefed each member with quantitative metrics about group and individual conversational nuances. The feedback assistant does not advise participants to change their behavior-rather, it informs them of the group and individual effects of the performance features in a positive, constructive way. We opted for this design to allow humans to control their own behavior modifications instead of influencing them with predefined suggestions. After the feedback session, the teams engaged in another round of group discussion through the video conferencing system, focusing on the other task.We noticed a statistically-significant change in balanced participation-that is, everyone spoke more equallyduring the second round. Participants' self-evaluations of their awareness of their own conversational skills, their teammates' conversational skills, and how often they allowed others to speak also showed statisticallysignificant improvement. We believe the proposed CoCo system improves the efficacy of video conferencing environments with the integration of analysis and feedback tools. Because our framework is publicly available at https://github.com/ROC-HCI/CollaborationCoach_PostFeedback, it could be used by other researchers to collect more virtual meeting data and expand our understanding of group dynamics.In summary, this paper makes the following contributions:\u2022 The development of a fully-automated video conferencing system that captures nonverbal features of both individuals and teams from participants' audio and video feeds; \u2022 The design of six crucial group interaction features (participation, speech overlap, turn-taking, emotional valence, attitude, and shared smile) and four sub-features (attention, anger, surprise, engagement, under attitude) to better describe the complexities of group discussion; \u2022 The implementation of a chat-based system that converses with participants and constructively reveals nonverbal metrics after the video conferencing session ends; \u2022 The design of a two-stage, team-building experiment with 39 participants (spread over 10 groups) to validate the developed framework and its potential effects; \u2022 The analysis of group performance and behavior metrics from both qualitative and quantitative perspectives to better understand team dynamics, in general.", "relwork": "An automated system that allows individuals to conduct a video conferencing session and receive automated feedback on team dynamics culls knowledge from affective computing, collaboration, teamwork, and rapport. In this paper, we reviewed two primary sources of information: systems that provide feedback on team dynamics and literature on teamwork and rapport. We first discuss existing systems intended to better understand team dynamics that also provide a feedback mechanism. We highlight the fundamental features that underpin effective group collaboration. We discuss the particular contributions to teamwork of features like turn-taking, overlap, participation, and sentiment.", "rq": "For researchers, CoCo can also help in understanding, evaluating, and developing better approaches for delivering feedback. Our feedback assistant provides information about every feature by explaining its importance. It also elaborates on the different possibilities of that feature's use in conversation. Whether this explanatory approach is more useful than providing a user with unexplained and direct feedback remains an open and interesting research question. Future research can build up on the CoCo framework to further investigate."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/243.xml", "intro": "In the recent big data era, scientific experiments need to handle massive amounts of heterogeneous data [43,47]. While data-intensive experiments open up the possibilities for interesting discoveries (such as through statistical analysis and applying advanced machine learning algorithms), there are several challenges with complex, data-intensive computation and the analysis process, including dealing with failure handling, optimal task scheduling, big data visualization, distributed job execution and real time execution monitoring [43,46]. The effective analysis and management of complex, multi-dimensional, and high volume data is challenging for an individual and often requires collaboration with multiple scientists [87,88]. In any case, scientific research often demands collaboration among scientists from multiple domains and with diverse expertise [46,73,74,87,88].Given the collaborative nature of modern complex scientific experiments, recent research identified Computer-Supported Collaborative Work (CSCW) technologies as necessary to support scientific experiments that require collaboration among multiple researchers [13,27,34,88]. Jirotka et al. determined that, while the relationship between scientific experiments (i.e., e-Science) and CSCW is relatively nascent, they together could yield significant benefits in answering complex research questions and in important knowledge discoveries [34]. With this motivation, we studied the concept of collaboration or groupware systems in the context of Scientific Workflow Management Systems (SWfMSs) for aiding with complex scientific experiments among multiple scientists via realtime collaboration. In this paper, we present several challenges and differences with collaborative SWfMSs in contrast to text or graphics editing groupware systems, and present a general framework for collaborative SWfMSs that leverages CSCW technologies to support scientific experiments.A scientific workflow is a facilitation or automation of a process as a part or whole [3,33] during which the targeted data are passed from one computational step to another for certain actions or processing as per some set of pre-defined rules or instructions [3,33,43]. A SWfMS automates a scientific workflow life cycle: composition, deployment, execution, and analysis [43,47], which is discussed in detail in Section 2. While SWfMSs are widely used in recent years for handling and managing the overall execution of complex scientific experiments [43,46], none of them directly support collaborative work among multiple users; hence users need to follow several time consuming manual steps for any required collaboration on a given data analysis task [73,74,87,88]. For example, for a collaborative design of a scientific workflow, a user first builds a part of a workflow (e.g., a sub-workflow), exports it from the local workflow engine and shares it with a collaborator for possible updates on the sub-workflow. Around 3910 such scientific workflows have been shared among 10665 members (as last noted in August 2018) for collaboration in myExperiment [14] a shared social space for scientific artifacts. The manual collaboration process is repeated a number of times to complete building the entire workflow comprising of several sub-workflows. This manual back and forth process for collaboration is often very time consuming, does not support real-time editing, and is often impractical as the collaborating group size increases over time.While the above statistics and scenario reveals the importance and necessity of collaborative SWfMSs, designing a real-time groupware system for workflow collaboration is non-trivial and differs from text or graphics editing groupware systems in a number of ways: i) Different Roles, scientific experiments often require adequate access control policies for sharing the workflow components, data products and provenance information among researchers with varying roles [5] and in the context of scientific data analysis, the varying roles might include domain user, pipeline composer, tool developer, data specialist, and so on depending on a given use-case scenario [46] (we present further details on varying roles in Section 5.3); ii) Collaborative Job Scheduling, collaborative job scheduling is required to orchestrate and efficiently schedule the independent workflow execution requests of researchers [87,88]; iii) Collaborative Job Management, in addition to the primary requirements of workflow job execution, monitoring, or failure handling, collaborative SWfMSs need to have a feedback system to orchestrate the overall data analysis process among the collaborators; iv) Plugin Architecture for Collaboration, for effective collaboration among research groups, a collaborative SWfMS must allow easy and real-time plugins of workflow tools; and, v) Collaborative Data Visualization, a collaborative SWfMS should facilitate collaborative data visualization to fully exploit collaborative data analysis.To address these challenges and requirements, we present a framework towards an effective design of a collaborative SWfMS for scientific data analysis. Our proposed framework adopts a plugin based architecture for workflow tools. As a proof of concept of the proposed framework, we also implement a collaborative SWfMS SciWorCS. As our proposed framework is not restricted to any particular research domain, we evaluate it with use-cases from two different research areas Designing Groupware Systems to Support Complex Scientific Data Analysis 9:3", "relwork": "In this section, we first present related work on CSCW in aiding with scientific experiments (i.e., in Section 3.1), and we then discuss recent work for supporting collaborative data analysis with SWfMSs (i.e., in Section 3.2).", "rq": "\u2022 RQ 1: How can we design collaboration in scientific workflow management system? \u2022 RQ 2: How can we implement a real-world collaborative SWfMS that is functional and performant by supporting consistency management, sub-workflow execution, visualization in various formats and so on? \u2022 RQ 3: How can we utilize our collaborative platform for different scientific analysis domains?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/23.xml", "intro": "Collaborative writing is an increasingly integral part of professional and academic work. Experts in business, engineering, and law report that they write together more often and more complex than ever before [18,48]. As such, a rich set of web-based collaborative writing systems has been developed, including tools such as Google Docs and Overleaf that enable new ways of writing together. In particular, synchronous writing, which refers to two or more people editing a document simultaneously, has emerged as a widely adopted practice. In contrast to the early 2000s, when synchronous editing was not well-received [24], a recent study focusing on undergraduate students showed that 95% of student teams used synchronous writing in their collaboratively-written class assignments [41]. The popularity of collaborative synchronous writing creates challenges and opportunities for researchers as well as the developers of such editors.In this paper, we focus on the collaborative synchronous writing experience in remote settings. It is crucial to consider the increasing level of distributed work as the number of employees working primarily from a location outside of their place of employment has tripled over the past 30 years [31]. The significance of remote collaboration was reflected in a survey we conducted on collaborative academic writing, where 73% of the respondents had participated in remote collaborative writing at least a few times in the past year.Our work draws inspiration from research that has shown the benefits of writing together at a distance. For example, a study showed that distributed groups who used an internetbased collaborative writing tool that enabled synchronous editing produced lengthier documents of higher quality than groups that used traditional word processors [30].However, it is challenging to maintain group awareness and mutual understanding while writing together in a remote setting. In addition to the overall complexity of the collaborative writing process, the physical distance between co-authors creates further challenges. Contrary to co-located, remote computer-supported cooperative work tools often limit implicit references such as deixis or gaze direction [37]. Therefore, specialized collaborative writing platforms need to provide enhanced coordination, group awareness, and collaborative writing activity support [30]. One possibility to improve these aspects is to build awareness tools that improve the comprehensibility of discussion about the document as well as allow efficient use of references [4]. In previous research, this has been achieved with the use of dual eye tracking technology, however, the context has been limited to collaborative game-playing [23,35,36] and pair programming [10,37]. No prior studies have combined research in synchronous collaborative writing and gaze visualization.The goal of this research is to study the potential usefulness of gaze sharing during the process of collaborative writing. In addition, we examine the effect of gaze sharing on the level and quality of collaboration by conducting a study on dyads (i.e. pairs). The study focuses on academic writing as its constraints on time-efficient delivery and length of text are likely to promote synchronous collaborative writing.We developed Eye-Write, a novel tool that incorporates gaze awareness functionality, to investigate the effect of gaze sharing on collaboration. Eye-Write extends the open-source collaborative text editor Firepad. Based on dual eye tracking technology, it visualizes the gaze location of each co-author and overlays it in real time on each of the active viewports of the mutually-edited text document.", "relwork": "Collaborative writing is an iterative and social process that involves a team focused on a common objective that negotiates, coordinates, and communicates during the creation of a common document [29]. Writing together has been shown to promote learning and encourage initiative, creativity, and critical thinking [26]. Recently, many scholars have become interested in synchronous remote collaborative writing, for example, in academic settings [46]. Collaboration at distance has also become a center of interest for researchers in the field of eye tracking. With the use of dual eye tracking technology, the shared gaze awareness has shown potential as a means to facilitate communication in remote problem solving [8,10]. In this section, we explore literature on collaborative writing and eye tracking.", "rq": "Open Research Questions"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/126.xml", "intro": "Tangibles have been used in educational settings as they have the potential to support the learning of abstract concepts [31,64]. One area of education where tangibles can be utilized to make abstract concepts more accessible is data visualization comprehension. Questions about teaching data visualization have become a frequent topic within the visualization community [8,21,59] and education policy [62]. However, learning to explore and make sense of data visualizations is a challenging task [5]. Appropriate tools could support students with visualizing, analyzing, and interpreting complex visualizations of data.So far, methods of interacting with data visualizations have mostly revolved around the use of pen and paper or Graphical User Interfaces (GUIs). These tools support developing an understanding by perceptually examining the representations. In contrast, tangible representations of data, Data Physicalizations [19], can be used to create Tangible User Interfaces (TUIs) [18]. These allow users to manipulate data visualizations more directly. We argue that these tangible representations of data can facilitate a better vehicle for users to grasp the information held in data visualizations than perceptive methods, because they enable a more engaging, embodied way of data exploration.Investigations on the applicability of data physicalizations in educational settings are still sparse [48]. Some studies have investigated how data sculptures can play a role in university courses [57]. Other studies have used tokens to construct visualizations within a workshop context [14] or investigated how people map data values to physical representations [17]. However, these forms of data physicalizations are static and thus lack the educational benefits that interactivity and feedback can provide.To expand on previous research on the design of data physicalizations for learning, this paper presents an exploratory study through qualitative evaluation of four teams of students interpreting scatterplots augmented by data physicalization using a portable Collaborative Data analysis system, namely CoDa (Figure 1). This system realizes a tokenbased interactive tangible scatterplot using an interactive liquid-crystal display (LCD) based on a magnetic tracking system [26]. Through manipulation of the physical tokens on the platform, the users can interact with the visualized data directly; moreover, they can use the control panel on the sidebar to change the modes of visual augmentation for descriptive statistical analysis and comparison across multiple graphs.This exploratory study was conducted to answer the research question; \"How do interactive data physicalizations support the collaborative interpretation of interactive data visualizations?\" In the study, 11 students in groups of 2 to 4, completed a data analysis task with personalized data. The qualitative results, which were analyzed from the dialogues between the students and their interactions with the system, show that the participants can jointly develop an understanding of the data through iterative interpretation processes with CoDa. We report on the students' process of data interpretation, how the interactions with the system aided their interpretations and shaped their collaborative behaviors, and the students' experiences of working with CoDa. And lastly, based on the lessons learned from the design, implementation, and evaluation of CoDa, we present implications for further educational applications of interactive data physicalizations.", "relwork": "Studies in the field of graph comprehension focus on how perceptual and cognitive processes influence the understanding of graphs in an educational context. Graph comprehension is an iterative process. When one switches between the data, the representations, and the underlying mathematical model of visualization, they \"..go through a process of invention, noticing, and revision that helps them develop insight into the relation between representations and the quantities they represent [46, p. 138]. \" Shah and Hoeffner [47] have identified three underlying critical points of a learner's understanding of graphs: understanding the meaning of specific visual representations, understanding of the graph structure, and understanding of the context of the data.Interactive models can increase the reasoning strategies that learners use [58], furthermore interactivity can support learning through the construction of mental models and hypothesis testing [40]. The users of interactive visualizations can develop inferences, test those inferences, and revise their expectations to evaluate concepts. Control over a visualization allows the user to decide what to evaluate next, in line with their interests and preferences.Interactive data visualizations might help students with the interpretation of data. However, Laina and Wilkerson [61] showed that not all middle-school students were able to link overlapping information that the students gathered from interactive data visualizations. There has been an increasing effort on the development of educational tools that support reasoning through interactive graphs [3,30,37,43]. For example, Alper et al. [3] developed an app that aimed to boost elementary school students' visualization literacy through a concreteness fading approach. M\u00e9ndez et al. [30] investigated whether top-down, as seen in iVolVer [32], or bottom-up construction approaches aided creativity and learning with InfoVis tools. However, these works investigate digital tools instead of physical manipulatives, which can provide more sensory engagement, accessibility, and group collaboration experiences for learning abstract concepts [64].", "rq": "A user study was conducted to address the research question: \"How do interactive data physicalizations support the collaborative interpretation of interactive data visualizations?\" Participants. We recruited 11 students (7 female, 4 male) from 3 separate classrooms at two secondary schools in the Netherlands. The average age of participants was 15.4 years old. The 11 participants worked in 4 teams that were formed by themselves; the team members were therefore familiar with each other. We collected their demographical information (Table ) in pre-task questionnaires. To alleviate the interpretation problems that the representational dilemma  could cause, we recruited students who had at least 3 years of foundational math background and were familiar with Procedure. We decided to link the analysis task to the project that the students were currently working on, therefore ensuring that the students would have enough contextual knowledge to interpret the data within the scatterplot. The datasets that the students would analyze were co-constructed with the students three weeks before the study took place. The data was collected in March 2019 from data.world and data.europa.eu, resulting in three different datasets:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/33.xml", "intro": "At the start of the study, participants came to our laboratory to be introduced and instructed on participation in the study. During this introduction, participants first completed a questionnaire to specify their demographics (gender, age, and ethnicity) and describe their prior use of heart rate sensors. The questionnaire also asked participants to list their most commonly used messaging applications (e.g., default phone messaging system, Facebook Messenger, etc.) and to identify the six people they most frequently contacted through those applications within the previous week. The former was used to track which applications they would likely use during the study and ensure that they would be made available through our study application. The latter was used to track whom the participants would most likely select as recipients for heart rate sharing messages. Given the novelty of heart rate sharing and the intimate nature of the heart [27], we expected that participants would primarily share their heart rate with their most frequent and closest contacts (most of the contacts listed were significant others and close friends and family). To track this information, whenever participants shared their heart rate, the application would ask them with whom among these six they shared, or if they shared with someone else (in which case, they were asked to provide that person's initials).While participants answered the questionnaire, an experimenter installed the study application on their phones to ensure compatibility for the study (we required phones with Android versions 5.0+). Participants were then briefed about the purpose of the study and given instructions on using the application and watch. We gave 77:6 \u2022 F. Liu et al.detailed instructions for sharing heart rate through the application, which included two methods: direct sharing (messaging their heart rate value in beats per minute to others), or broadcasting (messaging a URL for their heart rate live-stream to others), which we describe in detail in section 3.4. We also explained (verbally and in the consent form) which data would be sensed and collected during the study (e.g., heart rate data, activity, messages they sent related to the study), and the requisite permissions participants had to grant on their phones.After the experimenter explained the study to the participants, they stepped through the application setup. As part of the setup, participants entered their demographic information (age, gender, height, and weight, which were used to warn participants about potentially dangerous abnormal heart rates), calendar events in which they would be interested in broadcasting their heart rate, the six contacts they most frequently communicate with, and the daily 12-hour waking period during which they agreed to wear the watch and use the application. Then, participants were asked to wear the watch for two minutes while sitting and for two minutes while walking in order to determine an average heart rate baseline for each activity. These baselines were used by the application as part of the logic for prompting participants to share their heart rate (described in section 3.4). Participants were then free to leave and use the wristband and study application for the next two weeks.", "relwork": "A number of systems have been built to sense and monitor biosignals; however, most of these systems have focused on applications for individual use. For instance, popular commercial wearable heart rate monitors, such as the Fitbit or Mio watches, and several research systems have used heart rate to support fitness and physical health [20,44,47,54]. Affective Computing research has expanded biosignals to social applications (in addition to health), detecting emotional and psychological states for social skills training and virtual tutors [4,12,13,32,45]. However, these applications still target individual understanding and monitoring of physiological data.Few works have investigated biosignals systems that allow for sharing in social and communicative contexts. These include systems for supporting interpersonal relationships [42,53,61] and collaboration [55], increasing interactivity and encouragement in physical activities such as marathons [10,43,60], and facilitating engagement in presentations and entertainment [17,22,50]. All of these systems focused on very specific use cases and events, and were tested over short periods. Solv\u00e1k and colleagues sought to build on these prior efforts by investigating biosignals sharing in a more natural setting over a longer period of two weeks. They deployed a technology probe (a laptop that provided visual and aural feedback of heart rate) in the homes of five couples and analyzed their reactions to the probe, finding that heart rate was used as information about emotional states and fostered connection between household members [51]. A recent study by Hassib and colleagues expanded this work by going beyond couples' homes and deploying HeartChat, a mobile heart rate chat application, in the wild with seven pairs of close friends or partners. They similarly found that heart rate sharing was able to foster connection and awareness, and that heart rate acted as both an emotional and contextual cue [21]. However, in both of these past works, participants had limited control over sharing, and described situations in which they might not be willing to share heart rate because it was \"too personal,\" awkward, or not understandable. Authors from both works suggest that sharing one's heart rate could thus potentially undermine impression management.The present work furthers this line of research by deploying a heart rate sharing system on users' mobile phones in order to understand the everyday contexts in which users are most willing or unwilling to share their heart rate. Our system prompts users to make a decision about sharing in order to give them control over when, with whom, and how to share their heart rate with others. Additionally, sharing is conducted through existing messaging applications in order to provide a natural way through which heart rate can be communicated. Our work allows us to explore heart rate sharing at a deeper level, by investigating not only the contexts in which people would be willing to share their heart, but why they might share or not share: RQ1: When and why would people be willing or unwilling to share their heart rate with others?", "rq": "To explore our research questions and understand heart rate sharing in situ, we built an Android application that prompts participants to share their heart rate. We conducted a study that used the Experience Sampling Method (ESM)  to determine when, with whom, and why participants were more or less inclined to share their data using this system, as well as follow-up interviews to probe more deeply to understand users' experience, sharing decisions, and the social and communicative impact of those decisions."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/245.xml", "intro": "Previsualization (previs) is a collaborative process of planning scenes and shots within the pre-production stages of filmmaking. Traditionally, this process has been performed with drawings, concept images, sketches, etc. [Janson 2019], and it is not until recently that previs has been performed with 3D animation tools and software. The use of computer graphics-based (CG) technologies for providing visual effects within filmmaking is a well-established practice. However, the costs of these technologies are substantial, therefore it is important to carefully plan shots that require the expertise of CG professionals [Honthaner 2017]. With recent advancements in Virtual Reality (VR) technologies, the film industry sees a shift where planning for different scenes and takes can be done in a more immersive and tangible way. VR enables a user to be present in virtual environments resembling specific locations, and thus may help filmmakers to work creatively in a familiar setting. Furthermore, remote work is becoming increasingly common in the film industry and VR can be the ideal solution to foster collaboration within these settings. The advantage of using online multi-user VR is that it may enable a sense of being transported to real environments where co-workers can socially immerse with their colleagues. Remote collaborative previs may thus not only be an important tool for creativity and communication, but also for saving the time, expenses and environmental costs of travelling.The main research question addressed in this paper is: How useful and familiar can VR technologies be for remote collaboration in previsualization?To research this topic, we describe and evaluate a tool that provides an immersive and collaborative previs environment where filmmakers can create, discuss and validate different takes, shots and entire scenes. The tool features a VR interface where the intended users do not need advanced computer literacy to understand and master the functionalities. A within-subjects experiment was conducted, where the participants (professional filmmakers, primarily from the Stockholm Academy of Dramatic Arts) performed previs sessions in the proposed collaborative setup and in a control solo setup. In this paper, we analyze the results from our experiments to assess the impact of, and provide design insights on, remote collaborative VR technology that enables previsualization of film scenes.", "relwork": "In face-to-face interaction, communication can be achieved in a delay-free and multi-functional way, as is explained in [Clark and Brennan 2004], where Clark and Brennan claim that grounding, i.e. finding common ground, is important for how communication works effectively. Thus, the costs of constraints of mediums can affect formulation, production, and understanding. Collaboration calls for similar grounding as for general communication in order to work effectively. The importance of shared visual space, mental models, context, and speech is evident in earlier research [Carroll et al. 2006;Fussell et al. 2000;Gergle 2005;Gergle et al. 2005;Shakeri et al. 2017], where the context and understanding of how to collaborate and what to collaborate on can be made clear. In [Fussell et al. 2000], Fussel, et al., accounts for four types of visual information that is important for grounding in collaborative contexts: participants' heads and faces, participants' bodies and actions, shared task objects, and shared work context. Similarly, having a shared visual space in collaborative work environments makes participants' more likely to let their actions speak for themselves instead of having to explain verbally, simultaneously [Clark and Brennan 2004;Gergle et al. 2005].Technological solutions for remote collaboration have taken many different forms. From letters, telephone, video-conference systems to more advanced forms of technology that are now starting to show potential for creating efficient and natural forms of collaboration [Celata et al. 2018;Fussell et al. 2003;Johnson et al. 2015;Khan et al. 2016;Piumsomboon et al. 2016]. However, most of these authors also claim that the lack of developed technology can hinder good collaboration. Poor field-of-vision in interfaces and lack of asymmetry in controls are two examples of how the design of technical solutions could have been standing in the way of usable interfaces for interactive collaboration [Bateman et al. 2015]. This is something that VR technologies can be better suited for, as the immersive qualities of virtual headsets and its controls are extensive enough to facilitate grounding in collaborative environments. Advances in VR technologies has led to decreased prices and a more widespread usage of the technology [Feltham 2018;Ladwig and Geiger 2018;Perry 2015], which might represent clear opportunities for remote collaboration purposes. VR can be seen as a way of achieving near perfect co-presence with other people [Perry 2015]. Co-presence is an especially important aspect for attaining grounding in virtual 3D environments [Gergle et al. 2005;Ramsbottom 2015]. However, there are still problems with VR that prevents the medium of fully delivering on the promise of full copresence. In [Anthes et al. 2016], Anthes et al., raises a number of issues on the state of VR technologies. It is clear that representation of users is still a problem as a result of lack of information on facial expressions, eye-gazing, and body movement. Nevertheless, recent advancements are attempting to tackle these problems with the use of eye-tracking inside VR headsets, gloves that can represent fingers, etc. Thus, user representation is likely to improve in the near future [Ladwig and Geiger 2018;Pfeil et al. 2018]. As such, the affordances of elements in a VR environment needs to be carefully decided upon, when designing for VR. In [Ellis 1995], Ellis makes clear that affordances in VR should mimic the real world as much as possible, as the affordances of real artifacts are expected to have the same affordances in the virtual environment. Furthermore, in the virtual environment, extended affordances can be developed, as the virtual environment can in its nature facilitate this [Ellis 1995].In this paper, we focus on designing collaborative VR technologies that enable previsualization, a pre-production stage within filmmaking. Previsualization is a process where filmmakers create visual prototypes of scenes in a film that are otherwise difficult to visualize. Different use-cases are e.g. scenes with visual effects, expensive set constructions or digital set extensions. While hand-drawn floor plans and storyboards are still the dominating previsualization technique within creatives, high-budget productions commonly make 3D animated visualizations of the film, predominantly using 3D modelling and animation software such as Autodesk Maya 1 or Blender 2 . However, this process is not only time consuming and costly, it also delegates creative control over the previs to external animators. The possibilities to use real-time technologies such as game engines to provide more interactive and user friendly interfaces have been attempted before. The main promise of such technology is the ability for the creative team of a film-production to create the previs by themselves. In the work by [Nitsche 2009], a previs system was implemented using Unreal engine together with various custom built input devices for puppeteering characters and camera. The study argues that animation and camera control are the most important areas for previs. Several other studies [Ichikari et al. 2007;Mori et al. 2011;Spielmann et al. 2016;Tenmoku, R. and Ichikari, R. and Shibata, F. and Kimura, A. and Tamura 2006] have explored Mixed Reality (MR) interfaces for previs. These interfaces are however restricted to camera work (although [Spielmann et al. 2016] has basic key-framing functionality), and and thus does not exploit the possibilities to previsualize character movements in real time.In [Ramsbottom 2015], a VR previs system is presented where the user can create low resolution animations by adding and editing key frames on a timeline. The system used a rotation sensing headset (Oculus DK2) and a game pad for input, which limits the immersion and familiar interaction mechanisms included in later hardware. The system closest to ours is presented in [Muender 2018]. By evaluating their system on 6 expert users from film and theater (surprisingly none being a director or photographer), they found that their tool was practical for real-world applications. Like us, Muender et al., uses room scale VR with positional tracking, and a miniature approach where the users move scale models and cameras. Unlike us, they only create snapshot still-images of stiff characters, and they do not use a collaborative setting. We will contribute to the state-of-the-art in this field by performing a larger study with a greater number of film professionals, and by investigating the usefulness of distributed collaborative previs in VR.", "rq": "The main research question addressed in this paper is: How useful and familiar can VR technologies be for remote collaboration in previsualization?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/26.xml", "intro": "With remote work and video-mediated communication becoming increasingly common, we are seeing new challenges and opportunities emerge in how people conduct meetings. A popular meeting form is hybrid meetings [31,32], which involve physically co-located and remote participants. Such meetings sufer from reduced nonverbal communication [35] due to a lack of spatial consistency between the physical spaces from where meeting participants connect [5,29]. Unlike in face-to-face interaction, participants struggle to attract the attention of others through subtle glances and body gestures [14,15]. This may lead to primary room dominance [18] where remote participants feel excluded from the room of co-located participants [31,32]. To address these issues, Media Spaces (e.g., [17,26,29]) and Mixed Reality approaches (e.g., [5,9,28]) create a physically realistic frame of reference for a blended interpersonal space. This creates a sense of virtually \"being there\" together and enables deictic gestures with spatial referencing. However, these approaches dictate fxed spatial confgurations of users and require elaborate (and costly) physical hardware setups.We follow Hollan and Stornetta's suggestion to build telecommunication tools that go \"beyond being there\" [16] with the goal of developing new principles for supporting non-verbal communication and better inclusion of remote participants in hybrid meetings. Rather than establishing a physical frame of reference, we investigate the collaborative qualities of a malleable virtual frame of reference. Our work addresses the following research questions: RQ1 How do participants leverage a malleable blended space for going beyond being there? RQ2 How can participants use their camera feeds in a virtual frame of reference for deictic gestures? RQ3 How can we better support inclusion of remote participants in hybrid meetings? To this end, we built MirrorBlender -a prototype of a malleable video-conferencing system that enables remote and co-located participants to adjust the position, scale and translucency of video feeds in a shared 2D space. We refer to the video feeds as mirrors, which can either be mirrored camera feeds of participants or screen mirroring of shared content. MirrorBlender grants spatial consistency [29] by creating a shared 2D frame of reference in a What-You-See-Is-What-I-See (WYSIWIS) style [36] so that everyone can see the same layout of mirrors. To investigate our research questions, we deployed MirrorBlender in three hybrid meeting sessions each with three co-located and two remote participants.The paper thus contributes the following:\u2022 A novel approach to malleability in video communication through the manipulation of position, scale, and blending of camera and screen mirrors. \u2022 Identifcation of opportunities and challenges of enabling embodiment of camera mirrors for deictic gestures. \u2022 Insights into how hybrid meetings can be made more inclusive for remote participants.", "relwork": "We build upon recent work on hybrid meetings and motivate our study by reviewing research on media spaces and relevant commercial video-conferencing systems.", "rq": "In the following, we discuss the three research questions we have posed in the Introduction with the design and study of Mirror-Blender and put our results in the context of prior work. We further outline the implications of our work and how it points to future directions for supporting video-based collaboration."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/24.xml", "intro": "With the viewer placed among a circle of high school students, this scenario starts with a person introducing himself as the teacher of the class, welcoming everyone to the frst day of secondary school (Figure 1). The teacher shares his name and a hobby he enjoys. Each student then follows in turn, performing the same introduction. One of the students starts talking about something unrelated on his turn, leading the teacher to interrupt the student and cue the circle to continue. The last student before the viewer tells a joke about his skills in performing push-ups, which prompts all students in the circle to burst into laughter. As the collective laugh eases, the teacher asks the viewer to introduce him or herself. Twenty seconds of silence follow, with the students in diferent modes of attention and suggestive postures. The teacher proceeds by thanking the viewer and the round continues until everyone has introduced him or herself. The teacher concludes by thanking everyone for their participation.", "relwork": "This section introduces previous research on fear of public speaking, exposure therapy, and virtual reality.", "rq": "The frst author conducted initial coding of the data, using codes such as \"realism,\" \"task understanding,\" \"storytelling,\" and \"sharing an experience.\" Then, the codes were read through and organized into possible themes. Each theme was then mapped out in a conceptual model of the data corpus, with the author ensuring the extracted codes made sense within their themes. Lastly, the themes were refned to avoid overlap and refect the research question."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/252.xml", "intro": "Remote collaboration technology can enable a user at a local work place to quickly receive help from another expert user in a remote location. For example, a video call allows the remote guest user to quickly understand the situation of the local host user. In remote collaboration it is important to consider how the local host user can capture and broadcast a view of their surroundings. However, video calls have many limitations such as sharing a small field of view (FOV), limited resolution, or fixing the view of the remote guest user to that of the local host user. To overcome such limitations, it could be more efficient if the remote guest user could immerse themselves in a view of the local host user's environment. Virtual Reality (VR) technology enables this by having the user wear a Head Mounted Display (HMD) that provides an immersive viewing experience with a wider FOV compared to a standard phone or monitor.Using VR technology, 360 views of the surroundings can be shared from a panorama camera. Alternatively, other systems allow sharing a 3D reconstruction of a real world scene using a depth sensor and/or photogrammetry. Both of these techniques allow sharing the local host user's environment to a remote guest user, but each has some limitations. Sharing 360 panorama views can provide a high quality view without consuming a large amount of bandwidth but it is a 2D presentation that provides limited depth perception. In contrast, sharing a 3D reconstruction supports depth perception as well as the ability to navigate through the 3D model. However, the quality of 3D reconstruction and the amount of bandwidth required to transfer it are directly proportional to each other. So a high-quality 3D reconstruction of the user's environment would require a significant amount of bandwidth and is difficult to update in real time.In this paper, we present a novel Mixed Reality (MR) remote collaboration system (see Figure 1) that combines 360 and 3D reconstructions into one. This creates a system that aims to merge the advantages of the individual approaches while minimizing the limitations stated above.Compared to prior work, this paper makes a number of novel and significant contributions:(1) A novel MR Remote Collaboration technique that merges 360-views and 3D Reconstruction. (2) The first user study that compares 360 live panorama and 3D reconstruction based MR remote collaboration systems. (3) The first user study that explores the benefits and implications of combining 360-view and 3D reconstructed scene into a hybrid MR remote collaboration system.", "relwork": "Recently, researchers started to explore how 360 panorama camera can be used instead of a standard camera for remote collaboration. This allows the local host user to capture and broadcast the 360 surrounding view to the remote guest user who could turn their head while wearing the HMD. For example, JackIn Head [16] was a remote collaboration system using 6 cameras constantly capturing videos from different angles and processing them into a 360 high-quality spherical video image to live stream to another user wearing an HMD to view. The system is constructed as a headband that allows easy wearability on the head for the local host user. Tang et al. [37] created a 360 video chat system with a similar set up on the local host user side. In their system, they used a 360 camera on a monopod fixed to a user's backpack to broadcast the 360 surroundings to the viewer watching it on a tablet device. Most recently, the Shared Sphere system [21] used a 360 panorama camera attached to a Microsoft HoloLens [23] to capture and share the user's surroundings. With this system, both local host and remote guest users could look around independently while sharing visual communication cues through MR visualisation.These systems provide easy access to the 360 surroundings of the local host user by the remote guest user, who can look around independently. However, the viewing position of the remote guest user is strictly controlled by the local host user. So the remote guest user will not be able to look at a certain corner of a room or behind any occluded objects by walking closer, unless the local host user goes there.", "rq": "The aim of the first part of the study was to compare the two remote collaboration mediums: 360 live panorama video and 3D reconstructed scene. The main research questions in this part were:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/180.xml", "intro": "The growing distribution of affordable head-mounted displays (HMDs) introduces the technology of virtual reality (VR) to everyday environments, e.g. living rooms or shops 1 . In contrast to lab environments, in which the rooms are specially designed for VR experiences, these everyday environments are prone to interference or interruptions. These interruptions, to name but a few, can be caused by physical objects in the walking area of the VR user, noise originating from the real world or other persons being around. In this work, we focus on interruptions caused by real world bystanders, trying to collaborate with the HMD user.We call the situation of people being physically in the same place but visually in different worlds co-located mixed presence. The HMD user becomes part of the social community when wearing the HMD in an everyday environment. Therefore, many situations occur in which mixed-presence collaboration is needed, as for example in the following scenario:Scenario -A mother takes part in a business VR tele-presence conference from home. Her five year old son wants to show her a picture of his favorite dog. The kid will not accept a denial of attention, but the mother has professional obligations towards her colleagues to attend the meeting. Similar to our scenario, McGill et al. [7] could show that there is a need for HMD users to communicate with the world surrounding them. In most cases, this communication fails due to the visual barrier. As a result, the user has to take off the HMD, which is not only perceived as very annoying but also hinders her from carrying out the VR task [7]. Methods to overcome this problem are augmenting the real world into the virtual [7], sharing the physical space for playful asymmetric collaboration [3] or providing insight into the HMD user's environment on a head mounted screen that simultaneously serves as an input and output device for the real world user [4]. In contrast to previous work, we suggest to avoid mixing the two presence states and rather keep them separated. From research on presence in VR, we derive that VR users will benefit from this, as they are not reminded about the real world, keeping the focus on the virtual stimuli.We propose to use a shared surface as a mediation between the virtual and the real world (see figure 1). For the real world user, the shared surface can be any digital device providing a screen and being able to transmit or receive pictures, e.g. a tablet, a micro projector 2 or a tabletop display. The shared surface is rendered in VR as a \"digital twin\", located at the same physical position as in the real world. The concept of a shared surface is similar to the idea of using tabletop displays for tele-presence tasks [11]. However, in our co-located scenario, the real world user can see the HMD user and they are able to touch each other and shared objects. This might affect the collaboration leading to design opportunities different from tabletops.In this work, we address the following research questions, focusing on short-term collaboration in co-located mixedpresence scenarios:1. What is the effect of having a shared surface on user behavior, task performance and user experience?2. Collaboration in this scenario is asymmetric, as the HMD user cannot see the real world user. Is an additional augmentation of the bystander needed, as it is known from the work on tabletop displays for tele-presence?To answer our research questions, we conducted a betweensubject user study (N = 40). We compared the following conditions: (a) Real_World: both collaborators in the real world (baseline), (b) No_Avatar: shared surface without rendering the real world user in VR and (c) Avatar: shared surface with a point cloud representation of the real world bystander in the VR (see figure 1).Altogether our contributions are:\u2022 Insights on the usage of a shared surface in co-located mixed-presence scenarios\u2022 Design implications for co-located mixed-presence collaboration using a shared surface", "relwork": "Shared surfaces are well known to support collaborative tasks in real world [2,13] as well as in tele-presence scenarios [8].Both of these situations are called \"symmetric\", as the visual information and the possibility to interact with the virtual content are the same for both collaborators. In our work, we deal with an asymmetric situation, in which the real world collaborator can only see the HMD user's body, but not their eyes. The HMD user, on the other hand, is mentally in a remote situation and cannot see the real world user. To maintain awareness for each other, Tang et al. introduced shadow techniques to present the abstract arm position of the collaborators on the remote screen [11]. In a follow-up study, they used a video overlay of a real arm [12]. They found that the more realistic the virtual representation, the better the possibility to perform and understand directed gestures.The work on co-located mixed-presence collaboration is highly diverse. Gugenheimer et al. [3] present a system that augments the collaboration in both directions in a gaming context. The real world user gets an indication about the HMD user's VR experience by a projection on the floor. In contrast, we propose to create a shared frame of reference for both users at the same physical position in the real and the virtual world. Furthermore, we want to analyze short-term collaboration in a different context and therefore use a standardized task and quantitative as well as qualitative measures.Visual integration of the real world user in VR is often achieved by using green screen technology [7,15]. The mixture of both worlds generates a more symmetric interaction, in which the real world and the VR user have the same information about each other. In general, this additional information proofed beneficial for the collaboration. However, green screen technology is not part of everyday environments and probably not wanted in living rooms or shops. More sophisticated tracking technologies, e.g., Kinect, are neither well-suited for ad hoc collaboration in these contexts. There are not only technical challenges when rendering real world objects in VR, but also some drawbacks for the HMD user. Users might feel frightened when an avatar suddenly appears, it can lead to illogical situations and have a negative impact on the feeling of being present in VR [7]. In this work, we want to find out whether a shared surface, that can more easily be rendered in VR, can bridge the gap between both worlds by itself or if a representation of the real world user in VR is still needed.", "rq": "To answer our research questions, we conducted a betweensubject user study (N = 40). We compared the following conditions: (a) Real_World: both collaborators in the real world (baseline), (b) No_Avatar: shared surface without rendering the real world user in VR and (c) Avatar: shared surface with a point cloud representation of the real world bystander in the VR (see figure )."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/42.xml", "intro": "Virtual Reality (VR) devices, such as the HTC Vive 1 , have become quite popular during the last year. These technological developments have led not only to a tremendous amount of design and research opportunities-such as offering great potential to incorporate the human body as an interaction design resource-but also to design-related risks [14]. Although there are some exceptions 2 , the majority of VR game titles offer a single-player game experience. In public contexts, such as game festivals, demo sessions at conferences, and art exhibitions, typically VR players are being watched by spectators that can see the current game status via a monitor or a projection screen. However, VR technology cannot only be used to grant a unique and engaging experience for one or more VR players, but can also can be harnessed to establish a gateway between virtual and real space (VR players and spectators). One interesting field of research in this context concerns Augmented Virtuality (AV) in co-located settings for multiple participants. AV settings have the potential to create a connection between the VR player and the spectators as they merge real world objects into virtual worlds [9].Various questions arise, such as how to reduce the gap between the real and the virtual world, or how VR players in such heterogeneous scenarios should come into contact and interact with the spectators [8]. Research efforts within the field are still at an early stage, and fairly little is known about which factors contribute to an engaging co-experience among VR players and spectators. Thus, this submission proposes an approach via an AV installation that does not just take one VR player into account, but also the spectators of an installation (see Figure 1). Furthermore, it shall be investigated which factors influence the social aspect (co-presence) in an AV context: in detail we are interested in determining whether the virtual representation of the VR player (in our case: hands) and the verbal communication (speech) influence the relationship between him/her and the spectators.To find answers to our research questions, we created a playful experimental setting called \"Invisible Walls\". The installation offers different interaction techniques (physical movement and hand gestures) for VR player and the spectator. On the one hand, the VR player is able to come into contact with the spectators by using his/her hands, while, on the other hand, the spectators assume a more passive role by moving through the real/virtual environment (a more detailed description can be found in the section \"The installation: Invisible Walls\").Due to the co-located setting, we focused on one specific aspect in a multiplayer setting: co-presence. This phenomenon forms one of the sub-dimensions of social presence, in which players automatically detect and classify a form as another (i.e., possessing agency) [2]. The level of co-presence is influenced by the degree to which players appear to share an environment together, i.e. the degree of mutual salience of the player and the perceived other.Our submission should shed some light regarding co-presence and its contributing factors in an AV installation, whereas the scope can be framed by the following questions: Does the player's representation in VR (here: hands) foster the mutual salience and sensomotoric accessibility (i.e. co-presence)and if so, in which way does it affect the experience of copresence, and how do other channels moderate the effect of copresence in that regard (in our case: verbal communication)?Generally, we argue that by providing means for nonverbal communication (i.e., representation of the VR player's hands) and verbal communication during co-located gameplay in AV settings, a new interaction layer for the VR players and the spectators can emerge, leading to a smaller gap between them. Our findings should inform game designers and researchers if the inclusion of a virtual representation in a co-located AV setting changes the perceived co-presence, and how it potentially changes co-located gameplay.", "relwork": "For the past few decades, multiplayer VR and AV installations were basically confined to research labs or festival contexts, representing experiments in display technologies, computer graphics and network performance. Although graphics-driven multiplayer online games such as first-person shooters have definitely provided concepts for a shared awareness and an interaction between players, the actual physical, real-world position of players has not been utilized. However, co-location has been both a key feature and shortcoming of VR and AV installations since the initial CAVE systems [6]. Whereas only one VR user can interact in the CAVE, other users are simply spectators and are provided with the same perspective rendered for the guide. Even if two or more CAVE systems are connected, the co-located users are only offered a passive experience. A quite elaborate approach for a co-located playful experience, however, is proposed in the artistic CAVE installation World Skin 3 . In this interactive artwork, up to three spectators could interact with real photo cameras in a co-located virtual environment (VE). Since real world objects are merged into the VE, this artwork can be classified as one of the first co-located AV installations. Current state-of-the-art approaches, such as proposed by Chague et al. [5] are capable of providing a multi-user AV experience including the sense of touch by connecting the real world with the virtual world. Zhu et al. [17] propose a Shared Augmented Virtual Environment (SAVE), a mixed reality (MR) system that overlays the virtual world with real world objects captured by a Kinect depth camera. In their approach, the synthetic MR world contains both real and virtual objects that are rendered in real time with a correct representation of depth from both worlds. Nevertheless, although advances are being made to create a shared AV experience, many questions remain regarding the inclusion of spectators in such multi-user installations.One relevant aspect of co-location in VR and AV settings is the phenomenon called \"presence\", which has received an increased interest by the art and scientific community. Several articles 4 stress the significance and the novelty aspect of the term presence. However, the concept is not new: it is derived from \"tele-presence\" introduced by Marvin Minsky [10] in the 1980s and has undergone several developmental processes with a variety of definition attempts [4]. Lee [7], for instance, stated that, in general, presence can be understood as a psychological state in which the artificiality of an experience is not noticed by the subjects. In computermediated environments, several authors claim that presence refers to the sense of existing or of \"being there\" in a virtual space (e.g. [1,13]). Numerous authors support the idea that it is composed of three dimensions: spatial presence, self presence, and social presence [15].The last sub-dimension and most relevant for our submission, social presence, is experienced when subjects \"successfully simulate other humans or non-human intelligences\" [7]. In such a scenario, users do not notice the artificiality of the experienced social actors. Participants within a technologically mediated environment communicate as in a real human-tohuman communication [1]. This can be observed as users frequently interact in artificial spaces by applying social rules that they also utilize in the real world.According to Biocca and Harms [3], social presence can be further sub-divided into the following dimensions: co-presence, attentional allocation, perceived message understanding, perceived affective understanding, perceived affective interdependence, and perceived behavioral interdependence. Our project specifically deals with co-presence, which is the degree to which the recipients believe that they are not alone, their level of awareness of the other, and their sense of the degree to which the other is aware of them. Frank Biocca and Chad Harms [2] give the following definition for co-presence: \"Users experience the phenomenal state of co-presence when they automatically detect and classify a form as another (i.e., possessing agency). The level of co-presence is influenced by the degree to which the user and the agent appear to share an environment together, that is the degree of mutual salience and sensorimotor accessibility of the user and perceived other.\"Research dealing with co-presence in the context of AV and VR offers diverse opportunities for investigation. Roth et al. [11], for instance, propose an experimental method to investigate the effects of reduced social information and behavioral channels with full-body avatar embodiment on presence. The researchers compared physical-based and verbal-based social interactions in the real world and in VR. Study participants were represented by abstract avatars with a limited set of social cues. The experiment showed that subjects are capable of compensating for missing social cues by shifting to other communication channels.Many research projects deal with aspects of cooperation. Salimian et al. [12] describe a mixed reality environment in which they employ virtual arm embodiments to visualize the remote collaborator's arms on the physical tabletop. Tang et al. [16] examine the role of the user's body in a collaborative work setting and how it affects presence disparity. They articulate four design implications to mitigate the effects of presence disparity: embodiments should provide local feedback; they should visually portray people's interaction; they should display fine-grain movement and postures of hand gestures; and, last but not least, they should be positioned within the interaction space.As discussed in this section, the VR-spectator relationship and co-presence are particularly relevant issues in AV and VR approaches. However, research efforts within the field are still at an early stage, and fairly little is known about which factors contribute to co-presence among VR players and spectators.", "rq": "Research question: Will VR players experience a higher degree of co-presence if their body (in our case: hands) is visually represented in the virtual environment? Does the inclusion/exclusion of exterior audio sources (verbal communication) influence the perceived co-presence? Generally, we aim to evaluate nonverbal and verbal communication within a co-located and playful AV installation through a controlled experimental setting. The results of our study should contribute to reducing the gap between VR players and spectators. The following sections provide some information on the installation Invisible Walls, a description of the four conditions, the hypotheses, the participants, the procedure, and the results."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/4.xml", "intro": "Teams commonly collaborate around a shared visual document (e.g., user interface (UI) design or presentation slides). This is an essential process in both the workplace and academic settings. For productive and successful collaboration, frequent communication is necessary to develop a shared understanding between team members [25]. However, for teams of students, communicating efectively and efciently can be challenging. Students are generally novices who lack the design knowledge necessary to express themselves efectively when discussing their design. Additionally, diferences in course schedules and the lack of shared ofce space frequently restrict them to collaborate remotely and asynchronously [42].Communication-related burdens introduced by an asynchronous setting can discourage students from discussing with their teams. Most asynchronous communication channels rely on text (e.g., text messaging and Google Docs' comments [19]), but typing involves high physical and cognitive demand [28]. Also, although efective referencing is essential in developing a shared understanding [22], text makes referring to visual objects in the document challengingas pointing while typing is impossible. Unfortunately, students may lack the knowledge needed to overcome the restrictions of text [51].Furthermore, regular scheduling issues between students [29] may prevent them from frequently checking on their team's messages and documents. This introduces a signifcant delay in communication and team members' communicative needs may not be satisfed in a timely manner. In addition, when students do check on messages, they may have to look through a large volume of messages [56], and references might not be adequate as the objects referenced may have changed after the messages were sent [52].As shown by our formative study, feeling uncertain about receiving a response on time, and the aforementioned burdens when producing and consuming messages may lead to communication breakdowns in student teams [34]. Although professional teams have organizational support to handle these breakdowns [25], instructors may be ill-prepared or time-constrained to adequately provide similar support to students [41].In this paper, we propose a novel way of communication with multimodal messages of voice and clicks-referred to as linked tapes-as an alternative form of asynchronous communication. Linked tapes are multimodal messages created by simply speaking while pointing at relevant visual objects through clicks, interactions which could require less efort when compared to typing text messages. When a tape is produced, the current version of the document is stored to preserve the temporal context of the tape to enable change awareness [44]. Additionally, bidirectional links between objects and voice snippets are automatically generated by temporally mapping the modalities onto each other (Fig. 1). With the voice-to-object link, playing back a voice message can display relevant objects to allow the receiver to efortlessly understand the references in the message. With the object-to-voice link, selecting an object of interest can flter through numerous voice messages to retrieve only those relevant to the object and facilitate the receiver's navigation. The ease of producing linked tapes with the multimodal input and the support provided by the bidirectional links can vitalize communication in asynchronous teams and improve a shared understanding.We actualized communication based on linked tapes in Winder, a plugin for the collaborative UI design tool Figma. The plugin leverages the linked tapes' bidirectional links to implement three main features: (1) highlighting on playback (Fig. 1a), (2) inline thumbnails on transcripts (Fig. 1b), and (3) object-based search (Fig. 1c). In addition, Winder also aims to tackle the problem of communication delays. The plugin periodically prompts the user to produce linked tapes with the goal of preemptively obtaining information which may be needed by team members in the future. Tape-based communication could lessen the burden imposed by this approach-the efort of producing and consuming many messages. Thus, the drawbacks can be outweighed by the potential gains in shared understanding.To investigate the efect of Winder on the collaboration process of asynchronous teams, we conducted a fve-day study with eight teams of three students (N=24). Teams were tasked with designing the UI of a mobile application, which helps friends decide on what and where to eat. On the frst day, they discussed ideas as teams and, on subsequent days, each team member worked on the design on diferent time slots. Our fndings showed that the participant teams produced an average of 13.13 tapes and that the average tape length was 53.27 seconds. Analysis of survey responses revealed that, when compared to text messages, participants felt less burdened producing linked tapes due to the ease of speaking and clicking, and felt more confdent that their messages would not be misunderstood. Participants also expressed that bidirectional links facilitated navigation through and within tapes, as well as their understanding of these tapes. Furthermore, the study results suggest that tapes recorded preemptively could allow for communication at hand without having team members at hand.Our work contributes a novel multimodal asynchronous communication tool, Winder. Through lightweight interactions in production (i.e., click and voice) and bidirectional links for consumption, the system advances work in asynchronous communication by simultaneously decreasing burden for both senders and receiversprevious work facilitated either but not both. Furthermore, reducing at-the-moment burdens allows for an approach to tackle communication delays that would previously be overly burdensome: prompting users for preemptive recordings to satisfy future communication needs. As a secondary contribution, we present empirical fndings that demonstrate the potential of Winder to reduce bilateral communication burden and overcome the detriment of delays in student teams.", "relwork": "We frst review work on incorporating multiple modalities into asynchronous communication, then on communication anchored on documents, and fnally on handling communication delays.", "rq": "Due to difculties in objectively measuring burden or success in collaboration, our fndings instead focus on the experiences and perspectives of our participants. Findings from our study revealed how voice and clicks, and the bidirectional links in linked tapes could facilitate asynchronous communication. Additionally, prompting the user to record tapes was found to potentially satisfy some future communication needs, clear misunderstandings, and even allow team member to better coordinate their design work. We provide a summary of our fndings with mappings to research questions and system features at the end of the section in Table ."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/234.xml", "intro": "Augmented Reality (AR) combines real and virtual aspects by augmenting the real world with virtual objects [4]. It allows users wearing head-mounted displays (HMD) to have additional information within their view. Typical areas include education [5,8], industry [18,36,60], remote support [20,24] healthcare [49,58], emergency response [2] and many others. One of its main potentials is to provide guidance, orientation and awareness in different situations. These include remote settings such as supporting medical students in training [17] or technicians in maintaining machines [24], and co-located or in personal settings such as helping visually impaired people finding their way in an indoor area [53]. Such support can also be used to help people when objects (e.g., tools) or potential cooperators are occluded by walls, machinery or other objects providing a visual boundary, and need to be found. In these cases, AR may provide awareness and orientation by showing where the search target is. This was found to be helpful for service technicians in planes or buildings looking for cables or devices [6], for the exploration of structures inside buildings [42], for the coordination of emergency response units [26,62], when firefighters search for objects or people [50], for the maintenance of machines [24] and medical applications [61], to see where others are behind walls and what they are doing [7], to locate virtual objects located in other rooms [16], and many other application areas. Hence, providing information to help find objects or people out of sight is an important area of support. Various ways for providing this support in AR have been proposed, including (among others) shared maps [31,42,45], compasses for navigation [25,59] and x-ray vision to \"see through\" walls [6,61]. Despite existing work on comparing these mechanisms to traditional orientation support such as arrows (e.g., [54]), little is known on how the different mechanisms compare (e.g., [30,61]). We aim to shed light on this by comparing a map, an x-ray vision and a compass for individual and cooperative tasks of finding objects and people. Our results provide insights into the applicability of different mechanisms and their combinations, and also directions for designing these mechanisms.", "relwork": "Orientation (including navigation) is a typical area for AR. Bhorkar [11], de Belen et al. [3] and Schmalstieg and H\u00f6llerer [55] provide overviews over the current research of AR usage for orientation support. Among orientation aids for HMDs, the attention funnel [15] describes a prominent method to guide users while staying in the center of their field of view (FOV) [56]. Other popular mechanisms are maps to enable coordination [31] and topdown exocentric views [10,43]. In addition, similar to a compass Feiner et al. [25] implemented a tool pointing towards objects, which was found to be beneficial for initial orientation [23]. Suomela and Lehikoinen [59] advanced this idea to a cylindrical projection map of a target. Similarly, so called halos indicate the direction of off-screen objects at the edges of screens [9,[32][33][34]. Li et al. [41] found that maps are superior to arrows because they provide contextual information to users. In contrast to that, Schinke et al. [54] found that 3D arrows outperformed a map when searching for points of interest, because arrows do not afford users to match different reference systems. Chittaro and Venkataraman [19] compared 2D and 3D maps. They found significant faster completion times with 2D maps, which they explain with the familiarity of 2D maps. Morrisson et al. [45] point out that an AR based map can enhance collaboration among people sharing the map. D\u00fcnser et al. [22] showed that maps can support AR based navigation. In the context of search tasks, Funk et al. [30] found that users favor last-seen images over a map, most likely because of the familiarity they provide. Mulloni et al. [46] found that a compass outperformed a zooming interface of an exocentric view and a map for simple search tasks. They also found that the performance of one mechanism could be increased by combining it with another (e.g., compass was outperformed by compass and exocentric view). In an early study, Baudisch and Rosenholtz [9] found that Halos worked much better than pointing arrows for finding objects. This finding is supported by recent studies of Gruenefeld et al., who also found halos to be superior to arrows [33,34]. Comparing a 2D map with radar (compass) view, Eseng\u00fcn and Ince [23] found the 2D map to lead to quicker search success and to be less demanding for users. Renner and Pfeiffer [52] enhanced the concept to a sphere with waves showing the location of objects, which they found to be helpful but difficult to learn by users.", "rq": "As discussed above, there is no comparison of mechanisms available for AR support of orientation in occluded multiroom settings. This makes design decisions difficult. Our research aims to generate insights on this. It is driven by the following research question:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/94.xml", "intro": "The ability to remotely guide and instruct users in physical tasks has great value due to its ability to connect novices with experts and improve the way people learn new skills and trades [12,14]. The ability to re-skill and develop workers effectively is especially important as the nature of work changes and workforces become more dynamic [36]. Within the HCI community, researchers have leveraged novel interfaces such as AR, VR and other modalities to guide or teach physical skills and activities [1,13,43,48,51]. This research has driven commercial offerings which aim to direct users and provide guidance on job sites and during maintenance tasks [54, 55,56]. However, these approaches have typically relied on asynchronous learning, tutorial generation, or presenting contextually relevant information such as guidance cues. Additionally, these approaches often rely on a single modality of capture and presentation data streams (e.g., 2D video, AR), in order to teach or guide the remote participant. While these methods can be effective, the spatial nature of physical tasks is often lost or reduced, as is the ability to interact with an instructor in a bi-directional manner. A rich interaction with an instructor can result in tailored guidance and can close the loop by supporting demonstrations by the instructor [7,26,41,53].In recent years, telepresence technology has advanced rapidly with commoditization of real-time spatial capture devices [57], more prevalent availability of VR and AR interfaces, and novel interactions for mixed-reality (MR) interfaces [27,32]. These technologies have the potential to augment current training techniques and bridge the gap between instructor and learner by leveraging contextual cues, spatial information, allowing recording and playback of scenes, and enabling spatial annotations. However, it is not evident how to leverage these novel technologies in combination to exploit their unique value.While prior work has introduced specific configurations of MR-based instruction, we present a broader design space exploring this domain and highlight the importance and utility of moving between the different configurations, based on the learning sciences literature. From this exploration, we develop and introduce Loki (Figure 1), a system for physical task training that supports operations across the different dimensions of the proposed design space. In this work, we will refer to them as 'transitions'. Loki supports these transitions between the various modalities and data enabled by mixed-reality. These transitions are important to facilitate learning throughout the skill acquisition process, since the learner needs can change even within the course of a single session of learning a physical skill.Loki is comprised of two symmetric spaces. Each space supports a single user and contains an immersive mixedreality display utilizing pass-through AR to enable transitions between virtual and augmented reality. The physical environment of each user is spatially captured and streamed in real-time to the remote user alongside video, audio and annotation data. Both users are able to navigate between their local and remote environments in real-time, and also interact synchronously with live as well as recorded data. This flexibility allows for novel workflows that bring the instructor and learner closer together, which in turn allows for richer collaboration and improved training opportunities. We discuss and illustrate the value of Loki's mode transitions and corresponding system features through scenarios performed using the working Loki system. The scenarios illustrate that these additional affordances can allow users to learn a variety of physical tasks in a flexible manner. We also then discuss a qualitative user evaluation of Loki in which users learnt a 3D foam carving task remotely.The primary contributions of this paper are:\u2022 A design space that explores real-time bi-directional mixed-reality based remote training of physical tasks. \u2022 A set of interaction techniques that allow users to navigate and utilize the breadth of information and presentation modalities within this space as well as enable effective learning workflows. \u2022 The development of a real-time bi-directional depthcapture based mixed-reality telepresence system. \u2022 An initial qualitative evaluation of the utility of mode transitions and the Loki system itself.", "relwork": "Loki is grounded on past models of skill acquisition, and primarily builds on prior works in the areas of teaching and learning physical tasks, remote collaboration and immersive physical guidance systems.", "rq": "In this work, we have introduced a broader design space for exploring the domain of MR-based live instruction. We then presented Loki, a system that supports this flexible exploration for remote teaching of physical skills. By supporting a range of modalities and various mechanisms for data capture and rendering, Loki provides a rich communication medium that leverages spatial data, video, annotations and playback that helps connect people as they teach and learn real-world tasks. We showed the value of these different features by describing a variety of scenarios we carried out, from teaching guitar to aiding in sculpting and peer learning. We then described a qualitative user evaluation which showed that users were able to use Loki and found the different features and modes of Loki valuable. While some limitations to this technology exist, there is a range of interesting research questions that have emerged from this exploration."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/80.xml", "intro": "A wide variety of communication cues are used in face-to-face collaboration, such as audio (e.g., speech, paralinguistic, intonation), visual (e.g., gaze, gesture, facial expression, body posture), and environmental information (e.g., object manipulation, writing, drawing, spatial layout). These different cues are combined to create more efficient communication and better mutual understanding between remote collaborators. Advances in telecommunication technology have enabled rapid developments in methods for remote collaboration, such as real-time audio and video streaming, or mobile conferencing. However, most of these technologies cannot convey all of the same communication cues as those which are presented in face-to-face collaboration. For example, on a video conferencing link, some subtleties of the gaze or gestures might be lost. It could also be tricky to share the same spatial cues present in the local conversation or to share environmental information around.The use of Head-Mounted Displays (HMDs) with Mixed Reality (MR) technology creates the possibility for a more intuitive and immersive collaborative experience than with conventional 2D video-based systems. For example, by capturing and streaming 360 \u2022 video of the local worker's view into a Virtual Reality (VR) scene viewed by a remote expert, the remote expert can feel like he/she is sharing the local user's workspace, being able to inspect the local environment in a 360 view [29]. Similarly, Augmented Reality (AR) technology enables the remote expert to overlay virtual content onto the local worker's view, such as showing 3D virtual annotations on top of real objects to demonstrate how to manipulate them [22].We would like to study remote collaboration with natural communication cues in dynamically changing room-scale environments, such as remote maintenance of large machines or control rooms, crime scene forensics, emergency response, and remote teaching of dance or acting performance, etc. There are many diverse application areas where this could be valuable. MR remote collaboration has often been studied before from two perspectives: 1) Capturing more dimensional information about the local scene and building an unconstrained viewpoint for the remote expert [27]; 2) Adding and improving communication cues exchanged between local and remote users for more efficient and easier collaboration [32].In this paper, we present a novel MR remote collaboration system that supports live streaming of an immersive 3D view of the local worker's environment at room-scale. It also supports live sharing of the remote user's eye gaze and hand gestures back to the local user to convey spatial task instructions. This system combines the advantages of capture technologies and communication interfaces mentioned above while minimizing their limitations. Compared to prior work, the primary novel contributions of this paper include: 1) A MR remote collaboration system that enables sharing of hand gesture and eye gaze communication cues within a live 3D panorama; 2) A formal user study that compares hand gestures and eye gaze as visual cues (standalone and combined) with the conven-tional verbal communication in a live 3D panorama-based MR remote collaboration task.In the rest of the paper, we review related work and compare our approach to this prior work, and then describe the design and implementation of our prototype system. We report on a full user study with the platform focusing on the usability of proposed MR collaboration cues. We finally discuss the results we have found, conclusions, and directions for future work.", "relwork": "In this section, we discuss two perspectives of related work in MR remote collaboration: 1) Local scene capturing and sharing for the remote expert; 2) Remote communication cues shared between the local worker and the remote expert. We study these two aspects with our novel system and experiment.", "rq": "In the user study, we mainly investigated the following two research questions: 1) How does the sharing of eye gaze or hand gestures from the remote user affect collaboration in a MR remote collaboration interface? 2) What are the benefits of mixing both gaze and gesture cues for MR remote collaboration compared with using each cue alone?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/140.xml", "intro": "Visually impaired (VI) and blind children are increasingly educated in mainstream rather than special schools [42]. However, despite being included with their sighted peers, recent research identified persistent issues with participation [51,56], reduced opportunities for collaborative learning and social engagement [5,19] and potential for isolation [34]. These challenges are in part attributed to the structural and technical support that VI children receive in mainstream schools [34]. In particular, assistive learning technologies are often designed to be used by VI pupils alone and not by sighted peers, and can therefore reduce opportunities for inclusive learning experiences. Educational games could reduce barriers to inclusion. Games are effective means for learning [12] and, if designed with inclusion in mind, they can promote interactions that are equally engaging to both sighted and VI children [35]. However, accessible technologies, including accessible games, tend to focus on the needs of a specific target user group: for example those in need of accessibility support. This can work against objectives of inclusion since games that are designed to be accessible for VI players tend to be simplified to a point that makes them non-engaging to sighted players [48]. Meanwhile, games for sighted players are heavily visual, and therefore difficult to access for visually impaired players. In practice, this means games that can engage both visually impaired and sighted children are scarce. This raises a need to explore the design of technologies that promote inclusive play between disabled and non-disabled players in general, and poses a challenge for the design of inclusive educational games for disabled and non-disabled children in mainstream schools.In this work, we explored how to support inclusive play experiences between children with and without visual impairments with the aim of promoting inclusive learning experiences in mainstream schools. In particular, we explored the extent to which off-the-shelf robotic devices, which are not designed with accessibility in mind, could be used to design an inclusive educational game. Additionally, we analysed the forms of inclusive play experiences that this may engender. We thus extend current work in this area by focusing on co-designing an inclusive educational game in a new context of interaction (mainstream schools), and with a mix of stakeholders (children with and without VIs and their educators). This characterises our approach as one that does not use the needs of a particular group as a starting point. We do this by addressing three key research questions: 1) What challenges and barriers are CHI 2020 Paper CHI 2020, April 25-30, 2020, Honolulu, HI, USA there to inclusive play experiences for visually impaired and sighted children in mainstream schools? 2) How can we codesign inclusive play technologies with participants who have mixed visual abilities? and 3) In what way does the resulting technology support inclusive play experiences?To answer these questions, we conducted focus group discussions with experts in the education of visually impaired children to learn about challenges and barriers to inclusive play experiences. We then ran co-design workshops with visually impaired and sighted children and their educators to learn about and critique a commodity robot technology and design an inclusive educational game. From these activities we derived a set of guidelines for the design of inclusive play experiences and recruited a group of game designers to develop a game concept using these guidelines, which we then developed into a prototype and evaluated in an inclusive school. We thus make the following contributions: 1) a characterisation of barriers to inclusive play in mainstream schools; 2) a demonstration of how engaging children and educators with mixed visual abilities through inclusive co-design activities enable joint production of radically new conceptions of technologies for inclusive play experiences; 3) an outline and discussion of a set of broad guidelines for inclusive play for disabled and non-disabled children.", "relwork": "Inclusive education grew out of a wave of school reforms to address the structural causes of inequalities between students needs, especially students with disabilities [53]. Inclusive education emphasises practices that allow pupils to experience and embrace diversity, including teaching approaches that enable learners to participate fully in a mainstream setting regardless of their needs [47]. In the UK, the move toward inclusive education was accompanied by an increase in the number of teaching assistants (TAs) working in mainstream schools [17,44]. However, evidence from recent reviews suggests that TAs rarely receive the necessary support and training they need, and that this contributes to increasing provision and experiential challenges for pupils with additional support needs [17,20]. Children with VIs have complex needs that require appropriate provisions [3]. They have limited access to the curriculum via the visual medium, and accessing information via alternative mediums such as Braille, is often time-consuming or not possible [22]. A child with a severe VI is also likely to require additional support in developing social skills [43]. A number of researchers have developed novel accessibility and assistive technologies (ATs) that address such issues. Examples include, a portable note-taker to provide blind students with better access to classroom presentations [24], methods for auditory access to mathematical formulae [40], and rapid prototyping of learning materials using 3D printing [32].Research on mixed-ability classrooms also shows increasing evidence that inclusive education technologies designed for both visually impaired and sighted children can alleviate some of the challenges associated with inclusive provisions. For instance, Thieme et al. developed an inclusive tool for collaborative coding [49], Freeman et al. explored the introduction of auditory beacons to support mobility and social engagement for visually impaired and sighted children [19], and Metatla et al. demonstrated support for engagement between children with mixed visual abilities through inclusive co-design techniques [34,36] and voice-user interfaces [35]. Nonetheless, uptake of novel ATs in educational settings continues to be limited [7] due to a number of issues, including stigmatisation [46] and perceived usefulness [41]. Screen-reader and screen enlargement software and hardware continue to be the dominant ATs used by VI children in mainstream schools [32,34]. This suggests that involving children with and without visual impairments as well as their educators in the design of inclusive educational technologies can improve uptake by ensuring designs are informed by and adequately embedded within ongoing practices and provisions.", "rq": "We ran an evaluation workshop to examine the third research question: In what way does the developed prototype support inclusive play experiences? Three TAs and seven children who also took part in previous workshops participated in the Paper 143 evaluation (Romeo could not attend). They were split into two groups (four/three children and two/one TAs in each group) and played the game from start to finish, lasting between 30 and 40 minutes. One TA played the role of a planet kidnapper, instructed on where to go to hide kidnapped planets. One researcher played the role of the game narrator, guiding the children through play. Another researcher observed play, took notes and kept time."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/82.xml", "intro": "Collaborative virtual environments (CVEs) are emerging as a promising CSCW platform, through which distributed stakeholders can create, discuss, and review spatial content (e.g., 3D models, room layouts, animations, as in [7,28,71,79]). Such content is becoming increasingly important in creative industries such as architecture firms, video game companies, and design agencies. Simultaneously, in these domains, the modern workforce is also becoming more globalized and time-distributed, increasing the need for supporting asynchronous collaboration between teams working across different time-zones or working hours. In these settings, successful collaboration depends on support for asynchrony, which has several unique advantages over synchronous communication, such as: work parallelism, flexible time-coordination, reviewability, and reflection [40,58]. Several contexts where asynchronous collaboration becomes crucial in the evolution of a shared artifact were discussed in-depth by Tam and Greenberg [76].With the advancement of virtual reality technology, asynchronous collaboration in immersive 3D environments has become an active research area [14,32,44,50,60,67,80,85]. Multimodal recording-the capture and later playback of multiple, dynamic interaction modes, such as speech, locomotion, body language, and object manipulations-is at the foundation of asynchronous collaboration in these VR systems. In these studies, however, multimodal recording was regarded only as one of many features of a system, designed for a specific purpose such as training [85] or 3D design [80]. The state of the literature leaves room for more empirical research [84] on conceptualizing multimodal recording in VR and producing a generalizable understanding of it.We aim to address the knowledge gap in conceptualizing and understanding this multimodal asynchronous VR collaboration (MAVRC) as a type of CSCW, backed by an empirical understanding of its challenges and design considerations. VR, CSCW, and HCI literature reveals three aspects of MAVRC: social behaviors in mediated-communication, awareness in cooperative work, and authoring and consuming multimedia. Using these aspects as an intellectual lens, we begin to identify and tackle these knowledge gaps by formulating the following research questions:\u2022 How are social behaviors transferred from face-to-face to MAVRC? In VR, where collaborators can leverage embodied avatars with multimodal communication capacities, different aspects of social norms (e.g., proxemics, bias, anxiety, etc.) can be transferred to, or even amplified or diminished in CVEs [4,26,73]. However, MAVRC is fundamentally different from those of the previous studies in that asynchronous communication is one-way-the message sender cannot respond to the viewer's inquiry in real-time.It is an open question to what extent, or if at all, users feel the social presence of an embodied 3D representation of their asynchronous collaborator and display social behaviors towards it (e.g., what do they feel if the recorded avatar breaches their intimate space?). \u2022 What are the challenges of maintaining workspace awareness and coordination in MAVRC? It is critical for the success of any CSCW system to offer proper support for establishing and maintaining different types of awareness about others' activities: workspace awareness is the knowledge of what is going on and what others are doing [17,25,36]; asynchronous change awareness is the knowledge of how shared artefacts (e.g., 3D models, source code, documents) have evolved via the contributions of time-distributed collaborators [76]. The MAVRC setting is at the intersection of 3D and asynchrony, where the challenges of awareness may be exacerbated by the combination of spatial occlusion, limited viewpoint, and lack of real-time feedback mechanisms (e.g., dynamic activities of one's asynchronous collaborator can occur out of the user's view, and the collaborator would not know how to draw one's attention to it). \u2022 What are the challenges of navigating and creating multimodal recordings in MAVRC? Benefits and challenges of multimodal recording have been deeply studied in non-VR CSCW contexts, such as document annotation [86], design prototyping [49], and video production [62]. On the one hand, the rich, communicative capacities of recorded multimodal interactions help \u2026this plant should be by the window\u2026 \u2026this plant should be by the window\u2026 \u25ba REPLAYING \u2022 RECORDING Fig. 1. Capturing and replaying speech, pointing, and head movement for a spatial collaborative task using our experimental system; (left, in the view of a recording producer) a remote collaborator makes a request about moving the position of the plant; (right, in the view of a recording consumer) an asynchronous listener replays the recording, watching the avatar of the remote collaborator as they express themselves through multimodal interactions.collaborators express and understand nuanced, complex ideas effectively [13,55,87]. On the other hand, as Grudin pointed out, when consuming, browsing recorded speech can be tedious and slow for the consumer [34], and, when producing, it is known that multimodal recordings are harder to revise than text [72] and can provoke self-consciousness [1,52]. When it comes to supporting spatial tasks in immersive CVEs, however, there is a significant gap of knowledge regarding the challenges and design considerations of authoring or consuming multimodal recording in VR.To answer these questions, we conducted a qualitative, exploratory need-finding study based on our experimental system, in support of multimodal recording in immersive VR. Our participants were tasked to perform asynchronously collaborative 3D design tasks by viewing and recording multimodal interactions. Key findings from our observations and interviews include: (1) participants felt the social presence of asynchronous collaborators when viewing recordings, leading to proxemic behaviors and empathy, (2) immersion in VR can cause challenges in viewing multimodal recordings, such as viewpoint disorientation or confusion from different versions of 3D scenes, (3) recording (and viewing) co-expressive speech and body language enabled participants to express (and understand) nuanced ideas effectively but highlighted needs for editing (and browsing) tools. We discuss implications of these findings for designing MAVRC systems and present a set of design recommendations. Demonstration and preliminary evaluation of four proof-of-concept interfaces provide support for the feasibility of our design implications.The contributions of this paper are three-fold: (1) a conceptualization of multimodal recordingbased asynchronous collaboration in VR; (2) empirical findings on the user challenges of respecting proxemics, maintaining awareness, and consuming/producing multimodal recordings in MAVRC; and (3) design implications for multimodal recording for asynchronous collaboration: proactive proxemics management, animating changes in 3D, viewpoint display for awareness, and 3D navigational cues. We also touch on the transferability and generalizability of our findings on MAVRC to non-VR asynchronous 3D interactions (e.g., AR, mobile 3D, etc.). Through the exploration of asynchronous collaboration in VR, this work opens up new opportunities for richer forms of collaboration, lessening the challenges presented by geographical and temporal barriers.", "relwork": "Our study combines themes of asynchronous collaboration, CVEs, and multimodal annotation from HCI, CSCW, and VR literature.", "rq": "We aim to address the knowledge gap in conceptualizing and understanding this multimodal asynchronous VR collaboration (MAVRC) as a type of CSCW, backed by an empirical understanding of its challenges and design considerations. VR, CSCW, and HCI literature reveals three aspects of MAVRC: social behaviors in mediated-communication, awareness in cooperative work, and authoring and consuming multimedia. Using these aspects as an intellectual lens, we begin to identify and tackle these knowledge gaps by formulating the following research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/183.xml", "intro": "Novel and more engaging technologies, together with the possibility to exploit emerging trends in information and communication technology (e.g., 5G), are shaping the tourism and hospitality sector [3,15]. This scenario is actual today more than ever due to the critical situation we are experiencing because of the COVID-19 pandemic [6,21]. The pandemic has compelled researchers and industry to explore better-suited technologies to reboot the tourist industry and regain consumer confidence [10,24]. In this light, the tourism sector embraced immersive technologies, such as virtual and augmented reality (VR/AR), able to provide virtual representations of touristic places and to offer touristic innovative services and experiences, with the final aim to increase the likelihood to physically visit such sites in the future [6,11,14]. Indeed, the opportunity to explore such immersive technologies in the hospitality and tourism sector is not new. In fact, the recent widespread diffusion of mobile devices that enables access to multiple sources of information in a ubiquitous, continuously connected environment has changed the way we experience tourism-related services [8,13]. In addition, the emerged concept of sharing economy is appropriating a collaborative and peer-to-peer market model, which prioritizes utilization and accessibility over ownership [5]. Tourism and hospitality have shown to be one of the pioneering sectors for its growth: residents can share their goods (such as homes, cars), as well as their expert local knowledge (e.g. locals being tour guides) [5]. This interest is aligned with the increasing travellers' demand for authentic, experientially oriented opportunities with more meaningful interactions with locals [20].In this rich context, we present ShareCities, an application that allows visitors and \"future\" tourists to exploit the 360\u00b0virtual representation of a resident's room, including peculiar details related to her/his life/interests/hobbies to foster playful immersive interactions between the visitor and the local community. Using ShareCities, local inhabitants can publish information about themselves, such as their profiles and preferences about what to see and do in their locale, while visitors can browse authentic and unmediated information provided by locals and use the platform to initiate dialogue with them. In particular, in this paper, we present the results of an experiment, engaging 19 users in the role of \"future\" tourist to investigate the possibility to use personalized 360\u00b0rooms to foster curiosity and affinity as the first step to initiate a conversation between tourists and locals, before the actual face to face meeting. The final goal is to design a system that explores novel interactions to create connections between locals and tourists to facilitate authentic travel experiences mediated by locals.The rest of the paper is organized as follow. The following Section details background and related work in social interaction between tourists and locals, and participatory platform in tourism. Then, we present the ShareCities platform and functions. The paper continues presenting the study we carried out, the methodology and the results. Finally, we conclude with final remarks and future works.", "relwork": "In this Section, the main studies and projects that inspired our approach will be presented.", "rq": "This study main intent is to shed light on the use of 360\u00b0personalized virtual rooms to create connections and a sense of affinity, and eventually, empathy, between tourists and locals. The final goal is to increase the likelihood to physically visit the tourisrtic place in the future, and experience authentic travel interactions mediated by locals. To better frame our goal, we defined the following research question (RQ): \"Can the use of 360\u00b0VR personalized rooms facilitate the creation of connections, affinity, and empathy between the tourist and the local?\""}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/7.xml", "intro": "Efective communication is important in everyday social interactions. Within many organisations, large investments are made by training employees to communicate better. It is also well recognized that non-verbal communication plays a signifcant role in the competency of communication styles in a wide range of contexts [63]. Most training interventions rely on human trainers to provide feedback to learners, but this is costly, labor intensive, subjective and heavily reliant on the skill and experience of individual trainers. Research within the domain of afective computing and social signals processing have started to explore the potential of augmenting or replacing human trainers through the use of automated recognition of nonverbal signals with promising results [7,14,30,31,59]. However, most studies have focused on a narrow range of channels. They also tend to focus on evaluating the performance of one individual, rather than considering the interplay of signals between communicators.Our previous work has explored which multimodal signals best predict human ratings of communication skills in the context of TV interviews and developed a usable feedback display to provide participants with information about their performance [47]. Efective media skills are important for many organisations including commercial companies, political parties and non-profts, since performance in this context can have huge implications for organizational reputation and outcomes. To our knowledge, none of the previous afective computing interventions for communication skills have focused on this specifc domain.In the current paper, we report an experiment which was conducted to assess the impact of the feedback intervention we had developed on the efectiveness of training to improve media interview performance. We present a controlled between-groups pre-post experiment study where half of the trainees received standard media skills training and half received the standard training augmented with tailored feedback based on automated recognition of facial expression, vocal signals, hand movements and 'honest signals' [45]. We compared performance before and after training across both groups using subjective measures of performance and using measurements of the participants' displayed social signals. The methods used here have potential to be adapted to support real world training interventions for media skills. Longer-term, the results are relevant to the development of an automatic training feedback system to help learners self-refect upon their performance.", "relwork": "The social signal processing (SSP) domain aims to understand and interpret social interactions using nonverbal cues [65]. Signal expression depends highly on context. To recognize the signifcance of an expression researchers must note where an expression is displayed, when it is displayed and who the presenter is [65]. Later, researchers included the signifcance of why and how a cue is expressed [49]. Research in this feld has been successful in capturing postures [55], gestures [11], vocal behaviour [15] and inferring emotions from facial expression and eye movements [68]. The contexts which have been investigated includes job interviews [42], healthcare [26], public speaking [53,54] and in the classroom [3]. Earlier research investigated social signals in isolation (i.e. facial expression only during an interaction); however, research has demonstrated that multimodal analysis is more informative of understanding naturalistic interactions. Van den Stock and colleagues (2007) investigated emotions associated with body expression and found that when investigated in isolation, the recognition of emotions is incorrectly recognized and cannot be interpreted [57]. It is noted that this is a result of visual integration of such cues which are necessary for adaptive behavior when responding to others [2].Augmentation of social interactions requires the use of sensor and visual displays that provide trainees with real-time feedback on nonverbal behaviours. The purpose of this is to increase trainees awareness of their use of nonverbal signals and improve the quality of their behaviour in any given context. The behavioral feedback method has been used to provide the user with real-time feedback that is suited to the user, the context and the scenario [17]. This method of feedback provision includes observational learning, operant conditioning, social cognitive theory, perception, refection and action [18]. Several studies have investigated the efcacy of this method [6,52,53]. Even though these studies found promising results, researchers did not investigate whether this method was distracting. In contrast, a study found that visual displays during an interaction was not distracting [18]. However, research in cognitive functioning postulates that an increase in visual load is cognitively taxing [1] and could impact trainees overall performance.Studies using technology enhanced training have been successful in improving social skills or communication skills [31]. Researchers [27] developed My Automated Conversation CoacH (MACH). MACH is a social skills training platform which allows users to communicate with a virtual character. MACH captures facial expression and speech and generates information on the users use of nonverbal cues. Similarly, a study by Damian and colleagues [16] found that this technique was useful in improving job interview training in underprivileged adolescents. Another group of researchers developed a feedback system called ROC Speaking Framework [68] [70]. Researchers found that feedback of social signals during job interview training signifcantly improved in comparison to traditional methods of training. Similar results were found when attempting to improve communication skills in those with social impairments [60], public speaking [13,52,53], medical students [39], job interviews [3,9,31,32,[41][42][43]. However, no studies investigate whether social skills can be improved using automated feedback in the context of media skills training.", "rq": "Furthermore, research is typically limited to investigating unimodal (facial expression alone) or bimodal (e.g., facial expression and hand gestures) signals and no research exist which investigates the retention of skills over time when augmenting training. The current research aims to investigate whether communication skills can be improved using automated feedback to improve self-awareness. Based on the literature we aim to address the following research questions:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/55.xml", "intro": "Social interactions amongst people, especially within close relationships, are important for people's well-being and quality of life [24]. Even short interactions with strangers, for example in buses and trains, can improve people's moods [12]. With the upsurge of ubiquitous mobile and wearable devices, there has been a digital revolution in the enhancement of social interactions amongst collocated individuals. These systems typically aim to improve awareness of strangers to connect with each other and facilitate the ice breaking phase and support maintenance of conversations [27,36,37,47,51,54,56].As the purpose of these systems suggest, these studies thus far have been limited to facilitating interactions between strangers i.e., people that have not been in prior contact. Yet, there are many consumer applications supporting face-to-face (F2F) interactions amongst friends [14,60,65]. For instance, Octi [65] a social augmented reality app displays a virtual belt around people of their social media applications and photos. Despite this growth in the industry, there is very little research investigating how technologies can support social F2F interactions between closer relationships, such as friends and close friends, beyond the current stranger focused landscape.To gain an understanding of how to augment F2F interactions upon closeness of relationship, we ran a study in which 20 participants first created a faceted profile; one profile aimed at a particular friend (later termed private profile) and one profile aimed at strangers (coined public profile). Participants then attended a gathering where they could view each other's profiles depending upon the relationship. Profiles were accessed through head-mounted displays as illustrated in Figure 1. This builds upon prior studies on investigating how F2F interactions can be augmented with wearable devices amongst strangers (e.g., [36,47]) or friends (e.g., [43,60]). In these prior works the usage of augmentations in F2F interactions have been investigated either within strangers or friends, but not both at once, although it is common to have different closeness of relationships collocated in everyday life [39,69]. In this study, we contribute by providing:1. Knowledge of how digital self-presentations differed depending on closeness of relationship, 2. An understanding of how people use and perceive the faceted digital self-presentation when the different closeness of relationships are collocated.Our research is pertinent to social technology designers and researchers: for interaction designers our research gives indications on how to support faceting in digital self-presentations in F2F interactions; for researchers in social technologies we provide initial insights on how digital self-presentations are created depending on the closeness of relationships and towards multiple audiences.", "relwork": "Drawing from social identity theories, the multi-faceted nature of human beings was first identified by Goffman [18]. For instance, people may maintain an occupational role in their workplace whilst playing a family role amongst their family [21]. These roles are performed to 'audiences', which vary according to many dimensions such as gender, interests, occupation, spiritual life, and community [18,40,59]. The separation of self-presentation towards different audiences is called faceting [18]. In this paper, we focus on faceting according to closeness of relationship.Strangers and known people are often mixed in daily F2F interactions [42]. This mixture of closeness of social relationships induces a social situation known as multiple audience problem [16,17]. Multiple audience problems cause tension as individuals try to perform themselves differently depending on the audience and norms of the F2F settings [18]. For example, self-presentation towards a friend is typically more modest than towards a stranger [66], even when collocated. Although people traditionally aim to avoid multiple audience problems [39], it is common that people need to give two different impressions to two different audiences in the same social situation [69]; such as one towards strangers and another towards friends [39]. To manage multiple audience problems, different people use different strategies; such as whispering and gestures, and amongst stronger ties, conveying clues that are understandable only by a certain group [9,16,17]. Yet, as mobile devices become part of our daily interactions with each other, most of the research into faceting has not included the digital aspect [5,16,17,69].", "rq": "Based on the results of the research questions, we recommend future designers and researchers to consider the following when augmenting F2F interactions with digital self-presentations."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/196.xml", "intro": "Broadening access to learning experiences about artificial intelligence (AI) is increasingly important as AI becomes more integrated into our everyday lives. Individuals who have little knowledge of AI or how it works are engaging with an increasing number of commercially available AI devices and technologies. Growing concerns about AI's role in misinformation [3,37], data privacy breaches [43], and bias/discrimination [7] suggest that technology users need new skills to be able to engage with AI critically and thoughtfully. This skillset has been referred to in the literature as AI literacy (i.e. \"a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace\" [34]).There has been a recent surge in AI learning interventions for individuals without a computing background, with a focus on K-12 audiences (c.f. [34,53]). However, few efforts to date have focused on museums as venues for AI education, even though museums and science centers have historically played an important role in public science education [45]. We are investigating how to provide learning experiences in museums that can foster public AI literacy, both by developing novel AI education exhibits and by adapting existing AI research projects into educational experiences through which learners have the opportunity to interact with authentic cutting-edge research in the field.Prior work suggests that certain design features-collaboration, creativity, and embodied interaction-can help facilitate effective learning experiences in museums. Embodied interaction is an intuitive way to engage with exhibits and can aid in concretizing abstract concepts [23-25, 31, 41, 44]. We explore designs that utilize full-body interaction, tangible user interfaces, and spatial metaphors. Most visitors come to museums in groups [21], making collaboration an important part of the museum experience that contributes to learning and motivation [13,25,26]. We define collaboration in this paper as encompassing both shared dialogue and working together to achieve a shared goal. Finally, creative interactions have been shown to contribute to prolonged engagement at exhibits and can lead to personally-relevant meaning-making [4,18,26]. In this paper, we use the term creativity to refer to designs that encourage learners to generate personally creative (i.e. P-creative, or novel to the individual [5]) ideas by expressing themselves through activities like dance or generating novel artifacts and combinations of ideas. Research suggests that embodied interaction, collaboration, and creativity may also be effective at facilitating learning about computing [10,15,20,35,46,54].Our hypothesis is that interactions with embodied, collaborative, and/or creative AI learning activities in informal learning spaces lead to interest development in AI and improved understanding of AI. In this paper, we test this hypothesis by exploring two core research questions: 1) How can embodiment, collaboration, and creativity be used in museum exhibits to encourage interest development in and learning about AI? and 2) What design features contribute to engagement with activities that increase interest in and improve understanding of AI in informal learning spaces? To address the first question, we designed three AI literacy exhibits-Knowledge Net, Creature Features, and LuminAI-that each incorporate collaboration, creativity, and/or embodied interaction to varying degrees. We explore how to develop AI literacy exhibits \"from scratch\" with Knowledge Net and Creature Features. The third exhibit (LuminAI) explores how to adapt and augment an existing AI research project to facilitate an educational experience about/with authentic AI technology. To investigate the second research question, we conducted remote user studies with 14 family groups (38 participants) with two study sessions. The first session of users engaged with an early iteration of our prototypes, and the second session interacted with a later iteration of the same prototypes. We present results from these studies, focusing primarily on an analysis of participant surveys (supplemented with qualitative observations when relevant). Our analysis assesses the degree to which the exhibits supported collaboration, creativity, and embodied interaction, and explores the relationship of these design features to AI learning and interest development.", "relwork": "There is a growing body of research investigating how to design AIrelated learning experiences for novice audiences. Researchers are developing curricula for both K-12 audiences [2,48,50] and non-CS majors at universities [6,19,46]. Others are developing courses, interactive online tools, and programming platforms that can engage novice audiences in learning about AI (e.g. [1,15,30,54]). The exhibit designs presented in this paper are grounded in two recently published frameworks related to AI literacy. The first framework presents five \"big ideas\" that define areas of AI that are important for K-12 audiences to understand: 1) perception; 2) representation and reasoning; 3) learning; 4) natural interaction; and 5) societal impact [48]. The second framework is a set of AI literacy competencies and design considerations we developed based on a review of AI education literature [34]. The competencies are high-level ideas about AI intended for novice audiences, and the design considerations are intended to guide the development of AI literacy learning interventions. We used both of these frameworks to guide the design of the prototypes presented in this paper.In the remainder of this section, we review several AI education projects that emphasize our key design considerationscollaboration, creativity, and embodied interaction. There are numerous existing platforms that are designed to engage learners in creative programming activities involving AI. Cognimates is an add-on for the Scratch programming environment that allows learners to incorporate AI technologies like image or voice recognition in their Scratch programs [14]. Similar tools exist for other coding platforms (e.g. [1,28,51]), allowing learners to incorporate AI in their creative multimedia projects. Others have developed activities to engage learners in creatively imagining alternative AI futures-like an AI ethics activity that engages middle school students in redesigning YouTube [2]. A recent paper outlined a set of design principles for introducing co-creative AI research projects in public spaces-while not explicitly focused on AI education, we draw on several of these principles in our work [33]. Research also suggests that having learners enact embodied simulations of algorithms (either on their own or by programming an embodied AI device [15,49]) can help them to concretize abstract concepts [15,46]. Other platforms engage learners in building machine learning (ML) models of physical gestures like dance or sports moves [8,54]. There are fewer existing projects that are focused on collaboration. However, recent papers suggest that facilitating social dialogue, particularly between adults and children, is important in AI learning contexts [16,34,50]. AI plugins on platforms like Scratch also facilitate social learning by allowing learners to share their work with a wide audience and \"remix\" others' projects [40].", "rq": "Our findings indicate that learners enjoyed the embodied nature of the exhibits. Learners commented, \"It was a great blend of hands-on and screen time, \" and \"I liked the tactile manipulation of the tiles representing abstract info. \" Learners also liked seeing their personal movements captured in LuminAI and observing how the agent replayed them, modified them, or responded with familiar dance moves. Our findings indicate that the embodied interfaces facilitated a low barrier to entry for AI novices. One participant commented that they \"liked how the activity wasn't too complicated, and anyone could do it (as opposed to if we had to code the AI ourselves).\" We noted that interaction and discussion time was skewed towards the embodied component of the activity. For instance, in Knowledge Net, participants focused on selecting tiles and relationships to construct the network, with less time spent interacting with and discussing the chatbot. Similarly, participants spent significantly more time dancing with LuminAI than they did exploring the interactive visualization. This imbalance points to the engaging nature of the embodied interactions but raises additional research questions about how to foster AI learning experiences that span both physical and digital interfaces."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/2.xml", "intro": "Rarely is there a celebration without a cake. Apart from being an edible art, a customized cake is often a ceremonial symbol [11,16], which is special and personal, and closely associated with social relations and emotions [28]. Customized cake services enable clients to collaboratively personalize their cake in shape, color and favor with pastry chefs [63]. However, the customization process is not easy for both clients and chefs, which usually starts in a face-to-face meeting. Most of the follow-up communications are through text messages with the aid of reference cake pictures, which is insufcient for them to fully communicate their creative thoughts and to have a clear image of the fnal design [56,78]. Cake customization requires professional skills. Based on 2D reference pictures and texts, it is not only difcult for clients to express the ideal decorations they want [40,78], but also challenging for pastry chefs to immediately visualize and show the size and decorations of the cake to the clients [56]. Fig. 2 illustrates such difculties. VR technology is developing at an unprecedented speed, which can simulate users' physical presence in a virtual environment, allow them to move around, and to interact with virtual objects. With the shifting focus from isolated experiences to a social medium, social VR has attracted a large stream of research exploring its potential for creating innovative communication approaches, supporting remote experience sharing and collaboration in diverse scientifc, artistic, informational and educational domains [18,55].As a new remote communication medium [2], Social VR is distinguished from video conferencing tools by their capacity to portray 3D spatial information [68], to exploit users' natural behaviors, and to immerse users in the virtual world [13,32].We posit that social VR is a promising medium to support clients to remotely co-design customized cakes with pastry chefs. Social VR allows pastry chefs and clients who are physically separated to co-present in a shared virtual space, and to assist their cake co-design by providing intuitive virtual interaction techniques and real-time 3D visualizations of virtual cakes. Both clients and chefs can instantly see the real-size 3D cake visualizations as their codesign results.In this paper, we aim to address three research questions:\u2022 RQ1: What is the current communication process of cake customization between clients and pastry chefs? \u2022 RQ2: What are the design requirements for a social VR cake co-design tool (CakeVR)? \u2022 RQ3: To what extent the design and implementation of the CakeVR prototype meet the design requirements? Exploring cake co-design as a new social VR use case, this paper made two main contributions. First, it specifes the requirements for designing a social VR tool (CakeVR) to support remote co-design activities (Section 3). Second, it implemented the CakeVR prototype and had it evaluated by experts (i.e., pastry chefs and experienced cake customization clients) using the cognitive walkthrough method [54] (Section 4 and Section 5). The evaluation results show that CakeVR supports idea generation and decision making by allowing users to intuitively manipulate sizes, select favors, design the decorations, and check whether the design fts the celebration theme (Section 6). We also discussed how CakeVR can be adapted to other domains, and its potential to transform product design communication through remote interactive and immersive co-design (Section 7).", "relwork": "The section presents related work about the co-design method, cake customization practices and social VR technology.", "rq": "Our fndings have addressed the three research questions (RQ1-RQ3). The fndings of the requirement gathering interview (Section 3.4.1) describe the current communication process of cake customization between clients and pastry chefs (RQ1). The fndings in Section 3.4.2 identifed eight requirements based on interviews (RQ2). Our fndings from our expert evaluation (Section 6 show that social VR is a suitable tool to immerse users and enable them to remotely co-design cakes (RQ3)."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/192.xml", "intro": "Nowadays, virtual meetings are an essential tool for collaborative work across the entire globe or for those working from home. Compared to the more established video conferencing, virtual reality (VR) meetings do not use webcams to share video information of the meeting but instead personify attendees in the form of avatars in VR. The intensive usage of video conferencing software during the COVID-19 pandemic showed that such systems can cause privacy issues, while VR meetings lead to an increase in involvement, spatial presence, and experienced realism compared to video conference systems [42]. Besides all obvious advantages, we observe that attendees often multi-task during virtual meetings 1 , which makes it harder to moderate them and to ensure productivity of the conversation.Several information can be expressed through body language, such as emotion [12] as well as how carefully a person is listening to a conversation or how willing they are to communicate [5]. Thus, we easily understand whether a person is listening or doing something in parallel through body language in physical meetings [41]. Avatars however do not let us read the conversation involvement of meeting attendees very well as nonverbal signals given through mimic, gesture, and body posture are missing.Inspired by status signs used in video conference systems, such as Skype, to inform about the availability of a person to be contacted, we propose to provide the VR meeting attendees with information about other attendees' conversation status, attention, and engagement. Referring to our ability to \"read\" body language, we propose using avatars' body language to show if they are highly involved in the conversation, multi-tasking, or even not listening for some time.We expect that avatars acting in a way that represents the cognitive involvement of users in VR meetings can increase empathy between virtual meeting attendees, which -as a consequencewould ease work with remote collaborators and ensure communication productivity.We aim at extending previous research that investigated emotion estimated through the pose of avatars [9,26,28], effects of the body language of avatars in virtual meetings [45], and possibilities of influencing a conversation through body language [39] through proposing avatars' body language as non-verbal information about the conversation readiness and involvement in VR meetings.While standing up and talking on the phone to somebody else would be impolite in a physical meeting, we possibly could use such behaviors in a virtual meeting to indicate a parallel task. As no social rules for such behavioral meeting status exist, we aim to explore if and how a set of avatar behaviors would be interpreted in VR meetings. Hence, we created a simulation of a VR meeting with avatars showing typical as well as untypical behavior, such as carefully listening, checking their phone, and having a nap.In a user study, we found that the body language of an avatar can indeed represent the communication status of their corresponding user and provide design recommendations about what body language including behaviors are appropriate in VR meetings and which are not. With this work, we contribute to the field of VR collaboration as well as avatar research. We hope to inspire future work as this paper serves as a proof-of-concept, and more work is needed to develop better VR meeting systems.", "relwork": "Two research areas are particularly related to this work: (1) visualizations of the communication status in virtual meeting systems and (2) body posture and emotion expression of avatars.", "rq": "To better understand how avatar's body language can represent a communication status in a VR meetings, we determined following research question:"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/186.xml", "intro": "The science of recording human eye movements has its origins in the nineteenth century [Wade 2007] and is, by now, touching many areas of research. Nowadays, sophisticated tools and programs for generating eye movement data are available (see, for example, [Farnsworth 2019]). The use of eye tracking devices within research is commonly applied in cognitive sciences and has contributed to different discoveries [Matlin and Farmer 2017;Tanenhaus 2007]. Eye tracking data has, however, proven useful in many more scientific areas, covering topics such as dyslexic reading behavior [Ragozin and Kunze 2019;Robertson and Gallant 2019] or the impact of scrolling past Facebook messages [Chou et al. 2020].The focus of this work is on eye movement data, for which it is important to create appropriate visualizations to support analyses of the data in order to obtain qualitative information related to the research questions and tasks at hand. Eye tracking systems often already provide visualization options, but these are usually limited. Eye movement data sets can quickly become large, in turn, often resulting in overplotting and visual clutter [Rosenholtz et al. 2005].As key contributions we provide a web-based approach combining several existing and well-known visualization and interaction techniques [Yi et al. 2007]. Another novel aspect is the fact that not only separate visualization techniques can be chosen but also the same one several times to show different data aspects, particularly for comparison tasks. Further goals are to not require installation, helping users to get started, making the tool easy-to-use, and offering different visualization techniques necessary to enable researchers analyze different aspects of their eye movement data. For building the user interface, we used React [Walke 2013], a flexible componentbased JavaScript framework, together with Material-UI [Nguyen 2013]. The tool is available at https://dbl.p4ul.nl/.Used individually, visualization techniques are typically limited in value (e.g., heatmaps do not show sequential or temporal information [Bojko 2009;Burch 2016b]; gaze plots are susceptible to visual clutter [Goldberg and Helfman 2010;Rosenholtz et al. 2005]). Our tool addresses this issue by providing four visualization techniques that cover several data dimensions in eye movement data such as space, time, and participants, as well as additional attached data attributes: a scanpath visualization [Goldberg and Helfman 2010], a 2D density plot [Burch 2016a], a bee swarm [Blascheck et al. 2017], and a scarf plot [Yang and Wacharamanotham 2018]. We illustrate the usefulness of the interactive visualization tool by applying it to eye movement data from a formerly conducted eye tracking study investigating the readability and route finding tasks in public transport maps [Netzel et al. 2017]. Moreover, we conducted a user study to investigate if the interactive web-based visualization tool is understandable and usable to solve typical eye movement data tasks.", "relwork": "Exploring eye movement data in a combination of interactively linked visualization techniques has been studied before. For example, Bakardzhiev et al. [Bakardzhiev et al. 2020] describe another web-based solution but in their tool there is neither an option to inspect different data aspects with the same visualization type next to each other nor a way to share the found insights with others. Moreover, only a limited set of interactions is supported and they do not evaluate their tool to illustrate the usability aspects. The EyeClouds tool [Burch et al. 2019b], on the other hand, integrates several fixed but interactive views, making it less flexible and hard to exchange and drag-around views on users' demands, but positively, they provide a perspective on image-based clustering [Burch et al. 2020] that is not implemented in our tool. The VERP Explorer [Demiralp et al. 2015] combines views based on standard visualization techniques with a specific focus on recurrence plots in a very specific domain, hence, it does not allow to explore eye movement data in a more conventional way. Another specific visualization tool focuses on the combination of standard eye movement data visualizations [Menges et al. 2020], enriched by more advanced ones such as attention flows or 3D scanpath representations. Many more tools can be found that focus on very specific application domains such as fixation distances [Burch et al. 2019a], parallel scan paths [Raschke et al. 2014], or video visual analytics [Kurzhals et al. 2014[Kurzhals et al. , 2016] ] but they typically do not provide the standard easy-to-understand visualizations, nor do they allow to easily exchange the views, and to share the found insights with others.In general, many visualization techniques for supporting the analysis of eye movement data have been developed in the past. Blascheck et al. [Blascheck et al. 2017] describe two main categories of visualization techniques: point-based and AOI-based methods. Point-based visualizations are useful to provide a more detailed unaggregated view on the data while AOI-based methods are useful for including semantic information of the stimuli: Users are able to select areas of interest within a stimulus, after which this information can be integrated in the visualization. An example of such an AOI-based visualization is the scarf plot [Yang and Wacharamanotham 2018] which is also integrated in our tool. Also a scanpath representation in the form of a gaze plot [Goldberg and Helfman 2010] is supported. Although gaze plots provide a lot of information concerning gaze trajectories, they can be prone to visual clutter [Rosenholtz et al. 2005]. To address this issue, we implemented a dispersion-based algorithm to cluster the data points into more general fixation points [Salvucci and Goldberg 2000]. A novelty in our tool allows users to interactively change the dispersion value to a minimum of zero which removes visual clutter while the original scanpath is still preserved.Another popular visualization technique from the large repertoire is the heatmap or sometimes also called visual attention map [Bojko 2009;Burch 2016b]. A heatmap only provides an overview about the hot spots of visual attention, neither showing temporal aspects nor individual people's visual attention. It typically shows an aggregated form of the recorded eye movement data and focuses on space. Nonetheless, some approaches [Burch 2016b] also try to incorporate the temporal aspect in the final representation. Although heatmaps aggregate the recorded data a lot, they definitely provide a good overview, however, at the cost of hiding the stimulus in the background if a heatmap becomes very dense. Hence, one view is typically not sufficient, requiring a combination of several visualization techniques in a multiple coordinated view [Roberts 2003]. Moreover, to make the occlusion of the visual stimulus by the heatmap less problematic we use contour lines instead of explicit On the right, a scarf plot for the same stimulus is displayed. The sliders are positioned to the left at default values (20 for the bandwidth and 15 for the threshold). The time slider for the scarf plot is placed at 6.7 seconds. All users are selected in both views. The sliders at the bottom of each view allow to change parameters of the visualizations. density fields to encode the hot spots of visual attention on a stimulus. In addition, a bee swarm visualization can provide an animation of the fixation points [Li and Sayed 2012] to examine the temporal change, exploiting the Gestalt law of common fate [Wertheimer and Riezler 1944]. This animation-based concept is not useful for comparing data over time or regions and has some cognitive drawbacks [Tversky et al. 2002]. It, however, allows to quickly identify hot spots and their movement over time. As such it can be seen as a kind of rapid serial visual presentation mode [Spence and Witkowski 2013] to get a time-varying overview about the visual attention behavior.", "rq": "The focus of this work is on eye movement data, for which it is important to create appropriate visualizations to support analyses of the data in order to obtain qualitative information related to the research questions and tasks at hand. Eye tracking systems often already provide visualization options, but these are usually limited. Eye movement data sets can quickly become large, in turn, often resulting in overplotting and visual clutter ."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/90.xml", "intro": "Co-located couples stay connected on a regular basis by relying heavily on communication technologies like text messages and phone calls. They also talk and discuss their daily experiences when they meet each other in person [16,25]. Yet some couples are separated by distance and in a long distance relationship (LDR) due to career or studyrelated obligations, as well as frequent travel [19,32].Couples in LDRs may easily face difficulties in maintaining their relationship [32]. This makes computer-mediated communication systems beneficial for maintaining their relationships [1,25], if they can afford and have access to such technologies.Video communication technologies are now becoming a popular tool for supporting such needs as they allow LDR couples to simulate a face-to-face conversation over distance and even engage in acts of 'shared living' [25,26]. Yet the challenge is that video communication systems, as well as other computer-mediated communication systems, do not allow partners to experience the sensation of a physical touch [31]. Physical touch may include acts such as hugs, caresses, or handholding. Such touches have been shown to be critical for the physical and mental well-being of couples [12,31]. For these reasons, our research explores the design of systems to support physical touch over distance for long distance couples. Previous work has explored intimacy by designing prototypes based on the analogy of hand-holding using heat and movement [14] and wearable tactile devices for the arm [35] which are useful for conversation discourse and conveying positive affect. Our work builds on these efforts to explore acts of synchronous physical touches over distance using vibrotactile sensations.First, we present the design of a tangible communication system for distance-separated couples called Flex-N-Feel. With Flex-N-Feel, couples are able to transmit touch to one another over distance using a pair of interconnected gloves. Bending the fingers in one glove causes vibrations to appear in a second glove. Second, we present a lab study of Flex-N-Feel with nine couples where we asked them to use the tangible communication system where it was augmented with either a Skype audio-only call, or a Skype video call. Partners used the gloves in an open-ended fashion in both situations. Through our prototype design and evaluation, we sought to answer three main research questions: What are the important characteristics of a vibrotactile glove to facilitate a sense of touch between long distance couples? How do couples use a vibrotactile glove for supporting touch? How do video and audio connections support or hinder the use of vibrotactile gloves?Our evaluation of Flex-N-Feel revealed that couples valued the experience of touch and enjoyed the interaction more with a Skype video connection. They felt a strong sense of presence when using the gloves and felt more emotionally connected with their partner. Couples used the gloves in a variety of different ways, many of which we had not envisioned. These related to four themes: shared actions, playfulness, intimate touches, and presence. These results reveal design suggestions for systems focused on sharing touch over distance, including designing for appropriation, the augmentation of current communication technologies (as a starting point), and allowing users to manipulate the properties of the sensory medium and the mobility of the prototype.", "relwork": "Many couples in LDRs rely heavily on computer-mediated communication tools to connect over distance and these systems provide an ever-increasing range of ways to stay connected [32]. For example, text messaging is often used throughout the day by couples to share details about one's ongoing happenings or to ask quick questions [25]. Email is often used for coordinating tasks, sharing funny stories, or reminiscing about one's relationship [21,25]. We also see couples use video communication to stay connected [16,25,26,30] where some use open video links during the evening to connect with one another and simulate shared living [16,25,26]. Research has also shown that couples use video communication systems to engage in intimate activities and even sleep together with a video link open [16,25,26]. Couples even make video calls during shopping, touring locations, and sight-seeing activities [13,37]. Computermediated communication tools have been shown to increase feelings of closeness between couples [1], build mutual trust, and improve relationship satisfaction [9], yet they lack one of the most important tangible aspects of face-to-face conversation: the power of touch, which can express an array of various subtle messages [8].", "rq": "First, we present the design of a tangible communication system for distance-separated couples called Flex-N-Feel. With Flex-N-Feel, couples are able to transmit touch to one another over distance using a pair of interconnected gloves. Bending the fingers in one glove causes vibrations to appear in a second glove. Second, we present a lab study of Flex-N-Feel with nine couples where we asked them to use the tangible communication system where it was augmented with either a Skype audio-only call, or a Skype video call. Partners used the gloves in an open-ended fashion in both situations. Through our prototype design and evaluation, we sought to answer three main research questions: What are the important characteristics of a vibrotactile glove to facilitate a sense of touch between long distance couples? How do couples use a vibrotactile glove for supporting touch? How do video and audio connections support or hinder the use of vibrotactile gloves?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/53.xml", "intro": "An extensive body of research testifies to the fact that visits to cultural heritage sites are generally social in nature: people overwhelmingly come to sites in groups (e.g., as part of organized tours, school excursions, parties of family and friends), and even if alone, they still tend to be aware of other people's presence in their visiting experiences [ 21 , 60 , 82 ]. Moreover, sociality, and conversation in particular, are recognized as essential means to create engagement amongst individuals in museum environments and other cultural contexts [ 28 , 33 , 46 ]. Some visitors actually interpret the cultural visit as an \"encounter session\" wherein social interaction is their primary goal (see, for instance, the Utopian visitor type, as identified by Umiker-Sebeok [ 76 ]).Information technologies play a particularly vexed role in such social encounter scenarios. One of the most influential frameworks in research dealing with digital devices and engagement in museums is the Contextual Model of Learning, proposed by Falk and Dierking [ 20 ]. Herein the authors distinguish three different contexts of visitor interaction that should be carefully considered when designing a cultural experience-personal, physical, and socio-cultural-recognizing the interdependencies of each, but also the crucial importance of the latter (including social exchanges) to the visitor experience. What is key for our argument is that the use of mobile devices in museums often pits these contexts against one another by, for example, privileging the personalized experience (e.g., use of the visitor's own small phone or tablet; the use of individual headsets) above outward-facing engagement with others [ 84 , also see 28 , 30 ]. As we have reviewed elsewhere [ 35 , 59 ], studies are increasingly attempting to reconcile such divides, with an explicit concern for deploying digital technologies to enhance the socio-cultural context and facilitate social interaction during cultural visits.The existing scholarship hints at the positive impacts of designing sociality into digital experiences for museum visitors, but it does not yet robustly establish a relationship between visitor preferences, specific forms of social engagement, and the achievement of broader institutional and public goals. In this article, we present the results of a user study that systematically investigates the effects of group conversation and social interaction in the context of a digital cultural storytelling experience. Aiming at a detailed qualitative and quantitative assessment of the benefits and weaknesses (both for visitors themselves and for the institutions that host them) of promoting social interactions in a digital storytelling setting, we start with a story-based experience that was designed for individual use; we extend it with novel system-driven interaction prompts, and then we evaluate the two versions with 102 participants in pairs, comparing and analyzing the results over several dimensions.The story described below was created as part of a tour of the archaeological site of \u00c7atalh\u00f6y\u00fck in Turkey. It was originally designed to be experienced at the archaeological site itself. In this article, the story has been re-designed as an introductory digital experience that takes place ahead of the visit to the excavation areas, either within the site's Visitor Center or even before reaching the site. The story-based experience delivers audiovisual content on personal handheld devices, similar to mobile guides. To promote social interactions between visitors, participants are directly asked to verbally or physically interact with each other, at particular points in the story. To this end, instead of inducing or stimulating conversation indirectly, as in related work like Sotto Voce [ 26 ], or in experiments with narrative variations [ 8 ], we adopt a more direct approach by incorporating explicit interaction prompts into the story, hence guiding visitors into purposefully framed forms of social interaction and enhancing their awareness that this is a social experience.", "relwork": "In this section, we briefly review key research from the domains of digital storytelling and social interaction as applied in cultural heritage. While extensive, the literature suggests gaps in the existing knowledge base around digital media for social museum experiences. These gaps provide the impetus for our current work.", "rq": "Summary of Results on Research Questions RH7 and RH8: Examining the Effect of Visitor Profile on Social Interaction Preferences"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/185.xml", "intro": "Today, we rely heavily on digital technology to connect with others. Furthermore, with the global COVID-19 pandemic diminishing in-person social contact, technology-mediated communication is more prominent than ever before [35]. However, digital communication is well-known to be challenging due to limited access to important nonverbal cues, such as our body movements and facial expressions [32,60,63].An emerging area of research in HCI has explored a novel social cue for improving the way we interact over technology: our biosignals. Biosignals, such as heart rate and skin conductance, are well known to change according to our physical and emotional responses, and can be revealed in everyday interactions using wearable sensor technologies. For example, applications like Pulsoid or Onbeat 1 explore this possibility through livestreams of heart rate during gameplay or exercise. Researchers have shown that expressive biosignals, or biosignals displayed as a social cue, have the potential to facilitate communication as a means to recognize and express our emotions and physical being [21, 23, 26, 40-43, 50, 55, 57]. However, researchers have not yet described the role that biosignals play in communication. Biosignals are personal and private data that require careful design and consideration [20,22,26,41]. In particular, as cues that are sensed and recommended by systems, they present a new form of AI-mediated communication that could shape our interactions in unintended ways [25]. Thus, it is crucial that we understand the value and consequences of integrating them into our existing means of communicating.In the present work, we expand on expressive biosignals literature by demonstrating the efects of shifting from communication without biosignals to communication with biosignals. We designed, developed, and deployed Signifcant Otter, an Apple Watch and iPhone app that enables romantic couples to send heart rate-driven otter animations as messages to each other. By setting adaptive thresholds for each person based on their past heart rate and motion data, Signifcant Otter intelligently suggests animations that match their current emotional and physical state. To explore the design of expressive biosignals as AI-mediated communication, we incorporate AI-recommended sets of shareable sensed states. In a one-month within-subjects feld study, we investigate how couples' behaviors and perceptions are afected when shifting from a sensing OFF version of the app, with no biosignals sensed, to a sensing ON version, with biosignals sensed. We present qualitative results from interviews during the study and discuss opportunities and challenges for biosignals in communication.The core contributions of this work are: (1) Signifcant Otter 2 , a novel smartwatch and phone app that promotes communication and connection between romantic partners through animated avatars recommended based on heart rate; (2) an empirical study with 20 couples who used Signifcant Otter with sensing OFF and ON that demonstrates the value of biosignals as a lightweight and authentic social cue; (3) design implications and future directions for expressive biosignals research, including suggestions for integration into social platforms as a form of AI-mediated communication.", "relwork": "For the purposes of this study, we focus on communication between romantic couples. Given the intimate nature of physiological data [26], people feel most comfortable sharing them with close others [41], who may also be the most interested and equipped to understand them as limited contextual cues. For instance, couples can interpret work breaks or distance from home based how many steps their partner has made [14]. In lightweight communication, defned by quick exchanges [8] through minimal interaction or content generation, even minimal messages between close partners can convey meaning like \"thinking of you\" [7,28]. Thus, we target the closest partners: signifcant others.A breadth of HCI research has explored technologies that can support signifcant others, including those that integrate biosignals. In their review on technology-mediated intimacy, Hassenzahl and colleagues described diferent strategies for supporting important aspects of intimacy [16]. For example, physicalness represents the physical aspect of intimacy, and has been simulated through mediated touch [15] and gestures [12,52], as well as feeling someone else's heartbeat [64]. Expressivity describes expressing feelings through a language unique to the couple, such as mutual afection through \"on-of\" signals [29] or couple-specifc symbols [36]. Awareness of one's partner has been explored in systems that display a partner's presence, activities, and mood through availability [6,10] or sensed contextual information like location [2,66], motion [3,67], and heart rate [17], or a combination of these data [14].With the integration of biosignals, Signifcant Otter similarly incorporates physicalness, expressivity, and awareness to support intimate communication. Signifcant Otter can simulate physicalness through shared heart rate representing the body's physical state. It can support expressivity by providing an emotional language for couples through otter animations embedded with heart rate, which communication partners can use to create emotional meaning together [41]. Finally, the app's heart rate animations can enable awareness by providing contextual cues that display presence, activities, and mood [17,42]. We describe the full Signifcant Otter system in Section 3.", "rq": "We conducted a one-month study deploying Signifcant Otter in the wild with couples who were Apple Watch users. All participants used the app with sensing OFF for the frst two weeks and with sensing ON for the latter two weeks. We intentionally did not counterbalance the order of the versions, as our research questions focused on the shift from the status quo of communicating without biosignals to communicating with biosignals. Moreover, the removal of \"sensing\" as a feature in a counterbalanced study could disrupt participants' mental model of the app, as opposed to a feature update when switching from sensing OFF to ON. The study consisted of the following sessions: 4.2.1 Onboarding Session. Each couple completed a 30 minute onboarding session with one of the researchers over a video call. During this session, participants installed Signifcant Otter with sensing OFF on their iPhone and Apple Watch through Apple's TestFlight 8 and added the app as a complication on their watch. During the installation, participants completed a short questionnaire about their background and relationship with their partner. Then, we instructed them on using the app, and asked them to test it during the session to ensure that it was installed and working correctly."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/218.xml", "intro": "", "relwork": "e exibility o ered by the mobile devices to easily switch camera feeds and change device orientation enable their use for collaborative physical tasks. O'Hara [2006] conducted a diary study. In their sample 28% of video calls involved showing things in the environment to talk about and 22% were for performing functional tasks (e.g. planning events or seeking guidance). Similarly, Brubaker et al. [2012] note a growing trend in using video to support joint activities (e.g. seeking guidance to accomplish physical tasks) and experiences (e.g. giving a tour of a at). Jones et al. [2015] studied how people collaborate using mobile video and found that a serious shortcoming of commercial mobile video conferencing services is the lack of support for remote gesturing, which is known to be important to e ciently use video as a collaborative activity space. Previous work has shown the growing trend in mobile video telephony to use \"video-as-data\", instead of the conventional \"talking heads\" [Nardi et al., 1993]. ese new applications of mobile video require gesturing mechanisms e.g. to point out interesting details in the environment, or to e ectively communicate procedural instructions in a physical task.2.2 Gaze sharing in collaboration: Does the level of awareness matter?ere have been numerous studies on gaze awareness in collaboration, in tasks involving visual search [Brennan et al., 2008], programming [Stein and Brennan, 2004], trip-planning [Qvarfordt et al., 2005] and puzzle-solving [Velichkovsky, 1995].e most common approach to provide gaze awareness in collaboration is to present the gaze of the partner as an abstract visual element, such as a dot, ring or icon of the eye, overlaid on the shared visual space (notable exceptions are Trosterer et al. [2015] andD'Angelo et al [2017]).e previous studies on shared gaze can be classi ed, based on the level of awareness the producer of the gaze has on the gaze sharing. Qvarfordt et al. [2005] studied value of naturally occurring eye movement in a collaborative trip-planning task and found that gaze, even if not explicitly produced with the intention to communicate can aid deictic referencing, aid topic switching and help reduce ambiguity in communication. Similarly, Stein et al. [2004] found that eye gaze produced instrumentally (as opposed to intentionally), can help problem solving in a programming task. Liu et al. [2011] noted that naturally occurring gaze can help to e ciently achieve referential grounding. In all these studies, the producer of gaze was not aware that the partner would see their gaze point and thus did not use gaze as an explicit communication channel. us, the gaze point re ects their natural gaze behaviour.In contrast, other studies used a setup where the collaborator is aware that their gaze is being shared, and thus they use their gaze more explicitly to communicate. However, some studies showed the collaborators their own gaze point, providing accurate awareness of the point that is transferred and others did not show own gaze point to the collaborator. Akkil et al. [2016] and Higuch et al. [2016] studied a set-up where gaze of the remote instructor was physically projected to the task space of the partner. us, the instructor saw the physical projection of their own gaze point on the video captured by the situated camera, giving the instructor direct feedback of their own eye movements and accuracy of gaze tracking. Others have studied shared gaze in a collocated scenario, where both the collaborators are in front of the same display, enabling the collaborator to see their own gaze. Zhang et al. [2017] studied collocated visual search on a large screen, Maurer et al. studied gaze gaze sharing between a collocated game spectator and gamer [2015], and between passenger and driver in a driving simulator [2014]. Similar set-up was also used by Duchowski et al. [2004] in collaborative virtual environments.Similarly there are a number of studies involving shared gaze used in a set-up where the collaborators are aware of shared gaze and saw their partner's gaze, but did not see their own gaze point. Examples include, Brennan et al. [2008] in a collaborative visual search task, D' Angelo et al. [2016] and Muller et al. [2013] in puzzlesolving tasks, Lankes et al. [2017] during online game viewing and Maurer et al. [2016] during online cooperative gaming. Interestingly, Maurer et al. [2016] note that their participants commented they would have liked to see their own gaze point, along with the partner's gaze. In contrast, D'Angelo [2016] note that showing the own gaze pointer may not be a good idea, since it \"can produce a feedback loop that causes people to follow their own cursor\", when gaze tracking is not accurate.In summary, previous studies on shared gaze have used three di erent con gurations of gaze sharing, and found value for all three in the collaboration. is brings us to the question, are they all equally e ective? A comparative evaluation between the three setups would give us novel insights into the utility of each of the con gurations. is was the focus of our work.", "rq": "5 DISCUSSION RQ1: Does sharing gaze of the instructor that is produced implicitly, provide bene ts comparable to when the gaze is produced with the intention to communicate?"}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/230.xml", "intro": "Large touch displays are becoming an increasingly common feature of today's public, semi-public, and private environments [7]. Alongside information and advertising, they are frequently used for entertainment and advergames (i.e., games used for advertising). For example, a store may put up a large display touch game to sell a product or keep children occupied while parents shop their products. As we continue to develop and deploy large touch displays, it becomes important to understand the social situations created by adding a large interactive display in various public, semi-public, and private settings as a gaming device.Researchers have demonstrated that during large display interaction with multiple users, territorial division of the display is present [44,40,16]. In past research in varied task domains, it is clear that users consider screen real estate of the display and distribution is made equitably between all participating parties. However, work on display territoriality has largely focused on shared productivity or urban informatics tasks [44,40,16]. Across contexts such as playful interaction and gaming, designers still lack information needed to understand how the users allot space based on physical (real-world) distance and digital (tool use) distance. Notably, research has demonstrated that the extended-arm metaphor does not lead to freely approachable space. In other words, simply extending the users' reach alone will not result in users' freely using a shared display [11].Given the lack of information on issues of territoriality in public and semi-public large display gaming, and given the fact that we see advergames and edugames becoming more DOI: http://dx.doi.org/10.1145/3313831.3376319 commonplace in public and semi-public spaces [35,36], in this paper, we explore two related research questions.RQ1: Do the findings of territoriality from media sorting and urban informatics tasks apply similarly in a game situation?To understand the perceptions of player territory, we provide players with a visible differentiation of space and counterbalance two conditions. In the first condition, players are unable to physically move into another players' physical space, but may enter using a long range cursor technique. In contrast, our second condition allows for both physical movement of a players workspace, and digital long distance reaching. By studying the two levels of potential encroachment, we seek to understand players' perception of territoriality.Alongside basic questions of territory, it is also the case that games are complex environments and affect social behaviour of players [13,14]. Therefore, the collaborative environment around the large display may also be affected by the game content, leading to a second research question.", "relwork": "In this section, we provide an overview of large display research, large display gaming, and gaming and territoriality.", "rq": "To explore our research questions, we designed a computer game based on the space invaders trope. We characterize our game as arcade-style with a single, shared task and view, i.e. a tightly coupled, single-view, common-goal gaming experience. The game was purposely designed to test the boundaries of personal space and encroachments (i.e., the game served as a technical probe)  meant to stress what we know about collaborative interaction behaviours to better understand collaboration and territoriality. Figure  captures participants playing the game. The game is further outlined in our supplementary work."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/231.xml", "intro": "As display prices fall, large vertical interactive displays become more feasible in many settings. Researchers have found them useful for visualization [4], intelligence analysis [2], and developer meetings [7], among others, and there is use in industry as well (e.g., via the MS Surface Hub). Research on wall displays includes a number of studies on wall interaction [3,4,6,19], as well as introducing new interaction concepts and modalities (e.g., [5,16,22,23,35]). Scientists have investigated different interaction styles, e.g. gestural [10], multimodal [22], and proxemic [5] interaction, as well as a number of interesting research aspects such as locomotion [1,20], territoriality [3], equality of participation and dominance [9], and collaborative coupling styles [18].Often, these studies use video analysis and coding (e.g., [6,18,19,30]), a task that remains very time-consuming in spite of coding tools such as VCode [14] and Chronoviz [11]. For example, Jakobsen and Hornbaek [19] manually coded a total of 24:27 hours of video, in addition to using custom analysis software and computer vision. While domain-specific analysis tools in the related field of tabletop collaboration have been developed (e.g., [27,33]), they are not designed to handle wall display-specific aspects. In particular (and in contrast to tabletop interaction), user movement is an integral part of wall interaction, and visualizing this movement in space and time is thus important for interaction research. We contribute the Group Interaction Analysis Toolkit GIAnT (Figure 1) that aims to fill this gap. Development was driven by our analysis of existing research in collaborative wall display interaction, which in many cases relies on a deep understanding of user positions and movement. From this analysis, we derived a list of specific research topics that would benefit from tool support (referenced as T1-T6 throughout the paper):\u2022 T1: Locomotion. How do users move in front of the large display? How is movement used to access information (e.g., [2,4,20])? Under which circumstances do users change positions or switch places [19,21]? How often and in which situations do users switch between working close to the wall and at overview distance [4]? \u2022 T2: Territoriality [3,30] and social proxemics [15]. At what social distances from each other do users interact? Is there evidence for territoriality? \u2022 T3: Equality of participation [21,27]. Are individual users being left out [27] or dominant (e.g., [6,9])? \u2022 T4: Coupling styles [18]. Are users collaborating closely or working side-by-side? \u2022 T5: Awareness [13,34] of collaborators' actions and of interface changes. Do the users have a clear mental picture of the state of the interface? Of collaborators' actions? \u2022 T6: User roles. Are there (perhaps application-specific) user roles (e.g., Villains, Micro-managers, Architects [12]; turn-taking or driver/audience [21])?Intelligent Visualization Systems CHI 2017, May 6-11, 2017, Denver, CO, USAGIAnT supports analytical studies at large wall displays by providing innovative and focused visualizations that show aspects of multi-user interaction relevant to the above topics.In contrast to video annotation systems that generally only allow analysis of one point in time at once, GIAnT additionally supports working at an overview level, with aggregated data. The toolkit focuses on visualizing time periods of interaction data, allowing users to see information about complete or partial interaction sessions at a glance and thus speeding up analysis significantly. Additionally, GIAnT supports flexible zooms into shorter time periods, supporting seamless transition to a detailed analysis for periods of time where this is needed (details on demand [31]). Further, researchers can switch to video analysis when appropriate as well.GIAnT is a standalone application and extensible on several levels: It supports adding new data sources, derived data sets, and visualizations. It includes a number of carefully designed visualizations that support work on a diverse set of research topics. We validated GIAnT by analyzing how it supports work on the research topics T1-T6 and by using it to evaluate a cooperative game. GIAnT is freely available under a GPL license 1 and downloadable via github 2 .", "relwork": "There are a number of related publications that focus on analysis tools for multi-device and multi-user interaction.VisTaCo [33] visualizes touches on a tabletop, using usercoded touches as main data source. CollAid [27] expands upon this by supporting user-specific audio using a microphone array. Marquardt et al.'s Excite [26] supports analysis of proxemics information in conjunction with video coding, in effect automating the video coding process to a degree through queries in a proxemics database. Further, VICPAM [28] supports analysis of interactions with multiple desktop computers, visualizing application-specific data such as window activations on a multi-user timeline. We expand upon these works by supporting interactive wall displays. Significantly, we visualize user movements in addition to interactions, allowing analysis of locomotion (T1), social proxemics (T2) and coupling styles (T4), as well as adapting analysis support to wall displays and corresponding user motions.Tool support for interaction research also includes a number of video annotation and analysis tools. Among them are Hagedorn et al.'s VCode and VData [14] as well as Burr et al.'s VACA [8], which both focus on traditional video coding, supporting multiple video streams and a timeline with events as well as generic sensor data. Hofmann et al. [17] focus on collaborative annotation in educational settings. Further, Lasecki et al. [24] crowdsource the video coding process, using paid remote workers to code in parallel and thus significantly reducing the time needed. All of these focus on the video coding process and none of them show visualizations that include time periods or aggregate movement and interaction data over time. One work that does show time period visualizations is Chronovis [11], which supports overlays on maps to visualize paths but focuses on airplane pilot interfaces.", "rq": "The research questions the statistics help answer depend on the values displayed: For example, average distance from the wall and number of touches may be indicators of equality of participation (T3); the average distance from the wall can also show whether the user is looking at an overview or concerned with details (T1)."}
{"path": "/Users/mengxiayu/Documents/Research/CaptionMining/QuestionGeneration/data/papers/multimodalHI/MHI/xml/219.xml", "intro": "In the past decade, interactive tabletops have been successfully used in multiple domains, including healthcare, education, entertainment, and cultural exhibitions [2,18,34,36]. These devices show significant advantages in supporting multi-user interaction, namely due to their large shared surface. Also, tabletop geometry has been shown to encourage equitable participation among users as well as simultaneous individual interactions with the digital content [32,33]. Previous research highlights several benefits of using interactive tabletops: improving collaborative learning [37,55], supporting reflection-type conversations [32], enhancing social interaction [18], fostering creativity as well as engagement [10,17]. However, people with visual impairments can struggle to engage with such large surfaces [23,39]. We argue that not being able to use interactive tabletops, for example, due to visual impairments, and consequently participate in these group activities, can be a vehicle of social exclusion.Despite being an inherently visual technology, touchscreen devices can be used by blind users. Screen readers, such as Apple's VoiceOver [3] or Android's Talkback [20], allow users to explore and control the device by providing audio feedback for touch actions. However, when considering the spatial awareness of interface elements, these accessibility services are mostly designed for smaller form-factors, rather than large collaborative surfaces where the ability to locate items and establish relationships between them is more challenging. Examples of applications for such devices include exploring maps [39] and anatomic models in educational settings, or mind maps in brainstorming sessions [53]. In these, the ability to locate artifacts without losing spatial awareness and to relate them is relevant. While there is a large body of work on touchscreen accessibility, research has been restricted to single-user interaction [15,21,23,39].This raises the question of how accessibility services can support nonvisual collaborative activities on large touchscreen devices. Particularly, how can we inform users about the actions of others? This notion of monitoring the activity of others, which provides context for one's activities is defined as workspace awareness [14,28]. Workspace awareness is a crucial aspect of collaboration [28] and often relies on visual display techniques [30]. However, little is known about using audio for tabletop multi-user interaction, particularly on how to provide workspace awareness and support blind people in joint activities.We present a study that contributes to the understanding of auditory design in enabling nonvisual collaboration in large touchscreen surfaces. We developed a nonvisual tabletop prototype, shown in action in Figure 1, with three feedback modes. We examined workspace awareness in colocated collaborative activities to answer: is auditory feedback effective in supporting nonvisual collaboration in large touchscreens? How does the amount of auditory information delivered influence patterns of awareness information exchange? How do blind users leverage audio feedback to engage with one another in co-located collaborative tasks? How is task performance in large touchscreens affected by the amount of auditory information displayed?The main contributions of the paper are: first, the design of multi-user auditory feedback for nonvisual tabletop collaboration; second, analysis of the effects of auditory feedback on blind users' workspace awareness and task performance when using interactive tabletops; third, we describe emergent nonverbal collaboration behaviors when blind users engage in joint activities; and finally, we propose a set of implications for the design of multi-user auditory displays.", "relwork": "The related work reviewed in this section is two-fold: first, we discuss research on touchscreen accessibility for visually impaired users, including mobile devices and large surfaces; second, we examine previous attempts to create collaborative systems that leverage audio output as a feedback modality.", "rq": "6.1 Answering the Research Questions 1. Is auditory feedback effective in supporting nonvisual collaboration in large touchscreens? Results showed that all participants completed all tasks within the time limit. Participants were able to engage in parallel collaborative activities, independently of feedback mode. Furthermore, the multi-user nonvisual interactive tabletop fostered collaborative behaviors, particularly in the taskdependent and public feedback modes, shown by the increase of information requests. This means that these feedback modes effectively promoted workspace awareness and enabled participants to move between individual and joint work."}
