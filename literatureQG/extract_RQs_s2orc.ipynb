{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# import fasttext\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models import ldaseqmodel\n",
    "# from gensim import corpora\n",
    "# import gensim.downloader as api\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    \n",
    "    # stemming and lem\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': 'II. LITERATURE REVIEW', 'text': 'Moving over to the FinTech industry, [4] uses CNNs to make predictions for stock price changes based on the image of the time series plot. The author also attempts to colour code the time series, however the results of this approach were not positive. On the other hand, [5] use CNNs in a bank telemarketing case study, whereby the aim is to predict whether a customer will take up a particular marketing campaign based on a number of numeric and nominal features per customer. The results for this study yield an impressive 76.70% accuracy, which yields the highest accuracy amongst 7 classifiers. In order to incorporate external features in the forecasting model, [6] use a deep convolutional neural network to model short and long term influences of events of stock price movements. Results from this study show that CNNs can capture longer-term influence of news events than standard feed-forward networks.', 'cite_spans': [{'start': 37, 'end': 40, 'text': '[4]', 'ref_id': 'BIBREF3'}, {'start': 271, 'end': 274, 'text': '[5]', 'ref_id': 'BIBREF4'}, {'start': 667, 'end': 670, 'text': '[6]', 'ref_id': 'BIBREF5'}]}\n"
     ]
    }
   ],
   "source": [
    "path = '../data/s2orc_hci/s2orc_hci/pdf_parses/pdf_parses_0.jsonl'\n",
    "\n",
    "# load jsonl file\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "\n",
    "# for dSec in data[1]['body_text']:\n",
    "#     text = dSec['section'].lower()\n",
    "#     regexp = re.compile(r'introduction')\n",
    "#     if regexp.search(text):\n",
    "#         print(dSec)\n",
    "\n",
    "# for dSec in data[1]['body_text']:\n",
    "#     text = dSec['section'].lower()\n",
    "#     regexp = re.compile(r'related work')\n",
    "#     if regexp.search(text):\n",
    "#         print(dSec)\n",
    "\n",
    "\n",
    "for dSec in data[3]['body_text']:\n",
    "    text = str(dSec).lower()\n",
    "    print(dSec)\n",
    "    regexp = re.compile(r'RQ.*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        print(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = dSec['section'].lower()\n",
    "        regexp = re.compile(r'intro')\n",
    "        if regexp.search(text):\n",
    "            texts.append(dSec['text'])\n",
    "    return texts\n",
    "\n",
    "def get_relatedWork_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = dSec['section'].lower()\n",
    "        regexp = re.compile(r'related work')\n",
    "        if regexp.search(text):\n",
    "            texts.append(dSec['text'])\n",
    "    return texts\n",
    "\n",
    "def get_RQ_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = str(dSec).lower()\n",
    "        # non greedy match\n",
    "        regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "        # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "        matches = regexp.findall(text)\n",
    "        if matches:\n",
    "            texts.extend(matches)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"1. How do students assess the contribution of the flipped-classroom approach to the learning process and the watching of videos between classes as against the watching of videos in class? \n",
    "# 2. What are the relations between the assessment of the contribution of the flippedclassroom approach to the learning process and the students' background characteristics, feelings about having the lecturer and classmates nearby, and self-assessment of the learning ability?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    path = '../data/s2orc_hci/s2orc_hci/pdf_parses/pdf_parses_%d.jsonl'%i\n",
    "\n",
    "    # load jsonl file\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "\n",
    "\n",
    "    for dPaper in data:\n",
    "        rqs.append(get_RQ_text(dPaper))\n",
    "        intros.append(get_introduction_text(dPaper))\n",
    "        relatedWorks.append(get_relatedWork_text(dPaper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42260"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([rq for rq in rqs if rq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to jsonl file\n",
    "# with open('allQuestions_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "with open('RQs_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "    for idx, rq in enumerate(rqs):\n",
    "        if rq:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    'intro': intros[idx],\n",
    "                    'relatedWork': relatedWorks[idx],\n",
    "                    'rq': rq\n",
    "                }\n",
    "            ))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "# check how many papers have intro, related work and RQs\n",
    "with open('RQs_s2orc.jsonl', 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "    cnt = 0\n",
    "    for d in data:\n",
    "        if (d['intro'] or d['relatedWork']) and d['rq']:\n",
    "            cnt += 1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "# check how many papers have intro, related work and RQs\n",
    "with open('allQuestions_s2orc.jsonl', 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "    cnt = 0\n",
    "    for d in data:\n",
    "        if (d['intro'] and d['relatedWork']) and d['rq']:\n",
    "            cnt += 1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# only extract related works\n",
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    path = '../data/s2orc_hci/s2orc_hci/pdf_parses/pdf_parses_%d.jsonl'%i\n",
    "\n",
    "    # load jsonl file\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "\n",
    "\n",
    "    for dPaper in data:\n",
    "        rqs.append(get_RQ_text(dPaper))\n",
    "        intros.append(get_introduction_text(dPaper))\n",
    "        relatedWorks.append(get_relatedWork_text(dPaper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7345"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([r for r in relatedWorks if r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('NFT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c377a027f0bbb6c0821b68e5c065774f74ef070985e76133b9b1c3a883a152de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
