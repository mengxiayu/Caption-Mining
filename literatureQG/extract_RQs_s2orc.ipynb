{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# import fasttext\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models import ldaseqmodel\n",
    "# from gensim import corpora\n",
    "# import gensim.downloader as api\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    \n",
    "    # stemming and lem\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': 'INTRODUCTION AND BACKGROUND', 'text': 'Our focus is on these live and distributed lectures. In particular, we know that it can be quite problematic for the presenter to be aware of and naturally interact with remote students who are viewing the lecture as a webcast [9] . When the number of participants is small, this can sometimes be overcome using videoconferencing. As the class grows, however, it becomes more difficult [15] . Problems arise because screen space for visible representations of remote participants is limited, and humans have limited capacity for processing such representations [11] . Thus, the rich information that instructors ordinarily use to find appropriate interaction opportunities, to find good \"target\" students, and to time these interactions is missing. Clearly it is not possible to replicate all of the real-world cues in a distributed environment. This raises the question of whether there are critical bits of awareness information that would be useful for promoting interaction in these environments.', 'cite_spans': [{'start': 227, 'end': 230, 'text': '[9]', 'ref_id': 'BIBREF8'}, {'start': 386, 'end': 390, 'text': '[15]', 'ref_id': 'BIBREF14'}, {'start': 561, 'end': 565, 'text': '[11]', 'ref_id': 'BIBREF10'}]}\n"
     ]
    }
   ],
   "source": [
    "path = './s2orc_hci/pdf_parses/pdf_parses_0.jsonl'\n",
    "\n",
    "# load jsonl file\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "\n",
    "# for dSec in data[1]['body_text']:\n",
    "#     text = dSec['section'].lower()\n",
    "#     regexp = re.compile(r'introduction')\n",
    "#     if regexp.search(text):\n",
    "#         print(dSec)\n",
    "\n",
    "# for dSec in data[1]['body_text']:\n",
    "#     text = dSec['section'].lower()\n",
    "#     regexp = re.compile(r'related work')\n",
    "#     if regexp.search(text):\n",
    "#         print(dSec)\n",
    "\n",
    "\n",
    "for dSec in data[3]['body_text']:\n",
    "    text = str(dSec).lower()\n",
    "    print(dSec)\n",
    "    regexp = re.compile(r'RQ.*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        print(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = dSec['section'].lower()\n",
    "        regexp = re.compile(r'intro')\n",
    "        if regexp.search(text):\n",
    "            texts.append(dSec['text'])\n",
    "    return texts\n",
    "\n",
    "def get_relatedWork_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = dSec['section'].lower()\n",
    "        regexp = re.compile(r'related work｜relatedwork')\n",
    "        if regexp.search(text):\n",
    "            texts.append(dSec['text'])\n",
    "    return texts\n",
    "\n",
    "def get_RQ_text(dPaper):\n",
    "    texts = []\n",
    "    for dSec in dPaper['body_text']:\n",
    "        text = dSec[\"section\"] + dSec[\"text\"]\n",
    "        # non greedy match\n",
    "        # regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "        regexp = re.compile(r'RQ1|RQ2|RQ3|\\(RQ\\)|research question')\n",
    "        # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "        matches = regexp.findall(text)\n",
    "        if matches:\n",
    "            texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"1. How do students assess the contribution of the flipped-classroom approach to the learning process and the watching of videos between classes as against the watching of videos in class? \n",
    "# 2. What are the relations between the assessment of the contribution of the flippedclassroom approach to the learning process and the students' background characteristics, feelings about having the lecturer and classmates nearby, and self-assessment of the learning ability?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RQ1']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp = re.compile(r'RQ1|\\(RQ\\)|research question')\n",
    "text = str({\"section\": \"RQ1. Can DDL help EFL learners improve their lexico-grammatical use of abstract nouns in their writing?\", \"text\": \"The control group and the experimental group had similar error-free ratios in terms of the use of the ten target nouns in the pre-test, but in the post-test, the experimental group had much fewer errors in their texts. Also, even though the experimental group used various patterns in the post test, the patterns used by the control group were limited. For instance, excitement was used in a greater variety of grammatical patterns by the experimental group than by the control group. The experimental group used five grammatical patterns of excitement: 1) PP + excitement; 2) V + excitement; 3) excitement + V; 4) excitement + copular verb BE and 5) copular verb BE + excitement. However, the control group used the noun in only three patterns: 1) excitement + copular verb BE; 2) PP + excitement; 3) V + excitement. Moreover, while the control group preferred using simple patterns in the post test, the experimental group used more complex patterns. For example, the noun silence was used with various adjectives by the experimental group (sudden silence, awkward silence, strange silence, uncomfortable silence, breathless silence, unknown silence). That is, the results indicated that to a large extent, the experimental group outperformed the control group in both areas of syntactic variations and correct grammar use because of the use of DDL activities as in the studies of Huang (2014) , , Ucar and Yükselir (2015) , Vyatkina (2016) , and Yunus and Awab (2014).\", \"cite_spans\": [{\"start\": 1383, \"end\": 1395, \"text\": \"Huang (2014)\", \"ref_id\": \"BIBREF5\"}, {\"start\": 1400, \"end\": 1424, \"text\": \"Ucar and Yükselir (2015)\", \"ref_id\": \"BIBREF17\"}, {\"start\": 1427, \"end\": 1442, \"text\": \"Vyatkina (2016)\", \"ref_id\": \"BIBREF19\"}]})\n",
    "matches = regexp.findall(text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.75it/s]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    path = './s2orc_hci/pdf_parses/pdf_parses_%d.jsonl'%i\n",
    "\n",
    "    # load jsonl file\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "\n",
    "\n",
    "    for dPaper in data:\n",
    "        rqs.append(get_RQ_text(dPaper))\n",
    "        intros.append(get_introduction_text(dPaper))\n",
    "        relatedWorks.append(get_relatedWork_text(dPaper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27955"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4034"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([rq for rq in relatedWorks if rq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"IntroductionEmpirical methods are critical to gauge the scalability and robustness of proposed approaches, to assess progress and to stimulate new research questions. In the field of natural language generation, empirical evaluation has only recently become a top research priority (Dale, Eugenio et al. 1998) . Some empirical work has been done to evaluate models for generating descriptions of objects and processes from a knowledge base (Lester and Porter March 1997) , text summaries of quantitative data (Robin and McKeown 1996) , descriptions of plans (Young to appear) and concise causal arguments (McConachy, Korb et al. 1998) . However, little attention has been paid to the evaluation of systems generating evaluative arguments, communicative acts that attempt to affect the addressee's attitudes (i.e. evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor). The ability to generate evaluative arguments is critical in an increasing number of online systems that serve as personal assistants, advisors, or shopping assistants 1 . For instance, a shopping assistant may need to compare two similar products and argue why its current user should like one more than the other.\"],\n",
       " ['RQ1. Can DDL help EFL learners improve their lexico-grammatical use of abstract nouns in their writing?The control group and the experimental group had similar error-free ratios in terms of the use of the ten target nouns in the pre-test, but in the post-test, the experimental group had much fewer errors in their texts. Also, even though the experimental group used various patterns in the post test, the patterns used by the control group were limited. For instance, excitement was used in a greater variety of grammatical patterns by the experimental group than by the control group. The experimental group used five grammatical patterns of excitement: 1) PP + excitement; 2) V + excitement; 3) excitement + V; 4) excitement + copular verb BE and 5) copular verb BE + excitement. However, the control group used the noun in only three patterns: 1) excitement + copular verb BE; 2) PP + excitement; 3) V + excitement. Moreover, while the control group preferred using simple patterns in the post test, the experimental group used more complex patterns. For example, the noun silence was used with various adjectives by the experimental group (sudden silence, awkward silence, strange silence, uncomfortable silence, breathless silence, unknown silence). That is, the results indicated that to a large extent, the experimental group outperformed the control group in both areas of syntactic variations and correct grammar use because of the use of DDL activities as in the studies of Huang (2014) , , Ucar and Yükselir (2015) , Vyatkina (2016) , and Yunus and Awab (2014).',\n",
       "  'RQ2: What are the perceptions of EFL learners of the effect of DDL on their vocabulary learning and vocabulary use in writing?The results obtained from the questionnaire on the effectiveness of DDL confirmed the results mentioned above and showed that the participants were positive about the use of DDL activities in writing because they believed that DDL activities contributed to their vocabulary learning and use in writing. They thought DDL activities helped them learn the meaning of words, collocations, and usage of words. Thus, the results were mostly similar to the results of the previous studies on learner perceptions of DDL (e.g. Charles, 2012; Gaskell & Cobb, 2004; Huang, 2014; O\"Sullivan & Chambers, 2006; Sun, 2007; Yoon & Hirvela, 2004) . Similar to the study of Yoon and Hirvela (2004) the students found the corpus approach beneficial to the development of writing skill. Also, the results were in line with Vyatkina\"s study\"s results (2016) which showed that learners were willing to use DDL for independent learning in the future. Also, as in Sun\"s study (2007) , the students had very positive opinions on the DDL activities for writing skill. However, although in Huang\"s study the participants thought DDL activities were useful for learning grammatical patterns of the words, the participants in this study were not sure about their usefulness in learning grammatical patterns. As in Liu and Jiang\"s study (2009), corpus-based lexico-grammatical analysis caused some difficulties for some students. In addition, contrary to Huang\"s study (2014), students did not believe that studying concordances was time-consuming. Furthermore, they did not have problems with unfamiliar vocabulary while they were doing concordance activities unlike the participants in his study.'],\n",
       " ['importance of obtaining human perspectives on fairness in algorithm development [19] . While there are multiple steps in the development of an algorithm which can influence the fairness of the outcome, previous work has highlighted that the perceived fairness of predictors themselves can be utilised to inform fair algorithmic decision making [21] . However, developing a scalable and practical feature selection process capable of obtaining the opinion of a diverse population sample remains an open research question [20, 55, 60] . As a first step in overcoming this challenge, here we explore the perceptions of crowdworkers with regards to fair predictors in a recidivism prediction model.'],\n",
       " ['DiscussionOur modeling study was intended to explore design-related issues and predict results in human eye-tracking experiments that we plan to run. In human experiments, participants need to learn the grammar hidden in a sequence of symbols. To make learning easier, we chose a simple grammar which made it hard to interpret the effect of entropy; it could be the effect of entropy or the effect of entropy reduction. However, the proposed model is general enough to cover more complex grammars and diverse situations (e.g., selfpaced reading). We chose the Hidden Markov Model and the A2C architecture for the perception the decision making modules mainly for modeling convenience. The HMM can be replaced with a more elaborated neural language model when dealing with more complex grammars. The emphasis should be given to our architectual choice. The addition of the decision making module that has the ability to develop a policy on its own provides the system to control the amount of uncertainty flexibly in response to the task situations. Bicknell and Levy (2010) took the same approach similar to explain reading eye movement patterns, which influenced our work. Our work is different from theirs in that (1) we considered noisy memory more directly and (2) we used reinforcement learning to let the model discover a good decision policy; we believe both additions can lead us to interesting research questions.'],\n",
       " ['A. Website Satisfaction and TrustMoreover, there does not seem to exist a clear consensus among scholars about the nature of the relationship between satisfaction and trust. Some authors [43] , [44] consider that satisfaction is a determinant of trust. Their tests in the context of online business showed that previous positive shopping experiences result in high customer trust. However, other authors [45] , [46] reported just the opposite: trust influences satisfaction. For them, the strong image that customers have about a company helps them to perceive a high level of satisfaction. However, several other relevant demographic studies [4] , [7] , [15] , [17] , [47] , [48] represent both satisfaction and trust as unrelated variables in their research models. These research efforts are closely focused on the study of cultural differences, and they consider the impact of different design approaches on trust and satisfaction and, in turn, evaluate the relationship of these variables to online loyalty. As our research questions are closer to these studies, we decided to exclude the relationship between satisfaction and trust from our model. However, the consideration of the relationship between satisfaction and trust in the context of different national cultures is an interesting avenue for future research. Fig. 1 presents the research model guiding this investigation. The proposed research model was developed based on conceptual and theoretical studies in the domain of e-commerce. The model theorizes that web design attributes positively influence the user trust and satisfaction in a high-UA culture. In terms of website design, five design attributes/features suggested by research community (i.e., [4] , [11] , [22] , [49] , [50] ) include the following.'],\n",
       " ['DISCUSSIONRegarding RQ1, that pertains to determining if there is a difference in arousal between both groups in browsing tasks, we hypothesised that the ASD group would experience more arousal compared to the neurotypical users. However, this was not the case. This could be because an increase in arousal could be caused by diverse reasons. Moreover, a more direct approach would be to test each UI element based on this hypothesis. This was not however possible due to the small and uneven sample size making statistical group comparisons inappropriate. With regards to RQ2, about identifying differences in AOIs between groups, we were able to observe differences in visual and physiological patterns between both groups in our descriptive approach. Take for example our observation that the individuals with autism showed less arousal compared to the neurotypical group for several UI elements on the YouTube page. One element shows people in a happy state, while the other contains the thumbnail for a video regarding the death of physicist, Stephen Hawking. A UX researcher may relate this to symptoms where people with autism interpret affective expressions differently from neurotypical people [33] . In research by Baron-Cohen et al., they observed that individuals with autism do not recognise bodily expressions of affect as well as the neurotypical populace [4] . Therefore, an implication could be that, if a UI element contains a link that is a functionally significant aspect of the website, they must present it in a manner that does not rely primarily on facial expressions or affective cues. This is one such usability issue that can be identified using this methodology. Another behaviour that our methodology can help to uncover is that of understanding the affective states of users before leaving a web page. It may be the case that the initial UI elements that the users engage with eliciting higher arousal than the final ones, as is the case with the BBC and YouTube web page, or that participants experience an increase in arousal in the middle of their interaction compared to the beginning and end, as the case with the Amazon website or they experience lower arousal levels at the final UI elements as with the Adobe web page. Arousal could indicate positive states such as attraction, neutral ones (i.e. cognitive load), or negative affective states like frustration and stress. When participants experience an increase in arousal towards the end of an interaction, this could imply that they found what they were looking for, i.e., excitement in completing a task/goal, or that they were frustrated. Both of these cases could benefit from optimising the user interface. In the first instance, if users take a long time to find the item of interest on a page, it means that the user experience may be improved by re-positioning the element to a more visually accessible location, or by using a more attractive design to draw the attention of users towards that particular content. When users experience frustration with a UI element before leaving a web page, it could be an indication that the UI element has a usability problem. This type of diagnosis is mainly possible because we combined users visual scan path with a measure of their arousal levels. The implications for design include aggregation of different user groups of users, and potential modelling of their behaviour. For example, on an e-learning website, people with autism can have a different profile that takes account of and adapts to their unique traits and requirements. Eye-tracking is becoming more accessible, and we anticipate that webcameras and mobile phone cameras will one day have eye-tracking capabilities. Our methodology could then be used within social media and mobile applications. Posts and feeds can be treated as atomic UI elements so that the characteristics of different posts (sentiment, object classification, colour etc.) can be investigated against the visual scan sequence and the corresponding affective states that are elicited. Our work is not without limitations. Due to the limited accuracy of eye-tracking technology, our analysis has been based on group behaviour as opposed to individual behaviour. Another limitation is that our methodology can currently only be used in laboratory settings. Ambient light, inter-colour differences between (and within) stimuli and other environmental variables may introduce confounding factors which may yield different results in the wild. Therefore, our methodology needs to be optimised to handle these factors dynamically in naturalistic settings.'],\n",
       " ['Preliminary experiments can require thousands of iterations of training or more to address each research question explored, and human time and attention are limited. However, it remains to be determined whether rewards with the sparse, delayed properties similar to those generated by humans will be feasible to train RL controllers for continuous-time, continuous-state systems of a certain complexity. For these reasons, we have elected to create algorithms that generate rewards similar to those that a human would be likely to give; we refer to these computergenerated rewards as \"pseudo-human\" rewards. These rewards are generated in response to the current arm reaching movement properties of the system, and they are used as inputs to the RL controller. In this study, we will examine whether these [24] and [25] .'],\n",
       " ['Elements from Participatory Research(1) PR demands democracy and different levels of participation. We put great emphasis on collaborating with the YFMs democratically. We considered them as co-researchers who equally contribute to diverse aspects of the project, e.g., research questions, objectives, and outcomes. However, the co-researchers were not expected to participate fully in every stage of the research, i.e., in the sense of levels 6-8 defined in Wright et al.\\'s \"stage model of participation\" [94] (Figure 1 ). Based on our observations of the participants\\' behavior during the activities, we considered it to be neither sensible nor ethical to ask recently fled and possible traumatized young population at school age to increase their workload beyond their regular day-to-day activities. To fully involve them in data analysis, they would have needed to spent considerable time on studying and applying various analysis methods. As von Unger [90] stresses, the level of participation in every stage has to be adapted to the circumstances. Thus, high levels of participation should not be pursued at any cost. Nevertheless, YFMs had the power to make high-impact decisions in most of the other stages and contributed to those with high levels of participation (Wright et al.\\'s model-levels 7 and 8).'],\n",
       " [\"Affective Congruency of VC Facial ExpressionsGiven the evidence that behavioral mimicry creates rapport (e.g., [77] ), our secondary research question asked whether the congruency between VC (pre-decision) emotional expression and the participant's emotional state (as inferred on the basis of facial EMG activity and EDA) influences human cooperation and physiological responses. However, we found no differences between the congruent and incongruent expression conditions. It is of note that the present study provides only a weak test for the effect of affective congruency. Although pre-tests suggested that VC facial expressions were dictated by human emotional/physiological responses as intended, this was not necessarily completely the case for all participants. Also, the present study didn't take into account that the effects of emotion expression congruency on cooperation may vary by the type of emotion (e.g., happy versus angry expression). It may also be that the use of facial EMG activity (emotional expressions) alone would have been better than a combination of EMG and EDA in determining the VC facial expressions.\"],\n",
       " ['Bracket representationBoundary var. 4 ((barbie dress)( up games)) 0 1 0 3 (barbie ((dress up) games)) 2 0 1 2 (barbie (dress (up games))) 2 1 0 1 ((barbie (dress up)) games) 1 0 2 ity. For instance, in the case of NL chunking, it is not clear whether the chunk boundaries should correspond to the innermost parentheses in the nested segmentation marking very short chunks, or should one annotate the larger chunks corresponding to clausal boundaries. For this reason, Inter-Annotator Agreement (IAA) for flat annotation tasks is often poor (Bali et al., 2009; Hagen et al., 2011; Saha Roy et al., 2012) . However, low IAA does not necessarily imply low quality annotation, and could as well be due to the inherent ambiguity in the task definition with respect to granularity. Although we have illustrated the concept and problems of flat and nested annotations using the examples of sentence and query segmentation, these issues are generic and typical of any flat annotation scheme which tries to flatten or approximate an underlying hierarchical structure. There are three important research questions pertaining to the linguistic annotations of this kind:'],\n",
       " ['BACKGROUND AND MOTIVATIONRowlands noted in 2007 (Rowlands, Nicholas et al. 2007 ) that the process of selecting a useful book from the available options is a surprisingly under-studied part of the book selection process for all books, not just ebooks. This statement remains largely true, with some exceptions: Reutzel and Gali (Reutzel et al. 1998 ) studied children selecting fiction books in a physical library, and noticed they were influenced by shelf position and cover, rather than content. Moore\\'s work (Moore 1995) demonstrated the influence of shelf position on children\\'s selection practices in fiction books, and Borgman (Borgman et al. 1995) demonstrated the same bias in digital libraries. Among adults, cover clearly does have an influence in both bookshops (Buchanan and McKay 2011) and academic libraries (Stelmaszewska et al. 2004; Hinze, McKay et al. 2012) . It is not the only influence, however, Stieve showed in 2006 (Stieve et al. 2006 ) that when choosing between two similar books university students relied heavily on the table of contents; a behaviour that was also demonstrated \"in the wild\" in our own earlier work on both physical and digital academic libraries McKay, Hinze et al. 2012) . Finally, both Stelmaszewska (Stelmaszewska and Blandford 2004 ) and our own earlier work McKay, Hinze et al. 2012) show that book content has an impact on decision making in both physical and digital book libraries. . A striking aspect of this decision process is how quickly many selections are made, though how users assess content rapidly remains an open research question. It seems, then, that in rapid decision making in print book selection cover image and table of contents play a significant role, however how these artefacts affect the decision making process remains unclear.'],\n",
       " ['A. RQ1: Strengths, Weaknesses, Opportunities, and Threats2) Weaknesses: Shifting complexity: One problem often found by changing the software architecture to microservices according to different experience reports is that complexity shifts from module-level to a higher level [1] . The challenge of creating, controlling, and monitoring the system is increasing. The larger the number of individual services active in a system, the more complicated it is to keep track. In addition, the test complexity shifts, since services are smaller and contain only a sub-task which leads to simpler module tests. In addition, comparing the output with the same input data makes it easier to test different versions of a module. On the other hand, however, it is much more challenging to test the interaction between dozens of services at the integration or system test level. In summary, the complexity is shifting from the individual module complexity to a higher level, where architectural design, interaction between services, and monitoring takes place [8] .',\n",
       "  'A. RQ1: Strengths, Weaknesses, Opportunities, and ThreatsReduced dependency errors In most vehicle software architectures, such as component-based architectures, there is a multitude of dependencies between functions and subfunctions. This is however not only in contrast to the intention to design internal vehicle systems as modular as possible, but is also very error-prone, since changes to a sub-function can have effects on other elements. Vogelsang and Fuhrmann discussed this problem [9] , showing that the high degree of dependency leads to that a function developer is only aware of about 50% of the dependencies. While this is also a problem for microservice architectures, there are already available solutions developed and applied even at the scale of, for example, Netflix and Spotify.'],\n",
       " [\"Extensions to Other Domains of Self-TrackingThis paper has primarily explored and discussed design for people who have lapsed in tracking physical activity using a Fitbit. Our findings can likely be adapted in other domains in which people track their behaviors, sometimes surfacing new design challenges and research questions to consider. For example, many people self-track personal finances, with primarily manual tools (e.g., Quicken) or more automatic tools (e.g., Mint). Although many people are able to consistently track their finances for years, others abandon the practice in weeks or months [13] . Designs can adopt some of the principles we have examined with step count data. A person who tracked their finances consistently for a long time may prefer a holistic view of how their finances changed over time (e.g., Figure 7a 's view of how much money they spent or saved by month). This may avoid presenting obvious habits and better motivate a person toward a savings goal. However, this representation would be barren for a person who tracked for only a few weeks. Informed by representations explored here, one alternative would be day-by-day spending relative to a discretionary budget (Figure 7b ). Another might be a table divided by spending category (Figure 7c ). Both of these aim to provide meaningful feedback based on a relatively short period of tracking.\"],\n",
       " ['Summary and Research QuestionsResearch has developed a detailed understanding of selftracking in different domains by individuals who use both self-tracking technologies and paper notebooks [3, 20, 23, 45] . Recently, there has been a push to semiautomated [10] and flexible self-tracking [36] to overcome the limitations of current self-tracking technologies. However, there is a lack of dedicated research on how individuals use and especially design paper notebooks to engage in self-tracking [20] , and what this understanding might imply for the design of flexible self-tracking systems. In this study, we aim to inform the design of self-tracking tools through an analysis of paper bullet journal photographs and related conversation on Instagram, examining the following research questions: (1) how do bullet journalists design their paper notebooks; (2) how do bullet journalists use their notebooks to engage with others online; and (3) what design implications can be derived for future self-tracking technologies? Figure 1 : Example of a daily log according to [7] .'],\n",
       " ['RQ1. What is the Impact of the Validation Verdict?How important is a positive validation outcome for newcomer engagement? Can an early rejection discourage further participation? The theory suggests that when intrinsic motivation is present, positive task feedback can foster repeat participation [9, 17] . However such feedback should be performance-contingent, not merely task-contingent: positive feedback is only effective if it is linked to contribution performance [6, 16] . In contrast, negative feedback can reduce motivation when initial self-efficacy is low [5, 24] . Are these effects observable for newcomers in HOT?'],\n",
       " [\"INTRODUCTIONIn pervasive computing systems, there is often a need to provide users with a way to access and search through ubiquitous information associated with real world objects and locations. Technology such as Augmented Reality (AR) allows virtual information to be overlaid on the users' environment [1] , and can be used as a way to view contextual information. However, there are interesting research questions that need to be addressed: how to know when to present information to the user, how to decide what to present given the plenitude of information, and what is the best way for users to interact with the information. As pointed out in [2] , pervasive computing applications need to place few demands on the user's attention and be sensitive to context.\"],\n",
       " [\"This review indicates that the integration of loanwords into Japanese is only accomplished by their significant modification, modification that some researchers (e.g. Otake 2008) have argued limit their usefulness in language acquisition and production. However, it is also noted that gairaigo is an important element in the lexicon of present-day Japanese, with a considerable number of loanword types utilized in fields as diverse as advertising, politics, economics, entertainment and leisure. Given their ubiquity, gairaigo may potentially have a valuable role in L2 acquisition. Daulton (e.g. 2007) consistently argues that gairaigo offer a means by which learners can access, acquire, and use loanword cognate items, the L2 words from which they are derived. A corpus analysis of frequencies of gairaigo usage in L2 written texts may indicate the veracity of Daulton's claims -and the importance of loanwords for Japanese learners of English. Based partially on Daulton's (2007) methodology, we further the investigation by using a slightly larger longitudinal learner corpus as well as a NS corpus to identify norms and answer the following research questions (NNS, or non-native speakers, indicating Japanese learners of English):\"],\n",
       " [\"Part 1 -Online Opinion Survey.The results of our opinion survey (N = 178) [7] have provided the answer to RQ1. The feedback provided by respondents, presented in Figure 1 , indicates that the majority of people want their interaction with voice search system to be more human like. However, the opinions are divided when it comes to system's conversational initiative -with less than 50% of respondents who agreed that voice search system should ask more questions. In the answers provided to open 1 Values for performance have been inverted for comparability reasons questions, many respondents expressed need to for conversational system to have memory of their past interactions, and to ask follow up questions in order to clarify their intent. The insights obtained from the survey informed the design and the scope of 'Voice Interaction Studies' used in Part2 of the project. \"],\n",
       " ['Experiment II: RQ2: Extending to Corpora without Joint InstancesAs mentioned in Section 2, there is an overlap between the two collections in KB, so that there are books which are dually annotated, by both GTT and Brinkman concepts. This allows us to apply methods based on co-occurrence to find mappings [7] . However, it is not always the case that two thesauri have joint instances. In this section we evaluate whether our approach can be applied to the case where there are no joint instances, i.e., where there are no doubly annotated instances.'],\n",
       " ['INTRODUCTIONHowever, most work on gesture passwords so far has been carried out in laboratories [51, 48, 16, 26] , leaving their performance in the wild as an open research question. Field studies are important for understanding the user-chosen distribution of gesture passwords in realistic settings and how usable and memorable those could be.'],\n",
       " [\"I. INTRODUCTIONSoftware architecture is not only the final product of the design efforts, but more and more is including the decision history that brought to the system as it is finally specified [1] . One of the intended goals underlying this trend is, together with designing the system architecture, preserving domainexperts' knowledge/reasoning, which is implicitly exploited to reach the design solution and would be lost if not recorded appropriately. However, in software architecture knowledge management is a challenge [2] , and recording the information about a certain decision making process is an open research question: on the one hand, the stored decision information shall be detailed enough to turn out as useful for future decision making scenarios, even in cross-application and crossdomain situations; on the other hand, the efforts for recording first, and for retrieving/analysing/maintaining past decision data later, shall be reduced to the minimum [3] .\"],\n",
       " ['Limitations of Existing Approaches• Limitations of Cell-Oriented Approach: Another approach is to package each cell of the database table into an ORAM block. This approach increases the size of position map, which is an imperative component stored at the client in tree-based ORAMs. To eliminate the position map, oblivious 2D-grid structure (referred to as ODS-2D) [23] can be used to store the database table by clustering each O(log(N )) cells into an ORAM block and using the pointer trick to link the blocks together. However, this approach may increase the number of requests when the query requires fetching an entire row or column. This incurs end-to-end delay due to a large number of round-trip delays, and therefore, is not suitable for large databases. Cell-oriented packaging and its limitations are summarized in Figure 2 -(b). The above discussion indicates that there is a significant need for an efficient oblivious data structure that permits diverse types of queries on encrypted databases. Hence, in this paper, we seek answers to the following research questions:'],\n",
       " ['RQ1: Do PWs Generate Educationally Relevant Discussions?The educational benefits of PWs rest in their ability to stimulate discussions in which students\\' user interface designs are used as a basis for exploring the design principles and concepts taught in the course. One indication of whether PWs succeeded in this regard can be found in the results of the content analysis: On average, 31% of all PW talk was dedicated to DESIGN TALK. Given that DESIGN TALK was rooted in the user interfaces under review, we see that USER INTERFACE TALK, which constituted 26% of overall talk on average, necessarily served a key complementary role in such discussions. Taken together, DESIGN TALK and USER INTERFACE TALK composed a majority (53%) of all talk-a strong preliminary indication that PW discussions were educationally relevant. Delving deeper into the results, we find that 11% of DESIGN TALK (3% of all talk) actually enlisted the design concepts and principles explored in the course. Given that such concepts and principles were a key emphasis of the course within which the PWs were situated, one might be concerned that such a small percentage of DESIGN TALK 26:28 C. D. Hundhausen et al. was actually dedicated to them. However, the PW thrusts participants into a situation of design practice, where decision making is not based chiefly on research-based theory [Schön 1990 ]. Indeed, as Schön [1990] , aptly notes, design practitioners engage in a reflective conversation with their design materials, drawing extensively on their personal \"repertoire[s] of themes and examples\" (pp. 78-79) to make progress. That the design discussions we observed in the PWs contained a mix of theory, practical considerations, and common sense may not only reflect the realities of design practice, but also the authenticity of the design situations considered in the PWs.',\n",
       "  'RQ2: To What Degree Do Students Participate?The results indicate that the extent to which participants contributed DESIGN TALK varied by speaker type. The instructor contributed the most DESIGN TALK of any other speaker type (44%), followed by student design team members (26%), the test user (17%), and audience members (13%). We believe that this finding, when viewed through the lens of situated learning theory [Lave and Wenger 1991] , indicates that the PW facilitated opportunities for students to participate in increasingly central ways in design discussions as they took on different roles. At the periphery of the PW activity were student audience members, who contributed the least to the discussions. In this role, students mainly observed the activity. However, when they did contribute, their contributions were most likely to be on the topics that were most relevant to the course: DESIGN TALK and USER INTERFACE TALK. We speculate that, in their roles as somewhat detached observers, audience members were in a good position to focus and reflect on user interface and design issues, without being distracted by the procedural details of the activity.'],\n",
       " [\"Collaborative Human-Robot RelationshipsResearchers developed collaborative robots for H-R teams and have improved the quality of a robot's ability to collaborate as people do (Briggs & Scheutz, 2011; Chernova & Breazeal, 2010; St. Clair & Matarić, 2011 ). These developments have improved H-R collaboration but do not offer insights into the impact of collaboration on the human's performance or internal state. Hinds, Roberts, and Jones (2004) assessed human feelings of personal responsibility, blame, and credit in H-R collaboration. The authors teamed participants with either a human, a humanlike robot, or a machine-like robot in a subordinate, peer, or supervisor role. Results showed that the participants relied on human teammates more than robot teammates. Participants also relied more on robot peers than supervisors or subordinates. The difference in task performance or workload levels between the H-H and H-R teams, given the different relationship types, were not investigated; however, this research question is addressed in the presented investigation.\"],\n",
       " ['RQ1 Results: What information do shoutcasters seek to generate explanations, and where do they find it?We used two frameworks to investigate casters\\' information seeking behaviors. We turned to the Performance, Environ- ment, Actuators, Sensors (PEAS) model [31] to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory (IFT) to understand where casters did their information seeking, beginning with the places their desired information could be found. These places are called information \"patches\" in IFT terminology. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets, resources, successes, and failures, e.g., Figure 1 region 4 (showing that Blue has killed 9 of Red\\'s workers) and region 5 (showing that Blue has killed 19 units to Red\\'s 3, etc.). Table 2 shows that casters rarely consulted performance measures, especially those that examined past game states. However, they discussed basic performance measures available in the HUD (Figure 1 region 1) , which contained present state information, e.g., resources held or upgrade status.',\n",
       "  \"RQ2 Results: The How: How do shoutcasters seek the information they seek?Information Foraging Theory (IFT) explains why people (information predators) leave one patch to move to another, such when the casters left Actuator patches. According to IFT, predators choose navigations as cost/benefit decisions, based on the value of information in the patch a predator is already in Actuators Environment Performance Sensors Cue: ? Goal: assess scouting Cue: Units separating, fighting likely over Cue: Units co-located, impending combat likely Figure 2 . The A-E-P+S loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators, and returns there throughout the foraging process. If a caster interrupted the loop, they usually did so to return to the Actuators. versus the value per cost of going to another patch [29] . Staying in the same patch is generally the least expensive, but when there is less value to be gained by staying versus moving to another patch, the predator moves to the other patch. However, the predator is not omniscient: decisions are based upon the predator's perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types [27] and from the cues (signposts in their information environment) that point toward content available in other patches.\",\n",
       "  'RQ2 Results: The How: How do shoutcasters seek the information they seek?Interestingly, this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations (text or occasionally images) in previous IFT investigations that label a navigation device, like a hyperlink or button that leads to another information patch. In contrast, cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However, cues like this were considered cues because they \"provide users with concise information about content that is not immediately available\" [29] . They suggested high value in another location -in the case of combat, the Units tab.',\n",
       "  'RQ2 Results: The How: How do shoutcasters seek the information they seek?These Performance measures gave the shoutcasters at-a-glance information about the ways one player was winning. The most commonly used tab, for example, the Units Lost tab (Figure 3 ), showed the number of units lost and their total value, in terms of resources spent. This measure achieves \"at a glance\" by aggregating all the data samples together by taking a sum; derived values like this allow the visualization to scale to large data sets [32] . However, Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type, as Figure 4 shows, so type information was maintained with only positional data lost. This contrasts with the Minimap (medium aggregation), in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data (HUD), but these patches were also the only way to access past state information ( Table 2) .'],\n",
       " ['Motivation for the studyThe above four examples are extremely mundane and commonplace interactions which might occur in-car, and yet they deserve our consideration as moments of interaction in which digital technology might offer support. Such examples motivated us to design our in-car interactive system. Obviously, we see an external environment through a car window or a windshield, but we wondered how we might reimagine that moment of interaction through the window. Fig. 1 (above) shows a concept sketch of an interactive car window. Using the window, passengers can interact with the external environment. The system allows passengers to freeze (i.e., pause/stop) the scene and rewind the outside view by buffering scenes for several seconds. The system expands the idea of Google Street View [10] with an intuitive user-interface and passenger supporting functions. This simple idea seemed promising, however, it was questionable without an underpinning theoretical framework and some understanding of how users would actually interact with and respond to such a system. We thus defined and began to explore the research question given above.'],\n",
       " [\"INTRODUCTIONCBA is a decision-making system developed by Jim Suhr that is based on four principles: (1) Decision-makers must learn and skillfully use sound methods, (2) Decisions must be based on the importance of advantages, (3) Decisions must be anchored to relevant facts, and (4) Different decisions call for different methods. This paper will focus on the first principle (Suhr 1999) . CBA has gained more attention in the construction industry in recent years. This increase has been driven by demands for more collaborative project organizations and transparent decision-making processes; by the synergy of CBA with other agendas such as improving sustainability and safety; and by an increasing need to incorporate multiple factors into the decision-making process. However, as the construction industry simultaneously prioritizes delivery logics confined by tight schedules and budgets there is a competing desire to maximize efficiency and timeliness in training processes. Decision-makers seek training protocols with minimal disruption of delivery, even at the expense of quality in education. It is therefore crucial for decision-makers and coaches to think critically about how to provide the most effective training based on the financial resources of the project. This paper compares the experiences of four CBA coaches working separately in different countries and analyzes the skill development of the teams based on the methods of training they received. The primary research question is: What are the benefits and shortcomings of each of the CBA training methods employed by the coaches studied in this paper? This will be evaluated by comparing different styles of training with the learner's subsequent ability to implement CBA. First, this paper will present relevant literature on human learning to contextualize the need for a range of training options. The research methods used to observe more than 30 CBA trainings will be explained and the accompanying data presented. Finally, the outcomes will be discussed, and conclusions drawn regarding how this research can facilitate other coaches and decision-makers in the industry in personalizing CBA trainings to every unique audience.\"],\n",
       " ['IntroductionIn Denmark, there is currently an extensive focus on electronic patient records. The government has decided that by 2005 all Danish hospitals must have replaced the traditional paper-based patient records with electronic ones. However, it is up the regional authorities to decide on the details of deployment. Thus a number of pilot projects are currently in progress with the aim of developing and evaluating electronic patient record systems (see e.g. [2] ). In relation to a regional Danish research program entitled \"The Digital Hospital\" we have studied the use of a commercial EPR system currently in use at a large hospital (IBM IPJ 2.3). In addition to this, an experimental mobile EPR prototype extending the current system\\'s functionality was designed and evaluated. Driving this study, we were concerned with the following research questions: 1) what challenges characterize the use of contemporary electronic patient record systems? 2) How can these challenges be met by improved interaction design?'],\n",
       " [\"STATE OF THE ARTSeveral descriptive models seek to capture buying behaviors. Current literature reveals that there are six fundamental stages within the purchase decision making process [12] : -First, users become aware of a new need. This realization can result from companies' prospecting campaigns or the recommendation of a new product from friends. -Users will then determine from whom to buy, a process cal-led merchant brokering. In online environments, consumers can use a price comparison website to determine where to buy their goods. In this case, everything that customers experience becomes an essential building block of a rapport between a buyer and a seller. Users are likely to look for website qualities that promote trust, ease of navigation, and strong relevance of items recommended to them [12] . -In the meantime, they evaluate product alternatives in order to make the final choice. At this stage, called product brokering, interactions help recommender systems to understand their needs and present personalized options to them. -Stages 4 and 5 consist of negotiation and purchasing. The seller has to provide security and confidence in order to close the sale. -Finally, users' satisfaction in relation to the overall buying experience can be measured in a sixth stage, if post-purchase product service is involved. In this paper, we will focus on the product brokering stage where consumers evaluate product alternatives in order to make the final choice. At this stage, tracking users' interactions can help a recommender system understand their needs and present personalized options to them. According to Haubl et al., the product brokering stage can be divided into two steps [4] . During the first step, the active user identifies a subset of products to compare. During the second step, the different features and details of these products are compared in order to make a decision. Haubl also proved that the use of a recommender system leads to a reduction in the number of alternatives considered seriously for purchase [5] , and that a recommendation agent increases the number of non-dominated alternatives -i.e. not objectively inferior to any alternative [15] in the set of alternatives seriously considered for purchase. Based on such research, it is apparent that recommenders prove to be a useful tool to users, assuming that they provide items relevant to users' needs. However, the goal of personalization is not only to provide the right item to the right person, but also right away and at the right time. The time constraint has long been overlooked by researchers. In this paper, we aim to analyze both the impact of recommenders over time at the product brokering stage, thus extending the findings of [4] ), and the factors that influence the users. The research questions with regards to the setup of a recommender focus less on what to suggest, but rather when and why.\"],\n",
       " [\"TokenizationTokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997) . However, different research questions or applications induce different conceptions of the term 'word'. For a shallow morphosyntactic analysis (part of speech tagging), a 'simple' tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each.\"],\n",
       " [\"DISCUSSION: CRAFTING RESEARCH PRODUCTSResearch product qualities are also not scalar. Qualities are not a measure of magnitude or to what degree along the continuum of finish, for example, a research product achieves. As we learned in the case of the hook, a research product is either experienced as an artifact with a high degree of finish or it is not. There is always room for refinement; however, it became clear that each quality must be achieved and be present simultaneously. This is a fundamental difference with prototypes where, a design researcher can choose to emphasize (or 'filter out' [31] ) one aspect of the prototype at the expense of others, such as when a technical prototype is created to establish the technical possibilities with no regard for user experience [25] . In this way, prototype qualities are scalar. For example, a common strategy in prototyping is to iteratively advance the degree of finish and fidelity through a series of artifacts. This condition poses real challenges for designers of research products. Typically in design, questions of the use situation are asked through the prototyping process and the 'answer' is presented in a finished product. In designing a research product, there is the typical iteration and prototyping in trying to best formulate and carry research questions through an artifact. As a result, the finished research product depicts the design research team's 'best' articulation of how to ask and pursue the research question at that given time.\"],\n",
       " [\"RQ2:First, we are interested in how student groups write in the synchronous mode (i.e., practices), and what characterizes the different practices. We would intuitively expect that the ability to write simultaneously together in one location might manifest new practices of collaborative writing. However, it is also plausible that students might maintain the old ways of working together and might not utilize the affordances of technology or take advantage of the work environment. In this context, the students might decide to take turns to write in a document (Sequential Writing in Posner & Baecker's taxonomy [26] ), or delegate one student to write the whole document while others only provide comments and ideas (Scribe in [26] ). In addition, we examined how the writing styles differ in terms of collaboration behaviors (e.g., participation equality, group activeness), text quality, and quantity.\"],\n",
       " ['DOMINANT HYPERPARAMETER• Is there one or more hyper-parameters that afect more the accuracy of recommendations? • Could be established a procedure to check if diferent values for a specifc hyper-parameter can lead to signifcant diferences? To answer these research questions, we analysed the three hyperparameters of BPR-MF separately. In details, for a certain parameter, we want to defne a procedure to check if diferent values of that hyper-parameter lead to systems which show signifcant diferences in accuracy of recommendation. Let us suppose we fx the metric and the number of latent factors: we still have two other parameters that can vary. We computed all the possible combinations of the remaining hyper-parameters. From the set of combinations, we randomly chose 25 pairs of combinations. We recall that a pair of combinations corresponds to a pair of systems that share the number of latent factors, and difer in the number of iterations and learning rate. Now we compute the p-values of all these pairs and we order them by decreasing value. These values correspond to a pvalues curve which is peculiar for the considered metric and number of latent factors. Consequently, for the curve, the corresponding Discriminative Power can be computed. This procedure can be repeated to analyse the discriminative power of various values of latent factors parameter. Thus, the whole procedure can be repeated to analyse the remaining hyper-parameters. The results of this study is depicted in Tables 5. We used bold to highlight the best DP value for each hyper-parameter analysis. As suggested in [44] , the results between the two tables are not comparable. However, for both datasets, the number of latent factors seems to be the dimension on which variations in hyper-parameter value lead to signifcant diferences in recommendation accuracy. Moreover, for Movielens dataset, the DP value is much lower than the best values for \"Number of iterations\" and \"Learning rate\" hyper-parameter analysis. This suggests that \"Number of latent factors\" is dominant with respect to the other hyper-parameters. \"Learning rate\" dimension shows a diferent behaviour on the two datasets: on Movielens it shows big variations in terms of DP values, while on Amazon it shows oscillating performance. Finally, DP values confrm that conducting the study on the sub-grid was a reasonable choice.'],\n",
       " [\"Reinforcement Learning ApplicationsIn summary, most previous Reinforcement Learning approaches based on social signals in Human-Robot Interaction utilize a combination of implicit and explicit feedback. However, for a robot to acquire their human counterparts' sense of humor in an unobtrusive manner, the robot would need to learn to be humorous solely by learning from implicit feedback [19] . In this paper, we aim to address this open research question by exploring whether implicit feedback alone suffices to enable a robot to learn humor preferences.\"],\n",
       " ['DiscussionsHierarchical Dirichlet scaling process opens up a number of interesting research questions that should be addressed in future work. First, in the two scaling functions we proposed to model the correlation structure between topics and side information, we simply defined the relationship between topic k and label j through the scaling parameter w k j . However, this approach does not consider the correlation within topics and labels. Taking inspiration from previous work (Blei and Lafferty 2007; Mimno et al. 2007; Paisley et al. 2012 ) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships. Second, our posterior inference algorithm based on mean-field variational inference is tested with tens of thousands documents. However, modern data analysis requires inference of massive and/or streaming data. For a fast and efficient posterior inference, we can apply parallel or distributed algorithms based on a stochastic update (Hoffman et al. 2013; Ahn et al. 2014 ). Furthermore, we fix the number of labels before training but we need to find a way to model the unbounded number of labels for streaming data. '],\n",
       " ['There are two basic strengths of the AGL paradigm for use in the language sciences. First, it is used as a model system to study aspects of natural language processing; for example, syntactic or phonological processing, in isolation from semantic influence. Since an artificial language is novel to all participants, prior learning is controlled. AGL has been most widely used as a model system for syntax, but work related to phonology has also appeared (Tessier, 2007) . The second strength is the possibility to study a wide range of populations using identical, or at least comparable, paradigms. Populations have ranged from prelinguistic infants to adults, as well as non-human primates and songbirds (a review on comparative animal studies using AGL is, however, outside the focus of this chapter). Comparisons allow contributions to research questions on language acquisition and evolution. These two strengths of the paradigm will be illustrated throughout this chapter, where we will review the state-of-the-art in AGL research, including research with infants, with a particular focus on neuroimaging work. A main limitation of the paradigm, namely the constraints on generalization to natural languages, will also be addressed, through clarifications of some of the main differences between AGL and natural language research.'],\n",
       " [\"User Coherence and Magic BarrierIn this experiment, we assess the validity of the proposed coherence functions as good predictors for the magic barrier to answer the research question RQ1. With this goal in mind, we show in Table 4 the Spearman's correlation values between the coherence and the magic barrier per user (Pearson's correlation was very similar). Note that Pearson's correlation coefficient is designed to capture linear relationships between the two variables whereas Spearman's captures non-linear dependencies. Both correlations provide scores in the range of −1 to 1, where 1 denotes a perfect correlation, −1 represents an inverse correlation, and the absolute value is the strength of the relationship. We observe in Table 4 that the correlations for the weighted version of the coherence function (that is, where the importance of each feature in the user profile is ignored) show more predictive power only when the standard deviation is used. Besides, entropy and KLD do not perform very well. Additionally, Emotion keywords and Intended Audience seem to be the best feature spaces for most of the coherence formulations, and especially, for the cases where a strong correlation is obtained. We have to however note that these feature spaces offer a low coverage in terms of the items identified with these features [20] , thus this aspect should also be taken into account when selecting the feature to use.\"],\n",
       " ['INTRODUCTIONSeveral approaches have been proposed making use of the auditory and/or haptic modalities to enhance the user experience, to enable the use of the system in situations when visual attention cannot be on the device, and to cater also for visual disabilities. However, the vast majority of these systems have been designed for navigating a defined route or finding a specific place, thus limiting their use for truly exploring the city, which is also a relevant mode of sightseeing [5, 12] . This leads to our primary research question: how to design a non-visual interaction technique that supports exploratory, serendipitous discovery of POIs (see Fig. 1 for a conceptual illustration of the setting.).'],\n",
       " [\"INTRODUCTIONTo tackle such limitations and make interaction design simple, low cost, and intuitive, we probe three modalities: color, sound, and vibration. Previous studies have shown their impact on a person's perception [21, 36, 34] . However, few papers have comprehensively evaluated the effect of these modalities in scenarios involving affective communication with a social robot. Thus, this leads our research question as to how the three modalities affect a human's emotional perceptions through expressions.\"],\n",
       " [\"RQ1: Toward Four Factors of CuriosityThe four self-report 'base' scales in our survey were developed to measure constructs other than curiosity (as discussed above). However, a closer look at the semantics of the survey items and their design objectives revealed that specific items in each of these scales do in fact interrogate constructs of curiosity. Following up on this observation, we examined whether it was possible to construct a multi-factor 'curiosity' scale using survey items from these 'base' scales' existing motivation and player types. For this purpose, we extracted the ten curiosity-related survey items from the SOCIAL CAPI-TAL, OBSESSIVE/HARMONIOUS PASSION, BEHAVIOURAL ACTIVATION, and BRAINHEX instruments, and performed an exploratory factor analysis (EFA) on these to examine their latent factor structures. In deriving a joint factor structure from item sets with different response scales, we follow a common methodology from personality psychology [1, 13, 61] . The precise wording of the items selected can be found in Table 1 , and we will direct the reader to the specific positions when discussing the individual item groups.\",\n",
       "  \"RQ1: Toward Four Factors of CuriosityAll KMO values for individual items were > .7, which is well above the acceptable minimum of .5. Bartlett's test of sphericity, 2 (45) = 2, 757, p < .001, indicated that correlations between items were sufficiently large for EFA. We ran an initial principal component analysis (PCA) to obtain eigenvalues for each component in the data. Four components had eigenvalues above Kaiser's criterion of 1.0 and in combination explained 71% of the variance. Given the large sample size, and the convergence of the scree plot and Kaiser's criterion on four components, we retained these four factors (F1 to F4) in our final analysis. Table 1 shows the factor loadings after rotation. The oblique rotations oblimin and promax yielded the same factor structure as had been extracted using the orthogonal varimax rotation, indicating a stable pattern of four largely independent factors. Factor F1 had a high reliability, Cronbach's ↵ = .86, whereas factors F2 and F3 had moderate reliabilities with ↵ values of .67 and .61 respectively. Factor F4 had relatively low reliability, ↵ = .50. However, Kline acknowledges that, for psychological constructs like curiosity, Cronbach's ↵ can, realistically, be expected to be below .7 because of the diversity of the constructs being measured [20] . That said, our findings with respect to F4 should be interpreted with caution.\",\n",
       "  \"RQ2: Correlating Self-Reports and Behavioural DataDestiny tracks thousands of behavioural features about individual players. However, in any work attempting to correlate in-game behaviour with psychological factors, an initial challenge lies in isolating those behaviours with the most predictive potential [2, 48] . There are two approaches to such an analysis: either a 'bottom-up', exploratory approach where high numbers of variables are correlated with the selfreport scores, or a 'top-down' approach where hypotheses are used to define which behaviours to work with. While the exploratory approach can result in over-featuring and false positives, the top-down approach is exclusive, ignoring behaviours not included in the hypotheses. Hence, in this study, Figure 1 . The four curiosity factors and their correlated game behaviours emerging from the Destiny dataset (reprinted from [43] ). a combined approach was used, where an initial set of behavioural metrics was selected based on related work and theory, and these metrics were then used to formulate a series of hypothesized correlations of curiosity factors. The correlations were then inspected in a range of related behaviours in Destiny that fit the curiosity factors. Furthermore, we included data from both gameplay options in Destiny (e.g., PvE and PvP play), and from across the spectrum of activities in the game, including rates of objective completion, performance, and exploration.\"],\n",
       " ['\\uf0b7 RQ1: Does cooperative fitness tracking between co-workers improve the level of physical activity? For cooperative fitness tracking, besides the social effects of peer bonding, we observe that the proximity between the co-workers in the office environment may also affect its outcome. In the CSCW community, the effects of distance have been extensively concerned with workplace technologies and many social practices [4, 49] . For instance, an experiment by Bradner and Mark [5] showed that the likelihood of cooperation through communication technologies could be abandoned due to the increase of interpersonal distance. Similarly, Cummings and Kiesler [18] found that greater distance between team members could lead to lower cooperative performance. In our view, however, such classic CSCW narratives of distance effects and the way in which they influence technology-assisted workplace fitness promotion needs to be further explored. It is a worthy topic of study to understand the effects of physical proximity at work on cooperative fitness tracking in the office context. To this end, this study involves two types of co-workers, both those who are distributed and co-located (i.e. at the same site), to explore our second research question:'],\n",
       " ['Automatic detection of clickbaitsHowever, the research questions we investigate in this work are complementary to the earlier work. For example, in [14] , we identified linguistic characteristics that differentiate clickbait and traditional news headlines. Whereas, in this paper, we explore complementary questions specific to tweets such as whether clickbait tweets contain several entities which might lead to their increased visiblity, or whether the sentiment conveyed by the clickbait tweets differ from the non-clickbait tweets. Moreover, taking a very different direction compared to [14] , we study the production and consumption patterns of clickbaits in Twitter, and bring out interesting insights.'],\n",
       " ['INTRODUCTIONCSCW has a rich history of scholarship critiquing the uneven politics of classification and categorization (e.g., [5, 42] ). Classification of gender and its relation to selfpresentation is a secondary theme of this paper and an important area for continued research. However, our findings show that addressing categorization issues alone is not enough. Disclosure of personal information about identity transitions on Facebook is a more fundamental concern for participants in this work. Therefore, in this paper, we focus primarily on five research questions related to differential identity disclosure, as outlined here:'],\n",
       " ['INTRODUCTIONOur research investigates the social significance of technologically mediated human-animal interactions. We are interested in how tracking devices for dogs are used within domestic contexts in the everyday management of human-canine relationships and care-taking practices; we are interested in how these practices influence the behavior of and change both human and canine family members. However, since we cannot communicate with dogs in the same way that we communicate with humans, this kind of research clearly raises methodological issues to do with the interpretation of human-dog manifest interaction (similar issues arise in studies with young children and adults with communication impairments [15] ). Exploring these issues is important for the development of the emerging areas of human-animal interaction [33] and animal-computer interaction [17] . In order to study technology-mediated human-animal interactions or to develop user-centered technology for animals, we need to question what these interactions and the technology that mediates them might mean for animals as well as humans. Therefore, our research questions how technology might acquire and convey meaning for both; we question how this meaning might be inferred by or communicated between the two, and how it might inform the way in which the two adapt to each other and coevolve; we also question how this coconstructive [9] meaning exchange could be accessed and understood by those researching the interconnections between humans, animals and technology.'],\n",
       " ['Research Question 2What are the attitudes of Chinese Internet users toward Internet censorship? Do they support, oppose, or are they indifferent toward censorship? For this research question, we focus on the attitudes of Internet users who report being aware of Internet censorship in China. Previous research suggests that the majority of Chinese Internet users support an Internet controlling policy [Fallows 2008; Guo 2003 Guo , 2005 Guo , 2007 Guo and Feng 2012] . However, the majority of users who are aware of Internet censorship might actually be against it because, for example, censorship could hinder Internet usage. No studies have yet disentangled awareness and attitudes toward Internet censorship.'],\n",
       " ['RQ1: Intensified DocumentationRevision histories document several dimensions of knowledge creation in Wikipedia: the editor, time stamp, and con- tent changed in each revision. Table 1 shows the number of revisions, editors, talk page revisions, talk page editors, and pageviews to the ten articles with the most revisions. The amount of activity on these ten articles and talk pages dominates all of the activity on the other 121 pages in the sample combined. The Top 10 pages and their corresponding talk pages account for 76,194 (87.6%) of all revisions, and 5,449 (80.2%) editors made these revisions. Excluding BLM, these top articles are about events that generated a large amount of media attention outside of Wikipedia [10, 13, 19] . The activity for the \"Shooting of Michael Brown\" and the \"Shooting of Trayvon Martin\" contribute 52,941 (60.9%) of all revisions by themselves. The \"Shooting of Michael Brown\" article also had the most pageviews, accounting for 31.5% of all views in the Top 10, reflecting its influence as a major news event. Figure 2 visualizes the monthly activity for all articles in our corpus by number of revisions made (blue), unique editors contributing (green), and pages edited (red). The peaks in activity correspond closely to the death of Oscar Grant (January . We note one peak in July 2013 corresponds to the acquittal of George Zimmerman, the man who killed Martin, and consists almost entirely (98%) of edits to pages related to the shooting of Trayvon Martin and a few edits to pages about the shooting of Oscar Grant. The monthly activity plots illustrate that prominent events drive periods of high activity and reshape activity in aggregate. While they do not sustain the levels of peak activity, we observe a general trend towards an increasing level of activity across all three metrics as additional articles are created and added to the sample. However, the large peaks seem to suggest that activity in the BLM topic space is focused on individual events and does not necessarily imply sustained writing about the BLM-related topics.'],\n",
       " ['Discussion of the algorithmOur algorithm is also quite similar to the Foil (Quinlan 1990 ) algorithm, which forms the basis of many rule learning algorithms, most notably Ripper (Cohen 1995) . The key difference here is that Foil-based algorithms do not evaluate refinements on an absolute scale, but relative to their respective predecessors, i.e., they focus on the gain that a rule obtains in comparison to its predecessor. While this is a reasonable approach, gain-based algorithms can not directly compare the evaluation of two rules with different predecessors, and are therefore not able to identify the best rule encountered during the search. Instead, they always return the last rule searched. Thus, their performance crucially depends on the availability of a pruning heuristic or a stopping criterion, which determines when the refinement process should stop. Foil uses a heuristic based on minimal description length for this purpose (Quinlan 1990; Fürnkranz and Flach 2004) , whereas Ripper employs the incremental reduced error pruning technique, which prunes each rule after it has been learned (Fürnkranz and Widmer 1994; Fürnkranz 1997) . On the other hand, algorithms of the type shown in Algorithm 2 do not necessarily return the last rule searched, but the rule with the highest evaluation encountered during the search. In this case, a stopping heuristic assumes the role of a filtering criterion, which filters out unpromising candidates, but does not directly influence the choice of the best rule (Clark and Boswell 1991) . Because of this dependency on stopping criteria, we do not further consider gain-based heuristics in this paper. However, we note that an empirical study comparing gain-based to absolute heuristics is an open research question.'],\n",
       " ['DiscussionWith regard to the first two research questions, it is clear that there is a difference in test scores (Reading Comprehension, Immediate Vocabulary, and Delayed Vocabulary) of participants for three gloss conditions (L1 gloss, L2 gloss, and No gloss). Firstly, there was no difference between L1 and L2 gloss conditions while there was a significant difference between both gloss conditions and no-gloss condition in the reading comprehension test scores. Secondly, for both vocabulary test scores, the main effect comparing the gloss groups was significant. However, while the significance value was substantial for the differences between glossed and no gloss conditions, it was almost not significant for L1 and L2 gloss groups (p =.043). The reasons for these differences could be that glosses provide L1 translation or L2 description, so they are more practical than dictionaries in terms of accessibility (Hulstijn et al., 1996) . Since students can easily match the meanings with the words in context, reading process is not interrupted thanks to glosses (Rott & William, 2003) . By looking at the scopes of the studies in literature, Schmitt (2008) recommends using L1 glosses for low proficiency level learners by also adding that it does not matter using L1 or L2 glossing as long as the learners can understand the L2 description or L1 translation.'],\n",
       " ['INTRODUCTIONThe focus of this paper is on developing automatic classifiers to infer working conditions and stress related mental states from a multimodal set of sensor data: computer logging, facial expressions, posture and physiology. We present related work in Section 2. The dataset that we use is presented in Section 3. We identified two methodological and applied machine learning challenges, on which we focus our work: 1) Using several unobtrusive sensors to detect stress in office environments. We found that state of the art research in stress inference often relies on sophisticated sensors (e.g., eye tracker, body sensors), and/or uses data collected in rather artificial settings. We see possibilities to build human state estimation techniques for use in office environments. We aim to combine information from multiple weak indicator variables based on physically unobtrusive measurements. We address the following research questions: Can we distinguish stressful from non-stressful working conditions, and can we estimate mental states of office workers by using several unobtrusive sensors? Which modeling approaches are most successful? Which modalities/ features provide the most useful information? This helps to configure a minimal sensor set-up for office settings. We address these questions in Section 4. 2) Taking into account individual differences. We found that, in affective computing, often one generic model is learned for all users. This may work for something universal, as the expression of emotions. However, in earlier work [5] , [6] , we found that people differ in their (work) behavior: typical behavior of users already differs per person. Moreover, the way in which people express mental effort or stress may differ. This highlights a need to build personalized models for particular users or user groups, instead of one general model. We address the following research questions: How important are individual differences? Can we improve performance by building personalized models for particular user groups? We address these questions in Section 5. Finally, we present our Conclusions and Discussion in Sections 6 and 7.'],\n",
       " [\"RQ3. How is usability evaluated by the software standards-setting body?It appears that for these organizations, responsibility for usability-related evaluation is delegated to outside agencies, including trade press and vendors. As organizational context of use was not mentioned as an important determining factor of usability, this result is perhaps not surprising. However, it again represents a gap between usability evaluation methods suggested by HCI researchers or advocates of the socio-technical organizational perspective, and current practice by MIS managers. Most HCI researchers would not endorse usability evaluations conducted by agencies unfamiliar with the users, tasks, tools, etc. in place in the organization. However, the results here, along with those reported by Dillon et al (1993) suggest that this form of evaluation is frequently the case. For the organizations examined in this study, MIS professionals seem comfortable in basing evaluations of usability on test or reports from vendors, trade press, and other even users outside their organization. In conclusion, usability is generally not identified as a criterion used by managers in evaluating potential standard software systems. When it is identified as a separate criterion, it is not considered as important as other factors such as compatibility or feasibility. However, while the importance users' place on various criteria generally mirrors those of managers, the two groups are distinguishable in their ratings of usability. Unlike the evaluations of manager, users believe that usability is the most important criterion used in the evaluation of standardized software. It is clear that the human factors concerns of end-users are not explicitly represented by MIS managers or MIS steering committees.\"],\n",
       " [\"EVALUATING TOGETHER WITH AUTISTIC CHILDRENStarting with the definition of the goals of the evaluation, autistic children may already challenge researchers' pre-conceived expectations in terms of the purpose and evaluation criteria as well as the intended audience and the required methods. In classical researcher-driven evaluation, the selection of methods is generally derived from a combination of the research questions and the epistemological stance of the researchers (see [20] ). However, when working in PE it becomes important to decide on methods based on the abilities of the participants, and with a view to ensuring that the resulting data are meaningful to all involved. By separating the definition of goals of participatory evaluation from the methods, we separate the questions of what is evaluated from how it is evaluated. Both parts inherit different aspects of meaning making, agency and participation -as we will detail in PEACE below.\"],\n",
       " ['Analysis and ConclusionsFrom the development test results in Section 3, we note that the Stat-XFER systems\\' performance currently lags behind the state-of-the-art scores on the 2007 test data 3 . This may be in part due to the low volume of training data used for rule learning. A key research question in our approach is how to distinguish low-frequency correct and useful transfer rules from \"noisy\" rules that are due to parser errors and incorrect word alignments. We believe that learning rules from more data will help alleviate this problem by proportionally increasing the counts of good rules compared to incorrect ones. We also plan to study methods for more effective rule set pruning, regardless of the volume of training data used. The difference in metric scores between indomain and out-of-domain data is partly due to effects of reference length on the metrics used. Detailed output from METEOR and BLEU shows that the reference translations for the test2007 set are about 94% as long as the primary French-English system\\'s translations. On this set, our system has approximately balanced precision (0.62) and recall (0.66). However, the nc-test2007 references are only 84% as long as our output, a situation that hurts our system\\'s precision (0.57) but boosts its recall (0.68). METEOR, as a metric that favors recall, shows a negligible increase in score between these two test sets, while BLEU and TER report significant relative drops of 17.3% and 7.8%. This behavior appears to be consistent on the test2007 and nc-test2007 data sets across systems (Callison-Burch et al., 2007) .'],\n",
       " [\"LimitationsUsing change blind user interface design strategies has limitations. Inevitably some messages will create motion transients, and to fully exploit the technique will require robust object and people tracking. Most challenging is that some masking methods are not effective when the changes occur to the object of central interest in the scene (e.g. a face of a key person in an image) as opposed to an object of marginal interest. Change detection time for central interest objects is fast regardless of object color, position, and presence/absence [25] . Object position and presence are better encoded by the brain than surface properties, which makes these properties more difficult to change without triggering a detectable motion transient [2] . Therefore, change blindness is less likely to be effective for objects of strong interest. The worst-case scenario, however, is no worse than the current situation: a motion transient is created that mildly attracts the user's attention. Detecting what may be a user's central interest versus marginal interest in a ubiquitous computing environment is an active research question.\"],\n",
       " ['EXPERIMENTAL MEASURES FOR HRIExperimental designs and measures in HRI are becoming a field of research on their own, as proven by many summer schools, workshops and special sessions dedicated to the domain that focuses on the experimentation and evaluation of interactive robotic systems. 1 This phenomenon is driven by the lack of common benchmarks and standardized metrics to evaluate robotic systems and the quality of the interactions with the users. Indeed, user experience is central to validate the credibility and acceptability of a system. However, long-term social human-robot interaction is difficult to set up for a complex robotic system. Measuring the interaction quality in HRI, especially for social HRI, is essential but measures used are often very context dependent. From an epistemological point of view, as a new field of research, HRI has to develop strong metrics in order to guarantee its reproducibility and secure the findings of the domain. Of course, depending on the type of evaluation (online survey, large-scale experiment, case study, or longitudinal analyses) the metric used in HRI can vary. But some methodologies can be applied to ensure a common ground of knowledge. [12] , RAS [13] , GodSpeed [14] , IoS, COIRS [15] , . . . Bethel [9] , there are five primary methods of evaluation used in HRI: selfassessment, interviews, observational or behavioral measures, psychophysiology measures, and task performance metrics. In line with this work, Weiss [10] proposed the Usability, Social acceptance User experience and Societal impact (USUS) evaluation framework. The USUS framework gives methodological guidelines according to the research objectives that are aimed to measure usability, social acceptance, user experience, and societal impact. The USUS evaluation framework also provides indicators for each of the research questions and the associated methods of evaluation (i.e., expert evaluation, user studies, questionnaires, physiological measures, focus groups, and interviews). Be they in laboratory, field study, or Wizard-of-Oz experiments, the community often proposes scenario-based experimental protocols. Bethel and Murphy [9] recommends using at least three forms of evaluation in order to have reliable results for an experiment. We propose to group these categories and to give some examples of measures used in HRI.'],\n",
       " ['IntroductionContemporary professional translators rarely produce translations entirely from scratch. Instead, they increasingly rely on translation memories (TM) , that is, data bases of texts that have already been translated, and their translations. At translation time, translations of text fragments similar to the actual source text are retrieved from the data base and edited by the translator to bridge the mismatch between retrieved text fragments and an actual correct translation of the current source text. As the quality of the raw output of fully automatic machine translation (MT) systems is on the rise, so is the commercial interest in integrating MT as an alternative or supplement to traditional TMs into the professional translation workflow. Recent studies (Koehn 2009a; Flournoy and Duran 2009; Plitt and Masselot 2010; Federico et al. 2012; Green et al. 2013 ) have concluded that post-editing is, on average, more efficient than translating from scratch. However, the optimal form of human-computer interaction in the context of translation is still an open research question.'],\n",
       " ['INTRODUCTIONThe second technique, TILT, modeled existing techniques for controlling content on a large display \"remotely\" using a personal device (e.g. [11] ). In TILT, the ROI position on the tabletop was controlled via tilt gestures, made with the tablet and enabled by the tablet\\'s built-in motion sensors. Such \"remote\" cross-device interaction can facilitate individual work [11] ; however, its impact on teamwork is unclear. Given these uncertainties and the potential reachability issues introduced by TOUCH, we performed an empirical study to explore the impact of TOUCH and TILT under different seating positions on collaborative processes. In particular, we sought to answer the following research questions:',\n",
       "  'Supporting Joint Work (RQ2)A second feature of the ROI that supported joint work was the ability to independently position and share tablets in the environment. This feature enabled groups to form tableaux to facilitate joint comparison and discussion of selected data items. However, as Wallace et al. [37] found, using tablets for tableaux formation can be restrictive. It offers less physical space than the tabletop does to spread out data between collaborators. Thus, one could also consider enabling users to open selected data directly on the tabletop, or to enable selected data to be moved from the tablets to the tabletop to facilitate joint examination of the \"detailed\" data. This design direction should be explored carefully, however, as it may negate the collaborative benefits provided by the shared reference \"overview\" map on the tabletop.'],\n",
       " ['METHODSThe study was undertaken in two schools in Sweden with three groups of 70 newcomers in total, aged 25-55-two groups in Trelleborg and one group in Kvarnby -between April and June 2017. Participants were recruited with the assistance of teaching staff in both schools and did not constitute a homogenic group, but were made up of people with different backgrounds, belief systems, values, and reasons for leaving their old lands. However, it was made clear that any participation would be voluntary. Prior to undertaking the fieldwork, we had also visited both schools and held multiple meetings with the teaching and support staff -one meeting in Trelleborg and three meetings in Kvarnby -so as to (1) receive feedback on research design and approach, (2) make sure that the scope and remit of the study was understood by everyone involved, and (3) to create an engagement schedule that would not disrupt planned school activities. These meetings were accompanied by email dialogue between teachers and the research team. Whilst a wide range of newcomers was recruited, the groups were dominated by Syrian refugees who had been in Sweden between six months and two years All students belonged to language groups C and D, which meant that they were in the top two groups of Swedish as a foreign language and had previous experience of higher education or language learning. Group demographics, whilst mixed, were thus largely made up of participants from the professional classes in their country of origin. It was decided to conduct the study in the language common to all participants, namely Swedish. This was made possible with the research being conducted by two Swedish-speaking researchers. The research was, however, deliberately designed to be inclusive, regardless of educational background or skills. Similarly, the methods needed to be sufficiently flexible to accommodate a range of language abilities, facilitate the use of translation apps and allow for the use of supplementary techniques such as images and discussions in other languages. As a result, we used collaborative collage as the method for engaging with participants and gathering data. Collaborative collage is one of the engagement tools termed \"creative security methods\" [19] . It uses collaging techniques to enable small groups to discuss research questions and present their views in a collage produced on paper. Using this technique, space is created that allows participants to negotiate the language used within the small group, facilitates the use of images to supplement the written descriptions, and allows for the use of scribes to write down views. By allowing participants to work in this collaborative manner, a dynamic research environment was created where the participants became the prime narrators. This allowed their individual as well as their shared stories to emerge organically. were left at each research site and only pictures of these materials and summaries were retained by the researchers. Table 1 presents a summary of the geographical location for each group activity, the group size and composition, language ability and languages used, activity duration, and outputs. Whilst the language used for the outputs was Swedish and English, Arabic was used by some groups as the language in which to conduct the discussion, and translation apps and group work were used to translate the results of the discussion to contribute to the outputs. As part of the consultation with teaching staff as well as preparatory work with newcomer groups, we developed four research provocations, outlined in Table 2 . These provocations were deliberately designed to work at an instrumental level so that participants could answer in a manner that described their mobile phone use without reflecting on the meanings of that use. The provocations were supplemented by prompts, also listed in Table 2 , which gave participants an opportunity to provide further reflective answers that addressed abstract as well as practical aspects of their mobile phone use.'],\n",
       " ['Incremental Model Updates with Dynamic RecommendabilityIf recommendability of items is a monotonically decreasing function over time, one does not have to worry about these issues: {(u, l, t c ) ∈ P t : l = i} will be the empty set for items i ∈ R t +1 \\\\R t , since items that become recommendable are per defnition new in this context. In, for example, a news recommendation setting this makes perfect sense: older articles should not be considered for recommendation. In a retail environment, however, this is not the case: recommendability will often depend on seasonality and current stock. Table 1 shows the characteristics of the datasets we used to experimentally validate the efciency of our proposed approach. Movielens is the latest well-known Movielens dataset [4] , Netfix refers to the full dataset that was used for the famous Netfix-Prize [1] . For both movie datasets, we converted explicit ratings to binary implicit feedback, entirely disregarding the actual ratings. Outbrain is a dataset containing logs from users and articles they read, published in a recent Kaggle competition [14] . We use a deduplicated version of the frst 200 million logged user-item events in our experiments: in the case of recurring user-item pairs, we keep only the earliest entry. News is a proprietary real-world dataset consisting of roughly 96 million user-item pairs originating from article reads on the website of a large Belgian newspaper. Our algorithm, as well as the baseline methods, are implemented in C++ and compiled with all the available optimisation fags. Experiments ran on a single Intel Xeon processor. We aim to answer three research questions, respectively covered in the following sections:'],\n",
       " [\"Normative conflict.These normative conflicts can have negative consequences: Filippova and Cho, in a series of studies of FLOSS communities, showed that some forms of normative conflict had a negative impact on members' identification with the project and their intention to continue participating [16, 17] . However, we know relatively little about the conditions under which different sources of normsinjunctive vs. descriptive, local vs. imported-are most influential in online communities. Because there is insufficient prior work for us to generate a hypothesis about the specific nature of these influences, we ask a more general research question about normative conflict:\"],\n",
       " ['DiscussionThe results concerning the first two research questions do not support the findings of Steven (1991), Cobb (1997 Cobb ( , 1999 , Koosha and Jafarpour (2006) , Çelik (2011 Çelik ( ), Huang (2014 , Rezaee et al. (2014 ), or Daskalovska (2015 with regard to the effective use of corpus in teaching vocabulary. However, it should be noted that the present study did not have the same experimental setting as those studies. Steven (1991) and Cobb (1997 Cobb ( , 1999 did not examine collocations. Even though Koosha and Jafarpour (2006) , Çelik (2011 Çelik ( ), Huang (2014 and Daskalovska (2015) investigated the effectiveness of corpus consultation in teaching collocations, the focus was on different types of collocations, not on V+N collocations. The length of exposure to concordancing, however, was longer in Rezaee et al. (2014) than the current study in examining verb+noun collocations.'],\n",
       " ['CONCLUSIONArchDiff represents only the beginnings of our work in this area. The creation of a graphical user interface is clearly at the forefront of our further development efforts. However, we also intend to address some more fundamental research questions as part of our future work. Most notably, we intend to investigate how the differencing and merging algorithms can be adapted to support dynamic, run-time updates [13, 16] . Given that xADL 2.0 supports attaching implementation information (such as Java class files) to architectural elements, we intend to leverage the above results in developing a tool that \"merges\" architectural changes into a running system by removing, instantiating, and linking elements dynamically. Additionally, we intend to investigate the use and applicability of more fine-grained, semantic-based differencing and merging algorithms to further support the management of architectural change.'],\n",
       " ['Advantages of Using Input Streams-The input stream usually yields more data per trial than the transcribed string, since by definition |IS| ≥ |T |. Therefore, depending on the research questions being asked, analyzing the input stream for character-level errors may allow us to run fewer trials and save time and money on evaluations, which are often time-consuming and expensive [Jeffries et al. 1991] . Such savings will be possible if it is error data that we are after. However, if we are interested in learning rates as measured by speeds over sessions, analyzing IS will not reduce the number of sessions required. -When instructed to \"enter the text quickly and accurately\" [Soukoreff and MacKenzie 2003], subjects tend to fix most, if not all, of their errors in text entry trials. For example, in the study accompanying a character-level error analysis from the prior work [MacKenzie and Soukoreff 2002b] , subjects left only 2.23% errors in T . Other studies show even fewer uncorrected errors: 0.79% , 0.53% [Wobbrock et al. 2004] , and 0.36% [Wobbrock et al. 2003 ]. In the extreme case, if subjects correct all errors, P and T will be identical and no character-level error information will be available. Such a contingency does not reduce the value of IS, however, since corrected errors (the errors subjects made but fixed) are still captured therein. -Speed and uncorrected errors are tradeoffs in text entry. Therefore, to equitably compare speeds, some experiments [Lewis 1999 ] have required perfect transcription, where leaving errors in T is not permitted. But character-level error analyses of perfect transcription studies are useless when employing only P and T , since they will always be identical. Analyzing IS, on the other hand, allows for the extraction of character-level results, even in perfect transcription studies. -For stroke-based or handwritten text entry, such as Graffiti [Palm Inc. 1995] , one possible outcome of an attempted character is a nonrecognition. By definition, T cannot contain nonrecognitions, but IS can. Therefore, looking at IS can be valuable to designers who are trying to identify characters that are difficult to recognize in stroke-based text entry methods.'],\n",
       " ['Effects of Norm EnforcementThe effects of moderation on the quality of collaboratively created content and long term behavior of individual community members have not been investigated much. In the context of Wikipedia, Halfaker et al. [38] found that the action of \"reverting\" edits has the effect of reducing motivation and quantity of work, particularly for new editors. However, they also found that reverts result in higher quality contributions. Halfaker et al. [37] found that enforcing quality control mechanisms affects the retention of high-quality newcomers. Personalized warning messages, as opposed to pre-defined template messages, are found to be effective in retaining newcomers [33] . Motivated by this prior literature, which studies the effectiveness of norm enforcement actions both at the platform-level (i.e., articles) and individual member-level (i.e., editors), we seek to answer our research question about the effectiveness of NPOV norm enforcement both at the article level (RQ1) and at the editor level (RQ2).',\n",
       "  'Identifying Biased LanguageBias in Wikipedia can emerge from several factors including language style, editors\\' point of view, cited sources, coverage of topics, etc. Prior work focused on political bias [35] , cultural bias [13] , gender bias [75] , topic bias [29] , and authoritative bias [22] . Our goal in RQ1 and RQ2a is to study the effects of NPOV tagging in article-level and editor-level language. Particularly, we are interested in characterizing the bias in articles and editor contribution in terms of the linguistic style that may introduce bias. The Wikipedia Manual of Style [81] states that \"There are no forbidden words or expressions on Wikipedia, but certain expressions should be used with caution, because they may introduce bias. Strive to eliminate expressions that are flattering, disparaging, vague, or endorsing of a particular view point.\" Prior work [e.g., 40, 67] has addressed the task of identifying biased language in Wikipedia at different levels. However, we are not aware of a reliable system that can detect bias in terms of linguistic style in Wikipedia articles. Therefore, we use a set of linguistic style lexicons as a proxy to characterize biased language in Wikipedia. 15 The Wikipedia Manual of Style provides a list of words to watch in a prescriptive manner [79] , indicating style words that may introduce bias. We compiled these styles words into a lexicon called Words to Watch. Prior work of Recasens et al. [67] introduced the task of detecting bias inducing terms in phrases from Wikipedia articles and used a set of pre-compiled style lexicons that are indicative of expressions of attitude or point of view. These lexicons include hedges, factive verbs, assertive verbs, positive words, and negative words. We use these nine lexicons in addition to the words from Wikipedia manual. 16 A list of all the lexicons we used is shown in Table 1 with a description, sources, and example terms. To characterize the amount of biased language in a text, we compute the coverage of each lexicon words per token in the text.',\n",
       "  'RQ2: Editor-level Effects of NPOV CorrectionWhen an editor is corrected for NPOV, there is a significant reduction in their negative words usage (13.5%); however, we do not observe any statistically significant change for six of the ten lexicons relating to biased language, including Wikipedia\\'s own list of \"words to watch\" (RQ2a). These trends remain even after controlling for editor experience and talk page discussion during treatment. For RQ2b, we observe an increase in engagement for inexperienced editors after NPOV correction. One possible explanation is that NPOV correction helps inexperienced users become aware that their contributions are monitored by others, and this awareness could motivate them to contribute more. These results for RQ2 suggest that when corrected for NPOV, the quality of the writing style of editors does not improve significantly (at least, as measured by all but one of our lexicons), while it leads to a increase in editing activities for inexperienced editors. These findings partially conflict with the observations of Halfaker et al. [38] , who found that revert actions demotivate new editors and reduce the quantity of work, even as they increase the overall quality of contributions. Halfaker et al. [38] also found that reverts affect editors differently based on the experience of the editor who makes the revert. Future work could perform similar analysis to further understand the effects of NPOV correction on engagement based on the experience of correcting editors.'],\n",
       " ['INTRODUCTION AND BACKGROUNDPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. There has also been an interest in devices that support prolonged attentive reading. Researchers in \"appliance design\" investigated the usability of reading appliances [6] -the forerunners of contemporary reading devices, such as the Kindle and iPad. A recent resurgence in reading devices has resulted in research on their legibility and general usability (e.g. [9] ). However, this has tended to focus on display technology, rather than interaction design. Another question has emerged around input technology; prototype reading appliances used a stylus, whereas modern devices often adopt touch. The different affordances of the two technologies have provoked a question about their relative merits for different tasks. Annotation is a central task to attentive reading, and the suitability of touch-sensitive devices for annotation [4] has emerged as a key research question.'],\n",
       " ['RELATED WORKPublic datasets truly help accelerate research in an area, not just because they provide a benchmark, or a common language, through which researchers can communicate and compare their different algorithms in an objective manner, but also because compiling such a corpus is tedious workrequiring a lot of effort which many researchers may not have the resources to do. In the area of facial expression analysis, the Cohn-Kanade database, in its extended form named CK+, played a key role in advancing the state of the art in this area. The CK+ database, contains 593 recordings of posed and non-posed sequences. The sequences are recorded under controlled conditions of light and head motion, and range between 9-60 frames per sequence. Each sequence represents a single facial expression that starts with a neutral frame and ends with a peak facial action. Transitions between expressions are not included. Several systems use the CK, or CK+, databases for training and/or testing. Since it was first published, a number of papers have been published that were trained and/or tested on this data set including: Bartlett et al. [3] , Cohen et al. [5] , Cohn et al. [6] , Littlewort et al. [10] and Michel & El Kaliouby [15] . Since then, a few other databases have emerged, including: MMI [16] , SE-MAINE [14] , RU-FACS [2] , SAL [7] . A survey of databases and affect recognition systems can be found in [25] . However, there is a need for mechanisms to quickly and efficiently collect numerous examples of natural and spontaneous responses. Lab-based studies pose numerous challenges including recruitment, scheduling and payment. Efforts have been made to collect significant amounts of spontaneous facial responses; however, the logistics of a laboratory based study typically limits the number of participants to under 100, e.g. 42 in [13] . By using the internet we can make data collection efficient, asynchronous and less resource intensive, and get at least an order of magnitude more participants. Figure 2 shows the web-based framework that was used to crowdsource the facial videos and provides an overview of the user experience. The website was promoted on Forbes.com for the first day that it was live. Visitors may have found it via this route, a search engine or a shared link. Visitors to the website opt-in to watch short videos while their facial expressions are being recorded and analyzed. Immediately following each video, visitors get to see where they smiled and with what intensity. They can compare their \"smile track\" to the aggregate smile track. On the client-side, all that is needed is a browser with Flash support and a webcam. The video from the webcam is streamed in real-time at 15 frames a second at a resolution of 320x240 to a server where automated facial expression analysis is performed, and the results are rendered back to the browser for display. There is no need to download or install anything on the client side, making it very simple for people to participate. Furthermore, it is straightforward to easily set up and customize \"experiments\" to enable new research questions to be posed. For this experiment, we chose three successful Super Bowl commercials: 1. Doritos (\"House sitting\", 30 s), 2. Google (\"Parisian Love\", 53 s) and 3. Volkswagen (\"The Force\", 62 s). All three ads were somewhat amusing and were designed to elicit smile or laughter responses. Results showed that significant smiles were present in 71%, 65% and 80% of the responses to the respective ads.'],\n",
       " ['DISCUSSIONRQ1: Which kinds of task do people prefer to complete on mobile devices and what impact does time have on this? Our results suggest that users prefer to complete tasks on their mobile devices based primarily on two factors: (i) tasks on which they have more prior knowledge and/or interest; (ii) tasks that are more relevant to their current context. We do not expect users to look for absolutely new information while on the go, unless it is highly relevant to their context. For instance, assume a scenario where a user is having lunch with colleagues and they happen to discuss about the effect of cell towers on brain cancer. The user, presumably, has no prior knowledge about this topic. However, given that he/she is seated (situational context) and seeks to find the answer for the colleagues, it is highly probable that the user completes such a difficult task. Users do not tend to complete search tasks that they perceive to be more difficult, suggesting that they need more attention and time; probably preferring to complete such a task later, either on a desktop or in another context [23] , perhaps where they can better concentrate on it [33] . This supports the results of previous work, which showed that users are more engaged with the tasks in which they have more prior knowledge and interest [16] .'],\n",
       " ['Ownership, Security and PrivacyIt is clear, through both our study and previous findings, that there are privacy and security considerations to be made when an HIV+ individual shares their information [2] . There is currently much interest within healthcare to use personal information to research health phenomena, as well as for individuals to share data with their community to support reflection and identify trends and patterns. However, it is an open research question how to motivate HIV+ users to open up the access to their data for this purpose, and under what circumstances they would be willing to do so.'],\n",
       " ['Self-trackingThe idea of leveraging the self-tracked health data of many individuals to discover new insights across a group is not new. Recent research showed value in examining a single variable (e.g., number of steps per day, collected passively) tracked across many individuals to learn about a behavior (e.g., physical activity) at the group level [6] . In this case, like in other recent examples, the data leveraged was collected via existing commercial apps, with their own set of engagement strategies. However, designing self-tracking tools that promote sustained engagement among each user and at the same time enable researchers to learn at scale and discover new disease insights is an open research question.'],\n",
       " ['INTRODUCTIONRemote research methods such as the diary method or the Experience Sampling Method (ESM) [8] have always intrigued researchers, as they allow data gathering in their \"natural, spontaneous context\" [2] without being obtrusive and thereby in places where observation would be impossible or inappropriate. They have been applied both outside (e.g. see [3] . for a variety of usage scenarios) and inside HCI (e.g. [6] , [11] ) in various forms, such as pen & paper or PDAs. Besides, diaries and ESM have shown to minimize retrospective effects (e.g. compared to retrospective interviews) and allow both qualitative and quantitative data collection. They are especially useful in longitudinal studies as they allow analyzing and modeling changes within and between subjects [3] . Drawbacks however include a high burden on the participant and as a consequence thereof such diaries are often reduced to simple repeated questionnaires. Nevertheless, as our world is becoming more and more ubiquitous and HCI research is thereby more interested in investigating how people deal with such technology in the wild, the need for these methods has increased even more and technology itself has been a helping hand to support both the researcher and the participant. In this paper we present Pocket Bee (see figure  1 ), a multi-modal diary tool that allows participants to gather data in multiple ways on Android [1] based smart phones while allowing researchers to access this data immediately via a web-based control center and react on it accordingly, e.g. by sending out specific tasks or questionnaires. Pocket Bee integrates an easy to use client user interface that reduces the burden on the participant while maintaining a high flexibility towards the method and the possibility to capture in-depth data. We furthermore discuss several design goals which illustrate the importance of the tool for both the diary method and ESM. Early electronic diary or ESM tools focused on simply providing questionnaires on a PDA [2] , while current approaches have focused especially on the integration of sensor data to better support ESM (e.g. [7] ), multiple modalities to enrich the data-gathering process (e.g. [4] , [9] ), or the integrated testing of mobile device applications [5] . The existing tools seem to have focused on functionality and extensibility but not so much on the design of the client user interface itself, as it has been merely discussed in the according papers. However, an electronic device does not magically reduce the participants\\' burden for collecting data and it might even increase the burden for some users that are not familiar with smart phone technology. In the following sections we will first present our research questions for the user interface design and go on to discuss the user interface concepts by illustrating an upcoming study in the automotive sector.',\n",
       "  'Design Goals and SolutionsKhan et al. [10] did an analysis of current experience sampling tools and derived some requirements for future tools, such as multi-modality or instant synchronization. We agree with most of these and they influenced our choice of design goals. As stated before we will, however, focus on the research questions from a user interface design perspective and the methodological benefits that can be achieved thereby.'],\n",
       " ['Running till significance is reachedLet\\'s compare the distribution, under repeated sampling, of the t-statistic in the standard case vs with the above stopping rule (red) (Figure 9 ). We get bumps in the tails with the above stopping rule because, under repeated sampling, some proportion of trials which have p > 0.05 will be replaced by trials in which p < 0.05, leading to a redistribution of the probability mass in the t-distribution. This redistribution happens because we give ourselves more opportunities to get the desired p < 0.05 under repeated sampling. In other words, we have a higher Type I error than 0.05. It would of course be reasonable to take this approach if we appropriately adjust the Type I error; but this is not standard practice. Thus, when using the standard frequentist theory, one should fix one\\'s sample size in advance based on a power analysis, not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant. Of course, if your goal is only to get a significant result so that you can get your article published, such stopping rules will give better results than fixing your sample size in advance! 6 Multiple measures, multiple regions, degrees of freedom in analysis Gelman and Loken (2013) point out that there are in general too many ways to analyze the data: from the choice of the statistical test to decisions on what data points to exclude or include. This means that once we start looking hard enough, it is possible to find a significant difference and then tell a post-hoc theory that fits very neatly together. For that to happen, it is not necessary that a researcher would go on a \"fishing expedition\" (Gelman, 2013) , that is, it is not necessary that he/she would be actively trying to report any comparisons that happen to yield a p-value lower than 0.05; in many cases, the researcher just has too many degrees of freedom in the analysis and it is not clear which is the right way to analyze the data. A common example is to decide to report the result of a linear mixed model or an ANOVA or t-test, depending on which one of these yields a p-value below 0.05. Researchers often flexibly switch between linear mixed models and repeated measures ANOVA to tell \"the best story\" they can given the data. One problem here is that using the ANOVA or t-test where a linear mixed model with crossed subject and item random effects is suggested by the design artificially reduces the sources of variance (through aggregation), with the result that effects that are not really statistically significant under a linear mixed model end up being significant once one aggregates the data. But the other problem with shopping around for the test that gives us the lowest p-value is the one that Gelman and colleagues point out: we are introducing a degree of freedom in the analysis. Another commonly seen situation is flexibly analyzing different regions of interest (often aggregating them post-hoc) until a p < 0.05 result is found; for example, in Badecker and Straub (2002) , in their results section for experiment 5, they write: \"No significant differences emerged for any individual words or two-word regions. However when reading times are collapsed across the four positions following the reciprocal, reading times for this region were 48 ms longer in the multiple-match than in the single-match condition. . . \". This is an example of failing to find an effect in the region where it was expected a priori, and then trying to expand the regions of interest post-hoc. Similarly, even when studying the same research question, researchers will sometimes trim the data, and sometimes not.',\n",
       "  'Running till significance is reachedWe present three possible solutions to these problems. Linear mixed models can solve the multiple comparisons problem if all relevant research questions can be represented as parameters in one coherent hierarchical model (Gelman et al., 2012) , since the point estimates and their corresponding intervals are shifted toward each other via \"shrinkage\" or \"partial pooling\" (Gelman et al., 2012) . However, building a single hierarchical model that addresses all the research questions is not always trivial. For several regions of interest, it may be possible to fit a single model using Helmert contrasts (as in Nicenboim et al. (2015) ). This type of contrast compares each region with the average of the previous ones, such that it is possible to discover a change in the pattern of the effects. However, it is unclear if the effects should appear for all the trials in the same region, since some participants in some trials could start a certain process sooner predicting the structure of the item or could delay it due to fatigue, lapse of attention, or because they have not finished a previous cognitive process. Linear mixed models that can address the multiple measures in eye-tracking or the highly multidimensional data of EEG are even more difficult to specify.'],\n",
       " ['RQ1: The PreyFor example, Pair 4 and Pair 5 did not ask any Why or Why didn\\'t questions at all. Instead, they made remarks like: Pair4-P7: \"the Zerg is doing what they normally do.\" Pair4-P8: \"[The agent is] kind of doing the standard things.\" Pair5-P10: \"This is a standard build.\" 4 We did not count the number of shoutcaster comments that answered this question because we could not narrow them down in this way. That is, although many of their comments could be said to be applicable to this type of question, the same comments were also applicable to more specific questions. However, in cases of the unexpected, a fourth What prey pattern arose, in which participants questioned the phenomena before them. We counted 9 What questions of this type: Pair9-P17: \"...interesting that it\\'s not even using those.\" Pair10-P19: \"I don\\'t get it, is he expanding?\" Pair10-P19: \"Wow, what is happening? This is a weird little dance we\\'re doing.\" Pair10-P20: \"<when tracking military units> What the hell was that?\"',\n",
       "  \"RQ3: The Decisions and the CuesIn the RTS domain, players and intelligent agents make thousands of sequential decisions, and there is a paucity of literature that considers humans trying to understand AI decisions in such a setting. (A notable exception is McGregor et al. [25] .) There is, however, literature that starts with the AI's perspective: instances of its decision-making system components (i.e., neurons) that are interpretable by humans [44, 45] . In contrast, here we wanted to start with the human's perspective and the foraging paths that result from it: namely, how participants would identify behaviors that were not only potentially human interpretable, but also of interest.\",\n",
       "  'RQ3: The Decisions and the CuesInterestingly, participants had trouble with distractor cues even when the number of events competing for their attention was very low. For example, in the early stages of the game, players were focused on building economies and scouting. There was little to no fighting yet, so it was not the source of distracting cues. We were not surprised that the Expansion event at 13:45, when the game state had hundreds of objects and events, was the most often missed (5 instances). However, we were surprised that even when the game state was fairly simple -such as at 1:30 where the game had only 13 objects -participants missed the Expansion events. The extent of 5 Table 7 shows Pair 4 also finding eight Expansion decision points, but one of those is about the commitment to expand, based on building other structures to protect the base, rather than the action of building the base itself. 6 Reminder: Cues are signposts in the environment that the predator observes, such as rabbit tracks. Scent is what predators make of cues in their heads, such as thinking that rabbit tracks will lead to rabbits. distractibility the partipants showed even when so little was going on was beyond what we expected.'],\n",
       " [\"MOTIVATION AND RESEARCH QUESTIONSA guiding value of our work was to maintain the cultural authenticity of DotD in the game world. However, in designing a culturally faithful account of the festival, we faced challenges. One such challenge arose from the primary purpose of our game, which was to strengthen literacy. The world setting played a secondary role of support in motivating learners to continue playing the game. This meant that as designers we could not privilege a rich cultural exploration of the DotD. Such narrative constraints are inherent to serious games. Socialdrome, for example, aimed at developing children's social skills acquired in a fictional island called Cascara. While the island provided the context for numerous learning activities, its role was to motivate the learning goal of the game [23] . Thus, our first research question was: can cultural authenticity in game design be maintained by presenting children with a narrow perspective of the festival that retains its core message and rituals?\",\n",
       "  \"Co-Existence of Cultural Authenticity and AppropriationWhen the children were asked to engage with new perspectives on the supernatural, they drew on cultural knowledge that the researchers had given them. We found that children could not easily conceive how the dead and the living communicate. Lacking their own points of reference, they strongly relied on the cultural rituals and customs of the DotD for proposing how this could happen. Our findings echoed one of the concerns associated with foreignizing as a localization approach, namely, that it led to the exoticizing of particular details [5] . In our case, those details concerned being pulled into the world of the dead, which encapsulates only a small part of the DotD. At the same time, we did not find much evidence that the other significant disadvantage of foreignizing, cultural alienation, was taking place. While children heavily relied on our descriptions of DotD rituals, they simultaneously utilized the rituals as a vantage point for understanding the meaning of the festival. Indeed, the festival's emphasis on remembrance, intimacy and acceptance were understood and enacted by most of the teams. In directing children's construction of stories by providing the purpose and outcomes of the festival, we were able to maintain some cultural authenticity. In drawing this conclusion, however, it is important to recognize that the workshops were run with participants from an ethnically diverse school. Children's pre-existing exposure to cultural diversity might help to explain why they were open to exploring the rituals and narratives of other cultures. Nevertheless, in line with our first research question, our findings are encouraging for games such as our own where narrative exploration is not an end, but serves as a means for motivating another primary objective. We thus argue that game narrative can communicate cultural authenticity as long as it retains components that exemplify its core cultural values.\"],\n",
       " ['INTRODUCTIONScholars in the (Digital) humanities are active and motivated annotators [24] . They annotate all types of media at any level, with many layers of interpretation. Unsworth [21] identified annotating as one of the \"scholarly primitives.\" In the case of audio-visual (AV) or time-based media, manual or semi-automatic annotation of the media content is essential, given the fact that providing fully automated access is more challenging than to textual resources [15] . Scholarly work is also often described as a process [11] , where different stages occur over time (e.g., [4] ). Providing support to scholarly research requires the analysis of the complex research tasks where knowledge construction is involved. These are not limited to searching and retrieving a list of results, but also other series of scholarly primitives, e.g. classifying, linking [20] , comparing, sampling, illustrating, annotating [21] , or writing and collaborating [18] . However, little is known about how those complex tasks are performed in the context of scholarly research where AV media is the focal point, for instance, in media and communication studies. In addition, there is a lack of system support for the different Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. methods that this group of researchers uses in different research phases [9] . To understand how annotation should be supported along the research process of media scholars, different investigations are conducted within the CLARIAH project 1 , with a focus on research behavior, activities, models, and \"tool\" analyses. As part of these investigations, this paper presents two user studies about scholars\\' research processes, with a particular focus on the types of annotation-related activities. The research problem above results in the following research question: How is annotation of time-based media done in practice by media scholars, and other scholars who make intensive use of AV media, and in which stages of their research process is it used and how?',\n",
       "  '(Re)search as processDetailing the process of scholarly research is done either via \"prescriptive\" models used in textbooks, or discovered via empirical studies of researchers\\' behavior. Case [3, p. 222 ] lists a \\'classic\\' view on these stages, which typically start with imagining a research question (1), followed by determining what data are needed and designing a specific study to collect it (2), choosing and implementing research methods (3), analyzing and interpreting observations (4), and considering the overall results (5). The exact nature of these stages, however, is usually not as straightforward, and may vary across disciplines. Media scholars are at the intersection of humanities and social sciences [11] . The research process of social scientists is described in \\'prescriptive\\' models, for instance, Kendall [12] , who details a series of \"steps\" which vary depending on the approach (inductive or deductive). Common steps in the two approaches include: problem definition, literature review, research design, data collection and analysis. In the humanities, on the other hand, the research process cannot necessarily be captured in a sequential model [11] . However, there is current empirical evidence that there are broad phases in the research process of media and communication scholars [2] , and of media scholars using web archives [9] . Bron et al. [2] explored this issue by investigating a group of twenty-seven media studies researchers. The authors found common parts in the research processes, summarized in three phases with different associated activities: exploration (studying background material, developing initial research questions, initial information gathering), contextualization (revising research questions, gathering material or selecting data with a focused purpose) and presentation (organizing the data and selecting appropriate evidence to build up a case). Marsden et al. [13] briefly discuss a schematic view of the research process with AV materials in a number of humanities disciplines, which included annotation.',\n",
       "  'Presentation (A) (i) (ii) (B)Find keywords (2) Find/create Datasets (8, 9, 11) Figure 1: A process model of scholarly media annotation (numbers in parentheses correspond to Table 1) 4. FINDINGS Annotation activities in media scholarly research Table 1 summarizes the 21 activities identified in our first part of the analysis. We noticed that all participants indicated a number of activities centered on the creation of a corpus and its analysis. In terms of system support, for instance, while exploring a digital collection, scholars indicated the need for support in bookmarking, selecting groups of items after using faceted search or other filtering options, memory features such as query history, selection features for AV media fragments, and the addition of manual annotations, such as comments or tags. During analytic activities, even though the participants used different terms to explain their analysis methods, they often mentioned \\'coding\\' or \\'thematization\\'. In this stage, they also indicated desired functionalities and/or interface features related to refined segmentation, audio transcription, automatic image/audio analysis, fragment summarization, encoding, structuring relationships between coding terms, and linking back to original sources. Some scholars described that the analysis stage included a preparatory phase in which they defined their coding schemes, and/or made their data ready for analysis (e.g, by transcribing it). Our findings also confirmed that media and AV-centered scholars do not annotate time-based media in isolation, but in relation to other media (e.g., scripts, reviews, promotional materials, etc.). We noticed similar annotation activities as those supported by specialized qualitative data analysis software (QDA), see for instance [5] ). Table 2 shows the actual order that was followed by each researcher to accomplish the research project (the numbers refer to the activities in Table 1 ). This confirms previous studies about the lack of a strict sequence in the research process of humanists, and media and communication scholars (similar to Ellis\\' study of social scientists, as cited in Case [3, p. 291] ). Instead, research activities seem to be recurrent in the different research stages, but with a different degree of refinement. This is especially clear in the case of adding codes or tags to items or fragments, which is more loose in the initial exploratory stage, and more focused in the analysis stage. Likewise, some of the features above were suggested to have different purposes at different moments. For example, in initial research phases, visualization was used for exploratory data analysis. During the analysis, however, visualization was used to facilitate A model of the annotation process We grouped the 21 activities into categories, depicting them in a graphical conceptual representation, shown in Figure 1 . The categories include: the main generic research activities in the research process (a), also represented in (A) in the form of research phases. For representing this part, we departed from the model in [2] . However, in our data we observed that a great proportion of the activities was related to data analysis. Therefore, we extended the original sequence in Bron et al. [2] by adding \"analysis\" as a separate research phase. We also renamed the \"contextualization\" phase in Bron et al. [2] to \"Assembling\" to be more specific. The most important part of the figure, (B), shows the annotation-related activities, which we grouped into phases, from pre-focused annotation to the creation of new information objects. We used the concepts of \"pre-focused\" and \"fo- studies 1,3,9,11,7,12,13,17,18,19,21 cused\" from [22] to name the annotation-related stages. Indeed, previous research has found that scholars usually create their own set of semantic categories [25] , which correspond to their research questions. We grouped the other research activities in four layers: (i) connected to information seeking and searching (not necessarily system-mediated), (ii) broader stages related to the corpus, and (iii) more specific data processing related activities around the creation and annotation of those corpora.'],\n",
       " ['Performance Impressions and Attractiveness (RQ3)The correlation analysis of attractiveness attributes and performance impression yielded unexpected results (Table 6 ). It was observed that attractive was not significantly correlated to performance impression, while friendly (r = −0.27) and likeable (r = −0.26) had low negatively correlation (p < 0.05). Dislikeable was observed to have low positive correlation (r = 0.17; p < 0.05). Given the literature on gender and attractiveness and job performance [28, 50] , we divided the sample of receptionists based on gender. It was observed that for males (N = 79) there was no correlation between any attractiveness attributes and performance impression (r ∈ [0.01, −0.05]). However, for the female receptionists (N = 90), friendly (r = −0.47) and likeable (r = −0.48) was negatively correlated to performance impression (p < 0.001), while dislikeable was positively correlated (r = 0.40; p < 0.001). This result does not conform several of the results reported in the literature of attractiveness and performance, where a positive connection was often found [23, 50] . For further discussion, refer to Section 6.3.'],\n",
       " [\"DISCUSSIONWhen broadcasting as on Twitter, past research shows that most people post messages about themselves rather than sharing useful information [23] . Based on results in communication around tuning messages for the audience [19] and recent work showing that people think more about usefulness for the recipient as the audience size decreases [4] , we expected people would weigh recipients' preferences more when sharing to an individual. However, both people's sharing data and their self-reports underscore the importance of their own preferences. For RQ1, the answer is clearly that sharing is more driven by people's own preferences than recipients'.\"],\n",
       " ['EVALUATION OF INJECT IN 3 LOCAL NEWSPAPERSAll 3 newsrooms used the Adobe InCopy text editor, so the InCopy version of the INJECT sidebar was made available. However, a different third-party organisation that was contracted by the 3 newsrooms did not integrate it into the editor. Instead, all of the journalists were set up to use the web application version shown in Figure 3 in a separate browser window. The evaluation investigated whether journalists in the newsrooms produced articles that were: (RQ1) more novel and valuable with support from the INJECT tool, and: (RQ2) written more productively with this support. These qualities of novelty and value mapped to the requirements for news to be surprising and relevant revealed during a review of key qualities of news reported in 2016 [32] . It also investigated whether: (RQ3) factors such as increased newsroom autonomy, a work culture open to innovation, management support to train journalists and set up success conditions and the presence of innovative individuals [30] influenced the use and the effectiveness of the INJECT tool.'],\n",
       " ['Introduction3 Method Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005 ). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. WhereasÖstling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.',\n",
       "  'Genetic DistanceFollowing Rabinovich et al. (2017), we use phylogenetic trees from Serva and Petroni (2008) s and learn an Indo-European (IE) family their language representations. Crucially, t that the relationships found between their ations encode the genetic relationships languages. They use features based on s of POS tags, function words and cohesive We significantly expand on this work by g three language similarity measures ( §4). this, we offer a stronger explanation of what representations really represent. hod illustrates the data and problem we consider per. We are given a set of English goldtranslations from the official languages of pean Union, based on speeches from the Parliament. 1 We wish to learn language ations based on this data, and investigate the relationships which hold between the resultsentations (RQ2). For this to make sense, it nt to abstract away from the surface forms of ations as, e.g., speakers from certain regions to talk about the same issues. We therefore several levels of abstraction: i) training on s the exact same data as used by Rabinovich et al. inating from Europarl (Koehn, 2005) . a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. WhereasÖstling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.'],\n",
       " [\"IntroductionAn important part of a carpentry student's education involves trigonometry, which is needed for e.g. building roofs and bay windows. However, most carpentry students do not choose carpentry because of their interest in mathematics, but may instead find it difficult and demotivating. In this article we present a software system developed for the iOS platform using the iPhone. The system was developed for a vocational education, with focus on carpentry students and their teachers. The design process was iterative and participatory, involving students and teachers from day one [10] . The goals of the system were not fixed beforehand but emerged during the study, partly from discussion and partly from users' experiences when they tested the system at each iteration of the design process. We believe that smartphones and games can enrich mathematics teaching and have a positive effect on the motivation of students. Smartphones as educational tools is a new area for exploration, their sensors providing exciting possibilities for interaction and participation [8] . We understand learning to be a social practice where the learner is an active participator [14] . Game elements can furthermore enrich the learning process by providing motivation [7] . The study was done in collaboration with two vocational schools: Syddansk Erhvervsskole (SDE) and Erhvervsuddannelsescenter Vest (EUC); the publisher of mathematics books Erhvervsskolernes Forlag (EF); and the University of Southern Denmark. Our research question is: How can smartphones enrich mathematics teaching in vocational education? Including the question of how iterative and participatory design enriches the product. We begin by describing the iterative participatory design method and its roots, and presenting the educational game Math Mission. We describe the games designed during this study along with some of the development rationale. Then we take readers briefly through the design process: the initial field studies, the three user trials, and the final trial using naive users. User and teacher comments and our observation of game trials enabled us to improve the system in relation to usability, emerging learning goals, evaluation of the interaction with the smartphone's sensors, and gaming value.\"],\n",
       " ['MethodThe research question asked in this study is exploratory in nature. Typically, verbal protocols and/or observation techniques are used to gather exploratory data (Gilmore, 1990) . However, in this case, due to the nature of the research question, these approaches were not deemed appropriate. The coding of verbal protocols and observations allows the preconceptions of the researcher to possibly in#uence the results (Ericsson & Simon, 1984) . In this case, the researchers had many years of experience in both traditional and object-oriented approaches to systems development. (One of the researchers had 8 years of object-oriented experience.) As such, to address the research questions, the researchers chose to minimize researcher in#uence by using a cognitive mapping approach (Axlerod, 1976; Hu!, 1990; Eden, Ackermann & Cropper, 1992; Jones, 1995) . Speci\"cally, we used the self-Q technique (Bougon, Weick & Binkhorst, 1977; Weick & Bougon, 1986) . It allows participants to identify their perceptions while minimizing the potential for researcher bias during the data collection process. Due to the intensive nature of uncovering the participants\\' perceptions of the di$culties of using OO techniques, this type of approach seems appropriate.'],\n",
       " ['DiscussionThe last research question aimed to investigate the relationship between collocations and meaning. As pointed out by Webb and Kagimoto (2009) , there was a valid comparison between collocations and meaning on productive tests because, these tests had a similar format and they were not likely to be affected by any of the other tests. However, because of their different format (one was a multiple-choice test and the other was a translation test) the receptive knowledge of collocations and meaning tests may not have an accurate comparison. They found that the mean scores of both groups on the productive knowledge of meaning test were slightly higher than the scores on the productive knowledge of collocation test. However, in this study the mean scores of both receptive and productive task groups on the productive meaning and collocation tests were nearly the same. The mean scores of all participants on the productive knowledge of collocation test were 7.99 using the strict scoring system and 8.77 using the sensitive scoring system. This means that all of the participants knew 40% of the collocations productively according to the results of the strict scoring system. This rate increased to 44% in the sensitive scoring system. The mean scores of all participants on the productive knowledge of meaning test were 8.95 and 7.94, using the sensitive and strict scoring systems respectively. This indicates that the participants knew 40-45% of the meaning of collocations productively. As it can clearly be seen, the rates of productive knowledge of meaning and collocation were nearly the same in this study. This may have resulted from the treatment stage. As the collocations were encountered with their L1 meanings in the treatment and lower level learners pay great attention to the L1 meanings of vocabulary items, they could remember them as well as the collocation itself.'],\n",
       " ['INTRODUCTIONIn the software engineering community, there is a growing interest in leveraging a large collection of open source repositories-so called Big Code-to automatically infer API usage patterns from massive corpora [4, 16, 26, 30] . However, these API usage mining techniques provide limited support to help programmers explore concrete code examples from which API usage patterns are inferred, and understand the commonalities and variances across different uses. To bridge the gap, we aim to visualize hundreds of concrete code examples mined from massive code corpora in a way that reveals their commonalities and variances, and design a navigation model to guide the exploration of these examples. We draw motivation from prior work on visualizing large corpora of related documents, e.g., student coding assignments [8] , text [27, 21] , and image manipulation tutorials [17] , to pose the following research question: How might we extract, align, canonicalize, and display large numbers of usage examples for a given API?'],\n",
       " [\"The CALLAS ProjectOne of the research questions raised by CALLAS is the interpretation, understanding, and fusion of the multimodal sensor inputs. However, since multimodal data corpora with emotional content are rather rare, effort was made to create a setup, which simplifies the task of data acquisition. In one experiment, which targets the mapping between gestures/body movements and emotion, and their relation to other modalities, such as affective speech and mimics, user interaction is captured with two cameras, one focused on his head and one on his whole body, and a microphone near the head. Additionally the users interact with different devices, such as Nintendo's Wii Remote or a data glove by HumanWare 4 . To elicit the desired target emotion a procedure inspired by the Velten emotion induction method is used: first, a sentence with a clear emotional message is displayed and the user is given sufficient time to read it silently. Then the projection turns blank and the user is asked to express the according emotion through gesture and speech. It is up to the user to use own words or to say something, which is similar to the displayed sentence. Figure 2 illustrates the setting.\"],\n",
       " ['Waldo: A Marine Geophysics Sustained AggregationWaldo\\'s PG works closely with another PG as part of a long-term Sustained Aggregation throughout data collection, processing, and analysis while invoking one FO directly, and another implicitly through the actions of the Ocean Seismometer FO, in the data collection process. Waldo and his PI colleagues have a longstanding relationship to work to acquire funds and produce resources. The PIs who invoked this Sustained Aggregation and undertook the Ridge Experiment leverage their decades long relationships and their continuing desire to have their respective PGs work together to carry on and use this Sustained Aggregation, even as students within come and go as their respective research careers emerge and transform over time. In our effort to categorize this form of organizing it might at first glance be reasonable to perceive this work as a Federation entity. In our inquiry, however, there was not a formalized organizational structure in place nor a charter specifying rules for membership. Waldo and his two PI colleagues are choosing to maintain and sustain alignment of a close set of working relationships between their respective groups. The work of Martin\\'s PG invokes multiple different entities with varying degrees of planned permanence and formality of organizing to collect and analyze data to study the evolution of HIV, Fig. 4 . Martin\\'s PG conducts molecular and computational work where individuals work with physical samples (e.g., blood and plasma) to produce genetic sequences that can be analyzed computationally. This microbiological work is conducted as part of different Federation entities over time, but the dayto-day activities unfold within the context of this PG or in alignment with Intermittent Exchanges. The Federations handle work ranging from enrolling HIV infected patients in cohort studies and regularly collecting blood samples to analyzing blood samples for different phenomena and developing new vaccines. The various IEs are created when a resource is needed to accomplish a particular research activity, whether using a costly sequencing machine or using outside statistical expertise for analyses that are ultimately not sustained over time.]-> Martin\\'s PG invokes Federation, FO, and IE entities in their work collecting data. This process begins with a research scientist in Martin\\'s PG examining records kept by one of the Federation entities they work with to see if a sample necessary to address a particular research question is available and contains enough quantity of virus to make it viable and cost-effective to use. The PG member does this by examining the metadata stored by the Federation. If the research scientist proceeds with a given sample, then they will use one of the different wet lab protocols (Lynch 2002 ) that Martin\\'s PG has designed to \"work it up\" so that it can be sequenced, a point in which they invoke additional entities.'],\n",
       " ['RECIPROCITY AND DONATIONWhat is yet to be studied, and the main focus of this work, is whether these potential systematic differences in users and usages would lead to different donation behaviors on these pages and what page-level features may predict that. Consequently, our main research question is framed as follows: RQ. Do donations across Wikipedia pages vary in some systematic ways? Next we turn to the literature on charitable donations to understand how donation rates might vary across pages. Much prior research has studied why people make charitable contributions [2, 22, 40] . These works have identified a number of motivations people might have when making a donation, such as: altruism -increasing another\\'s welfare without any external rewards [6] ; impure altruism -giving motivated by increasing one\\'s positive emotional feeling or warm-glow [1, 2] ; peer pressure, authority [46] ; prestige, respect, friendship, and other psychological objectives [4, 37] ; social acclaim or avoidance of scorn [3, 35] ; image or reputation considerations [42] , increase in one\\'s self-esteem [13] , as well as income or tax benefits [13, 36] . Surveys have also been conducted specifically on Wikipedia for why people do and do not donate to Wikipedia [18] indicating important motivations such as passion for organization\\'s mission or cause. Furthermore, experiments conducted by Wikipedia itself showed that e.g., a message from the site\\'s founder, Jimmy Wales, was more likely to encourage a donation than similar messages not attributed to Wales 2 , which relates to the principle of authority mentioned earlier [46] . This body of literature suggests a number of factors that influence or predict people\\'s likelihood to donate. However, many of these factors would not suggest that rates of donation contribution would vary systematically across pages on Wikipedia. First, some of these factors may not be applicable in the context of Wikipedia banner campaigns. For example, peer pressure or friendship should not be a factor in this context as these donation solicitations are coming from Wikimedia as an organization. Similarly image motivation or reputation considerations as well as avoidance of scorn or desire to receive social acclaim or prestige should not be motivations here, as people\\'s contributions in the banner campaigns are not publicized by default (people can opt to share it with their social network pages. Prime examples are impure altruism and self-esteem motivations as there is no reason why a particular page would trigger a stronger positive emotional feeling or warm-glow after donation than any other page. Similarly the appeal of authority, such as a Wikipedia\\'s founder, should have the same effect regardless of the page on which it is presented, as this authority represents and promotes Wikipedia as an entire organization. Same is true when considering a motivation to support Wikipedia\\'s cause or mission statement, both stay the same and there is no reason why certain pages would lead to higher motivation related to organization\\'s cause than others. Finally income or tax benefits are naturally not affected by a particular page on which a donation was given. On the other hand, one reason for donation that has been widely studied and that could predict differences in donation contributions across pages is reciprocity. Reciprocity is a social norm. Reciprocity suggests that people should respond in-kind to others: help others who help them [19] . Extended to the context of charitable giving, people may be both compelled by the reciprocity motive because \"they have benefited from the charities\\' activities in the past or anticipate the need for their services in the future.\" [13] . Prior work from both controlled experiments and field studies have demonstrated the consistency and strength of reciprocity [10, 16, 24, 29] . Research has found that given a reciprocity motive, the way to increase one\\'s compliance with a request is by increasing their sense of indebtedness. For example, in a controlled study, participants who received a soft drink from a confederate were more likely to comply with their requests [41] . Similar effects have also been extended in field studies of charitable contributions. For example, in a field study soliciting donations, experimenters found that offering a small gift increased frequency of donations by 17 percent, and that offering a big gift increased frequency of donation by a whopping 75 percent [15] .'],\n",
       " ['Utilities-Is it beneficial to use the CoDL algorithm with the hard-EM approach, which finds the best assignment of the hidden variables, as opposed to EM, which calculates the full posterior distribution? -How is the CCM approach compared to other approaches? First, we ask the question about what are the benefits of using CoDL as opposed to the standard EM and hard EM algorithms. We then compare CoDL to other approaches of encoding long distance relationships. Note that we can often design a heavily engineered and tailored model for a specific task. However, this process is challenging and time consuming, and must be repeated for every new task. On the other hand, CCMs provide an easy to use model-specification language that works for all tasks. Therefore, we compare HMM CCM to several tailored models to see if our general purpose model can match the performance of a specifically designed model. It is natural to expect that a tailored model will perform at least as well as CoDL; however, if CoDL matches the performance of a tailored model, we consider it a success. We also compared the results to recent approaches of using expectation constraints (Bellare et al. 2009 ). Among all of the research questions, the most important one is to verify whether adding constraints can improve the models or not. Again, while CCMs are not the only way to incorporate constraints, they provide a nice interface so that users do not need to invent a tailored model for every task.'],\n",
       " ['RQ3: Is the DU map useful for choosing EA parameters or components?When using the LT, usage follows a similar pattern, but the evolution of diversity is dramatically different. Here, GOMEA LT exhibits a markedly different convergence behavior compared to GOMEA RT and GOMEA RTd . Due to the high selection pressure of GOM, the pattern of nodes with a positive contribution to the fitness quickly spread in the population, and the interdependencies among those nodes are captured by mutual information. Because the LT is built from this information the hierarchical crossover masks are made to mix individuals according to these patterns. Therefore, there is a mutual, reinforced convergence of genotype and the structure of the LT itself in GOMEA LT , which results in the rapid loss of diversity. This can be clearly seen in the DU map. It should be noted that this type of diversity loss is desirable as it is the result of extremely effective mixing behavior as a result of the right patterns being present in the population and being correctly modeled by the LT. Indeed, the fast convergence of GOMEA LT does not compromise the success rate when compared with GOMEA RT and GOMEA RTd . Rather, using the LT results in the second-best performance. This means that, almost half of the times (0.4 success rate), the right patterns are present in the population and are correctly modeled by the FOS. We hypothesize that either the correct information is present in the population, and a perfect solution is quickly found, or the wrong information is modeled and GOMEA LT quickly converges to a suboptimal solution. If a larger population size were to be chosen, the performance of GOMEA LT will increase because it enables more robust learning of the salient linkage information, and the consequent propagation of the correct patterns of nodes. This result was experimentally shown on different problems by adopting a framework of multiple interleaved runs with increasing population size (and tree height) [68] . Here, we consider only a single population size however. By looking at the DU map of GOMEA LT , we can see that, on average, the population almost completely converges in roughly the first quarter of the evolution. The improvement that can be achieved during the subsequent generations is likely to be minimal. This is a key insight to improve the performance of GOMEA LT : given the same budget (i.e., evaluations or time), it is better to use a bigger population size for less generations.'],\n",
       " [\"Study HypothesesHowever, sensemaking translucence may also come with costs. Analysts may feel compelled to share preliminary thoughts, and read their partners' emergent hypotheses. This may increase the cognitive demand of the crimesolving task. On the other hand, by reducing the need for explicit verbal sharing of information, our interface may reduce the time and effort required for the task [14, 47] . There is also a potential for the suspect visualization to be distracting. Since the direction of impact is unclear, we pose a research question:\"],\n",
       " ['Conclusion & Future Work Conclusion & Future WorkWe have presented two different approaches to address learning and practice in pointing device evaluations. We think longitudinal designs are a must in such studies. Learning or practice effects clearly are not easy to come by and only longitudinal designs provide the flexibility to distinguish between learning of the device and the task. However, more research is needed regarding useful transfer and retention tasks that are able to address this issue. Longitudinal designs could also give more insight into usage strategies and how these might change over time. Besides, other influencing factors such as the motivation of users have to be considered. Since input device experiments rely on very basic tasks, they are currently very dependent on a controlled environment. Thereby, they often lack ecologic validity and so future research should also investigate how to evaluate input devices in the field using longitudinal designs (e.g. in combination with diaries [10] ). Finally, the overall goal should be to develop a framework of longitudinal evaluation methods in HCI. While the social sciences have developed a methodological framework for longitudinal research during the last decades, distinguishing between several different approaches and stating which type of research questions demand which kind of approach or method and the appropriate analysis method [e.g. 11], such a framework is still missing for human-computer studies.'],\n",
       " ['IntroductionSeveral previous authors have done preliminary investigations into the structure of language representations:Östling and Tiedemann (2017), Malaviya, Neubig, and Littell (2017) , and Johnson et al. (2017) in the context of language modeling and machine translation, all of them using multilingual data. In this work we follow up on the findings of Rabinovich, Ordan, and Wintner (2017) , who, by using language representations consisting of manually specified feature vectors, find that the structure of a language representation space is approximately preserved by translation. However, their analysis only stretches as far as finding a correlation between their language representations and genetic distance, even though the latter is correlated to several other factors. We apply a multilingual language model to this problem, and evaluate the learned representations against a set of three language properties: (i) genetic distance (families), (ii) a novel measure of syntactic similarity (structural), and (iii) distance of language communities (geographical). We investigate: the Europarl corpus (Koehn, 2005) . They employ a feature-engineering approach to predict source languages and learn an Indo-European (IE) family tree using their language representations. Crucially, they posit that the relationships found between their representations encode the genetic relationships between languages. They use features based on sequences of POS tags, function words and cohesive markers. We significantly expand on this work by comparing three language similarity measures ( §4). By doing this, we offer a stronger explanation of what language representations really represent. Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. WhereasÖstling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.',\n",
       "  'Genetic DistanceFollowing Rabinovich et al. (2017), we use phylogenetic trees from Serva and Petroni (2008) hod illustrates the data and problem we consider per. We are given a set of English goldtranslations from the official languages of pean Union, based on speeches from the Parliament. 1 We wish to learn language ations based on this data, and investigate the relationships which hold between the resultsentations (RQ2). For this to make sense, it nt to abstract away from the surface forms of ations as, e.g., speakers from certain regions to talk about the same issues. We therefore several levels of abstraction: i) training on s the exact same data as used by Rabinovich et al. inating from Europarl (Koehn, 2005) . a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. WhereasÖstling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.'],\n",
       " ['INTRODUCTIONThe use of tabletops for co-located collaborative search is an ongoing topic in HCI research [18] . Tabletops can offer diverse benefits and potentials for collaborative search such as a closer face-to-face collaboration and more equitable working style [22] , an increased awareness and better group work experience [1] , and a horizontal form-factor whose affordances are well-suited to follow-up activities (e.g. sorting, sensemaking, making a purchasing decision) [1, 18] . However, other potentials of tabletops for search are still unexplored, e.g. the use of \"hybrid surfaces\" like [14, 25] that use tangible interaction with physical props in combination with multi-touch [15] . Except a single design study for video search [11] , such hybrid tabletop interaction has not been used in search scenarios yet. Furthermore, in the light of the popularity of tabletops (e.g. Microsoft Surface) in showrooms or flagship stores, it is surprising that no prior research has focused on the obvious task of collaborative search for products in a retail environment. In this paper we therefore present Facet-Streams (Figure 1 ), a novel design for collaborative faceted product search. It uses a hybrid interactive surface that combines information visualization techniques (a filter/flow metaphor [28] ) with tangible and multi-touch interaction to materialize collaborative search on a tabletop. Thereby, unlike in most previous work, our notion of search does not mean to populate an empty workspace with the results from a keyword search. Instead we mean a process of faceted collaborative filtering of a product catalog until the amount of results is sufficiently small to review and decide [10] . Furthermore, in a retail environment like a flagship store a \"good\" customer experience with \"soft\" factors such as fun, innovative design and social experience is often valued over \"hard\" factors such as task completion times and rates. Our work has therefore been guided by three research questions: (Q1) Does our design turn collaborative product search into a fun and social experience with increased group awareness? (Q2) Can we support the great variety of different search strategies and collaboration styles in different teams with a simple but flexible design? (Q3) Can we harness the expressive power of facets and Boolean logic without exposing users to complex formal notations? In the following, we discuss related work and the specifics of our context of use. Then, we introduce Facet-Streams and the underlying design rationale. We describe two user studies and discuss their results in terms of user experience, collaboration styles, and awareness. We conclude by summarizing our results and discussing them with respect to our research questions.'],\n",
       " ['INTERMEDIATE-LEVEL KNOWLEDGEAccording to Höök and Löwgren [36] \"In the HCI field, the dominant approach to knowledge construction is to design innovative interaction schemes and to evaluate them empirically through more or less rigorous use studies.\" This can then be seen has having two major aims (i) to present the actual artefacts developed and (ii) to contribute to a more general understanding of HCI. Our assumption is that the same reasoning can be applied to CCI being a subfield of HCI. Further, Höök and Löwgren argue that HCI research mainly produces knowledge at the level of instances and theories as illustrated in Figure 1 . Artefact-centered papers can be said to present instances, which may or may not be based on theories. However, the idea behind the notion of intermediate-level knowledge is that the space in-between the instances and the theories is non-empty and can be filled with knowledge constructs that are more general than the particular instances but have a different scope and purpose than general level theories [35, 36, 42] . Some examples are Patterns, Guidelines, Methods and Tools, Strong Concepts, Bridging Concepts, and Annotated Portfolios. While guidelines and methods are well-established notions within both HCI and CCI, other intermediate-level knowledge forms like strong and bridging concepts were introduced rather recently. We therefore provide an overview of Strong concepts, Bridging concepts and Annotated portfolios since they are the most relevant forms of intermediate-level knowledge for approaching our third research question.'],\n",
       " ['Experience of Feedback DesignsTogether with the results for RQ3, these responses suggest that our users experienced the fish visualization as engaging and even enchanting (as defined by [32] ), as they offered an experience of being \"caught up and carried away\". However, users did feel that the playfulness designed into the fish sacrificed ease of use, glanceability, and peripherality, which are important in the task-related setting for which GroupMeter is designed for. They liked the unobtrusiveness of the bars, but also criticized their lack of excitement.',\n",
       "  \"DISCUSSIONWith respect to RQ2, our results show that people changed their behavior when seeing feedback. They spent more time agreeing with each other, less time discussing the brainstormed ideas when seeing the fish, and drastically decreased their disagreement when seeing the bars. The fact that GroupMeter was able to elicit changes in communication behavior is especially encouraging given previous research suggesting that people's choice of words in conversation is largely spontaneous, unintentional, and uncontrolled [9, 39] . In our study, participants were nonetheless able to effect changes in their communication patterns (e.g., agreeing) in both visualization conditions. Our results thus demonstrate the power of automated linguistic analysis for teamwork feedback in stimulating reflection on and change in behavior. This leaves open the question, however, of which specific behavioral changes are desirable. We did not explicitly pose normative guidelines, although length of bar and distance of fish from the center possibly implied that more agreements were preferred. This, in addition to the mere presence of the feedback guiding participants toward self-focused awareness, might have led them to conform more to the group [13] . If fewer agreements and more discussion are favorable behaviors [21, 27] , integrating appropriate guidelines for effective teamwork into the design is an important future direction.\"],\n",
       " [\"IntroductionThe flow of knowledge requires an effective knowledge management (KM) strategy and the mobilisation, integration, sharing, and application of tacit and explicit knowledge in a dynamic manner. However, most KM frameworks lay an emphasis on managing explicit knowledge by focussing on the processes of capture, storage, retrieval, transfer and application (Argote and Ingram 2000; Sunassee and Sewry 2002; Dyba 2003; Arling and Chun 2011) . Tacit knowledge, on the other hand, needs the key mechanisms of interaction and feedback for effective sharing and use (Polanyi 1967; Nonaka and Takeuchi 1995; Kreiner 2002; Xue et al. 2011; Margaryan et al. 2011) . Within a dynamic and holistic knowledge approach, the existing and created tacit and explicit knowledge are mobilised and integrated, and made available to collaborative team members. The need therefore exists for a KM framework which addresses the requirements to facilitate the exchange and application of tacit knowledge, in addition to explicit knowledge. The paper addresses this gap by presenting a model that makes tacit and explicit knowledge available for organisational practices and routines through the supporting mechanisms of interaction and feedback. Specifically, the paper investigated the research question of how knowledge generated during development activities can be leveraged and effectively applied to ensure long-term sustainability. The developed model makes available and accessible dynamic tacit and explicit knowledge that is applied for effective decision-making and problem-solving, and provides the long-term and continuous perspective for sustainable development and improved environmental impact. The proposed model was validated during a case study conducted at one of the world's leading software organisation which currently employs more than 250,000 individuals Dalcher 2010, 2013) .\"],\n",
       " [\"A. Research QuestionsThe main goal of this study is summarize and characterize the existing proposals in the academic literature of architectural evaluations of SoS, according to the types of evaluations and QA's mostly addressed by them. To achieve this goal, we defined the following research questions: [14] , We articulated the search string with keywords derived from the research questions. However, since the study is a systematic mapping, we followed Petersen's suggestion [13] [6] and used only Population and Intervention.\",\n",
       "  \"V. RESULTS OF THE MAPPING2) RQ2: Which quality attributes are addressed by the techniques?: We considered the QA's used for proposals validation, as well as those that could be addressed outside the validation but were mentioned by the authors. Consequently the mostly addressed QA's are: Performance (11/22 ≈ 50%), flexibility (6/22 ≈ 27%), robustness (6/22 ≈ 27%), and reliability (4/22 ≈ 18%) (see Figure 4) . Notice that operational QA's predominate over other types related with postdeployment, like maintainability, evolvability, sustainability and others. This is really interesting because in the SoS context the constituent systems must be reconfigured to collaborate and address a common goal; however, QA's related to those properties are addressed to a lesser extent. An example of systems (that can be considered as constituent systems) that claim for these QA's are legacy and SCADA systems, which 1 [18]- [39] 3) RQ3: Which kinds of validation are used by the studies reporting techniques?: Most proposals offer no empirical validation: 10/22 (≈ 45%) were illustrated with an example, whilst just 8/22 (≈ 36%) were validated with a case study (see Figure 3) . It is curious to note that quantitative proposals preferred validation via illustrations, although the proposal [23] validated through experiment was also in this category (see Figure 4) . Finally, is interesting remark that the proposal that used an experiment as a validation method focused only on one QA. within them, 18/22 are quantitative, mostly using techniques related to meta-heuristics and fuzzy logic. These types of techniques have mainly been developed to evaluate the interaction of sets of systems according to the needs and expectations of stakeholders. In the same line, but to a lesser degree, were techniques that use formal models to corroborate the behavior of the design of the architecture. On the other hand, four hybrid techniques were found, of which only one comes from the software engineering community: a mixture of two well-known techniques, ATAM and MTW, but for which no validation was found. Respect to QA's, most approaches are operational, like performance, reliability, etc. Flexibility, despite being a nonoperational attribute, was highly considered according to the evolutionary nature of SoS; however, proposals that address it from a qualitative perspective were not found. We aimed to identify existing proposals to evaluate architectures and to determine which QA's were most addressed. To this end, we designed and executed an SLM which yield articles since 2005 to 2017, with an increasing number of publications per year (see Figure 5 ). However, considering the related works mentioned in Section III, we consider that there is a gap between the most QA's addressed by the proposals found in this SLM, and the key QA's identified recently.\"],\n",
       " ['INTRODUCTIONRecent literature has described how parents go online to find social support and to overcome judgment they might experience in their offline environments [5, 6, 23, 37, 62] . However, much of that work has focused on mothers rather than fathers. Research that has addressed the role of fathers in particular examines how two SAHDs use social media, but the sample in the study was small [5] . We build on that work here to understand how SAHDs use social media related to their roles and identities as SAHDs. We explore the following research questions:'],\n",
       " ['INTRODUCTIONPrevious literature regarding this subject states that more populated cities are over-represented in Twitter, while less populated cities are under-represented [11] . Because of overrepresentation of populated places, in addition to the lack of balance in physical population distribution in Chile, content from or about non largely populated locations is lost in the timeline of tweets. In addition, it is hard to find local content, as the only salient ways to find content are to click on trending topics, which by definition are biased towards more populated cities, and by searching. But this implies that the information seeker already knows what to look for, if an information need effectively exists. As such, there is no current way to explore a geographically diverse timeline: users have the responsibility to follow diverse accounts. However, current interfaces do not allow users to see a diversity of tweets according to any criteria. Since Twitter recommends users (the \"who to follow\" functionality) and tweets (the \"discover\" tab) based on account connections and activity, users who do not have diverse connections will not receive diverse recommendations. Motivated by the situation described, in this paper we propose a methodology to address the following research questions:'],\n",
       " [\"DISCUSSIONWe aimed to investigate the effects of two individual traits on user's perception of diversity. We categorized the participants into two groups according to their MS and VM scores. The results indicate that for both users with high MS and users with high VM, their perceptions of diversity for recommendations in ComBub are significantly higher than the perceived diversity in SimBub, notwithstanding that the actual diversity of recommendations in ComBub is consistent with that in SimBub. Therefore, we could confirm the hypotheses H2, H3 and answer the research question RQ2 by finding the positive effects of MS and VM on perceived diversity. Of note, for those who have low MS, ComBub led to a significantly lower perception of diversity than SimBub. Thus, the additional audio features shown in ComBub do not seem to supply real benefits for users with low MS in terms of perception of diversity. Even the additional features in a visualization may result in Session: Personalized Recommender Systems IV & Intelligent User Interfaces UMAP'18, July 8-11, 2018, Singapore higher cognitive load and a negative impact on user experience [4] . A similarly reversed result was not found for users with low VM. The correlation analysis results show a significantly positive correlation between individual traits (MS, VM) and the perceived diversity in visualization ComBub; however, no such a significant correlation was found for SimBub. This result implies that an advanced visualization interface like ComBub allows experts to leverage their attribute knowledge to perceive higher diversity. In contrast, novices seem to prefer a simple interface that does not require intimate attribute knowledge such as the meaning of each audio features introduced in ComBub.\"],\n",
       " ['Research DataTo answer our research questions, we used two datasets: (1) user comments posted on SPIEGEL Online 1 (SPON) and (2) the \"One Million Posts\" (OMP) corpus [56] . We selected the SPON news page for two reasons. First, SPON is the most-read online German newspaper according to Alexa.com [2] . Second, the topics covered are diverse and structured in articles, forums, and comments. We collected a comprehensive sample of published user comments from 01-01-2000 to 28-02-2017 with their respective metadata and all archived articles and forums. The data collection took one week and we did not notice any changes of forum features between old and new forums. Our sample comprises 11,276,843 comments (with title, text, timestamp, username, department, and quoted comments if available), 515,522 articles (with title, introduction, text, date, and partly author names), and 181,399 forums (with title and department). Most SPON articles are signed by an acronym to state the author, while the acronyms are assigned to full names in the imprint. However, we could only identify the full author names for 16% of the news articles as many assignments were missing. Additionally, we used the partly annotated comments of OMP, a dataset that consists of 11,773 labeled and one million unlabeled German online user comments posted on DER STANDARD, an Austrian newspaper website. The authors define the annotation category \"feedback\" as: \"Sometimes users ask questions or give feedback to the author of the article or the newspaper in general, which may require a reply/reaction\" [56] . This description is equivalent to our meta-comment definition.'],\n",
       " ['B. Sharing and Reusing DecisionsWe have seen in RQ1 that the preparation of a decision requires a lot of effort. This is aligned with the findings from van Heesch and Avgeriou [21] on the reasoning process of professional architects. Despite the effort that is being spent on these activities, we found that decisions are not shared within the company but are only consumed within the context of a project. However, reusing existing decisions as a basis for upcoming decisions could significantly reduce the effort of collecting and researching information. Of course, decisions cannot be applied without a critical reflection since the contexts of projects differ. Nevertheless, decisions can be used as a source of inspiration and information (e.g. \"Which alternatives were considered?\" or \"How where they evaluated?\"). Another aspect of sharing is the ability to make decisions available for stakeholders by exporting stakeholder-specific reports. We found that this is an important use-case for architects, since decisions are often used for communication with stakeholders, e.g. during review meetings.',\n",
       "  'C. Perceived Benefits, Efficiency and FlexibilityRQ2 showed that there is only limited time available for documentation. This is supported by the study conducted by Tang et al. [19] , who identified time-pressure as one of the main barriers for documenting decisions. However, not enough time for documenting decisions can be seen as an indicator that the architects did not see enough direct benefits to make decision documentation a higher priority task. Although, all participants reported long-term benefits of decision documentation, such as avoiding knowledge vaporization, the immediate benefits of documenting decisions were not that evident. Architects only mentioned direct benefits in those projects, in which decisions were immediately used for reviews or where the project focussed on extending the knowledgebase of the company, e.g. creating architecture prototypes of emerging technologies.'],\n",
       " ['Use of Numbers.Use of Rhetorical Questions. We also found that sarcastic utterances that use rhetorical questions (RQ), especially in discussion forums (e.g., IAC v2 ) are hard to identify. Oraby et al. (2016) hypothesized that sarcastic utterances of RQ type are of the following structure: They contain questions in the middle of a post that are followed by a statement. Because many discussion posts are long and might include multiple questions, question marks are not very strong indicators for RQ. et al. (2014) showed that by providing additional conversation context, humans could identify sarcastic utterances that they were unable to identify without the context. However, it will be useful to understand whether a specific part of the conversation context triggers the sarcastic reply. To begin to address this issue, we conducted a qualitative study to understand (a) whether human annotators can identify parts of context that trigger the sarcastic reply and (b) if attention weights can signal similar information. For (a) we designed a crowdsourcing experiment (Crowdsourcing Experiment 1 in Section 6.1), and for (b) we looked at the attention weights of the LSTM networks (Section 6.2).'],\n",
       " ['Supporting effective disengagementsWorkshop discussions also hint at a need for users to be able to reflect on their experience so as to obtain maximum benefit, with reflection potentially happening years after the engagement. Partly, this then requires organizational structures that ensure that access to records of interaction can be maintained, so that individuals can return to interfaces so as to revisit and reflect on content. There is, however, an interesting research question of whether technology can specifically support personal reflection. This may touch on recent HCI work around digital souvenirs of experience [15] , the design of which has been explicitly motivated in terms of the support that can be provided for reflection on experiences.'],\n",
       " [\"INTRODUCTIONHere as a first step, we focus on Airbnb hosts in the United States as a case study. This is based on two reasons. First, although Airbnb is a pioneering example of the sharing economy, there have been few empirical studies about its service providers. Second, currently US is Airbnb's largest market [17] , and extensive SES and other characteristics data are available, allowing us to approach our questions systematically. Our research questions are important due to several theoretical and practical reasons. First, participation in the sharing economy may be more appealing to people with lower-income than to richer ones, as it may provide potential revenue stream. Several studies have already pointed out that monetary compensation is one of the main motivations for participation in the sharing economy [16, 20, 26] . Second, many sharing economy platforms greatly reduce the costs associated with joining the markets. For instance, they have provided a system to make the payment safe, have verified registered customers, and have made it easy for would-be workers to join the platform. Therefore, they may attract workers from across the income spectrum. Third, there are still entry barriers to participate, such as having an underutilized room located in a neighborhood where wouldbe guests are willing come, and having the ability to deal with issues involved in managing listings. Therefore, certain groups of individuals may not afford becoming a host. Fourth, heterogeneous housing characteristics, such as location and decoration, together with the existence of racial discrimination on Airbnb as demonstrated recently [10, 11] , may make some listings more profitable, while others not, which may affect the joining of Airbnb. On the practical side, Airbnb has repeatedly claimed that it has been employed mainly by lower-income residents as their revenue supplement, which is yet to be verified. To answer our research questions, we use data crawled from Airbnb between May and August, 2016 and employ standard regression methods to control for other characteristics, such as housing and attractiveness. Our modeling results show that after controlling for demographic, housing, and attractiveness characteristics, income has a strong negative effect on the participation in Airbnb; areas with lower median household income tend to have more hosts. We also find that education is another influential factor; areas where there are a larger portion of residents with higher education degrees are associated with more hosts. However, when we consider the performance of listings, as measured by the number of newly received reviews, we find that income has a positive effect on entire-home listings, even after controlling for listing-and host-level characteristics. This means that entire-home listings located in areas with higher income tend to have more new reviews. Our findings therefore may suggest the disadvantage of SES-disadvantaged areas and the advantage of SES-advantaged areas.\"],\n",
       " [\"IntroductionThe study presented was entangled in the last phase of a research project on supporting burglary prevention advisors in their daily work. In the burglary prevention (BP) scenario, police trained in technical security visit residents at their homes to advise them on how to secure their properties against burglary (Giesbrecht et al. 2015; Comes and Schwabe 2016a, b) . The public mandate of police crime prevention units includes, among others, promoting the implementation of crime prevention measures and enabling communities to prevent burglary cases from happening. BP advisors act upon this task; however, they often lack systematic training for it. Depending on their career, they rely on an introductory hands-on training, general police officer schooling, exposure to burglary or crime cases in their previous appointments (e.g., during patrol or investigator duties), as well as experience from previous advisory encounters, and their technical expertise. Prone to influence by the complex nature of interpersonal communication, they differ significantly in how they motivate or enable their advisees and, as this study unveils, how they shape their task during this communication. Some focus on transferring the message: Bdon't be afraid!^, whereas others exaggerate stories from criminal statistics. After the rollout of the SmartProtector, a tablet-based tool designed according to persuasive technology guidelines to support a range of persuasive practices, the differences between the various advisors emerged. Each advisor favoured a stockpile of routines (stories, arguments, explanations, etc.) which were activated based on their preconceptions and observations about the advisee, the local or situational circumstances and the private perception of the advisor's task. This study makes clear that behaviours that were originally considered a mundane part of a conversation (e.g., a story from the neighbourhood), could be recognized as an essential and routinized persuasive device. The SmartProtector was appropriated as far as it could be meaningfully applied in the routines. This sheds new light on the persuasive aspect of the work: it grows out of a range of conversational routines and is not like a debate with explicit arguments or targeted behaviours. Consequently, supporting persuasion is less about extending the persuasive arsenal with technology but rather about equipping IT with a meaning that fits the stories, explanations, and narratives the advisors used to provide, and about affording new behaviours that may turn into routines. Overall, this study proposes the picture of persuasive practices as routines originating as strong stereotypes, as well as the advisor's opinion towards ongoing organizational discourses and tensions. Transforming those practices with IT requires consideration of multiple cues about the situation and its background rather than an optimistic assumption about the improvisational character of practices. The current study arrived at those insights by pursuing the following research questions:\",\n",
       "  \"Supporting advisory services and advisory practicesCurrently, advisory services are undergoing an intensive phase of transformation. The changes are driven by technology entering the encounters, expectations of the customers, as well as by the easy, on-line access to information, which was previously available only to experts (Schwabe and Nussbaumer 2009; Dolata and Schwabe 2017b) . Due to this transformation, the main focus of advisory services moves from information provision and recommendation to joint problem solving (Dolata and Schwabe 2017b) . This is reflected in the emergence of new advisory practices (Dolata and Schwabe 2017c, a; Fischer et al. 2017) . For instance, energy advisors involve the advisee during the analysis of specific, individual data for providing a suitable recommendation rather than generic suggestions (Fischer et al. 2017) . Similarly, bank advisors try to learn and document many aspects of an advisee's life, while asking typical questions, to offer an individualized rather than a standard, off-the-shelf package (Kilic et al. 2017) . The advisory practices encompass situative, improvisational elements (Fischer et al. 2014 (Fischer et al. , 2017 , as well as routinized behaviours and conversational strategies (Dolata and Schwabe 2017a, c; Kilic et al. 2017) . For example, in a BP advisory service, an advisor may routinely suggest a specific rim door lock with a bolt and present its working mechanism with a specimen; however this behavior will emerge if the door explicitly requires such a lock or the advisor notices that the advisee uses a (potentially insecure) chain lock (Dolata et al. 2016) . Given the fact that the advisory practices combine routinized and situative behaviour, and considering the ongoing transformation of advisory services, this field offers a range of relevant research questions: What is the main point of an advisory service if not the information transfer between an expert and a layperson? How can IT support new practices that outgrow previously existing routines? How should IT take account of the situative character or situative activation of some practices?\"],\n",
       " ['DRIVING simulators are widely used for various purposes in driver training, traffic research, and automotive development [1] . When using a driving simulator, one does so to target some driving-related objectives (e.g., driver learning out-comes, traffic research questions, and vehicle design decisions) that involve the human driver as a crucial component, but where the use of a real vehicle for attaining the objectives is deemed unsafe, too costly, or otherwise ineffective. The objectives themselves, however, will typically remain focused on real driving, i.e., the use of the simulator is motivated by the assumption that the results will validly transfer to real vehicles and real traffic.'],\n",
       " ['(E)xpectation, (I)ntensity, (P)ower, and (V)alenceboth classifier and human correlations on two test recordings, averaged over three raters. Overall, our results show that obtaining a high correlation between various human raters for the audio-visual Solid SAL data is indeed challenging. To mitigate a similar problem, Nicolaou et al. [76] proposed a method for achieving high interrater agreement and segmenting the continuous sequences into shorter clips. However, extending this method for automatic, dimensional, and continuous emotion detection remains a challenge. Moreover, during annotation, the human raters were exposed to audio-visual Solid SAL data without explicitly being instructed to pay attention to any particular cue (e.g., head gestures) or modality (e.g., audio or video). Therefore, it is not possible to conclude which cues or modalities were dominant for which ratings. In general, these issues still remain as open research questions in the field [74] , [75] .'],\n",
       " ['Problem Formulation and IdentifiabilityHowever, if the true parameters were (r, p, q) = (0.1, 0.2, 0.3) with proportions of BOWs being 0.4%, 19.8%, 8.1%, respectively, it is easy to verify that the system would have multiple valid solutions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and (0.1180, 0.1841, 0.3030) . In general, ifp(x) is known from the training BOW corpus, when can we guarantee to uniquely recover the bigram LM θ? This is the question of identifiability, which means the transition matrix θ satisfying (1) exists and is unique. Identifiability is related to finding unique solutions of a system of polynomial equations since (1) is such a system in the elements of θ. The details are beyond the scope of this paper, but applying the technique in (Basu and Boston, 2000) , it is possible to show that for W = 3 (including d ) we need longer documents (|x| ≥ 5) to ensure identifiability. The identifiability of more general cases is still an open research question.'],\n",
       " ['Test materialsAn annotated evaluation resource would be needed to measure the robustness of a parser against human judgments. It would, however, be a daunting task to annotate noisy texts and their corresponding correct counterparts for all four of the parsers. We therefore made the simplifying assumption, like (Bigert et al. 2005) , that a parser is robust if it is able to produce a similar analysis for a correct sentence and a noisy version of the same sentence. Our assumption is that if a parser is able to do this, it will be able to perform in a robust way when it is confronted by noisy inputs. By making this assumption, we were therefore able to perform evaluations by using unannotated texts. It is clear that as the level of noise in the inputs increases, the performance of a system degrades correspondingly. The extent to which this occurs can be measured by increasing the number of mistakes in the input sentences and observing the effect that this has on its performance. In order to investigate the effect of increasing the amount of distortion in the input, and to answer our second research question, we constructed a test corpus which contained sentences with error-free sentences and their noisy counterparts with one or more spelling errors. Our test set had three error levels: each of the noisy sentences contained between one to three misspelled words. We started the test set construction by selecting 19 sentences from a public domain web page. We then altered one, two or three words per test sentence and this gave us a total of 443 test sentences -255 with one error and 94 with two and three errors respectively. The length of each of these sentences was between 5 and 36 words and the average length was 16.32 words per sentence. We then introduced misspellings manually into the sentences by deleting, adding and swapping characters, permitting only one edit operation per word. We based character additions on the keyboard proximity of letters in order to simulate errors in naturally-occurring texts. Since our purpose was not to evaluate robustness on structurally distorted sentences, we only permitted alterations that did not create an acceptable (valid) word.'],\n",
       " [\"Visual Elements and the Emotion ProcessOf the all possible appraisal dimensions, perhaps the most frequently used scheme for describing emotion is the combination of valence and arousal [40] . Valence refers to the pleasantness of the experience and arousal to how much the emotion is associated with activation. For example, feeling calm is pleasant but not an active emotion, while feeling energetic is a pleasant and active emotion. Sadness is a negative and deactivating emotion, and anger is a negative and arousing emotion. While these two are not the only relevant dimensions of emotional experience, they are often the most salient and can be used for rich descriptions of emotion in human-computer interaction [41] . Regarding emotional experiences of pictorial representations, for instance, works of art can differ in their potential to cause arousal. Works of art which possess ability to evoke high arousal are most likely perceived as dramatic and dynamic, and works of art with low potential on eliciting arousal are generally perceived as static and harmonious [46] . This paper focuses on studying the most salient visual elements in user interfaces and their relation to emotions attributed and elicited by them from the perspective of future programmers, in order to provide usable insight for the evaluation and design of visual elements in user interfaces that promote user experience and thus to benefit usercentered visual user interface design. This study focuses on the following research questions: What are the most salient visual elements in web pages from programmers' point of view? What kind of emotions do the salient visual elements elicit? How are the salient visual elements evaluated in the appraisal process? Advances in Human-Computer Interaction were allowed to return the templates anonymously without information that could be used to identify them, because the data collection was organized as a part of a university course. Thus, the reported age and gender information is from 40 participants, while ten participants answered without identification information. However, the average age of the 40 participants did not differ much from the average age of the participants in the course: the average age of the participants in the course was 25.0 years (SD = 5.8 and range .\"],\n",
       " [\"Place Analysis (RQ2)Of all the 1,323 check-ins, 626 (47.3%) were at private places, and the rest (52.7%) at public places (Table 2) . Of the 626 check-ins at private venues, 62% were reported from their own home, 30% from their friends' home, while the rest (8%) occurred at either their workplace or other private venues (e.g., student hostel, someone else's home while baby-sitting, etc.). A large number of check-ins at private venues might be due to two factors: a) the majority of participants (83%) reported living with their parents, and b) spending a night outside is relatively costly given the demographics and income earning status of participants. After manually browsing through some of the videos taken in private places, we found that some videos indeed show young people in large family homes (with a large living room and kitchen), but also studios and For check-ins at public places, 30% were at bars, followed by 27% at PBS (including public parks, lakeside, etc.) as shown in Figure 3b . Restaurants and travel each contributed around 10% of all check-ins. We observe that a significant portion of check-ins happened at PBS, which suggests that youth spend a considerable amount of time hanging out in these spaces away from mainstream nightlife areas. This provides support for the qualitative work done with youth in the US context [2] . The PBS videos are specially interesting as they are unfiltered. Videos in dark parks or squares where people hang out, and video taken on streets outside commercial venues, are commonly found in our data. These places also provide support for qualitative work on the practices of Swiss youth in these venues [8] . Note that both Lausanne and Zurich have a scenic lakeside used often for recreational activities. Overall, these findings reflect that the study indeed captured different patterns of participants' nightlife behavior. Figure 3d . In the inset of the same figure, we plot the overall 4SQ check-in distribution (i.e., without any temporal filtering). For both the temporally filtered and overall distribution, food places receive the most number of checkins, which is in contrast with our study findings. Similar to previous work [5] , we observe that places visited during night are more represented in our crowdsourcing study compared to Foursquare e.g., events category did not contain a single check-in for temporally filtered 4SQ data. For some of the categories, the check-in distribution of our study (Figure 3b ) is similar to the temporally filtered 4SQ check-ins; however it is significantly different with respect to the overall 4SQ check-in distribution. These findings point towards limitations of social media in terms of representativeness and temporal resolution at least in the context of Switzerland [45] .\"],\n",
       " ['INTRODUCTIONKruger [7] provides a detailed survey of robots being used in assembly lines. Several collaborative robots have been developed for work environments [3, 10] ; however, till date there have been no mobile robots which work with humans on automotive assembly lines. Our work is aimed at developing a mobile robot which can work along side humans in automotive assembly lines. Here, we highlight the key research challenges faced in developing this system along with obtained solutions and open research questions.'],\n",
       " ['INTRODUCTIONAs summarized in Table 1 , predictive physics simulation is not a new concept, and it has been increasingly utilized by recent artificial intelligence (AI) and animation research (e.g., [11, 32] ). However, research has focused on nonplayer characters or enabling simple high-level control of a simulated avatar, e.g., making a bipedal character walk in a desired direction. This has obvious applications in action games such as God of War [27] or Uncharted [20] , which presently need vast quantities of animation data to enable such high-level control. However, there are also various games such as Angry Birds [25] and QWOP [2] , where the player directly controls simulations on a low level. In such games, predictive simulation seems less explored. This observation prompts our primary research question: What novel possibilities and challenges emerge from combining predictive simulation with low-level direct control of simulated characters?'],\n",
       " ['Research Question 1:However, informal or unplanned meetings between people separated horizontally into different subgroups of the formal organizational structure may be more affected by space [11] , which would present problems for communication for inspiration. In investigating our next research question, we proceed to test whether this is the case.'],\n",
       " [\"DiscussionIn answering our research questions about the effects of personalized scaffolding, we found some evidence for the fact that effective learning goal recommendations can scaffold self-directed learning and increase task performance. Recommending learning goals, either adapted to the users' task through the domain model (fixed scaffolding), or in addition adapted to the users' skills by taking into account the user model (personalized scaffolding), resulted in statistically significant effects in relation to the control condition for two of the dependent variables (task performance and perceived support). For both dependent variables, not only were the results of statistic significance, but also highly significant from a practical perspective. As to the performance, participants in the experimental conditions solved three times as many exercises correctly as in the control condition. In terms of perceived support, the difference between the means on the measurement scale was larger than 5.8 points which equates to a mean difference of over one point on a four point Likert scale. An important question is why the two experimental conditions did not differ with regard to these dependent variables. Clearly, we were not able to produce as strong effects with the recommendations based on the user model as human tutors were able to (e.g. in [12] ). Of course, a human tutor might be more sensitive to the exact feedback to give, tutors also serve additional functions (like motivational support), and in the Azevedo case, the tutor was also asked to only provide procedural and strategic scaffolding. When looking at number of learning goals selected in our study, however, there was a small but significant effect in that personalized scaffolding led to a smaller number of selections than the other two conditions. We take this as promising evidence that personalization was successful (as it reduced search), but that it did not produce effects on task performance or perceived support.\"],\n",
       " [\"INTRODUCTIONThe research question we sought to answer with this work is whether it is possible to build a memory system for human-robot interaction in a data-driven way. To our knowledge, there have not been any previous data-driven, behavior-learning systems designed and evaluated for the purpose of modeling memories in human-robot interaction. The most similar work to our proposed method is context-sensitive recurrent neural architectures [13] [14] [15] . But, they have only been applied to unimodal interaction data (dialogs) without automatic speech recognition errors, which are common in HRI. Furthermore, we go a step further than training a testing a model (Section 6) by first analyzing the types of memory-dependent behaviors that occur in conversation that helps to resolve ambiguous customer questions. However, the topics modeled there are at a course level, and only the most recent topic is considered in deciding the android's action. However, our system can respond to specific actions that occur anywhere in the interaction history.\"],\n",
       " ['Question Q5: evaluating the clusterings on internal criteriaIn this section, we will investigate the trade-off between the ARI and two internal measures: the silhouette index (SI) (Rousseeuw 1987 ) and the density-based cluster validation (DBCV) score , both of which were also used in answering the previous research questions. The SI was chosen as it is well-known, and the extensive studies by Arbelaitz et al. (2013) and Vendramin et al. (2010) identify it as one of the best performing measures. The DBCV score was chosen as it is one of the few internal measures that does not have a spherical bias, and instead is based on the within-and between-cluster density connectedness of clusters. Although it does not have a spherical bias, the DBCV score comes with its own limitations; for example, it is strongly influenced by noise, and biased towards imbalanced clusterings (Van Craenendonck and Blockeel 2015) . Both of them range in [−1, 1], with higher values being better. Figure 4 shows how well the semi-supervised methods score on the internal measures for six datasets. In most cases, COBS performs comparable to its competitors. A notable exception is the parkinsons dataset, for which FOSC-OpticsDend produces clusterings that score significantly higher on both the DBCV score. Interestingly, the ARI of these clusterings is near zero. For parkinsons, the clusterings with the highest ARI score low on the internal measures. This, however, does not necessarily imply that the clustering does not identify any inherent structure (although this can be the case), it only means that it does not identify structure as it is defined by the silhouette score (i.e. spherical structure) or the DBCV score (i.e. density structure).'],\n",
       " ['Research MethodWe applied grounded theory [21] to analyse our data. Grounded theory is a systematic qualitative research methodology in the social sciences that emphasizes generation of theory from data gathered in the process of conducting research. Due to the small sample size and choice of projects studied, we cannot provide a theory. However, we used this systematic approach to handle the richness of the qualitative data we gathered. The main steps in the analysis were coding, memo writing, axial coding, sorting, and writing of findings. After each interview, we conducted open coding, meaning that we indexed the whole interview with codes as they came to mind, independently of previous findings. Memo writing served to condense the findings and identify the first results. Then, concepts and categories emerged from the codes. In the middle and at the end of the interviewing phase we conducted axial coding. In axial coding we looked through all interviews and codes and recoded all interviews such that the emerging categories were well covered. At the end of the process, we had 175 codes and 9 families. Some codes were too specific and did not belong to any family. The codes were then sorted graphically to establish which codes belong together and how they influence each other. We do not show the codes for spatial reasons. We describe the results in the main part of the paper, see Section 6, on the ground of three main categories: work context (research question 2), motivation, and work practice (research question 1) of each role. Then we indicate the strategies that employees used to overcome the obstacles in their daily work (research question 3). In the end we analyse the impact of the different factors on user-centred design.'],\n",
       " ['RESULTSIn the second research question, we want to evaluate the efficacy of the algorithm from human perspective. On this regard, we require annotations on the accuracy of each timeinstances. However, this is a huge task as there are as many as 50 time-instances for each pattern and 5 patterns per video. It was impractical to ask the participants to annotate the accuracy for each instance in the video. We solve this problem by recruiting thirty workers in the Mechanical Turk 2 website. In order to ensure high quality in the answers, we accept the turkers who completed at-least 1000 tasks with 99% acceptance rate. In addition, we perform a qualifying round; where we manually selected 30 turkers based on their performance on annotating a ground truth. These filtering and qualification round techniques were performed in light to the work of Mitra et al. [20] .'],\n",
       " ['DiscussionMoving from static to dynamic sensing involved a process of unearthing the infrastructure, both conceptually and, in some cases, literally. By pulling the sensors out of the ground (or wherever their static installations were located) and providing ways to move them through the environment, CENS research shifted from static to dynamic infrastructure development. We contrast our characterization of CENS researchers as unearthing their infrastructure with Bowker\\'s (1994) notion of \"infrastructural inversion.\" Infrastructural inversion is a methodological move that involves examining the infrastructural changes that preceded or accompanied a particular claim. CENS researchers adjusted their views about the components and configurations of their sensor technologies because of the difficulties encountered in reliably deploying static sensor networks. It would be a stretch, however, to say that CENS researchers themselves performed an infrastructural inversion because the changes in assumptions and technologies that occurred in CENS were the result of years of iterative technology development, testing, and refinement with particular scientific research questions as goals. It is certainly true that CENS technology development, both the processes and the products, changed significantly over the life of the Center. If infrastructural inversion does not encapsulate that change, another concept, or set of concepts is required.'],\n",
       " ['Objectives, rationale and research questionsQuestions like these mark out the domain CSR projects are expected to explore. They provide the rationale for the structure of the CSR research agenda depicted below (cf., Figure 1 ). This structure takes account of a dictum ascribed to Max Planck: \"Knowledge has to precede application\" (Gruss, 2002) . However, it also takes account of the fact that research with a view to supporting engineering must not be confined to an ivory tower. Rather, it should be motivated by and cater to real needs, in line with the strategic goals of (public) European research funding. The two boxes at the bottom of the above diagram represent a wide range of possible scenarios that may provide this motivation, and also guide and validate the research effort proper. In fact, most CSR projects are committed, one way or another, to some concrete application scenario pertaining to one of these boxes.'],\n",
       " [\"INTRODUCTIONOver the last 20-30 years children have become important users of technologies, such as the Internet, smart phones, and iPads. However, in 2002 Druin [5] wrote that 'a child's role in the design of new technology has historically been minimized'. According to Druin this was caused by several factors, such as getting access to children who attend school all day, existing power structures, biases and assumptions between adults and children, and young children's difficulty in verbalizing their thoughts. Since then, much has happened, especially through journals and conferences dedicated to this topic, such as the International Journal of Child Computer Interaction and the Interaction Design and Children conference. However, while clear advances have been made for typically developing children, it may be less common to involve developmentally diverse children in design. In this paper we present a literature review of how this group of developmentally diverse children has been included in the design process, focusing on their role and the methods and techniques used. More specifically, we aim to answer the following research questions:\"],\n",
       " [\"Contrasting the Knowledge Bases (RQ1)Related work suggests that with adequate support the output of non-professionals can be improved to match that of the professionals in many cases [5, 21, 28] . In our study, the best treatments by non-professionals were assessed very similarly to the best by professionals. We find this remarkable, given how the professionals' treatments were carefully curated by a group of seasoned LBP professionals with decades of practical knowledge from the field. The low-performing treatments by non-professionals, however, were clearly assessed lower than the worst professionals' treatments (Table 4) .\",\n",
       "  \"Contrasting the Knowledge Bases (RQ1)Crowdsourcing literature suggests that laymen's contributions, when augmented and filtered in novel ways, can often replace the need for experts [27] . In our case, however, the professionals were quick to point that the treatments by non-professionals need to be scientifically validated before being even considered to be included as treatment suggestions in any kind of national authoritative guidelines. This type of manoeuvring between medical liability and practical help has been also identified earlier as one of the key challenges in medical peer-support systems [15] . There must exist a clear division between official treatments by clinical professionals and our crowdsourced treatment database accessible via Back Pain Workshop.\",\n",
       "  'Contrasting the Knowledge Bases (RQ1)In addition to simply providing \"good treatments\", the nonprofessionals\\' knowledge base is valuable in other ways as well. Based on an informal content analysis, the treatments in the non-professionals\\' knowledge base were not as medically exactly articulated in nature as in the professionals\\' one. They also at times included typos, bad grammar, and upper-case text. However, textual nonprofessional descriptions on which treatments seem to work can be used by professionals in learning about the realworld experiences of potential patients [20] .'],\n",
       " ['Personalisation vs composite modelsAs machine learning involves the use of at least one of these approaches, all works highlighted in Table 6 provide a basis for addressing this research question. However, we note that the number of comparative works is limiting (e.g. Pejovic and Musolesi [43] compare learning from a combined set of unordered cases and personalised ordered cases). Additionally there has been little empirical investigation into a framework for facilitating a hybrid approach to training data, where any complexity and accuracy trade-offs could be improved upon.'],\n",
       " [\"Collaboration as negotiating and sharing meaningAgreement on word meaning and reference is a complex process (Clark 1992; Clark and Wilkes-Gibbs 1986) . First, the involved parties implicitly identify differences and lack of agreement if it comes to a meaning or a reference. Then, they locate the disagreement in a series of turns. And, finally, they engage in a recursive process which ends up with a mutual acceptance of a word's meaning (Clark and WilkesGibbs 1986) . This shows that establishing a meaning of a word is a highly collaborative process which involves communication and coordination, as well as negotiation if such is necessary (Clark 1996; Clark and Wilkes-Gibbs 1986) . As explained, those processes can become more complicated if an external actor or IT proposing its terminology gets introduced into the setting -the number of potential candidate references and meanings grow which makes the identification, location and negotiation of a disagreement more complex. However, visual support can also make conversational coordination more efficient (Brennan 2005) . IT, if carefully designed, can even take the role of a boundary object (Star 2010; Star and Griesemer 1989) , i.e., a computer can provide room for flexible interpretations, such that each group of users sees them as coherent with their own environment, while keeping a fixed and unambiguous core. While collaborative technology has been proposed as a boundary object in multiple scenarios (Henderson 1991; Lee 2007; Star 2010) , it is necessary to delineate the original concept from the notion of technology proposed in this manuscript. A boundary object allows members of different groups to read different meanings specific to their needs from the same material (Henderson 1991 ) -the meanings they identify do not need to be identical, but form a common denominator such that work on the individual or collaborative tasks can be continued (Lee 2007) . However, this can exactly pose danger to the effect of a service encounter: if an advisee accepts his own, inadequate vision of a concept which is not equal with the advisor's one, and does not negotiate it, he may fail in the implementation phase. In other words, while research has identified potential of technology for sharing core meaning across groups of users, no conclusive answer can be given on whether IT use will make vocabulary transfer and entrainment easier (due to its boundary character) or harder (due to its role as a third collaborator with its own lexicon). Thus, the research questions we ask in the current study are neither trivial nor irrelevant.\"],\n",
       " ['B. Connection to Psychologyobserver were willing to provide examples of what they ct, the robot could learn how to act via Learning from onstration [24] - [26] or Inverse Reinforcement Learning [29] . Doing so in a high-dimensional space, however, is an active area of research. cond, the robot must find a trajectory that minimizes C. is tractable in low-dimensional spaces, or if C is convex. e efficient trajectory optimization techniques do exist for dimensional spaces and non-convex costs [30] , they are ct to local minima, and how to alleviate this issue in ice remains an open research question [31] , [32] .'],\n",
       " [\"Correlation of CAF in writing abilityThe main (quantitative) finding obtained with regard to the second research question is that every CAF domain improved at the group level based on stanines. CAF components for writing ability were also highly correlated with each other suggesting that they are interconnected. Particularly, fluency is the most strongly correlated with proficiency. This finding is consistent with the one obtained by Larsen-Freeman (2006) who reported an increase in every CAF domain as proficiency improved. It is also partly consistent with Robinson's (2001 Robinson's ( , 2005 cognition hypothesis which states that it is not only possible but also natural that complexity and accuracy receive concurrent attention from the learner. However, contrary to Robinson who states that fluency develops separately, this study found that fluency increased in tandem with complexity and accuracy.\"],\n",
       " [\"INTRODUCTIONApart from the product and process approach, which has been widely used to teach writing, the genre-based approach (GBA) has also been proposed. There are three schools or models of genre each focusing on different concepts: English for Specific Purposes (ESP), Systematic Functional Linguistics (SFL), and the New Rhetoric (NR). However, this study focuses only on the Systematic Functional Linguistics (henceforth SFL) because the concept and details of the SFL genre meet the content of the target course adopted in this study. A review of the literature reveals that numerous studies have employed the GBA (SFL) to equip Thai tertiary students with needed writing skills (Krisnachinda, 2006; Kongpetch, 2006; Chaisiri, 2010) . These studies have reported that the students' writing ability has been improved and they also have developed positive attitudes toward the GBA. However, most research in Thailand has been conducted with English-major students who usually have quite good command of English language skills. This study investigates the writing achievement of undergraduate engineering students for whom the writing skill is also necessary not only for their academic but also their professional success. Beer (2005) argues that over 40% of the work time of engineers is spent on writing and ranks the ability to write as the most important skill in engineers' success. For the Thai context, teaching engineering students is seen as a challenge because many of them have poor English background knowledge. Most of them study only a few English courses in the first and second academic years, as evidenced in the findings of Wattanasakulpusakorn (1996) , who found that undergraduate engineering students had limited writing ability and were unable to pass writing exams. Thus, it was interesting to employ the GBA to determine if this particular approach could help improve the engineering students' writing ability. Therefore, the present study aims at answering the following research questions:\"],\n",
       " ['Forecasting doesn\\'t work (RQ3)1-week and 2-week forecasts perform poorly (r 2 = 0.41 and 0.51, respectively). In addition to greater noise, the forecasts show a tendency for spikes (e.g., January 2014 in Figure 6b ) and an \"echo\" effect of the same length as the forecast horizon. We speculate that the latter is due to training data that includes summer periods when phase does not matter; when applied to times near the peak when it does, the model cannot compensate. These results contrast with prior work. For example, we reported several situations with high correlations between Wikipedia article traffic and official data under a forecasting offset [57] . However, this work had no independent test set; i.e., it reported training set correlations, which are very likely to be higher than test set correlations. Similarly, Bardak & Tan [13] reported linear regression models whose performance was best at a 5-day forecast. This work tested a large number of algorithms and parameterizations and used internal crossvalidation rather than a test set that strictly followed the training, the latter being a more realistic setting.'],\n",
       " ['How do we work with found data?The study we present in this paper asks and answers our empirical research question about the extent to which bot-bot reverts in Wikipedia constitute bot-bot conflict. However, in this paper, we also frame our approach to answering this particular empirical question as an epistemological issue in computational social science: when working with large-scale \"found data\" [36] of the traces users leave behind when interacting on a platform, how do we best operationalize culturally-specific concepts like conflict in a way that aligns with the particular context in which those traces were made? In other words, how do we integrate the rich, thick descriptions of qualitative contextual inquiry in the practice of computational social science to arrive at a more holistic analysis? How can a research team both dive deep into the complexity of particular cases and scale up to the massive size of a dataset like those found in contemporary social computing platforms?',\n",
       "  \"Mathbot's disambiguation links.This can be considered a task conflict between the two bots in Hinds & Bailey's typology, although there was no conflict at all between the two bots' developers. Unlike the conflict over Mathbot, there was no fundamental disagreement about whether these references ought to appear or not that was fought through bots. Both bots were developed to cooperate with each other and their developers fully agreed with each other about what tasks ought to be done and how, but the bug caused CyberBot II to change its task from fixing references to removing them. When the issue was raised to CyberBot II's developer by multiple Wikipedians, the developer quickly disabled the bot, investigated, identified the bug, fixed it, re-enabled the bot, and wrote a short note explaining what went wrong. 17 7 COMMENT CODING 7.1 Overview The previous section gave qualitative descriptions of different kinds of bot-bot revert events, giving enough of the local context to understand to what extent they are or are not conflict. On its own, these cases are sufficient counter-examples to critique the assumption that bot-bot reverts necessarily indicate conflict. These cases also gave us a set of different kinds of bot-bot revert events, which we had classified as various kinds of bot-bot conflict or non-conflict. However, we still did not have an answer to our empirical research question: How many of the bot-bot reverts in our dataset are instances of genuine conflict like the one between AnomieBot and CyberBot II, and how many were cases of routine, even collaborative work like fixing double redirects? As we had hundreds of thousands of revert events, manually reviewing and investigating them all ourselves was not feasible. Because the work requires substantial trace literacy of the Wikipedian community, crowdsourcing approaches would also be lacking.\"],\n",
       " ['IntroductionWe evaluate two classes of computational models for logical metonymy. The classes represent the two main current approaches in lexical semantics: probabilistic and distributional models. Probabilistic models view the interpretation as the assignment of values to random variables. Their advantage is that they provide a straightforward way to include context, by simply including additional random variables. However, practical estimation of complex models typically involves independence assumptions, which may or may not be appropriate, and such models only take first-order co-occurrence into account 1 . In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem. Distributional models typically do not require any independence assumptions, and include second-order co-occurrences. At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010) .'],\n",
       " [\"INTRODUCTIONAdditionally, research has explored how technological innovations can support various caregiver populations with their caregiver responsibilities [37] , and the potential for technology-mediated social support amongst caregivers (e.g., through online support communities [87] ). However, despite the fact that caregivers' needs are interdependent [12] , prior work has typically explored the design of technologies that address discrete caregiver needs (e.g., self-care through PA or increased social connectedness). Therefore, research is needed to examine the specific barriers that AD caregivers face to wellness, and how these barriers can be addressed in concert. Our work seeks to address this research gap through a formative study exploring how pervasive games can jointly address barriers to PA and social connectedness in the AD caregiver population. Taking this approach allows us to explore the broader question of how technology can simultaneously address multiple caregiver needs; such an approach has the potential to be an effective and efficient means of enabling impactful engagements with health technologies in a population that is significantly overburdened. Our work is guided by the following research questions:\",\n",
       "  'Designing for Minimized InteractionsBeyond goal-setting an important area of health technology research is nurturing app engagement. Prior work has demonstrated that health apps often have high drop-off rates, with users quickly abandoning them [24, 26] . In response to this trend, one of the primary threads of research within health technology research has been exploring how to increase interaction with health systems. This goal has been sought in part because consistent engagement with the technology is seen as necessary to give users sufficient exposure to the technology to induce change. However, our work suggests that a different design orientation and research question is needed for the AD caregiver population, that is: what is the minimal level of engagement needed to support change? Framed differently, there is a need for work that examines the sweet spot of minimizing user interaction and maximizing benefits.'],\n",
       " ['What content attracts new participants?We explore these research questions using nine months of data from a Facebook page titled \"Valor por Michoacán SDR 1 \"(VXM), which translates as \"Courage for Michoacán.\" This page was created to inform people of \"unsafe situations\" or \"situaciones de riesgo\" (SDR) in the state of Michoacán. This type of activity has been reported in previous work that examined how residents of communities afflicted by violence used Twitter to form alert networks and help one another identify potential danger on the streets [29] . However, shortly after its creation, VXM began to focus more on reporting about the activities of an armed group in the region, the self-defense forces [12] .'],\n",
       " ['DISCUSSION AND CONCLUSIONSIt is worth noting some differences between document retrieval using a bag of words and frame retrieval using a bag of visual words: 1) because visual features overlap in the image, some spatial information is implicitly preserved (i.e., randomly shuffling bits of the image around will almost certainly change the bag-of-visual-words description). This is in contrast to the bag-of-words representation of text, where all spatial information between words (e.g., the word order or proximity) is discarded. 2) An image query typically contains many more visual words than a text query, as can be seen in Fig. 8 , a query region of a reasonable size may contain 30-100 visual words. However, since the visual words are a result of (imperfect) detection and also might be occluded in other views, only a proportion of the visual words may be expected to match between the query region and target image. This differs from the web-search case, where a query is treated as a conjunction, and all words should match in order to retrieve a document or web page. 3) Internet search engines exploit cues such as the link structure of the Web [6] and web page popularity (the number of visitors over some period of time) to compute a static rank [31] of web pages. This query independent rank provides a general indicator of a quality of a web page and enables more efficient and in some cases more accurate retrieval. For example, the inverted file index can be ordered by the static rank, allowing the retrieval algorithm to access the high-quality documents first. An interesting research question would be to develop an analog to static ranking for video collections.'],\n",
       " ['Weighted Graph Visualization and ComparisonComparison of weighted graphs is an open research question. However, various comparison techniques are proposed for unweighted graphs in a number of domains for problems related to characterization of metabolic pathways [8, 36] , business process models [2] and software evolution [11] .'],\n",
       " ['MethodologyAs the data was a kind of document, they would be included as qualitative data (Creswell, 2013) . In this paper however data collection was also in form of quantitative, as an instrument called AntConc v 3.4.3w would be used to draw information. Later in the discussion, the interpretation would be provided firstly by quantitative data to answer the first research question. The quantitative analysis employed chi-square test by Microsoft Excel 2010. Qualitative interpretation, which discourse analysis was used to extract further analysis, was then delivered to support the quantitative findings (Creswell, 2004) .'],\n",
       " ['Addressing Research Questions7.1.1 RQ1: Are the swipe length and shape of touch gestures related to changes in cognitive performance? and RQ2: Is the speed of touch gestures related to changes in cognitive performance? We found that overall swipe speed features were positively correlated with TMTA and TMTB in Tetris and Candy Crush, while overall swipe speed features in Fruit Ninja and Candy Crush were positively correlated with RESIN. These results imply that increases in swipe speed were associated with decreases in performance on these cognitive functions (visual search, mental flexibility and response inhibition). This finding demonstrates a clear diversion from the studies on traditional handwriting movement [28, 36, 44] . Indeed, in handwriting studies, higher hand movement speed is correlated with increase in cognitive performance. However, we consider that the seemingly contradictory finding can be explained by the significant differences in user intention, between handwriting and gameplay. Based on gameplay observations, we noted that fast and erratic gestures in games like Fruit Ninja, tend to occur when players narrowly miss certain objects that they are expected to interact with, within a limited time frame. We therefore hypothesise that the temporal nature of game interaction can lead to more erratic and fast gestures, as the cognitive abilities of the players decline. The positive correlation between the directness index of swipes and response inhibition ability as well as the swipe patterns idiosyncratic to Fruit Ninja provide evidence to support this explanation. It is consistent with the negative correlation we identify between VISP and swipe speed in Fruit Ninja. The results seem to suggest that these fast swipes, demonstrate decreases in visuospatial abilities, and are indeed rushed movements.'],\n",
       " ['RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsHinduja and Patchin (2010) have reported that like traditional bullying, cyberbullying includes \"being ignored, disrespected, picked on, or otherwise hassled\" [12] (p. 208). However, when newer technological features are used to debase people, such as spreading rumors, stalking, or threatening, cyberbullying is more harmful and dangerous than traditional bullying. With the rapid changes in technology in recent years, it is important to revisit the effects of newer technology (e.g., apps) on cyberbullying.',\n",
       "  \"RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsJacobs et al. [13] conducted focus-groups with cyber victims aged 12 to 15 and developed a coding scheme for cyberbullying. They found: a) common forms and consequences of victimizations; b) victims' perceptions and attitudes towards cyberbullying; c) reasons for cyberbullying; and d) reactions after being cyberbullied. However, the authors report that most findings are explained by the previous literature, and they did not hold follow-up individual interviews so that participants could share their experiences more privately or deeply.\",\n",
       "  \"RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsRecent efforts in the HCI literature have focused on mechanisms to counter cyberbullying. Sutherland et al. [25] studied users' readiness for reporting cyberbullying to authorities as a function of severity of bullying in animated scenarios. Fan et al. [10] designed a social media app to foster positive online behavior and prevent cyberbullying. Ashktorab and Vitak's [2] participatory design study identified effective cyberbullying interventions. Our research, however, focuses on understanding the current state of cyberbullying, given the emergence of features that were not explored in past research [19] . Design interventions are believed to be more likely to succeed if built on a strong understanding of how the features of these new apps affect cyberbullying.\"],\n",
       " ['IV. EMPIRICAL STUDY 1: CRASH DETECTION CAPABILITYThe goal of our first study is to evaluate the effectiveness of CRASHSCOPE at discovering crashes in Android apps as compared to state-of-the-art approaches for testing mobile apps. The quality focus of this first study concerns the fault detection capabilities of CRASHSCOPE in terms of locating crashes. The context of this study consists of 61 open-source Android apps previously used to evaluate automated testing approaches in [29] , as well as five approaches for automated input generation (listed in Table II) . We investigated the following research questions (RQs): A. Methodology In order to compare CRASHSCOPE against other stateof-the-art automated input generation tools for Android, we utilized a subset of subject apps and tools available in the Androtest testing suite [8] , [29] . We chose to perform this study on a subset of the tools offered by the Androtest artifact due to runtime issues, namely, some tools would not run consistently on the set of provided subject apps (e.g., the tools would launch an emulator but not the app), causing inconsistent results we chose to exclude. However, when contacted, the authors of the tool were helpful in supporting us. We believe the tools tested against constitute a diverse representation of the publicly available Android testing tools. The Androtest suite contains 68 subject applications for testing; however, when recompiling the applications to run the tools and extract the apps from the VM to run with CRASHSCOPE, seven of the subject apps failed to compile with the instrumentation necessary to gather code-coverage results. Therefore, each tool in the suite was allowed to run for one hour for each of the remaining 61 subject apps, five times, whereas we ran all 12 combinations of the CRASHSCOPE strategies once on each of these apps. It is worth noting that the execution of tools in the Androtest suite (except for Android monkey) can not be controlled by a criteria such as maximum number of events.'],\n",
       " ['ACCOMMODATION & CONTEXT UDPATE.Immediately related to this question is the mounting theoretical and experimental evidence that triggers may be \"hard\" or \"soft\" depending of the possibility of accommodation (Abusch, 2010; cf. Jayez et al, 2014) and the extent to which accommodation is possible (Beaver & Zeevat, 2012) , which brings us to the third research question that explores whether different triggers may exhibit different online processing behaviors. Anaphoric triggers such as pronouns and also/too are found to be hard to accommodate, whereas factives and aspectual predicates are found to accommodate easily (Schwarz, 2014) . This distinction between hard and soft triggers raises the questions of how context update may work when considered in light of the memory retrieval process. Accommodation of a presupposition can be thought of as a recovery strategy that is initiated when memory retrieval fails. It could be that the distinction between hard and soft triggers is directly related to memory retrieval mechanisms. For instance, a direct-access, cue-based retrieval mechanism may be best suited for hard triggers during the processing of presuppositions since these require exhaustive search of context which would be burdensome for a search-based mechanism. However, the situation is less clear for soft triggers. The fact that they are easier to accommodate and perhaps even non-anaphoric in nature suggests that the consequences of triggering an accommodation process due to an accidental memory retrieval failure are less severe. This suggests that the constraints on possible memory mechanisms may be looser for soft triggers than for hard ones, suggesting several empirically testable possibilities.'],\n",
       " [\"IntroductionOn the one hand, the purposefully design trustless mining protocol [6] [41] [46] does not require a third-party entity to authorize transactions but merely miners' consensus, which in turn supports people's trust in blockchain [41] [57] . On the other hand, the emerging social organization of mining practices brings forward issues of trust among miners such as the risk of 51% attack [15] [19] or of selfish miners [16] [25] [47] [55] , explored mostly within the security research area. Relevant HCI works on blockchain and its trust related issues have started to emerge [33] [48] [49] . We agree with the argument that blockchain offers a unique perspective to explore trust as its characteristics contrast with the centralized, regulated, and nonanonymous traditional transaction systems which have informed the existing HCI models of trust [23] [45] . However, apart from modeling-based security research on mining, we know little about miners' practices from their first-person perspective, and how the specific blockchain's characteristics impact on their trust. To address this gap, we report on interviews with 20 Bitcoin blockchain miners about their mining practices and related trust challenges, in order to explore the following research questions: 1. Which are miners' motivations for bitcoin mining? 2. Which are Bitcoin blockchain's' characteristics impacting on miners' trust and its dimensions? 3. Which is the social organization of mining practices: are there different approaches and types of miners? 4. Which are the main trust challenges and how do people attempt to mitigate them?\"],\n",
       " ['CONCLUSIONSThis paper showed the performance of an interparticipant emotion recognition tagging approach using participants\\' EEG signals, gaze distance and pupillary response as affective feedbacks. The feasibility of an approach to recognize emotion in response to videos is shown. Although the results were based on a fairly small video data set due to experimental limitations, the promising accuracy can be scalable to more samples from a larger population. The improved performance using multimodal fusion techniques leads to the conclusion that by adding other modalities, such as facial expressions, accuracy as well as robustness should further improve. Results from our previous studies [3] showed that there is a significant difference between peoples\\' emotional self-assessments in response to videos. However, there usually exists one most popular emotional tag for which there is significant agreement in a population. This \"most popular emotion\" has been shown to be detectable with monitoring users\\' bodily responses. Moreover, the population tags give the retrieval system a higher chance of success in a given population. We have shown that it is possible to design an accurate and user-independent classification protocol to recognize emotions from pupillary reflex, EEG signals in response to video content. Moreover, we have shown that for the video data set utilized, the nonverbal affective cues can replace affective self-report with comparable emotion recognition performance and no requisite of direct user inputs. We can thus answer positively to our two research questions. Thierry Pun received the electrical engineer diploma in 1979. He received the PhD degree in image processing for the development of a visual prosthesis for the blind in 1982 from the Swiss Federal Institute of Technology, Lausanne. He is head of the Computer Vision and Multimedia Laboratory, Computer Science Department, University of Geneva, Switzerland. He was a visiting fellow from 1982 to 1985 at the National Institutes of Health, Bethesda, Maryland. After being a CERN fellow from 1985 to 1986 in Geneva, Switzerland, he joined the University of Geneva in 1986, where he is currently a full professor in the Computer Science Department. He has authored or coauthored about 300 full papers as well as eight patents. His current research interests, related to affective computing and multimodal interaction, concern: physiological signals analysis for emotion assessment and braincomputer interaction, multimodal interfaces for blind users, data hiding, multimedia information retrieval systems. He is a member of the IEEE.'],\n",
       " ['Designing individualized technology to quit smokingNeeds and perspectives of providers, who are also important stakeholders in the treatment of nicotine addiction, are relatively less studied to inform design of technology. Recent independent survey studies in the US [42] and Australia [67] showed providers have primarily positive attitudes towards potential use of technology for smoking cessation support. Both providers and clients preferred features in applications that allow users to track their progress, personalize, match, and adapt to changing interests and needs of clients, and help manage withdrawal symptoms and medication needs for nicotine addiction [27, 42] . However, majority of providers did not consider current apps to be effective for smoking cessation [42] . Research is needed to further incorporate perspectives and expertise of providers on how these needs can be addressed through designing technology for individualized support. Understanding strategies and barriers to in-person counseling can provide insights into opportunities for technology to build upon these strategies and address challenges in practice. To develop an in-depth empirical understanding of provider practices and inform design for individualized needs, we investigated the following research questions: '],\n",
       " [\"4.\\uf6dc CorporaThe Map Task Corpus is a tasked-based corpus that is the linguistic product of a cooperative task involving two participants. The Instruction Givers have a marked route on their map and give directions to the Instruction Followers who have no route. The maps are not identical, which elicits unscripted problem solving dialog. Because of the domain, we predicted a strong spatial predisposition in the lexical and syntactic collocations of this dialog. However, to ensure that any Map Task Corpus findings were based purely on their spatiality, another task-based corpus was selected to include in the analysis: the TRAINS Corpus. This corpus is based on the routing and scheduling of freight trains. The corpus shares with Map Task its basis as a task based corpus, but it is more temporal and directional in nature than the spatial Map Task Corpus. A selection of non-task based spoken dialogs were also included in the analysis. Following the methodology of Biber (1988) , the spoken dialogs found in the London Lund Corpus were included in the analysis and broken up into six different speech situations: spontaneous speech, prepared speech, face to face conversations, telephone conversations, interviews, and broadcast speech. Our primary purpose in including the LLC was to use it as a means to compare natural dialogs and task-based dialogs. As such, we were also interested in the possibility that bigrams might be powerful enough to delimit natural dialogs from task-based dialogs in a factor analysis based on the idea that task-based dialogs were more instructional in nature and depended on a more controlled lexical domain. Because there has been ample attention given to dialectical differences between American and British spoken dialects (Biber 1987; Helt 2001) , we also included American spoken dialogs. These included the Santa Barbara Corpus and the Switchboard Corpus. The Santa Barbara Corpus is a collection of natural speech recordings taken from people across the United States. The Switchboard Corpus, on the other hand, is a collection of about 2,400 two-sided random topic telephone conversations taken from 543 speakers from all areas of the United States. Since one of Biber's (1988) primary research questions was the delineation of spoken and written dimensions, we also included the LOB and the Brown corpora. The Brown corpus is a collection of written American texts published in 1961. It comprises 500 text samples of about 2,000 words each and totals about one million words. Each text sample is categorized into one of fifteen registers including religion, science, fiction, humor, and press reports. The LOB Corpus is a direct replication of the Brown Corpus, but is based on 1961 text samples taken from British written sources. Based on the work of Biber (1988) and Louwerse et al. (2004) it was thought that the written corpora would be distinguished from the spoken corpora as a result of the written texts being more integrated and less fragmented and involved. A brief description of the corpus used in this investigation is found in Table 1 .\"],\n",
       " [\"INTRODUCTIONA Patient Reported Outcome Measure is any report of the status of a patient's health condition that comes directly from the patient, without interpretation of the patient's response by a clinician or anyone else [5] . PROMs are regularly acquired by healthcare professionals through administering questionnaires to patients. The necessity for hospitals to acquire PROMs is based on the need for providing evidence of performing value-based health care. Up till recently PROM data were mainly collected with paper-and-pencil methods, and since a few years Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. HRI '18 Companion, March 5-8, 2018 e-health solutions such as apps on tablets or smartphones are used, where answer options can be selected by touch buttons. However, elderly people often do not have the e-health literacy for using these devices [6] , or find the technology difficult to use due to their disabilities and chronic diseases. A social robot which can conduct a verbal dialogue, supported by gestures and an answer display, would not require e-health literacy from the patient. There is evidence that such social robots are perceived as more supportive than e-health solutions such as an interactive tablet [2] . Our research question was therefore twofold: 1) how to design a dialogue, using a social robot with a screen, for acquiring PROMs by direct verbal and visual communication, and 2) what is the resulting usability in terms of user satisfaction, effectiveness and efficiency?\"],\n",
       " ['Bandwidth CharacteristicsWe address RQ3 in light of Indian Country\\'s infrastructural limitations described in the Introduction and Related Works sections. We investigate the impact media richness has on the propagation of individual tweets in the Native American advocates data set. We argue that all tweets are essentially bulletins that enable asynchronous interaction between the poster and audience. However, the richness of individual tweets can vary considerably depending on the presence, type, and size of media embedded in the tweet. Of the 5,172 unique tweets we observe in the Native American advocates data set, 1.7% contain embedded video content, 35.8% contain embedded photo content, and 62.5% do not contain any embedded content. Per Daft and Lengel\\'s definition, we consider tweets with embedded media to be richer than those that lack embedded media [14, 35] . Moreover, we consider tweets with embedded videos or GIFs to be richer than tweets with embedded photos based on the fact that such media offers the \"simultaneous transmission of multiple information cues\" [35] . Similarly, we consider tweets with embedded videos to be richer than tweets with embedded GIFs, as the audio component lends the expression of a greater \"variety of languages\" [35] .'],\n",
       " ['INTRODUCTIONMobile phones have become truly ubiquitous and have permeated each and every walk of life; they accompany us throughout the day, from first thing in the morning to last thing at night. However, in conjunction with the meteoric rise in mobile device ownership, researchers have begun to highlight a number of growing pains such as the impact on social order when using technology during collocated interactions [30, 40, 44, 52] . Given the increased prevalence of mobile devices, an arguably important yet understudied research question we seek to address in this paper is how do individuals conduct the interactional work of interleaving mobile device use with their ongoing conversation?'],\n",
       " ['IntroductionWe investigate this research question on the example of discriminative training for patent translation, using the algorithm for multi-task learning with 1 / 2 regularization presented by Simianer et al. (2012) . We compare multi-task learning on \"natural\" tasks given by IPC sections to multitask learning on \"random\" tasks given by random shards and to baseline models trained on independent tasks and pooled tasks. We find that both versions of multi-task learning improve over independent or pooled training. However, differences between multi-task learning on IPC tasks and random tasks are small. This points to a more general regularization effect of multi-task learning and indicates a broad applicability of multi-task learning techniques. Another advantage of the 1 / 2 reg-ularization technique of Simianer et al. (2012) is a considerable efficiency gain due to parallelization and iterative feature selection that makes the algorithm suitable for big data applications and for large-scale training with millions of sparse features. Last but not least, our best result for multitask learning improves by nearly 2 BLEU points over the standard MERT baseline.',\n",
       "  'ConclusionOur research question regarding the superiority of \"natural\" or \"random\" tasks was shown to be undetermined for the application of patent translation. The obvious question for future work is if and how a task division can be found that improves multi-task learning over our current results. Such an investigation will have to explore various similarity metrics and clustering techniques for IPC sub-classes (Wäschle and Riezler, 2012a) , e.g., for the goal of optimizing clustering with respect to the ratio of between-cluster to within-cluster similarity for a given metric. However, the final criterion for the usefulness of a clustering is necessarily application specific (von Luxburg et al., 2012) , in our case specific to patent translation performance. Nevertheless, we hope that the presented and future work will prove useful and generalizable for related multi-task learning scenarios.'],\n",
       " ['IntroductionOne of the central issues in bilingualism research is code-switching, which is the alternative use by bilinguals of two or more languages in the same conversation. Under this general term, different forms of bilingual behaviors are subsumed. Sometimes switching occurs between the turns of different speakers in the conversation, sometimes between utterances within a single turn, and sometimes even within a single utterance (Hudson, 1980) . However, in today\"s technological era, the question arises if code-switching on the internet occurs in the same way as face-to-face communication. It appears as there is no sufficient evidence in the Saudi context which could show the different functions of code switching in online communication, therefore, the current study examines the bilingual world on the internet and aims to answer the following research questions to fill that research gap.'],\n",
       " ['FINDINGS, DISCUSSION AND FUTURE WORKRQ3 -Future Jobs: Older peoples\\' opinions about humans doing hazardous work were not affected by our exhibit whereas opinions of younger people were. Our hypothesis, H3 was partially accepted as we did expose one effect of our exhibit on participants\\' opinions. For one question scene, C, for all generations except BabyB+, the ratings of humans doing the task (QOp1) were lower for those exposed to the exhibit than for those not exposed. i.e. they thought that it was less likely that humans would be doing the task in 10 to 15 years\\' time. Scene C showed an \"Analog sensor read\". It could well be that most of our participants felt that it was likely this fairly simple task would be within the capability of robots soon and so humans would not be doing it, whereas the other scenes showed tasks that were viewed as less straight-forward by our participants. The fact that, for the other scenes, opinions were not significantly affected by exposure could mean we lacked enough data. However, we could interpret this as either a) our interactive exhibit strategy of presenting a balanced view, aimed at visitors seeking new information activating the Scientific Literacy Model [5] , was not effective as it only had a partial effect on opinions, or b) experiencing our exhibit mostly confirmed pre-existing opinions about the future of RAI in hazardous environments.'],\n",
       " ['Pilot studyZhang et al. designed an activity for simulating different natural expressions including anger and disgust in their participants [68] . Therefore, the naturally stimulated anger videos had very low intensity compared to an acted dataset such as the one we used in our previous work [41] . Naturalistic anger source videos were hard to recognize due to their very low intensity. However, since our research questions were focused on pain, we decided to continue with the study and include anger and disgust in addition to pain to make sure we do not ask participants to only watch and label only one type of expression (pain).',\n",
       "  'DISCUSSIONWe found support for our first research question, which supports earlier findings in the literature, that clinicians are less accurate than laypersons in decoding pain [21, 23, 43, 35, 27, 17] . Our work showed that this effect is found regardless of simulation embodiment (robot or virtual character), which suggests even the novelty of the embodiment is not sufficient to incur improved decoding skills. However, since training has been shown to help mitigate these clinical habituation effects, expressive, interactive RPSs may be useful in the medical education curriculum to be used alongside procedural skills training. For example, rather than having infrequent and contextless patient empathy training sessions [58] , clinical learners could be continually learning pain decoding skills as they practice other critical care skills on patient simulators (e.g., performing physical exams, assessing patient state, etc.).'],\n",
       " [\"III. DESCRIPTIVE AND INFERENTIAL STATISTICSThe differences between the ESP2 and PSP2 thus confirmed one more procedure was required to prove on which set did the subject group outperformed the other, on the ESP2 or on the PSP2.This demanded applying the T-test to determine the statistical difference between the means on the two paired sets of scores. Once the T-test was applied to the two paired sample groups (Tables 2-d and 2 -e, below), it was discovered that the difference in means between the two sets (ESP2 vs. PSP2) were 16.85 vs. 19 .42 with significance levels of 0.048 and 0.049. This indicates that there is a significant difference in performance between ESP2 vs. PSP2. Thus, as far as these particular sets of groups are concerned, the second research question as to the selection of the topic(s) is answered in favor of the subject groups in PSP2. However, in order to see whether or not the differences among the groups are statistically significant, the one-way ANOVA procedure was run. The results of the ANOVA procedure for the ESP3 and PSP3 are given below in Tables  3-3 According to Table 3 -b, the results of ANOVA procedure on mean differences of ESP3 post-test as reflected in the F-ratio of 5.074 and the significance level of 0.002 (sig<%5) indicate that because the level of statistical significance is lower than %5,it can logically be argued that there is a statistically significant difference among the subject groups. To locate where the differences between the groups lie, the post-hoc Tukey's HSD was applied resulting in Table 3 -c. A similar examination of the results of ANOVA on the PSP3 post-test with F-ratio of 5.809 and the statistical significance of 0.001 belonging to the means of the groups reveal that in this case because the significance level is lower than %5, it can be asserted that there is a statistically significant difference among the subject groups of PSP3. To locate the differences between the groups, the post-hoc Tukey's HSD was applied which resulted into Table 3 -c'.\"],\n",
       " [\"DISCUSSIONFrom our findings, we conclude that a vertical shift (65 cm) of the VHS can effectively reduce fatigue during overhead interaction (H1), while maintaining the illusion of owning the virtual hand (H2). Performance is only slightly affected by this shift (less than 4% decrease) and the user feels in control of the virtual hand (H3). The decrease in task performance could be addressed by increasing the size of the moving target; with a target of 3 cm radius, participants would have stayed within the target's bounds 90% of the time (in the Ownershift condition, 90% of measured errors were smaller than 0.03). Additionally, it stands to reason that training could improve task performance [19] . As interpreted by Kohli et al. [26] , in a warped virtual space, users are less precise at touching targets, but precise enough. However, it remains to be explored how a gradual shift might affect more delicate manipulations (e.g., drawing a straight horizontal line). We find that participants performed equally well with the gradual shift in the Ownershift technique, as when interacting with an instantly shifted virtual hand, which has earlier been shown to be more effective than retargeting through interpolation [19] . The qualitative results further suggest that a gradual shift is preferable, since it was described as much less noticeable and less disruptive. Furthermore, it did not require a mental recalibration phase, since the initial 1:1 mapping in Ownershift allows initial ballistic movements toward the target (RQ1). This is in line with earlier work [19] , which found that the instant shift was disorienting, when it meant that participants first had to look around to locate the hand, before being able to start interacting. Importantly, we found no evidence that task performance was diminished, while the VHS was gradually being shifted (RQ2).\"],\n",
       " ['Personality Models in Multi-Cultural Settings (RQ1)Previous work [9, 10] has extensively used Call data for inferring personality traits, showing its predictive power especially for Agreeableness and Extraversion. In our models, however, Call features did not turn out to be predictive for these sociability-relevant traits. This might be due to a recent and considerable shift to Internet based call and messenger services such as FaceTime, WhatsApp, Viber and similar mobile apps 6 .'],\n",
       " ['Feature selection for K-means1. ACC and NMI illustrate that KFDRL performs better than other feature selection algorithms on most of the datasets. 2. The traditional dimensionality reduction algorithm LapScore, which is a modified for Laplacian Eigenmaps (LE), recognizes and utilizes manifold structure embedded in high dimensional data by graph Laplacian without learning mechanism. The results reported in Tables 3 and 4 show that the joint-framework or two-step algorithms can perform better than traditional and one-step algorithms. 3. In comparison to other algorithms, KFDRL demonstrated superiorities. First, it simultaneously benefits from manifold and discriminative information, because the objective function proposed in KFDRL combines manifold learning and discriminative regression learning. Second, the constraints imposed on the scaled assignment cluster matrix H and on the transformation matrix W can help it to find physically meaningful features (a forced constraint imposed to the scaled assignment cluster matrix H and the transformation matrix W can fit the physical meaning). The best results obtained with feature selection for each dataset have better performance than the clustering results obtained on the original datasets. It indicates that feature selection can not only reduce the size of date to increase the computation speed, but also efficiently remove the redundant and noisy information demonstrating the great significance of data pre-processing. 4. KFDRL cannot show a better performance than the other algorithms in a few cases. For example, it shows that JELSR performs better than KFDRL with ACC metric on Sonar dataset. Moreover, it shows that KFDRL performs worse than all features selected and DFSC with NMI metric on the Isolet dataset, as well as JELSR and LSPE on the Umist dataset. It is noted that these methods mentioned above are all single-step methods, which describe their better performance. We explain several reasons for the worse performance of KFDRL. First, Sonar and Isolet are both voice data. The advantage of KFDRL can be identified in the task of clustering image data because the embedded dimension and the number of classes are considered to be identical (Cai et al. 2008 (Cai et al. , 2011 Liu et al. 2012) , i.e. c m. However, it cannot have better results on the Sonar and Isolet datasets. We believe the embedding learning employed in JELSR and LSPE on the Umist dataset has superiority over other techniques. As reported in Tables 3 and 4 an algorithm resulting in a better performance of clustering different datasets than others does not exist. Hence, the best algorithm still needs to be chosen based on the nature of the dataset to be clustered and finding an algorithm best for clustering all the different datasets will be an open research question for future. 5. In general, KFDRL performs better than JELSR on most of the datasets, which indicates that the manifold learning alone is not efficient in many cases. Nonetheless, JELSR outperforms KFDRL on the Sonar and Umist dataset, which may be a result of the inherently discriminant structure of the datasets. Hence, a discriminative based algorithm cannot perform as well as it does in the case of non-discriminant datasets. It is worth reminding that KFDRL includes discriminative and manifold learning whereas JELSR is based on manifold learning only.'],\n",
       " ['Cost/BenefitIn theory, a sound mental model enables a person to reason effectively about their best course of action in a given situation [10] . Thus, we expected participants with sounder mental models (the With-scaffolding participants, according to the RQ1 results) to debug more effectively than those with less sound models. For example, knowing that the recommender could be steered more effectively by using unique, highly specific words (e.g., \"Merseybeat\") rather than broad, common descriptors (e.g., \"oldies\") should have helped such participants debug the agent\\'s reasoning more effectively than participants who did not understand this. Surprisingly, when using participants\\' perceptions of cost/benefit as a surrogate for effectiveness, the soundness of participants\\' mental models showed little impact on this measure of debugging effectiveness. However, mental model transformation was tied with cost/benefit: participants who most improved the soundness of their mental models reported that the effort of debugging was significantly more worthwhile than participants whose mental models improved less, or not at all ( Table 2 , row 1 & Figure 4A ).'],\n",
       " [\"INTRODUCTIONThus, this research is aimed to portray students' perceptions on their teacher's roles in EFL classroom by using social semiotic approach. The participants are the 11 th grade students in a state senior high school in Banjarmasin. The students express their perceptions on teachers' role through drawings. Clarebout et al (2007) argues that drawings can be used to identify nuances and ambivalences within a person's belief system, indicating they would be useful when studying pupil conceptions. This research strategy is increasingly being used to probe students' feelings about how they teachers accomplish their role in the classroom. This research is going to answer a single research question: How do students perceive their teacher's roles in the EFL classroom viewed from their drawings? 2. LITERATURE REVIEW 2.1 TEACHERS' ROLES To achieve an effective teaching-learning process, teachers ought to know their roles in the classroom. In the process of transferring knowledge, teachers are expected to play particular roles. However, the implementation of the role itself depends on the situation of the educational institution, the students, and the subject matter we are required to teach, and so on. Considering those various situations, teachers are required to play more than one role. According to Harmer (2007) , there are five roles of the teachers: (1) teacher as a controller, (2) Teacher as a prompter, (3) teacher as participant, (4) teacher as resource, and (5) teacher as tutor. Furthermore, those five roles are becoming the primary issue in this research. The writer intends to investigate the realization of those five roles by analyzing students' perception through their drawings 2.2 SOCIAL SEMIOTIC APPROACH In this study, the semiotic sign which will be analyzed is students' drawing. Images show not the thing actually seen but its representations in human consciousness. Turkcan (2013) proposes that semiotic approach has increasingly gained importance so that people, who live in a visual bombardment today, can analyze the codes included in visual culture and understand the form of visual communication. Halliday (1978) states that social semiotic theory provides the basis for the study of semiotic resources other than language (e.g. images, architecture, music, mathematical symbolism, gesture, clothing). Moreover, Aiello (2006) argues that the main aim of social semiotics is to look systematically at how textual strategies are deployed to make certain meanings. He also adds that social semiotics concentrates on practices of meaning-making and considers how we make meaning using various semiotic resources, modes and their affordances. Then, what is developed by Kress and van Leeuwen's (1996) and Van Leeuwen and Jewitt (2001) is how we can use social semiotics as an analytical tool to access these meanings by drawing on the contexts and cultures of its production and how we as individuals assign meanings to our texts through our prior knowledge, exposure and use of semiotic resources and modal affordance in our communicative practices. Kress and Van Leeuwen (1996) have extended this idea to images, using a slightly different terminology: representational meaning, interactive meaning, and compositional meaning.\"],\n",
       " ['INTRODUCTIONShared leadership is the distribution of leadership among team members and is characterized by the sharing of leadership roles [12, 13] . Shared leadership, however, offers the possibility for every team member to be included in team decisions which potentially promises more inclusion and better team experiences. This paper seeks to build on prior literature examining shared leadership in virtual teams [5, 13] . The research question this study attempts to address is: \"How does shared leadership impact the identification, satisfaction and actual performance of virtual teams?\" This study has three goals: 1) To examine the impacts of shared leadership on an individual\\'s willingness to identify with their team in light of their race and gender. Team identification has been found to be an important process variable for the success of virtual teams [9, 45] . However, little research has been done to understand the conditions that facilitate team identification when these teams are racially and gender diverse. 2) Investigate the impact of shared leadership on team satisfaction. Virtual teams are often criticized for being overly sterile and too task focused [44] . As work becomes both more virtual and collaborative, understanding what factors facilitate a positive team work environment becomes increasingly important 3) Examine the impact of shared leadership on the performance of virtual teams. Prior research has only posited positive impacts between shared leadership and virtual team performance [5, 13] ; however, there are reasons to believe that shared leadership could potentially decrease the performance of virtual teams.'],\n",
       " ['DiscussionConcerning the global evaluation of the mobile phone prototype, the present results revealed significant effects both of manipulated usability and manipulated aesthetics on perceived appeal, with comparable medium to large effect sizes (usability: 2 = .078; aesthetics: 2 = .100). Both factors explained a similar amount of variance in perceived appeal. Therefore, our results suggest that perceived appeal, serving for a global evaluation of noninstrumental quality [21] , depends on aesthetics and usability more or less to the same extent. This finding corresponds to a result of Lee and Koubek [15] , who found that manipulated aesthetics and usability both significantly affect user preference, which in turn might be determined by perceived appeal. Regarding the \"indirect\" effect of aesthetics on perceived usability (research questions 3.1 and 3.2), we found that goodness completely mediates the relationship between perceived beauty and perceived usability (H3.1) but not between manipulated aesthetics and perceived usability (Hypothesis 3.2). Consequently, neither perceived beauty nor manipulated aesthetics seem to be directly linked to perceived usability. However, the former finding (H3.1) corresponds to the inference rule of evaluative consistency, which implies an indirect effect of (perceived) beauty on pragmatic quality, as stated by Hassenzahl and Monk [4] as well as by van Schaik et al. [13, page 17] . Following van Schaik et al. [13] , this kind of inference results in a highly beauty-driven overall evaluation (\"What is beautiful is good\" and \"I like it, it must be good on all attributes\"), which may include the judgment of usability. Concerning the argument of van Schaik et al. [13] , it is noteworthy to mention that the relation between perceived beauty and perceived usability is based only on variance due to subjects. Consequently, we may assume that the mediation effect solely depends on an individual evaluative process and therefore might be better characterized as \"What is perceived as beautiful is perceived as good\" and \"What is perceived as good is perceived as usable, \" independent of whether a system is really ugly or beautiful (in the sense of manipulated aesthetics). On the contrary, in the current study, the mediation analysis for manipulated aesthetics, goodness, and perceived usability (Hypothesis 3.2) did not reveal an effect, because manipulated aesthetics and perceived usability were not correlated. Regarding this finding, we may conclude that there is neither a direct nor an indirect link between aesthetics and perceived usability, or, in other words: people judge the usability of a system independently of whether it is ugly or beautiful (in the sense of manipulated aesthetics).'],\n",
       " [\"Lexical access (RT): size and frequency effects (RQ3)Following Laufer and Nation (2001) and based on Tanabe (2016) , the participants have been equally divided into two groups (High and Low groups) using the mediansplit on their ENVST scores in order to establish whether the group with a larger vocabulary size also showed faster accessibility in the tested foreign languages. Table  3 illustrates just this. It becomes obvious from the mean scores and paired t-tests that there is a statistically significant difference between the two groups' L3 (t(26) = 9.10, p < .001) and L2 vocabulary scores (t(26) = 2.87, p < .01). Furthermore, in support of size effect, the t-tests also indicate that students with a larger vocabulary also had a faster response time in English (t(26) = -2.03, p = .04), but not Romanian t(26) = -1.14, p = .26). The reason for this probably lies in the fact that the more proficient the group is in a language, the less variability can be detected in their RT scores. Figure 4 furthermore shows that there is a slight difference between the response times to cognates and non-cognates. In order to test whether the difference overall between response time to correct answers on cognates and non-cognates is significant, t-tests were conducted. In the case of the ENVST, t = -.689, p = 0.491, R 2 = .016 (cognate M = 12.43 and non-cognate M = 12.64), which indicates that there is no difference between the means. However in the case of the Romanian test (t = -2.23, p = 0.03, R 2 = .048, cognate M = 9.61 and non-cognate M = 10.12) and the Hungarian test (t = -2.27, p = 0.02, R 2 = .039, cognate M = 6.27 and non-cognate M = 6.54) there is a significant difference between the means, indicating that even in the case of multiple choice tests cognates are likely to be recognised faster than non-cognate items, albeit the advantage of cognates is not as accentuated as in yes/no studies.\"],\n",
       " ['Commodity Cluster SolutionsMulti-core processors will allow the on-chip rateof-computation to continue to grow exponentially with Moore\\'s Law. However, as just mentioned, this does not address the issue of primary memory speed. This so called \"Memory Wall\" was predicted many years ago [37] but microprocessors have been able to avoid the the growing disparity by using ever-larger on-chip caches. Thus it possible that some clever research will emerge that again postpones the bottleneck issue; however to date the only solution has been to create ever-larger data caches. There are other unanswered research questions as well, such as how hundreds cores on a single chip will communicate (on-chip and across the nodes of cluster). The point is not to say that multi-chip processors have insurmountable problems. Rather just to note that the technology is not without risk.'],\n",
       " ['Human Ability to Read Text at Various OrientationsThe work of both groups seems to confirm the intuitive notion that reading orientation should be a key concern to designers of tabletop systems. However: their work is not directly applicable to tabletop collaborative groupware research. First, rather than allow participants free and natural movement during the experiments, the position and orientation of their heads was constrained. Second, in both sets of experiments, readability was determined by how quickly non-conforming strings were identified. In the Tinker (1972) study, this was done at the semantic level, as participants were required to find the word in paragraphs that spoiled their meaning. In the Koriat and Norman (1985) experiment, this was done at the syntactic level, as the study consisted of the presentation to the participant a series of strings of characters, which subjects were required to identify as either words or non-words. Though this experimental design enabled them to answer their research questions, we note that the identification of non-conforming or gibberish strings is not directly applicable to real user interface scenarios. In most applications, textual artifacts consist of either common words or domain terms that might be expected by the user. This assumption might aide in the reading of text at varying orientations, and so should be considered when evaluating user performance of reading at varying orientations.'],\n",
       " [\"Trust in CMCEarly research questioned the efficacy of text-based CMC environments as vehicles through which trust could be established, suggesting that they did not provide enough nonverbal social cues to foster trust development [2] . More recent studies have challenged this view and demonstrated that that while trust may develop more swiftly in face-toface settings, individuals can reach similar levels of trust in text-based CMC environments when given enough time Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CSCW'08, November 8-12, 2008 , San Diego, California, USA. Copyright 2008 ACM 978-1-60558-007-4/08/11...$5.00. [18] . However, the vast majority of research on trust development in CMC focuses primarily on outcome measures of trust. Only recently has work begun to examine the processes by which groups develop trust. One mechanism through which we believe trust may be achieved is through the use of linguistic mimicry.\"],\n",
       " ['IntroductionThere is already a huge number of scientific papers concerning the application of data analytics and related technologies in maintenance [11] [12] [13] . However, there is no research effort reported to systematically review the current state of this new wave of industrial revolution. Therefore, the purpose of this paper is to find and cluster those relevant to maintenance use in a production context. This aim of the paper is to provide a systematic literature review, within data analytics covering the maintenance of production systems. More specifically, seven research questions are listed below. They provide a more appropriate answer as to the purpose of this paper:'],\n",
       " ['INTRODUCTIONWhile most HCI research on memory technologies has focused on episodic memory impairments such as those associated with dementia, less work has focused on building technologies for autobiographical memory impairments to help people living with other mental disorders such as depression. Memory impairments in depression are however fundamentally different; their effect is felt not through the loss of episodic memories, but rather difficulties in the retrieval of episodic memories through higher levels of autobiographical memories such as general events and lifetime periods. Another distinct body of work explored computerized interventions for depression such as online Cognitive Behavioral Therapy (CBT). Such interventions address memory impairment, not as the main focus, but rather through a subset of psycho-educational materials concerned with negative thinking patterns, and tools for tracking mood [9, 10, 17] , which provide limited support for the distinctive autobiographical memory impairments associated with depression. Our work aims to bridge these two strands of work to contribute to the design of novel classes of technologies that specifically address memory impairments in depression. We argue that understanding the specific memory impairments in depression, and the new range of challenges they pose, offers a rich opportunity to extend HCI research on memory technologies in new directions. This paper is an initial step towards exploring this space and focuses on the following research questions: 1. How are memory impairments in depression addressed through tailored interventions used in clinical and neuropsychological practice? 2. What is the role of materials in these memory interventions and how are they employed in therapeutic practice? 3. How can therapeutic memory interventions for depression inform the design of novel memory technologies?'],\n",
       " [\"E-LearningTo put it shortly, previous research has generally provided us with some practical and theoretical insights to understand e-leaning instruction (e.g., Alsultanny, 2006; Kılıçkaya & Krajka, 2010) . As the above review suggests, incidental lexical knowledge certainly makes a significant contribution to L2 learners' lexical knowledge. However, little is known about L2 lexical knowledge in situated learning situation. Therefore, this study aimed to investigate the effectiveness of computer-assisted instruction on intermediate L2 learners' vocabulary achievement. Hence, the following research questions were addressed: \"],\n",
       " ['Results and DiscussionHowever, the findings of the current study differ from the study by Jafari and Shokrpour (2012) , which showed high (M=3.72) usage of support reading strategies. These rather contradictory results may be due to different style of data collection carried out by the authors. Jafari and Shokrpour (2012) distributed the questionnaires after they administered a reading comprehension test. The respondents were instructed to answer the questionnaires based on the strategies they used when answering the test. These differences may have influenced the findings of the studies. The fourth research questions sought to investigate the usage of global reading strategies (GLOB). Table 3 reveals that global reading strategies are the least frequently used strategies with overall mean of 3.30. The results were consistent with both Jafari and Shokrpour (2012) and Magogwe (2013) studies, in which both studies showed moderate usage of global reading strategies with means of 3.14 and 3.42, respectively. The findings from the three studies suggested that global reading strategies were the least prevalent strategies used by the students. A possible explanation for this might be that the students were not exposed to listed strategies. Another possible explanation for this is that the students might think that the strategies were not important when reading academic texts. The last research question sought to identify five least frequently used strategies by the students to read academic texts. Table 4 shows that most students were not familiar with global reading strategies, which explains why it had the lowest overall mean as compared with two other strategies. The findings reveal that the students might not be aware of global reading strategies. Another possible explanation is that the students might not have been explicitly introduced to these strategies. Global reading strategies are important in assisting students to comprehend their academic texts. If the students were not exposed and thought the strategies were not important for them, it is imperative for the language instructors to introduce the strategies to the students and put emphasis on the importance of the strategies. '],\n",
       " ['Previous Work on MultithreadingAn open research question is whether software-based multithreading can successfully tolerate more modest forms of latency, such as the remote latencies in hardware DSMs (e.g., the SGI Origin [7] ). To implement software-based multithreading, we need two software mechanisms: (i) the ability to switch between threads; and (ii) a mechanism for knowing when to trigger thread switches. The former mechanism is clearly feasible, since software can save and restore all thread-specific state (e.g, registers, the program counter, any condition codes, etc.). The latter mechanism, however, had been lacking in the past, since there was no way for software to directly observe and react to cache misses in a sufficiently lightweight fashion. (Note that the signal handler mechanism used to trigger thread switches in software DSMs is not applicable to cache misses, since it is too costly and can only react to page-level access violations.) Fortunately, a mechanism which provides this functionality was recently proposed: informing memory operations [4] .'],\n",
       " [\"INTRODUCTIONOur focus is particularly on issues related to scientific validity. We discuss the construction of research questions and hypotheses, threats to validity, replicability, and separation between researchers and participants. This setting of scope allows us to address most of the empirical research published in the HCI field [13, 32] . However, it does not address all prominent research interests in HCI. For example, in the 'third paradigm' of HCI [20, 21, 74] , an evaluation may aim at knowledge that is pluralistic, situated, and not necessarily valid in a traditional sense, to inspire new design ideas. Stakeholders may assume active roles that go beyond those of 'user' or 'participant'. However, as we argue, the present-future gap stretches our assumed 'scientific' notion of validity, because prototypes entail value-imbued assumptions about possible or desirable futures.\"],\n",
       " [\"MCS ApplicationsMany MCS applications target a particular research question rather than general usage. For example, StudentLife collects information about university students' mental health, academic performance, and behavioral trends [38] . In contrast, MyExperience was one of the first general-purpose MCS applications; however, it runs on Windows Mobile devices (1% of current market share) 3 and requires knowledge of an XML-like scripting language for configuration [17] . The sensing capabilities of AndWellness are limited to location and activity type, and the app is confined to Android and requires XML knowledge for configuration [19] . Survalitics aims to be a general-purpose survey instrument, but it is confined to Android and requires manual modification of Java code [27] . Reporter is a commercial app ($3.99 per installation) for iOS that deploys scheduled surveys. 4 AWARE is similar to Sensus in many respects, but the iOS version currently requires manual compilation and deployment of source code [15] . Funf offers a comprehensive set of sensors and non-technical configuration; however, it lacks scheduled and sensor-triggered surveys, and it is only available for Android [1] . Paco allows researchers to design mobile sensing plans for iOS and Android devices without programming knowledge; 5 however, sensing in Paco is limited primarily to app-and call-based events, and this limitation carries over to surveys, which can only be triggered on the basis of these events.\"],\n",
       " ['Evaluation of the SDS.There are alternative measures that measure similar constructs, such as the Credibility scales by McCroskey and Teven [54] . However, the METI instrument differs from these in that it explicitly focuses on epistemic trust, that is, whether the target of the evaluations is a trustworthy source for the knowledge that the user seeks. This was desirable for our research question.'],\n",
       " ['III. RESULTS AND DISCUSSIONTable Ⅴ includes the questionnaire. All participants were allowed to write their perception of the task freely. Their responses are classified into eight categories. With respect to the second research question, the results of the questionnaire revealed that 20% of the participants perceived the task as difficult, while 15% of them perceived it as enjoyable or found importance in the usage of language. However, the results of a separate questionnaire regarding the difficulty of the task indicated that approximately 70% of the participants perceived the task level to be of an appropriate difficulty level. Most of them used the mini dictionary in confirming the meaning of words during the discussion and their group work was supportive with good feedback from peers. Furthermore, the participants seemed to perceive three factors for the effectiveness of the task. Eighteen percent of the students found it effective for practicing vocabulary, 8% of them mentioned spelling, and 13% of them mentioned listening practice. Some students referred to comprehension. Ten percent of the participants mentioned that this task helped them understand the text and 10% of them perceived the task efficient for hypothesis testing. In sum, the participants seemed to perceive the dictogloss activity as effective in promoting skills related to the productive knowledge of English. Interestingly, no one mentioned grammatical aspects of language learning. Nabei [23] points out that students perceive teacher talk as one of the most useful learning sources. The participants in the current study might have attended more to vocabulary since the teacher focused on their vocabulary knowledge during class discussions.'],\n",
       " ['A. Quantitative AnalysisOften times, comparative L2 research tend to interpret L2 learners\\' deficiency in the target language as a failure in nativelikeness. However, as Cook (2015) pointed out, we should focus on \"the reasons why L2 users create novel ways of thinking rather than in their putative deficiency compared to monolingual native speakers\" (p. 157) [19] . Therefore, in addition to providing quantitative evidence on the general tendency of \"how\" Chinese speakers\\' perform comparing with native speakers, the second research question addressed the \"why\" question by elucidating prominent features in Chinese speakers\\' production of motion verbal phrases.'],\n",
       " ['INTRODUCTIONThe high-level research question in this work is whether cycling and serpentining -as two perspectives of reexamining top-N list -improve user experience. However, we are not trying to optimize a particular user experience. We recognize that different experiences may require different approaches. A situation where a site recommends a single item cannot benefit from serpentining. A user who treats the top-N list as a \"to-do\" list, taking the top item each time, would not be served well by cycling. Rather, we want to see how these manipulations relate to user experience in the hopes of guiding designers in adopting them, or offering them to users. Similarly to the finding from Ziegler\\'s work [32] that users are willing to accept a certain loss of accuracy in order to have more diverse recommendations, we expect that the perceived accuracy of recommendations may get reduced because of the manipulation; however, we test whether the accuracy reduction may be preferred in exchange for the exposure to a broader and \"fresher\" set of items.'],\n",
       " ['HRI as an Intrinsically Interdisciplinary FieldHRI is closely related to HCI. Both disciplines address the problem of human engagement with interactive systems (e.g., with computer systems, smartphones, or robots). Initially considered as an interdisciplinary subject, contemporary HCI is a research discipline in its own right, with an established community, conferences and scientific journals, and specific paradigms, approaches, and methods. HCI study programs exist at undergraduate, master, and PhD levels, with dedicated syllabi and textbooks. Professorships in HCI are routinely appointed, although HCI scholars are often embedded within computer science (or similar) departments. Altogether, these are concrete indicators of a disciplinary field, although they do not yet indicate an institutionalized discipline. However, understanding the historical evolution of HCI requires familiarity with the challenges that the discipline has faced and still partly faces (e.g., Bødker, 2006 Bødker, , 2015 Harrison, Tatar, & Sengers, 2007; Rogers, 2012; Shackel, 2009) . If HRI will follow a similar evolution into a cohesive discipline or will rather develop as a collection of research questions that have to be addressed by several independent disciplines remains an open question (Weiss, 2012) .'],\n",
       " [\"DebriefingSomewhat problematic is that there is currently no 'end' to the data collection beyond users deleting the app. Unless all users abandon the app, we will at some point have to end data collection and ought to inform users when doing this. It may also be responsible to inform users of our findings from the study, particularly if the app is found not to support positive change. Either way, it is likely be more responsible to end logging rather than disable or withdraw the app itself. An ending remains a design task and debriefing is enmeshed with this. Grimpe et al. (2014) argue research should not just anticipate outcomes, but look forward to and navigate potential lines of technology adoption. With Quped we may anticipate answers to our research question, but we also need to think about the purposes beyond behaviour change that this app may be put to. Anticipation ought to look beyond the results of the study. We must recognise that users are accruing data with this app, which may be valuable to them and belongs to them as much (or more so) than us. This means that we should not suddenly disable the app or cause people to lose data. Our current attitude to this is that the app will remain available if the study ends and that logging will be turned off. However, it might be that in the future we provide the ability to export data out of the app.\"],\n",
       " ['DISCUSSIONIn the present study, we investigated (1) whether older adults\\' experience of interacting with robots would affect their explicit attitudes and/or acceptance toward robots compared to younger adults; (2) whether state curiosity toward robots differs between the two age groups; and (3) whether older adults implicitly hold more negative attitudes toward robots than younger adults. To address the first research question, we examined whether there exists an age difference in pretest-posttest explicit attitude ratings. We found age-related differences in TAM-PEU scores. Surprisingly, perceived ease of use toward the assistive robot was stronger in older adults than in younger adults, even after interacting with the robot. This might seem to be contradictory to the IAT results, suggesting that older adults more strongly associated robots with negative attributes. One possible explanation is that younger adults are more experienced with new technologies such as the voice control function. Their standard of \"perceived ease of use\" might, therefore, be higher than older adults\\'. For example, if the assistive robot could not respond to voice commands immediately, younger adults might become annoyed due to the long response latency. Then they might think that the usability of the assistive robot was not good. On the other hand, older adults might be less familiar with new technologies. Thus, once they found that they could communicate with the assistive robot with voice commands, even if the tasks were as simple as searching news on the Internet and it might take a while for the assistive robot to complete this task, it was still sufficient for older adults to be satisfied because they did not have to control the robot via the \"complicated\" graphical user interface. Previous studies suggested that technology experience influenced robot acceptance across age groups [20] . Perceived enjoyment also enhanced robot acceptance in older adults [10] . For older adults with lower technology familiarity, positive experience might be a key factor to the enhancement of robot acceptance. However, we did not observe other age-related differences in other explicit measures of NARS, TAM, and STAI, suggesting that the explicit attitudes and acceptance toward the assistive robot for the older and the younger participants were similar in many other aspects.',\n",
       "  \"DISCUSSIONFor our second research question, we first compared younger and older adults' state curiosity toward robots and found a significant difference: older adults appeared to be less curious about the robot than younger adults. We then examined the role of personal association and found a significant moderating effect. Our findings support our hypothesis that older adults were less curious than younger adults in general; yet, this age difference was moderated by personal association. In other words, for topics that older adults perceived as a higher level of personal meaning and relatedness, they would invest up to the same level of curiosity in the topic as would younger adults. At first glance, this finding on state curiosity may seem to be in conflict with the findings of our other explicit attitudinal measures (i.e., we found age difference in state curiosity but not in other acceptance measures except the TAM PEU subscale). However, the nature of curiosity is slightly different from mere acceptance in the sense that curiosity captures the willingness to invest energy and resources to obtain information. Hence, it may be seen as one step further from simple likes or dislikes, which may explain why the result differs from the other explicit attitudinal measures. According to the selective engagement theory [21] , people would become more selective in how they allocate their cognitive resources with age. It could be related to a well-supported fact that the older adults are less willing to learn technology via trail-and-error compared to other training methods such as reading a printed manual [40] . The Diffusion of Innovation theory [41] points out that trialability or testability are keys for potential adopters to evaluate an innovation, and it would predict that older adults are less likely to be early adopters of new technology (e.g., robots) compared to younger adults because older adults might be less willing to actually use it. However, the selection would be based on personal association. However, previous studies did not test this hypothesis in the context of willingness to allocate time and effort for technology-related information seeking. Thus, the present study offers some insights into the usefulness of the selective engagement theory in the context of technology-related curiosity. This part of the study has the limitation of measuring personal association with a single item. Also, this study cannot provide evidence for causation. Future studies should consider including a more comprehensive scale to capture curiosity or directly manipulate one's personal association for a technological product in an experiment. Despite these limitations, this finding is particularly interesting because it suggests the role of personal association on curiosity, and this has vast implications for future robotic designs, adult education, and other services to older adults. Echoing the findings of other explicit attitude measures, we recommend future designers of services dedicated for older adults to include some elements of personal association/relevance (i.e., personalized familiarity or relatedness).\"],\n",
       " [\"IntroductionSome authors have recently claimed that concern traces [4, 14] improve predictability of software instabilities [6] . A concern usually spans a number of scattered architectural decisions and, possibly, is associated with multiple requirements [3, 4, 14] . For instance, concern traces allow describing which elements in architectural artefacts were influenced by a stakeholder's concern. However, all studies analysing the value of concern traces focused on implementation artefacts [4, 7] rather than on early software development artefacts. This paper presents a first exploratory evaluation on the effectiveness of concern traceability for assessing architecture stability. More specifically, we address the following research questions:\"],\n",
       " [\"INTRODUCTIONThese cognitive challenges have been addressed by current research. In the psychology research literature, the cognitive capabilities of individuals and the role of the physical environment thereupon are discussed extensively [cf. 27, 28, 31] . Research on memory aid systems highlights suitable technical support to recall information [cf. [11, 15, 16, 21] . However these current research discussions address mostly the individual level or single-user scenarios, and insights on the cognitive aspects in mobile collaborations and how the individuals' cognitive capabilities influence their collaborative behavior are scarce. To provide appropriate support, it is important to understand the role of the OOS-OOM phenomenon in mobile collaborations. This brings us to our first research question: RQ1: How does forgetting influence mobile counseling sessions?\",\n",
       "  'INTRODUCTIONThe cognitive aspects in collaborations are broadly discussed in research on computer supported collaborative work (CSCW). Researchers provide high-level insights on the concept of collaborative memory [2, 24] and extensively discuss how to support collaboration partners to create and use a collaborative memory [1, 2, 19] . However, current research focuses mostly on stationary scenarios, and insights on the novel challenges and their influence on group members\\' cognitive capabilities and the creation of a collabora-tive memory are lacking. Current research on mobile memory aid systems addresses the individual\\'s cognitive capability, and presents diverse solutions to facilitate memory cue creation [15, 16, 21] . However, research insights on suitable support accounting for multiple users are scarce. Both the collaborative situation of the clientconsultant relationship and the mobile setting give rise to new challenges for developing suitable support systems. We thus asked ourselves how an appropriately designed \"memory aid\" could support actors to create and use a collaborative memory in their mobile collaboration. After reviewing related literature on psychology research, CSCWresearch and memory aid systems, as well as discussing different design options, it was \"working with pictures\" that turned out to be a surprisingly useful concept for responding to the characteristics of mobile collaborations and to bring information back into the minds of the actors. Thus, in this study we further pursue the second research question'],\n",
       " ['Geostatistical ModelingWhile autocorrelation was ignored in HCI and related fields for many years, this is increasingly no longer the case. However, the methods that have been used to control for autocorrelation in HCI thus far -spatial error and spatial lag models -do not capture the spatial relationship between the demographics in one area and the PokéStop density nearby. Spatial Durbin models, an emerging best practice in the geostatistics literature, do capture this type of dependence, which is fundamental to our analysis. As cross-region relationships between dependent and independent variables like those in our analysis (more formally, \"exogenous spatial relationships\") are quite common in spatial data studied in HCI, spatial Durbin models will likely prove useful for HCI research questions outside the context of this paper. Overviews of spatial Durbin modeling can be found in Yang et al. [64] and Elhorst [7] .'],\n",
       " [\"Results and DiscussionTo address RQ1 we compare the performance of RMUB against that of the other baselines. We observe that the RMUB method clearly outperforms the UB, UIR and URM baselines for P@5, nDCG@5 and nDCG@10, in both MovieLens 100K (see Table 1a ) and 1M (Table 1b) . Its performance in terms of P@50, however, is similar to some of the baselines, showing that our method is able to rank higher than such baselines interesting items for the user, at least until some reasonable cut-off, which in this case seems to be 50. Moreover, since this method takes two parameters (k and λ), we analyse now its performance sensitivity. Due to space constraints, we only explore in Figure 2 the neighbourhood size k, but we include the performance for two values of λ -the optimal (λ = 0.1) and neutral (λ = 0.5) configurations -where a negligible difference is obtained. We can also notice in the same figure that the baseline NC+P obtains a much better performance than RMUB, consistently with results reported in [2] . Table 1 also shows that the coverage results for the NC+P baseline are better than for RMUB in their optimal settings. We further observed (we omit the detailed results here for the sake of space) that the coverage of NC+P decreases with larger k's (as reported in [2] ), whereas the coverage of RMUB increases, but at the expense of losing precision. All in all, our answer to RQ1 is that relevance models as a standalone method for neighbour selection are useful but not optimal.\"],\n",
       " ['RQ1: What is the effect of asking for contributions?In this research, we seek to deepen our understanding of several key questions related to the design of precision crowdsourcing requests. First of all, fundamentally we are interested in whether our requests are effective at eliciting contributions. The core idea of precision crowdsourcing is to turn information consumers into contributors through a series of interventions. We are motivated to study not only the immediate response to precision crowdsourcing requests, but also the impact on long-term behaviors. As shown by Masli et al. [20] , \"techniques that manipulate users into participating and contributing information may succeed in the short-term but might cause long-term harm, because users tend to recognize the manipulations and may consider them unfair\". Looking at the positive side, previous research also shows that entry barriers and other opportunities for members to make community-specific investments can increase users\\' commitment to the system [14] . In this paper, we study two categories of long term behaviors: long-term commitment and long-term contribution. We are interested to know whether asking users to contribute information increases or decreases their usage of the system, or has no significant effect at all. Further, asking may achieve users\\' compliance as expected, however may affect users\\' voluntary contribution out of the requests when we stopping prompting.'],\n",
       " ['INTRODUCTIONWe identify that container-like understandings of urban tourism space [51] , as represented in the tourist-historic city model [12] [13] [14] , resemble the traditional framing of space as a \"natural fact\" [64] in the CSCW community. Both approaches, however, are lacking satisfying explanations of how tourism space can emerge and develop in residential neighborhoods. The tourist-historic city model solely takes into account tourism-related facilities and infrastructure as defining elements of tourism space. Such structures are often non-existing in new urban tourism areas like Kreuzkölln, one of our case-study neighborhoods. Still, this neighborhood without any major sights is becoming a tourism hotspot [33, 55] . Against this background, we follow a constructionist understanding of urban tourism space [51, 68, 136] . We argue that tourism space, like tourist sights [84] , is socially constructed through representations [99, 102] and performances [15, [45] [46] [47] 74] . That means we no longer regard tourism facilities and infrastructure as the central elements defining tourism space. Instead, people are the major agents who transform places and landscapes into tourist destinations. They attach meanings and values to places and objects [37, 102] , produce written, oral, or pictorial representations of them, and thus contribute to the discourse of how places or objects are to be perceived. Finally, following the performative turn in tourism studies [74, 75] , we argue that places need to be enacted through \"bodily performances\" [15, 75] . Practices, such as picture taking or collectively \"gazing\" [122] upon a building, are necessary to enact places in a touristic manner [76] . Taking these theoretical considerations into account, we analyze how two different Berlin neighborhoods, Kreuzkölln and City West, are socially constructed in Airbnb listings. The following three questions guided our research: RQ1: How are the two neighborhoods Kreuzkölln and City West constructed as tourism spaces in Airbnb listings? RQ2: How does the space construction differ between these two neighborhoods? RQ3: How do the neighborhood descriptions differ between Airbnb hosts and the destination management and marketing organization (DMO)?'],\n",
       " [\"ResultsBesides, a first analysis of the data exhibits many outliers, in particular for the one-command listeners. To understand the presence of these outliers, we manually scrutiny some of them and their change history. We observe that some of these outliers are GUI listeners which size has been reduced over the commits. For instance, we identified outliers that contained multiple GUI commands before commits that reduced them as one-or two-command listeners. Such listeners distort the analysis of the results by considering listeners that have been large, as one-or two-command listeners. We thus removed those outliers from the data set, since outliers removal, when justified, may bring benefits to the data analysis [25] . We compute the box plot statistics to identify and then remove the outliers. Figure 2 depicts the number of fault fixes per LoC (i.e., FIX) of the analyzed GUI listeners. We observe an increase of the fault fixes per LoC when CMD ≥ 3. These results are detailed in Table 1 . The mean value of FIX constantly increases over CMD. Because these data follow a monotonic relationship, we use the Spearman's rank-order correlation coefficient to assess the correlation between the number of fault fixes per LoC and the number of GUI commands in GUI listeners [25] . We also use a 95 % confidence level (i.e., p-value<0.05). This test exhibits a low correlation (0.4438) statistically significant with a p-value of 2.2 × 10 −16 . Regarding RQ1, on the basis of these results we can conclude that the number of GUI commands per GUI listeners does not have a strong negative impact on fault-proneness of the GUI listener code. This result is surprising regarding the global increase that can be observed in Figure 2 . One possible explanation is that the mean of the number of bugs per LoC slowly increases over the number of commands as shown in the first row of Table 1 . On the contrary, the range of the box plots of Figure 2 strongly increases with 3-command listeners. This means that the 3+-command data sets are more variable than for the 1-and 2-command data sets. Figure 3 depicts the number of commits per LoC (i.e., COM-MIT) of the analyzed GUI listeners. These results are also detailed in Table 1 . We observe that COMMIT does not constantly increases over CMD. This observation is assessed by the absence of correlation between these two variables (0.0570), even if this result is not statistically significant with a p-value of 0.111. We can, however, observe in Figure 3 an increase of COMMIT for the three-command listeners. Regarding RQ2, on the basis of these results we can conclude that there is no evidence of a relationship between the number of GUI commands per GUI listeners and the change-proneness of the GUI listener code.\"],\n",
       " ['METHODOLOGYThe method described henceforth was influenced and heavily indebted to the exemplary review into Sustainable HCI conducted by DiSalvo et al (2010) [25] and the previous research [26, 41] which inspired their approach to reviewing and framing research questions. However, the focus in this review is shifted towards interpersonal rather than environmental relations. Of crucial importance is how the HCI community conceptualises prosociality and the scope of human agency within the economy. The review process began by constructing a relevant corpus of papers from which specific examples could then be selected. The search term \"Prosocial HCI\" was used in the ACM Guide to Computing Literature with all results noted and added to the corpus. Any papers that were cited in each corpus paper were then also examined to see if they fit within the original scope and were added where suitable. The criteria for determining suitability were twofold. Firstly, it was asked whether each work had an explicit goal which was related to prosocial economic relations.'],\n",
       " ['INTRODUCTIONThere are two implicit assumptions in the above research question, which are intuitive but not always confirmed by prior studies. The first assumption is that profiles length positively affects user utility. Some works show that profile length of new users is positively correlated to the accuracy of recommendations in term of user utility [12] [13] [4] . However, this result cannot be easily generalized, as its supporting experiments are limited to item-based collaborative algorithms, and accuracy is measured only in terms of error metrics: RMSE [13] and MAE [4] . Moreover, [28] finds that the correlation between profile length and utility is not always present, but it depends on the elicitation strategy adopted. These studies instill some doubts on the general assumption that a longer profile corresponds to more accurate recommendations. We may wonder, for example, to what extent we can claim that the fallout of a content-based recommender algorithm improves with the profile length.'],\n",
       " ['INTRODUCTIONTelehealth systems are commonly used for managing acute and chronic conditions where regular monitoring of vital signs (e.g. heart rate and weight) is crucial. However, requirements for general health support are different and include education, monitoring, psychological support (motivation), and adding social components for patient support. In previous work, we demonstrated the advantages of leveraging consumer-level devices such as off-theshelf computers and motion-sensing input devices to make telehealth more accessible and affordable [1] . In this paper, we systematically categorise and analyse a range of sensing devices for healthcare, particularly their capability to extend the functionalities of telehealth systems. We evaluate each device based on its technology, pros and cons of its usage in healthcare, and findings from related studies. We try to answer the following research questions: 1) \"how can off-the-shelf consumer-level sensing devices be categorised for healthcare applications?\", and 2) \"how have these devices already been leveraged in healthcare?\" Section 2 reviews and analyses common consumer-level sensing devices. Section 3 presents results of the analysis and discusses how these devices can be leveraged in telehealth systems and Section 4 concludes the paper.'],\n",
       " ['Trade-Off between Dynamic Programming and Feature LocalityIt is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005) . There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task.'],\n",
       " ['4value oi the Beta attribute of tile top-most chooser. After classifying the input structure according to tile features it contains (already yielding a paxticular subtype of CHOOSER), the type of the topmost node of the input structure is then recursively expanded. Expansion is performed by rewriting all embedded types through unifying substitution of their definitio~ls until no filrther rewriting is possible (i.e., until all types are ground types). Expansion terminates with a conq)lcte description coralmtible with the input partial description and with the definitions ill the feature type system representing \"all the linguistic strata defined. In the general case, we will end up with not just sac description, lint rather with a set which is then to be interpreted as a disjunction of possil~le solutions to the initial problem. The complete structure which is the result of the interprctation of the semantic specification (given under the nero feltture) is given in Figure 3 HPSG, does provide cxtensive detail at this level. Now, due to the strict modularity enforced in our translation, it is possible to explore combinations of approaches and, moreover, to combine descriptions from a theory like HPSG with the kind of descriptions employed in Systemic Linguistics or its Computational instautlations. This has been shown to be possible il~ a simple experiment carried out by Martin Emelc where an existing HPSG grammar was taken and the semantics of that grammar (a simple situation semantics-informed frame-like representation) was rewritten to give tim syntagmatic categories and structures of the SFG. This makes it possible to describe the information ot)tained front the two approaches within a single executable declarative specification. Her(\\'., however, our utain concern has been with making available the higher-levels of specification, and so wc will abstract away front the string to syutagmatic structure component of the mapl)iug and take as the \\'input\\' specification the lowest level ofinforroation obtained from the SFG, as shown at)ove. Therefore, we proceed by putting this specification in tile syn slot of the IthNK-CHOOSER relation. Tcrnl rewriting applies to construct the sere side of tim relation and also to complete tile syn sl)ecification. The result is again tile COml)lete specification of the set of constraints titat describe the structure, which is again the structure shown in Figure 3 . This is precisely the same linguistic-sign that was produced as a result of \"generation\", starting froln the imrc semautic part of the descril)tion - descriptions beiug developed within the PENMAN, KOMET and POLYGLOSS projects. We have shown that systemic-functional grammars and semantics can easily be converted to the TFS formalism. This has produced a fragment that can both generate and analyse. Furthermore, the analysis achieved with our experimental fragment supports the mapping from surface representation to deep semantic levels of representation that are far removed from the contingencies of surface syntax. These represeutatimts also preserve breadth, in that the semantic distinctions necessary for generation concerning \\'pragmatic\\' information such as textual organization and interpersonal communicative goals are also recovered. It is especially imt)ortaut that all of these diverse levels have now been made accessible for analysis within a system where there is only one representational formalism and only one interpretational device operating on the representations. This paper has described and motivated the basis for a host of important further research questions, some of which we are now following up. For example, the fragment we have illustrated here is very small: the problem of handling large lattices needs to be addressed both on implementational aud theoretical levels. A fldl specification of the grammar component of PENMAN alone as we describe it here would involve tens, possibly hundreds, of thousands of types: this ueeds to be supported by sufficiently powerful and robust implementations. But on the theorcticai level, there are also further nmdularities within the SFG account that we have not yet utilized to constrain term explosions due to forming cross-products across sublattices: two areas here clearly present themselves ---stronger modularization according to the paradigmatic/syntagmatic dimensiou and according to functional regions in the grammar [4] , which already provide a meta-level of organization across sublattices that remains unused. A fnrther area is a closer study of the similarities and differeuces between, e.g., the information of the SFG and the HPSG modules --it is to be expected that there is currently duplication which could be more effectively distributed, perhaps providing a more effective TFS translation. Finally, the availability of a representation of some systenfic-functional grammars in a standard formalism should further facilitate comparison and evaluation of the grammatic\\',d description with respect to other current cmnputational accounts of grammar: it should be more straightforward to identify the distinctive features and claims of the approach, thus opening the door to an easier exchange of information and analyses. Further, performing the TFS translation for the entire PENMAN grammar would provide an effective test of the TFS formalism (and its implementation) overall since there are no comparable grammars (i.e., paradigmatic feature based without a phrase structure skeleton) of this size available elsewhere.'],\n",
       " [\"RQ1a: Search Engine Bias Quantification FrameworkFor instance, the last row of Table 1 computes OB(q, r) at rank r = 5 with respect to the situation shown in Figure 1 . Note that the bias score s 2 of the top-ranked item i 2 is given the highest weight, followed by the bias score s 4 of the second-ranked item i 4 , and so on. This follows the intuition Session: Politics, Party, Policy, & Participation CSCW 2017, February 25-March 1, 2017, Portland, OR, USA that bias in the higher ranked items are likely to influence the user more than bias in the lower ranked items. 2 Ranking Bias: The ranking bias is intended to capture the additional bias introduced by the ranking system, over the bias that was already present in the set of relevant items (i.e., relevant to q) input to the ranking system. If possible, ranking bias could be measured by auditing the exact ranking system being deployed by the search engines. However, for any commercial search engine deployed in the real-world, it is infeasible to know the internal details of the ranking system. Hence, we view the ranking system as a 'black box' where we only observe the inputs and outputs.\"],\n",
       " ['PROCESS: SCAFFOLDING AN ITERATIVE AND REFLECTIVE PROCESSTo support the process of children engaging as protagonists, we employed a process model that could support this particular stance throughout the entire design process. The model was based on six main activities. The circular model illustrates design as an iterative process, since all design outcomes eventually lead to new research questions and new problem framings. Moreover, iterations occur within and between each of the activities and with increaasing experience, eventually crisscrossing effects are created that use the model as a framework for navigating through one\\'s own design project. The model contains six main design activities, each including several sub-activities: (1) the design brief, for framing a complex challenge and planning the design process; (2) field studies, to explore and research the context and users; (3) ideation, for the creative development of ideas using various techniques and materials; (4) fabrication, for mock-up and prototyping of concepts using digital technologies; (5) argumentation, for testing a design concept or product and reflecting on the design moves and arguments of the process; and (6) reflection, for reflecting on the learning outcome-or design competence-developed through the entire design process (see Fig. 1 ). For a description of the process model, see Smith et al. [34] . We recognize that our process model is one among several suitable design models to illustrate iterative design processes. However, this particular model was designed to embed certain characteristics that support children as protagonists throughout the design process. First, the design model supported an ongoing interplay between divergent and convergent thinking and doing. Divergence and convergence notoriously compel the children to open up their design process and take in new perspectives and subsequently deselect (potentially important) aspects in their efforts to reach a meaningful design solution. The interplay between divergence and convergence demands a high level of commitment, agency, and determination. These in turn sustain the children\\'s role as protagonists in the process. Second, the design model incorporates argumentation and reflection as the closing activities of each iteration. By positioning reflection as the outcome of the design activity, the design process closely ties its aim to the objective of developing a reflecting stance toward technology and design, as described in the \"child as protagonist\" perspective. Third, the design model does not prescribe either specific actions or project measures. It merely indicates how the design process develops. This is to encourage the children to be mindful about their process, their collaboration, and their choices during the process, rather than focusing solely on the tangible outcome. There are no formal instructions in the model, indicating that there is no correct way for children to proceed through the stages of project framing, research, ideation, and fabrication to the final stages of argumentation and reflection. This lack of authority in the model transfers a high level of self-efficacy and agency to the children. They have to stay in charge of the design process while they gradually explore the design brief and activities provided by the design experts.'],\n",
       " ['IntroductionFurthermore, as social robots, drones face a robotontological issue, namely, safety. Dautenhahn and his colleagues [13] studied human comfort while interacting with a social robot. They thought that feelings of safety with a robot would be impossible to study and instead user comfort should be the focus. In this paper, perceived safety includes 2 Advances in Human-Computer Interaction both of these meanings and we constructed a variable that could influence the level of satisfaction in social drone services. Little by little, academic focus has shifted to human perceptions of social drones. Cauchard and her colleagues [14] studied drones as a type of social computing that features an affective factor. However, this is merely a starting point in the study of social drones. In this paper, we will explore the relationship between user satisfaction and two fundamental issues: drone control conditions and perceived safety. Thus, in order to empirically investigate this relationship, the present study examines the following research question. [15] define an avatar or an agent as follows: \"a perceptible digital representation whose behaviors reflect those executed, typically in real time, by a specific human being.\" These terms are often encountered when using a computer application or playing a game. Examples include the clipper in MS office, Siri on an iPhone, and various avatars featured in the game Second Life. Depending on who controls these characters (a human versus the system), there can be different perceptions or service evaluations. Lim and Reeves [9] studied the engagement of avatars and agents, part of the game experience, and found that playing a game with avatars showed improved engagement over the use of agents. Concerning general interactions, Cauchard and her colleagues [14] studied the social evaluations during interactions with digital human representations and observed that there is a difference in social evaluations of avatar and agent environments when digital human representations were made to smile, one of the social cues of interaction. Namely, there was a tendency toward a negative evaluation of the smile of a digital human, which is an agent.'],\n",
       " ['RQ4. How is sleep debt related to mood?A meta-analysis of laboratory sleep studies found that sleep deprivation significantly negatively impacts mood [42] . However, in contrast, a study of college students who were sleep deprived for 24 hours showed no significant changes in mood, such as negative mood states of anger and anxiety [43] . The authors explain this to the notion that 24 hours of sleep deprivation is long enough to affect fatigue but not long enough to impact mood. A large cross-sectional survey study of college students though did find a relationship between self-reported sleep disturbances and negative mood [28] . Because sleep loss over 24 hours did not affect mood, we expect that longer accumulated sleep could affect mood. Therefore in this research question we examine sleep debt.'],\n",
       " [\"INTRODUCTIONAs HCI has responded to broader societal challenges such as sustainability [16] and healthy living [2, 7, 15 ] the field has increasingly explored how technologies might be used to promote behavioural change. Working in this area, the focus of this paper is on the participatory design of a range of technologies to motivate exercise for people recovering from a stroke at home. Over a three-year time frame, we have worked closely with clinicians and patients to understand the stroke experience and how we might meet the varying needs of our participants. Here we present an account of the development of distinct solutions to motivate post-stroke rehabilitation exercises for four individuals who wished to recover upper limb functionality after stroke, and who volunteered to participate in our project. Following participatory design sessions in their homes, we have deployed four prototypes for periods ranging from four weeks to seven months. All participants needed to do rehabilitative exercise regularly at home, without professional support. However, adherence to programmes of rehabilitation at home is poor [20] perhaps because rehabilitative exercise can be boring and difficult to do. In response to these challenges, the research question driving this work was whether we could improve participants' adherence to a rehabilitation schedule by developing technologies that tapped into their individual motivations, but which are supportive of broader care goals, i.e., bridging between the domestic life of individuals recovering from stroke and their clinical care programmes.\"],\n",
       " ['Need to Support Learning Mechanics as well as ContentIn contrast to the uniform inclusion of effective teaching principles for reading in all of the games, with the exception of Nessy, the remaining four games reflected less effort in supporting learning of the game mechanics (gameplay mechanics; Table 3 ). Typically in games the player develops an understanding of the game play schema through experiencing failures at various points in the game and then trying again [50] . However, within learning games it is difficult to separate failure due to the game mechanic or failure due to a gap in understanding the learning content. Previous work has shown when children experience breakdowns during learning games they may need support with both the learning content and with working out the game mechanics [51] . This need for support has been found to increase in pace with the complexity of game mechanics [47, 52] . Plass et al. [50] recommend in learning game design the choice of game mechanics should not introduce these unnecessary confounds. Whilst the reviewed games mainly utilised more familiar multiple choice mechanics, given the young learner group we argue that they will still need opportunities to become familiar with the broader game play schema prior to focusing on new learning content. The most appropriate form for this support remains an open research question.'],\n",
       " ['PRIVACY ISSUES IN HOLISTIC RECOMMENDATIONAlthough holistic recommendation may provide undeniable advantages to end users, they use large amounts of potentially sensitive/private data (e.g., health data, location data, sexual preferences, and so on). Moreover, models and algorithms trained by using such behavioural data may leverage discriminating patterns, e.g., genderor ethnicity-based decisions that results in recommending two completely different products to two different persons (e.g., with different gender or ethnicity), sharing exactly the same behaviour. In addition, it is important to trace provenance and proof of the data in the user and context model, in the perspective to make the models explainable and scrutable by the user [24] and give her the possibility to know how the data in the models are gathered and inferred. Consequently, an ethical holistic recommender system algorithm or model must be fair, explainable and privacy-preserving. Fairness. With the recent advances in artificial intelligence and machine learning and the resulting concerns for their consequences on human freedom and rights, in the last decade, many research groups have addressed fairness issues [9, 11] . The topic, however, has been only superficially addressed in the recommender systems community [9] , even though this is a crucial aspect of holistic recommendation, due to the heterogeneity and complexity of data sources. Consequently, many research questions are still far from being solved or even discussed, in some cases. First, how widespread is the problem of algorithmic bias in recommender systems? How to address the complexity of holistic recommender systems in an efficient way without affecting the accuracy of the recommendations too much? How to detect discrimination in the underlying algorithms? How to regularize them in order to dismiss potentially discriminative patterns and decisions?'],\n",
       " [\"Mexican Americans and being Chicano in San FranciscoThe third analysis of San Francisco English, by Moonwomon (1992) , focused specifically on the low vowel system, analyzing phonetic shift in the vowels of TRAP, LOT, and THOUGHT. This study, however, eliminated ethnicity as part of the analysis through its methodological design. By confining her speaker sample to white women, Moonwomon investigated research questions particularly relevant to that demographic, considering variation in socioeconomic class as it is realized between speakers of comparable sex and ethnicity. At the same time, Moonwomon's choice to define her participant sample in this way implied that ethnic variation (and gender or sex variation as well) has such a potentially strong impact on the analysis of San Francisco English that it was beyond the scope of her study. Her analysis did confirm that the sound changes found among younger speakers in the Hinton et al. (1987) study (of which she was a part) were in progress within this sample, specifically that TRAP was raising and fronting before nasals, lowering and backing elsewhere, and that LOT and THOUGHT were moving toward merger. Furthermore, Moonwomon found differences between middle-class and working class women with respect to attitudes toward ethnic shift in San Francisco, with some of the working class participants expressing greater resistance to the increased Asian and specifically Chinese presence in their neighborhood. However, Moonwomon did not draw specific connections between the low vowel production patterns of these women and their orientations toward ethnicity, although she did find more advanced (i.e., more raised before nasals) productions of the TRAP vowel for the middle-class speakers. As Bucholtz' (2007, 10 inter alia) work in a Bay Area high school suggests, discourse about race often stands in for discourse about class, and analyses of class ideologies in the San Francisco area are inseparable from the local construction of ethnicity, particularly since the early 1990s.\"],\n",
       " ['MotivationHowever, Karamanis (2003) pointed out that many metrics of coherence can be derived from the claims of Centering, all of which could be used for the type of text structuring assumed in this paper. Hence, a general methodology for identifying which of these metrics represent the most promising candidates for text structuring is required, so that at least some of them can be compared empirically. This is the second research question that this paper addresses, building upon previous work on corpus-based evaluations of Centering, and particularly the methods used by Poesio et al. (2004) . We use the gnome corpus (Poesio et al., 2004) as the domain of our experiments because it is reliably annotated with features relevant to Centering and contains the genre that we are mainly interested in.'],\n",
       " ['Content Focuses on Assessment and Analysis (RQ2)However, these categories may be less suitable for the early stage. At this stage, students may benefit most from reflecting on how they approached the design, how the design compares to other solutions, and how it fits in a broader social and cultural context [37] . Feedback in the categories brainstorming (3%), comparison (3%), processoriented (2%), free association (2%), and identity-invoking (<1%) would best speak to those issues, but these categories were rarely addressed by any of the four sources.'],\n",
       " ['CONCLUSION AND FUTURE WORKPurchase incidence is a key pillar of every customer choice decision [7, 3] . Yet, so far it has only received marginal attention in the context of recommender systems. However, as commercial recommendations aim at influencing customer choices [2] , purchase incidence should be incorporated when a recommendation is made. For this reason, we have analyzed customer data from an in-store recommendation system at a brick-and-mortar grocery retailer that provides customers with tailored price promotions. Our findings support the view that recommender systems should not only focus on the product or service to recommend but also on the timing of the recommendation [12] . Regarding the first research question we find that in many fast moving categories, there exists an optimal point in time for a recommendation which maximizes its acceptance among customers. In particular, we show that considering this optimum and categoryspecific individual inter-purchase times, has a positive effect on the success of recommendations, increasing precision by up to 6 %. Thus, answering the third research question, incorporating this additional information improves recommender systems. Moreover, we find that the optimal point in time for a recommendation precedes the average category inter-purchase time for most categories, indicating accelerated category purchases. Finally, regarding the second research question, we find that recommendations can influence individual inter-purchase times.'],\n",
       " ['IntroductionIdeally a good paragraph is that it considers a topic to be discussed. The paragraph topic directs discussion on what ideas which are going to be further explored in a paragraph. The topic of discussion is commonly stated in a phrase and not in a complete sentence. Next to them, a good paragraph is composed of three main components; topic sentence, supporting sentence, and the concluding sentence. Then the question is that how these difficulties can be minimized or even eliminated within writing learning process that students have better paragraph writing ability. This simple question can be optionally solved by applying Process based Approach as the approach to teach students in learning to write English paragraph. Process based Approach is a teaching of process triggering students to create their own ideas step-by-step to result in a comprehensive organization of paragraph. Therefore, to provide for an appropriate approach in teaching writing will be able to lead students into having higher sense of creativity in learning to write English, particularly in learning to write English paragraph. Process based approach, based on the elaboration on the teaching approach, is claimed to be an effective teaching approach to pump up students\\' creativity in learning to write. Seeing the phenomena above, the researchers have in depth interest to research a process based approach to teaching writing entitled:\"Process based Approach towards Students\\' Writing English Paragraph Ability\". The use of process based approach is expected to be effective steps for students in the efforts of creatively developing their English paragraph writing ability. The main research question of the study is: Is Process based Approach effective towards students\\' English paragraph writing ability at the fourth semester students of English Department at IKIP Mataram in the academic year of 2016/2017? Oshima (2006: 2) defines paragraph as a group of related sentences that discuss one (and usually only one) main idea. A paragraph can be as short as one sentence or as long as ten sentences. The number of sentences is unimportant, however, the paragraph should be long enough to develop the main idea clearly. Further, Irawati, (2013: 5) states that paragraph is the basic unit of composition. Paragraph usually consists of several sentences, they are topic sentence, supporting or developing sentence and concluding sentence. Thus, it can be concluded that paragraph is a unit of composition consisting of several related sentences; topic sentence, supporting sentence, and concluding sentence which are coherently composed and usually developing one main idea.'],\n",
       " ['We looked at frequency and order of scenario types.When it comes to the setup, although we used a multitouch interactive surface throughout the study, predominantly a single domain expert led most of the interaction in each case study session. The rest of the participants discussed seated or standing, but refrained from interacting. This finding is supported by other studies on collaboration around interactive displays [47, 53] . We note that although most participants did not interact directly with the display, they were however actively involved in the exploration. For example, they proposed new research questions, requested to see particular views or to refine existing criteria. We also observed at least two instances in UC1 where domain experts explored the Pareto front in their own laptops.',\n",
       "  'Subjective User FeedbackTo the best of our knowledge, there are no other studies that looked at collaborative model exploration in real-world settings. In terms of results, we found similar processes to those described in general sensemaking literature [28, 41] . Our contribution here, however, is in identifying why these processes tend to occur (e.g., storytelling to recap), and when they occur (e.g., storytelling periodically for large groups, or at the end of big chunks of exploration for smaller groups). Alignment is a particular process to model exploration, also described in [11] . However, our work considers more complex models, multiple computational stages and co-located expertise. We discuss next seven key findings from our study, relating them to our initial research questions, and comment on the applicability of our methods to other domains:',\n",
       "  'Subjective User Feedback1. Exploration as Multiple Linked Analysis Scenarios [Q1] We observed that exploration is split into mini-exploration scenarios. Trade-off analysis starts with a preliminary exploration often leading to a focused research question. The remainder of the exploration is characterised by the nonlinear interleaving of new and refined hypotheses and research questions [41] , operating on a variety of exploration objects. Those scenarios denote a shift in the research questions and hypotheses set out by experts, which often result in change of focus in the model or data space. A parallel can be drawn between our approach and the data-frame model described in [28] . For instance, their \"reframing\" maps to our new scenario, and \"elaborating the frame\" to our refine scenario. In our case, however, re-framing revealed itself to happen specifically when participants shift their research questions and hypotheses. Furthermore, we provide a more fine grained analysis of the exploration, by crossing high level categories of interest (e.g., insight, expertise), with exploration objects (correlation, exploration method).'],\n",
       " ['II. RELATED WORKIn robotics, the delay inherent to control loops can have a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous robots. Visual servoing of a robot, for example, can be sensitive to the delays introduced through image acquisition and processing [9] . Similarly, delays in proprioception can produce instabilities during dynamic motion generation. In [2] , a dynamically smooth controller has been proposed that can deal with delay in proprioceptive readings. However, the approach assumes constant and known time-delay. A major milestone in robot control with time-delay was the ROTEX experiment [8] . Here, extended Kalman filters and graphical representation were used to estimate the state of objects in space, thereby enabling sensor-based long-range teleoperation. How to effectively deal with such communication delays has been a central research question in robotic tele-operation. Delays in robot control loops are not limited to sensor measurements only. A prominent approach for dealing with actuation delays is the Smith Predictor [17] . The Smith Predictor assumes a model of the plant, e.g. robot system, and can become unstable in the presence of model inaccuracies. A different approach has been proposed in [3] . A neural network was first trained to predict the state of mobile robots based on positions, orientations, and the previously issued action commands. The decision making process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in our paper follows a similar line of thought. However, instead of predicting specific states of the robot, we are interested in predicting the delay occurring at different parts of the control loop.'],\n",
       " ['Interaction-Dominant DynamicsHowever, not all research questions can be addressed in within-participant designs (e.g., research where learning effects during first exposure are important, applied research-for example on psychotherapy), not all manipulations can be operationalized in terms of continuous incremental changes (e.g., research employing complex action-sequences, research contrasting qualitatively different situations), and not all studies can be conducted in a way to collect a sufficient number of data points to conduct fractal analysis (e.g., questionnaire research, observational research, research employing invasive measurement procedures…). For some of these cases, effectively relying on the CDD logic (at least its measurement and analysis part) might be inevitable. For other cases, some work arounds might be to run different experimental conditions back-to-back, and continue measuring across the period where one task end and the new task (or experimental condition begins). The transition between two experimental conditions can be similarly informative as to whether these two conditions are comparable, or lead to a re-organization of the cognitive system (Wallot 2014) .'],\n",
       " [\"Main findings:VII. THREATS TO VALIDITY In 2015, Petersen et al. [16] created a checklist for objectively assessing the quality of systematic mapping studies. In this context a score can be computed as the ratio of the number of actions taken in a study in comparison to the total number of actions in the checklist. In our case we achieve a score of 65%, far higher than most systematic studies in the literature, which have a distribution with a median of 33% and 48% as the absolute maximum value. As always, however, threats to validity are unavoidable. The following reports on the main threats to validity to our study and how we mitigated them. External validity. The most severe external threat of our study consists in the fact that our primary studies are not representative of the state of the art on architecting microservices. As a solution, we applied a search strategy consisting of both automatic search and backward-forward snowballing on the selected studies in combination. Also, we considered only peer-reviewed papers and excluded the so-called grey literature (e.g., white papers, editorials, etc.); nevertheless, this potential bias did not impact our study significantly since considered papers have undergone a rigorous peer-reviewed process, which is a well-established requirement for high quality publications. We also applied well-defined and previously validated inclusion and exclusion criteria, which we refined iteratively by considering the pilot studies of our review. Internal validity. We rigorously defined the research protocol of our study and we iteratively defined our classification framework by rigorously applying the keywording process. The syntheses of the collected data have been performed by applying well-assessed descriptive statistics. During the horizontal analysis we made a sanity test of the extracted data by cross-analyzing parameters of the classification framework. Construct validity. We mitigated this potential bias by automatically searching the studies on multiple data sources, independently of publishers' policies or business concerns; also we are reasonably confident about the construction of the search string since the terms used are very general and suited to our research questions; the automatic search has been complemented with snowballing. Also, we rigorously selected the potentially relevant studies according to welldocumented inclusion and exclusion criteria. This selection stage was performed by one researcher and, as suggested by [20] , a random sample of potentially relevant studies was identified and the inter-researcher agreement was ensured. Conclusion validity. We rigorously defined and iteratively refined our classification framework, so that we could reduce potential biases during the data extraction process. In so doing we also have the guarantee that the data extraction process was aligned with our research questions. More in general, we mitigated potential threats to conclusion validity by applying the best practices coming from three different guidelines on systematic studies [7, 16, 20] . We applied those best practices in each phase of our study and we documented each phase in a publicly available research protocol, thus making our study easy to be replicated by other researchers.\"],\n",
       " [\"The relationship between academic literacy and vocabulary size at word bandsIn order to gain more insights into the link between vocabulary size and academic literacy (related to the second research question), the relationship between vocabulary size and academic literacy was explored further by mapping each frequency word-band score onto TALL levels. The aim was to examine the relationship between vocabulary size and academic literacy at the level of word bands, and to find out which frequency word bands were completely mastered by the students. This was achieved by running a one-way ANOVA at each of the frequency word bands. The mean scores and standard deviations are presented in Table 4, while  Table 5 presents the ANOVA results. Table 4 , scores at each word band vary from one level to another, which entails that the same levels identified by the TALL are also identified by the VLT scores. Furthermore, on average, the mean scores achieved out of 30 are satisfactory -we found a mean score of 28.05 at the AWL, 29.03 at the 2,000-word, 28.20 at the 3,000-word, and 25.26 at the 5,000-word bands. These scores were weighed against Schmitt et al.'s (2001) cut-off point. According to Schmitt et al. (2001) , a vocabulary band is mastered if the score at that band is at least 24 out of 30. Considering the above scores, it appears that a huge majority of the participants has achieved the suggested threshold. Note, however, that as many as 11.30% of the students (i.e. 39 students out of 345 participants) did not reach the minimum required score.\"],\n",
       " [\"DiscussionThe first question aimed to discover whether the initial expectations of children would be lower when presented with a low-fidelity prototype. The mean scores before the children played the game were similar with the Screen version having the highest mean. It was expected that the iPad version would have the highest score but this did not prove to be the case. In a study examining aesthetics in prototypes with adults, users appeared to compensate for deficiencies in aesthetic design by overrating the aesthetic qualities of reduced fidelity prototypes [12] . The issue of overcompensation might have occurred with the children being over enthusiastic when rating, for example the screen version, when rating the game with the Smileyometer; additionally it has been shown in other studies that children are generous in their evaluations of software [42] , but this is generally associated with younger children. Although the age range of the children was 7-9 these were balanced within each groups, see Table 5 for average age of the children in each group. The second research question aimed to establish if there is any difference, depending on fidelity, between children's ratings of their overall experience of playing the game. The results of the Smileyometer after the children had played the game suggest that the low-fidelity sketch version is similar to the high-fidelity iPad version, whilst the screen version was lower. However the Again Again table showed that only 58% of the children who played the sketched version stated that they wished to play the game again. This is compared to 69% for the iPad and 75% for the screen version. All three versions were favored by the children in so far as there were no children stating that they did not want to play the game again on the iPad, only one did not wish to play on the screen and just two did not want to play on the sketch version. It would appear as though the game experience can be predicted through analyzing lower fidelity prototypes (within the constraints of this game genre) using the two tools within the Fun Toolkit.\"],\n",
       " [\"IntroductionChiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. However, due to lack of linguistic knowledge, Chiang's HPB model contains only one type of non-terminal symbol X, often making it difficult to select the most appropriate translation rules. 1 One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima'an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable.\"],\n",
       " [\"INTRODUCTIONUsers often represent the last line of defense between attackers and organizations. User responses to security warnings is thus a critical aspect of behavioral security [5, 26, 34] . A major inhibitor of the effectiveness of security warnings is habituation: diminished attention due to frequent exposure to warnings [36] . Through this process-also known as warning blindness [53] or fatigue [1] -users' attention to warnings can attenuate to the point where they hardly see the warning any longer. Although this problem is widely recognized [e.g., 3, 6, 21, 32, 38, 46] , a major limitation of past studies that examine habituation is that they used cross-sectional (i.e., single point in time) experimental designs. However, habituation is fundamentally a neurobiological phenomenon that develops over time [40] . Thus, past research on habituation to security warnings has provided only a snapshot of a dynamic problem. Our first research question is therefore:\"]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rq for rq in rqs if rq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to jsonl file\n",
    "# with open('allQuestions_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "with open('RQs_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "    for idx, rq in enumerate(rqs):\n",
    "        if rq:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    'intro': intros[idx],\n",
    "                    'relatedWork': relatedWorks[idx],\n",
    "                    'rq': rq\n",
    "                }\n",
    "            ))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# check how many papers have intro, related work and RQs\n",
    "with open('RQs_s2orc.jsonl', 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "    cnt = 0\n",
    "    for d in data:\n",
    "        if ( d['relatedWork']) and d['rq']:\n",
    "            cnt += 1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "# check how many papers have intro, related work and RQs\n",
    "with open('allQuestions_s2orc.jsonl', 'r', encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "    cnt = 0\n",
    "    for d in data:\n",
    "        if (d['intro'] and d['relatedWork']) and d['rq']:\n",
    "            cnt += 1\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# only extract related works\n",
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    path = './s2orc_hci/pdf_parses/pdf_parses_%d.jsonl'%i\n",
    "\n",
    "    # load jsonl file\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "\n",
    "\n",
    "    for dPaper in data:\n",
    "        rqs.append(get_RQ_text(dPaper))\n",
    "        intros.append(get_introduction_text(dPaper))\n",
    "        relatedWorks.append(get_relatedWork_text(dPaper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4034"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([r for r in relatedWorks if r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "038ac43ff01c31cd641835814f979bd9daf9dcda5104326cfb2cc592d72d42d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
