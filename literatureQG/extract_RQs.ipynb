{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# import fasttext\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models import ldaseqmodel\n",
    "# from gensim import corpora\n",
    "# import gensim.downloader as api\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    \n",
    "    # stemming and lem\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although all these works rely on rapport-building conversational strategies, few of them investigate how rapport-building dialogues influence the perceived quality of the items recommended, or people's compliance towards the recommendations. Moreover, they do not investigate the impact of users' interaction mode on users' perceptions. In this paper, we aim at building a conversational recommender system that recommends recipes while building rapport with its users. More specifically, in this paper, we focus on the following research questions: RQ1: How does the way users interact with a conversational recommender system influence their perception of and their intention to cook recommended recipes? RQ2: How do a conversational recommender system's conversational strategies influence users' perception of and their intention to cook recommended recipes?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in xmls\n",
    "# select sections containing discussion\n",
    "tree = etree.parse('./papers/conversationalAgent/HC_paper_all/xml/17.xml')\n",
    "\n",
    "# select sections containing discussion\n",
    "nodes = tree.xpath(\"(//*[(\"\n",
    "        \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "        \",'rq1:') or \"\n",
    "        \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "        \",'rq 1:')\"\n",
    "        \")]) [last()]\")\n",
    "if len(nodes) != 0:\n",
    "    node = nodes[0]\n",
    "    title = ''.join(node.xpath(\"text()\"))\n",
    "    text = ''.join(node.xpath(\"../descendant::*/text()\")[1:])\n",
    "    # print(title)\n",
    "else:\n",
    "    print(\"NOT FOUND!\")\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction_text(path):\n",
    "    \"\"\"Get discussion text from xml file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        Discussion text.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(path)\n",
    "    nodes = tree.xpath(\"(//*[local-name()='head' and (\"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'introduction')\"\n",
    "            \")]) [last()]\")\n",
    "    if len(nodes) != 0:\n",
    "        node = nodes[0]\n",
    "        title = ''.join(node.xpath(\"text()\"))\n",
    "        text = ''.join(node.xpath(\"../descendant::*/text()\")[1:])\n",
    "        # print(title)\n",
    "    else:\n",
    "        print(\"NOT FOUND!\")\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_discussion_text(path):\n",
    "    \"\"\"Get discussion text from xml file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        Discussion text.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(path)\n",
    "    nodes = tree.xpath(\"(//*[local-name()='head' and (\"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'discussion') or \"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'implication')\"\n",
    "            \")]) [last()]\")\n",
    "    if len(nodes) != 0:\n",
    "        node = nodes[0]\n",
    "        title = ''.join(node.xpath(\"text()\"))\n",
    "        text = ''.join(node.xpath(\"../descendant::*/text()\")[1:])\n",
    "        # print(title)\n",
    "    else:\n",
    "        print(\"NOT FOUND!\")\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_relatedWork_text(path):\n",
    "    \"\"\"Get discussion text from xml file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        related work text.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(path)\n",
    "    nodes = tree.xpath(\"(//*[local-name()='head' and (\"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'related work') or \"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'background')\"\n",
    "            \")]) [last()]\")\n",
    "    if len(nodes) != 0:\n",
    "        node = nodes[0]\n",
    "        title = ''.join(node.xpath(\"text()\"))\n",
    "        text = ''.join(node.xpath(\"../descendant::*/text()\")[1:])\n",
    "        # print(title)\n",
    "    else:\n",
    "        print(\"NOT FOUND!\")\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def get_RQ_text(path):\n",
    "    # Read in xmls\n",
    "    # select sections containing discussion\n",
    "    tree = etree.parse(path)\n",
    "\n",
    "    # select sections containing discussion\n",
    "    nodes = tree.xpath(\"(//*[(\"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'rq1:') or \"\n",
    "            \"contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz')\"\n",
    "            \",'rq 1:')\"\n",
    "            \")]) [last()]\")\n",
    "    if len(nodes) != 0:\n",
    "        node = nodes[0]\n",
    "        title = ''.join(node.xpath(\"text()\"))\n",
    "        text = ''.join(node.xpath(\"../descendant::*/text()\")[1:])\n",
    "        # print(title)\n",
    "    else:\n",
    "        print(\"NOT FOUND!\")\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./papers/conversationalAgent/HC_paper_all/xml/17.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Healthy eating implies complex decision making processes [6], including being aware of healthy options and choosing among them [24]. One solution to overcome this issue and help people to make healthier choices is to develop health-aware food recommender systems [31]. While significant effort has been put recently into optimizing the food selection algorithms [30], many other factors can also influence users\\' overall experience when interacting with a recommender system [14]. Indeed, the way the recommendation is presented [18], the system\\'s response time [33], or even the length of the system\\'s utterances [20] can have an influence on users\\' perception of the system.One trend to improve users\\' experience is to make the interaction more natural by designing the recommendation process as a conversation [23]. Besides helping users to achieve task-oriented goals, conversations can also fulfill interpersonal functions, such as building rapport [29]. Rapport can be described as a dynamic process that can be achieved when people \"click\" with each other or feel the interaction is due to \"chemistry\" [27]. Human-human studies have found that rapport between two people can influence task performance in situations as diverse as peer-tutoring [25] and negotiation [7]. Based on these findings, it becomes important to endow recommender systems with social conversational infrastructure that would allow them to build rapport with their users to improve task effectiveness.In this paper, we present a conversational system able to recommend recipes matching users\\' needs while building rapport with them. More specifically, our work focuses on investigating how the conversational skills of a recipe recommender system and the interaction modes it offers to its users would influence users\\' perception and their intention to cook. First, we describe the design of our system and its architecture before we explain how the recommendation process works. Then, we evaluate our system through an experiment in which we study the impact of our system\\'s conversational skills and interaction mode on its persuasiveness. Our main contributions are (1) a rapport-building conversational approach to deliver recipe recommendations adapted to users\\' needs and habits and (2) a subjective evaluation investigating the influence of a recommender system\\'s conversational skills and interaction mode on users\\' perception of the system, users\\' perception of the interaction and users\\' intention to cook the recommended recipes.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_introduction_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although all these works rely on rapport-building conversational strategies, few of them investigate how rapport-building dialogues influence the perceived quality of the items recommended, or people's compliance towards the recommendations. Moreover, they do not investigate the impact of users' interaction mode on users' perceptions. In this paper, we aim at building a conversational recommender system that recommends recipes while building rapport with its users. More specifically, in this paper, we focus on the following research questions: RQ1: How does the way users interact with a conversational recommender system influence their perception of and their intention to cook recommended recipes? RQ2: How do a conversational recommender system's conversational strategies influence users' perception of and their intention to cook recommended recipes?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_RQ_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Food Recommender Systems. A common approach for food recommender systems is to recommend a recipe based on its ingredients. In [8], for example, the authors developed a system that relies on recipes that people like to infer their preferred ingredients. The system then recommends new recipes containing the previously inferred ingredients. In [9], the authors developed a system that collects users' preferences by asking them to rate and tag the recipes they usually cook at home. The system then relies on user's preferences to rank recipes and deliver recommendations with the highest scores. This Matrix Factorization algorithm outperformed the content-based approach proposed by [8]. Other approaches only rely on dietary information to recommend recipes that would match users' needs. YumMe, the recommender system developed in [36], automatically extracts dietary information from pictures of recipes to form a user profile. The system then relies on this user profile to deliver subsequent recommendations. In [11], authors analyzed people's eating behavior and clustered people in two categories: those interested in getting healthy recipes, and those who did not care about that. They found that two of the main recipe rating predictors for the first group were the fat and calorific content of the recipe, and decided to incorporate these features in their recommendation process.All these works focus on improving recommendation algorithms. They do not investigate how the modality of the interaction between the system and its users can improve users' experience which, according to [15], should not be neglected.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relatedWork_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# picking out papers with no RQs detected\n",
    "# papers = glob.glob('./papers/conversationalAgent/HC_paper_all/xml/*.xml')\n",
    "with open('RQs.jsonl', 'r') as f:\n",
    "    dPapers = [json.loads(line) for line in f]\n",
    "paperPaths = ['./' + '/'.join(d['path'].split('/')[-5:]) for d in dPapers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for d in dPapers if d['relwork']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPapers = glob.glob('./papers/**/*.xml', recursive=True)\n",
    "# allPapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: ./papers/conversationalAgent/HC_paper_all/162.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/88.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/176.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/63.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/77.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/189.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/214.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/200.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/201.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/215.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/76.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/188.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/62.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/89.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/163.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/175.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/161.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/149.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/74.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/60.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/203.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/217.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/202.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/61.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/75.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/49.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/148.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/160.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/174.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/158.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/170.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/71.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/65.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/59.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/206.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/212.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/207.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/58.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/70.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/159.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/99.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/167.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/66.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/198.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/8.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/211.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/205.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/204.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/210.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/9.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/67.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/199.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/172.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/98.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/166.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/101.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/129.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/14.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/15.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/128.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/114.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/100.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/102.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/16.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/103.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/117.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/113.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/107.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/13.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/106.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/112.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/138.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/104.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/110.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/11.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/39.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/38.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/105.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/139.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/120.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/108.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/21.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/35.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/34.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/20.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/121.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/137.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/123.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/36.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/22.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/23.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/37.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/122.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/136.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/132.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/126.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/33.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/27.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/26.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/32.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/133.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/119.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/125.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/30.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/18.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/19.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/31.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/25.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/130.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/124.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/118.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/143.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/157.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/81.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/95.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/180.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/194.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/42.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/56.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/4.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/209.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/208.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/57.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/5.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/43.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/181.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/94.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/156.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/142.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/140.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/168.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/96.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/82.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/197.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/69.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/183.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/7.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/55.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/41.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/40.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/6.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/182.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/196.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/68.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/83.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/169.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/97.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/141.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/179.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/87.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/151.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/145.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/50.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/2.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/44.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/192.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/186.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/187.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/79.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/193.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/45.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/51.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/3.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/144.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/150.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/178.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/86.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/84.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/90.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/146.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/152.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/1.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/53.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/219.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/190.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/184.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/52.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/46.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/153.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/147.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/91.pdf: No such file or directory\n",
      "cp: ./papers/conversationalAgent/HC_paper_all/85.pdf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# delete and recreate folder nonRQs\n",
    "os.system('rm -r ./papers/nonRQs')\n",
    "os.makedirs('./papers/nonRQs', exist_ok=False)\n",
    "os.makedirs('./papers/nonRQs/conversationalAgent', exist_ok=False)\n",
    "os.makedirs('./papers/nonRQs/multimodalHI', exist_ok=False)\n",
    "\n",
    "for paper in allPapers:\n",
    "    if paper not in paperPaths:\n",
    "        os.system('cp ' + paper + ' ./papers/nonRQs/%s'%paper.split('/')[2])\n",
    "        try:\n",
    "            os.system('cp ' + '/'.join(paper.split('/')[:4] + [paper.split('/')[5].split('.')[0] + '.pdf']) + ' ' + ' ./papers/nonRQs/%s'%paper.split('/')[2])\n",
    "        except:\n",
    "            pass\n",
    "    # else:\n",
    "        # print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('cp ' + '/'.join(paper.split('/')[:4] + [paper.split('/')[5].split('.')[0] + '.pdf']) + ' ' + ' ./papers/nonRQs/%s'%paper.split('/')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./papers/multimodalHI/MHI/85.pdf'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(paper.split('/')[:4] + [paper.split('/')[5].split('.')[0] + '.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML2JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = convert_tei_xml_file_to_s2orc_json('./papers/multimodalHI/MHI/xml/10.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '10',\n",
       " 'pdf_hash': '',\n",
       " 'metadata': {'title': 'Designing AACs for People with Aphasia Dining in Restaurants',\n",
       "  'authors': [{'first': 'Mmachi',\n",
       "    'middle': [],\n",
       "    'last': \"God'sglory Obiorah\",\n",
       "    'suffix': '',\n",
       "    'affiliation': {'laboratory': '',\n",
       "     'institution': 'Irvine Northwestern University Evanston',\n",
       "     'location': {'settlement': 'Irvine, Evanston', 'region': 'IL, CA, IL'}},\n",
       "    'email': ''},\n",
       "   {'first': 'Anne',\n",
       "    'middle': ['Marie'],\n",
       "    'last': 'Piper',\n",
       "    'suffix': '',\n",
       "    'affiliation': {'laboratory': '',\n",
       "     'institution': 'Irvine Northwestern University Evanston',\n",
       "     'location': {'settlement': 'Irvine, Evanston', 'region': 'IL, CA, IL'}},\n",
       "    'email': 'ampiper@uci.edu'},\n",
       "   {'first': 'Michael',\n",
       "    'middle': [],\n",
       "    'last': 'Horn',\n",
       "    'suffix': '',\n",
       "    'affiliation': {'laboratory': '',\n",
       "     'institution': 'Irvine Northwestern University Evanston',\n",
       "     'location': {'settlement': 'Irvine, Evanston', 'region': 'IL, CA, IL'}},\n",
       "    'email': 'michael-horn@northwestern.edu'}],\n",
       "  'year': '',\n",
       "  'venue': None,\n",
       "  'identifiers': {}},\n",
       " 'abstract': [{'text': 'There is a growing need to design augmentative and alternative communication (AAC) devices that focus on supporting quality of life goals, such as increased social participation in leisurely activities. Yet, designing AAC applications that support leisurely activities is difcult, as the activity might require novel and specifc language in a timely manner. Through observations and contextual interviews with people with aphasia, their social partners, and speech-language therapists, we characterize the important but challenging nature of supporting one specifc leisure activity: meal ordering in restaurants. Based on our observational and interview data, we design and explore three prototype AAC systems to support people with aphasia in ordering meals in restaurants. Each prototype integrates a diferent AI technology, contributing insights into how AI may enhance AAC usage and design. The study opens up questions of designing accessible restaurant experiences for neurodivergent people and the role of AI in AAC devices more broadly.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Abstract',\n",
       "   'sec_num': None},\n",
       "  {'text': '• Human-centered computing → Accessibility technologies; Interactive systems and tools.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Abstract',\n",
       "   'sec_num': None}],\n",
       " 'body_text': [{'text': 'Aphasia is a communication disorder that afects the production and comprehension of speech as well as one\\'s ability to read and write [43] . It often results from damage to the language center of the brain, which may be caused by stroke. People who have aphasia present along a spectrum and have varying abilities in terms of generating and comprehending speech and written language. Aphasia has been described as \"the most important potential consequence of stroke\" [56] as one\\'s ability to communicate has a profound impact on their quality of life [16, 54, 56] . One\\'s inability to communicate as a result of aphasia can lead to depression, social isolation and emotional distress [54, 56] . Despite the importance of designing AAC devices to support quality of life goals for people with aphasia, limited work has sought to understand their desired contexts of use and how technology may support people with aphasia in their quality of life goals.',\n",
       "   'cite_spans': [{'start': 134, 'end': 138, 'text': '[43]', 'ref_id': None},\n",
       "    {'start': 467, 'end': 471, 'text': '[56]', 'ref_id': 'BIBREF58'},\n",
       "    {'start': 551, 'end': 555, 'text': '[16,', 'ref_id': 'BIBREF15'},\n",
       "    {'start': 556, 'end': 559, 'text': '54,', 'ref_id': 'BIBREF56'},\n",
       "    {'start': 560, 'end': 563, 'text': '56]', 'ref_id': 'BIBREF58'},\n",
       "    {'start': 684, 'end': 688, 'text': '[54,', 'ref_id': 'BIBREF56'},\n",
       "    {'start': 689, 'end': 692, 'text': '56]', 'ref_id': 'BIBREF58'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'INTRODUCTION',\n",
       "   'sec_num': '1'},\n",
       "  {'text': \"Our work focuses on understanding how to design AACs for social and leisurely activities that might improve the quality of life of people with aphasia. In particular, this work is motivated by the difculties people with aphasia face while dining in unfamiliar restaurants. An aphasia.org survey of people with aphasia about dining in restaurants reported moderate to severe difculty levels by about 78% of respondents [44] . In addition, text-only printed menus have been named as a particular source of discomfort to people with aphasia especially after initially acquiring aphasia [23, 53] . Furthermore, studies about older adults and people with impairments name food choice as a means of expressing one's autonomy and independence [58] .\",\n",
       "   'cite_spans': [{'start': 418,\n",
       "     'end': 422,\n",
       "     'text': '[44]',\n",
       "     'ref_id': 'BIBREF45'},\n",
       "    {'start': 583, 'end': 587, 'text': '[23,', 'ref_id': 'BIBREF22'},\n",
       "    {'start': 588, 'end': 591, 'text': '53]', 'ref_id': 'BIBREF55'},\n",
       "    {'start': 736, 'end': 740, 'text': '[58]', 'ref_id': 'BIBREF60'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'INTRODUCTION',\n",
       "   'sec_num': '1'},\n",
       "  {'text': \"Using a high-tech AAC device (e.g., Lingraphica) is one way by which an individual's communication skills (production as well as reception) can be maximized for functional and efective communication [40] . Yet, these devices are often made up of large vocabularies that are difcult to search. They typically hold thousands of words [46, 47] that are organized into categories and subcategories. The users' core vocabulary, which might include their most accessed words is displayed on the initial interface of the device for easy access. Novel and less frequently used words are accessed by navigating through the diferent categories and subcategories on the device. To help address the issue of novel contexts, where words my not be loaded into the device or are buried deep in subcategories, prior work has considered context-aware AAC devices [36] . Context-aware techniques, however, may still miss important language and cues in the environment that would be helpful for communication (e.g., local signage and printed content).\",\n",
       "   'cite_spans': [{'start': 199,\n",
       "     'end': 203,\n",
       "     'text': '[40]',\n",
       "     'ref_id': 'BIBREF41'},\n",
       "    {'start': 332, 'end': 336, 'text': '[46,', 'ref_id': 'BIBREF47'},\n",
       "    {'start': 337, 'end': 340, 'text': '47]', 'ref_id': 'BIBREF49'},\n",
       "    {'start': 846, 'end': 850, 'text': '[36]', 'ref_id': 'BIBREF36'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'INTRODUCTION',\n",
       "   'sec_num': '1'},\n",
       "  {'text': 'This paper presents a three-year efort comprised of observations in both clinical and naturalistic settings and the design and exploration of three prototype applications for people with aphasia. We began by conducting 43 hours of observations of people with aphasia during therapy, which informed our understanding of aphasia as well as the dynamics of communication within the context of therapy. These sessions also helped us identify the challenges and limitations of current AAC devices, particularly around leisurely social activities. Based on our initial insights, we sought to understand the practices and experience of dining in restaurants for people with aphasia and their social contacts. We conducted contextual interviews with people with aphasia dining in restaurants and iteratively designed three novel applications to support independent meal ordering. The applications integrate photo captioning, OCR technology, and context-awareness to provide vocabulary in novel contexts and support meal ordering in unfamiliar restaurants. We tested PhotoSearch and MenuSpeak with speech-language therapists and people with aphasia in laboratory settings and evaluated OrderEat through a case study with one person with aphasia over four sessions of use, including in naturalistic restaurant settings.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'INTRODUCTION',\n",
       "   'sec_num': '1'},\n",
       "  {'text': \"Through the design and evaluation of these prototypes, we make three contributions to the accessibility and HCI literature. First, we extend knowledge around designing for leisurely activities for people with aphasia by focusing on the novel case of ordering meals in restaurants, a challenging but important social setting for communication. Second, we open up a discussion of accessibility in restaurants for neurodivergent people, particularly around the process of ordering and customizing a meal. Third, we contribute insights into designing AAC devices that integrate AI, such as image captioning, OCR technology, and geographic data. We refect on challenges of handling inaccuracies in real-time in order to support communication, and ways to present information generated in realtime in a comprehensible manner as well as how AI in AACs might hinder or support one's sense of autonomy.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'INTRODUCTION',\n",
       "   'sec_num': '1'},\n",
       "  {'text': 'We briefy review related work on the design and use of AACs, how AAC use and communication more broadly is shaped through social interaction, and prior advances in context-aware AAC technologies.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'RELATED WORK',\n",
       "   'sec_num': '2'},\n",
       "  {'text': 'Augmentative and alternative communication (AAC) encapsulates a broad set of strategies, techniques, and tools that support individuals with speech and/or language impairments in expressing their thoughts, desires, feelings, and ideas [40] . AACs include communication strategies such as gestures, prosody, pantomiming, facial expressions, residual speech and body language, low-tech communication tools that augment or replace speech (e.g., picture boards, photographs, pencil and paper), and high-tech communication devices [4] . Studies have also extended the defnition of AACs to include strategies and tools that enhance social participation and social networks with the broader goal to support improved quality of life [10, 48] . People who have acquired neurological conditions such as amyotrophic lateral sclerosis (ALS), traumatic brain injury (TBI), brainstem impairment, severe, chronic aphasia and apraxia of speech, primary progressive aphasia (PPA) and dementia often use AACs to augment or replace their speech.',\n",
       "   'cite_spans': [{'start': 235,\n",
       "     'end': 239,\n",
       "     'text': '[40]',\n",
       "     'ref_id': 'BIBREF41'},\n",
       "    {'start': 526, 'end': 529, 'text': '[4]', 'ref_id': None},\n",
       "    {'start': 725, 'end': 729, 'text': '[10,', 'ref_id': 'BIBREF9'},\n",
       "    {'start': 730, 'end': 733, 'text': '48]', 'ref_id': 'BIBREF50'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Augmentative and Alternative Communication',\n",
       "   'sec_num': '2.1'},\n",
       "  {'text': 'Experiencing aphasia often results in reduced social participation [56] , underscoring the importance of understanding the social contexts in which people use AACs and what kinds of support they want in those contexts. While some research has focused on designing AACs for leisurely activities [1, 25, 45, 47] , most research and commercial communication devices cater to functional needs. For instance, communication technologies for people with aphasia are able to support speech production in repetitive tasks like practicing speech during therapy. They are also efective at generating speech for everyday routine tasks, such as indicating that the person wants to brush their teeth or comb their hair. Yet, people with aphasia not only want to express their functional needs, they want to express their wants and opinions [61] . There is a need to feel like one is having an adult conversation versus simply training on words during therapy [34] . In addition, studies have described many users of high-tech AAC devices as partial and reluctant, as they may prefer to use other strategies in certain situations, such as with familiar conversation partners and for quick, easy, or routine tasks [8, 39] . Nevertheless, there are certain tasks and communication situations that require specifc vocabulary, are difcult due to the time pressure of a situation, and cause frustration when communication does not go well. For example, activities like dining in an unfamiliar restaurant, playing a board game, or discussing a sporting event are all important social activities but require specifc vocabulary that is usually unsupported by or difcult to fnd on AAC devices.',\n",
       "   'cite_spans': [{'start': 67,\n",
       "     'end': 71,\n",
       "     'text': '[56]',\n",
       "     'ref_id': 'BIBREF58'},\n",
       "    {'start': 294, 'end': 297, 'text': '[1,', 'ref_id': 'BIBREF0'},\n",
       "    {'start': 298, 'end': 301, 'text': '25,', 'ref_id': 'BIBREF24'},\n",
       "    {'start': 302, 'end': 305, 'text': '45,', 'ref_id': 'BIBREF46'},\n",
       "    {'start': 306, 'end': 309, 'text': '47]', 'ref_id': 'BIBREF49'},\n",
       "    {'start': 826, 'end': 830, 'text': '[61]', 'ref_id': 'BIBREF63'},\n",
       "    {'start': 945, 'end': 949, 'text': '[34]', 'ref_id': 'BIBREF34'},\n",
       "    {'start': 1198, 'end': 1201, 'text': '[8,', 'ref_id': 'BIBREF7'},\n",
       "    {'start': 1202, 'end': 1205, 'text': '39]', 'ref_id': 'BIBREF40'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Augmentative and Alternative Communication',\n",
       "   'sec_num': '2.1'},\n",
       "  {'text': 'Human communication is inherently social and co-constructed among multiple parties. Goodwin compellingly shows how people with aphasia act as competent speakers through the use of gesture, eye gaze, and coordination of interaction with one\\'s social partners [26] [27] [28] . Related to this theorizing of communication as social interaction, researchers have sought to understand how conversation partners (caregivers, family members and other social contacts) contribute to communication involving people who use AAC devices. Importantly, however, Apler [3] warns against the trope of \"giving voice\" or that the devices give voice to a person rather than viewing technology as playing a part in socially situated and emergent communication. This interactive framing of communication has shaped the design and study of AAC devices. For example, Shin et al. designed a system to support caregivers in sharing information about AAC use by children with complex communication needs in order to better support them [55] . Similarly, Beukelman et al. focus on the ways in which communication partners can contribute to successful communication [8] . Roark et al. build on this by bringing conversation partners into the predictive system in order to increase the pace of communication with people who use AACs [51] . Fiannaca et al. draw on the social model [38] of communication and supported communication [34] for people with aphasia through a group ware system that addresses privacy and the pace of conversation for both communication partners [21] . Finally, Valencia et al. investigate constraints that afect the conversational agency of people who use AACs [57] . Their study leveraged practices of familiar and unfamiliar conversation partners to understand conversational constraints and make recommendations for design, which include designing solutions that preserve the AAC user\\'s turn in conversation (e.g. through conversational robots or asking questions directly to the AAC user) and leveraging successful practices of close conversation partners. Building on this literature, the present study views the interactive and collaborative nature of communication as a crucial aspect of AAC design and use.',\n",
       "   'cite_spans': [{'start': 258,\n",
       "     'end': 262,\n",
       "     'text': '[26]',\n",
       "     'ref_id': 'BIBREF26'},\n",
       "    {'start': 263, 'end': 267, 'text': '[27]', 'ref_id': 'BIBREF27'},\n",
       "    {'start': 268, 'end': 272, 'text': '[28]', 'ref_id': 'BIBREF28'},\n",
       "    {'start': 555, 'end': 558, 'text': '[3]', 'ref_id': 'BIBREF2'},\n",
       "    {'start': 1011, 'end': 1015, 'text': '[55]', 'ref_id': 'BIBREF57'},\n",
       "    {'start': 1139, 'end': 1142, 'text': '[8]', 'ref_id': 'BIBREF7'},\n",
       "    {'start': 1305, 'end': 1309, 'text': '[51]', 'ref_id': 'BIBREF53'},\n",
       "    {'start': 1353, 'end': 1357, 'text': '[38]', 'ref_id': 'BIBREF39'},\n",
       "    {'start': 1403, 'end': 1407, 'text': '[34]', 'ref_id': 'BIBREF34'},\n",
       "    {'start': 1544, 'end': 1548, 'text': '[21]', 'ref_id': 'BIBREF20'},\n",
       "    {'start': 1660, 'end': 1664, 'text': '[57]', 'ref_id': 'BIBREF59'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Social and Collaborative Use of AACs',\n",
       "   'sec_num': '2.2'},\n",
       "  {'text': \"High-tech AAC devices are not always accepted and used by people with aphasia. Some reasons identifed in the literature include the inability of the device to support real-time communication in a timely manner and the form factor of the device [59] . Some studies have sought to increase the throughput of AAC devices and timely access to vocabulary through novel predictive algorithms and technologies [14, 36, 47] . One study explored predicting the most relevant next word from the word currently being accessed by the user [47] . Although Nikolova at al. reported moderate success, this method was unsuccessful in determining the frst word for initiating sentence composition. To address this limitation, some studies have explored one's context (e.g., geographic location) to anticipate relevant words prior to beginning a conversation [18, 31, 35, 36, 49] . Though understanding one's context can help to streamline the vocabulary of a device, the information provided by one's context might still be vague enough that the re-organized vocabulary is still too large to be searched by people with aphasia in a timely manner during conversation. For instance, conversational topics during a single meal can span up to 73 topics [6] . Context awareness has also been paired with machine learning and natural language processing models to predict relevant words for people with aphasia [12, 13, 15] ; however, these predictions are often approximations that require people with aphasia to make choices about the fnal word output in ways that are difcult for them to do because of their language impairment. Studies on designing AACs for people with aphasia have explored photo-capturing for archiving and preparing information for future communication [1, 2, 37] but not yet integrating AI (e.g., photo-captioning, OCR) to understand the content of photos (see also [9, 29, 32] ). Relatedly, MacLeod et al. have explored ways to communicate inaccuracies of AI-captioned photographs to people with vision impairments [42] . We know far less about how people with aphasia, who may have upper body mobility impairments along with difculty understanding text, are able to capture and discern the quality of their photos needed for AI systems to work. In this work, we draw on observations of people with aphasia interacting in clinical and naturalistic settings as well as a design exploration of three prototype systems that integrate AI for accessibility.\",\n",
       "   'cite_spans': [{'start': 244,\n",
       "     'end': 248,\n",
       "     'text': '[59]',\n",
       "     'ref_id': 'BIBREF61'},\n",
       "    {'start': 403, 'end': 407, 'text': '[14,', 'ref_id': 'BIBREF13'},\n",
       "    {'start': 408, 'end': 411, 'text': '36,', 'ref_id': 'BIBREF36'},\n",
       "    {'start': 412, 'end': 415, 'text': '47]', 'ref_id': 'BIBREF49'},\n",
       "    {'start': 527, 'end': 531, 'text': '[47]', 'ref_id': 'BIBREF49'},\n",
       "    {'start': 841, 'end': 845, 'text': '[18,', 'ref_id': 'BIBREF17'},\n",
       "    {'start': 846, 'end': 849, 'text': '31,', 'ref_id': 'BIBREF31'},\n",
       "    {'start': 850, 'end': 853, 'text': '35,', 'ref_id': 'BIBREF35'},\n",
       "    {'start': 854, 'end': 857, 'text': '36,', 'ref_id': 'BIBREF36'},\n",
       "    {'start': 858, 'end': 861, 'text': '49]', 'ref_id': 'BIBREF51'},\n",
       "    {'start': 1232, 'end': 1235, 'text': '[6]', 'ref_id': 'BIBREF5'},\n",
       "    {'start': 1388, 'end': 1392, 'text': '[12,', 'ref_id': 'BIBREF11'},\n",
       "    {'start': 1393, 'end': 1396, 'text': '13,', 'ref_id': 'BIBREF12'},\n",
       "    {'start': 1397, 'end': 1400, 'text': '15]', 'ref_id': 'BIBREF14'},\n",
       "    {'start': 1754, 'end': 1757, 'text': '[1,', 'ref_id': 'BIBREF0'},\n",
       "    {'start': 1758, 'end': 1760, 'text': '2,', 'ref_id': 'BIBREF1'},\n",
       "    {'start': 1761, 'end': 1764, 'text': '37]', 'ref_id': 'BIBREF38'},\n",
       "    {'start': 1868, 'end': 1871, 'text': '[9,', 'ref_id': 'BIBREF8'},\n",
       "    {'start': 1872, 'end': 1875, 'text': '29,', 'ref_id': 'BIBREF29'},\n",
       "    {'start': 1876, 'end': 1879, 'text': '32]', 'ref_id': 'BIBREF32'},\n",
       "    {'start': 2018, 'end': 2022, 'text': '[42]', 'ref_id': 'BIBREF43'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'AI for People with Aphasia',\n",
       "   'sec_num': '2.3'},\n",
       "  {'text': 'We began our inquiry with observations of speech-language therapy involving people with aphasia to understand the experience of aphasia, dynamics of communication, and the way therapists support training and use of AAC devices.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'CLINIC-BASED OBSERVATION: THERAPY SESSIONS FOR PEOPLE WITH APHASIA',\n",
       "   'sec_num': '3'},\n",
       "  {'text': 'We recruited seven participants from a clinic in the Midwestern region of the United States (Ruth, Larry, Veronica, Peter, Theodore, Esther, John; see Table 1 ). We completed 43 hours of observation of therapy sessions. Ruth and Esther attended these sessions with a family member, a caregiver or both. During the therapy sessions, the frst author took feld notes. Field notes also included responses to our inquiries from speech language therapists who were present during our observation sessions to clarify and deepen our understanding of ongoing therapy activities. We analyzed data from our observations using a thematic analysis approach [11] . We expanded our feld notes flling in details after each observation session. We developed memos from our feld notes for each participant. We developed codes inductively from the data while flling in details and writing initial memos. Our initial codes attended to actors during therapy, their roles and goals, observed processes, the tools and goals of therapy, understanding aphasia and the use and limitations of current AAC devices in therapy. We refned and re-organized these codes through discussions and more analytic memoing. This process led to the emergence of the themes detailed below.',\n",
       "   'cite_spans': [{'start': 644,\n",
       "     'end': 648,\n",
       "     'text': '[11]',\n",
       "     'ref_id': 'BIBREF10'}],\n",
       "   'ref_spans': [{'start': 157, 'end': 158, 'text': '1', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Method',\n",
       "   'sec_num': '3.1'},\n",
       "  {'text': '3.2.1 Frustration with Word Finding and Specificity of Expression. AAC devices used during therapy did not adequately support specifc expression. This lack of support for specifcity was re-iterated by one therapist, who talked about how his client was able to communicate that she did not like something but was unable to make an argument for why she did not like it. People with aphasia who are fuent also give us insight into the frustrations of being unable to fnd words. For example, Larry said to the therapist, \"I can see the car for the snowmobile but, I couldn\\'t get the word. It was so frustrating. \" Even when objects are physically present and people with aphasia can reference it through gesture (e.g., pointing), they are often frustrated by their inability to fnd the word for the object or generate a specifc expression.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Findings',\n",
       "   'sec_num': '3.2'},\n",
       "  {'text': 'Beyond Basic Communication Tasks. High-tech AAC devices are robust in that they can hold thousands of words which can be combined to create novel and complex sentences. Most often, sentences practiced with the device were about routine and everyday activities. For example, clients practiced sentences such as \"brush my hair\", \"brush my teeth\" and \"How are you?\" Therapy sessions brought to light other activities and settings in which people with aphasia were having difculty communicating and for which their high-tech AACs were unable to support them. For instance, family members and therapists named addressing cards as things Esther and Ruth would like to do. Veronica also spoke of creating a shopping list. Theodore spoke of playing bridge while John followed baseball games and created icons for his favorite players during one therapy session. This suggests that a variety of social activities and hobbies may be unsupported by existing strategies or devices.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Moving',\n",
       "   'sec_num': '3.2.2'},\n",
       "  {'text': \"Adapting High-Tech AACs and Other Digital Devices. While therapists recommend a number of high-tech AAC devices for people with severe aphasia, people with mild to moderate aphasia often adapt non-dedicated digital devices (e.g. mobile phones and iPads) for AAC purposes. For instance, Larry requested a dictionary on his phone to help him with his word practice. His therapist also created a recording of words and saved it on a USB stick for him. In addition, family members of people with mild to moderate aphasia are anxious to know whether there are digital tools that might support their family member to recover from aphasia. Ruth's daughter asked the therapist about Compass 1 , which is an application for 1 : List of study participants and descriptions of their presentation of aphasia. Our report of participants' communicative abilities was based on therapists' assessment and our observations during study sessions. Additionally, participants who tested the prototypes self-reported their reading, writing, speaking and comprehension abilities using a printed aphasia friendly (i.e., text and image/icon based) Likert-style questionnaire, receiving assistance from a community or clinic volunteer as needed.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 715, 'end': 716, 'text': '1', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': '3.2.3',\n",
       "   'sec_num': None},\n",
       "  {'text': 'iOS, and whether it would be useful for her mother. John often used Google Earth on his iPad to talk about himself and his past rather than icons on his AAC application. These adaptations suggest that people with mild to moderate forms of aphasia are both willing to use and interested in a range of high-tech AAC applications.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '3.2.3',\n",
       "   'sec_num': None},\n",
       "  {'text': 'As others have shown [27, 50] , communication between therapists and clients is a collaborative process that involves multiple parties. Therapists are skilled at leveraging the communicative competencies of people with aphasia by progressively creating context and semantic overloading (asking the same question in diferent ways) or connecting the therapy activities to the client\\'s real-life experiences to support production. For instance, during Ruth\\'s naming practice, in order to evoke a particular word from Ruth, the therapist would ask questions like \"what might you use this for?\" and \"where might you fnd this?\" If all of the diferent ways of evoking a response from the person with aphasia failed, the therapist might then sound out the frst few words of the desired production. This practice allowed the client to take more control of their production during therapy.',\n",
       "   'cite_spans': [{'start': 21,\n",
       "     'end': 25,\n",
       "     'text': '[27,',\n",
       "     'ref_id': 'BIBREF27'},\n",
       "    {'start': 26, 'end': 29, 'text': '50]', 'ref_id': 'BIBREF52'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting Relational Goals through Social Conversation.',\n",
       "   'sec_num': '3.2.4'},\n",
       "  {'text': 'Being able to take the lead in conversation is especially important for people with acquired aphasia because of the loss of autonomy and independence that often accompanies the loss of speech. Yet, family members often struggle with understanding when and how to support people with aphasia. For example, during a therapy session, Esther rejected help from her husband by turning to him and sternly saying \"ba ba ba ba!\" Theodore explained to his therapists that situations of having the wrong words come out and his wife not understanding is very hard. He said that his wife thinks he says some of the things he says on purpose. He also talks about how conversation between his wife and himself was a key thing in their relationship and how that has been taken away. Thus technology may play a role in not only supporting unfolding conversation but also working towards relational goals during its use.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting Relational Goals through Social Conversation.',\n",
       "   'sec_num': '3.2.4'},\n",
       "  {'text': 'Across our observational data and further supported by related work [48, 56] , we found that AACs were rarely designed for specifc social contexts. Yet, social activities, such as eating out in restaurants, are considered highly important but challenging communication contexts. The process of meal ordering in restaurants, for instance, presents novel situations to people with aphasia and requires specifc, time-sensitive responses from them. To complement our clinic-based observations and understand interaction in one particular social context, we decided to more deeply examine the experience of people with aphasia eating out in restaurants, focusing on their meal ordering practices and interaction with social partners, restaurant staf, and menu content.',\n",
       "   'cite_spans': [{'start': 68,\n",
       "     'end': 72,\n",
       "     'text': '[48,',\n",
       "     'ref_id': 'BIBREF50'},\n",
       "    {'start': 73, 'end': 76, 'text': '56]', 'ref_id': 'BIBREF58'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'CONTEXTUAL INTERVIEWS: UNDERSTANDING DINING IN RESTAURANTS',\n",
       "   'sec_num': '4'},\n",
       "  {'text': \"We interviewed and observed four people with aphasia (Ben, Esther, John, Kyle; see Table 1 ) as they ordered meals in restaurants of their choice. The lead researcher used observation of therapists as a way to hone her own skills of how to interact with PWA and modelled therapists' interaction strategies [19, 41, 60] . We tried to support diverse and fuid data collection methods for participants. We prepared open-ended questions with yes/no counterparts. We provided pens and paper for participants and encouraged them to use their own AACs (e.g. Kyle used pen and paper as well as the auto correct on his phone to spell words). Participants who had some auditory comprehension and were fuent (Kyle & Ben\",\n",
       "   'cite_spans': [{'start': 306,\n",
       "     'end': 310,\n",
       "     'text': '[19,',\n",
       "     'ref_id': 'BIBREF18'},\n",
       "    {'start': 311, 'end': 314, 'text': '41,', 'ref_id': 'BIBREF42'},\n",
       "    {'start': 315, 'end': 318, 'text': '60]', 'ref_id': 'BIBREF62'}],\n",
       "   'ref_spans': [{'start': 89, 'end': 90, 'text': '1', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Method',\n",
       "   'sec_num': '4.1'},\n",
       "  {'text': ') were asked open-ended questions and they responded with speech. Participants with limited fuency (e.g. John) were asked yes/no questions. Participants were supported by social contacts when they were present and social contacts confrmed their responses with PWA. Only one participant (Esther) who had a more severe presentation of aphasia was unable to respond to yes/no questions. We asked open-ended questions to her husband and caregiver. Three participants attended the session with a social contact and one participant attended alone.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Method',\n",
       "   'sec_num': '4.1'},\n",
       "  {'text': 'During the study, we asked participants to order a meal for themselves as they normally would and to use whatever strategies were typical and comfortable for them. A researcher observed them as they did this and took notes. Observations focused on the fow of conversation among participants (including social contacts and restaurant staf), the arrangement of the environment and materials (e.g., paper menus), and the overall experience of deciding upon and ordering a meal. After the participants carried out the task, the researcher asked them and their social contacts questions about their current and previous dining experiences. We analyzed our interview and observational data for themes [11] similar to our process for clinical observations.',\n",
       "   'cite_spans': [{'start': 695,\n",
       "     'end': 699,\n",
       "     'text': '[11]',\n",
       "     'ref_id': 'BIBREF10'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Method',\n",
       "   'sec_num': '4.1'},\n",
       "  {'text': 'Participants expressed that dining in restaurants was a means of social engagement and a leisurely way to spend their time. For example, Kyle shared that he had visited over 1000 restaurants in the city, loved food and wrote blogs about his experiences in restaurants even before he acquired aphasia. Ben planned outings with his friends from high school, which included ordering meals from restaurants. His sister shared that life becomes pretty routine when one has aphasia, and dining in restaurants is one way to break the routine. John and his mother shared that dining in restaurants was one way they spent their time together whenever she visited him. Thus, dining in restaurants is an important social activity for people with aphasia. Here, we report on how people prepare for meal ordering, achieve understanding through multiple modalities of meal information, collaborative negotiate a meal order, and reveal a need for further support in menu exploration.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Findings',\n",
       "   'sec_num': '4.2'},\n",
       "  {'text': 'Often preparatory work is done by people with aphasia, their family members, their caregivers and other social contacts prior to dining out. This preparatory work is done because of the time it might take to carry out ordering activities impromptu and the social pressures on patrons of restaurants to respond to those waiting on them within a particular time frame. For instance, after Esther\\'s husband had given the waiter their main dish orders, rather than ask the waiter to return later for their choice of appetizers, he ordered a salad for Esther without referring to the menu or her again. The process of choosing the main dish had already taken Esther and her husband a lot of time and efort. Preparatory work is also critical depending on who the person with aphasia is dining with. For instance, Ben shared that when he dines with his friends, he and his family have to look up the menu for the restaurant prior to dining. This preparatory work included determining the restaurant he and his friends would dine in, looking up the menu online and deciding what to eat. He said, [I] look on the menu before, [and I] put [it] on the phone.\" He calls his friends to fnd out where they are eating, looks up the menu on the Internet with family members, and saves his choice into his phone. When this work is not done, the social contact might end up making a choice for the person with aphasia, as was the case for Esther.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Preparatory Work before Dining.',\n",
       "   'sec_num': '4.2.1'},\n",
       "  {'text': 'Options through Images, Text, and Numbers. When dining in an unfamiliar restaurant, there are a variety of choices to be made including the preferred restaurant and the preferred meal. Furthermore, there are several other choices one might make in a restaurant that are not included on the menu. For example, there might be vegetarian options for a meal, the customer might want a diferent type of sauce, or want to know if meals could be shared. In the excerpt below, Ben describes how he fares when there are several options on a menu and the orders are labelled as numbers. Ben cannot fnd numbers without starting from the very frst digit. He says, \"usually they have like a list. . . I would have to sit there and go \\'one, two, three, four, fve, ten\\'... But if they don\\'t have that (order numbers), then that\\'s trouble, like sometimes, I go to that place (Subway)...you know they have all the list like onions, tomatoes, blah blah blah. \" Ben also described an experience where he had the order number wrong a couple of times so that when his meal was served, it was not what he expected and had to eat the meal anyway. The fact that Ben cannot make new choices and has to eat an unwanted and an unexpected meal will impact Ben\\'s satisfaction from his mealtime experience. He explained, \"I think I\\'ve done it a long time ago and I\\'ve been getting it wrong; and I\\'ll look at the thing (the meal) and that\\'s not what I wanted but I\\'ll have to eat it anyway because I\\'m not want to go \\'this is not what I want and return it . . . \\'\" Social contacts emphasized the importance of images. It was easier for people with aphasia to have conversations around images displayed on the menu. This was the case with Ben. John also chose his meal by pointing to a picture on the menu. They would then support this pointing gesture with words if they were fuent or more gestures if they were not. To emphasize the need for images by people with aphasia, Ben\\'s sister also shared that sometimes words (food ingredient) might be spoken out and the person with aphasia simply would not be able to make any meaning out of the word. Images, however, do not reveal everything about a meal. Esther\\'s husband described a situation where Esther selected a meal solely based on its image on the menu but did not like it when it was served. Words often carry information that pictures might not. For example, while one might be unable to tell that a meal is spicy or sweet from its image, the name of the meal might reveal thismaking it difcult for someone who cannot read text.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Understanding Meal',\n",
       "   'sec_num': '4.2.2'},\n",
       "  {'text': 'Ordering as a Collaborative Process. Our observations reveal the ways in which meal ordering is a collaborative process among people with aphasia, their social partners, and restaurant staf. Social contacts communicated contents of the menus to people with aphasia using the person with aphasia\\'s menu, often perusing the menu alongside them. They summarized the content of the menu with statements such as \"these are all sliders,\" or \"these are all sandwiches.\" They built conversation around images of meal items when they were present and relied on prior knowledge of what the person with aphasia preferred to speed up the process of exploring the menu and making a meal choice. For instance, Ben\\'s sister simply says, \"I know you won\\'t like this\" or \"I think you\\'d like these\" rather than reading out each meal item in the menu with the accompanying ingredients and having Ben comment on each one. Below is an excerpt of Ben and his sister exploring a menu:',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Meal',\n",
       "   'sec_num': '4.2.3'},\n",
       "  {'text': 'Ben: (pointed to the Jerk Chicken Sandwich and asked) Is that the one we talked about (they had looked at the menu while they were waiting to begin the session) Ben: has everything, no mushrooms, no onions (Ben does not like mushrooms or onions and was confrming that \"everything\" did not include mushrooms and onions) Ben: I don\\'t want a big salad. Ben: What do you think I should take? Sister: This is hot. (Pointing to a picture of a meal with some sauce) Do you want it (sauce) on the side? Sister: (Pointing to the sandwiches) Comes with a whole sandwich and you get your choice [of sandwich fllings] (There was a bunch of text on the top of the menu with choices for sandwich fllings). Sister: (Pointing to an image of a sandwich) You\\'re not gonna want this. This is a bigger sandwich. . . It\\'s messy. I don\\'t think you\\'ll like it. Ben: I did do this before. (Points to the image of a Jerk Chicken Pasta) I don\\'t like that (The mushrooms). Ben: Can I get the mushrooms . . . get them out of them? Can I switch mushrooms with broccoli? Some strategies used by restaurant staf were helpful and facilitated communication with people with aphasia. For example, images of meals allowed people with aphasia and their social contact to point and have conversations about the meal. In addition, waiters often asked questions to their patrons about how meals should be served. In asking those questions, they often presented options to their patrons such as \"would you like that with romaine lettuce or arugula?\" These follow up questions were helpful when people with aphasia were having difculties fnding words or when they could not produce speech. Often when the word is spoken out, the person with aphasia might repeat it after the waiter if they are able to do so.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Meal',\n",
       "   'sec_num': '4.2.3'},\n",
       "  {'text': 'Need for Independent Meal Exploration. For people with aphasia, being in an unfamiliar environment accentuates their dependence on others. If they are unable to comprehend text-based menus, they might rely on others to make choices for them. If they are unable to produce speech, they might have to rely on others to communicate their choices to the person serving the meal. Ben, who is fuent but unable to comprehend text, explains that he has to trust the judgment of family members when making a meal choice in an unfamiliar restaurant. He said, \". . . if we are having dinner at a . . . em . . . em, a pizza place . . . far away, I look at the thing (the menu) and I can\\'t read ... what I\\'m writing (what is written on the menu), so I said \\'well what will you have?\\' (to a family member present), and I say \\'yeah I\\'ll have that too\"\\' Even when exploring a menu together, it is difcult for the social contacts to go through each item on the menu and name each one to the person with aphasia. Often the number of items on the menu makes an in-depth exploration of the menu prohibitive. Other times, the social contact simply does not have the tools to communicate information about the meal to the person with aphasia. For example, the words on the menu may not have meaning to the person with aphasia when verbally spoken, as was the case for Esther. Sometimes communication between the social contact fails and the person with aphasia is unable to produce speech or has difculty fnding their words. In these cases, the social contact falls back on prior knowledge of the person with aphasia to help them make meal choices. For example, while dining with Esther, after her husband had run through the items on the menu, she pointed at the slider on the menu (the menu had no images, only text), but her husband and caregiver thought she would not enjoy her choice as much as a hamburger. They tried to convince her to change her order, but she indicated through her facial expression and other gestures that she would not. Though this prior knowledge is helpful as a fall back and speeds up the process of meal selection, it may adversely afect the person with aphasia\\'s ability to explore the menu and discover new meals. Social contacts may choose to skip over an option based on prior knowledge of the person with aphasia, thus not giving them the chance to make decisions themselves. As we learned from our clinical observations, family members and caregivers may lack the knowledge and ability to support people with aphasia in ways they desire.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '4.2.4',\n",
       "   'sec_num': None},\n",
       "  {'text': 'In summary, people with aphasia and their family members (and/or caregivers, dining partners, friends etc) have to do extra work prior to dining in restaurants to avoid the social pressures of doing the same work in public. As a result, they are unable to participate in traditional in-restaurant social practices like menu exploration and conversations around menu items that typically make dining in restaurants enjoyable. In addition, the diferent modalities for communication provided for people to engage with menus are important for making informed decisions about what to eat. These modalities are not standardized in restaurants. Diferent restaurants provide diferent modalities and include varying details in menus increasing the risk of a negative experience for people with aphasia.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '4.2.4',\n",
       "   'sec_num': None},\n",
       "  {'text': 'Informed by fndings from our previous phases of work, we designed three complementary prototypes to support people with aphasia while they dine in restaurants. The overarching design goal is to support communication in novel contexts-specifcally dining in restaurants-by facilitating specifc language productions, giving control to people with aphasia while using the device, and providing progressive context to support comprehension of novel vocabulary. Additionally, our design exploration aimed to understand the considerations for integrating AI applications into tools for people with aphasia. The prototypes are mobile Android operating system-based and explore the following core functionalities: (1) automated captioning of camera images taken by the user, which can be explored through text-to-speech and corresponding images from the web;',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'PROTOTYPE DESIGN AND EXPLORATION',\n",
       "   'sec_num': '5'},\n",
       "  {'text': '(2) making printed, text-based information and menus interactive through OCR; and (3) automatically pulling geographic restaurant data and associated menu information, which the user can explore and add to a meal. The software for each prototype was installed on a 0.79lb, 8.35in by 4.96in by 0.35in Samsung Galaxy tablet to be tested with participants. In the sections below, we describe each prototype, our processes of evaluation, and key fndings.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'PROTOTYPE DESIGN AND EXPLORATION',\n",
       "   'sec_num': '5'},\n",
       "  {'text': \"Following fndings from our observations, one scenario we aimed to support is to enable people with aphasia to use their mobile device camera to take photos of objects within a restaurant (e.g., mufn in a pastry case, basket of bread on a table) and make language associated with those items interactive. The goal is to support word fnding difculties when specifc language is required (e.g., diferentiating between two types of similar foods when gesturing will not sufce). To achieve this, we created a prototype that captions photos taken by users in real-time and returns related photos based on generated captions. The photographs were captioned by Cloudsight's API 2 . Each word could be elaborated with more images when tapped by the user (see Figure 1 ). All images and captions (phrases and words) could be tapped to play them aloud.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 756, 'end': 757, 'text': '1', 'ref_id': 'FIGREF0'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'PhotoSearch: Understanding Language through Physical Objects and Photo Captioning',\n",
       "   'sec_num': '5.1'},\n",
       "  {'text': 'Ten images for each word were retrieved from openclipart.org and Pixabay.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'PhotoSearch: Understanding Language through Physical Objects and Photo Captioning',\n",
       "   'sec_num': '5.1'},\n",
       "  {'text': 'We solicited feedback on our initial prototype from three people with aphasia (Ben, Adam, Hannah; see Table 1 ) and four speechlanguage therapists. Participants were asked to take photos of three objects and two scenes. Study sessions were semi-structured and open-ended to allow for exploration of the prototype, and the manipulation of the prototype was not limited to the pre-determined tasks. During the study session, a researcher demonstrated how to use the prototype and then prompted participants to do the same. We also used aphasia friendly instructional material (instructional text with images of interfaces) to communicate study details to participants. We interviewed fuent participants after each prototype test. We observed non-fuent participants for signs of distress, enjoyment, surprise and other emotions while using the prototype. Each session lasted from 20 to 60 minutes. Sessions were also video recorded.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 108, 'end': 109, 'text': '1', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'PhotoSearch: Understanding Language through Physical Objects and Photo Captioning',\n",
       "   'sec_num': '5.1'},\n",
       "  {'text': 'The use of multiple related images and words seemed to help people with aphasia to come to an understanding of a particular word. For example, Ben took a photo of a set of shelves in the testing room, which returned the phrase \"white wooden shelves\". After he clicked on the word \"wooden\", he clicked on the image associated with \"wooden\" and was able to explore other images for the word \"wooden\". He explained that viewing multiple image representations helped him better understand what the word \"wooden\" meant: \"Yeah, it\\'s good cause I really didn\\'t know what it was but now I know it\\'s eh like a wooden like this (He taps on the wooden table in front of him). See, I like that, it\\'s cool. \"',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Understanding vocabulary through multiple image representations.',\n",
       "   'sec_num': '5.1.1'},\n",
       "  {'text': 'Participants suggested that image captioning can be used to add new words to an AAC 2 https://cloudsight.ai/ device\\'s base vocabulary. One therapist said that searching for words by taking a photo of your environment could be used to fnd novel words that are not currently in the base vocabulary of an AAC device and also give the person with aphasia liberty to fnd these words independently. They said, \"I could see this [in] sort of more novel situations. . . yeah . . . things that are outside the ordinary or new things that have come up. Or allowing the person . . . you know, if they are able to operate this easily, more freedom to do more of the programming on their own versus the family member or an SLP. \" However, they also pointed out that there are other concepts that people with aphasia would like to express that may not be physically present or even have physical form.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Adding new words to an AAC device.',\n",
       "   'sec_num': '5.1.2'},\n",
       "  {'text': \"Hannah had a very limited vocabulary (e.g. they could say 'yes', 'no', 'I don't know') and difculty fnding words. They understood text and speech, had auditory comprehension and were able to repeat words when they heard them. When words were generated by the device, they were each able to read the words out loud demonstrating how photo capturing might supply novel vocabulary to people with word-fnding difculties and a limited vocabulary.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting word-finding and speech production. Adam and',\n",
       "   'sec_num': '5.1.3'},\n",
       "  {'text': 'Understanding abstract vocabulary through phrases. Although our focus was on supporting communication within a restaurant context, we learned that photo captioning phrases may help people understand abstract vocabulary. While using PhotoSearch, Ben saw that the word \"during\" was represented with an image of a clock. He did not understand its meaning when he heard the word spoken out after he tapped the word. He returned to the complete phrase describing the original captured photo and tapped on it (\"Black 5 door hatchback near building during daytime\"). This helped him to fully grasp the meaning of the word \"during\", echoing the importance of exploring phrases and words within them.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '5.1.4',\n",
       "   'sec_num': None},\n",
       "  {'text': 'to one minute to retrieve captions. Ben and Adam both commented on this time. Ben by asking if the time taken was normal and Adam by humming the popular Jeopardy jingle. Thus, indicating their expectation for an even more timely word retrieval.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Timely Word Retrieval. It took PhotoSearch up to 25 seconds',\n",
       "   'sec_num': '5.1.5'},\n",
       "  {'text': \"Adam and Hannah's had difculties capturing photos with the tablet, which led us to include a grip to the device in subsequent sessions with them. We continued to make changes to the device and software to support their ability to grip it during photo-capturing.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Timely Word Retrieval. It took PhotoSearch up to 25 seconds',\n",
       "   'sec_num': '5.1.5'},\n",
       "  {'text': \"The second prototype we designed and explored had the goal of transforming printed, text-based information within a restaurant setting into an interactive resource for communication. Although people with aphasia often point to items on a printed, physical menu to indicate their desired order, they may be unable to read the text themselves but could understand if the menu was read aloud to them. Additionally, they may want to speak their order themselves rather than pointing to items on a page, and they could do so with rehearsal before ordering (e.g., listening to a menu option and repeating it aloud). To support this, we designed an application that captures an image of a paper-based menu and converts it to interactive text (Figure 2 ). We used Google's Mobile Vision text API 3 for text recognition. The recognized text is associated with speech and descriptive images and could be spoken out by the device when the user clicks on it. Images describing the meal are retrieved from Edamam API 4 (See Figure 2 ). The user selected ingredients following the meal item, which could be added or removed from the meal. We tested this prototype with three people with aphasia (Ben, Adam, Hannah; see Table 1 ) in a laboratory setting. Participants were asked to use the system to capture images of printed, textbased menus from local restaurants. They were then asked to select their preferred meal and indicate how they wanted the meal to be served. During the study session, a researcher demonstrated how to use the prototype and then prompted participants to do the same. These sessions were also semi-structured. Participants' exploration of the prototype was not limited to this task. We interviewed fuent participants after each prototype test. We observed non-fuent participants for signs of distress, enjoyment and/or surprise. Each session lasted for 20 to 60 minutes. Sessions were also video recorded. Only Ben and Adam were able to complete this task.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 743, 'end': 744, 'text': '2', 'ref_id': 'FIGREF1'},\n",
       "    {'start': 1018, 'end': 1019, 'text': '2', 'ref_id': 'FIGREF1'},\n",
       "    {'start': 1211, 'end': 1212, 'text': '1', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'MenuSpeak: Accessing Printed, Text-based Language through OCR',\n",
       "   'sec_num': '5.2'},\n",
       "  {'text': 'Challenges of taking photos for OCR capture. The frst step of the task-which involved taking a good quality photo to allow for OCR-was challenging for all participants, and others have similar issues with photo taking due to motor impairments co-occurring with stroke [9, 32] . Taking good quality photos is important for recognizing text. Adam and Hannah had difculty grasping the device and focusing the device to take good photos. Hannah was unable to complete the task as a result. This suggested that immediate and ongoing feedback would aid in the photo taking process. Ben was unable to comprehend any text at all and was hesitant about capturing a photograph of the paper menu. He expressed that he was unsure about what part of the document was relevant and what parts were not. This highlighted the need to associate text with speech and images as soon as he began interacting with the device. We improved the design so that the application immediately captured and annotated a document within its feld of view if the image was sufcient quality.',\n",
       "   'cite_spans': [{'start': 268,\n",
       "     'end': 271,\n",
       "     'text': '[9,',\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 272, 'end': 275, 'text': '32]', 'ref_id': 'BIBREF32'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '5.2.1',\n",
       "   'sec_num': None},\n",
       "  {'text': 'Making data available in a timely manner is essential, especially in contexts such as dining out. Text detection allows for the information on the menu to be available to the person with aphasia in a timely manner rather than typing or writing it out, assuming one has the ability to do so. Though Ben had enough upper body motor control to enter data into a mobile device and let the device read it out to him, there are many reasons why this may not be a viable solution for him. We observed that because Ben could not understand text, he had a hard time fnding his place and keeping track of copying text word by word. Adam and Hannah on the other hand had acute hemiparesis and experienced difculty writing and producing speech even though they understood text. Taking a photo was a much quicker way to access information for them. Adam expressed surprise when the captured text was converted into interactive form with speech and images in real-time.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Just-in-time vocabulary and speech.',\n",
       "   'sec_num': '5.2.2'},\n",
       "  {'text': 'Making a printed menu interactive through OCR and presenting that interactive text alongside images of items appeared to facilitate new kinds of interaction and understanding for participants who were unable to read text. Ben explained, \"...so I know what that is ... (pointing to the tablet screen and list of ingredients and corresponding images on the screen) for me, I know what that is but when I look here (referring to the paper based menu on the table) I have no idea what that is. This is good for me.\" He went through the list of detected text selecting each item until he came to a text item he could not understand. He clicked on the next button that displayed what the meal looked like and said, \"I didn\\'t know what it was but now I know what it is\". Thus, multimodal representations of language (i.e., visual, text, auditory) are crucial for understanding. Overlaying the interactive text on the image of the printed menu (Figure 2 , left) may help people understand the print and digital representations together. ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 944, 'end': 945, 'text': '2', 'ref_id': 'FIGREF1'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Aiding understanding through multimodal representations.',\n",
       "   'sec_num': '5.2.3'},\n",
       "  {'text': 'Our third prototype enables the user to input the desired restaurant by text or voice or pull up on a map (Figure 3 ). The application then automatically pulls up restaurant data and associated menu information, which the user can explore, add to a meal, and revisit in a shopping cart. These were strategies that previous data collection revealed were already being used by Esther, John and Ben.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 114, 'end': 115, 'text': '3', 'ref_id': None}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'OrderEat: Leveraging Location-Based Restaurant Data and Menu Information',\n",
       "   'sec_num': '5.3'},\n",
       "  {'text': \"To provide relevant information in real-time and in novel contexts to the person with aphasia, we retrieved menus from the Internet (allmenus.com) in real-time based on the users' location and other input from the user. Business logos were retrieved from Clearbit's API to provide familiar cues for users, and they could click on the logos to retrieve the menu of the restaurant. After the user chooses a restaurant, they can explore categories of food items, which are associated images in real-time through a local database of images and icons designed for people with speech-language impairments retrieved from smartysymbols.com along with Edamam's database of food images. The textual knowledge-base includes menus from the Internet (allmenus.com) and a dictionary of food-items scrapped from Wikipedia and from a Kaggle dataset. For each text describing a meal category and a meal item, noun chunks, the root noun, lemma and stemmer (i.e. base forms of words) were generated using Spacy's NLP API. Irrelevant nouns were fltered from the data by looking up their root nouns in the food dictionary. To associate images with the food item text, the application searches the local database iterating through nouns in the meal name starting with the full text, then the last noun in the meal name using its root noun, noun chunk, lemma, noun stemmer, and infections of the noun and then doing the same for preceding nouns. The local image database is given priority. If an image is not found, the meal category text is associated with the restaurant logo or a generic image and the meal item text is associated its meal category image. By extracting relevant nouns from natural language description of menu items in online menus, we were able to present these nouns as options to users of OrderEat.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'OrderEat: Leveraging Location-Based Restaurant Data and Menu Information',\n",
       "   'sec_num': '5.3'},\n",
       "  {'text': 'We tested OrderEat with one person with aphasia (Ben) over four sessions of use. Ben had a traumatic brain injury approximately eight years prior to our study. Following his injury, his speech and comprehension were impaired to the point that he was unable to communicate with family members, but he had recovered some communication abilities through therapy. Ben could not read or write text at the time of our meeting but he was fuent and could comprehend speech. Ben was relatively independent. He was able to drive his car within his neighborhood. He had a job with a grocery store but worked reduced hours due to fatigue. Ben used an iPhone. He was able to navigate his phone using his recognition of images and text-to-speech functionality of his phone. He used applications like the message application, Safari for search, and also had a Facebook account. He lived with his parents. They helped him with correspondence through emails. We conducted two technology testing sessions where we met with Ben in a local library. These sessions focused on detecting accessibility, usability, and technical problems with the system. They also helped familiarize Ben with OrderEat before he used it in an actual restaurant setting. We then tested OrderEat with Ben in the restaurant we met in during our initial contextual observation session and, fnally, we met with Ben in a restaurant that was unfamiliar to him, where he used OrderEat once more. Ben attended the two sessions in the restaurants with his sister. We analyzed video data (86 mintues from library testing sessions) alongside 49 pages of feld notes from observation data using a thematic analysis approach [11] . We extracted and transcribed sections of our video data that aligned with an initial set of codes (drawing the attention of others, initiating conversation, making specifc request, reference to specifc items, follow up questions, extended conversation, correction, evolution of conversation and item discovery). The researchers that met with Ben also took note of occurrences of these codes while detailing and expanding our feld notes from sessions carried out in the feld, giving particular attention to how OrderEat shaped interaction. We igure 3: OrderEat application allows users to search for the desired restaurant on the map or choose it from the list (left). enus are retrieved as meal categories to explore (center). Items in the shopping cart can be further expanded to reveal ingreients in the meal (right). F M d also paid attention to how OrderEat supported or hindered various activities within this context (turn taking, placing an order, asking a question, correcting an order).',\n",
       "   'cite_spans': [{'start': 1669,\n",
       "     'end': 1673,\n",
       "     'text': '[11]',\n",
       "     'ref_id': 'BIBREF10'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'OrderEat: Leveraging Location-Based Restaurant Data and Menu Information',\n",
       "   'sec_num': '5.3'},\n",
       "  {'text': \"Demands on Ordering. Without preparing for a meal and knowing one's order ahead of time, participants explained that it is likely you will be forced to have a meal you do not enjoy or to stick with what you always order. OrderEat supported Ben in exploring the menu and choosing a meal, which was an activity that was typically done outside the restaurant setting and prior to ordering a meal in an unfamiliar restaurant. During the session in the restaurant familiar to Ben, his sister explained that he could use the application to prepare his order in the restaurant similar to what he would otherwise do at home prior to coming to the restaurant. The ability to search through meal options and plan his order with OrderEat enabled Ben to choose meals that he enjoyed. During the meetings with Ben in the familiar and unfamiliar restaurants, we observed that he had specifc plans for choosing meals. In the familiar restaurant, while exploring the menu, Ben commented that he was going to try something diferent. Together with OrderEat and support from his sister, he was able to fnd a meal he had never eaten before. He was able to explore the ingredients in the meal. During the session, he commented that he enjoyed the meal. In the unfamiliar restaurant meeting, since Ben had never dined in this restaurant before, his plan was to order a meal that was familiar to him. With little help from his sister, he was able to use the device's image and text/speech association to locate the particular meal he wanted.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting Preparatory Work and Temporal',\n",
       "   'sec_num': '5.3.1'},\n",
       "  {'text': \"One of the difcult aspects of meal ordering is the time pressure of a server coming to your table when you feel unprepared or unready to give your order. In previous data collection sessions, Ben shared that one of his strategies for retrieving information in time to order his meals was to 'write down' names of meals in his phone and practice them for some time using the text-to-speech functionality on his phone and then repeat them to the waiter as quickly as possible so that he would not forget the words. Although Ben was also able to save information on his iPhone and retrieve it as needed, retrieving and communicating a desired meal to a communication partner was a tedious process that involved going between several applications on his phone and was better done in advance of dining in the restaurant. Ben's sister suggested that the software include a 'favorites' section because people with aphasia might dine in the same restaurant but be unable to fnd the words for a meal they had enjoyed in the past. Referencing a favorite section would help the person with aphasia fnd the words in a timely manner, and we included this feature in our fnal meeting with Ben. 5.3.2 Exploration, Discovery, and Initiating Conversation. OrderEat supported Ben in exploring the menu independently and at his own pace. In our earlier observations with people with aphasia, we observed that the pace of perusing the menu was largely dictated by their social contacts, generating options and relaying choices available in the menu to the person with aphasia. And, their exploration of the menu was often quick. While using OrderEat, Ben went through the menu, category by category and item by item. He clicked items to hear their names, he clicked the hints button to see the ingredients in the meal, and he expanded meal items to see more images describing the meal (see excerpt below).\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting Preparatory Work and Temporal',\n",
       "   'sec_num': '5.3.1'},\n",
       "  {'text': 'Ben: (commenting on \"lunch time\" category in the menu) I was not going to have that (those categories have several things in them, he did not like.) Sister: [He] went out of his comfort zone but stuck to his norm. Ben: It had all these things but [I] didn\\'t like this and this and it will be plain. (Ben is saying that he would have to remove too many ingredients.) Ben: I don\\'t want a hamburger (there were hamburgers on the menu) OrderEat: (Ben clicks) Seafood Entrees OrderEat: (Ben clicks) Jackfruit Tacos OrderEat: (Ben clicks) Chicken Quesadilla Ben: Okay. (He selects the chicken Quesadilla into his cart and can see that there are onions in them.) Sister: You could remove the onions. Ben: No. When Ben clicked on items, in a way he was able to initiate a conversation about the item. His sister could hear them and could then support him with more information about the meal or make a comment like \"I know you\\'d like that\" or \"I don\\'t think you\\'d like that. \" During our frst meeting his sister drew on previous knowledge about what he liked, the limited images on the paper menu and pauses as he perused the menu to communicate menu items to him. OrderEat also allowed Ben to discover detailed information about the menu independently, and he even noticed certain items before his sister brought them up for discussion. By our fnal meeting, Ben took more control of many aspects of the diferent activities involved in ordering a meal in a restaurant. He explored the menu almost entirely on his own because his sister had to answer a phone call while he chose a meal. He was able to fnd a meal he wanted and showed it to his sister when she ended her call. Beyond menu exploration, Ben was also able to fnd a specifc nearby restaurant on OrderEat for our last study session whose names he could not generate because of his impairment. He used the visual information aforded by the map to navigate to the location of the restaurant on the map and then retrieved surrounding restaurants. He used this method to localize his search and generate words to support his communication with his sister.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting Preparatory Work and Temporal',\n",
       "   'sec_num': '5.3.1'},\n",
       "  {'text': 'Word Finding and Understanding. Information retrieval and word fnding is hard for people with aphasia [24] . Study sessions with Ben revealed that the shopping cart supported him to keep track of words or meal names that he would use to order and retrieve them quickly for conversation. During our initial contextual interview with Ben in a restaurant (Section 4), his sister reminded him to point when he could not remember the name of the meal he wanted to order. In the second in-restaurant meeting (using OrderEat), Ben shared that the name for the meal was long, so he had to click on its name on OrderEat again to hear it and then say the name to the waiter. In our third in-restaurant meeting with Ben (using OrderEat) while he was giving his order to the waiter, he referenced the shopping cart to speak out the names of his appetizer and his main dish. Ben and his sister explained:',\n",
       "   'cite_spans': [{'start': 102,\n",
       "     'end': 106,\n",
       "     'text': '[24]',\n",
       "     'ref_id': 'BIBREF23'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting',\n",
       "   'sec_num': '5.3.3'},\n",
       "  {'text': \"Sister: The device [is] great cause it talks ... it's all about the pictures. (She asks Ben if that is not right) Ben: (agrees that it is true but also says) you can actually [hear] what you've [clicked/selected]. OrderEat did not always fnd images for every meal item and sometimes the images representing the meal items were inaccurate. Ben confrmed the accuracy of images using the 'expand image' option to retrieve several images, the 'may contain' button to see ingredients in the meal, and by having the device speak out the names of item. Although he appreciated being able to easily explore the menu and hear the names of items in the menu, his sister reiterated the need for images and not just any image but images of the specifc meals that would be served. An example of the need for accurate images was made evident in the study session in the unfamiliar restaurant with Ben. While Ben was exploring pizzas, a generic image was used to represent several pizza options. Ben decided not to opt for any of them because he was uncertain of their content. OrderEat's design gave priority to clear, icon-like images designed specifcally for people with speech and language impairments. Although these images were desirable for generic communication in everyday life, Ben and his sister expressed a preference for images with details specifc to particular restaurants.\",\n",
       "   'cite_spans': [{'start': 175,\n",
       "     'end': 181,\n",
       "     'text': '[hear]',\n",
       "     'ref_id': None}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Supporting',\n",
       "   'sec_num': '5.3.3'},\n",
       "  {'text': 'In this section we revisit fndings from across our feld work and design explorations to refect on accessible dining experiences and the role of AI technology in AAC devices.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'DISCUSSION',\n",
       "   'sec_num': '6'},\n",
       "  {'text': 'While the present study focuses specifcally on the experiences of people with aphasia in restaurant settings, our work raises the larger issue of designing technologies, environments, and processes that support neurodivergent people in restaurant settings more generally. Findings from our project resonate with the ways in which restaurants are providing kiosks and other digital menus and ordering services for their patrons. In-restaurant kiosks, however, often do not provide sufcient information that allow people with speech-language impairments to fully take advantage of the options available to them. For example, menu items are often represented as text and images but not often with speech or audio. Though images convey a lot of meaning, our analysis found that images, text, and speech together provide important multimodal detail for understanding. Moving towards multimodal digital menus and kiosks would support a broader range of communication needs and experiences.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Rethinking the Design of Accessible Dining Experiences for Neurodivergent People',\n",
       "   'sec_num': '6.1'},\n",
       "  {'text': 'When thinking about accessible dining, presenting information to patrons is only one part of the experience. Enabling people to explore, manipulate, and customize an order without requiring entry of specifc text or speaking with a server is critical. That is, even when the ingredients in meals are visible to patrons through images, people with complex communication needs might not have the ability or desire to communicate the specifcs of their order. Dining may even cause distress for neurodivergent people because of the social pressures to communicate, as it did for one of our participants. Given this, participants explained that it is all too common to receive a meal they did not want or like. Future work could examine integrating information that becomes available in the moment (e.g., daily specials) as well as the ability to solicit wait staf knowledge of what a meal tastes like or which meals are popular.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Rethinking the Design of Accessible Dining Experiences for Neurodivergent People',\n",
       "   'sec_num': '6.1'},\n",
       "  {'text': \"Our work, along with that of others [17, 21] , argues for the importance of attending to how other people (e.g., family members, caregivers) and the material workspace (e.g., placement of food items, printed menus) shape communication and access. As part of this, we observed that AAC devices were often used jointly by the person with aphasia and their social partner during the process of dining but also during observations in the clinic. Acceptance of AACs by family members, such as Ben's sister fnding value in OrderEat, afects continued use of AACs [5, 39, 52] . Beyond this, we can observe that joint use of AACs, particularly in leisure settings, can foster shared social engagement. For example, Ben's sister joined him in navigating the map to fnd a desired restaurant, which prompted further discussion grounded in the map representation. They also used the interactive menu to make connections to life events and friends and family who were not present. This usefulness of the AAC to one's conversation partner and the joint use of the AAC may reduce the social awkwardness associated with AACs in public places [59] . Moreover, use of tools like OrderEat may transform transactional, need-based dialog of meal ordering into rich opportunities for social engagement.\",\n",
       "   'cite_spans': [{'start': 36,\n",
       "     'end': 40,\n",
       "     'text': '[17,',\n",
       "     'ref_id': 'BIBREF16'},\n",
       "    {'start': 41, 'end': 44, 'text': '21]', 'ref_id': 'BIBREF20'},\n",
       "    {'start': 556, 'end': 559, 'text': '[5,', 'ref_id': 'BIBREF4'},\n",
       "    {'start': 560, 'end': 563, 'text': '39,', 'ref_id': 'BIBREF40'},\n",
       "    {'start': 564, 'end': 567, 'text': '52]', 'ref_id': 'BIBREF54'},\n",
       "    {'start': 1125, 'end': 1129, 'text': '[59]', 'ref_id': 'BIBREF61'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Rethinking the Design of Accessible Dining Experiences for Neurodivergent People',\n",
       "   'sec_num': '6.1'},\n",
       "  {'text': 'Although AI and accessibility is a growing topic of interest [7, 22, 30, 33] , there has been relatively little work that has explored how AI technology can support communication for people with aphasia (e.g., [36, 47] ). Here, we synthesize insights from across our project to guide future work involving the integration of AI into AAC devices.',\n",
       "   'cite_spans': [{'start': 61, 'end': 64, 'text': '[7,', 'ref_id': 'BIBREF6'},\n",
       "    {'start': 65, 'end': 68, 'text': '22,', 'ref_id': 'BIBREF21'},\n",
       "    {'start': 69, 'end': 72, 'text': '30,', 'ref_id': 'BIBREF30'},\n",
       "    {'start': 73, 'end': 76, 'text': '33]', 'ref_id': 'BIBREF33'},\n",
       "    {'start': 210, 'end': 214, 'text': '[36,', 'ref_id': 'BIBREF36'},\n",
       "    {'start': 215, 'end': 218, 'text': '47]', 'ref_id': 'BIBREF49'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Considerations for Integrating AI into AAC Applications',\n",
       "   'sec_num': '6.2'},\n",
       "  {'text': 'One of the core challenges is in handling AI inaccuracy, particularly in accessibility applications. Perhaps designers are discouraged by early failures of artifcial intelligence to deliver on the promise to support people with aphasia in communication [12, 13, 15] . As others have noted [9, 29] , the inaccuracy of AI and range of estimates a system can generate makes it difcult to integrate them into accessible systems, especially when the technology relies on human decision that may be difcult to make due to disability. For example, depending on the particular experience of aphasia, it might be a difcult task for people with speech and language impairments to understand system outputs that have a reasonable margin of error. In our work, we found that integrating multiple modalities of information (text, images, audio) supported participants in handling inaccuracies in the image search results and automatic captioning. As others have done [42] , presenting multiple possible results from the AI system can help the user build a more complete picture of what the actual meaning might be as well as possible variation (e.g., displaying several images of salad to convey the concept of salad). Future work might explore strategies to communicate uncertainty of AI predictions to people with aphasia, when language understanding is the issue in question for both the person and the AI system.',\n",
       "   'cite_spans': [{'start': 253,\n",
       "     'end': 257,\n",
       "     'text': '[12,',\n",
       "     'ref_id': 'BIBREF11'},\n",
       "    {'start': 258, 'end': 261, 'text': '13,', 'ref_id': 'BIBREF12'},\n",
       "    {'start': 262, 'end': 265, 'text': '15]', 'ref_id': 'BIBREF14'},\n",
       "    {'start': 289, 'end': 292, 'text': '[9,', 'ref_id': 'BIBREF8'},\n",
       "    {'start': 293, 'end': 296, 'text': '29]', 'ref_id': 'BIBREF29'},\n",
       "    {'start': 954, 'end': 958, 'text': '[42]', 'ref_id': 'BIBREF43'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Considerations for Integrating AI into AAC Applications',\n",
       "   'sec_num': '6.2'},\n",
       "  {'text': 'Another consideration for AI integration into AACs is the careful selection of language and images that help people construct meaning. In this work, we combined a local database of images made for people with speech-language impairments with images from the Internet to describe words. The local database images were much like clip-art and clear cartoon-like images with white-space. They lent consistency and clarity to results, but were also abstract. Internet data on the other hand was less predictable but provided images with detail and variation. This combination of imagery was particularly important in our design context, where iconography (e.g., abstract hamburger) is presented alongside detailed, contextualized photos (e.g., specifc type hamburger on a plate) to aid with meaning making. Similar kinds of decisions are required when dealing with novel text generation. More work is needed in this area to understand the level at which to associate images with words to support comprehension by people with aphasia. In Photo-Search, we took a word by word approach, while in OrderEat we extracted relevant nouns from sentences and associated them with images. Again, word by word association with images can easily become distracting. Extraction of nouns to associate with images also allowed us to simulate closed communication by presenting the nouns in such a way that the user could manipulate them (e.g. they could be added to, removed from, or substituted in a meal) or have them as part of a phrase for communication.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Considerations for Integrating AI into AAC Applications',\n",
       "   'sec_num': '6.2'},\n",
       "  {'text': \"Our work also reveals the importance of designing for augmentation rather than automation. In spite of gains that may come from AI in the design of AACs for people with aphasia, designers have to pay attention to the potential for artifcial intelligence solutions to be invasive and take too much control over one's decision making that is essential for feelings of autonomy. Just as human caregivers and social partners were willing to step in and make decisions for people with aphasia, technology has the potential to do the same without careful consideration. That is, augmentative communication technologies should stay as such rather than overstepping their bounds and doing all tasks for the individual. An example of this is the way that Ben used OrderEat to rehearse what he wanted to say rather than having the system say it aloud to the server or automatically place the order for him. The act of ordering itself might be the more important issue to preserve in the context of possible automation. It is key to preserve important cultural practices and social rituals associated with leisure activities, which may be altered by changes in the representation introduced by accessible technologies [20] . If these practices are not preserved, previously enjoyed activities like dining in a restaurant might be avoided or abandoned (e.g Ben shared that whereas he enjoyed reading prior to his injury, he no longer enjoyed books because the only way he could interact with them was through audio). In the context of AACs, automation has the potential to work against goals of social participation rather than towards them.\",\n",
       "   'cite_spans': [{'start': 1207,\n",
       "     'end': 1211,\n",
       "     'text': '[20]',\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Considerations for Integrating AI into AAC Applications',\n",
       "   'sec_num': '6.2'},\n",
       "  {'text': \"Aphasia presents uniquely in diferent individuals making it challenging to design digital solutions to support communication tools that work well for all people. Thus, while this work extends our knowledge about people with aphasia, the challenges they face in dining settings, and how AI solutions might be used to better support their communication, the prototypes designed and tested in this study are best suited for people with mild to moderate forms of aphasia and especially those who have difculties comprehending and producing text. For example, Ben's ability to comprehend speech was crucial for detecting errors in AI predictions and would not have been possible for someone without the same ability.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'LIMITATIONS',\n",
       "   'sec_num': '7'},\n",
       "  {'text': 'In this work, we sought to understand how to design AAC devices to support communication in leisurely settings, focusing on dining and ordering meals in restaurant settings. In addition to extensive feld observations, we designed and explored several prototypes for these settings that integrate various AI technology. Our work contributes new knowledge about how public dining spaces might be made more accessible for neurodivergent people. Beyond this, we demonstrate the potential for AI technologies to be integrated into AAC applications while raising considerations around the design and implementation of such techniques.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'CONCLUSION',\n",
       "   'sec_num': '8'},\n",
       "  {'text': 'https://us.tobiidynavox.com/pages/compass-product-support',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '',\n",
       "   'sec_num': None},\n",
       "  {'text': 'https://developers.google.com/vision 4 https://developer.edamam.com/',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': '',\n",
       "   'sec_num': None}],\n",
       " 'back_matter': [{'text': 'This work was supported in part by the National Science Foundation (grant IIS-1551574). Any opinions, fndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily refect the views of the NSF. We also want to thank Aaron Wilkins, Edna Babbit and participants in this study for sharing their immense knowledge about aphasia and experiences with us.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'ACKNOWLEDGMENTS',\n",
       "   'sec_num': None}],\n",
       " 'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "   'title': 'XTag: Designing an Experience Capturing and Sharing Tool for Persons with Aphasia',\n",
       "   'authors': [{'first': 'Abdullah',\n",
       "     'middle': [],\n",
       "     'last': 'Al Mahmud',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Rikkert', 'middle': [], 'last': 'Gerits', 'suffix': ''},\n",
       "    {'first': 'Jean-Bernard', 'middle': [], 'last': 'Martens', 'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '325--334',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1868914.1868953']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF1': {'ref_id': 'b1',\n",
       "   'title': 'The design and feld evaluation of PhotoTalk: a digital image communication application for people',\n",
       "   'authors': [{'first': 'Meghan',\n",
       "     'middle': [],\n",
       "     'last': 'Allen',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Joanna', 'middle': [], 'last': 'Mcgrenere', 'suffix': ''},\n",
       "    {'first': 'Barbara', 'middle': [], 'last': 'Purves', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'Proceedings of the 9th international ACM SIGACCESS conference on Computers and accessibility',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '187--194',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1296843.1296876']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF2': {'ref_id': 'b2',\n",
       "   'title': 'Giving voice: Mobile communication, disability, and inequality',\n",
       "   'authors': [{'first': 'Meryl',\n",
       "     'middle': [],\n",
       "     'last': 'Alper',\n",
       "     'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF4': {'ref_id': 'b4',\n",
       "   'title': \"Family members' perceptions of augmentative and alternative communication device use\",\n",
       "   'authors': [{'first': 'Rita',\n",
       "     'middle': ['L'],\n",
       "     'last': 'Bailey',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Howard', 'middle': ['P'], 'last': 'Parette', 'suffix': ''},\n",
       "    {'first': 'Julia', 'middle': ['B'], 'last': 'Stoner', 'suffix': ''},\n",
       "    {'first': 'Maureen', 'middle': ['E'], 'last': 'Angell', 'suffix': ''},\n",
       "    {'first': 'Kathleen', 'middle': [], 'last': 'Carroll', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': 'Language, Speech, and Hearing Services in Schools',\n",
       "   'volume': '37',\n",
       "   'issue': '1',\n",
       "   'pages': '50--60',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1044/0161-1461(2006/006)']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF5': {'ref_id': 'b5',\n",
       "   'title': 'Topics of meal-break conversations',\n",
       "   'authors': [{'first': 'Susan',\n",
       "     'middle': [],\n",
       "     'last': 'Balandin',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Teresa', 'middle': [], 'last': 'Iacono', 'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'Augmentative and Alternative Communication',\n",
       "   'volume': '14',\n",
       "   'issue': '',\n",
       "   'pages': '131--146',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/07434619812331278316']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF6': {'ref_id': 'b6',\n",
       "   'title': 'What is the Point of Fairness? Disability, AI and the Complexity of Justice',\n",
       "   'authors': [{'first': 'Cynthia',\n",
       "     'middle': ['L'],\n",
       "     'last': 'Bennett',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Os', 'middle': [], 'last': 'Keyes', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'SIGACCESS Access. Comput',\n",
       "   'volume': '125',\n",
       "   'issue': '5',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3386296.3386301']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF7': {'ref_id': 'b7',\n",
       "   'title': 'AAC for adults with acquired neurological conditions: A review',\n",
       "   'authors': [{'first': 'David',\n",
       "     'middle': ['R'],\n",
       "     'last': 'Beukelman',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Susan', 'middle': [], 'last': 'Fager', 'suffix': ''},\n",
       "    {'first': 'Laura', 'middle': [], 'last': 'Ball', 'suffix': ''},\n",
       "    {'first': 'Aimee', 'middle': [], 'last': 'Dietz', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'Augmentative and Alternative Communication',\n",
       "   'volume': '23',\n",
       "   'issue': '',\n",
       "   'pages': '230--242',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/07434610701553668']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF8': {'ref_id': 'b8',\n",
       "   'title': 'VizWiz: Nearly Real-time Answers to Visual Questions',\n",
       "   'authors': [{'first': 'P', 'middle': [], 'last': 'Jefrey', 'suffix': ''},\n",
       "    {'first': 'Chandrika', 'middle': [], 'last': 'Bigham', 'suffix': ''},\n",
       "    {'first': 'Hanjie', 'middle': [], 'last': 'Jayant', 'suffix': ''},\n",
       "    {'first': 'Greg', 'middle': [], 'last': 'Ji', 'suffix': ''},\n",
       "    {'first': 'Andrew', 'middle': [], 'last': 'Little', 'suffix': ''},\n",
       "    {'first': 'Robert', 'middle': ['C'], 'last': 'Miller', 'suffix': ''},\n",
       "    {'first': 'Aubrey', 'middle': [], 'last': 'Miller', 'suffix': ''},\n",
       "    {'first': 'Brandyn', 'middle': [], 'last': 'Tatarowicz', 'suffix': ''},\n",
       "    {'first': 'Samuel', 'middle': [], 'last': 'White', 'suffix': ''},\n",
       "    {'first': 'Tom', 'middle': [], 'last': 'White', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Yeh', 'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A)',\n",
       "   'volume': '24',\n",
       "   'issue': '',\n",
       "   'pages': '1--24',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1805986.1806020']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF9': {'ref_id': 'b9',\n",
       "   'title': 'Key principles underlying research and practice in AAC',\n",
       "   'authors': [{'first': 'Sarah',\n",
       "     'middle': ['W'],\n",
       "     'last': 'Blackstone',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Michael', 'middle': ['B'], 'last': 'Williams', 'suffix': ''},\n",
       "    {'first': 'David', 'middle': ['P'], 'last': 'Wilkins', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'Augmentative and Alternative Communication',\n",
       "   'volume': '23',\n",
       "   'issue': '',\n",
       "   'pages': '191--203',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/07434610701553684']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF10': {'ref_id': 'b10',\n",
       "   'title': 'Using thematic analysis in psychology',\n",
       "   'authors': [{'first': 'Virginia',\n",
       "     'middle': [],\n",
       "     'last': 'Braun',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Victoria', 'middle': [], 'last': 'Clarke', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': 'Qualitative Research in Psychology',\n",
       "   'volume': '3',\n",
       "   'issue': '2',\n",
       "   'pages': '77--101',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1191/1478088706qp063oa']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF11': {'ref_id': 'b11',\n",
       "   'title': 'Practical simplifcation of English newspaper text to assist aphasic readers',\n",
       "   'authors': [{'first': 'John',\n",
       "     'middle': [],\n",
       "     'last': 'Carroll',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Guido', 'middle': [], 'last': 'Minnen', 'suffix': ''},\n",
       "    {'first': 'Yvonne', 'middle': [], 'last': 'Canning', 'suffix': ''},\n",
       "    {'first': 'Siobhan', 'middle': [], 'last': 'Devlin', 'suffix': ''},\n",
       "    {'first': 'John', 'middle': [], 'last': 'Tait', 'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'Proceedings of the AAAI-98 Workshop on Integrating Artifcial Intelligence and Assistive Technology',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '7--10',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF12': {'ref_id': 'b12',\n",
       "   'title': 'Simplifying Text for Language-Impaired Readers',\n",
       "   'authors': [{'first': 'John',\n",
       "     'middle': [],\n",
       "     'last': 'Carroll',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Guido', 'middle': [], 'last': 'Minnen', 'suffix': ''},\n",
       "    {'first': 'Darren', 'middle': [], 'last': 'Pearce', 'suffix': ''},\n",
       "    {'first': 'Yvonne', 'middle': [], 'last': 'Canning', 'suffix': ''},\n",
       "    {'first': 'Siobhan', 'middle': [], 'last': 'Devlin', 'suffix': ''},\n",
       "    {'first': 'John', 'middle': [], 'last': 'Tait', 'suffix': ''}],\n",
       "   'year': 1999,\n",
       "   'venue': 'Ninth Conference of the European Chapter',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '269--270',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF13': {'ref_id': 'b13',\n",
       "   'title': 'Context and Common Ground',\n",
       "   'authors': [{'first': 'H', 'middle': [], 'last': 'Herbert', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Clark', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': 'Encyclopedia of Language & Linguistics',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '105--108',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1016/B0-08-044854-2/01088-9']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF14': {'ref_id': 'b14',\n",
       "   'title': 'A connectionist approach to word sense disambiguation',\n",
       "   'authors': [{'first': 'W', 'middle': [], 'last': 'Garrison', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Cottrell', 'suffix': ''}],\n",
       "   'year': 1989,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF15': {'ref_id': 'b15',\n",
       "   'title': 'Finding a focus for quality of life with aphasia: Social and emotional health, and psychological well-being',\n",
       "   'authors': [{'first': 'Madeline',\n",
       "     'middle': [],\n",
       "     'last': 'Cruice',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Linda', 'middle': [], 'last': 'Worrall', 'suffix': ''},\n",
       "    {'first': 'Louise', 'middle': [], 'last': 'Hickson', 'suffix': ''},\n",
       "    {'first': 'Robert', 'middle': [], 'last': 'Murison', 'suffix': ''}],\n",
       "   'year': 2003,\n",
       "   'venue': 'Aphasiology',\n",
       "   'volume': '17',\n",
       "   'issue': '',\n",
       "   'pages': '333--353',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/02687030244000707']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF16': {'ref_id': 'b16',\n",
       "   'title': 'It Doesn\\'t Win You Friends\": Understanding Accessibility in Collaborative Writing for People with Vision Impairments',\n",
       "   'authors': [{'first': 'Maitraye',\n",
       "     'middle': [],\n",
       "     'last': 'Das',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Darren', 'middle': [], 'last': 'Gergle', 'suffix': ''},\n",
       "    {'first': 'Anne', 'middle': ['Marie'], 'last': 'Piper', 'suffix': ''}],\n",
       "   'year': 2019,\n",
       "   'venue': 'Proc. ACM Hum.-Comput',\n",
       "   'volume': '26',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3359293']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF17': {'ref_id': 'b17',\n",
       "   'title': 'Towards Providing Just-in-time Vocabulary Support for Assistive and Augmentative Communication',\n",
       "   'authors': [{'first': 'Carrie',\n",
       "     'middle': [],\n",
       "     'last': 'Demmans Epp',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Justin', 'middle': [], 'last': 'Djordjevic', 'suffix': ''},\n",
       "    {'first': 'Shimu', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': 'Karyn', 'middle': [], 'last': 'Mofatt', 'suffix': ''},\n",
       "    {'first': 'Ronald', 'middle': ['M'], 'last': 'Baecker', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '33--36',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2166966.2166973']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF18': {'ref_id': 'b18',\n",
       "   'title': 'Approach Matters: Linking Practitioner Approaches to Technology Design for People with Dementia',\n",
       "   'authors': [{'first': 'Emma', 'middle': [], 'last': 'Dixon', 'suffix': ''},\n",
       "    {'first': 'Amanda', 'middle': [], 'last': 'Lazar', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '1--15',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3313831.3376432']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF19': {'ref_id': 'b19',\n",
       "   'title': 'Restricted Access: Media, Disability, and the Politics of Participation',\n",
       "   'authors': [{'first': 'Elizabeth',\n",
       "     'middle': [],\n",
       "     'last': 'Ellcessor',\n",
       "     'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF20': {'ref_id': 'b20',\n",
       "   'title': 'AACrobat: Using Mobile Devices to Lower Communication Barriers and Provide Autonomy with Gaze-Based AAC',\n",
       "   'authors': [{'first': 'Alexander',\n",
       "     'middle': [],\n",
       "     'last': 'Fiannaca',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Ann', 'middle': [], 'last': 'Paradiso', 'suffix': ''},\n",
       "    {'first': 'Mira', 'middle': [], 'last': 'Shah', 'suffix': ''},\n",
       "    {'first': 'Meredith Ringel',\n",
       "     'middle': [],\n",
       "     'last': 'Morris',\n",
       "     'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '683--695',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2998181.2998215']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF21': {'ref_id': 'b21',\n",
       "   'title': 'Fairness Issues in AI Systems That Augment Sensory Abilities',\n",
       "   'authors': [{'first': 'Leah',\n",
       "     'middle': [],\n",
       "     'last': 'Findlater',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Steven', 'middle': [], 'last': 'Goodman', 'suffix': ''},\n",
       "    {'first': 'Yuhang', 'middle': [], 'last': 'Zhao', 'suffix': ''},\n",
       "    {'first': 'Shiri', 'middle': [], 'last': 'Azenkot', 'suffix': ''},\n",
       "    {'first': 'Margot', 'middle': [], 'last': 'Hanley', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'SIGACCESS Access. Comput',\n",
       "   'volume': '125',\n",
       "   'issue': '8',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3386296.3386304']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF22': {'ref_id': 'b22',\n",
       "   'title': 'The use and impact of a supported aphasia-friendly photo menu tool on iPads in the inpatient hospital setting: a pilot study',\n",
       "   'authors': [{'first': 'Katherine',\n",
       "     'middle': [],\n",
       "     'last': 'Francis',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Katina', 'middle': [], 'last': 'Swan', 'suffix': ''},\n",
       "    {'first': 'Tanya', 'middle': [], 'last': 'Rose', 'suffix': ''},\n",
       "    {'first': 'Marie', 'middle': [], 'last': 'Hopper', 'suffix': ''},\n",
       "    {'first': 'Zane', 'middle': [], 'last': 'Hopper', 'suffix': ''},\n",
       "    {'first': 'Ian', 'middle': [], 'last': 'Hughes', 'suffix': ''},\n",
       "    {'first': 'Melissa', 'middle': [], 'last': 'Lawrie', 'suffix': ''},\n",
       "    {'first': 'Rachel', 'middle': [], 'last': 'Wenke', 'suffix': ''}],\n",
       "   'year': 2019,\n",
       "   'venue': 'Aphasiology',\n",
       "   'volume': '0',\n",
       "   'issue': '',\n",
       "   'pages': '1--21',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/02687038.2019.1686747']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF23': {'ref_id': 'b23',\n",
       "   'title': 'An Exploratory Study into the Accessibility of a Multi-User Virtual World for Young People with Aphasia',\n",
       "   'authors': [{'first': 'Julia',\n",
       "     'middle': [],\n",
       "     'last': 'Galliers',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Stephanie', 'middle': [], 'last': 'Wilson', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': 'Proceedings of the 27th International BCS Human Computer Interaction Conference',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF24': {'ref_id': 'b24',\n",
       "   'title': 'Words Are Not Enough: Empowering People with Aphasia in the Design Process',\n",
       "   'authors': [{'first': 'Julia',\n",
       "     'middle': [],\n",
       "     'last': 'Galliers',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Stephanie', 'middle': [], 'last': 'Wilson', 'suffix': ''},\n",
       "    {'first': 'Abi', 'middle': [], 'last': 'Roper', 'suffix': ''},\n",
       "    {'first': 'Naomi', 'middle': [], 'last': 'Cocks', 'suffix': ''},\n",
       "    {'first': 'Jane', 'middle': [], 'last': 'Marshall', 'suffix': ''},\n",
       "    {'first': 'Sam', 'middle': [], 'last': 'Muscroft', 'suffix': ''},\n",
       "    {'first': 'Tim', 'middle': [], 'last': 'Pring', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Proceedings of the 12th Participatory Design Conference',\n",
       "   'volume': '1',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF26': {'ref_id': 'b26',\n",
       "   'title': 'Conversational Frameworks for the Accomplishment of Meaning in Aphasia',\n",
       "   'authors': [{'first': 'Charles',\n",
       "     'middle': [],\n",
       "     'last': 'Goodwin',\n",
       "     'suffix': ''}],\n",
       "   'year': 2003,\n",
       "   'venue': 'Conversation and Brain Damage, Charles Goodwin',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '90--116',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF27': {'ref_id': 'b27',\n",
       "   'title': \"A Competent Speaker Who Can't Speak: The Social Life of Aphasia\",\n",
       "   'authors': [{'first': 'Charles',\n",
       "     'middle': [],\n",
       "     'last': 'Goodwin',\n",
       "     'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': 'Journal of Linguistic Anthropology',\n",
       "   'volume': '14',\n",
       "   'issue': '',\n",
       "   'pages': '151--170',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF28': {'ref_id': 'b28',\n",
       "   'title': 'Human Sociality as Mutual Orientation in a Rich Interactive Environment: Multimodal Utterances and Pointing in Aphasia',\n",
       "   'authors': [{'first': 'Charles',\n",
       "     'middle': [],\n",
       "     'last': 'Goodwin',\n",
       "     'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '96--125',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF29': {'ref_id': 'b29',\n",
       "   'title': 'VizLens: A Robust and Interactive Screen Reader for Interfaces in the Real World',\n",
       "   'authors': [{'first': 'Anhong', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': \"'\", 'middle': [], 'last': 'Xiang', 'suffix': ''},\n",
       "    {'first': 'Haoran', 'middle': [], 'last': \"Anthony' Chen\", 'suffix': ''},\n",
       "    {'first': 'Samuel', 'middle': [], 'last': 'Qi', 'suffix': ''},\n",
       "    {'first': 'Suman', 'middle': [], 'last': 'White', 'suffix': ''},\n",
       "    {'first': 'Chieko', 'middle': [], 'last': 'Ghosh', 'suffix': ''},\n",
       "    {'first': 'Jefrey', 'middle': ['P'], 'last': 'Asakawa', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Bigham', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'Proceedings of the 29th Annual Symposium on User Interface Software and Technology',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '651--664',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2984511.2984518']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF30': {'ref_id': 'b30',\n",
       "   'title': 'Toward Fairness in AI for People with Disabilities SBG@a Research Roadmap',\n",
       "   'authors': [{'first': 'Anhong', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Ece', 'middle': [], 'last': 'Kamar', 'suffix': ''},\n",
       "    {'first': 'Jennifer',\n",
       "     'middle': ['Wortman'],\n",
       "     'last': 'Vaughan',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hanna', 'middle': [], 'last': 'Wallach', 'suffix': ''},\n",
       "    {'first': 'Meredith Ringel',\n",
       "     'middle': [],\n",
       "     'last': 'Morris',\n",
       "     'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'SIGACCESS Access. Comput.',\n",
       "   'volume': '125',\n",
       "   'issue': '2',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3386296.3386298']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF31': {'ref_id': 'b31',\n",
       "   'title': 'Using Context History and Location in Context-aware AAC Systems for Speech-language Impairments',\n",
       "   'authors': [{'first': 'Masato',\n",
       "     'middle': [],\n",
       "     'last': 'Md Sazzad Hossain',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hiromi', 'middle': [], 'last': 'Takanokura', 'suffix': ''},\n",
       "    {'first': 'Hideki', 'middle': [], 'last': 'Sakai', 'suffix': ''},\n",
       "    {'first': ';', 'middle': [], 'last': 'Katagiri', 'suffix': ''},\n",
       "    {'first': 'Hong', 'middle': [], 'last': 'Iaeng', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Kong', 'suffix': ''}],\n",
       "   'year': 2018,\n",
       "   'venue': 'Proceedings of the International MultiConference of Engineers and Computer Scientists',\n",
       "   'volume': '1',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF32': {'ref_id': 'b32',\n",
       "   'title': 'Supporting Blind Photography',\n",
       "   'authors': [{'first': 'Chandrika',\n",
       "     'middle': [],\n",
       "     'last': 'Jayant',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hanjie', 'middle': [], 'last': 'Ji', 'suffix': ''},\n",
       "    {'first': 'Samuel', 'middle': [], 'last': 'White', 'suffix': ''},\n",
       "    {'first': 'Jefrey', 'middle': ['P'], 'last': 'Bigham', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'The Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '203--210',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2049536.2049573']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF33': {'ref_id': 'b33',\n",
       "   'title': 'Artifcial Intelligence Fairness in the Context of Accessibility Research on Intelligent Systems for People Who Are Deaf or Hard of Hearing',\n",
       "   'authors': [{'first': 'Sushant',\n",
       "     'middle': [],\n",
       "     'last': 'Kafe',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Abraham', 'middle': [], 'last': 'Glasser', 'suffix': ''},\n",
       "    {'first': 'Sedeeq', 'middle': [], 'last': 'Al-Khazraji', 'suffix': ''},\n",
       "    {'first': 'Larwan', 'middle': [], 'last': 'Berke', 'suffix': ''},\n",
       "    {'first': 'Matthew', 'middle': [], 'last': 'Seita', 'suffix': ''},\n",
       "    {'first': 'Matt', 'middle': [], 'last': 'Huenerfauth', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'SIGACCESS Access. Comput.',\n",
       "   'volume': '125',\n",
       "   'issue': '4',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3386296.3386300']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF34': {'ref_id': 'b34',\n",
       "   'title': 'Supported Conversations for Adults with Aphasia: Methods and Resources for Training Conversation Partners',\n",
       "   'authors': [{'first': 'Aura', 'middle': [], 'last': 'Kagan', 'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'Journal of Aphasiology',\n",
       "   'volume': '12',\n",
       "   'issue': '',\n",
       "   'pages': '816--830',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/02687039808249575']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF35': {'ref_id': 'b35',\n",
       "   'title': 'What we talk about: Designing a context-aware communication tool for people with aphasia',\n",
       "   'authors': [{'first': 'Shaun',\n",
       "     'middle': ['K'],\n",
       "     'last': 'Kane',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Barbara', 'middle': [], 'last': 'Linam-Church', 'suffix': ''},\n",
       "    {'first': 'Kyle', 'middle': [], 'last': 'Althof', 'suffix': ''},\n",
       "    {'first': 'Denise', 'middle': [], 'last': 'Mccall', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '49--56',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2384916.2384926']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF36': {'ref_id': 'b36',\n",
       "   'title': 'Context-aware Communication Support System with Pictographic Cards',\n",
       "   'authors': [{'first': 'Gunhee', 'middle': [], 'last': 'Kim', 'suffix': ''},\n",
       "    {'first': 'Jukyung', 'middle': [], 'last': 'Park', 'suffix': ''},\n",
       "    {'first': 'Manchul', 'middle': [], 'last': 'Han', 'suffix': ''},\n",
       "    {'first': 'Sehyung', 'middle': [], 'last': 'Park', 'suffix': ''},\n",
       "    {'first': 'Sungdo', 'middle': [], 'last': 'Ha', 'suffix': ''}],\n",
       "   'year': 2009,\n",
       "   'venue': 'Proceedings of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF38': {'ref_id': 'b38',\n",
       "   'title': 'When Words Fall Short: Helping People with Aphasia to Express',\n",
       "   'authors': [{'first': 'Tom',\n",
       "     'middle': [],\n",
       "     'last': 'Koppenol',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Abdullah', 'middle': ['Al'], 'last': 'Mahmud', 'suffix': ''},\n",
       "    {'first': 'Jean-Bernard', 'middle': [], 'last': 'Martens', 'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'Computers Helping People with Special Needs',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '45--48',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF39': {'ref_id': 'b39',\n",
       "   'title': 'Accessible technology and models of disability',\n",
       "   'authors': [{'first': 'E', 'middle': [], 'last': 'Richard', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Ladner', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '25--31',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF40': {'ref_id': 'b40',\n",
       "   'title': 'Promoting acceptance of augmentative and alternative communication by adults with acquired communication disorders',\n",
       "   'authors': [{'first': 'Joanne',\n",
       "     'middle': [],\n",
       "     'last': 'Lasker',\n",
       "     'suffix': ''}],\n",
       "   'year': 2001,\n",
       "   'venue': 'Augmentative and Alternative Communication',\n",
       "   'volume': '17',\n",
       "   'issue': '',\n",
       "   'pages': '141--153',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/aac.17.3.141.153']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF41': {'ref_id': 'b41',\n",
       "   'title': 'Aphasia and AAC: Enhancing Communication Across Health Care Settings',\n",
       "   'authors': [{'first': 'Joanne',\n",
       "     'middle': ['P'],\n",
       "     'last': 'Lasker',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Kathryn', 'middle': ['L'], 'last': 'Garrett', 'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'The ASHA Leader',\n",
       "   'volume': '13',\n",
       "   'issue': '',\n",
       "   'pages': '10--13',\n",
       "   'other_ids': defaultdict(list,\n",
       "               {'DOI': ['10.1044/leader.FTR1.13082008.10']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF42': {'ref_id': 'b42',\n",
       "   'title': 'Supporting People with Dementia in Digital Social Sharing',\n",
       "   'authors': [{'first': 'Amanda',\n",
       "     'middle': [],\n",
       "     'last': 'Lazar',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Caroline', 'middle': [], 'last': 'Edasis', 'suffix': ''},\n",
       "    {'first': 'Anne', 'middle': ['Marie'], 'last': 'Piper', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '2149--2162',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3025453.3025586']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF43': {'ref_id': 'b43',\n",
       "   'title': \"Understanding Blind People's Experiences with Computer-Generated Captions of Social Media Images\",\n",
       "   'authors': [{'first': 'Haley',\n",
       "     'middle': [],\n",
       "     'last': 'Macleod',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Cynthia', 'middle': ['L'], 'last': 'Bennett', 'suffix': ''},\n",
       "    {'first': 'Meredith',\n",
       "     'middle': ['Ringel'],\n",
       "     'last': 'Morris',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Edward', 'middle': [], 'last': 'Cutrell', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '5988--5999',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3025453.3025814']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF45': {'ref_id': 'b45',\n",
       "   'title': 'Tips for Eating at Restaurants When You Have Aphasia',\n",
       "   'authors': [],\n",
       "   'year': 2020,\n",
       "   'venue': 'National Aphasia Association',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF46': {'ref_id': 'b46',\n",
       "   'title': 'Empowering Expression for Users with Aphasia through Constrained Creativity',\n",
       "   'authors': [{'first': 'Timothy',\n",
       "     'middle': [],\n",
       "     'last': 'Neate',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Abi', 'middle': [], 'last': 'Roper', 'suffix': ''},\n",
       "    {'first': 'Stephanie', 'middle': [], 'last': 'Wilson', 'suffix': ''},\n",
       "    {'first': 'Jane', 'middle': [], 'last': 'Marshall', 'suffix': ''}],\n",
       "   'year': 2019,\n",
       "   'venue': 'Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '1--12',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3290605.3300615']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF47': {'ref_id': 'b47',\n",
       "   'title': 'The design of ViVA: A mixed-initiative visual vocabulary for aphasia',\n",
       "   'authors': [{'first': 'Sonya',\n",
       "     'middle': [],\n",
       "     'last': 'Nikolova',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jordan', 'middle': [], 'last': 'Boyd-Graber', 'suffix': ''},\n",
       "    {'first': 'Perry', 'middle': ['R'], 'last': 'Cook', 'suffix': ''}],\n",
       "   'year': 2009,\n",
       "   'venue': 'Extended Abstracts on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '4015--4020',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1520340.1520610']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF49': {'ref_id': 'b49',\n",
       "   'title': 'Click on Bake to Get Cookies: Guiding Word-fnding with Semantic Associations',\n",
       "   'authors': [{'first': 'Sonya',\n",
       "     'middle': [],\n",
       "     'last': 'Nikolova',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Marilyn', 'middle': [], 'last': 'Tremaine', 'suffix': ''},\n",
       "    {'first': 'Perry', 'middle': ['R'], 'last': 'Cook', 'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '155--162',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1878803.1878832']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF50': {'ref_id': 'b50',\n",
       "   'title': 'Moving toward employment using AAC: case study',\n",
       "   'authors': [{'first': 'A', 'middle': [], 'last': 'Odom', 'suffix': ''},\n",
       "    {'first': 'Michael', 'middle': [], 'last': 'Upthegrove', 'suffix': ''}],\n",
       "   'year': 1997,\n",
       "   'venue': 'Augmentative and Alternative Communication',\n",
       "   'volume': '13',\n",
       "   'issue': '',\n",
       "   'pages': '258--262',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/07434619712331278078']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF51': {'ref_id': 'b51',\n",
       "   'title': 'Enhancing Access to Situational Vocabulary by Leveraging Geographic Context',\n",
       "   'authors': [{'first': 'Rupal', 'middle': [], 'last': 'Patel', 'suffix': ''},\n",
       "    {'first': 'Rajiv', 'middle': [], 'last': 'Radhakrishnan', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'Situational+Vocabulary+by+Leveraging+Geographic+Context&id=EJ899370 Publisher: Assistive Technology Industry Association and SEAT Center',\n",
       "   'volume': '4',\n",
       "   'issue': '',\n",
       "   'pages': '99--114',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF52': {'ref_id': 'b52',\n",
       "   'title': 'Introducing multimodal paper-digital interfaces for speech-language therapy',\n",
       "   'authors': [{'first': 'Anne',\n",
       "     'middle': ['Marie'],\n",
       "     'last': 'Piper',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Nadir', 'middle': [], 'last': 'Weibel', 'suffix': ''},\n",
       "    {'first': 'James', 'middle': ['D'], 'last': 'Hollan', 'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '203--210',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/1878803.1878840']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF53': {'ref_id': 'b53',\n",
       "   'title': 'Towards technology-assisted co-construction with communication partners',\n",
       "   'authors': [{'first': 'Brian', 'middle': [], 'last': 'Roark', 'suffix': ''},\n",
       "    {'first': 'Andrew', 'middle': [], 'last': 'Fowler', 'suffix': ''},\n",
       "    {'first': 'Richard', 'middle': [], 'last': 'Sproat', 'suffix': ''},\n",
       "    {'first': 'Christopher', 'middle': [], 'last': 'Gibbons', 'suffix': ''},\n",
       "    {'first': 'Melanie', 'middle': [], 'last': 'Fried-Oken', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '22--31',\n",
       "   'other_ids': defaultdict(list, {}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF54': {'ref_id': 'b54',\n",
       "   'title': \"Augmentative and Alternative Communication use: family and professionals' perceptions of facilitators and barriers\",\n",
       "   'authors': [{'first': 'Nátali',\n",
       "     'middle': [],\n",
       "     'last': 'Romano',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Regina Yu Shon', 'middle': [], 'last': 'Chun', 'suffix': ''}],\n",
       "   'year': 2018,\n",
       "   'venue': 'CoDAS',\n",
       "   'volume': '30',\n",
       "   'issue': '',\n",
       "   'pages': '',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1590/2317-1782/20162017138']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF55': {'ref_id': 'b55',\n",
       "   'title': '2011-08. Aphasia friendly written health information: content and design characteristics',\n",
       "   'authors': [{'first': 'Tanya',\n",
       "     'middle': ['A'],\n",
       "     'last': 'Rose',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Linda', 'middle': ['E'], 'last': 'Worrall', 'suffix': ''},\n",
       "    {'first': 'Louise', 'middle': ['M'], 'last': 'Hickson', 'suffix': ''},\n",
       "    {'first': 'Tammy', 'middle': ['C'], 'last': 'Hofmann', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'International Journal of Speech-Language Pathology',\n",
       "   'volume': '13',\n",
       "   'issue': '',\n",
       "   'pages': '335--347',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.3109/17549507.2011.560396']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF56': {'ref_id': 'b56',\n",
       "   'title': 'Quality of life with and without aphasia',\n",
       "   'authors': [{'first': 'Katherine',\n",
       "     'middle': [],\n",
       "     'last': 'Ross',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Robert', 'middle': [], 'last': 'Wertz', 'suffix': ''}],\n",
       "   'year': 2003,\n",
       "   'venue': 'Aphasiology',\n",
       "   'volume': '17',\n",
       "   'issue': '',\n",
       "   'pages': '355--364',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/02687030244000716']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF57': {'ref_id': 'b57',\n",
       "   'title': 'TalkingBoogie: Collaborative Mobile AAC System for Non-Verbal Children with Developmental Disabilities and Their Caregivers',\n",
       "   'authors': [{'first': 'Donghoon',\n",
       "     'middle': [],\n",
       "     'last': 'Shin',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jaeyoon', 'middle': [], 'last': 'Song', 'suffix': ''},\n",
       "    {'first': 'Seokwoo', 'middle': [], 'last': 'Song', 'suffix': ''},\n",
       "    {'first': 'Jisoo', 'middle': [], 'last': 'Park', 'suffix': ''},\n",
       "    {'first': 'Joonhwan', 'middle': [], 'last': 'Lee', 'suffix': ''},\n",
       "    {'first': 'Soojin', 'middle': [], 'last': '', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '1--13',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3313831.3376154']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF58': {'ref_id': 'b58',\n",
       "   'title': 'Quality of life measurement and outcome in aphasia',\n",
       "   'authors': [{'first': 'Simona',\n",
       "     'middle': [],\n",
       "     'last': 'Spaccavento',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Angela', 'middle': [], 'last': 'Craca', 'suffix': ''},\n",
       "    {'first': 'Marina', 'middle': ['Del'], 'last': 'Prete', 'suffix': ''},\n",
       "    {'first': 'Rosanna', 'middle': [], 'last': 'Falcone', 'suffix': ''},\n",
       "    {'first': 'Antonia', 'middle': [], 'last': 'Colucci', 'suffix': ''},\n",
       "    {'first': 'Angela', 'middle': ['Di'], 'last': 'Palma', 'suffix': ''},\n",
       "    {'first': 'Anna', 'middle': [], 'last': 'Loverre', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': 'Neuropsychiatric Disease and Treatment',\n",
       "   'volume': '10',\n",
       "   'issue': '',\n",
       "   'pages': '27--37',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.2147/NDT.S52357']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF59': {'ref_id': 'b59',\n",
       "   'title': 'Conversational Agency in Augmentative and Alternative Communication',\n",
       "   'authors': [{'first': 'Stephanie',\n",
       "     'middle': [],\n",
       "     'last': 'Valencia',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Amy', 'middle': [], 'last': 'Pavel', 'suffix': ''},\n",
       "    {'first': 'Jared', 'middle': ['Santa'], 'last': 'Maria', 'suffix': ''},\n",
       "    {'first': '(', 'middle': [], 'last': 'Seunga', 'suffix': ''},\n",
       "    {'first': ')', 'middle': [], 'last': 'Gloria', 'suffix': ''},\n",
       "    {'first': 'Jefrey', 'middle': ['P'], 'last': 'Yu', 'suffix': ''},\n",
       "    {'first': 'Henny', 'middle': [], 'last': 'Bigham', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Admoni', 'suffix': ''}],\n",
       "   'year': 2020,\n",
       "   'venue': 'Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '1--12',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3313831.3376376']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF60': {'ref_id': 'b60',\n",
       "   'title': 'Access to food choices by older people in residential aged care: An integrative review',\n",
       "   'authors': [{'first': 'Donna', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Bronwyn', 'middle': [], 'last': 'Everett', 'suffix': ''},\n",
       "    {'first': 'Tifany', 'middle': [], 'last': 'Northall', 'suffix': ''},\n",
       "    {'first': 'Amy', 'middle': ['R'], 'last': 'Villarosa', 'suffix': ''},\n",
       "    {'first': 'Yenna', 'middle': [], 'last': 'Salamonson', 'suffix': ''}],\n",
       "   'year': 2018,\n",
       "   'venue': 'Collegian',\n",
       "   'volume': '25',\n",
       "   'issue': '',\n",
       "   'pages': '457--465',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1016/j.colegn.2017.11.004']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF61': {'ref_id': 'b61',\n",
       "   'title': 'Designing Conversation Cues on a Head-Worn Display to Support Persons with Aphasia',\n",
       "   'authors': [{'first': 'Kristin',\n",
       "     'middle': [],\n",
       "     'last': 'Williams',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Karyn', 'middle': [], 'last': 'Mofatt', 'suffix': ''},\n",
       "    {'first': 'Denise', 'middle': [], 'last': 'Mccall', 'suffix': ''},\n",
       "    {'first': 'Leah', 'middle': [], 'last': 'Findlater', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '231--240',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/2702123.2702484']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF62': {'ref_id': 'b62',\n",
       "   'title': \"Co-Design Beyond Words: 'Moments of Interaction' with Minimally-Verbal Children on the Autism Spectrum\",\n",
       "   'authors': [{'first': 'Cara', 'middle': [], 'last': 'Wilson', 'suffix': ''},\n",
       "    {'first': 'Margot', 'middle': [], 'last': 'Brereton', 'suffix': ''},\n",
       "    {'first': 'Bernd', 'middle': [], 'last': 'Ploderer', 'suffix': ''},\n",
       "    {'first': 'Laurianne', 'middle': [], 'last': 'Sitbon', 'suffix': ''}],\n",
       "   'year': 2019,\n",
       "   'venue': 'Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems',\n",
       "   'volume': '',\n",
       "   'issue': '',\n",
       "   'pages': '1--15',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1145/3290605.3300251']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None},\n",
       "  'BIBREF63': {'ref_id': 'b63',\n",
       "   'title': 'What people with aphasia want: Their goals according to the ICF',\n",
       "   'authors': [{'first': 'Linda',\n",
       "     'middle': [],\n",
       "     'last': 'Worrall',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sue', 'middle': [], 'last': 'Sherratt', 'suffix': ''},\n",
       "    {'first': 'Penny', 'middle': [], 'last': 'Rogers', 'suffix': ''},\n",
       "    {'first': 'Tami', 'middle': [], 'last': 'Howe', 'suffix': ''},\n",
       "    {'first': 'Deborah', 'middle': [], 'last': 'Hersh', 'suffix': ''},\n",
       "    {'first': 'Alison', 'middle': [], 'last': 'Ferguson', 'suffix': ''},\n",
       "    {'first': 'Bronwyn', 'middle': [], 'last': 'Davidson', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'Aphasiology',\n",
       "   'volume': '25',\n",
       "   'issue': '',\n",
       "   'pages': '309--322',\n",
       "   'other_ids': defaultdict(list, {'DOI': ['10.1080/02687038.2010.508530']}),\n",
       "   'num': None,\n",
       "   'urls': [],\n",
       "   'raw_text': '',\n",
       "   'links': None}},\n",
       " 'ref_entries': {'FIGREF0': {'num': None,\n",
       "   'uris': None,\n",
       "   'text': 'Figure 1: The PhotoSearch prototype enables user to take a photo of an object, and the photo is captioned in real-time with a phrase. Text captions can be played aloud (left). Words and images can be expanded to show related images to aid understanding (right).',\n",
       "   'type_str': 'figure'},\n",
       "  'FIGREF1': {'num': None,\n",
       "   'uris': None,\n",
       "   'text': 'Figure 2: MenuSpeak allows user the to take a photo of a paper menu (left) and then tap on blocks of text to play the item aloud and display photos of the meal or item (right).',\n",
       "   'type_str': 'figure'},\n",
       "  'FIGREF2': {'num': None,\n",
       "   'uris': None,\n",
       "   'text': 'Figure 4: Ben using OrderEat in a laboratory setting',\n",
       "   'type_str': 'figure'},\n",
       "  'TABREF0': {'num': None,\n",
       "   'text': 'Limited fuency (while his sentences are well formed, his speech is hard to comprehend), able to comprehend RES speech, able to write (with some spelling errors) and read text.',\n",
       "   'html': None,\n",
       "   'content': '<table><tr><td colspan=\"2\">Participant Gender</td><td>Description of Aphasia</td><td>Observation (obs),</td></tr><tr><td/><td>(Age)</td><td/><td>Restaurant(res),</td></tr><tr><td/><td/><td/><td>Lab, Field</td></tr><tr><td>Ruth</td><td>F (80\\'s)</td><td colspan=\"2\">Paraphasia (producing words that are irrelevant to the context but with a correct syntax), fuent, able to OBS</td></tr><tr><td/><td/><td>comprehend speech, limited ability to comprehend written text.</td></tr><tr><td>Larry</td><td>M (66)</td><td colspan=\"2\">Primary progressive aphasia (Loss of language over time), fuent (with word fnding difculties and difculty OBS</td></tr><tr><td/><td/><td>pronouncing certain words), able to comprehend speech, difculty reading and writing.</td></tr><tr><td colspan=\"2\">Veronica F (66)</td><td colspan=\"2\">Anomic aphasia (Difculty fnding words), fuent, able to comprehend speech, difculty reading and writing, OBS</td></tr><tr><td/><td/><td>paraphasia regarding numbers.</td></tr><tr><td>Peter</td><td>M (66)</td><td colspan=\"2\">Non-fuent (apraxia), limited auditory comprehension, able to copy written text, right-side hemiparesis (weak-OBS</td></tr><tr><td/><td/><td>ness on the right-side of the body).</td></tr><tr><td colspan=\"2\">Theodore M (83)</td><td colspan=\"2\">Anomic aphasia (Difculty fnding words), fuent, able to comprehend speech, paraphasia regarding numbers. OBS</td></tr><tr><td>Esther</td><td>F (67)</td><td colspan=\"2\">Non-fuent with apraxia, very limited auditory comprehension, able to copy text, very limited comprehension OBS, RES</td></tr><tr><td/><td/><td>of written text, right-side hemiparesis.</td></tr><tr><td>John</td><td>M (40\\'s)</td><td colspan=\"2\">Non-fuent with apraxia, limited auditory comprehension, able to copy written text, limited comprehension of OBS, RES</td></tr><tr><td/><td/><td>text, right-side hemiparesis.</td></tr><tr><td>Kyle</td><td>M (52)</td><td/></tr></table>',\n",
       "   'type_str': 'table'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relatedWork_text(path):\n",
    "    \"\"\"convert tei xml to json, and then get related work text from json file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        related work text.\n",
    "    \"\"\"\n",
    "    paper = convert_tei_xml_file_to_s2orc_json(path).as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'related work' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    relatedWork = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(relatedWork)\n",
    "\n",
    "def get_intro_text(path):\n",
    "    \"\"\"convert tei xml to json, and then get introduction text from json file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        related work text.\n",
    "    \"\"\"\n",
    "    paper = convert_tei_xml_file_to_s2orc_json(path).as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'intro' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    text = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_RQ_text(path):\n",
    "    \"\"\"convert tei xml to json, and then get RQs text from json file.\n",
    "    \n",
    "    Args:\n",
    "        path: Paper path.\n",
    "    \n",
    "    Returns:\n",
    "        related work text.\n",
    "    \"\"\"\n",
    "    paper = convert_tei_xml_file_to_s2orc_json(path).as_json()['body_text']\n",
    "    texts = []\n",
    "    # for dSec in paper['body_text']:\n",
    "    text = str(paper).lower()\n",
    "    # non greedy match\n",
    "    regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        texts.extend(matches)\n",
    "    return '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ben: what do you think i should take?\n",
      " ben: can i get the mushrooms . . . get them out of them?\n"
     ]
    }
   ],
   "source": [
    "print(get_RQ_text('./papers/multimodalHI/MHI/xml/10.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPapers = glob.glob('./papers/**/*.xml', recursive=True)\n",
    "# allPapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [01:21<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for path in tqdm(allPapers):\n",
    "    paths.append(path)\n",
    "    relatedWorks.append(get_relatedWork_text(path))\n",
    "    intros.append(get_intro_text(path))\n",
    "    rqs.append(get_RQ_text(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 260 134\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum([1 for t in relatedWorks if t]),\n",
    "    sum([1 for t in intros if t]),\n",
    "    sum([1 for t in rqs if t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to jsonl file\n",
    "# with open('allQuestions_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "with open('RQs_TEI2JSON.jsonl', 'w', encoding='utf8') as f:\n",
    "    for idx, rq in enumerate(rqs):\n",
    "        # if rq:\n",
    "        if True:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    'intro': intros[idx],\n",
    "                    'relatedWork': relatedWorks[idx],\n",
    "                    'rq': rq\n",
    "                }\n",
    "            ))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('NFT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c377a027f0bbb6c0821b68e5c065774f74ef070985e76133b9b1c3a883a152de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
