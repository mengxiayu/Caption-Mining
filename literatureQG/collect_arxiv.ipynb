{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect pdf links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"https://export.arxiv.org/api/query?search_query=cat:cs.HC&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with libreq.urlopen(query) as url:\n",
    "    r = url.read()\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed title: ArXiv Query: search_query=cat:cs.HC&amp;id_list=&amp;start=0&amp;max_results=2000\n",
      "Feed last updated: 2022-09-22T00:00:00-04:00\n",
      "totalResults for this query: 10958\n",
      "itemsPerPage for this query: 2000\n",
      "startIndex for this query: 0\n"
     ]
    }
   ],
   "source": [
    "# Parse the XML feed\n",
    "# feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "# feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "feed = feedparser.parse(r)\n",
    "\n",
    "print('Feed title: %s' % feed.feed.title)\n",
    "print('Feed last updated: %s' % feed.feed.updated)\n",
    "print('totalResults for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "print('itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage)\n",
    "print('startIndex for this query: %s'   % feed.feed.opensearch_startindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through each entry, and print out information\n",
    "pdfLinks = []\n",
    "for entry in feed.entries:\n",
    "    # print('e-print metadata')\n",
    "    # print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "    # print('Published: %s' % entry.published)\n",
    "    # print('Title:  %s' % entry.title)\n",
    "    \n",
    "    # # feedparser v4.1 only grabs the first author\n",
    "    # author_string = entry.author\n",
    "    \n",
    "    # # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # # - this will only grab the first affiliation encountered\n",
    "    # #   (the first affiliation for the first author)\n",
    "    # # Please email the list with a way to get all of this information!\n",
    "    # try:\n",
    "    #     author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    # except AttributeError:\n",
    "    #     pass\n",
    "    \n",
    "    # print('Last Author:  %s' % author_string)\n",
    "    \n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    # try:\n",
    "    #     print('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
    "    # except AttributeError:\n",
    "    #     pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            # print('abs page link: %s' % link.href)\n",
    "            pass\n",
    "        # print(link)\n",
    "        elif link.title == 'pdf':\n",
    "            # print('pdf link: %s' % link.href)\n",
    "            pdfLinks.append(link.href)\n",
    "    \n",
    "    # # The journal reference, comments and primary_category sections live under \n",
    "    # # the arxiv namespace\n",
    "    # try:\n",
    "    #     journal_ref = entry.arxiv_journal_ref\n",
    "    # except AttributeError:\n",
    "    #     journal_ref = 'No journal ref found'\n",
    "    # print('Journal reference: %s' % journal_ref)\n",
    "    \n",
    "    # try:\n",
    "    #     comment = entry.arxiv_comment\n",
    "    # except AttributeError:\n",
    "    #     comment = 'No comment found'\n",
    "    # print('Comments: %s' % comment)\n",
    "    \n",
    "    # # Since the <arxiv:primary_category> element has no data, only\n",
    "    # # attributes, feedparser does not store anything inside\n",
    "    # # entry.arxiv_primary_category\n",
    "    # # This is a dirty hack to get the primary_category, just take the\n",
    "    # # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # # this, please email the list!\n",
    "    # print('Primary Category: %s' % entry.tags[0]['term'])\n",
    "    \n",
    "    # # Lets get all the categories\n",
    "    # all_categories = [t['term'] for t in entry.tags]\n",
    "    # print('All Categories: %s' % (', ').join(all_categories))\n",
    "    \n",
    "    # # The abstract is in the <summary> element\n",
    "    # print('Abstract: %s' %  entry.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdfLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\programming\\NSF_lecture\\Obsidian-Concept-Map\\literatureQG\\collect_arxiv.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/programming/NSF_lecture/Obsidian-Concept-Map/literatureQG/collect_arxiv.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m l\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse pdfs into jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipdf\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [3:44:06<00:00,  6.72s/it]    \n"
     ]
    }
   ],
   "source": [
    "dPapers = []\n",
    "for l in tqdm(pdfLinks[:2000]):\n",
    "    try:\n",
    "        dPapers.append(\n",
    "            # scipdf.parse_pdf(l + '.pdf')\n",
    "            scipdf.parse_pdf_to_dict(l + '.pdf')\n",
    "        )\n",
    "    except:\n",
    "        dPapers.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dPapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump dPapers to jsonl\n",
    "with open('arxiv_HCI_2000.jsonl', 'w') as f:\n",
    "    for d in dPapers:\n",
    "        if d: f.write(json.dumps(d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse the jsons to rqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json, \\\n",
    "    convert_tei_xml_soup_to_s2orc_json\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SciPDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dPapersParsed = []\n",
    "# for d in tqdm(dPapers):\n",
    "#     try:\n",
    "#         article_dict = scipdf.convert_article_soup_to_dict(d, as_list=False)\n",
    "#         dPapersParsed.append(article_dict)\n",
    "#     except:\n",
    "#         dPapersParsed.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relatedWork_text(d):\n",
    "    paper = d['sections']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    text = []\n",
    "    for d in paper:\n",
    "        if 'related work' in d['heading'].lower():\n",
    "            text = [d['heading'] + \": \" + d['text']]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_intro_text(d):\n",
    "    paper = d['sections']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    text = []\n",
    "    for d in paper:\n",
    "        if 'intro' in d['heading'].lower():\n",
    "            text = [d['heading'] + \": \" + d['text']]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_RQ_text(d):\n",
    "    paper = d['sections']\n",
    "    texts = []\n",
    "    # for dSec in paper['body_text']:\n",
    "    text = str(paper).lower()\n",
    "    # non greedy match\n",
    "    regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?(.]*\\?')\n",
    "    # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        texts.extend(matches)\n",
    "    return '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:42<00:00, 47.46it/s]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for text in tqdm(dPapers):\n",
    "    try:\n",
    "        relatedWorks.append(get_relatedWork_text(text))\n",
    "    except:\n",
    "        relatedWorks.append(\"\")\n",
    "    try:\n",
    "        intros.append(get_intro_text(text))\n",
    "    except:\n",
    "        intros.append(\"\")\n",
    "    try:\n",
    "        rqs.append(get_RQ_text(text))\n",
    "    except:\n",
    "        rqs.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803 1432 371\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum([1 for t in relatedWorks if t]),\n",
    "    sum([1 for t in intros if t]),\n",
    "    sum([1 for t in rqs if t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"II. RELATED WORKS: Several concepts of haptic human-drone interaction (HDI) were suggested, such as the direct encounter of the user's hand with a drone explored by Abdullah et al. [9] for 1D gravitation force simulation. More complex systems apply additional haptic proxies attached to the drone for kinesthetic feedback, such as lightweight haptic extentions for passive and active feedback scenarios developed by Hoppe et al. [10]. Many applications for haptic drones were suggested by Yamaguchi et al. [11] in Virtual Reality (VR) scenario of interaction with a virtual sword and Abtahi et al. [12] in VR scenario of a virtual wardrobe. All the mentioned above scenarios allowed users to experience tactile interactions over a large area freely. However, they are focused on displays with limited contact forces supported by the rotors of drones. Additionally, the static form of such displays is not able to provide separate feedback to each of the user's fingers. Therefore, Tsykunov et al. [13] proposed the concept of high-fidelity HDI-based display was proposed. However, the suggested kinesthetic feedback by wired drones remained sensitive to pulling forces by the human hand.\\nIn this paper, we propose a novel concept of a haptic display in which drones deliver vibromotors on flexible cords to the user's fingertips. Unlike previous solutions, DandelionTouch eliminates the weight and bulk of wearable devices and achieves the robustness of HDI by providing high-fidelity vibrotactile feedback, thus lowering the force applied to the drones. The drone swarm follows the position of the user's hand with low latency, thereby delivering a wide range of vibration patterns upon contact with virtual surfaces.\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatedWorks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip relatedWorks, intros, rqs into a jsonl file\n",
    "with open('arxiv_HCI_2000_RQs.jsonl', 'w') as f: \n",
    "    for rw, intro, rq in zip(relatedWorks, intros, rqs):\n",
    "        f.write(json.dumps({\n",
    "            'relatedWork': rw,\n",
    "            'intro': intro,\n",
    "            'rq': rq\n",
    "        }) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relatedWork_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'related work' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    relatedWork = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(relatedWork)\n",
    "\n",
    "def get_intro_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'intro' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    text = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_RQ_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    texts = []\n",
    "    # for dSec in paper['body_text']:\n",
    "    text = str(paper).lower()\n",
    "    # non greedy match\n",
    "    regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        texts.extend(matches)\n",
    "    return '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.61it/s]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for text in tqdm(dPapers):\n",
    "    try:\n",
    "        relatedWorks.append(get_relatedWork_text(text))\n",
    "    except:\n",
    "        relatedWorks.append(\"\")\n",
    "    try:\n",
    "        intros.append(get_intro_text(text))\n",
    "    except:\n",
    "        intros.append(\"\")\n",
    "    try:\n",
    "        rqs.append(get_RQ_text(text))\n",
    "    except:\n",
    "        rqs.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 43 14\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum([1 for t in relatedWorks if t]),\n",
    "    sum([1 for t in intros if t]),\n",
    "    sum([1 for t in rqs if t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to jsonl file\n",
    "# with open('allQuestions_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "with open('RQs_TEI2JSON.jsonl', 'w', encoding='utf8') as f:\n",
    "    for idx, rq in enumerate(rqs):\n",
    "        # if rq:\n",
    "        if True:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    'intro': intros[idx],\n",
    "                    'relatedWork': relatedWorks[idx],\n",
    "                    'rq': rq\n",
    "                }\n",
    "            ))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('NFT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c377a027f0bbb6c0821b68e5c065774f74ef070985e76133b9b1c3a883a152de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
