{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect pdf links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"https://export.arxiv.org/api/query?search_query=cat:cs.HC&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with libreq.urlopen(query) as url:\n",
    "    r = url.read()\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed title: ArXiv Query: search_query=cat:cs.HC&amp;id_list=&amp;start=0&amp;max_results=2000\n",
      "Feed last updated: 2022-09-21T00:00:00-04:00\n",
      "totalResults for this query: 10958\n",
      "itemsPerPage for this query: 2000\n",
      "startIndex for this query: 0\n"
     ]
    }
   ],
   "source": [
    "# Parse the XML feed\n",
    "# feedparser._FeedParserMixin.namespaces['http://a9.com/-/spec/opensearch/1.1/'] = 'opensearch'\n",
    "# feedparser._FeedParserMixin.namespaces['http://arxiv.org/schemas/atom'] = 'arxiv'\n",
    "\n",
    "feed = feedparser.parse(r)\n",
    "\n",
    "print('Feed title: %s' % feed.feed.title)\n",
    "print('Feed last updated: %s' % feed.feed.updated)\n",
    "print('totalResults for this query: %s' % feed.feed.opensearch_totalresults)\n",
    "print('itemsPerPage for this query: %s' % feed.feed.opensearch_itemsperpage)\n",
    "print('startIndex for this query: %s'   % feed.feed.opensearch_startindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through each entry, and print out information\n",
    "pdfLinks = []\n",
    "for entry in feed.entries:\n",
    "    # print('e-print metadata')\n",
    "    # print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "    # print('Published: %s' % entry.published)\n",
    "    # print('Title:  %s' % entry.title)\n",
    "    \n",
    "    # # feedparser v4.1 only grabs the first author\n",
    "    # author_string = entry.author\n",
    "    \n",
    "    # # grab the affiliation in <arxiv:affiliation> if present\n",
    "    # # - this will only grab the first affiliation encountered\n",
    "    # #   (the first affiliation for the first author)\n",
    "    # # Please email the list with a way to get all of this information!\n",
    "    # try:\n",
    "    #     author_string += ' (%s)' % entry.arxiv_affiliation\n",
    "    # except AttributeError:\n",
    "    #     pass\n",
    "    \n",
    "    # print('Last Author:  %s' % author_string)\n",
    "    \n",
    "    # feedparser v5.0.1 correctly handles multiple authors, print them all\n",
    "    # try:\n",
    "    #     print('Authors:  %s' % ', '.join(author.name for author in entry.authors))\n",
    "    # except AttributeError:\n",
    "    #     pass\n",
    "\n",
    "    # get the links to the abs page and pdf for this e-print\n",
    "    for link in entry.links:\n",
    "        if link.rel == 'alternate':\n",
    "            # print('abs page link: %s' % link.href)\n",
    "            pass\n",
    "        # print(link)\n",
    "        elif link.title == 'pdf':\n",
    "            # print('pdf link: %s' % link.href)\n",
    "            pdfLinks.append(link.href)\n",
    "    \n",
    "    # # The journal reference, comments and primary_category sections live under \n",
    "    # # the arxiv namespace\n",
    "    # try:\n",
    "    #     journal_ref = entry.arxiv_journal_ref\n",
    "    # except AttributeError:\n",
    "    #     journal_ref = 'No journal ref found'\n",
    "    # print('Journal reference: %s' % journal_ref)\n",
    "    \n",
    "    # try:\n",
    "    #     comment = entry.arxiv_comment\n",
    "    # except AttributeError:\n",
    "    #     comment = 'No comment found'\n",
    "    # print('Comments: %s' % comment)\n",
    "    \n",
    "    # # Since the <arxiv:primary_category> element has no data, only\n",
    "    # # attributes, feedparser does not store anything inside\n",
    "    # # entry.arxiv_primary_category\n",
    "    # # This is a dirty hack to get the primary_category, just take the\n",
    "    # # first element in entry.tags.  If anyone knows a better way to do\n",
    "    # # this, please email the list!\n",
    "    # print('Primary Category: %s' % entry.tags[0]['term'])\n",
    "    \n",
    "    # # Lets get all the categories\n",
    "    # all_categories = [t['term'] for t in entry.tags]\n",
    "    # print('All Categories: %s' % (', ').join(all_categories))\n",
    "    \n",
    "    # # The abstract is in the <summary> element\n",
    "    # print('Abstract: %s' %  entry.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdfLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://arxiv.org/pdf/0712.2168v1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse pdfs into jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipdf\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:58<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "dPapers = []\n",
    "for l in tqdm(pdfLinks[:100]):\n",
    "    try:\n",
    "        dPapers.append(\n",
    "            # scipdf.parse_pdf(l + '.pdf')\n",
    "            scipdf.parse_pdf_to_dict(l + '.pdf')\n",
    "        )\n",
    "    except:\n",
    "        dPapers.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dPapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse the jsons to rqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doc2json.grobid2json.tei_to_json import convert_tei_xml_file_to_s2orc_json, \\\n",
    "    convert_tei_xml_soup_to_s2orc_json\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SciPDF Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dPapersParsed = []\n",
    "# for d in tqdm(dPapers):\n",
    "#     try:\n",
    "#         article_dict = scipdf.convert_article_soup_to_dict(d, as_list=False)\n",
    "#         dPapersParsed.append(article_dict)\n",
    "#     except:\n",
    "#         dPapersParsed.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relatedWork_text(d):\n",
    "    paper = d['sections']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    text = []\n",
    "    for d in paper:\n",
    "        if 'related work' in d['heading'].lower():\n",
    "            text = [d['heading'] + \": \" + d['text']]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_intro_text(d):\n",
    "    paper = d['sections']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    text = []\n",
    "    for d in paper:\n",
    "        if 'intro' in d['heading'].lower():\n",
    "            text = [d['heading'] + \": \" + d['text']]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_RQ_text(d):\n",
    "    paper = d['sections']\n",
    "    texts = []\n",
    "    # for dSec in paper['body_text']:\n",
    "    text = str(paper).lower()\n",
    "    # non greedy match\n",
    "    regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?(.]*\\?')\n",
    "    # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        texts.extend(matches)\n",
    "    return '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 54.33it/s]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for text in tqdm(dPapers):\n",
    "    try:\n",
    "        relatedWorks.append(get_relatedWork_text(text))\n",
    "    except:\n",
    "        relatedWorks.append(\"\")\n",
    "    try:\n",
    "        intros.append(get_intro_text(text))\n",
    "    except:\n",
    "        intros.append(\"\")\n",
    "    try:\n",
    "        rqs.append(get_RQ_text(text))\n",
    "    except:\n",
    "        rqs.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 67 13\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum([1 for t in relatedWorks if t]),\n",
    "    sum([1 for t in intros if t]),\n",
    "    sum([1 for t in rqs if t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'RELATED WORK: The area of multi-platform UI development falls under the umbrella of what is being termed as the \"variety challenges\" [33]. There are new challenges for application and solution developers due to the emergence of a variety of users, a variety of devices and channels, and a variety of roles and functions. We would categorize the problem of multi-platform UI development arising due to the emergence of a variety of devices and channels. This research area is relatively new and there has not been a lot of published literature in this area. There have been some approaches towards solving this problem. Building \"plastic interfaces\" [7,30] is one such method in which the UIs are designed to \"withstand variations of context of use while preserving usability\". This methodology uses concepts from modelbased approaches that we will discuss next in building UIs.\\nTranscoding [4,12,13] is a technique used in the World Wide Web for adaptively converting web-content for the increasingly diverse kinds of devices that are being used these days to access web pages. This process also requires some kind of transformation that occurs on the HTML web page to convert it to the desired format. Transcoding assumes multiple forms. In the simplest form, semantic meaning is inferred from the structure of the web page and the page is transformed using this semantic information. A more sophisticated version of transcoding associates annotations with the structural elements of the web page and the transformation occurs based on these annotations. Another version infers semantics based on a group of web pages. Although these approaches work, they are not too extensible since it is not always possible to infer semantic information from the structural elements of web pages.',\n",
       " 'RELATED WORK: Our work draws together three main areas of research. Space limitations preclude an extended discussion; additional references are contained in (Aoki et al., 2002;Woodruff, Aoki et al., 2001;Woodruff, Szymanski et al., 2001).\\nInteraction in museum settings. The importance of social interaction to museum visitors is well known (e.g., (Hood, 1983)). There are two types of studies of particular interest.\\nMcManus observed visitor usage of text labels; she noted that visitors were inclined to treat exhibit labels as conversation to which they had been party (McManus, 1989). A number of studies of museum visitors have been conducted using methods derived from conversation analysis (see, e.g., (Falk & Dierking, 2000), Ch. 6, and (vom Lehn, Heath, & Hindmarsh, 2001). These studies focus on talk, interaction and learning in conventional environments; here, we have focused on the effects of electronic guidebooks on social interaction and learning resources.\\nElectronic guidebooks. The cultural heritage community has formally studied electronic guidebooks for many years (Screven, 1975). Related work in HCI has focused on aspects such as, e.g., location-aware computing (Abowd et al., 1997) and only recently have significant user studies been reported (e.g., (Cheverst, Davies, Mitchell, Friday, & Efstratiou, 2000)). The HCI studies focus on system design and evaluation; here, we focus on the effects of our system on visitor interaction.\\nMedia and interaction. There is an extremely rich literature on collaborative multimedia environments; of particular interest are media spaces (Mackay, 1999). Many of these systems have been evaluated, but most apply either ethnographic techniques or quantitative methods to studies of installed workplace systems. In this study, we apply conversation analytic techniques to the study of a mobile, leisure-activity system that provides shared access to application content.',\n",
       " 'RELATED WORK: Designers have been working with qualitative social scientists for over a decade. Reports on this work often appear in the (overlapping) areas of HCI, computersupported cooperative work (CSCW), and computermediated communication (CMC).\\nThe application of CA to system design has been the major subject of several books [15,25,28] and a number of papers. There are two main ways that CA has been applied. First, designers have attempted to use CA as a source of principles (commonly referred to as conversational \"rules\" or \"protocols\") for making systems more \"conversational\" in their interactions. For example, CA has been used for modeling conversation in CMC systems [1,5,22] and interactive systems simulating human conversational patterns [7,10]. Other researchers have attempted to apply such \"rules,\" essentially by analogy, to interactive systems of other kinds [11,18,26]. Second, CA has been used as an experimental \"evaluation\" method for screen-based interfaces [8,18], speech interfaces [28] and CMC systems such as collaborative virtual environments (CVEs) [6,13]. That is, users are recorded as they attempt to use a system, and their interactions are examined using conversation analytic techniques.\\nIn spite of this activity, there are no detailed, \"how to\" discussions of the involvement of conversation analysts in the iterative design process. There have been a variety of reports of experiences involving ethnographers (e.g., [4,14,23]); many issues raised in these reports do generalize, but (as we have discussed) application of CA raises its own particular issues. In one report [8], a designer applied CA-derived methods to address usability issues during iterative design.\\nHowever, the designer/analyst was entirely concerned with using detailed examination to understand the particulars of specific problematic human-computer interactions -\"to interpret what was really going on in the interactions\" [8, p.190] -as opposed to making generalized findings about the organization of interactions, which is the goal of CA. Perhaps more to the point, a scenario involving a single person who plays the role of both designer and analyst reveals little about the mechanics of bringing a professional social scientist into a design team.',\n",
       " '',\n",
       " 'RELATED WORK: The IBM WebSphere Transcoding Publisher (WTP) [12] is a commercial product that supports an HTML-to-VoiceXML transcoder. Transcoders can be plugged into a WTP server that can be configured as a proxy. Client-side browsers can use this proxy to obtain transcoded content. The proxy intercepts the HTTP from the client-side browser, fetches the requested HTML document, transcodes it, and forwards on the transcoded document to the browser. In the case of the HTML-to-VoiceXML transcoder, a voice browser could be configured to use the proxy to receive VoiceXML versions of HTML web pages. IBM has an HTML-to-VoiceXML transcoder for use with WTP that splits the HTML into two main sections in the VoiceXML code it produces: a main content section and a listing of all the links on the page [7]. In addition, a menu is added to the VoiceXML file to allow users to navigate between the two other sections, or to exit.\\nThe main content section contains the text from the web page. The transcoder divides the main content into subsections based on heading tags (e.g. <h1>, <h2>, etc.). The text between heading tags is used to create menus and speech recognition grammar choices in the VoiceXML code. Users can speak any of the prompted choices to listen to the paragraphs following the heading tags. The link list section contains a listing of all the links available on the page. The text between the HTML link tags (e.g. <a></a>) is used in the speech recognition grammar choices and as the text to be read in the list of link choices. This transcoder has a simple translation strategy that works for simple HTML documents with clearly structured heading tags and for documents that the text between the <a></a> tags is context independent. Unfortunately, most web pages do not have these characteristics. For example, if the transcoder translates a page that has no heading tags in it, the result is of very low usability. The resulting VoiceXML code presents all the text on the page sequentially without any user control.\\nAnother approach that solves some of the problems of automatic transcoding is one that uses annotations. Two papers [2][8] describe annotation-based transcoding research done at the IBM Tokyo Research Laboratory. In both of these cases they use the Transcoding Architecture within the WTP. Hori et al. [8] designed a system to make HTML documents suitable for small-screen devices. Asakawa et al. [2] used the same external annotations method and organize visually separate sections of an HTML page so they can be presented together in a voice presentation. Both projects focused on using annotations to mark the importance value of an HTML page subsections and to then reconstruct the page for browsing in a PDA or a voice browser. Our approach relies on external annotations for the first phase (Phase I) of transcoding. In Hori [8] and Asakawa [2] annotations are used to highlight sections of pages that are of interest for later processing. We use annotations to remove unneeded data and to insert our own new tags. The result is an XML file with information for a voice interface. Then in Phase II of the transcoding, we use a servlet transcoder to translate this XML stream into VoiceXML output. The Aurora transcoding system [9] has some similarities with our work. They adapt web pages based on semantic information and build an XML document with extracted information. Their intent is for the transcoded document to support navigation in Internet Explorer and in IBM Homepage Reader. Their work is more geared towards specific tasks that users may perform, such as interaction with an auction site. The semantic information used for the transcoding must be produced manually.\\nSpeech Application Language Tags (SALT) [16] is a project that is in its early stages. SALT tags are added to an HTML document so that users with a special browser can interact with the Web using graphics and voice at the same time. SALT is different from VoiceXML in several ways. First, it is intended to extend the existing web browser with a voice interface, and not as an alternative interface style. Also, SALT takes a programming approach to adding voice to the web, while VoiceXML uses a document-based approach. SALT applications are composed of objects , triggers and events, while VoiceXML applications are built by combining tags into one or more documents.\\nXHTML+Voice [19] has an approach that is similar to SALT, with a focus on supporting multi-modal devices. In this approach, existing VoiceXML tags are integrated into XHTML. In contrast, SALT tries to integrate new tags into HTML.\\nSeveral Interfaces, Single Logic (Sisl) [3] is an architecture and domain-specific language (DSL) for single service logic to support multiple user interfaces. It provides a high-level abstraction of the user-system interaction. The idea is to design the transaction to be provided in DSL and then to convert it into several interfaces (including, for example, VoiceXML and HTML). Sisl is useful when trying to build a system from scratch that will support multiple interfaces. H owever, it does not support converting existing HTML pages to VoiceXML.\\nUIML is a language designed to build multi-platform interfaces. The language relies on a number of transformations, similar to transcoding, that changes code from a high level representation (platform independent) to a platform specific representation. Farooq et al. [1] have developed transformations to convert generic UIML to platform-specific UIML. This work can also be included in the same category with Sisl, in that they both can be used to create multi-platform interfaces from scratch, but cannot help in the transcoding of existing content. Others have used UIML to produce voice interfaces [14].\\nEmacspeak [15] uses information from an HTML file to support auditory interfaces. It obtains its data by analyzing HTML tags, such as h1, h2. The resulting file is motivated by the needs of visually impaired users. With the new auditory version, users can listen to the prompt and respond making selections through keyboard.\\nTell-me and BeVocal [17][4] are two centralized services that provide access to information that is often found on the web via a phone voice user interface. The personnel at either of these organizations organize the information gathered from the web in a form that a suitable for use in a voice user interface. This approach has higher usability than the transcoding approach but the information available is restricted to only those web pages that the central service has \"converted.\" They take advantage of VoiceXML to provide a more flexible and economic voice service than with traditional interactive voice response (IVR) systems. Some transcoding research has also been done for PDAs to provide access to the WWW based information on handheld devices. Orkut et al. [5] divide web pages using the syntactic structure of HMTL pages. Using HTML tags like <p>, <ol> and <table>, they split a Web page into several units and create multiple representations of it (incremental, keyword and summary), to compactly display these units on a PDA. Their worked allows the user to have an overview of a web page stored in their PDA and to explore interesting parts of the page. Some strategies, like displaying first few words of a sentence and progressively expanding to more content, fit well on small screens using pen-based input interaction. However, many of their ideas are not applicable for voice interaction.',\n",
       " '',\n",
       " \"RELATED WORK: Much of the inspiration for the Ubiquitous Interactor comes from early attempts to achieve device independence, or in other ways simplify development work by working on a higher level than device details.\\nWe have already mentioned that lack of hardware standards created a need of device independent applications during the seventies and the eighties. User Interface Management Systems like Mike [12] and UofA* [14] addressed this problem, together with model-based approaches like Humanoid [16]. Others proposed more partial solutions to shield developers from differences in input devices [9], or guide them in the selection of input devices and interaction techniques [7].\\nIn current research, device independence is addressed in two different research fields, that of ubiquitous and mobile computing and that of universal access. The Ubiquitous Interactor (UBI) has its origin in the ubiquitous and mobile research, but provides solutions that can be of use in universal access too.\\nXWeb is a representative of work in the mobile and ubiquitous research field [13]. Inspired by the Web and Web browsers, XWeb encodes the data sent between application and client in a device independent format. Clients are responsible for the generation of user interfaces. Clients only generate user interfaces of one single type, so users get the same type of user interface to all XWeb services unless they use different clients. However, in XWeb service providers cannot control the presentation of the user interface, something that is provided in UBI.\\nUser Interface Markup Language (UIML), is an XML compliant markup language for specification of user interfaces [1]. This description is converted to another language, for example Java or HTML. UIML differs from UBI in that its descriptions cannot take advantage of device specific features, and it only supports user-driven interaction.\\nUnified User Interfaces (UUI) [15] are a representative of the universal access research community. UUI is a design and engineering framework composed by three parts: a method for design, a software architecture, and tools. The goal of UUI is to provide user interfaces tailored to different user groups and situations of use in terms of users' physical capabilities, preferences and usage context. UUI is a project with very large scope, making all user interfaces accessible to all users. This means that they take into account a large number of factors (e.g. contextual and environmental) that make the system more complex than we believe is necessary to solve the problems UBI is addressing.\",\n",
       " 'Background and related work: Many of the phenomena seen in our data have been reported in studies of remote communication systems designed to facilitate \"informal workplace communication\" -the kind of \"opportunistic, brief, context-rich and dyadic\" (Nardi et al., 2000) interactions that happen between physically proximate workers (Whittaker, Frohlich, & Daly-Jones, 1994). Such systems include classic open-channel environments such as video spaces (e.g., (Dourish, Adler, Bellotti, & Henderson, 1996)) and audio spaces (e.g., (Ackerman, Starr, Hindus, & Mainwaring, 1997)); more recently, there have been similar studies of various kinds of textual messaging tools such as IM (Isaacs, Walendowski, Whittaker, Schiano, & Kamm, 2002;Nardi et al., 2000). A common finding is that providing a continuous connection between users facilitates a bursty conversation style, in which formal conversational openings and closings are infrequent even though long pauses occur between bursts of talk, as well as a more conventional focused conversation style.\\nRecent analyses of IM use have highlighted communication behaviors that are now strongly associated with IM. Nardi et al. introduced the notions of plausible deniability (relying on the sender\\'s lack of information, e.g., about the presence of the recipient, to excuse unresponsiveness) and intermittent conversation (interactions with long pauses between individual turns) (Nardi et al., 2000), and Voida et al. applied an affordance analysis to explain certain similiarities between IM and voice communication (Voida, Newstetter, & Mynatt, 2002). \"Hanging out\" -i.e., IM as social space -features prominently in teen and young adult use of IM (Grinter & Palen, 2002).\\nAnother recent thread of research concerns the social use of mobile communication media, including SMS (Grinter & Eldridge, 2001). Mobile phones, like IM, enable the construction of social spaces (Ito, 2001;Ling & Yttri, 2002) as well as affording other ritual communication (Taylor & Harper, 2002), though social uses must be carefully managed in public spaces (Palen, Salzman, & Youngs, 2001;Weilenmann & Larsson, 2001).\\nMobile phones are widely used for micro-coordination -the use of dynamic, just-in-time activity coordination in lieu of extensive pre-planning (Ling & Yttri, 2002).\\nWe are aware of very few studies of the use of handheld two-way radio (\"walkie-talkie\") communication. The \"Denver Project\" introduced radios to Xerox service technicians, assessing their patterns of adoption as well as use (Orr, 1995). A study at Interval Research supplied radios to teens for use during a weekend-long rock concert (Strub, 1997). To our knowledge, there have been no published studies of cellular radio, which affords wide-area communication.',\n",
       " '',\n",
       " 'Related Work: Numerous studies have explored the use of highly mobile devices, like hand-held computers and PDAs, to support specific applications. For example, Waycott & Kukulska-Hulme (2003), studied the use of PDAs for reading course material. Newcomb, Pashley & Stasko (2003), studied the use of PDAs to support grocery shopping, and Spain, Phipps, Rogers & Chaparro (2001) explored the utility of PDAs as a data collecting device.\\nThe study described here is different; it focuses on the overall utility of PDAs to a particular class of users, rather than focusing on the utility of the device for a particular application.',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'RELATED WORK: Moving application windows among various displays has been the focus of research in multiple ubiquitous computing environments. In i-Land, a room with an interactive electronic wall (DynaWall), computer-enhanced chairs, and an interactive table, three methods were introduced for moving application windows on the DynaWall. [10,1] Two of these methods, shuffling and throwing, are implemented using gestures. Shuffling is done by drawing a quick left or right stroke above the title bar of a window. This will move the window a distance equal to the width of the window in the gestured direction. Throwing is done by making a short gesture backward, then a longer gesture forward. This will move the window a distance proportional to the ratio between the backward and forward movement. The throwing action requires practice because there is no clear indication of how far something will move prior to using it. The final method for moving windows in i-Land is taking. If a user\\'s hand is placed on a window for approximately half a second, that window shrinks into the size of an icon. The next time the user touches any display, the window will grow behind the hand back to its original size. In Stanford\\'s iRoom, the PointRight system allows users to use a single mouse and keyboard to control multiple displays. [3] Changing displays is accomplished by simply moving the cursor off the edge of a screen. Currently, iRoom does not move applications across displays, but this mouse technique could be extended to dragging application windows as well.\\nAnother approach for manipulating objects (text, icons and files) on a digital whiteboard is \"Pick-and-Drop\". [6] Using Pick-and-Drop, the user can move an object by selecting it on a screen with a stylus (a small animation is provided where the object is lifted and a shadow of the object appears) then placing it on another screen by touching the desired screen with the again. The benefits of this approach include a more tangible copy/paste buffer and a more direct approach than using FTP or other file transfer techniques.\\nA more general method for controlling devices in a ubiquitous computing environment is Microsoft\\'s XWand. [4,12] The XWand is a wireless sensor package in the shape of a wand that senses its own orientation with respect to the room. While the original idea is to use it in combination with gestures to control various devices (stereo, TV, lights, etc), it could also be extended to move application windows between multiple displays. This new concept could then be incorporated with the ideas in PointRight by allowing various input devices to be used, such as the XWand. ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'II. RELATED WORK: ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " \"RELATED WORK: There is an extensive literature on the domestic environment (see e.g. [24] for a survey), and [6] is an excellent example of how ethnographic findings in this environment may be applied to design. In addition, there is an abundant literature on smart home research; [2,17] provide an excellent introduction to this topic. However, a major limitation of research in this area is that the user experience has primarily been investigated in built laboratories [11,17], the notable exception being Mozer's intriguing experience as the sole occupant of the Adaptive House [25]. In this paper, we report a study of real-life, long-term user experiences with home automation systems that emerged in response to the needs of the residents.\\nLiterature on spirituality and technology is significantly more sparse than that on smart homes, but compelling arguments have been made regarding its importance, and [1,26] call for more research in this area. The recent work of Wyche et al. on the use of communication technologies by pastors in megachurches is an excellent exploration in this vein [31], and we hope to further contribute to the literature on spirituality and technology by exploring the needs and experiences of the Orthodox Jewish population.\\nThere is a rich and extensive literature on Jewish community and Jewish practice, including Sabbath rituals e.g. [8,9,15]. This research focuses primarily on issues such as the meaning, history, regulations, and experience of Jewish life. Many practical guides exist regarding Sabbath behavior, e.g. [5], and multiple professional and religious organizations provide guidance on and develop specialized Sabbath Day technologies for the Jewish population, e.g.\\n[32]. Some mainstream commercial vendors also supply appliances such as ovens with Sabbath modes. The imagination of the press has occasionally been captured by these technologies [12,13]. These reports have focused on technology and rules for its use, rather than on user experience. To our knowledge this paper reports the first study of user experience with Sabbath technology -the first study to examine issues such as day-to-day practices with Sabbath technology, which technologies people choose to employ and find useful in their homes, user response to and perception of Sabbath technology, the interaction of Sabbath technology with family life, etc.\",\n",
       " 'Related Work: ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatedWorks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relatedWork_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'related work' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    relatedWork = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(relatedWork)\n",
    "\n",
    "def get_intro_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    # if section name contains 'related work', get the section number, and then get all subsections\n",
    "    section_num = '-1'\n",
    "    for d in paper:\n",
    "        if 'intro' in d['section'].lower():\n",
    "            section_num = d['sec_num']\n",
    "            break\n",
    "    # get all subsections\n",
    "    text = [d['section'] + \": \" + d['text'] for d in paper if section_num and d['sec_num'] and d['sec_num'].startswith(section_num)]\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def get_RQ_text(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    paper = convert_tei_xml_soup_to_s2orc_json(soup, \"\", \"\").as_json()['body_text']\n",
    "    texts = []\n",
    "    # for dSec in paper['body_text']:\n",
    "    text = str(paper).lower()\n",
    "    # non greedy match\n",
    "    regexp = re.compile(r'[- a-z([]*?(?:\\d.|:) (?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    # regexp = re.compile(r'(?:what|how|why|is|are|can|to what extent) [^[?]*\\?')\n",
    "    matches = regexp.findall(text)\n",
    "    if matches:\n",
    "        texts.extend(matches)\n",
    "    return '\\n'.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.61it/s]\n"
     ]
    }
   ],
   "source": [
    "rqs = []\n",
    "intros = []\n",
    "relatedWorks = []\n",
    "\n",
    "for text in tqdm(dPapers):\n",
    "    try:\n",
    "        relatedWorks.append(get_relatedWork_text(text))\n",
    "    except:\n",
    "        relatedWorks.append(\"\")\n",
    "    try:\n",
    "        intros.append(get_intro_text(text))\n",
    "    except:\n",
    "        intros.append(\"\")\n",
    "    try:\n",
    "        rqs.append(get_RQ_text(text))\n",
    "    except:\n",
    "        rqs.append(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 43 14\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sum([1 for t in relatedWorks if t]),\n",
    "    sum([1 for t in intros if t]),\n",
    "    sum([1 for t in rqs if t]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to jsonl file\n",
    "# with open('allQuestions_s2orc.jsonl', 'w', encoding='utf8') as f:\n",
    "with open('RQs_TEI2JSON.jsonl', 'w', encoding='utf8') as f:\n",
    "    for idx, rq in enumerate(rqs):\n",
    "        # if rq:\n",
    "        if True:\n",
    "            f.write(json.dumps(\n",
    "                {\n",
    "                    'intro': intros[idx],\n",
    "                    'relatedWork': relatedWorks[idx],\n",
    "                    'rq': rq\n",
    "                }\n",
    "            ))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('NFT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c377a027f0bbb6c0821b68e5c065774f74ef070985e76133b9b1c3a883a152de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
