{"intro": ["Empirical methods are critical to gauge the scalability and robustness of proposed approaches, to assess progress and to stimulate new research questions. In the field of natural language generation, empirical evaluation has only recently become a top research priority (Dale, Eugenio et al. 1998) . Some empirical work has been done to evaluate models for generating descriptions of objects and processes from a knowledge base (Lester and Porter March 1997) , text summaries of quantitative data (Robin and McKeown 1996) , descriptions of plans (Young to appear) and concise causal arguments (McConachy, Korb et al. 1998) . However, little attention has been paid to the evaluation of systems generating evaluative arguments, communicative acts that attempt to affect the addressee's attitudes (i.e. evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor). The ability to generate evaluative arguments is critical in an increasing number of online systems that serve as personal assistants, advisors, or shopping assistants 1 . For instance, a shopping assistant may need to compare two similar products and argue why its current user should like one more than the other."], "relatedWork": [], "rq": ["IntroductionEmpirical methods are critical to gauge the scalability and robustness of proposed approaches, to assess progress and to stimulate new research questions. In the field of natural language generation, empirical evaluation has only recently become a top research priority (Dale, Eugenio et al. 1998) . Some empirical work has been done to evaluate models for generating descriptions of objects and processes from a knowledge base (Lester and Porter March 1997) , text summaries of quantitative data (Robin and McKeown 1996) , descriptions of plans (Young to appear) and concise causal arguments (McConachy, Korb et al. 1998) . However, little attention has been paid to the evaluation of systems generating evaluative arguments, communicative acts that attempt to affect the addressee's attitudes (i.e. evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor). The ability to generate evaluative arguments is critical in an increasing number of online systems that serve as personal assistants, advisors, or shopping assistants 1 . For instance, a shopping assistant may need to compare two similar products and argue why its current user should like one more than the other."]}
{"intro": ["Most studies have aimed to investigate the effects of DDL on learners\" vocabulary and grammar (Coxhead & Byrd, 2007; Huang, 2014; Liu & Jiang, 2009; Ucar & Y\u00fckselir, 2015; Varley, 2009; Vyatkina, 2016; Yunus & Awab, 2014) . Liu and Jiang (2009) examined the effects of integrating corpus and contextualized lexico-grammar in foreign and second language teaching. The analysis of their data revealed that learners improved their command of lexico-grammar, increased critical understanding of grammar, and enhanced discovery learning skills. However, the study brought to light that corpus-based lexico-grammar analysis caused some difficulties for many students. examined the relationship between one type of datadriven learning (DDL) and inductive-deductive learning styles and found that the participants improved their grammar significantly after teacher-led guided DDL induction. Their findings pointed out that guided DDL type induction may be beneficial for both deductive and inductive learners irrespective of their learning styles. Ucar and Y\u00fckselir (2015) investigated the impacts of corpus-based activities on verb-noun collocation learning in EFL classes. Their study had an experimental design and consisted of 30 participants. The experimental group was taught verbnoun collocations through corpus-based materials, and the control group learnt collocations via conventional methods. They found a statistically significant difference between the experimental and the control group which showed that corpus-based activities had a significant impact on the teaching of verb-noun collocations in EFL classes."], "relatedWork": [], "rq": ["RQ1. Can DDL help EFL learners improve their lexico-grammatical use of abstract nouns in their writing?The control group and the experimental group had similar error-free ratios in terms of the use of the ten target nouns in the pre-test, but in the post-test, the experimental group had much fewer errors in their texts. Also, even though the experimental group used various patterns in the post test, the patterns used by the control group were limited. For instance, excitement was used in a greater variety of grammatical patterns by the experimental group than by the control group. The experimental group used five grammatical patterns of excitement: 1) PP + excitement; 2) V + excitement; 3) excitement + V; 4) excitement + copular verb BE and 5) copular verb BE + excitement. However, the control group used the noun in only three patterns: 1) excitement + copular verb BE; 2) PP + excitement; 3) V + excitement. Moreover, while the control group preferred using simple patterns in the post test, the experimental group used more complex patterns. For example, the noun silence was used with various adjectives by the experimental group (sudden silence, awkward silence, strange silence, uncomfortable silence, breathless silence, unknown silence). That is, the results indicated that to a large extent, the experimental group outperformed the control group in both areas of syntactic variations and correct grammar use because of the use of DDL activities as in the studies of Huang (2014) , , Ucar and Y\u00fckselir (2015) , Vyatkina (2016) , and Yunus and Awab (2014).", "RQ2: What are the perceptions of EFL learners of the effect of DDL on their vocabulary learning and vocabulary use in writing?The results obtained from the questionnaire on the effectiveness of DDL confirmed the results mentioned above and showed that the participants were positive about the use of DDL activities in writing because they believed that DDL activities contributed to their vocabulary learning and use in writing. They thought DDL activities helped them learn the meaning of words, collocations, and usage of words. Thus, the results were mostly similar to the results of the previous studies on learner perceptions of DDL (e.g. Charles, 2012; Gaskell & Cobb, 2004; Huang, 2014; O\"Sullivan & Chambers, 2006; Sun, 2007; Yoon & Hirvela, 2004) . Similar to the study of Yoon and Hirvela (2004) the students found the corpus approach beneficial to the development of writing skill. Also, the results were in line with Vyatkina\"s study\"s results (2016) which showed that learners were willing to use DDL for independent learning in the future. Also, as in Sun\"s study (2007) , the students had very positive opinions on the DDL activities for writing skill. However, although in Huang\"s study the participants thought DDL activities were useful for learning grammatical patterns of the words, the participants in this study were not sure about their usefulness in learning grammatical patterns. As in Liu and Jiang\"s study (2009), corpus-based lexico-grammatical analysis caused some difficulties for some students. In addition, contrary to Huang\"s study (2014), students did not believe that studying concordances was time-consuming. Furthermore, they did not have problems with unfamiliar vocabulary while they were doing concordance activities unlike the participants in his study."]}
{"intro": [], "relatedWork": [], "rq": ["importance of obtaining human perspectives on fairness in algorithm development [19] . While there are multiple steps in the development of an algorithm which can influence the fairness of the outcome, previous work has highlighted that the perceived fairness of predictors themselves can be utilised to inform fair algorithmic decision making [21] . However, developing a scalable and practical feature selection process capable of obtaining the opinion of a diverse population sample remains an open research question [20, 55, 60] . As a first step in overcoming this challenge, here we explore the perceptions of crowdworkers with regards to fair predictors in a recidivism prediction model."]}
{"intro": ["However, we do not have comprehensive understanding of their relative contribution to processing load. Empirically, the estimation of surprisal and entropy values requires a language model, the quality of which depends on many factors (e.g., the corpus size, the model type) (c.f., Goodkind and Bicknell, 2018 argued the effect of surprisal was robust when the measures were estimated using a wide range of language models with different qualities). Also surprisal and entropy values tend to be highly correlated in natural languages, which makes it difficult to tease apart their relative roles in online language processing."], "relatedWork": [], "rq": ["DiscussionOur modeling study was intended to explore design-related issues and predict results in human eye-tracking experiments that we plan to run. In human experiments, participants need to learn the grammar hidden in a sequence of symbols. To make learning easier, we chose a simple grammar which made it hard to interpret the effect of entropy; it could be the effect of entropy or the effect of entropy reduction. However, the proposed model is general enough to cover more complex grammars and diverse situations (e.g., selfpaced reading). We chose the Hidden Markov Model and the A2C architecture for the perception the decision making modules mainly for modeling convenience. The HMM can be replaced with a more elaborated neural language model when dealing with more complex grammars. The emphasis should be given to our architectual choice. The addition of the decision making module that has the ability to develop a policy on its own provides the system to control the amount of uncertainty flexibly in response to the task situations. Bicknell and Levy (2010) took the same approach similar to explain reading eye movement patterns, which influenced our work. Our work is different from theirs in that (1) we considered noisy memory more directly and (2) we used reinforcement learning to let the model discover a good decision policy; we believe both additions can lead us to interesting research questions."]}
{"intro": [], "relatedWork": [], "rq": ["A. Website Satisfaction and TrustMoreover, there does not seem to exist a clear consensus among scholars about the nature of the relationship between satisfaction and trust. Some authors [43] , [44] consider that satisfaction is a determinant of trust. Their tests in the context of online business showed that previous positive shopping experiences result in high customer trust. However, other authors [45] , [46] reported just the opposite: trust influences satisfaction. For them, the strong image that customers have about a company helps them to perceive a high level of satisfaction. However, several other relevant demographic studies [4] , [7] , [15] , [17] , [47] , [48] represent both satisfaction and trust as unrelated variables in their research models. These research efforts are closely focused on the study of cultural differences, and they consider the impact of different design approaches on trust and satisfaction and, in turn, evaluate the relationship of these variables to online loyalty. As our research questions are closer to these studies, we decided to exclude the relationship between satisfaction and trust from our model. However, the consideration of the relationship between satisfaction and trust in the context of different national cultures is an interesting avenue for future research. Fig. 1 presents the research model guiding this investigation. The proposed research model was developed based on conceptual and theoretical studies in the domain of e-commerce. The model theorizes that web design attributes positively influence the user trust and satisfaction in a high-UA culture. In terms of website design, five design attributes/features suggested by research community (i.e., [4] , [11] , [22] , [49] , [50] ) include the following."]}
{"intro": ["There are many methods of investigating UX problems from both qualitative and quantitative paradigms, including questionnaires, eye-tracking and physiological computing. Qualitative methods of investigating UX problems often require participants to communicate their experience. This may be inaccurate due to many reasons such as lack of self-reflection with regards to one's own difficulties, as well as difficulties with communication in general, which is also one of the diagnostic criteria for ASD [32] . An alternative approach which requires less verbalisation is using selfreported scales to elicit feedback from users. However, recollecting experiences requires cognitive processing and self-reflection and, because of these reasons, it may be highly subjective and inaccurate. Also, some people with autism, ADHD (Attention Deficit Hyperactivity Disorder) and similar developmental disorders may exhibit cognitive impairment [43] , thereby limiting the accuracy of their reported scores. Due to these limitations, we argue that methods that require eliciting intentional feedback from individuals with autism are less reliable for understanding the UX challenges that people with autism face. Usability metrics such as error rates and completion times can be used to detect common problems that users experience on the web. Predominantly, these metrics answer the question -\"Is there a problem?\" but, discovering problems is only the first step to improving UX. Analysis of gaze behaviour with metrics like fixation location/duration and saccades (visual transitions) can be used to answer the more advanced question -\"Where is the problem?\" but, this is also limited because knowing where the problem lies may not always lead to a solution. Trying a different approach may alleviate the problem for one user, but, without knowing why the problem exists, we may not have an understanding of which group of users the problem affects. People with autism are not the only atypical groups of users on the web. In addition to the idiosyncracies of typical users, people with learning, developmental, obsessive-compulsive (OCD), and other types of disorders may also require special considerations when carrying out studies to identify UX issues specific to these user groups. The answer to the question -\"Why is there a problem there?\" provides more context so that researchers can offer recommendations to overcome the existing UX issue(s)."], "relatedWork": [], "rq": ["DISCUSSIONRegarding RQ1, that pertains to determining if there is a difference in arousal between both groups in browsing tasks, we hypothesised that the ASD group would experience more arousal compared to the neurotypical users. However, this was not the case. This could be because an increase in arousal could be caused by diverse reasons. Moreover, a more direct approach would be to test each UI element based on this hypothesis. This was not however possible due to the small and uneven sample size making statistical group comparisons inappropriate. With regards to RQ2, about identifying differences in AOIs between groups, we were able to observe differences in visual and physiological patterns between both groups in our descriptive approach. Take for example our observation that the individuals with autism showed less arousal compared to the neurotypical group for several UI elements on the YouTube page. One element shows people in a happy state, while the other contains the thumbnail for a video regarding the death of physicist, Stephen Hawking. A UX researcher may relate this to symptoms where people with autism interpret affective expressions differently from neurotypical people [33] . In research by Baron-Cohen et al., they observed that individuals with autism do not recognise bodily expressions of affect as well as the neurotypical populace [4] . Therefore, an implication could be that, if a UI element contains a link that is a functionally significant aspect of the website, they must present it in a manner that does not rely primarily on facial expressions or affective cues. This is one such usability issue that can be identified using this methodology. Another behaviour that our methodology can help to uncover is that of understanding the affective states of users before leaving a web page. It may be the case that the initial UI elements that the users engage with eliciting higher arousal than the final ones, as is the case with the BBC and YouTube web page, or that participants experience an increase in arousal in the middle of their interaction compared to the beginning and end, as the case with the Amazon website or they experience lower arousal levels at the final UI elements as with the Adobe web page. Arousal could indicate positive states such as attraction, neutral ones (i.e. cognitive load), or negative affective states like frustration and stress. When participants experience an increase in arousal towards the end of an interaction, this could imply that they found what they were looking for, i.e., excitement in completing a task/goal, or that they were frustrated. Both of these cases could benefit from optimising the user interface. In the first instance, if users take a long time to find the item of interest on a page, it means that the user experience may be improved by re-positioning the element to a more visually accessible location, or by using a more attractive design to draw the attention of users towards that particular content. When users experience frustration with a UI element before leaving a web page, it could be an indication that the UI element has a usability problem. This type of diagnosis is mainly possible because we combined users visual scan path with a measure of their arousal levels. The implications for design include aggregation of different user groups of users, and potential modelling of their behaviour. For example, on an e-learning website, people with autism can have a different profile that takes account of and adapts to their unique traits and requirements. Eye-tracking is becoming more accessible, and we anticipate that webcameras and mobile phone cameras will one day have eye-tracking capabilities. Our methodology could then be used within social media and mobile applications. Posts and feeds can be treated as atomic UI elements so that the characteristics of different posts (sentiment, object classification, colour etc.) can be investigated against the visual scan sequence and the corresponding affective states that are elicited. Our work is not without limitations. Due to the limited accuracy of eye-tracking technology, our analysis has been based on group behaviour as opposed to individual behaviour. Another limitation is that our methodology can currently only be used in laboratory settings. Ambient light, inter-colour differences between (and within) stimuli and other environmental variables may introduce confounding factors which may yield different results in the wild. Therefore, our methodology needs to be optimised to handle these factors dynamically in naturalistic settings."]}
{"intro": [], "relatedWork": [], "rq": ["Preliminary experiments can require thousands of iterations of training or more to address each research question explored, and human time and attention are limited. However, it remains to be determined whether rewards with the sparse, delayed properties similar to those generated by humans will be feasible to train RL controllers for continuous-time, continuous-state systems of a certain complexity. For these reasons, we have elected to create algorithms that generate rewards similar to those that a human would be likely to give; we refer to these computergenerated rewards as \"pseudo-human\" rewards. These rewards are generated in response to the current arm reaching movement properties of the system, and they are used as inputs to the RL controller. In this study, we will examine whether these [24] and [25] ."]}
{"intro": [], "relatedWork": [], "rq": ["Elements from Participatory Research(1) PR demands democracy and different levels of participation. We put great emphasis on collaborating with the YFMs democratically. We considered them as co-researchers who equally contribute to diverse aspects of the project, e.g., research questions, objectives, and outcomes. However, the co-researchers were not expected to participate fully in every stage of the research, i.e., in the sense of levels 6-8 defined in Wright et al.'s \"stage model of participation\" [94] (Figure 1 ). Based on our observations of the participants' behavior during the activities, we considered it to be neither sensible nor ethical to ask recently fled and possible traumatized young population at school age to increase their workload beyond their regular day-to-day activities. To fully involve them in data analysis, they would have needed to spent considerable time on studying and applying various analysis methods. As von Unger [90] stresses, the level of participation in every stage has to be adapted to the circumstances. Thus, high levels of participation should not be pursued at any cost. Nevertheless, YFMs had the power to make high-impact decisions in most of the other stages and contributed to those with high levels of participation (Wright et al.'s model-levels 7 and 8)."]}
{"intro": ["The interpretation of nonverbal behavior is strongly dependent on the context (e.g., a smile after having defected may be interpreted differently compared to a smile after having cooperated [13] ). However, and even though the influence of VC emotional displays on decision making is likely to be mediated by human emotions, there is a paucity of studies examining human emotional-motivational processes, in particular at the level of brain activities and facial expressions, elicited by VC facial displays in different contexts (e.g., preceding interactional events; but see [14] ). Therefore, the present study was designed to examine how the facial displays of a VC influence human approach/ withdrawal motivation, as indexed by electroencephalographic (EEG) asymmetry over the prefrontal cortex (relative activity of the left and right hemispheres), and emotional expressions, as indexed by facial electromyography (EMG), during the Iterated Prisoner's Dilemma game (with monetary or equivalent stakes). We expected that inner emotional-motivational processes (putatively indexed by frontal EEG asymmetry) and emotional facial expressions may be dissociated, assuming that display rules (i.e., learned rules dictating the management of emotional expressions based on social circumstances) might affect emotional expressions also in human-VC interaction (see e.g., [15] ). We then also examined how frontal EEG asymmetry and facial EMG activity predict cooperation in the game, and how the outcome of the game affects physiological responses."], "relatedWork": [], "rq": ["Affective Congruency of VC Facial ExpressionsGiven the evidence that behavioral mimicry creates rapport (e.g., [77] ), our secondary research question asked whether the congruency between VC (pre-decision) emotional expression and the participant's emotional state (as inferred on the basis of facial EMG activity and EDA) influences human cooperation and physiological responses. However, we found no differences between the congruent and incongruent expression conditions. It is of note that the present study provides only a weak test for the effect of affective congruency. Although pre-tests suggested that VC facial expressions were dictated by human emotional/physiological responses as intended, this was not necessarily completely the case for all participants. Also, the present study didn't take into account that the effects of emotion expression congruency on cooperation may vary by the type of emotion (e.g., happy versus angry expression). It may also be that the use of facial EMG activity (emotional expressions) alone would have been better than a combination of EMG and EDA in determining the VC facial expressions."]}
{"intro": [], "relatedWork": ["Like chunking, query segmentation is an important step towards query understanding and is generally believed to be useful for Web search (see Hagen et al. (2011) for a survey). Automatic query segmentation algorithms are typically evaluated against a small set of human-annotated queries (Bergsma and Wang, 2007) . The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012) . To address the issue of data scarcity, Hagen et al. (2011) created a large set of manually segmented queries through crowdsourcing 2 . However, their approach has certain limitations because the crowd is already provided with a few possible segmentations of a query to choose from. Nevertheless, if large scale data has to be procured crowdsourcing seems to be the only efficient and effective model for the task, and has been proven to be so for other IR and linguistic annotations (see Lease et al. (2011) for examples). It should be noted that almost all the work on query segmentation, except (Huang et al., 2010) , has considered only flat segments.", "We do not know of any previous work that compares flat and nested schemes of annotation. In fact, Artstein and Poesio (2008) , in a detailed survey of IAA metrics and their usage in NLP, mention that defining IAA metrics for trees (hierarchical annotations) is a difficult problem due to the existence of overlapping annotations. Vadas and Curran (2011) and Brants (2000) discuss measuring IAA of nested segmentations employing the concepts of precision, recall, and f-score. However, neither of these studies apply statistical correction for chance agreement."], "rq": ["Bracket representationBoundary var. 4 ((barbie dress)( up games)) 0 1 0 3 (barbie ((dress up) games)) 2 0 1 2 (barbie (dress (up games))) 2 1 0 1 ((barbie (dress up)) games) 1 0 2 ity. For instance, in the case of NL chunking, it is not clear whether the chunk boundaries should correspond to the innermost parentheses in the nested segmentation marking very short chunks, or should one annotate the larger chunks corresponding to clausal boundaries. For this reason, Inter-Annotator Agreement (IAA) for flat annotation tasks is often poor (Bali et al., 2009; Hagen et al., 2011; Saha Roy et al., 2012) . However, low IAA does not necessarily imply low quality annotation, and could as well be due to the inherent ambiguity in the task definition with respect to granularity. Although we have illustrated the concept and problems of flat and nested annotations using the examples of sentence and query segmentation, these issues are generic and typical of any flat annotation scheme which tries to flatten or approximate an underlying hierarchical structure. There are three important research questions pertaining to the linguistic annotations of this kind:"]}
{"intro": [], "relatedWork": ["That system interfaces influence human behaviour is a central tenet of HCI. This principle is most heavily relied upon in safety-critical systems (see for example (Casey 1998; Salvucci 2001) ). Work on information systems, however, has also demonstrated that user information behaviour is affected by interface design: for example Jones et al showed that users requested more search results for unranked Boolean queries than ranked results in 1998 (Jones, Cunningham et al. 1998 ). More recently, and reflecting a larger change in interface, Ballard et al demonstrate that users are 15-20 times more likely to refine their search queries in a new generation catalogue than a traditional library catalogue (Ballard et al. 2011 ).", "Rowlands noted in 2007 (Rowlands, Nicholas et al. 2007 ) that the process of selecting a useful book from the available options is a surprisingly under-studied part of the book selection process for all books, not just ebooks. This statement remains largely true, with some exceptions: Reutzel and Gali (Reutzel et al. 1998 ) studied children selecting fiction books in a physical library, and noticed they were influenced by shelf position and cover, rather than content. Moore's work (Moore 1995) demonstrated the influence of shelf position on children's selection practices in fiction books, and Borgman (Borgman et al. 1995) demonstrated the same bias in digital libraries. Among adults, cover clearly does have an influence in both bookshops (Buchanan and McKay 2011) and academic libraries (Stelmaszewska et al. 2004; Hinze, McKay et al. 2012) . It is not the only influence, however, Stieve showed in 2006 (Stieve et al. 2006 ) that when choosing between two similar books university students relied heavily on the table of contents; a behaviour that was also demonstrated \"in the wild\" in our own earlier work on both physical and digital academic libraries McKay, Hinze et al. 2012) . Finally, both Stelmaszewska (Stelmaszewska and Blandford 2004 ) and our own earlier work McKay, Hinze et al. 2012) show that book content has an impact on decision making in both physical and digital book libraries. . A striking aspect of this decision process is how quickly many selections are made, though how users assess content rapidly remains an open research question. It seems, then, that in rapid decision making in print book selection cover image and table of contents play a significant role, however how these artefacts affect the decision making process remains unclear."], "rq": ["BACKGROUND AND MOTIVATIONRowlands noted in 2007 (Rowlands, Nicholas et al. 2007 ) that the process of selecting a useful book from the available options is a surprisingly under-studied part of the book selection process for all books, not just ebooks. This statement remains largely true, with some exceptions: Reutzel and Gali (Reutzel et al. 1998 ) studied children selecting fiction books in a physical library, and noticed they were influenced by shelf position and cover, rather than content. Moore's work (Moore 1995) demonstrated the influence of shelf position on children's selection practices in fiction books, and Borgman (Borgman et al. 1995) demonstrated the same bias in digital libraries. Among adults, cover clearly does have an influence in both bookshops (Buchanan and McKay 2011) and academic libraries (Stelmaszewska et al. 2004; Hinze, McKay et al. 2012) . It is not the only influence, however, Stieve showed in 2006 (Stieve et al. 2006 ) that when choosing between two similar books university students relied heavily on the table of contents; a behaviour that was also demonstrated \"in the wild\" in our own earlier work on both physical and digital academic libraries McKay, Hinze et al. 2012) . Finally, both Stelmaszewska (Stelmaszewska and Blandford 2004 ) and our own earlier work McKay, Hinze et al. 2012) show that book content has an impact on decision making in both physical and digital book libraries. . A striking aspect of this decision process is how quickly many selections are made, though how users assess content rapidly remains an open research question. It seems, then, that in rapid decision making in print book selection cover image and table of contents play a significant role, however how these artefacts affect the decision making process remains unclear."]}
{"intro": [], "relatedWork": [], "rq": ["A. RQ1: Strengths, Weaknesses, Opportunities, and Threats2) Weaknesses: Shifting complexity: One problem often found by changing the software architecture to microservices according to different experience reports is that complexity shifts from module-level to a higher level [1] . The challenge of creating, controlling, and monitoring the system is increasing. The larger the number of individual services active in a system, the more complicated it is to keep track. In addition, the test complexity shifts, since services are smaller and contain only a sub-task which leads to simpler module tests. In addition, comparing the output with the same input data makes it easier to test different versions of a module. On the other hand, however, it is much more challenging to test the interaction between dozens of services at the integration or system test level. In summary, the complexity is shifting from the individual module complexity to a higher level, where architectural design, interaction between services, and monitoring takes place [8] .", "A. RQ1: Strengths, Weaknesses, Opportunities, and ThreatsReduced dependency errors In most vehicle software architectures, such as component-based architectures, there is a multitude of dependencies between functions and subfunctions. This is however not only in contrast to the intention to design internal vehicle systems as modular as possible, but is also very error-prone, since changes to a sub-function can have effects on other elements. Vogelsang and Fuhrmann discussed this problem [9] , showing that the high degree of dependency leads to that a function developer is only aware of about 50% of the dependencies. While this is also a problem for microservice architectures, there are already available solutions developed and applied even at the scale of, for example, Netflix and Spotify."]}
{"intro": ["Personal informatics, defined as the process of collecting and reflecting on personal data [29] , is now common practice [17] . However, people often have trouble maintaining tracking, for example keeping their devices charged [13, 22] or forgetting to wear them [13] . Epstein et al. refer to this idea that people stop actively tracking as lapsing [13] . Some people return to tracking, either by resuming use of the same tool or selecting a new tool that better serves their needs or is easier to maintain using. Others abandon tracking entirely. Although some abandon tracking because it provided little value [11] , others leave tracking with feelings of frustration that tracking did not help them achieve their goals [11, 27] or guilty they could not sustain the habit of tracking [7, 11] ."], "relatedWork": [], "rq": ["Extensions to Other Domains of Self-TrackingThis paper has primarily explored and discussed design for people who have lapsed in tracking physical activity using a Fitbit. Our findings can likely be adapted in other domains in which people track their behaviors, sometimes surfacing new design challenges and research questions to consider. For example, many people self-track personal finances, with primarily manual tools (e.g., Quicken) or more automatic tools (e.g., Mint). Although many people are able to consistently track their finances for years, others abandon the practice in weeks or months [13] . Designs can adopt some of the principles we have examined with step count data. A person who tracked their finances consistently for a long time may prefer a holistic view of how their finances changed over time (e.g., Figure 7a 's view of how much money they spent or saved by month). This may avoid presenting obvious habits and better motivate a person toward a savings goal. However, this representation would be barren for a person who tracked for only a few weeks. Informed by representations explored here, one alternative would be day-by-day spending relative to a discretionary budget (Figure 7b ). Another might be a table divided by spending category (Figure 7c ). Both of these aim to provide meaningful feedback based on a relatively short period of tracking."]}
{"intro": ["Personal informatics technologies aim to support the collection of personally relevant data for the purpose of self-reflection and gaining self-knowledge [41] , and include systems such as wearable fitness tracking devices, food journaling tools, and smart journals [20] . These selftracking tools offer many potential benefits, such as providing automated data capture and visualisations of behavioural and physiological data to become \"fitter, happier, and more productive\" [19] . However, the design of automated self-tracking technologies lacks flexibility [36] and often fails to support people's practical goals and emotional needs. For example, prior works suggest that people abandon consumer health technologies over time because of a lack of personally meaningful insights [22, 38] , and switch to paper notebooks to avoid unintended effects and to overcome technological boundaries [3, 33] . Addressing these issues, recent research has proposed leveraging flexible self-tracking to account for people's individual and changing self-tracking practices [10, 30, 36] ."], "relatedWork": [], "rq": ["Summary and Research QuestionsResearch has developed a detailed understanding of selftracking in different domains by individuals who use both self-tracking technologies and paper notebooks [3, 20, 23, 45] . Recently, there has been a push to semiautomated [10] and flexible self-tracking [36] to overcome the limitations of current self-tracking technologies. However, there is a lack of dedicated research on how individuals use and especially design paper notebooks to engage in self-tracking [20] , and what this understanding might imply for the design of flexible self-tracking systems. In this study, we aim to inform the design of self-tracking tools through an analysis of paper bullet journal photographs and related conversation on Instagram, examining the following research questions: (1) how do bullet journalists design their paper notebooks; (2) how do bullet journalists use their notebooks to engage with others online; and (3) what design implications can be derived for future self-tracking technologies? Figure 1 : Example of a daily log according to [7] ."]}
{"intro": [], "relatedWork": [], "rq": ["RQ1. What is the Impact of the Validation Verdict?How important is a positive validation outcome for newcomer engagement? Can an early rejection discourage further participation? The theory suggests that when intrinsic motivation is present, positive task feedback can foster repeat participation [9, 17] . However such feedback should be performance-contingent, not merely task-contingent: positive feedback is only effective if it is linked to contribution performance [6, 16] . In contrast, negative feedback can reduce motivation when initial self-efficacy is low [5, 24] . Are these effects observable for newcomers in HOT?"]}
{"intro": ["In pervasive computing systems, there is often a need to provide users with a way to access and search through ubiquitous information associated with real world objects and locations. Technology such as Augmented Reality (AR) allows virtual information to be overlaid on the users' environment [1] , and can be used as a way to view contextual information. However, there are interesting research questions that need to be addressed: how to know when to present information to the user, how to decide what to present given the plenitude of information, and what is the best way for users to interact with the information. As pointed out in [2] , pervasive computing applications need to place few demands on the user's attention and be sensitive to context."], "relatedWork": ["Speech recognition in human-computer interfaces has been a subject of extensive study (for a review, see [15] ). For example, the observed sound information has been augmented using a model of attention based on measuring the head posture [16] , and [17] by measuring gaze on a computer display. However, as far as we are aware, the idea of combining gaze-based and speech-based implicit input about the interests and context in interaction with persons and objects in the real world is novel.", "user. Already in the Touring Machine [3] more information and menu choices were shown for objects that had remained in the center of the user's view for a long enough time. This kind of contextual user feedback is, however, more explicit than implicit by nature. With our gaze tracking hardware we have been able to detect the implicit targets of the user's attention and to use that data in information filtering. As described in the previous section, there have been studies on using gaze as a form of relevance feedback, but to the best of our knowledge, the current work is the first one to use implicit gaze data for contextual information filtering in an AR setup and to evaluate its usefulness with a user study."], "rq": ["INTRODUCTIONIn pervasive computing systems, there is often a need to provide users with a way to access and search through ubiquitous information associated with real world objects and locations. Technology such as Augmented Reality (AR) allows virtual information to be overlaid on the users' environment [1] , and can be used as a way to view contextual information. However, there are interesting research questions that need to be addressed: how to know when to present information to the user, how to decide what to present given the plenitude of information, and what is the best way for users to interact with the information. As pointed out in [2] , pervasive computing applications need to place few demands on the user's attention and be sensitive to context."]}
{"intro": [], "relatedWork": [], "rq": ["This review indicates that the integration of loanwords into Japanese is only accomplished by their significant modification, modification that some researchers (e.g. Otake 2008) have argued limit their usefulness in language acquisition and production. However, it is also noted that gairaigo is an important element in the lexicon of present-day Japanese, with a considerable number of loanword types utilized in fields as diverse as advertising, politics, economics, entertainment and leisure. Given their ubiquity, gairaigo may potentially have a valuable role in L2 acquisition. Daulton (e.g. 2007) consistently argues that gairaigo offer a means by which learners can access, acquire, and use loanword cognate items, the L2 words from which they are derived. A corpus analysis of frequencies of gairaigo usage in L2 written texts may indicate the veracity of Daulton's claims -and the importance of loanwords for Japanese learners of English. Based partially on Daulton's (2007) methodology, we further the investigation by using a slightly larger longitudinal learner corpus as well as a NS corpus to identify norms and answer the following research questions (NNS, or non-native speakers, indicating Japanese learners of English):"]}
{"intro": [], "relatedWork": [], "rq": ["Part 1 -Online Opinion Survey.The results of our opinion survey (N = 178) [7] have provided the answer to RQ1. The feedback provided by respondents, presented in Figure 1 , indicates that the majority of people want their interaction with voice search system to be more human like. However, the opinions are divided when it comes to system's conversational initiative -with less than 50% of respondents who agreed that voice search system should ask more questions. In the answers provided to open 1 Values for performance have been inverted for comparability reasons questions, many respondents expressed need to for conversational system to have memory of their past interactions, and to ask follow up questions in order to clarify their intent. The insights obtained from the survey informed the design and the scope of 'Voice Interaction Studies' used in Part2 of the project. "]}
{"intro": [], "relatedWork": [], "rq": ["Experiment II: RQ2: Extending to Corpora without Joint InstancesAs mentioned in Section 2, there is an overlap between the two collections in KB, so that there are books which are dually annotated, by both GTT and Brinkman concepts. This allows us to apply methods based on co-occurrence to find mappings [7] . However, it is not always the case that two thesauri have joint instances. In this section we evaluate whether our approach can be applied to the case where there are no joint instances, i.e., where there are no doubly annotated instances."]}
{"intro": ["However, most work on gesture passwords so far has been carried out in laboratories [51, 48, 16, 26] , leaving their performance in the wild as an open research question. Field studies are important for understanding the user-chosen distribution of gesture passwords in realistic settings and how usable and memorable those could be.", "In addition, previous work has focused on using gesturebased authentication for a single account or phone unlocking [51, 50, 26, 48, 16] , and has not considered it for multiaccount configurations. However, people manage multiple accounts at the same time in reality [25, 31] . Previous work also shows how multi-account settings affect the authentication process. For example, a study showed that multi-account interference significantly impacts the ease of authentication of facial graphical passwords [21] . Therefore, it is crucial to explore how gesture passwords would be different under the multi-account context."], "relatedWork": ["There is, however, limited literature applying ESM to mobile authentication studies. One study utilized ESM to capture participants' perceptions towards unlocking behaviors, revealing reasonings behind leaving a phone unlocked [28] . Another similar self-reporting methodology, diary studies, has been used in recent research. One diary study on the cost of password policies had 32 staff members record 196 password events over one week [34] . Another study asked participants to record password events when they log into their accounts using desktop computers or laptops [31] . A diary study showed that authentication tasks lowered the productivity of employees in an organization [49] ."], "rq": ["INTRODUCTIONHowever, most work on gesture passwords so far has been carried out in laboratories [51, 48, 16, 26] , leaving their performance in the wild as an open research question. Field studies are important for understanding the user-chosen distribution of gesture passwords in realistic settings and how usable and memorable those could be."]}
{"intro": ["Software architecture is not only the final product of the design efforts, but more and more is including the decision history that brought to the system as it is finally specified [1] . One of the intended goals underlying this trend is, together with designing the system architecture, preserving domainexperts' knowledge/reasoning, which is implicitly exploited to reach the design solution and would be lost if not recorded appropriately. However, in software architecture knowledge management is a challenge [2] , and recording the information about a certain decision making process is an open research question: on the one hand, the stored decision information shall be detailed enough to turn out as useful for future decision making scenarios, even in cross-application and crossdomain situations; on the other hand, the efforts for recording first, and for retrieving/analysing/maintaining past decision data later, shall be reduced to the minimum [3] ."], "relatedWork": [], "rq": ["I. INTRODUCTIONSoftware architecture is not only the final product of the design efforts, but more and more is including the decision history that brought to the system as it is finally specified [1] . One of the intended goals underlying this trend is, together with designing the system architecture, preserving domainexperts' knowledge/reasoning, which is implicitly exploited to reach the design solution and would be lost if not recorded appropriately. However, in software architecture knowledge management is a challenge [2] , and recording the information about a certain decision making process is an open research question: on the one hand, the stored decision information shall be detailed enough to turn out as useful for future decision making scenarios, even in cross-application and crossdomain situations; on the other hand, the efforts for recording first, and for retrieving/analysing/maintaining past decision data later, shall be reduced to the minimum [3] ."]}
{"intro": ["Various privacy enhancing technologies have been developed toward addressing the aforementioned privacy vs. data utilization dilemma. For instance, the client can use special encryption techniques such as SQL-aware encryption (e.g., [1] , [2] ) or searchable encryption with various security, efficiency and query functionality trade-offs (e.g., [3] , [4] , [5] , [6] , [7] , [8] , [9] , [10] ) to achieve the data confidentiality and usability on the cloud. However, even such encryption techniques might not be sufficient for privacycritical database applications (e.g., healthcare) since sensi- \u2022 Work done when the second, the third and the fourth authors were employed at Oregon State University. E-mail: {ozkaptac, hackebeg, attila.yavuz}@oregonstate.edu."], "relatedWork": [], "rq": ["Limitations of Existing Approaches\u2022 Limitations of Cell-Oriented Approach: Another approach is to package each cell of the database table into an ORAM block. This approach increases the size of position map, which is an imperative component stored at the client in tree-based ORAMs. To eliminate the position map, oblivious 2D-grid structure (referred to as ODS-2D) [23] can be used to store the database table by clustering each O(log(N )) cells into an ORAM block and using the pointer trick to link the blocks together. However, this approach may increase the number of requests when the query requires fetching an entire row or column. This incurs end-to-end delay due to a large number of round-trip delays, and therefore, is not suitable for large databases. Cell-oriented packaging and its limitations are summarized in Figure 2 -(b). The above discussion indicates that there is a significant need for an efficient oblivious data structure that permits diverse types of queries on encrypted databases. Hence, in this paper, we seek answers to the following research questions:"]}
{"intro": [], "relatedWork": [], "rq": ["RQ1: Do PWs Generate Educationally Relevant Discussions?The educational benefits of PWs rest in their ability to stimulate discussions in which students' user interface designs are used as a basis for exploring the design principles and concepts taught in the course. One indication of whether PWs succeeded in this regard can be found in the results of the content analysis: On average, 31% of all PW talk was dedicated to DESIGN TALK. Given that DESIGN TALK was rooted in the user interfaces under review, we see that USER INTERFACE TALK, which constituted 26% of overall talk on average, necessarily served a key complementary role in such discussions. Taken together, DESIGN TALK and USER INTERFACE TALK composed a majority (53%) of all talk-a strong preliminary indication that PW discussions were educationally relevant. Delving deeper into the results, we find that 11% of DESIGN TALK (3% of all talk) actually enlisted the design concepts and principles explored in the course. Given that such concepts and principles were a key emphasis of the course within which the PWs were situated, one might be concerned that such a small percentage of DESIGN TALK 26:28 C. D. Hundhausen et al. was actually dedicated to them. However, the PW thrusts participants into a situation of design practice, where decision making is not based chiefly on research-based theory [Sch\u00f6n 1990 ]. Indeed, as Sch\u00f6n [1990] , aptly notes, design practitioners engage in a reflective conversation with their design materials, drawing extensively on their personal \"repertoire[s] of themes and examples\" (pp. 78-79) to make progress. That the design discussions we observed in the PWs contained a mix of theory, practical considerations, and common sense may not only reflect the realities of design practice, but also the authenticity of the design situations considered in the PWs.", "RQ2: To What Degree Do Students Participate?The results indicate that the extent to which participants contributed DESIGN TALK varied by speaker type. The instructor contributed the most DESIGN TALK of any other speaker type (44%), followed by student design team members (26%), the test user (17%), and audience members (13%). We believe that this finding, when viewed through the lens of situated learning theory [Lave and Wenger 1991] , indicates that the PW facilitated opportunities for students to participate in increasingly central ways in design discussions as they took on different roles. At the periphery of the PW activity were student audience members, who contributed the least to the discussions. In this role, students mainly observed the activity. However, when they did contribute, their contributions were most likely to be on the topics that were most relevant to the course: DESIGN TALK and USER INTERFACE TALK. We speculate that, in their roles as somewhat detached observers, audience members were in a good position to focus and reflect on user interface and design issues, without being distracted by the procedural details of the activity."]}
{"intro": [], "relatedWork": [], "rq": ["Collaborative Human-Robot RelationshipsResearchers developed collaborative robots for H-R teams and have improved the quality of a robot's ability to collaborate as people do (Briggs & Scheutz, 2011; Chernova & Breazeal, 2010; St. Clair & Matari\u0107, 2011 ). These developments have improved H-R collaboration but do not offer insights into the impact of collaboration on the human's performance or internal state. Hinds, Roberts, and Jones (2004) assessed human feelings of personal responsibility, blame, and credit in H-R collaboration. The authors teamed participants with either a human, a humanlike robot, or a machine-like robot in a subordinate, peer, or supervisor role. Results showed that the participants relied on human teammates more than robot teammates. Participants also relied more on robot peers than supervisors or subordinates. The difference in task performance or workload levels between the H-H and H-R teams, given the different relationship types, were not investigated; however, this research question is addressed in the presented investigation."]}
{"intro": ["Because shoutcasters explain in parallel to gathering their information, we guided part of our investigation using Information Foraging Theory (IFT) [29] , which explains how people go about their information seeking activities. It is based on naturalistic predator-prey models, in which the predator (shoutcaster) searches patches (parts of the information environment) to find prey (evidence of players' decision process) by following the cues (signposts in the environment that seem to point toward prey) based on their scent (predator's guess at how related to the prey a cue is). IFT constructs have been used to explain and predict people's information-seeking behavior in several domains, such as understanding navigations through web sites or programming and software engineering environments [5, 8, 9, 18, 23, 26, 27, 28, 33] . However, to our knowledge, it has not been used before to investigate explaining RTS environments like StarCraft."], "relatedWork": ["Constructing effective explanations of AI is not straightforward, especially when the underlying AI system is complex. Both Kulesza et al. [16] and Guestrin et al. [30] point to a potential trade-off between faithfulness and interpretability in explanation. The latter group developed an algorithm that can explain (in a \"black box\" or \"model-agnostic\" fashion) predictions of any classifier in a faithful way, and also approximate it locally with an interpretable model. They described a fidelity-interpretability trade-off, in which making an explanation more faithful was likely to reduce its interpretability, and vice versa. However, humans manage this trade-off by accounting for many factors, such as the audience's current situation, their background, amount of time available, etc. One goal of the current study is to understand how expert human explainers, like our shoutcasters, manage this trade-off."], "rq": ["RQ1 Results: What information do shoutcasters seek to generate explanations, and where do they find it?We used two frameworks to investigate casters' information seeking behaviors. We turned to the Performance, Environ- ment, Actuators, Sensors (PEAS) model [31] to situate what information casters sought in a common framework for conceptualizing intelligent agents. We drew from Information Foraging Theory (IFT) to understand where casters did their information seeking, beginning with the places their desired information could be found. These places are called information \"patches\" in IFT terminology. Table 2 columns 1 and 2 show the correspondence between PEAS constructs and patches in the game that the casters in our data actually used. Performance measures showed assets, resources, successes, and failures, e.g., Figure 1 region 4 (showing that Blue has killed 9 of Red's workers) and region 5 (showing that Blue has killed 19 units to Red's 3, etc.). Table 2 shows that casters rarely consulted performance measures, especially those that examined past game states. However, they discussed basic performance measures available in the HUD (Figure 1 region 1) , which contained present state information, e.g., resources held or upgrade status.", "RQ2 Results: The How: How do shoutcasters seek the information they seek?Information Foraging Theory (IFT) explains why people (information predators) leave one patch to move to another, such when the casters left Actuator patches. According to IFT, predators choose navigations as cost/benefit decisions, based on the value of information in the patch a predator is already in Actuators Environment Performance Sensors Cue: ? Goal: assess scouting Cue: Units separating, fighting likely over Cue: Units co-located, impending combat likely Figure 2 . The A-E-P+S loop was a common information foraging strategy some casters used in foraging for agent behavior. It starts at the Actuators, and returns there throughout the foraging process. If a caster interrupted the loop, they usually did so to return to the Actuators. versus the value per cost of going to another patch [29] . Staying in the same patch is generally the least expensive, but when there is less value to be gained by staying versus moving to another patch, the predator moves to the other patch. However, the predator is not omniscient: decisions are based upon the predator's perception of the cost and value that other patches will actually deliver. They form these perceptions from both their prior experience with different patch types [27] and from the cues (signposts in their information environment) that point toward content available in other patches.", "RQ2 Results: The How: How do shoutcasters seek the information they seek?Interestingly, this cue type was different from the static cues most prior IFT research has used. Cues tended to be static decorations (text or occasionally images) in previous IFT investigations that label a navigation device, like a hyperlink or button that leads to another information patch. In contrast, cues like the onset of combat are dynamic and often did not provide an affordable direct navigation. However, cues like this were considered cues because they \"provide users with concise information about content that is not immediately available\" [29] . They suggested high value in another location -in the case of combat, the Units tab.", "RQ2 Results: The How: How do shoutcasters seek the information they seek?These Performance measures gave the shoutcasters at-a-glance information about the ways one player was winning. The most commonly used tab, for example, the Units Lost tab (Figure 3 ), showed the number of units lost and their total value, in terms of resources spent. This measure achieves \"at a glance\" by aggregating all the data samples together by taking a sum; derived values like this allow the visualization to scale to large data sets [32] . However, Table 2 indicates that the lower data aggregation patches were more heavily used. The casters used the Production tab to see units grouped by type, as Figure 4 shows, so type information was maintained with only positional data lost. This contrasts with the Minimap (medium aggregation), in which type information is discarded but positional information maintained at a lower granularity. The casters used Performance measure patches primarily to understand present state data (HUD), but these patches were also the only way to access past state information ( Table 2) ."]}
{"intro": ["Car manufacturers are also seeking to explore new roles for the car. Increasingly they are considering car interiors as interactive spaces. For example, Toyota in collaboration with the Copenhagen Institute of Interaction Design (CIID) have illustrated concept designs for interactive car windows that enable passengers in a motor vehicle to interact with the world around them [38] . General Motors have also explored the possibilities of the car window [31] . Their concept, 'The Windows of Opportunity,' presented designs for an interactive window developed specifically for rear passengers. They also introduced the concept of car window 'apps' including an animated agent, finger drawing and a music player. Daimler's concept car, the Mercedes-Benz F 015 Luxury in Motion, explores the possibilities of the self-driving car [1] . With the arrival of self-driving technology [5] , the passenger experience is likely to fundamentally change. People will likely have time to do things other than driving during their journeys. Daimler have reconceptualised the motor car as a private retreat and a mobile living space. These three concepts for future cars all variously utilise the car windows as an interactive surface, postulating that this might be used to interact with elements of the external environment. However, the user experience of such interactive systems for passengers has not been well studied, and thus there is a need to better understand the design and use of these technologies as they are emerging."], "relatedWork": ["Passengering Support. As automobiles are so imperative to our life, there have been a number of studies of in-car interfaces and interactions. For example [21, 35] who specifically focused on how to enhance in-car experience for drivers whilst ameliorating the risks of driving. However, the advent of self-driving technology makes driving easier and will give a driver opportunities to do things other than driving during the journey. This implies a need to explore passengering support in the car."], "rq": ["Motivation for the studyThe above four examples are extremely mundane and commonplace interactions which might occur in-car, and yet they deserve our consideration as moments of interaction in which digital technology might offer support. Such examples motivated us to design our in-car interactive system. Obviously, we see an external environment through a car window or a windshield, but we wondered how we might reimagine that moment of interaction through the window. Fig. 1 (above) shows a concept sketch of an interactive car window. Using the window, passengers can interact with the external environment. The system allows passengers to freeze (i.e., pause/stop) the scene and rewind the outside view by buffering scenes for several seconds. The system expands the idea of Google Street View [10] with an intuitive user-interface and passenger supporting functions. This simple idea seemed promising, however, it was questionable without an underpinning theoretical framework and some understanding of how users would actually interact with and respond to such a system. We thus defined and began to explore the research question given above."]}
{"intro": ["CBA is a decision-making system developed by Jim Suhr that is based on four principles: (1) Decision-makers must learn and skillfully use sound methods, (2) Decisions must be based on the importance of advantages, (3) Decisions must be anchored to relevant facts, and (4) Different decisions call for different methods. This paper will focus on the first principle (Suhr 1999) . CBA has gained more attention in the construction industry in recent years. This increase has been driven by demands for more collaborative project organizations and transparent decision-making processes; by the synergy of CBA with other agendas such as improving sustainability and safety; and by an increasing need to incorporate multiple factors into the decision-making process. However, as the construction industry simultaneously prioritizes delivery logics confined by tight schedules and budgets there is a competing desire to maximize efficiency and timeliness in training processes. Decision-makers seek training protocols with minimal disruption of delivery, even at the expense of quality in education. It is therefore crucial for decision-makers and coaches to think critically about how to provide the most effective training based on the financial resources of the project. This paper compares the experiences of four CBA coaches working separately in different countries and analyzes the skill development of the teams based on the methods of training they received. The primary research question is: What are the benefits and shortcomings of each of the CBA training methods employed by the coaches studied in this paper? This will be evaluated by comparing different styles of training with the learner's subsequent ability to implement CBA. First, this paper will present relevant literature on human learning to contextualize the need for a range of training options. The research methods used to observe more than 30 CBA trainings will be explained and the accompanying data presented. Finally, the outcomes will be discussed, and conclusions drawn regarding how this research can facilitate other coaches and decision-makers in the industry in personalizing CBA trainings to every unique audience."], "relatedWork": [], "rq": ["INTRODUCTIONCBA is a decision-making system developed by Jim Suhr that is based on four principles: (1) Decision-makers must learn and skillfully use sound methods, (2) Decisions must be based on the importance of advantages, (3) Decisions must be anchored to relevant facts, and (4) Different decisions call for different methods. This paper will focus on the first principle (Suhr 1999) . CBA has gained more attention in the construction industry in recent years. This increase has been driven by demands for more collaborative project organizations and transparent decision-making processes; by the synergy of CBA with other agendas such as improving sustainability and safety; and by an increasing need to incorporate multiple factors into the decision-making process. However, as the construction industry simultaneously prioritizes delivery logics confined by tight schedules and budgets there is a competing desire to maximize efficiency and timeliness in training processes. Decision-makers seek training protocols with minimal disruption of delivery, even at the expense of quality in education. It is therefore crucial for decision-makers and coaches to think critically about how to provide the most effective training based on the financial resources of the project. This paper compares the experiences of four CBA coaches working separately in different countries and analyzes the skill development of the teams based on the methods of training they received. The primary research question is: What are the benefits and shortcomings of each of the CBA training methods employed by the coaches studied in this paper? This will be evaluated by comparing different styles of training with the learner's subsequent ability to implement CBA. First, this paper will present relevant literature on human learning to contextualize the need for a range of training options. The research methods used to observe more than 30 CBA trainings will be explained and the accompanying data presented. Finally, the outcomes will be discussed, and conclusions drawn regarding how this research can facilitate other coaches and decision-makers in the industry in personalizing CBA trainings to every unique audience."]}
{"intro": ["The design of electronic patient records is a huge challenge for the HCI community, raising a wide range of still unanswered questions related to issues such as screen layout, interaction design, and integration into work processes. Where should the systems be located and who should enter the data? How do we make sure that input is complete and accurate? How are the different work processes in healthcare structured and coordinated? What is the most useful way of displaying and accessing the vast quantity of patient data? [4] . In the light of these questions, a lot of research has been published in the HCI literature about EPR systems and how to meet challenges related to design and use of computer system in healthcare. Specifically, much attention has been given to issues such as information sharing [5] , support for cooperation [6] and privacy [12] . While much of this research is based on studies on the use of traditional paper-based patient records, suggesting viable electronic counterparts, however, little research has been published based on studies that inquire into the use of the mass of EPR systems already in use.", "In Denmark, there is currently an extensive focus on electronic patient records. The government has decided that by 2005 all Danish hospitals must have replaced the traditional paper-based patient records with electronic ones. However, it is up the regional authorities to decide on the details of deployment. Thus a number of pilot projects are currently in progress with the aim of developing and evaluating electronic patient record systems (see e.g. [2] ). In relation to a regional Danish research program entitled \"The Digital Hospital\" we have studied the use of a commercial EPR system currently in use at a large hospital (IBM IPJ 2.3). In addition to this, an experimental mobile EPR prototype extending the current system's functionality was designed and evaluated. Driving this study, we were concerned with the following research questions: 1) what challenges characterize the use of contemporary electronic patient record systems? 2) How can these challenges be met by improved interaction design?"], "relatedWork": [], "rq": ["IntroductionIn Denmark, there is currently an extensive focus on electronic patient records. The government has decided that by 2005 all Danish hospitals must have replaced the traditional paper-based patient records with electronic ones. However, it is up the regional authorities to decide on the details of deployment. Thus a number of pilot projects are currently in progress with the aim of developing and evaluating electronic patient record systems (see e.g. [2] ). In relation to a regional Danish research program entitled \"The Digital Hospital\" we have studied the use of a commercial EPR system currently in use at a large hospital (IBM IPJ 2.3). In addition to this, an experimental mobile EPR prototype extending the current system's functionality was designed and evaluated. Driving this study, we were concerned with the following research questions: 1) what challenges characterize the use of contemporary electronic patient record systems? 2) How can these challenges be met by improved interaction design?"]}
{"intro": [], "relatedWork": [], "rq": ["STATE OF THE ARTSeveral descriptive models seek to capture buying behaviors. Current literature reveals that there are six fundamental stages within the purchase decision making process [12] : -First, users become aware of a new need. This realization can result from companies' prospecting campaigns or the recommendation of a new product from friends. -Users will then determine from whom to buy, a process cal-led merchant brokering. In online environments, consumers can use a price comparison website to determine where to buy their goods. In this case, everything that customers experience becomes an essential building block of a rapport between a buyer and a seller. Users are likely to look for website qualities that promote trust, ease of navigation, and strong relevance of items recommended to them [12] . -In the meantime, they evaluate product alternatives in order to make the final choice. At this stage, called product brokering, interactions help recommender systems to understand their needs and present personalized options to them. -Stages 4 and 5 consist of negotiation and purchasing. The seller has to provide security and confidence in order to close the sale. -Finally, users' satisfaction in relation to the overall buying experience can be measured in a sixth stage, if post-purchase product service is involved. In this paper, we will focus on the product brokering stage where consumers evaluate product alternatives in order to make the final choice. At this stage, tracking users' interactions can help a recommender system understand their needs and present personalized options to them. According to Haubl et al., the product brokering stage can be divided into two steps [4] . During the first step, the active user identifies a subset of products to compare. During the second step, the different features and details of these products are compared in order to make a decision. Haubl also proved that the use of a recommender system leads to a reduction in the number of alternatives considered seriously for purchase [5] , and that a recommendation agent increases the number of non-dominated alternatives -i.e. not objectively inferior to any alternative [15] in the set of alternatives seriously considered for purchase. Based on such research, it is apparent that recommenders prove to be a useful tool to users, assuming that they provide items relevant to users' needs. However, the goal of personalization is not only to provide the right item to the right person, but also right away and at the right time. The time constraint has long been overlooked by researchers. In this paper, we aim to analyze both the impact of recommenders over time at the product brokering stage, thus extending the findings of [4] ), and the factors that influence the users. The research questions with regards to the setup of a recommender focus less on what to suggest, but rather when and why."]}
{"intro": [], "relatedWork": [], "rq": ["TokenizationTokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997) . However, different research questions or applications induce different conceptions of the term 'word'. For a shallow morphosyntactic analysis (part of speech tagging), a 'simple' tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each."]}
{"intro": [], "relatedWork": [], "rq": ["DISCUSSION: CRAFTING RESEARCH PRODUCTSResearch product qualities are also not scalar. Qualities are not a measure of magnitude or to what degree along the continuum of finish, for example, a research product achieves. As we learned in the case of the hook, a research product is either experienced as an artifact with a high degree of finish or it is not. There is always room for refinement; however, it became clear that each quality must be achieved and be present simultaneously. This is a fundamental difference with prototypes where, a design researcher can choose to emphasize (or 'filter out' [31] ) one aspect of the prototype at the expense of others, such as when a technical prototype is created to establish the technical possibilities with no regard for user experience [25] . In this way, prototype qualities are scalar. For example, a common strategy in prototyping is to iteratively advance the degree of finish and fidelity through a series of artifacts. This condition poses real challenges for designers of research products. Typically in design, questions of the use situation are asked through the prototyping process and the 'answer' is presented in a finished product. In designing a research product, there is the typical iteration and prototyping in trying to best formulate and carry research questions through an artifact. As a result, the finished research product depicts the design research team's 'best' articulation of how to ask and pursue the research question at that given time."]}
{"intro": ["In parallel, researchers from the field of education have studied the use of collaborative writing for learning. Collaborative group work is increasingly popular in classrooms as the enhanced sharing features available in cloud-based technology supports effective instruction. Drawing from sociocultural theories of learning, research has found that collaborative writing enhances writing quality [31] , sense of audience [29] , the pooling of knowledge and ideas [9] , and opportunities to socialize with specific discourse communities [36] . Most of the previous studies examined the practices of collaborative writing in an asynchronous mode. For example, Kessler and Bikowski [16] focused on asynchronous feedback and commenting practices and found that collaboration typically occurred at the later stage of writing, rather than throughout the writing process. However, little is known about the ways students collaborate when they can write in a synchronous, collocated environment, such as with Google Docs, and how various aspects of such collaboration (e.g., number of coauthors, participation equity) may relate to a document's quality and quantity."], "relatedWork": [], "rq": ["RQ2:First, we are interested in how student groups write in the synchronous mode (i.e., practices), and what characterizes the different practices. We would intuitively expect that the ability to write simultaneously together in one location might manifest new practices of collaborative writing. However, it is also plausible that students might maintain the old ways of working together and might not utilize the affordances of technology or take advantage of the work environment. In this context, the students might decide to take turns to write in a document (Sequential Writing in Posner & Baecker's taxonomy [26] ), or delegate one student to write the whole document while others only provide comments and ideas (Scribe in [26] ). In addition, we examined how the writing styles differ in terms of collaboration behaviors (e.g., participation equality, group activeness), text quality, and quantity."]}
{"intro": [], "relatedWork": [], "rq": ["DOMINANT HYPERPARAMETER\u2022 Is there one or more hyper-parameters that afect more the accuracy of recommendations? \u2022 Could be established a procedure to check if diferent values for a specifc hyper-parameter can lead to signifcant diferences? To answer these research questions, we analysed the three hyperparameters of BPR-MF separately. In details, for a certain parameter, we want to defne a procedure to check if diferent values of that hyper-parameter lead to systems which show signifcant diferences in accuracy of recommendation. Let us suppose we fx the metric and the number of latent factors: we still have two other parameters that can vary. We computed all the possible combinations of the remaining hyper-parameters. From the set of combinations, we randomly chose 25 pairs of combinations. We recall that a pair of combinations corresponds to a pair of systems that share the number of latent factors, and difer in the number of iterations and learning rate. Now we compute the p-values of all these pairs and we order them by decreasing value. These values correspond to a pvalues curve which is peculiar for the considered metric and number of latent factors. Consequently, for the curve, the corresponding Discriminative Power can be computed. This procedure can be repeated to analyse the discriminative power of various values of latent factors parameter. Thus, the whole procedure can be repeated to analyse the remaining hyper-parameters. The results of this study is depicted in Tables 5. We used bold to highlight the best DP value for each hyper-parameter analysis. As suggested in [44] , the results between the two tables are not comparable. However, for both datasets, the number of latent factors seems to be the dimension on which variations in hyper-parameter value lead to signifcant diferences in recommendation accuracy. Moreover, for Movielens dataset, the DP value is much lower than the best values for \"Number of iterations\" and \"Learning rate\" hyper-parameter analysis. This suggests that \"Number of latent factors\" is dominant with respect to the other hyper-parameters. \"Learning rate\" dimension shows a diferent behaviour on the two datasets: on Movielens it shows big variations in terms of DP values, while on Amazon it shows oscillating performance. Finally, DP values confrm that conducting the study on the sub-grid was a reasonable choice."]}
{"intro": [], "relatedWork": [], "rq": ["Reinforcement Learning ApplicationsIn summary, most previous Reinforcement Learning approaches based on social signals in Human-Robot Interaction utilize a combination of implicit and explicit feedback. However, for a robot to acquire their human counterparts' sense of humor in an unobtrusive manner, the robot would need to learn to be humorous solely by learning from implicit feedback [19] . In this paper, we aim to address this open research question by exploring whether implicit feedback alone suffices to enable a robot to learn humor preferences."]}
{"intro": ["A large body of this research is rooted in the dependent Dirichlet process (DDP) (MacEachern 1999) where the probabilistic random measure is defined as a function of covariates. Most DDP approaches rely on the generalization of Sethuraman's stick breaking representation of DP (Sethuraman 1991) , incorporating the time difference between two or more data points, the spatial difference among observed data, or the ordering of the data points into the predictor dependent stick breaking process (Duan et al. 2007; Dunson and Park 2008; Griffin and Steel 2006) . Some of these priors can be integrated into the hierarchical construction of DP (Srebro and Roweis 2005) , resulting in topic models where temporally-or spatially-proximate data are more likely to be clustered. These existing DP approaches, however, cannot be easily extended to model underlying topics of a document collection. One reason is that the extension requires to develop a new tractable inference algorithm from models with intractable posterior distributions.", "The HDSP models the topic proportions of a document as a dependent variable of observable side information. This modeling approach differs from the traditional definition of a generative process where the observable variables are generated from a latent variable or parameter. For example, Zhu et al. (2009) and Mcauliffe and Blei (2007) propose generative processes where the observable labels are generated from a topic proportion of a document. However, a more natural model of the human writing process is to decide what to write about (e.g., categories) before writing the content of a document. This same approach is also successfully demonstrated in Mimno and McCallum (2012) ."], "relatedWork": [], "rq": ["DiscussionsHierarchical Dirichlet scaling process opens up a number of interesting research questions that should be addressed in future work. First, in the two scaling functions we proposed to model the correlation structure between topics and side information, we simply defined the relationship between topic k and label j through the scaling parameter w k j . However, this approach does not consider the correlation within topics and labels. Taking inspiration from previous work (Blei and Lafferty 2007; Mimno et al. 2007; Paisley et al. 2012 ) that showed correlations among topics, we can define a scaling function with a prior over the topics and labels to capture their complex relationships. Second, our posterior inference algorithm based on mean-field variational inference is tested with tens of thousands documents. However, modern data analysis requires inference of massive and/or streaming data. For a fast and efficient posterior inference, we can apply parallel or distributed algorithms based on a stochastic update (Hoffman et al. 2013; Ahn et al. 2014 ). Furthermore, we fix the number of labels before training but we need to find a way to model the unbounded number of labels for streaming data. "]}
{"intro": [], "relatedWork": [], "rq": ["There are two basic strengths of the AGL paradigm for use in the language sciences. First, it is used as a model system to study aspects of natural language processing; for example, syntactic or phonological processing, in isolation from semantic influence. Since an artificial language is novel to all participants, prior learning is controlled. AGL has been most widely used as a model system for syntax, but work related to phonology has also appeared (Tessier, 2007) . The second strength is the possibility to study a wide range of populations using identical, or at least comparable, paradigms. Populations have ranged from prelinguistic infants to adults, as well as non-human primates and songbirds (a review on comparative animal studies using AGL is, however, outside the focus of this chapter). Comparisons allow contributions to research questions on language acquisition and evolution. These two strengths of the paradigm will be illustrated throughout this chapter, where we will review the state-of-the-art in AGL research, including research with infants, with a particular focus on neuroimaging work. A main limitation of the paradigm, namely the constraints on generalization to natural languages, will also be addressed, through clarifications of some of the main differences between AGL and natural language research."]}
{"intro": ["Recommender systems aim to help people find items of interest from a large pool of potentially interesting items. However, when receiving these recommendations not all users are equally satisfied. One reason for this is, e.g. the choice of the recommendation algorithm. However, even when we account for this aspect, some users may receive better recommendations than others. Previous research has analysed this issue and characterised it as a matter of user inconsistency, that is, users have an inherent noise when interacting with the recommender system, which then affects the reliability of the recommendations produced. This concept is know as the magic barrier of recommender systems, a term coined by Herlocker et al. [9] , referring to the upper bound on rating prediction accuracy: above it any further improvements on the evaluation metrics are meaningless [1, 18] ."], "relatedWork": [], "rq": ["User Coherence and Magic BarrierIn this experiment, we assess the validity of the proposed coherence functions as good predictors for the magic barrier to answer the research question RQ1. With this goal in mind, we show in Table 4 the Spearman's correlation values between the coherence and the magic barrier per user (Pearson's correlation was very similar). Note that Pearson's correlation coefficient is designed to capture linear relationships between the two variables whereas Spearman's captures non-linear dependencies. Both correlations provide scores in the range of \u22121 to 1, where 1 denotes a perfect correlation, \u22121 represents an inverse correlation, and the absolute value is the strength of the relationship. We observe in Table 4 that the correlations for the weighted version of the coherence function (that is, where the importance of each feature in the user profile is ignored) show more predictive power only when the standard deviation is used. Besides, entropy and KLD do not perform very well. Additionally, Emotion keywords and Intended Audience seem to be the best feature spaces for most of the coherence formulations, and especially, for the cases where a strong correlation is obtained. We have to however note that these feature spaces offer a low coverage in terms of the items identified with these features [20] , thus this aspect should also be taken into account when selecting the feature to use."]}
{"intro": ["However, hand-held visual interfaces may suffer from issues such as screen reflections interfering with content visibility [16, 37] and require visual attention on the device, which results in sharing cognitive resources between the device and the environment [6, 26] . This may disconnect the user from the surroundings and hamper the user experience [3] . A remedy for these issues can be provided by headmounted displays or non-visual interfaces.", "Several approaches have been proposed making use of the auditory and/or haptic modalities to enhance the user experience, to enable the use of the system in situations when visual attention cannot be on the device, and to cater also for visual disabilities. However, the vast majority of these systems have been designed for navigating a defined route or finding a specific place, thus limiting their use for truly exploring the city, which is also a relevant mode of sightseeing [5, 12] . This leads to our primary research question: how to design a non-visual interaction technique that supports exploratory, serendipitous discovery of POIs (see Fig. 1 for a conceptual illustration of the setting.)."], "relatedWork": [], "rq": ["INTRODUCTIONSeveral approaches have been proposed making use of the auditory and/or haptic modalities to enhance the user experience, to enable the use of the system in situations when visual attention cannot be on the device, and to cater also for visual disabilities. However, the vast majority of these systems have been designed for navigating a defined route or finding a specific place, thus limiting their use for truly exploring the city, which is also a relevant mode of sightseeing [5, 12] . This leads to our primary research question: how to design a non-visual interaction technique that supports exploratory, serendipitous discovery of POIs (see Fig. 1 for a conceptual illustration of the setting.)."]}
{"intro": ["To tackle such limitations and make interaction design simple, low cost, and intuitive, we probe three modalities: color, sound, and vibration. Previous studies have shown their impact on a person's perception [21, 36, 34] . However, few papers have comprehensively evaluated the effect of these modalities in scenarios involving affective communication with a social robot. Thus, this leads our research question as to how the three modalities affect a human's emotional perceptions through expressions."], "relatedWork": ["Affective communication for social robots has been discussed intensively in recent years. Various robots are now designed, from humanoids to androids, to be able to establish interactions with humans [3, 1, 16]. They have been intensively studied in various scenarios such as education [35, 14] , autism therapy [5, 11] , guidance [18] , and driving support [17, 25] . For such social robots, natural language has been considered to be a significant interactive modality. However, the current state-of-the-art in natural language and related technologies is still far from satisfying [37] . Previous research has shown that over 80% of human communication is encoded in facial expressions and body movements [32] . Hence, non-verbal cues are viewed as essential affective communication methods [7] . For instance, facial expressions have been a popular mechanism for showing affection with robots as well as body movement [9, 8, 24, 31] , posture [9, 8, 24, 31] , and orientation [9, 8, 10, 28] . Although such interaction methods are natural and effective, they are limited by the embodiment of robots. As many currently-in-use robots are appearance-constrained, they do not have the abilities to provide social cues through modalities such as facial expression, gesture, and gaze [7, 6] . Thus, it is important to explore other interactive modalities that are simple, low cost, but intuitive.", "Three alternative modalities, namely color, sound, and vibration, have also been investigated particularly in HCI and psychology. For instance, a number of studies on the role of color and light [23, 22, 26, 27, 34, 15] have been carried out; a handful of studies, particularly on semantic-free utterances (SFU), have explored the design of sounds that allow emotion and intent expressions with machines [19, 21, 20, 37] ; Vibration feedback has mostly been studied as an auxiliary means to support the communication of emotions [30, 36, 4] . However, no previous work discussed affective interaction through combinations of the three modalities. Such multi-modal approaches are important since there are currently no sound principles for expressing particular emotions though single modalities."], "rq": ["INTRODUCTIONTo tackle such limitations and make interaction design simple, low cost, and intuitive, we probe three modalities: color, sound, and vibration. Previous studies have shown their impact on a person's perception [21, 36, 34] . However, few papers have comprehensively evaluated the effect of these modalities in scenarios involving affective communication with a social robot. Thus, this leads our research question as to how the three modalities affect a human's emotional perceptions through expressions."]}
{"intro": ["Video games and their virtual worlds are great sandboxes for the investigation of human behaviour. How people play a digital game can reveal things about their character, their personal attitudes, and even their motivations in life. Curiosity is a form of intrinsic motivation that plays a central role in many aspects of human behaviour. Beyond that, curiosity is a concept frequently used by game designers, because it drives player engagement and keeps people deeply involved with a game. For instance, in 2012 renowned game designer Peter Molyneux designed an experimental game [G1] which specifically revolved around the concept of curiosity. Players had to collectively dig through layers of small cubes, to discover the 'story' surrounding the giant cube that contained them. Early research into digital games indicated the possibility of constructing personality profiles from observations of a player's in-game behaviour [50, 54, 55] . However, the exact role different facets of curiosity play in digital game behaviour has not yet been studied, even though the satisfaction of appetitive motives such as curiosity is an integral component of player engagement [23, 26] . More broadly, curiosity has been identified as a motivational factor drawing people into playing today's most successful online games, such as the online shared-world shooter Destiny [G5]. In these online games, curiosity may manifest itself in many forms, including seeking information or experiences [63] , and social curiosity [56] about other players."], "relatedWork": [], "rq": ["RQ1: Toward Four Factors of CuriosityThe four self-report 'base' scales in our survey were developed to measure constructs other than curiosity (as discussed above). However, a closer look at the semantics of the survey items and their design objectives revealed that specific items in each of these scales do in fact interrogate constructs of curiosity. Following up on this observation, we examined whether it was possible to construct a multi-factor 'curiosity' scale using survey items from these 'base' scales' existing motivation and player types. For this purpose, we extracted the ten curiosity-related survey items from the SOCIAL CAPI-TAL, OBSESSIVE/HARMONIOUS PASSION, BEHAVIOURAL ACTIVATION, and BRAINHEX instruments, and performed an exploratory factor analysis (EFA) on these to examine their latent factor structures. In deriving a joint factor structure from item sets with different response scales, we follow a common methodology from personality psychology [1, 13, 61] . The precise wording of the items selected can be found in Table 1 , and we will direct the reader to the specific positions when discussing the individual item groups.", "RQ1: Toward Four Factors of CuriosityAll KMO values for individual items were > .7, which is well above the acceptable minimum of .5. Bartlett's test of sphericity, 2 (45) = 2, 757, p < .001, indicated that correlations between items were sufficiently large for EFA. We ran an initial principal component analysis (PCA) to obtain eigenvalues for each component in the data. Four components had eigenvalues above Kaiser's criterion of 1.0 and in combination explained 71% of the variance. Given the large sample size, and the convergence of the scree plot and Kaiser's criterion on four components, we retained these four factors (F1 to F4) in our final analysis. Table 1 shows the factor loadings after rotation. The oblique rotations oblimin and promax yielded the same factor structure as had been extracted using the orthogonal varimax rotation, indicating a stable pattern of four largely independent factors. Factor F1 had a high reliability, Cronbach's \u21b5 = .86, whereas factors F2 and F3 had moderate reliabilities with \u21b5 values of .67 and .61 respectively. Factor F4 had relatively low reliability, \u21b5 = .50. However, Kline acknowledges that, for psychological constructs like curiosity, Cronbach's \u21b5 can, realistically, be expected to be below .7 because of the diversity of the constructs being measured [20] . That said, our findings with respect to F4 should be interpreted with caution.", "RQ2: Correlating Self-Reports and Behavioural DataDestiny tracks thousands of behavioural features about individual players. However, in any work attempting to correlate in-game behaviour with psychological factors, an initial challenge lies in isolating those behaviours with the most predictive potential [2, 48] . There are two approaches to such an analysis: either a 'bottom-up', exploratory approach where high numbers of variables are correlated with the selfreport scores, or a 'top-down' approach where hypotheses are used to define which behaviours to work with. While the exploratory approach can result in over-featuring and false positives, the top-down approach is exclusive, ignoring behaviours not included in the hypotheses. Hence, in this study, Figure 1 . The four curiosity factors and their correlated game behaviours emerging from the Destiny dataset (reprinted from [43] ). a combined approach was used, where an initial set of behavioural metrics was selected based on related work and theory, and these metrics were then used to formulate a series of hypothesized correlations of curiosity factors. The correlations were then inspected in a range of related behaviours in Destiny that fit the curiosity factors. Furthermore, we included data from both gameplay options in Destiny (e.g., PvE and PvP play), and from across the spectrum of activities in the game, including rates of objective completion, performance, and exploration."]}
{"intro": [], "relatedWork": [], "rq": ["\uf0b7 RQ1: Does cooperative fitness tracking between co-workers improve the level of physical activity? For cooperative fitness tracking, besides the social effects of peer bonding, we observe that the proximity between the co-workers in the office environment may also affect its outcome. In the CSCW community, the effects of distance have been extensively concerned with workplace technologies and many social practices [4, 49] . For instance, an experiment by Bradner and Mark [5] showed that the likelihood of cooperation through communication technologies could be abandoned due to the increase of interpersonal distance. Similarly, Cummings and Kiesler [18] found that greater distance between team members could lead to lower cooperative performance. In our view, however, such classic CSCW narratives of distance effects and the way in which they influence technology-assisted workplace fitness promotion needs to be further explored. It is a worthy topic of study to understand the effects of physical proximity at work on cooperative fitness tracking in the office context. To this end, this study involves two types of co-workers, both those who are distributed and co-located (i.e. at the same site), to explore our second research question:"]}
{"intro": [], "relatedWork": ["Clickbaits can be thought as the digital successor to the tabloidization of print journalism [50] . Tabloids disrupted a long-held approach towards journalistic gatekeeping by focusing more on soft news than hard news, and on sensationalizing the content over the detailed truthful reporting of events. There have been concerns in the journalism community regarding the tabloidization of news and its potential threat to democracy [45, 50] . However, on the other hand, several studies have noted that softening of news by the tabloids helped raising political awareness among politically inattentive citizens [5, 17] ."], "rq": ["Automatic detection of clickbaitsHowever, the research questions we investigate in this work are complementary to the earlier work. For example, in [14] , we identified linguistic characteristics that differentiate clickbait and traditional news headlines. Whereas, in this paper, we explore complementary questions specific to tweets such as whether clickbait tweets contain several entities which might lead to their increased visiblity, or whether the sentiment conveyed by the clickbait tweets differ from the non-clickbait tweets. Moreover, taking a very different direction compared to [14] , we study the production and consumption patterns of clickbaits in Twitter, and bring out interesting insights."]}
{"intro": ["Taking gender transition as a case study of identity transition more broadly, what particular SNS practices add to or diminish stress as individuals change gender on SNSs? Gender transitions have already been shown to be stressful offline [29] . In this work, we demonstrate that they are also stressful online. However, simply knowing that something is stressful is not enough. We must understand how and why these transitions are stressful to effectively design technology and SNSs that counter stress while leveraging one's online social network for transition support. Designing such systems with considerations for life transition support can reduce stress for transgender people and others during periods of identity transition.", "CSCW has a rich history of scholarship critiquing the uneven politics of classification and categorization (e.g., [5, 42] ). Classification of gender and its relation to selfpresentation is a secondary theme of this paper and an important area for continued research. However, our findings show that addressing categorization issues alone is not enough. Disclosure of personal information about identity transitions on Facebook is a more fundamental concern for participants in this work. Therefore, in this paper, we focus primarily on five research questions related to differential identity disclosure, as outlined here:"], "relatedWork": ["However, SNSs can also be potentially harmful spaces during life transitions. The public, open nature of SNSs can place users at risk of harassment, which can complicate and even impede life transitions online [16, 36] . Communication with one's Facebook network can cause added stress during life transitions when friends offer unhelpful advice, and passive consumption of news feed content can decrease social support [11] . Additionally, SNSs open up a whole new set of \"digital possessions,\" such as photographs, messages, and even SNS profiles themselves, many of which must be sorted through and changed during a life transition [38] . By further examining the potentially negative aspects of using SNSs during life transitions, we seek to understand how to best support transition processes online, including ways to preserve privacy and allow for optimal network support.", "In particular, SNSs complicate life transitions due to complexities around disclosure of transition-related information and self-presentation online. People carefully curate their self-presentation depending on their intended audience [26] . Impression management [26] has always been an important topic in SNS research [7] , but is particularly relevant when considering complex and multiple identities that emerge during major life changes. On SNSs, people commonly present different information depending on the audience [6, 15, 23, 34, 35] . Maintaining multiple SNS profiles is a common practice, but is a burden and comes with the risk of unintended \"leakage\" between accounts [15] . Particularly for marginalized groups, having incompatible faceted identities may cause people to worry more about posting on Facebook, thus leading them to use email, a private platform, more often [23] . However, even with email, people maintain multiple accounts for use in different settings [28] . People use both mental strategies and account management behaviors on Facebook to \"divide the platform into separate spaces\" in order to manage disclosures among different groups of friends [32:288] . Customizing privacy settings, for instance creating lists of friends, is a common account management technique for disclosure, and has been shown to increase the amount of content shared on Facebook [40] . However, many Facebook users misinterpret privacy settings [1] , which can lead to information being shared with unintended audiences [34, 47] . Although users employ many strategies to manage disclosure on SNSs during life transitions, in this work we show that some of these behaviors are associated with increased transition-related stress."], "rq": ["INTRODUCTIONCSCW has a rich history of scholarship critiquing the uneven politics of classification and categorization (e.g., [5, 42] ). Classification of gender and its relation to selfpresentation is a secondary theme of this paper and an important area for continued research. However, our findings show that addressing categorization issues alone is not enough. Disclosure of personal information about identity transitions on Facebook is a more fundamental concern for participants in this work. Therefore, in this paper, we focus primarily on five research questions related to differential identity disclosure, as outlined here:"]}
{"intro": ["With the popularization of location-tracking applications, researchers have investigated how tracking practices between humans can affect the behavior, relationships and lives of members in tight-knit groups such as families [3, 18] . However, the use of tracking technology is rapidly extending to nonhuman family members, such as cats and dogs [9] , and pet owners can now choose between a wide variety of purposely designed GPS devices (e.g., Tagg [32], GlobalPetFinder [6] , SpotLight [31] , Retrieva [30] ). In spite of the fact that tracking pets is becoming a significant social trend, there has been very little research on this subject. In human-animal interaction research, only few studies have looked at the use of GPS devices during specific activities such as hunting [26, 33] . They have shown how tracking technology affords new interactional opportunities that affect the role of both humans and dogs during the hunt.", "Our research investigates the social significance of technologically mediated human-animal interactions. We are interested in how tracking devices for dogs are used within domestic contexts in the everyday management of human-canine relationships and care-taking practices; we are interested in how these practices influence the behavior of and change both human and canine family members. However, since we cannot communicate with dogs in the same way that we communicate with humans, this kind of research clearly raises methodological issues to do with the interpretation of human-dog manifest interaction (similar issues arise in studies with young children and adults with communication impairments [15] ). Exploring these issues is important for the development of the emerging areas of human-animal interaction [33] and animal-computer interaction [17] . In order to study technology-mediated human-animal interactions or to develop user-centered technology for animals, we need to question what these interactions and the technology that mediates them might mean for animals as well as humans. Therefore, our research questions how technology might acquire and convey meaning for both; we question how this meaning might be inferred by or communicated between the two, and how it might inform the way in which the two adapt to each other and coevolve; we also question how this coconstructive [9] meaning exchange could be accessed and understood by those researching the interconnections between humans, animals and technology."], "relatedWork": ["The human-dog relationship has a long history [27] , yields many benefits [21] , and plays an important role in society [24] . There are currently around 8 million pet dogs in the UK, with over 23% of households including at least one dog [29] . Owners are legally responsible for their dogs' welfare and behavior [4] , both of which imply always keeping track of them. Indeed, not being able to keep track of one's dog may have serious repercussions. Dogs often accompany their humans on outings or holidays, and 'walkies' are typically part of their daily routine. When outdoors, many owners favor letting their dogs off the lead, so they can properly exercise and express more natural behavior, both of which are important for their welfare and positive integration in the household. However, when off lead, dogs can be easily distracted by smells, sights or sounds, and cover long distances in short periods of time. Hence, owners have to constantly balance the benefits of giving their dogs freedom against the risks of not being able to retrieve them. When a dog wanders off, there is a risk that she might get lost, especially if she finds herself in unfamiliar territory. While microchipping makes it easier for a dog to be reunited with her family, the system relies on her being found by someone who has her interest at heart as well as access to the supporting infrastructure. An unsupervised dog might be abducted for ransom or never to be returned, with some breeds being especially at risk. In rural areas, a wandering dog might also become the target of farmers, who have the legal right to shoot dogs on sight if they enter private land and appear to threaten livestock. In urban areas, on the other hand, she could cause or become the victim of a road accident, the consequences of which the owner would be liable for. While out of sight, a dog could also become injured and physically impaired, due to a variety of possible causes. It is within this context that, for an increasing number of dog owners, location-tracking technology becomes a tool which enables them to fulfill their social responsibility, towards both their dogs and other members of society, within daily care-taking practices."], "rq": ["INTRODUCTIONOur research investigates the social significance of technologically mediated human-animal interactions. We are interested in how tracking devices for dogs are used within domestic contexts in the everyday management of human-canine relationships and care-taking practices; we are interested in how these practices influence the behavior of and change both human and canine family members. However, since we cannot communicate with dogs in the same way that we communicate with humans, this kind of research clearly raises methodological issues to do with the interpretation of human-dog manifest interaction (similar issues arise in studies with young children and adults with communication impairments [15] ). Exploring these issues is important for the development of the emerging areas of human-animal interaction [33] and animal-computer interaction [17] . In order to study technology-mediated human-animal interactions or to develop user-centered technology for animals, we need to question what these interactions and the technology that mediates them might mean for animals as well as humans. Therefore, our research questions how technology might acquire and convey meaning for both; we question how this meaning might be inferred by or communicated between the two, and how it might inform the way in which the two adapt to each other and coevolve; we also question how this coconstructive [9] meaning exchange could be accessed and understood by those researching the interconnections between humans, animals and technology."]}
{"intro": ["Internet censorship has attracted research attention for some time. In the 1980s, before the Internet was widely commercialized, instituting censorship was discussed for a college campus network [Foley 1989 ]. More recently, studies have examined how Internet censorship discourages users' practices of contributing online content [Lindtner et al. 2008; Shklovski and Kotamraju 2011] , the ways in which censorship impacts discussions on social media [Chen et al. 2013] , and how it undermines the government's credibility [Richet 2013 ]. However, to date, little attention has focused on users' awareness of Internet censorship and their attitudes toward it. This is important, as users' perceptions have been shown to impact their Internet experience, their level of trust in online content, and their online behaviors [Guo and Feng 2012; Shklovski and Kotamrju 2011; Wang and Mark 2013] . Understanding user attitudes toward censorship has implications for both Internet policy makers and information technology designers because Internet censorship, as a major Internet regulation mechanism, is being increasingly deployed in some countries. Thus, understanding users' attitudes can apply in a global context, as the impacts of censorship practices expand [Verhulst 2006 ].", "Some research has focused on the influence of Internet censorship on certain individuals and groups, such as how it affects Chinese activists, and how they interacted with censorship to challenge the official discourse [Habermas 2006; Kalathil and Boas 2001; MacKinnon 2008; Taubman 1998 ]. However, more recently, some researchers began to notice that a large portion of Chinese Internet users may conform to government ideologies and are not interested in political matters; therefore, they do not use the Internet as Westerners presumed [Wallis 2011; Wu 2012 Wu , 2013 . These works have also studied users' different patterns of Internet usage in the context of Internet censorship, but the perception of censorship has so far not received much attention.", "In addition, an assumption used in previous studies is that the majority, if not all, of Internet users who live in a country with Internet censorship (e.g., China) are aware of its existence. However, this premise has not been established. Further, contrary to the perspective of a Western audience, some people in such countries may even support Internet censorship as well as government ideologies consistent with censorship [Wu 2013 ]. This has been found in an experiment study with some Chinese university students [Guo and Feng 2012] and in a survey study with Chinese citizens (although these studies also conflate users' perceptions of the abstract concept of censorship with the actual experience) [Fallows 2008; Guo 2003 Guo , 2005 Guo , 2007 ]. Yet, also in China, quite a few studies have revealed that many individuals have used specific strategies to circumvent Internet censorship [Castells 2009; King et al. 2013; MacKinnon 2008 MacKinnon , 2009 Roberts et al. 2009; Shklovski and Kotamraju 2011; Zhu et al. 2013; Ng 2013] . Such behaviors indicate that contrary to the aforementioned studies showing support of Internet censorship, some users exhibit anti-censorship orientations. We believe that the inconsistency between users' reported pro-censorship attitudes and their anticensorship behaviors deserves further exploration."], "relatedWork": ["Between 2000 and 2007, 80% of urban Chinese citizens (Internet users and non-users) agreed that there should be control over the Internet. In 2007, when asked \"who should be responsible for controlling or managing the Internet?\", almost 85% said that they supported government control over the Internet [Guo 2003 [Guo , 2005 [Guo , 2007 . This high acceptance rate of Internet regulation is used by the Chinese government to justify its restrictive Internet policy and censorship practice [Xinhua.net 2009] . However, as discussed previously, support for the concept of controlling the Internet does not necessarily imply support for all kinds of censorship practices. For example, in the same 2007 survey mentioned earlier, 87% of respondents supported controlling pornographic information, but only 27% agreed that online chatting should be censored [Guo 2007] .", "Users' attitudes toward censorship may also be influenced by their political and psychological factors. Guo and Feng [2012] examined the relationship between procensorship attitudes and several political and psychological factors, including the authoritarian personality (i.e., the tendency that people are more likely to obey and submit to authority [Adorno et al. 1950] ), the third-person effect (i.e., the tendency for people to think others are more influenced by the media than they are [Davison 1983] ) and the social and political context of a reformed China (e.g., such as singlechild policy) [Guo and Feng 2012] . These researchers found that an authoritarian personality was not a consistent predictor for pro-censorship attitudes, nor was the third-person effect predictive. However, another study found that the authoritarian personality was a significant predictor of pro-censorship attitudes toward restricting gambling advertising on TV [Youn et al. 2000] . These contradictory empirical findings reflect the complexity of examining censorship together with political and psychological factors.", "Demographic differences also are related to users' attitudes of censorship. Women were found to be more likely than men to support censoring pornography in films or magazines [Gunther 1995] . Ho and Lui [2003] found that women and older people showed a greater desire to restrict adult-oriented materials, but this was not related to their acceptance of pornographic content filtering. However, as both studies are not specific to the context of Internet censorship in China, it is unclear whether these findings would apply to a general system of Internet censorship."], "rq": ["Research Question 2What are the attitudes of Chinese Internet users toward Internet censorship? Do they support, oppose, or are they indifferent toward censorship? For this research question, we focus on the attitudes of Internet users who report being aware of Internet censorship in China. Previous research suggests that the majority of Chinese Internet users support an Internet controlling policy [Fallows 2008; Guo 2003 Guo , 2005 Guo , 2007 Guo and Feng 2012] . However, the majority of users who are aware of Internet censorship might actually be against it because, for example, censorship could hinder Internet usage. No studies have yet disentangled awareness and attitudes toward Internet censorship."]}
{"intro": [], "relatedWork": ["From the beginning, social media systems played a critical role in the rise and popularization of BLM [13] . Twitter has generated a great deal of activity as a place where content and issues specific to the African American community have gained attention [40] . Following the deaths of Martin, Brown, and others, the dissemination of images, videos, and hashtags on Twitter and other social media served a pivotal role leading to the emergence of BLM [10, 19] . However, the impact of the BLM movement in social computing extends far beyond social media. We seek to understand this broader impact by studying activity in Wikipedia around topics related to BLM."], "rq": ["RQ1: Intensified DocumentationRevision histories document several dimensions of knowledge creation in Wikipedia: the editor, time stamp, and con- tent changed in each revision. Table 1 shows the number of revisions, editors, talk page revisions, talk page editors, and pageviews to the ten articles with the most revisions. The amount of activity on these ten articles and talk pages dominates all of the activity on the other 121 pages in the sample combined. The Top 10 pages and their corresponding talk pages account for 76,194 (87.6%) of all revisions, and 5,449 (80.2%) editors made these revisions. Excluding BLM, these top articles are about events that generated a large amount of media attention outside of Wikipedia [10, 13, 19] . The activity for the \"Shooting of Michael Brown\" and the \"Shooting of Trayvon Martin\" contribute 52,941 (60.9%) of all revisions by themselves. The \"Shooting of Michael Brown\" article also had the most pageviews, accounting for 31.5% of all views in the Top 10, reflecting its influence as a major news event. Figure 2 visualizes the monthly activity for all articles in our corpus by number of revisions made (blue), unique editors contributing (green), and pages edited (red). The peaks in activity correspond closely to the death of Oscar Grant (January . We note one peak in July 2013 corresponds to the acquittal of George Zimmerman, the man who killed Martin, and consists almost entirely (98%) of edits to pages related to the shooting of Trayvon Martin and a few edits to pages about the shooting of Oscar Grant. The monthly activity plots illustrate that prominent events drive periods of high activity and reshape activity in aggregate. While they do not sustain the levels of peak activity, we observe a general trend towards an increasing level of activity across all three metrics as additional articles are created and added to the sample. However, the large peaks seem to suggest that activity in the BLM topic space is focused on individual events and does not necessarily imply sustained writing about the BLM-related topics."]}
{"intro": [], "relatedWork": [], "rq": ["Discussion of the algorithmOur algorithm is also quite similar to the Foil (Quinlan 1990 ) algorithm, which forms the basis of many rule learning algorithms, most notably Ripper (Cohen 1995) . The key difference here is that Foil-based algorithms do not evaluate refinements on an absolute scale, but relative to their respective predecessors, i.e., they focus on the gain that a rule obtains in comparison to its predecessor. While this is a reasonable approach, gain-based algorithms can not directly compare the evaluation of two rules with different predecessors, and are therefore not able to identify the best rule encountered during the search. Instead, they always return the last rule searched. Thus, their performance crucially depends on the availability of a pruning heuristic or a stopping criterion, which determines when the refinement process should stop. Foil uses a heuristic based on minimal description length for this purpose (Quinlan 1990; F\u00fcrnkranz and Flach 2004) , whereas Ripper employs the incremental reduced error pruning technique, which prunes each rule after it has been learned (F\u00fcrnkranz and Widmer 1994; F\u00fcrnkranz 1997) . On the other hand, algorithms of the type shown in Algorithm 2 do not necessarily return the last rule searched, but the rule with the highest evaluation encountered during the search. In this case, a stopping heuristic assumes the role of a filtering criterion, which filters out unpromising candidates, but does not directly influence the choice of the best rule (Clark and Boswell 1991) . Because of this dependency on stopping criteria, we do not further consider gain-based heuristics in this paper. However, we note that an empirical study comparing gain-based to absolute heuristics is an open research question."]}
{"intro": [], "relatedWork": [], "rq": ["DiscussionWith regard to the first two research questions, it is clear that there is a difference in test scores (Reading Comprehension, Immediate Vocabulary, and Delayed Vocabulary) of participants for three gloss conditions (L1 gloss, L2 gloss, and No gloss). Firstly, there was no difference between L1 and L2 gloss conditions while there was a significant difference between both gloss conditions and no-gloss condition in the reading comprehension test scores. Secondly, for both vocabulary test scores, the main effect comparing the gloss groups was significant. However, while the significance value was substantial for the differences between glossed and no gloss conditions, it was almost not significant for L1 and L2 gloss groups (p =.043). The reasons for these differences could be that glosses provide L1 translation or L2 description, so they are more practical than dictionaries in terms of accessibility (Hulstijn et al., 1996) . Since students can easily match the meanings with the words in context, reading process is not interrupted thanks to glosses (Rott & William, 2003) . By looking at the scopes of the studies in literature, Schmitt (2008) recommends using L1 glosses for low proficiency level learners by also adding that it does not matter using L1 or L2 glossing as long as the learners can understand the L2 description or L1 translation."]}
{"intro": ["The focus of this paper is on developing automatic classifiers to infer working conditions and stress related mental states from a multimodal set of sensor data: computer logging, facial expressions, posture and physiology. We present related work in Section 2. The dataset that we use is presented in Section 3. We identified two methodological and applied machine learning challenges, on which we focus our work: 1) Using several unobtrusive sensors to detect stress in office environments. We found that state of the art research in stress inference often relies on sophisticated sensors (e.g., eye tracker, body sensors), and/or uses data collected in rather artificial settings. We see possibilities to build human state estimation techniques for use in office environments. We aim to combine information from multiple weak indicator variables based on physically unobtrusive measurements. We address the following research questions: Can we distinguish stressful from non-stressful working conditions, and can we estimate mental states of office workers by using several unobtrusive sensors? Which modeling approaches are most successful? Which modalities/ features provide the most useful information? This helps to configure a minimal sensor set-up for office settings. We address these questions in Section 4. 2) Taking into account individual differences. We found that, in affective computing, often one generic model is learned for all users. This may work for something universal, as the expression of emotions. However, in earlier work [5] , [6] , we found that people differ in their (work) behavior: typical behavior of users already differs per person. Moreover, the way in which people express mental effort or stress may differ. This highlights a need to build personalized models for particular users or user groups, instead of one general model. We address the following research questions: How important are individual differences? Can we improve performance by building personalized models for particular user groups? We address these questions in Section 5. Finally, we present our Conclusions and Discussion in Sections 6 and 7."], "relatedWork": ["Setz et al. [10] present work in which they use EDA measurements to distinguish cognitive load and stress. 32 participants solved arithmetic tasks on a computer, without (cognitive load condition) or with time pressure and social evaluation (stress condition). To address individual differences, data was also normalized per participant by using a baseline period. However, the non-relative features turned out to work better. Leave-one-person-out cross validation yielded an accuracy of 82 percent to distinguishing both conditions. The authors 'suggest the use of non-relative features combined with a linear classification method' (p.416)."], "rq": ["INTRODUCTIONThe focus of this paper is on developing automatic classifiers to infer working conditions and stress related mental states from a multimodal set of sensor data: computer logging, facial expressions, posture and physiology. We present related work in Section 2. The dataset that we use is presented in Section 3. We identified two methodological and applied machine learning challenges, on which we focus our work: 1) Using several unobtrusive sensors to detect stress in office environments. We found that state of the art research in stress inference often relies on sophisticated sensors (e.g., eye tracker, body sensors), and/or uses data collected in rather artificial settings. We see possibilities to build human state estimation techniques for use in office environments. We aim to combine information from multiple weak indicator variables based on physically unobtrusive measurements. We address the following research questions: Can we distinguish stressful from non-stressful working conditions, and can we estimate mental states of office workers by using several unobtrusive sensors? Which modeling approaches are most successful? Which modalities/ features provide the most useful information? This helps to configure a minimal sensor set-up for office settings. We address these questions in Section 4. 2) Taking into account individual differences. We found that, in affective computing, often one generic model is learned for all users. This may work for something universal, as the expression of emotions. However, in earlier work [5] , [6] , we found that people differ in their (work) behavior: typical behavior of users already differs per person. Moreover, the way in which people express mental effort or stress may differ. This highlights a need to build personalized models for particular users or user groups, instead of one general model. We address the following research questions: How important are individual differences? Can we improve performance by building personalized models for particular user groups? We address these questions in Section 5. Finally, we present our Conclusions and Discussion in Sections 6 and 7."]}
{"intro": ["However, increased demand for new information technologies and the rise of EUC has not always led to significant organizational improvements (e.g. see Galletta and Hufnagel, 1992; Harrison and Dick, 1987; and Pyburn, 1986-87) . For example, Eason (1988) reports on research which shows that only 20% of implemented systems achieve their intended benefits, while 80% achieve only marginal impact or worse, fail. MIS managers must facilitate end-user acceptance of technology, while at the same time maintaining some organizational control over technology resources. This study examines the means and extent to which managers of end-user computing are achieving these objectives. Brown and Bostrom (1989) (following Henderson and Treacy, 1986) state that in order to successfully manage end-user computing, an organization must establish a planning and evaluation infrastructure to establish EUC goals, allocate resources, develop policies and procedures, and assess performance. They conclude that one structural mechanism often incorporated to achieve these goals is a steering committee with both IS and end-user representation ."], "relatedWork": [], "rq": ["RQ3. How is usability evaluated by the software standards-setting body?It appears that for these organizations, responsibility for usability-related evaluation is delegated to outside agencies, including trade press and vendors. As organizational context of use was not mentioned as an important determining factor of usability, this result is perhaps not surprising. However, it again represents a gap between usability evaluation methods suggested by HCI researchers or advocates of the socio-technical organizational perspective, and current practice by MIS managers. Most HCI researchers would not endorse usability evaluations conducted by agencies unfamiliar with the users, tasks, tools, etc. in place in the organization. However, the results here, along with those reported by Dillon et al (1993) suggest that this form of evaluation is frequently the case. For the organizations examined in this study, MIS professionals seem comfortable in basing evaluations of usability on test or reports from vendors, trade press, and other even users outside their organization. In conclusion, usability is generally not identified as a criterion used by managers in evaluating potential standard software systems. When it is identified as a separate criterion, it is not considered as important as other factors such as compatibility or feasibility. However, while the importance users' place on various criteria generally mirrors those of managers, the two groups are distinguishable in their ratings of usability. Unlike the evaluations of manager, users believe that usability is the most important criterion used in the evaluation of standardized software. It is clear that the human factors concerns of end-users are not explicitly represented by MIS managers or MIS steering committees."]}
{"intro": ["When working with autistic children 1 in particular, it is important to acknowledge that they perceive the world in which they live very differently compared to non-autistic, adult researchers [9] . In recent years, an increasing number of projects have involved autistic children in participatory design (e.g., [36, 17, 1, 34, 26] ). However, their participation in the evaluation phase has been nearly non-existent because it is deemed very difficult to elicit concrete feedback from autistic children [15] . While communication with autistic children is indeed complex, it may be that a lack of methods offers a better explanation for why they have been included in co-design, but not in evaluation. PEACE -our approach to PE with autistic children, addresses this gap and, in addition, offers researchers working with neurodiverse [8] user groups in general a tool through which to engage them in evaluation processes."], "relatedWork": [], "rq": ["EVALUATING TOGETHER WITH AUTISTIC CHILDRENStarting with the definition of the goals of the evaluation, autistic children may already challenge researchers' pre-conceived expectations in terms of the purpose and evaluation criteria as well as the intended audience and the required methods. In classical researcher-driven evaluation, the selection of methods is generally derived from a combination of the research questions and the epistemological stance of the researchers (see [20] ). However, when working in PE it becomes important to decide on methods based on the abilities of the participants, and with a view to ensuring that the resulting data are meaningful to all involved. By separating the definition of goals of participatory evaluation from the methods, we separate the questions of what is evaluated from how it is evaluated. Both parts inherit different aspects of meaning making, agency and participation -as we will detail in PEACE below."]}
{"intro": [], "relatedWork": [], "rq": ["Analysis and ConclusionsFrom the development test results in Section 3, we note that the Stat-XFER systems' performance currently lags behind the state-of-the-art scores on the 2007 test data 3 . This may be in part due to the low volume of training data used for rule learning. A key research question in our approach is how to distinguish low-frequency correct and useful transfer rules from \"noisy\" rules that are due to parser errors and incorrect word alignments. We believe that learning rules from more data will help alleviate this problem by proportionally increasing the counts of good rules compared to incorrect ones. We also plan to study methods for more effective rule set pruning, regardless of the volume of training data used. The difference in metric scores between indomain and out-of-domain data is partly due to effects of reference length on the metrics used. Detailed output from METEOR and BLEU shows that the reference translations for the test2007 set are about 94% as long as the primary French-English system's translations. On this set, our system has approximately balanced precision (0.62) and recall (0.66). However, the nc-test2007 references are only 84% as long as our output, a situation that hurts our system's precision (0.57) but boosts its recall (0.68). METEOR, as a metric that favors recall, shows a negligible increase in score between these two test sets, while BLEU and TER report significant relative drops of 17.3% and 7.8%. This behavior appears to be consistent on the test2007 and nc-test2007 data sets across systems (Callison-Burch et al., 2007) ."]}
{"intro": [], "relatedWork": ["Ambient display research presupposes that changing information in the user's periphery preserves a sense of calm better than alerting the user of changes directly and that peripheral display will achieve the goal of putting \"us at home, in a familiar place\" [35] . 2 The user, however, must still cognitively process these changes. For instance, although interior windows can help people maintain a sense of connectedness with nearby activity, many people must still close the blinds and shut the door to minimize peripheral cues in order to concentrate. Cubicle dwellers express frustrations at the peripheral cues that they must endure."], "rq": ["LimitationsUsing change blind user interface design strategies has limitations. Inevitably some messages will create motion transients, and to fully exploit the technique will require robust object and people tracking. Most challenging is that some masking methods are not effective when the changes occur to the object of central interest in the scene (e.g. a face of a key person in an image) as opposed to an object of marginal interest. Change detection time for central interest objects is fast regardless of object color, position, and presence/absence [25] . Object position and presence are better encoded by the brain than surface properties, which makes these properties more difficult to change without triggering a detectable motion transient [2] . Therefore, change blindness is less likely to be effective for objects of strong interest. The worst-case scenario, however, is no worse than the current situation: a motion transient is created that mildly attracts the user's attention. Detecting what may be a user's central interest versus marginal interest in a ubiquitous computing environment is an active research question."]}
{"intro": [], "relatedWork": [], "rq": ["EXPERIMENTAL MEASURES FOR HRIExperimental designs and measures in HRI are becoming a field of research on their own, as proven by many summer schools, workshops and special sessions dedicated to the domain that focuses on the experimentation and evaluation of interactive robotic systems. 1 This phenomenon is driven by the lack of common benchmarks and standardized metrics to evaluate robotic systems and the quality of the interactions with the users. Indeed, user experience is central to validate the credibility and acceptability of a system. However, long-term social human-robot interaction is difficult to set up for a complex robotic system. Measuring the interaction quality in HRI, especially for social HRI, is essential but measures used are often very context dependent. From an epistemological point of view, as a new field of research, HRI has to develop strong metrics in order to guarantee its reproducibility and secure the findings of the domain. Of course, depending on the type of evaluation (online survey, large-scale experiment, case study, or longitudinal analyses) the metric used in HRI can vary. But some methodologies can be applied to ensure a common ground of knowledge. [12] , RAS [13] , GodSpeed [14] , IoS, COIRS [15] , . . . Bethel [9] , there are five primary methods of evaluation used in HRI: selfassessment, interviews, observational or behavioral measures, psychophysiology measures, and task performance metrics. In line with this work, Weiss [10] proposed the Usability, Social acceptance User experience and Societal impact (USUS) evaluation framework. The USUS framework gives methodological guidelines according to the research objectives that are aimed to measure usability, social acceptance, user experience, and societal impact. The USUS evaluation framework also provides indicators for each of the research questions and the associated methods of evaluation (i.e., expert evaluation, user studies, questionnaires, physiological measures, focus groups, and interviews). Be they in laboratory, field study, or Wizard-of-Oz experiments, the community often proposes scenario-based experimental protocols. Bethel and Murphy [9] recommends using at least three forms of evaluation in order to have reliable results for an experiment. We propose to group these categories and to give some examples of measures used in HRI."]}
{"intro": ["Contemporary professional translators rarely produce translations entirely from scratch. Instead, they increasingly rely on translation memories (TM) , that is, data bases of texts that have already been translated, and their translations. At translation time, translations of text fragments similar to the actual source text are retrieved from the data base and edited by the translator to bridge the mismatch between retrieved text fragments and an actual correct translation of the current source text. As the quality of the raw output of fully automatic machine translation (MT) systems is on the rise, so is the commercial interest in integrating MT as an alternative or supplement to traditional TMs into the professional translation workflow. Recent studies (Koehn 2009a; Flournoy and Duran 2009; Plitt and Masselot 2010; Federico et al. 2012; Green et al. 2013 ) have concluded that post-editing is, on average, more efficient than translating from scratch. However, the optimal form of human-computer interaction in the context of translation is still an open research question."], "relatedWork": [], "rq": ["IntroductionContemporary professional translators rarely produce translations entirely from scratch. Instead, they increasingly rely on translation memories (TM) , that is, data bases of texts that have already been translated, and their translations. At translation time, translations of text fragments similar to the actual source text are retrieved from the data base and edited by the translator to bridge the mismatch between retrieved text fragments and an actual correct translation of the current source text. As the quality of the raw output of fully automatic machine translation (MT) systems is on the rise, so is the commercial interest in integrating MT as an alternative or supplement to traditional TMs into the professional translation workflow. Recent studies (Koehn 2009a; Flournoy and Duran 2009; Plitt and Masselot 2010; Federico et al. 2012; Green et al. 2013 ) have concluded that post-editing is, on average, more efficient than translating from scratch. However, the optimal form of human-computer interaction in the context of translation is still an open research question."]}
{"intro": ["Prior studies show that, procedurally, co-located groups often accomplish their taskwork and teamwork using a mix of independent and joint work, in a work style referred to as \"mixed-focus\" collaboration [14, 17, 34] . Providing both personal and shared workspaces in an XDE aims to facilitate these distinct work modes. However, a specific crossdevice interaction design used in a given XDE is likely to impact the ability of group members to engage in, and shift between, these work modes. Yet, few studies have examined the impact of cross-device interaction techniques on mixed-focused collaboration. To address this gap, we conducted a user study to examine how different cross-device interaction techniques can impact independent and joint work processes during a representative collaborative task that involves mixed-focus collaboration.", "The second technique, TILT, modeled existing techniques for controlling content on a large display \"remotely\" using a personal device (e.g. [11] ). In TILT, the ROI position on the tabletop was controlled via tilt gestures, made with the tablet and enabled by the tablet's built-in motion sensors. Such \"remote\" cross-device interaction can facilitate individual work [11] ; however, its impact on teamwork is unclear. Given these uncertainties and the potential reachability issues introduced by TOUCH, we performed an empirical study to explore the impact of TOUCH and TILT under different seating positions on collaborative processes. In particular, we sought to answer the following research questions:"], "relatedWork": [], "rq": ["INTRODUCTIONThe second technique, TILT, modeled existing techniques for controlling content on a large display \"remotely\" using a personal device (e.g. [11] ). In TILT, the ROI position on the tabletop was controlled via tilt gestures, made with the tablet and enabled by the tablet's built-in motion sensors. Such \"remote\" cross-device interaction can facilitate individual work [11] ; however, its impact on teamwork is unclear. Given these uncertainties and the potential reachability issues introduced by TOUCH, we performed an empirical study to explore the impact of TOUCH and TILT under different seating positions on collaborative processes. In particular, we sought to answer the following research questions:", "Supporting Joint Work (RQ2)A second feature of the ROI that supported joint work was the ability to independently position and share tablets in the environment. This feature enabled groups to form tableaux to facilitate joint comparison and discussion of selected data items. However, as Wallace et al. [37] found, using tablets for tableaux formation can be restrictive. It offers less physical space than the tabletop does to spread out data between collaborators. Thus, one could also consider enabling users to open selected data directly on the tabletop, or to enable selected data to be moved from the tablets to the tabletop to facilitate joint examination of the \"detailed\" data. This design direction should be explored carefully, however, as it may negate the collaborative benefits provided by the shared reference \"overview\" map on the tabletop."]}
{"intro": ["We use theories of positive and negative security [e.g. 23, 39, 42] to establish a conceptual framework that brings together technological protection from potential harms of mobile phone use (negative security) with the concept of the mobile phone as a facilitator of personal freedoms (positive security). Using this framework to analyse our data, we conclude that the mobile phone is central to the lives of newcomers and, for many, it feels like an extension to their limbs. To this end, mobile phone practices and the agencies such practices generate create a safe space from which newcomers can build a new life, take advantage of the freedoms of being in a new land and, at the same time, maintain kin and friendship connections with the old land. However, such freedoms bring threats and vulnerabilities to these safe spaces that newcomers need to be attentive to and require support for, in order to respond effectively. By conceptualising mobile phones as the enabler of safe spaces, identifying both the threats to and vulnerabilities of those spaces and the concomitant threats and vulnerabilities such spaces may introduce, we bring the two sides of the security argument to the attention of the HCI community."], "relatedWork": [], "rq": ["METHODSThe study was undertaken in two schools in Sweden with three groups of 70 newcomers in total, aged 25-55-two groups in Trelleborg and one group in Kvarnby -between April and June 2017. Participants were recruited with the assistance of teaching staff in both schools and did not constitute a homogenic group, but were made up of people with different backgrounds, belief systems, values, and reasons for leaving their old lands. However, it was made clear that any participation would be voluntary. Prior to undertaking the fieldwork, we had also visited both schools and held multiple meetings with the teaching and support staff -one meeting in Trelleborg and three meetings in Kvarnby -so as to (1) receive feedback on research design and approach, (2) make sure that the scope and remit of the study was understood by everyone involved, and (3) to create an engagement schedule that would not disrupt planned school activities. These meetings were accompanied by email dialogue between teachers and the research team. Whilst a wide range of newcomers was recruited, the groups were dominated by Syrian refugees who had been in Sweden between six months and two years All students belonged to language groups C and D, which meant that they were in the top two groups of Swedish as a foreign language and had previous experience of higher education or language learning. Group demographics, whilst mixed, were thus largely made up of participants from the professional classes in their country of origin. It was decided to conduct the study in the language common to all participants, namely Swedish. This was made possible with the research being conducted by two Swedish-speaking researchers. The research was, however, deliberately designed to be inclusive, regardless of educational background or skills. Similarly, the methods needed to be sufficiently flexible to accommodate a range of language abilities, facilitate the use of translation apps and allow for the use of supplementary techniques such as images and discussions in other languages. As a result, we used collaborative collage as the method for engaging with participants and gathering data. Collaborative collage is one of the engagement tools termed \"creative security methods\" [19] . It uses collaging techniques to enable small groups to discuss research questions and present their views in a collage produced on paper. Using this technique, space is created that allows participants to negotiate the language used within the small group, facilitates the use of images to supplement the written descriptions, and allows for the use of scribes to write down views. By allowing participants to work in this collaborative manner, a dynamic research environment was created where the participants became the prime narrators. This allowed their individual as well as their shared stories to emerge organically. were left at each research site and only pictures of these materials and summaries were retained by the researchers. Table 1 presents a summary of the geographical location for each group activity, the group size and composition, language ability and languages used, activity duration, and outputs. Whilst the language used for the outputs was Swedish and English, Arabic was used by some groups as the language in which to conduct the discussion, and translation apps and group work were used to translate the results of the discussion to contribute to the outputs. As part of the consultation with teaching staff as well as preparatory work with newcomer groups, we developed four research provocations, outlined in Table 2 . These provocations were deliberately designed to work at an instrumental level so that participants could answer in a manner that described their mobile phone use without reflecting on the meanings of that use. The provocations were supplemented by prompts, also listed in Table 2 , which gave participants an opportunity to provide further reflective answers that addressed abstract as well as practical aspects of their mobile phone use."]}
{"intro": ["Many important recommender system use-cases are highly dynamic in nature: news, movie, music or retail recommenders all want to incorporate new behaviour into their models as quickly as possible. With new user-item interactions arriving at high rates, the need for dynamic models that can efciently handle incremental updates in approximately real time becomes more and more apparent [9] . In the context of highly dynamic environments where items have limited lifetimes, this issue becomes even more pressing. News websites typically only want to recommend recent articles, and interactions with newly written articles need to be incorporated into the model as quickly as possible. Auction websites frequently deal with items that are only available for a few days and face the same concerns. Many more examples exist. Traditional Collaborative Filtering (CF) approaches fall short in this setting, as frequent model updates often become too time consuming. Typically, the entire CF model will be retrained at certain fxed points in time, after which the updated model is then deployed. For highly dynamic usecases, the time between subsequent model updates should ideally be kept minimal, in order to allow information from new incoming user-item interactions to be incorporated into the recommendation process as soon as possible. However, as more and more data arrives, the iterative recomputation of the entire model becomes more and more costly as well, putting a hard upper limit on the frequency with which model updates can be performed. We see a fundamental divide here, and such a trade-of is unacceptable for many present-day applications. A clear need arises for CF models that can instantaneously process new transactions and incorporate them into the model in an incremental manner, while avoiding the periodical re-processing of old data."], "relatedWork": ["Nearest-neighbour or similarity join processing is not a new problem, and has been thoroughly investigated in the last 15 to 20 years. Most recent trends for speeding up computation tend to either focus on approximate solutions [12] , distributed algorithms [31, 32] or incremental approaches [25, 30] . The frst notable work in the latter area is the kNNJoin + algorithm [30] , which uses the iDistance similarity measure [28, 29] and a Sphere-tree index to efciently reduce the high-dimensional search to a single dimension. However, when updating two points i and j, the distance between these two points still needs to be re-evaluated in the high-dimensional space before the index can be updated to enable efcient nearest neighbour search. Moreover, this work was aimed at a dimensionality ranging from 20 to 50 and only 100 000 data points, whereas we focus on much larger but very sparse datasets consisting of millions of dimensions, as is typical for recommender systems. Yang et al. propose a method called HDR-tree for incrementally updating nearest neighbour joins in the context of recommender systems [25] , exploiting the distance-preserving properties of Principal Component Analysis (PCA). Their algorithm focuses on content-based fltering with a strict window size of recent items that they consider for recommendations, whereas our algorithm focuses on collaborative fltering with a much more fexible set 1 Code available at: https://github.com/olivierjeunen/dynamicindex of recommendable items that can change over time. Furthermore, they require a fxed set of users, which is too restrictive for the more typical setting we consider. In the context of CF algorithms for streaming scenarios, multiple online learning approaches for matrix factorization, learning-to-rank and neural network models have been presented as well [5, 17, 23, 24] . Several incremental or online learning algorithms specifcally for nearest-neighbour-based CF models have also been published in recent years. Liu et al. propose an incremental learning algorithm that includes temporal information in their novel similarity measure to tackle concept drift in users' preferences over time [10] . The work of Luo et al. focuses on reducing model storage complexity and increasing rating prediction accuracy by incrementally learning biases on top of similarities [11] . TencentRec is a framework implementing several well-known recommendation algorithms in a streaming environment to provide real-time recommendations [6] . Their variant prunes probable dissimilar items, leading to an approximate solution instead of an exact one. Another neighborhood-based approach is proposed by Subbian et al., where a probabilistic data structure is used to approximate item-item similarities and provide recommendations in a real-time manner [21] . Sreepada and Patra present a novel similarity measure that is incrementally learned more easily than other common similarity measures, called item tendency [20] .", "However, most of the above-mentioned methods [10, 11, 20, 21 ] rely on explicit-feedback data, which is vastly diferent than the implicit-feedback data use-case we tackle with this work in terms of similarity measure computation as well as general aspects of the dataset. Moreover, several of these methods [6, 21] use approximations to speed up computation time, at the cost of similarity-(and as a consequence recommendation-) accuracy. In this work, we focus on the task of exact nearest-neighbour and similarity computations from implicit-feedback data, without the use of any approximations or need of explicit rating data. In addition, with our approach, nonrelevant items or users are not considered at computation time, which allows us to work directly on the high-dimensional space, as we can take maximal advantage of the highly sparse nature of the data. Finally, as our algorithm only needs a simple inverted index to efciently identify afected pairs of items when updates arrive, we can formulate it in accordance with the MapReduce paradigm, ensuring scalability through parallel processing [2] ."], "rq": ["Incremental Model Updates with Dynamic RecommendabilityIf recommendability of items is a monotonically decreasing function over time, one does not have to worry about these issues: {(u, l, t c ) \u2208 P t : l = i} will be the empty set for items i \u2208 R t +1 \\R t , since items that become recommendable are per defnition new in this context. In, for example, a news recommendation setting this makes perfect sense: older articles should not be considered for recommendation. In a retail environment, however, this is not the case: recommendability will often depend on seasonality and current stock. Table 1 shows the characteristics of the datasets we used to experimentally validate the efciency of our proposed approach. Movielens is the latest well-known Movielens dataset [4] , Netfix refers to the full dataset that was used for the famous Netfix-Prize [1] . For both movie datasets, we converted explicit ratings to binary implicit feedback, entirely disregarding the actual ratings. Outbrain is a dataset containing logs from users and articles they read, published in a recent Kaggle competition [14] . We use a deduplicated version of the frst 200 million logged user-item events in our experiments: in the case of recurring user-item pairs, we keep only the earliest entry. News is a proprietary real-world dataset consisting of roughly 96 million user-item pairs originating from article reads on the website of a large Belgian newspaper. Our algorithm, as well as the baseline methods, are implemented in C++ and compiled with all the available optimisation fags. Experiments ran on a single Intel Xeon processor. We aim to answer three research questions, respectively covered in the following sections:"]}
{"intro": ["Open online collaborations have gained both public and scholarly attention for their ability to channel the effort of thousands of volunteer contributors into the creation of valuable products and services. However, key features that contribute to the success of open collaborations, such as low barriers to entry and porous boundaries, can also make them vulnerable to disruption [20] . High membership churn and low member commitment create challenges for socializing new members and enforcing standards of behavior. Furthermore, open collaborations' dependence on voluntary, and often anonymous, contributions limits their ability to directly sanction rulebreakers. These challenges can lead to conflict, burnout, and quality control issues that threaten the community's survival [10] . Developing and maintaining strong social norms is one strategy that these communities employ to sustain themselves in the face of such challenges [1] .", "Prior work has recorded various sources of normative influence in online communities-through explicitly stated rules and guidelines [41] , implicit inference of the behavior of others [60] , participants' prior experiences with other social groups [68] , and through corrective mechanisms [31] . However research thus far has mainly examined one source of normative influence at a time and has not distinguished between different types of social norms. We argue that for better community management and design of open collaborations, it is important to examine distinct sources of normative influence and their interactions. Some sources of normative influence within a community may have a greater impact on behavior than others in particular contexts and/or for particular individuals. Furthermore, norms may conflict with one another, leading to unpredictable-and often, undesirable-outcomes [5] . This normative conflict may in turn lead to greater overall membership turnover and other risks to community success, such as the departure of core members [17, 50] ."], "relatedWork": [], "rq": ["Normative conflict.These normative conflicts can have negative consequences: Filippova and Cho, in a series of studies of FLOSS communities, showed that some forms of normative conflict had a negative impact on members' identification with the project and their intention to continue participating [16, 17] . However, we know relatively little about the conditions under which different sources of normsinjunctive vs. descriptive, local vs. imported-are most influential in online communities. Because there is insufficient prior work for us to generate a hypothesis about the specific nature of these influences, we ask a more general research question about normative conflict:"]}
{"intro": [], "relatedWork": [], "rq": ["DiscussionThe results concerning the first two research questions do not support the findings of Steven (1991), Cobb (1997 Cobb ( , 1999 , Koosha and Jafarpour (2006) , \u00c7elik (2011 \u00c7elik ( ), Huang (2014 , Rezaee et al. (2014 ), or Daskalovska (2015 with regard to the effective use of corpus in teaching vocabulary. However, it should be noted that the present study did not have the same experimental setting as those studies. Steven (1991) and Cobb (1997 Cobb ( , 1999 did not examine collocations. Even though Koosha and Jafarpour (2006) , \u00c7elik (2011 \u00c7elik ( ), Huang (2014 and Daskalovska (2015) investigated the effectiveness of corpus consultation in teaching collocations, the focus was on different types of collocations, not on V+N collocations. The length of exposure to concordancing, however, was longer in Rezaee et al. (2014) than the current study in examining verb+noun collocations."]}
{"intro": ["Change is an unavoidable and intrinsic part of any kind of effort involving software architecture. Not only do individual architectures evolve over time, but the advent of product line architectures has brought the problem of managing architectural change to a whole new level [2] . New products are continuously introduced, existing products evolve, and old products are phased out. The set of architectural changes resulting from these actions must be carefully managed. This kind of management involves addressing two key problems: (1) capturing architectural changes, and (2) understanding the architectural changes that define the difference between two products (or two versions of the same product) and propagating these architectural changes to yet another, third (version of a) product. The first problem has already been addressed through the advent of architectural description languages that incorporate facilities for capturing different versions of a product line architecture [10, 17] . The second problem, however, has not been addressed as of yet.", "This paper begins to address this problem and is based on the recognition that understanding and propagating architectural changes bears great resemblance to a similar, long-standing issue in the field of configuration management: understanding the exact nature of source code changes as they have been made over time and propagating selected changes from one version of a software system to another [4] . To address this problem, differencing and merging algorithms have been developed [3] . However, direct application of these algorithms to architectures would not yield the desired result. Because existing algorithms typically only operate on textual artifacts and are line-based in their operations, they cannot be aware of any specific architectural semantics and therefore offer little help, particularly in the understanding of architectural changes. Nonetheless, these algorithms form a solid basis upon which our approach is based. Specifically, we have adapted them in making three contributions to the field of software architecture. First, we have enhanced an existing representation for product line architectures, xADL 2.0 [5] , with a representation in which the exact difference between two products in a product line architecture can be captured. Second, we have created a differencing algorithm that uses the representation to create an understanding of the exact set of architectural changes that constitute a difference between two products. Third, we have created a merging algorithm through which it is possible to propagate such architectural changes to other products in the product line."], "relatedWork": [], "rq": ["CONCLUSIONArchDiff represents only the beginnings of our work in this area. The creation of a graphical user interface is clearly at the forefront of our further development efforts. However, we also intend to address some more fundamental research questions as part of our future work. Most notably, we intend to investigate how the differencing and merging algorithms can be adapted to support dynamic, run-time updates [13, 16] . Given that xADL 2.0 supports attaching implementation information (such as Java class files) to architectural elements, we intend to leverage the above results in developing a tool that \"merges\" architectural changes into a running system by removing, instantiating, and linking elements dynamically. Additionally, we intend to investigate the use and applicability of more fine-grained, semantic-based differencing and merging algorithms to further support the management of architectural change."]}
{"intro": ["With the advent of new devices and text entry methods, however, comes new pressure on evaluation methodologies. Rigorous evaluations of new methods are required to validate their strengths and weaknesses; informal, subjective results do not warrant trust or comparison [MacKenzie and Soukoreff 2002a] . For this reason, current practice is to rival two or more text entry methods against one another in controlled experiments. In these experiments, subjects are told to transcribe presented phrases as quickly and accurately as possible ] while data is logged for later analysis.", "For example, one artificially constrained experimental paradigm disallows erroneous characters completely. As the subject transcribes text on a line beneath the presented text, any attempted character that does not match the character directly above it is not displayed. Often in this paradigm, the entry of an erroneous character results in an audible \"beep.\" Subjects may incur many successive beeps, without ever seeing characters appear because their entries do not match the presented character at their current position. What's worse, the backspace is rendered irrelevant, even though backspace is the second most common keystroke in real desktop text entry after space [MacKenzie and Soukoreff 2002a] . However, inferring intention in this paradigm is trivial because we assume that a subject is always trying to enter the next character in the presented text, even though this is often not the case. Not surprisingly, the experience for subjects entering text in these types of evaluations is potentially frustrating, since each error effectively creates a \"road block\" that stops them abruptly, often for many entries. Nevertheless, this paradigm's ease of use has caused many to employ it [Venolia and Neiberg 1994; Isokoski and Kaki 2002; Evreinova et al. 2004; Ingmarsson et al. 2004] ."], "relatedWork": [], "rq": ["Advantages of Using Input Streams-The input stream usually yields more data per trial than the transcribed string, since by definition |IS| \u2265 |T |. Therefore, depending on the research questions being asked, analyzing the input stream for character-level errors may allow us to run fewer trials and save time and money on evaluations, which are often time-consuming and expensive [Jeffries et al. 1991] . Such savings will be possible if it is error data that we are after. However, if we are interested in learning rates as measured by speeds over sessions, analyzing IS will not reduce the number of sessions required. -When instructed to \"enter the text quickly and accurately\" [Soukoreff and MacKenzie 2003], subjects tend to fix most, if not all, of their errors in text entry trials. For example, in the study accompanying a character-level error analysis from the prior work [MacKenzie and Soukoreff 2002b] , subjects left only 2.23% errors in T . Other studies show even fewer uncorrected errors: 0.79% , 0.53% [Wobbrock et al. 2004] , and 0.36% [Wobbrock et al. 2003 ]. In the extreme case, if subjects correct all errors, P and T will be identical and no character-level error information will be available. Such a contingency does not reduce the value of IS, however, since corrected errors (the errors subjects made but fixed) are still captured therein. -Speed and uncorrected errors are tradeoffs in text entry. Therefore, to equitably compare speeds, some experiments [Lewis 1999 ] have required perfect transcription, where leaving errors in T is not permitted. But character-level error analyses of perfect transcription studies are useless when employing only P and T , since they will always be identical. Analyzing IS, on the other hand, allows for the extraction of character-level results, even in perfect transcription studies. -For stroke-based or handwritten text entry, such as Graffiti [Palm Inc. 1995] , one possible outcome of an attempted character is a nonrecognition. By definition, T cannot contain nonrecognitions, but IS can. Therefore, looking at IS can be valuable to designers who are trying to identify characters that are difficult to recognize in stroke-based text entry methods."]}
{"intro": [], "relatedWork": [], "rq": ["Effects of Norm EnforcementThe effects of moderation on the quality of collaboratively created content and long term behavior of individual community members have not been investigated much. In the context of Wikipedia, Halfaker et al. [38] found that the action of \"reverting\" edits has the effect of reducing motivation and quantity of work, particularly for new editors. However, they also found that reverts result in higher quality contributions. Halfaker et al. [37] found that enforcing quality control mechanisms affects the retention of high-quality newcomers. Personalized warning messages, as opposed to pre-defined template messages, are found to be effective in retaining newcomers [33] . Motivated by this prior literature, which studies the effectiveness of norm enforcement actions both at the platform-level (i.e., articles) and individual member-level (i.e., editors), we seek to answer our research question about the effectiveness of NPOV norm enforcement both at the article level (RQ1) and at the editor level (RQ2).", "Identifying Biased LanguageBias in Wikipedia can emerge from several factors including language style, editors' point of view, cited sources, coverage of topics, etc. Prior work focused on political bias [35] , cultural bias [13] , gender bias [75] , topic bias [29] , and authoritative bias [22] . Our goal in RQ1 and RQ2a is to study the effects of NPOV tagging in article-level and editor-level language. Particularly, we are interested in characterizing the bias in articles and editor contribution in terms of the linguistic style that may introduce bias. The Wikipedia Manual of Style [81] states that \"There are no forbidden words or expressions on Wikipedia, but certain expressions should be used with caution, because they may introduce bias. Strive to eliminate expressions that are flattering, disparaging, vague, or endorsing of a particular view point.\" Prior work [e.g., 40, 67] has addressed the task of identifying biased language in Wikipedia at different levels. However, we are not aware of a reliable system that can detect bias in terms of linguistic style in Wikipedia articles. Therefore, we use a set of linguistic style lexicons as a proxy to characterize biased language in Wikipedia. 15 The Wikipedia Manual of Style provides a list of words to watch in a prescriptive manner [79] , indicating style words that may introduce bias. We compiled these styles words into a lexicon called Words to Watch. Prior work of Recasens et al. [67] introduced the task of detecting bias inducing terms in phrases from Wikipedia articles and used a set of pre-compiled style lexicons that are indicative of expressions of attitude or point of view. These lexicons include hedges, factive verbs, assertive verbs, positive words, and negative words. We use these nine lexicons in addition to the words from Wikipedia manual. 16 A list of all the lexicons we used is shown in Table 1 with a description, sources, and example terms. To characterize the amount of biased language in a text, we compute the coverage of each lexicon words per token in the text.", "RQ2: Editor-level Effects of NPOV CorrectionWhen an editor is corrected for NPOV, there is a significant reduction in their negative words usage (13.5%); however, we do not observe any statistically significant change for six of the ten lexicons relating to biased language, including Wikipedia's own list of \"words to watch\" (RQ2a). These trends remain even after controlling for editor experience and talk page discussion during treatment. For RQ2b, we observe an increase in engagement for inexperienced editors after NPOV correction. One possible explanation is that NPOV correction helps inexperienced users become aware that their contributions are monitored by others, and this awareness could motivate them to contribute more. These results for RQ2 suggest that when corrected for NPOV, the quality of the writing style of editors does not improve significantly (at least, as measured by all but one of our lexicons), while it leads to a increase in editing activities for inexperienced editors. These findings partially conflict with the observations of Halfaker et al. [38] , who found that revert actions demotivate new editors and reduce the quantity of work, even as they increase the overall quality of contributions. Halfaker et al. [38] also found that reverts affect editors differently based on the experience of the editor who makes the revert. Future work could perform similar analysis to further understand the effects of NPOV correction on engagement based on the experience of correcting editors."]}
{"intro": ["Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. There has also been an interest in devices that support prolonged attentive reading. Researchers in \"appliance design\" investigated the usability of reading appliances [6] -the forerunners of contemporary reading devices, such as the Kindle and iPad. A recent resurgence in reading devices has resulted in research on their legibility and general usability (e.g. [9] ). However, this has tended to focus on display technology, rather than interaction design. Another question has emerged around input technology; prototype reading appliances used a stylus, whereas modern devices often adopt touch. The different affordances of the two technologies have provoked a question about their relative merits for different tasks. Annotation is a central task to attentive reading, and the suitability of touch-sensitive devices for annotation [4] has emerged as a key research question.", "In this paper, we focus on co-located collaborative reading, as opposed to remote reading, in consequence of initial research [8] that investigated the social context of that activity in detail, drawing a contrast between digital and printed media. The study of users' current behaviour allowed us to narrow and focus the scope of the work undertaken here. However, we also drew on a range of prior research from mobile HCI."], "relatedWork": ["Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. There has also been an interest in devices that support prolonged attentive reading. Researchers in \"appliance design\" investigated the usability of reading appliances [6] -the forerunners of contemporary reading devices, such as the Kindle and iPad. A recent resurgence in reading devices has resulted in research on their legibility and general usability (e.g. [9] ). However, this has tended to focus on display technology, rather than interaction design. Another question has emerged around input technology; prototype reading appliances used a stylus, whereas modern devices often adopt touch. The different affordances of the two technologies have provoked a question about their relative merits for different tasks. Annotation is a central task to attentive reading, and the suitability of touch-sensitive devices for annotation [4] has emerged as a key research question.", "In this paper, we focus on co-located collaborative reading, as opposed to remote reading, in consequence of initial research [8] that investigated the social context of that activity in detail, drawing a contrast between digital and printed media. The study of users' current behaviour allowed us to narrow and focus the scope of the work undertaken here. However, we also drew on a range of prior research from mobile HCI."], "rq": ["INTRODUCTION AND BACKGROUNDPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. There has also been an interest in devices that support prolonged attentive reading. Researchers in \"appliance design\" investigated the usability of reading appliances [6] -the forerunners of contemporary reading devices, such as the Kindle and iPad. A recent resurgence in reading devices has resulted in research on their legibility and general usability (e.g. [9] ). However, this has tended to focus on display technology, rather than interaction design. Another question has emerged around input technology; prototype reading appliances used a stylus, whereas modern devices often adopt touch. The different affordances of the two technologies have provoked a question about their relative merits for different tasks. Annotation is a central task to attentive reading, and the suitability of touch-sensitive devices for annotation [4] has emerged as a key research question."]}
{"intro": [], "relatedWork": ["Public datasets truly help accelerate research in an area, not just because they provide a benchmark, or a common language, through which researchers can communicate and compare their different algorithms in an objective manner, but also because compiling such a corpus is tedious workrequiring a lot of effort which many researchers may not have the resources to do. In the area of facial expression analysis, the Cohn-Kanade database, in its extended form named CK+, played a key role in advancing the state of the art in this area. The CK+ database, contains 593 recordings of posed and non-posed sequences. The sequences are recorded under controlled conditions of light and head motion, and range between 9-60 frames per sequence. Each sequence represents a single facial expression that starts with a neutral frame and ends with a peak facial action. Transitions between expressions are not included. Several systems use the CK, or CK+, databases for training and/or testing. Since it was first published, a number of papers have been published that were trained and/or tested on this data set including: Bartlett et al. [3] , Cohen et al. [5] , Cohn et al. [6] , Littlewort et al. [10] and Michel & El Kaliouby [15] . Since then, a few other databases have emerged, including: MMI [16] , SE-MAINE [14] , RU-FACS [2] , SAL [7] . A survey of databases and affect recognition systems can be found in [25] . However, there is a need for mechanisms to quickly and efficiently collect numerous examples of natural and spontaneous responses. Lab-based studies pose numerous challenges including recruitment, scheduling and payment. Efforts have been made to collect significant amounts of spontaneous facial responses; however, the logistics of a laboratory based study typically limits the number of participants to under 100, e.g. 42 in [13] . By using the internet we can make data collection efficient, asynchronous and less resource intensive, and get at least an order of magnitude more participants. Figure 2 shows the web-based framework that was used to crowdsource the facial videos and provides an overview of the user experience. The website was promoted on Forbes.com for the first day that it was live. Visitors may have found it via this route, a search engine or a shared link. Visitors to the website opt-in to watch short videos while their facial expressions are being recorded and analyzed. Immediately following each video, visitors get to see where they smiled and with what intensity. They can compare their \"smile track\" to the aggregate smile track. On the client-side, all that is needed is a browser with Flash support and a webcam. The video from the webcam is streamed in real-time at 15 frames a second at a resolution of 320x240 to a server where automated facial expression analysis is performed, and the results are rendered back to the browser for display. There is no need to download or install anything on the client side, making it very simple for people to participate. Furthermore, it is straightforward to easily set up and customize \"experiments\" to enable new research questions to be posed. For this experiment, we chose three successful Super Bowl commercials: 1. Doritos (\"House sitting\", 30 s), 2. Google (\"Parisian Love\", 53 s) and 3. Volkswagen (\"The Force\", 62 s). All three ads were somewhat amusing and were designed to elicit smile or laughter responses. Results showed that significant smiles were present in 71%, 65% and 80% of the responses to the respective ads."], "rq": ["RELATED WORKPublic datasets truly help accelerate research in an area, not just because they provide a benchmark, or a common language, through which researchers can communicate and compare their different algorithms in an objective manner, but also because compiling such a corpus is tedious workrequiring a lot of effort which many researchers may not have the resources to do. In the area of facial expression analysis, the Cohn-Kanade database, in its extended form named CK+, played a key role in advancing the state of the art in this area. The CK+ database, contains 593 recordings of posed and non-posed sequences. The sequences are recorded under controlled conditions of light and head motion, and range between 9-60 frames per sequence. Each sequence represents a single facial expression that starts with a neutral frame and ends with a peak facial action. Transitions between expressions are not included. Several systems use the CK, or CK+, databases for training and/or testing. Since it was first published, a number of papers have been published that were trained and/or tested on this data set including: Bartlett et al. [3] , Cohen et al. [5] , Cohn et al. [6] , Littlewort et al. [10] and Michel & El Kaliouby [15] . Since then, a few other databases have emerged, including: MMI [16] , SE-MAINE [14] , RU-FACS [2] , SAL [7] . A survey of databases and affect recognition systems can be found in [25] . However, there is a need for mechanisms to quickly and efficiently collect numerous examples of natural and spontaneous responses. Lab-based studies pose numerous challenges including recruitment, scheduling and payment. Efforts have been made to collect significant amounts of spontaneous facial responses; however, the logistics of a laboratory based study typically limits the number of participants to under 100, e.g. 42 in [13] . By using the internet we can make data collection efficient, asynchronous and less resource intensive, and get at least an order of magnitude more participants. Figure 2 shows the web-based framework that was used to crowdsource the facial videos and provides an overview of the user experience. The website was promoted on Forbes.com for the first day that it was live. Visitors may have found it via this route, a search engine or a shared link. Visitors to the website opt-in to watch short videos while their facial expressions are being recorded and analyzed. Immediately following each video, visitors get to see where they smiled and with what intensity. They can compare their \"smile track\" to the aggregate smile track. On the client-side, all that is needed is a browser with Flash support and a webcam. The video from the webcam is streamed in real-time at 15 frames a second at a resolution of 320x240 to a server where automated facial expression analysis is performed, and the results are rendered back to the browser for display. There is no need to download or install anything on the client side, making it very simple for people to participate. Furthermore, it is straightforward to easily set up and customize \"experiments\" to enable new research questions to be posed. For this experiment, we chose three successful Super Bowl commercials: 1. Doritos (\"House sitting\", 30 s), 2. Google (\"Parisian Love\", 53 s) and 3. Volkswagen (\"The Force\", 62 s). All three ads were somewhat amusing and were designed to elicit smile or laughter responses. Results showed that significant smiles were present in 71%, 65% and 80% of the responses to the respective ads."]}
{"intro": [], "relatedWork": [], "rq": ["DISCUSSIONRQ1: Which kinds of task do people prefer to complete on mobile devices and what impact does time have on this? Our results suggest that users prefer to complete tasks on their mobile devices based primarily on two factors: (i) tasks on which they have more prior knowledge and/or interest; (ii) tasks that are more relevant to their current context. We do not expect users to look for absolutely new information while on the go, unless it is highly relevant to their context. For instance, assume a scenario where a user is having lunch with colleagues and they happen to discuss about the effect of cell towers on brain cancer. The user, presumably, has no prior knowledge about this topic. However, given that he/she is seated (situational context) and seeks to find the answer for the colleagues, it is highly probable that the user completes such a difficult task. Users do not tend to complete search tasks that they perceive to be more difficult, suggesting that they need more attention and time; probably preferring to complete such a task later, either on a desktop or in another context [23] , perhaps where they can better concentrate on it [33] . This supports the results of previous work, which showed that users are more engaged with the tasks in which they have more prior knowledge and interest [16] ."]}
{"intro": ["In 2014, an estimated 36.9 million people were living with HIV or AIDS globally, and 1.2 million lost their lives to the disease that same year [23] . While HIV cannot be cured, people who live with HIV can control the virus and live healthy lives by taking antiretroviral (ARV) drugs. However, this is only possible if these individuals comply with the strict and complicated regime required with taking ARV medication, as well as monitor their health closely. Doing so is not easy, and recent research has shown that in the United States only one-third of HIV+ adults are able to successfully manage their health [3] .", "Research in HCI has focused on the technology and information needs in PHI. In particular, researchers have explored the challenges and benefits of PHI for people living with various health-related conditions, such as diabetes [16, 18] and bipolar disorder [8] . Many of their findings provide implications that can be generalized across numerous health conditions. However, previous research has not yet looked at a health condition as complex and stigmatized as HIV, and whether or not the previous results can apply remains yet unknown."], "relatedWork": [], "rq": ["Ownership, Security and PrivacyIt is clear, through both our study and previous findings, that there are privacy and security considerations to be made when an HIV+ individual shares their information [2] . There is currently much interest within healthcare to use personal information to research health phenomena, as well as for individuals to share data with their community to support reflection and identify trends and patterns. However, it is an open research question how to motivate HIV+ users to open up the access to their data for this purpose, and under what circumstances they would be willing to do so."]}
{"intro": ["However, there are many diseases for which it is not well understood which data types are pertinent to self-track, and for which specific relevant variables have not been fully enumerated. This lack of knowledge has been noted in the literature for rare diseases [106] . It is also significant when designing for enigmatic diseases. Across several diseases, enigmatic conditions seem to share heterogeneous symptoms, unexplained differences in treatment responses, and lack of symptom specificity [1, 14, 36, 56, 86, 92] . Many enigmatic diseases are relatively prevalent; in fact, the heterogeneity in symptoms may be due to the large number of people affected. Examples of enigmatic diseases include chronic diseases like interstitial cystitis, psoriasis, Crohn's disease, and chronic fatigue syndrome. Irritable bowel syndrome is another such chronic condition, where diet is an established data type to monitor, but specific diet triggers are unknown and in fact vary from one patient to another. Even relatively well understood diseases may have aspects that remain enigmatic: for example, why post-meal blood glucose spikes vary from one individual to another is an active area of research [103] .", "Previous HCI research has shown success in designing selfexperimentation tools and n-of-1 studies to help individual users identify which specific variables are useful to monitor when managing their own health [50, 51, 105] . In such selfexperimentation systems however, the emphasis on supporting each user individually might come at the expense of deriving insights across all users. By letting users track and experiment with what dimensions and variables they think are relevant to their own experience of their condition, these systems promote individual selfdiscovery, but might make it challenging to standardize and learn across individuals. There is an unmet opportunity to broaden the current conceptualization of personal health informatics to include not just self-tracking for the sake of self-knowledge and self-discovery but self-tracking for improving knowledge of disease across a group. This is particularly true for enigmatic conditions. Self-tracking data of enigmatic conditions, when taken in aggregate across a user group, can provide a novel view of the disease, can help enhance scientific knowledge about the disease, and can help bridge the gap that exists between the patient experience of these enigmatic conditions and their current medical understanding."], "relatedWork": [], "rq": ["Self-trackingThe idea of leveraging the self-tracked health data of many individuals to discover new insights across a group is not new. Recent research showed value in examining a single variable (e.g., number of steps per day, collected passively) tracked across many individuals to learn about a behavior (e.g., physical activity) at the group level [6] . In this case, like in other recent examples, the data leveraged was collected via existing commercial apps, with their own set of engagement strategies. However, designing self-tracking tools that promote sustained engagement among each user and at the same time enable researchers to learn at scale and discover new disease insights is an open research question."]}
{"intro": ["Remote research methods such as the diary method or the Experience Sampling Method (ESM) [8] have always intrigued researchers, as they allow data gathering in their \"natural, spontaneous context\" [2] without being obtrusive and thereby in places where observation would be impossible or inappropriate. They have been applied both outside (e.g. see [3] . for a variety of usage scenarios) and inside HCI (e.g. [6] , [11] ) in various forms, such as pen & paper or PDAs. Besides, diaries and ESM have shown to minimize retrospective effects (e.g. compared to retrospective interviews) and allow both qualitative and quantitative data collection. They are especially useful in longitudinal studies as they allow analyzing and modeling changes within and between subjects [3] . Drawbacks however include a high burden on the participant and as a consequence thereof such diaries are often reduced to simple repeated questionnaires. Nevertheless, as our world is becoming more and more ubiquitous and HCI research is thereby more interested in investigating how people deal with such technology in the wild, the need for these methods has increased even more and technology itself has been a helping hand to support both the researcher and the participant. In this paper we present Pocket Bee (see figure  1 ), a multi-modal diary tool that allows participants to gather data in multiple ways on Android [1] based smart phones while allowing researchers to access this data immediately via a web-based control center and react on it accordingly, e.g. by sending out specific tasks or questionnaires. Pocket Bee integrates an easy to use client user interface that reduces the burden on the participant while maintaining a high flexibility towards the method and the possibility to capture in-depth data. We furthermore discuss several design goals which illustrate the importance of the tool for both the diary method and ESM. Early electronic diary or ESM tools focused on simply providing questionnaires on a PDA [2] , while current approaches have focused especially on the integration of sensor data to better support ESM (e.g. [7] ), multiple modalities to enrich the data-gathering process (e.g. [4] , [9] ), or the integrated testing of mobile device applications [5] . The existing tools seem to have focused on functionality and extensibility but not so much on the design of the client user interface itself, as it has been merely discussed in the according papers. However, an electronic device does not magically reduce the participants' burden for collecting data and it might even increase the burden for some users that are not familiar with smart phone technology. In the following sections we will first present our research questions for the user interface design and go on to discuss the user interface concepts by illustrating an upcoming study in the automotive sector."], "relatedWork": [], "rq": ["INTRODUCTIONRemote research methods such as the diary method or the Experience Sampling Method (ESM) [8] have always intrigued researchers, as they allow data gathering in their \"natural, spontaneous context\" [2] without being obtrusive and thereby in places where observation would be impossible or inappropriate. They have been applied both outside (e.g. see [3] . for a variety of usage scenarios) and inside HCI (e.g. [6] , [11] ) in various forms, such as pen & paper or PDAs. Besides, diaries and ESM have shown to minimize retrospective effects (e.g. compared to retrospective interviews) and allow both qualitative and quantitative data collection. They are especially useful in longitudinal studies as they allow analyzing and modeling changes within and between subjects [3] . Drawbacks however include a high burden on the participant and as a consequence thereof such diaries are often reduced to simple repeated questionnaires. Nevertheless, as our world is becoming more and more ubiquitous and HCI research is thereby more interested in investigating how people deal with such technology in the wild, the need for these methods has increased even more and technology itself has been a helping hand to support both the researcher and the participant. In this paper we present Pocket Bee (see figure  1 ), a multi-modal diary tool that allows participants to gather data in multiple ways on Android [1] based smart phones while allowing researchers to access this data immediately via a web-based control center and react on it accordingly, e.g. by sending out specific tasks or questionnaires. Pocket Bee integrates an easy to use client user interface that reduces the burden on the participant while maintaining a high flexibility towards the method and the possibility to capture in-depth data. We furthermore discuss several design goals which illustrate the importance of the tool for both the diary method and ESM. Early electronic diary or ESM tools focused on simply providing questionnaires on a PDA [2] , while current approaches have focused especially on the integration of sensor data to better support ESM (e.g. [7] ), multiple modalities to enrich the data-gathering process (e.g. [4] , [9] ), or the integrated testing of mobile device applications [5] . The existing tools seem to have focused on functionality and extensibility but not so much on the design of the client user interface itself, as it has been merely discussed in the according papers. However, an electronic device does not magically reduce the participants' burden for collecting data and it might even increase the burden for some users that are not familiar with smart phone technology. In the following sections we will first present our research questions for the user interface design and go on to discuss the user interface concepts by illustrating an upcoming study in the automotive sector.", "Design Goals and SolutionsKhan et al. [10] did an analysis of current experience sampling tools and derived some requirements for future tools, such as multi-modality or instant synchronization. We agree with most of these and they influenced our choice of design goals. As stated before we will, however, focus on the research questions from a user interface design perspective and the methodological benefits that can be achieved thereby."]}
{"intro": ["Psycholinguistics has a long tradition of using experimental methods, but in recent years, linguists working in areas such as syntax, semantics, and pragmatics have also started to embrace empirical methods (see Arunachalam (2013) for a review of the more commonly used methods). As a consequence, basic familiarity with experiment design is becoming a core requirement for doing linguistics. However, just knowing how to carry out an experiment is not enough; a good understanding of statistical theory and inference is also necessary. In this article, we present the most important issues that researchers need to be aware of when carrying out statistical inference using frequentist methods (as opposed to Bayesian approaches, see Nicenboim and Vasishth (2016) ). We focus on frequentist methods because the statistical tools of choice in psycholinguistics and linguistics are usually frequentist ones; examples are the t-test, analysis of variance (ANOVA), and linear mixed models. Given software such as R (R Core Team, 2014) , it is extremely easy to obtain statistics such as t-of F-values, and the corresponding p-values. However, it is equally easy to misunderstand what these mean; in particular, a misinterpretation of the p-value often leads researchers to draw conclusions from their data that are not supported by the underlying statistical theory. We start the paper illustrating the meaning of tand p-value and discussing some common misconceptions by means of the onesample t-test or (equivalently) the paired t-test. We use this test as an example because of its relative simplicity and because it happens to be a very frequently used one in linguistics and psycholinguistics. For ANOVAs and linear mixed models, the situation is more complex, but the same logic and issues described below also apply. We then show the importance of power, and Type I, II, S, and M errors using simulations based on linear mixed models. Two further important topics discussed are: the problems involved in running participants till significance is reached, and the issues involved in experiments with multiple measures, multiple regions of interest, and too many degrees of freedom in analysis."], "relatedWork": [], "rq": ["Running till significance is reachedLet's compare the distribution, under repeated sampling, of the t-statistic in the standard case vs with the above stopping rule (red) (Figure 9 ). We get bumps in the tails with the above stopping rule because, under repeated sampling, some proportion of trials which have p > 0.05 will be replaced by trials in which p < 0.05, leading to a redistribution of the probability mass in the t-distribution. This redistribution happens because we give ourselves more opportunities to get the desired p < 0.05 under repeated sampling. In other words, we have a higher Type I error than 0.05. It would of course be reasonable to take this approach if we appropriately adjust the Type I error; but this is not standard practice. Thus, when using the standard frequentist theory, one should fix one's sample size in advance based on a power analysis, not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant. Of course, if your goal is only to get a significant result so that you can get your article published, such stopping rules will give better results than fixing your sample size in advance! 6 Multiple measures, multiple regions, degrees of freedom in analysis Gelman and Loken (2013) point out that there are in general too many ways to analyze the data: from the choice of the statistical test to decisions on what data points to exclude or include. This means that once we start looking hard enough, it is possible to find a significant difference and then tell a post-hoc theory that fits very neatly together. For that to happen, it is not necessary that a researcher would go on a \"fishing expedition\" (Gelman, 2013) , that is, it is not necessary that he/she would be actively trying to report any comparisons that happen to yield a p-value lower than 0.05; in many cases, the researcher just has too many degrees of freedom in the analysis and it is not clear which is the right way to analyze the data. A common example is to decide to report the result of a linear mixed model or an ANOVA or t-test, depending on which one of these yields a p-value below 0.05. Researchers often flexibly switch between linear mixed models and repeated measures ANOVA to tell \"the best story\" they can given the data. One problem here is that using the ANOVA or t-test where a linear mixed model with crossed subject and item random effects is suggested by the design artificially reduces the sources of variance (through aggregation), with the result that effects that are not really statistically significant under a linear mixed model end up being significant once one aggregates the data. But the other problem with shopping around for the test that gives us the lowest p-value is the one that Gelman and colleagues point out: we are introducing a degree of freedom in the analysis. Another commonly seen situation is flexibly analyzing different regions of interest (often aggregating them post-hoc) until a p < 0.05 result is found; for example, in Badecker and Straub (2002) , in their results section for experiment 5, they write: \"No significant differences emerged for any individual words or two-word regions. However when reading times are collapsed across the four positions following the reciprocal, reading times for this region were 48 ms longer in the multiple-match than in the single-match condition. . . \". This is an example of failing to find an effect in the region where it was expected a priori, and then trying to expand the regions of interest post-hoc. Similarly, even when studying the same research question, researchers will sometimes trim the data, and sometimes not.", "Running till significance is reachedWe present three possible solutions to these problems. Linear mixed models can solve the multiple comparisons problem if all relevant research questions can be represented as parameters in one coherent hierarchical model (Gelman et al., 2012) , since the point estimates and their corresponding intervals are shifted toward each other via \"shrinkage\" or \"partial pooling\" (Gelman et al., 2012) . However, building a single hierarchical model that addresses all the research questions is not always trivial. For several regions of interest, it may be possible to fit a single model using Helmert contrasts (as in Nicenboim et al. (2015) ). This type of contrast compares each region with the average of the previous ones, such that it is possible to discover a change in the pattern of the effects. However, it is unclear if the effects should appear for all the trials in the same region, since some participants in some trials could start a certain process sooner predicting the structure of the item or could delay it due to fatigue, lapse of attention, or because they have not finished a previous cognitive process. Linear mixed models that can address the multiple measures in eye-tracking or the highly multidimensional data of EEG are even more difficult to specify."]}
{"intro": ["Real-time strategy (RTS) games are a popular test bed for artificial intelligence (AI) research, and platforms supporting such research continue to improve (e.g., [41] ). The RTS domain is challenging for AI due to real-time adversarial planning requirements within sequential, dynamic, and partially observable environments [30] . Since these constraints transfer to the real world, improvements in RTS agents can be applied to other domains, for example, mission planning and execution for AI systems trained to control a fleet of unmanned aerial vehicles (UAVs) in simulated environments [38] . However, the intersection of two complex domains, such as AI and flight, poses challenges: who is qualified to assess behaviors of such Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. IUI 2018, March 7-11, 2018 , Tokyo, Japan. Copyright \u00a9 2018 ACM ISBN 978-1-4503-4945-1/18/03 ...$15.00. http://dx.doi.org/10.1145/3172944.3172946 a system? For example, how can a domain expert, such as a flight specialist, assess whether the system is making its decisions for the right reasons?", "Our setting was StarCraft replay files. A StarCraft replay file contains an action history of a game, but no information about the players (i.e., no pictures of players and no voice audio). This anonymized set-up enabled us to tell our participants that one of the players was an AI agent. (We detail this design further in the Methodology section.) In addition, the participants had functionality to seek additional information about the replay, such as navigating around the game map, drilling down into production information, pausing, rewinding, fast-forwarding, and so on ( Figure 1 ). However, we wanted a higher level of abstraction than features specific to StarCraft. Specifically, we aimed for (1) applicability to other RTS environments, and (2) connection with other research about humans seeking information. To that end, we turned to Information Foraging Theory (IFT)."], "relatedWork": ["Ideally, mental models of a system would help people gain the understanding they need to assess an AI agent, but this is not always the case. Tullio et al. [39] examined mental models for a system that predicted the interruptibility of their managers. They found that the overall structure of their participants' mental models was largely unchanged over the 6 week study, although they did discount some initial misconceptions. However, their study did not deeply engage in explanation; it was mostly visualization. In other work, Bostandjiev et al. [3] studied a music recommendation system and found that explanation led to a remarkable increase in user-satisfaction. In an effort to improve mental models by increasing transparency of a machine learning system, Kulesza et al. [17] identified principles for explaining (in a \"white box\" fashion) how a machine learning based system makes its predictions more transparent to the user. In their study, participants using a prototype following the principles observed an improvement of their mental model quality by up to 52%.", "We drew upon Information Foraging Theory (IFT) to investigate the information that people would seek in the RTS domain. In IFT terms, when deciding where to forage for information, predators (our participants) make cost/benefit estimates, weighing the information value per time cost of staying in the current patch (location on the game map or tab with supplemental information) versus navigating to another patch [34] . However, predators are not omniscient: they decide based on their perceptions of the cost and value of the available options. Predators form these perceptions using their prior experience with similar patches [32] and the cues (signposts in their information environment like links and indicators) that point toward various patches. Of course, predators' perceived values and costs are often inaccurate [33] .", "IFT constructs have been used to understand humans' information-seeking behavior in other domains, particularly web navigation [6, 10] , debugging [9, 21, 32] , and other software development tasks [28, 31, 33, 36] . However, to our knowledge, it has not been used in RTS environments like StarCraft. Our paper aims to help fill this gap."], "rq": ["RQ1: The PreyFor example, Pair 4 and Pair 5 did not ask any Why or Why didn't questions at all. Instead, they made remarks like: Pair4-P7: \"the Zerg is doing what they normally do.\" Pair4-P8: \"[The agent is] kind of doing the standard things.\" Pair5-P10: \"This is a standard build.\" 4 We did not count the number of shoutcaster comments that answered this question because we could not narrow them down in this way. That is, although many of their comments could be said to be applicable to this type of question, the same comments were also applicable to more specific questions. However, in cases of the unexpected, a fourth What prey pattern arose, in which participants questioned the phenomena before them. We counted 9 What questions of this type: Pair9-P17: \"...interesting that it's not even using those.\" Pair10-P19: \"I don't get it, is he expanding?\" Pair10-P19: \"Wow, what is happening? This is a weird little dance we're doing.\" Pair10-P20: \"<when tracking military units> What the hell was that?\"", "RQ3: The Decisions and the CuesIn the RTS domain, players and intelligent agents make thousands of sequential decisions, and there is a paucity of literature that considers humans trying to understand AI decisions in such a setting. (A notable exception is McGregor et al. [25] .) There is, however, literature that starts with the AI's perspective: instances of its decision-making system components (i.e., neurons) that are interpretable by humans [44, 45] . In contrast, here we wanted to start with the human's perspective and the foraging paths that result from it: namely, how participants would identify behaviors that were not only potentially human interpretable, but also of interest.", "RQ3: The Decisions and the CuesInterestingly, participants had trouble with distractor cues even when the number of events competing for their attention was very low. For example, in the early stages of the game, players were focused on building economies and scouting. There was little to no fighting yet, so it was not the source of distracting cues. We were not surprised that the Expansion event at 13:45, when the game state had hundreds of objects and events, was the most often missed (5 instances). However, we were surprised that even when the game state was fairly simple -such as at 1:30 where the game had only 13 objects -participants missed the Expansion events. The extent of 5 Table 7 shows Pair 4 also finding eight Expansion decision points, but one of those is about the commitment to expand, based on building other structures to protect the base, rather than the action of building the base itself. 6 Reminder: Cues are signposts in the environment that the predator observes, such as rabbit tracks. Scent is what predators make of cues in their heads, such as thinking that rabbit tracks will lead to rabbits. distractibility the partipants showed even when so little was going on was beyond what we expected."]}
{"intro": [], "relatedWork": [], "rq": ["MOTIVATION AND RESEARCH QUESTIONSA guiding value of our work was to maintain the cultural authenticity of DotD in the game world. However, in designing a culturally faithful account of the festival, we faced challenges. One such challenge arose from the primary purpose of our game, which was to strengthen literacy. The world setting played a secondary role of support in motivating learners to continue playing the game. This meant that as designers we could not privilege a rich cultural exploration of the DotD. Such narrative constraints are inherent to serious games. Socialdrome, for example, aimed at developing children's social skills acquired in a fictional island called Cascara. While the island provided the context for numerous learning activities, its role was to motivate the learning goal of the game [23] . Thus, our first research question was: can cultural authenticity in game design be maintained by presenting children with a narrow perspective of the festival that retains its core message and rituals?", "Co-Existence of Cultural Authenticity and AppropriationWhen the children were asked to engage with new perspectives on the supernatural, they drew on cultural knowledge that the researchers had given them. We found that children could not easily conceive how the dead and the living communicate. Lacking their own points of reference, they strongly relied on the cultural rituals and customs of the DotD for proposing how this could happen. Our findings echoed one of the concerns associated with foreignizing as a localization approach, namely, that it led to the exoticizing of particular details [5] . In our case, those details concerned being pulled into the world of the dead, which encapsulates only a small part of the DotD. At the same time, we did not find much evidence that the other significant disadvantage of foreignizing, cultural alienation, was taking place. While children heavily relied on our descriptions of DotD rituals, they simultaneously utilized the rituals as a vantage point for understanding the meaning of the festival. Indeed, the festival's emphasis on remembrance, intimacy and acceptance were understood and enacted by most of the teams. In directing children's construction of stories by providing the purpose and outcomes of the festival, we were able to maintain some cultural authenticity. In drawing this conclusion, however, it is important to recognize that the workshops were run with participants from an ethnically diverse school. Children's pre-existing exposure to cultural diversity might help to explain why they were open to exploring the rituals and narratives of other cultures. Nevertheless, in line with our first research question, our findings are encouraging for games such as our own where narrative exploration is not an end, but serves as a means for motivating another primary objective. We thus argue that game narrative can communicate cultural authenticity as long as it retains components that exemplify its core cultural values."]}
{"intro": ["Scholars in the (Digital) humanities are active and motivated annotators [24] . They annotate all types of media at any level, with many layers of interpretation. Unsworth [21] identified annotating as one of the \"scholarly primitives.\" In the case of audio-visual (AV) or time-based media, manual or semi-automatic annotation of the media content is essential, given the fact that providing fully automated access is more challenging than to textual resources [15] . Scholarly work is also often described as a process [11] , where different stages occur over time (e.g., [4] ). Providing support to scholarly research requires the analysis of the complex research tasks where knowledge construction is involved. These are not limited to searching and retrieving a list of results, but also other series of scholarly primitives, e.g. classifying, linking [20] , comparing, sampling, illustrating, annotating [21] , or writing and collaborating [18] . However, little is known about how those complex tasks are performed in the context of scholarly research where AV media is the focal point, for instance, in media and communication studies. In addition, there is a lack of system support for the different Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. methods that this group of researchers uses in different research phases [9] . To understand how annotation should be supported along the research process of media scholars, different investigations are conducted within the CLARIAH project 1 , with a focus on research behavior, activities, models, and \"tool\" analyses. As part of these investigations, this paper presents two user studies about scholars' research processes, with a particular focus on the types of annotation-related activities. The research problem above results in the following research question: How is annotation of time-based media done in practice by media scholars, and other scholars who make intensive use of AV media, and in which stages of their research process is it used and how?"], "relatedWork": [], "rq": ["INTRODUCTIONScholars in the (Digital) humanities are active and motivated annotators [24] . They annotate all types of media at any level, with many layers of interpretation. Unsworth [21] identified annotating as one of the \"scholarly primitives.\" In the case of audio-visual (AV) or time-based media, manual or semi-automatic annotation of the media content is essential, given the fact that providing fully automated access is more challenging than to textual resources [15] . Scholarly work is also often described as a process [11] , where different stages occur over time (e.g., [4] ). Providing support to scholarly research requires the analysis of the complex research tasks where knowledge construction is involved. These are not limited to searching and retrieving a list of results, but also other series of scholarly primitives, e.g. classifying, linking [20] , comparing, sampling, illustrating, annotating [21] , or writing and collaborating [18] . However, little is known about how those complex tasks are performed in the context of scholarly research where AV media is the focal point, for instance, in media and communication studies. In addition, there is a lack of system support for the different Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. methods that this group of researchers uses in different research phases [9] . To understand how annotation should be supported along the research process of media scholars, different investigations are conducted within the CLARIAH project 1 , with a focus on research behavior, activities, models, and \"tool\" analyses. As part of these investigations, this paper presents two user studies about scholars' research processes, with a particular focus on the types of annotation-related activities. The research problem above results in the following research question: How is annotation of time-based media done in practice by media scholars, and other scholars who make intensive use of AV media, and in which stages of their research process is it used and how?", "(Re)search as processDetailing the process of scholarly research is done either via \"prescriptive\" models used in textbooks, or discovered via empirical studies of researchers' behavior. Case [3, p. 222 ] lists a 'classic' view on these stages, which typically start with imagining a research question (1), followed by determining what data are needed and designing a specific study to collect it (2), choosing and implementing research methods (3), analyzing and interpreting observations (4), and considering the overall results (5). The exact nature of these stages, however, is usually not as straightforward, and may vary across disciplines. Media scholars are at the intersection of humanities and social sciences [11] . The research process of social scientists is described in 'prescriptive' models, for instance, Kendall [12] , who details a series of \"steps\" which vary depending on the approach (inductive or deductive). Common steps in the two approaches include: problem definition, literature review, research design, data collection and analysis. In the humanities, on the other hand, the research process cannot necessarily be captured in a sequential model [11] . However, there is current empirical evidence that there are broad phases in the research process of media and communication scholars [2] , and of media scholars using web archives [9] . Bron et al. [2] explored this issue by investigating a group of twenty-seven media studies researchers. The authors found common parts in the research processes, summarized in three phases with different associated activities: exploration (studying background material, developing initial research questions, initial information gathering), contextualization (revising research questions, gathering material or selecting data with a focused purpose) and presentation (organizing the data and selecting appropriate evidence to build up a case). Marsden et al. [13] briefly discuss a schematic view of the research process with AV materials in a number of humanities disciplines, which included annotation.", "Presentation (A) (i) (ii) (B)Find keywords (2) Find/create Datasets (8, 9, 11) Figure 1: A process model of scholarly media annotation (numbers in parentheses correspond to Table 1) 4. FINDINGS Annotation activities in media scholarly research Table 1 summarizes the 21 activities identified in our first part of the analysis. We noticed that all participants indicated a number of activities centered on the creation of a corpus and its analysis. In terms of system support, for instance, while exploring a digital collection, scholars indicated the need for support in bookmarking, selecting groups of items after using faceted search or other filtering options, memory features such as query history, selection features for AV media fragments, and the addition of manual annotations, such as comments or tags. During analytic activities, even though the participants used different terms to explain their analysis methods, they often mentioned 'coding' or 'thematization'. In this stage, they also indicated desired functionalities and/or interface features related to refined segmentation, audio transcription, automatic image/audio analysis, fragment summarization, encoding, structuring relationships between coding terms, and linking back to original sources. Some scholars described that the analysis stage included a preparatory phase in which they defined their coding schemes, and/or made their data ready for analysis (e.g, by transcribing it). Our findings also confirmed that media and AV-centered scholars do not annotate time-based media in isolation, but in relation to other media (e.g., scripts, reviews, promotional materials, etc.). We noticed similar annotation activities as those supported by specialized qualitative data analysis software (QDA), see for instance [5] ). Table 2 shows the actual order that was followed by each researcher to accomplish the research project (the numbers refer to the activities in Table 1 ). This confirms previous studies about the lack of a strict sequence in the research process of humanists, and media and communication scholars (similar to Ellis' study of social scientists, as cited in Case [3, p. 291] ). Instead, research activities seem to be recurrent in the different research stages, but with a different degree of refinement. This is especially clear in the case of adding codes or tags to items or fragments, which is more loose in the initial exploratory stage, and more focused in the analysis stage. Likewise, some of the features above were suggested to have different purposes at different moments. For example, in initial research phases, visualization was used for exploratory data analysis. During the analysis, however, visualization was used to facilitate A model of the annotation process We grouped the 21 activities into categories, depicting them in a graphical conceptual representation, shown in Figure 1 . The categories include: the main generic research activities in the research process (a), also represented in (A) in the form of research phases. For representing this part, we departed from the model in [2] . However, in our data we observed that a great proportion of the activities was related to data analysis. Therefore, we extended the original sequence in Bron et al. [2] by adding \"analysis\" as a separate research phase. We also renamed the \"contextualization\" phase in Bron et al. [2] to \"Assembling\" to be more specific. The most important part of the figure, (B), shows the annotation-related activities, which we grouped into phases, from pre-focused annotation to the creation of new information objects. We used the concepts of \"pre-focused\" and \"fo- studies 1,3,9,11,7,12,13,17,18,19,21 cused\" from [22] to name the annotation-related stages. Indeed, previous research has found that scholars usually create their own set of semantic categories [25] , which correspond to their research questions. We grouped the other research activities in four layers: (i) connected to information seeking and searching (not necessarily system-mediated), (ii) broader stages related to the corpus, and (iii) more specific data processing related activities around the creation and annotation of those corpora."]}
{"intro": [], "relatedWork": [], "rq": ["Performance Impressions and Attractiveness (RQ3)The correlation analysis of attractiveness attributes and performance impression yielded unexpected results (Table 6 ). It was observed that attractive was not significantly correlated to performance impression, while friendly (r = \u22120.27) and likeable (r = \u22120.26) had low negatively correlation (p < 0.05). Dislikeable was observed to have low positive correlation (r = 0.17; p < 0.05). Given the literature on gender and attractiveness and job performance [28, 50] , we divided the sample of receptionists based on gender. It was observed that for males (N = 79) there was no correlation between any attractiveness attributes and performance impression (r \u2208 [0.01, \u22120.05]). However, for the female receptionists (N = 90), friendly (r = \u22120.47) and likeable (r = \u22120.48) was negatively correlated to performance impression (p < 0.001), while dislikeable was positively correlated (r = 0.40; p < 0.001). This result does not conform several of the results reported in the literature of attractiveness and performance, where a positive connection was often found [23, 50] . For further discussion, refer to Section 6.3."]}
{"intro": ["Many people get recommendations for movies, music, articles, and products through their social connections both online and off. Online, we often think of sharing primarily as a public broadcast through tweets, status updates, and the like. Much online sharing, however, is narrower, targeted at specific audiences (as with Google+ circles or Pinterest boards) or directed [5] at specific individuals through email, chat, and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. person-to-person messages (e.g. suggesting movies on Netflix [17] ). Recent studies show that sharing content through email is still popular [5, 1] , surpassing social media in certain product categories [24] .", "Recommender systems within social networks will also benefit from better models of sharing decisions. These models can be used to support sharing online by suggesting which items to share and who to share them with [5, 33] . However, while there is extensive research on understanding people's rating behavior and predicting their preferences for items [29] , little is known about people's online sharing behavior and its predictability.", "Based on the primary motivations of individuation and altruism, we can expect people to share content that is some balance of their own and others' interests. What that balance is, and how it comes to be so, however, is an open question and our main focus in this paper. A recent study on Twitter suggests that the balance is tilted toward the self: around 80% of people primarily share content about their activities and opinions, while only 20% share informational content more likely to be useful to others [23] . However, this may be because of the broadcast nature of sharing on Twitter where in the absence of a specific audience, sharing becomes an expression of one's thoughts and ideas [36] . When people share to specific recipients, they may be more likely to think about usefulness for the recipient (and thus be more altruistic) than when they share to larger groups [4, 5] ."], "relatedWork": [], "rq": ["DISCUSSIONWhen broadcasting as on Twitter, past research shows that most people post messages about themselves rather than sharing useful information [23] . Based on results in communication around tuning messages for the audience [19] and recent work showing that people think more about usefulness for the recipient as the audience size decreases [4] , we expected people would weigh recipients' preferences more when sharing to an individual. However, both people's sharing data and their self-reports underscore the importance of their own preferences. For RQ1, the answer is clearly that sharing is more driven by people's own preferences than recipients'."]}
{"intro": [], "relatedWork": [], "rq": ["EVALUATION OF INJECT IN 3 LOCAL NEWSPAPERSAll 3 newsrooms used the Adobe InCopy text editor, so the InCopy version of the INJECT sidebar was made available. However, a different third-party organisation that was contracted by the 3 newsrooms did not integrate it into the editor. Instead, all of the journalists were set up to use the web application version shown in Figure 3 in a separate browser window. The evaluation investigated whether journalists in the newsrooms produced articles that were: (RQ1) more novel and valuable with support from the INJECT tool, and: (RQ2) written more productively with this support. These qualities of novelty and value mapped to the requirements for news to be surprising and relevant revealed during a review of key qualities of news reported in 2016 [32] . It also investigated whether: (RQ3) factors such as increased newsroom autonomy, a work culture open to innovation, management support to train journalists and set up success conditions and the presence of innovative individuals [30] influenced the use and the effectiveness of the INJECT tool."]}
{"intro": ["Several previous authors have done preliminary investigations into the structure of language representations:\u00d6stling and Tiedemann (2017), Malaviya, Neubig, and Littell (2017) , and Johnson et al. (2017) in the context of language modeling and machine translation, all of them using multilingual data. In this work we follow up on the findings of Rabinovich, Ordan, and Wintner (2017) , who, by using language representations consisting of manually specified feature vectors, find that the structure of a language representation space is approximately preserved by translation. However, their analysis only stretches as far as finding a correlation between their language representations and genetic distance, even though the latter is correlated to several other factors. We apply a multilingual language model to this problem, and evaluate the learned representations against a set of three language properties: (i) genetic distance (families), (ii) a novel measure of syntactic similarity (structural), and (iii) distance of language communities (geographical). We investigate:", "3 Method Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005 ). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate."], "relatedWork": [], "rq": ["Introduction3 Method Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005 ). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.", "Genetic DistanceFollowing Rabinovich et al. (2017), we use phylogenetic trees from Serva and Petroni (2008) s and learn an Indo-European (IE) family their language representations. Crucially, t that the relationships found between their ations encode the genetic relationships languages. They use features based on s of POS tags, function words and cohesive We significantly expand on this work by g three language similarity measures ( \u00a74). this, we offer a stronger explanation of what representations really represent. hod illustrates the data and problem we consider per. We are given a set of English goldtranslations from the official languages of pean Union, based on speeches from the Parliament. 1 We wish to learn language ations based on this data, and investigate the relationships which hold between the resultsentations (RQ2). For this to make sense, it nt to abstract away from the surface forms of ations as, e.g., speakers from certain regions to talk about the same issues. We therefore several levels of abstraction: i) training on s the exact same data as used by Rabinovich et al. inating from Europarl (Koehn, 2005) . a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate."]}
{"intro": ["An important part of a carpentry student's education involves trigonometry, which is needed for e.g. building roofs and bay windows. However, most carpentry students do not choose carpentry because of their interest in mathematics, but may instead find it difficult and demotivating. In this article we present a software system developed for the iOS platform using the iPhone. The system was developed for a vocational education, with focus on carpentry students and their teachers. The design process was iterative and participatory, involving students and teachers from day one [10] . The goals of the system were not fixed beforehand but emerged during the study, partly from discussion and partly from users' experiences when they tested the system at each iteration of the design process. We believe that smartphones and games can enrich mathematics teaching and have a positive effect on the motivation of students. Smartphones as educational tools is a new area for exploration, their sensors providing exciting possibilities for interaction and participation [8] . We understand learning to be a social practice where the learner is an active participator [14] . Game elements can furthermore enrich the learning process by providing motivation [7] . The study was done in collaboration with two vocational schools: Syddansk Erhvervsskole (SDE) and Erhvervsuddannelsescenter Vest (EUC); the publisher of mathematics books Erhvervsskolernes Forlag (EF); and the University of Southern Denmark. Our research question is: How can smartphones enrich mathematics teaching in vocational education? Including the question of how iterative and participatory design enriches the product. We begin by describing the iterative participatory design method and its roots, and presenting the educational game Math Mission. We describe the games designed during this study along with some of the development rationale. Then we take readers briefly through the design process: the initial field studies, the three user trials, and the final trial using naive users. User and teacher comments and our observation of game trials enabled us to improve the system in relation to usability, emerging learning goals, evaluation of the interaction with the smartphone's sensors, and gaming value."], "relatedWork": [], "rq": ["IntroductionAn important part of a carpentry student's education involves trigonometry, which is needed for e.g. building roofs and bay windows. However, most carpentry students do not choose carpentry because of their interest in mathematics, but may instead find it difficult and demotivating. In this article we present a software system developed for the iOS platform using the iPhone. The system was developed for a vocational education, with focus on carpentry students and their teachers. The design process was iterative and participatory, involving students and teachers from day one [10] . The goals of the system were not fixed beforehand but emerged during the study, partly from discussion and partly from users' experiences when they tested the system at each iteration of the design process. We believe that smartphones and games can enrich mathematics teaching and have a positive effect on the motivation of students. Smartphones as educational tools is a new area for exploration, their sensors providing exciting possibilities for interaction and participation [8] . We understand learning to be a social practice where the learner is an active participator [14] . Game elements can furthermore enrich the learning process by providing motivation [7] . The study was done in collaboration with two vocational schools: Syddansk Erhvervsskole (SDE) and Erhvervsuddannelsescenter Vest (EUC); the publisher of mathematics books Erhvervsskolernes Forlag (EF); and the University of Southern Denmark. Our research question is: How can smartphones enrich mathematics teaching in vocational education? Including the question of how iterative and participatory design enriches the product. We begin by describing the iterative participatory design method and its roots, and presenting the educational game Math Mission. We describe the games designed during this study along with some of the development rationale. Then we take readers briefly through the design process: the initial field studies, the three user trials, and the final trial using naive users. User and teacher comments and our observation of game trials enabled us to improve the system in relation to usability, emerging learning goals, evaluation of the interaction with the smartphone's sensors, and gaming value."]}
{"intro": [], "relatedWork": [], "rq": ["MethodThe research question asked in this study is exploratory in nature. Typically, verbal protocols and/or observation techniques are used to gather exploratory data (Gilmore, 1990) . However, in this case, due to the nature of the research question, these approaches were not deemed appropriate. The coding of verbal protocols and observations allows the preconceptions of the researcher to possibly in#uence the results (Ericsson & Simon, 1984) . In this case, the researchers had many years of experience in both traditional and object-oriented approaches to systems development. (One of the researchers had 8 years of object-oriented experience.) As such, to address the research questions, the researchers chose to minimize researcher in#uence by using a cognitive mapping approach (Axlerod, 1976; Hu!, 1990; Eden, Ackermann & Cropper, 1992; Jones, 1995) . Speci\"cally, we used the self-Q technique (Bougon, Weick & Binkhorst, 1977; Weick & Bougon, 1986) . It allows participants to identify their perceptions while minimizing the potential for researcher bias during the data collection process. Due to the intensive nature of uncovering the participants' perceptions of the di$culties of using OO techniques, this type of approach seems appropriate."]}
{"intro": [], "relatedWork": [], "rq": ["DiscussionThe last research question aimed to investigate the relationship between collocations and meaning. As pointed out by Webb and Kagimoto (2009) , there was a valid comparison between collocations and meaning on productive tests because, these tests had a similar format and they were not likely to be affected by any of the other tests. However, because of their different format (one was a multiple-choice test and the other was a translation test) the receptive knowledge of collocations and meaning tests may not have an accurate comparison. They found that the mean scores of both groups on the productive knowledge of meaning test were slightly higher than the scores on the productive knowledge of collocation test. However, in this study the mean scores of both receptive and productive task groups on the productive meaning and collocation tests were nearly the same. The mean scores of all participants on the productive knowledge of collocation test were 7.99 using the strict scoring system and 8.77 using the sensitive scoring system. This means that all of the participants knew 40% of the collocations productively according to the results of the strict scoring system. This rate increased to 44% in the sensitive scoring system. The mean scores of all participants on the productive knowledge of meaning test were 8.95 and 7.94, using the sensitive and strict scoring systems respectively. This indicates that the participants knew 40-45% of the meaning of collocations productively. As it can clearly be seen, the rates of productive knowledge of meaning and collocation were nearly the same in this study. This may have resulted from the treatment stage. As the collocations were encountered with their L1 meanings in the treatment and lower level learners pay great attention to the L1 meanings of vocabulary items, they could remember them as well as the collocation itself."]}
{"intro": ["Code-sharing sites like GitHub hold the promise of documenting all the common and uncommon ways of using an API in practice, including many alternative usage scenarios that are not typically shown in curated examples. However, given the large amount of code available online, it is challenging for developers to efficiently browse the enormous volume of search results. It is certainly infeasible for developers to examine more than a few code examples simultaneously. In practice, programmers often investigate a handful of search results and return to their own code due to limited time and attention [3, 23, 5] . Prior work has shown that individual code examples may suffer from API usage violations [29] , insecure coding practices [7] , unchecked obsolete usage [31] , and comprehension difficulties [25] . Therefore, inspecting a few examples may leave out critical safety checks or desirable usage scenarios.", "In the software engineering community, there is a growing interest in leveraging a large collection of open source repositories-so called Big Code-to automatically infer API usage patterns from massive corpora [4, 16, 26, 30] . However, these API usage mining techniques provide limited support to help programmers explore concrete code examples from which API usage patterns are inferred, and understand the commonalities and variances across different uses. To bridge the gap, we aim to visualize hundreds of concrete code examples mined from massive code corpora in a way that reveals their commonalities and variances, and design a navigation model to guide the exploration of these examples. We draw motivation from prior work on visualizing large corpora of related documents, e.g., student coding assignments [8] , text [27, 21] , and image manipulation tutorials [17] , to pose the following research question: How might we extract, align, canonicalize, and display large numbers of usage examples for a given API?"], "relatedWork": [], "rq": ["INTRODUCTIONIn the software engineering community, there is a growing interest in leveraging a large collection of open source repositories-so called Big Code-to automatically infer API usage patterns from massive corpora [4, 16, 26, 30] . However, these API usage mining techniques provide limited support to help programmers explore concrete code examples from which API usage patterns are inferred, and understand the commonalities and variances across different uses. To bridge the gap, we aim to visualize hundreds of concrete code examples mined from massive code corpora in a way that reveals their commonalities and variances, and design a navigation model to guide the exploration of these examples. We draw motivation from prior work on visualizing large corpora of related documents, e.g., student coding assignments [8] , text [27, 21] , and image manipulation tutorials [17] , to pose the following research question: How might we extract, align, canonicalize, and display large numbers of usage examples for a given API?"]}
{"intro": [], "relatedWork": [], "rq": ["The CALLAS ProjectOne of the research questions raised by CALLAS is the interpretation, understanding, and fusion of the multimodal sensor inputs. However, since multimodal data corpora with emotional content are rather rare, effort was made to create a setup, which simplifies the task of data acquisition. In one experiment, which targets the mapping between gestures/body movements and emotion, and their relation to other modalities, such as affective speech and mimics, user interaction is captured with two cameras, one focused on his head and one on his whole body, and a microphone near the head. Additionally the users interact with different devices, such as Nintendo's Wii Remote or a data glove by HumanWare 4 . To elicit the desired target emotion a procedure inspired by the Velten emotion induction method is used: first, a sentence with a clear emotional message is displayed and the user is given sufficient time to read it silently. Then the projection turns blank and the user is asked to express the according emotion through gesture and speech. It is up to the user to use own words or to say something, which is similar to the displayed sentence. Figure 2 illustrates the setting."]}
{"intro": [], "relatedWork": [], "rq": ["Waldo: A Marine Geophysics Sustained AggregationWaldo's PG works closely with another PG as part of a long-term Sustained Aggregation throughout data collection, processing, and analysis while invoking one FO directly, and another implicitly through the actions of the Ocean Seismometer FO, in the data collection process. Waldo and his PI colleagues have a longstanding relationship to work to acquire funds and produce resources. The PIs who invoked this Sustained Aggregation and undertook the Ridge Experiment leverage their decades long relationships and their continuing desire to have their respective PGs work together to carry on and use this Sustained Aggregation, even as students within come and go as their respective research careers emerge and transform over time. In our effort to categorize this form of organizing it might at first glance be reasonable to perceive this work as a Federation entity. In our inquiry, however, there was not a formalized organizational structure in place nor a charter specifying rules for membership. Waldo and his two PI colleagues are choosing to maintain and sustain alignment of a close set of working relationships between their respective groups. The work of Martin's PG invokes multiple different entities with varying degrees of planned permanence and formality of organizing to collect and analyze data to study the evolution of HIV, Fig. 4 . Martin's PG conducts molecular and computational work where individuals work with physical samples (e.g., blood and plasma) to produce genetic sequences that can be analyzed computationally. This microbiological work is conducted as part of different Federation entities over time, but the dayto-day activities unfold within the context of this PG or in alignment with Intermittent Exchanges. The Federations handle work ranging from enrolling HIV infected patients in cohort studies and regularly collecting blood samples to analyzing blood samples for different phenomena and developing new vaccines. The various IEs are created when a resource is needed to accomplish a particular research activity, whether using a costly sequencing machine or using outside statistical expertise for analyses that are ultimately not sustained over time.]-> Martin's PG invokes Federation, FO, and IE entities in their work collecting data. This process begins with a research scientist in Martin's PG examining records kept by one of the Federation entities they work with to see if a sample necessary to address a particular research question is available and contains enough quantity of virus to make it viable and cost-effective to use. The PG member does this by examining the metadata stored by the Federation. If the research scientist proceeds with a given sample, then they will use one of the different wet lab protocols (Lynch 2002 ) that Martin's PG has designed to \"work it up\" so that it can be sequenced, a point in which they invoke additional entities."]}
{"intro": ["In the 2016-2017 campaign Wikimedia raised $91 million from 6.1 million donations, an increase from the 2015-2016 campaign where $77 million was raised. The donation-based revenue is especially important and useful for these sites that value content neutrality [2, 22] ; not relying on advertisement revenue can help preserve their process integrity and independence 1 . One of the primary strategies to solicit donation is through fundraising banners [26] . As stated in their Fundraising Report (2016) (2017) , banners are an integral part of their fundraising model, which \"facilitates the transition from reading Wikipedia, to wanting to contribute to the site through a donation.\" [55] Further, through their survey studies, they have found that \"Wikipedia readers don't consider our fundraising content too intrusive or aggressive.\" But as with all fundraising campaigns, there continues to be a concern of burnout [28, 30] . Through extensive A/B testing of banner content, the fundraising team has been able to improve the overall efficiency of these banners, reducing both the banner impressions during the campaign, as well as the amount of time to reach the fundraising goal. However, an important part of the banner-based fundraising, that may have been overlooked, is how properties of specific pages on which banners are displayed influence or predict donation behavior. Different pages may draw different visitors for different reasons: the \"Kim Kardashian\" page on Wikipedia may be attracting a different type of visitor with a different purpose than the \"Computer-supported cooperative work\" page. Are there potential systematic differences in donation rates across pages? What are some of these differences? Exploring these questions may lead to more effective use of donation banners that are tailored to page contents. This can further improve the efficacy of these fundraising efforts while minimizing burnout. In this work, we explore how different properties of a Wikipedia page predict donation behaviors. Specifically, using aggregated banner fundraising data from the 2015-2016 campaign for both the English and French language versions, we tested how topic category, page quality and type, and page dwell time affects donation. Our findings suggest a reciprocity hypothesis in donation: that people give because they received or anticipate future benefits [38,40:5] . In this context, the benefits can stem from content that offers task-oriented utility [27] . Congruent with this view, we found that articles of higher quality (e.g., completeness, informativeness, and accuracy [50] ), and those that provide task-oriented contents (i.e., articles related to academic or professional activities than to leisure, reading of which has also been found to be more often motivated by work or school tasks [44] ) attract a higher rate of donation. Relatedly, we found that pages that ask users for extra interaction steps to reach the contents lead to a much lower donation rates. These are pages such as redirect, list and disambiguation that may be thought of as impeding users' task progress. These have a much more negative impact on donation rates than even the pages with low quality content. This work's main contributions are the following: 1) Using real-life Wikipedia donation datasets for two language versions we show the existence of a reciprocity mechanism in donation."], "relatedWork": [], "rq": ["RECIPROCITY AND DONATIONWhat is yet to be studied, and the main focus of this work, is whether these potential systematic differences in users and usages would lead to different donation behaviors on these pages and what page-level features may predict that. Consequently, our main research question is framed as follows: RQ. Do donations across Wikipedia pages vary in some systematic ways? Next we turn to the literature on charitable donations to understand how donation rates might vary across pages. Much prior research has studied why people make charitable contributions [2, 22, 40] . These works have identified a number of motivations people might have when making a donation, such as: altruism -increasing another's welfare without any external rewards [6] ; impure altruism -giving motivated by increasing one's positive emotional feeling or warm-glow [1, 2] ; peer pressure, authority [46] ; prestige, respect, friendship, and other psychological objectives [4, 37] ; social acclaim or avoidance of scorn [3, 35] ; image or reputation considerations [42] , increase in one's self-esteem [13] , as well as income or tax benefits [13, 36] . Surveys have also been conducted specifically on Wikipedia for why people do and do not donate to Wikipedia [18] indicating important motivations such as passion for organization's mission or cause. Furthermore, experiments conducted by Wikipedia itself showed that e.g., a message from the site's founder, Jimmy Wales, was more likely to encourage a donation than similar messages not attributed to Wales 2 , which relates to the principle of authority mentioned earlier [46] . This body of literature suggests a number of factors that influence or predict people's likelihood to donate. However, many of these factors would not suggest that rates of donation contribution would vary systematically across pages on Wikipedia. First, some of these factors may not be applicable in the context of Wikipedia banner campaigns. For example, peer pressure or friendship should not be a factor in this context as these donation solicitations are coming from Wikimedia as an organization. Similarly image motivation or reputation considerations as well as avoidance of scorn or desire to receive social acclaim or prestige should not be motivations here, as people's contributions in the banner campaigns are not publicized by default (people can opt to share it with their social network pages. Prime examples are impure altruism and self-esteem motivations as there is no reason why a particular page would trigger a stronger positive emotional feeling or warm-glow after donation than any other page. Similarly the appeal of authority, such as a Wikipedia's founder, should have the same effect regardless of the page on which it is presented, as this authority represents and promotes Wikipedia as an entire organization. Same is true when considering a motivation to support Wikipedia's cause or mission statement, both stay the same and there is no reason why certain pages would lead to higher motivation related to organization's cause than others. Finally income or tax benefits are naturally not affected by a particular page on which a donation was given. On the other hand, one reason for donation that has been widely studied and that could predict differences in donation contributions across pages is reciprocity. Reciprocity is a social norm. Reciprocity suggests that people should respond in-kind to others: help others who help them [19] . Extended to the context of charitable giving, people may be both compelled by the reciprocity motive because \"they have benefited from the charities' activities in the past or anticipate the need for their services in the future.\" [13] . Prior work from both controlled experiments and field studies have demonstrated the consistency and strength of reciprocity [10, 16, 24, 29] . Research has found that given a reciprocity motive, the way to increase one's compliance with a request is by increasing their sense of indebtedness. For example, in a controlled study, participants who received a soft drink from a confederate were more likely to comply with their requests [41] . Similar effects have also been extended in field studies of charitable contributions. For example, in a field study soliciting donations, experimenters found that offering a small gift increased frequency of donations by 17 percent, and that offering a big gift increased frequency of donation by a whopping 75 percent [15] ."]}
{"intro": [], "relatedWork": [], "rq": ["Utilities-Is it beneficial to use the CoDL algorithm with the hard-EM approach, which finds the best assignment of the hidden variables, as opposed to EM, which calculates the full posterior distribution? -How is the CCM approach compared to other approaches? First, we ask the question about what are the benefits of using CoDL as opposed to the standard EM and hard EM algorithms. We then compare CoDL to other approaches of encoding long distance relationships. Note that we can often design a heavily engineered and tailored model for a specific task. However, this process is challenging and time consuming, and must be repeated for every new task. On the other hand, CCMs provide an easy to use model-specification language that works for all tasks. Therefore, we compare HMM CCM to several tailored models to see if our general purpose model can match the performance of a specifically designed model. It is natural to expect that a tailored model will perform at least as well as CoDL; however, if CoDL matches the performance of a tailored model, we consider it a success. We also compared the results to recent approaches of using expectation constraints (Bellare et al. 2009 ). Among all of the research questions, the most important one is to verify whether adding constraints can improve the models or not. Again, while CCMs are not the only way to incorporate constraints, they provide a nice interface so that users do not need to invent a tailored model for every task."]}
{"intro": [], "relatedWork": [], "rq": ["RQ3: Is the DU map useful for choosing EA parameters or components?When using the LT, usage follows a similar pattern, but the evolution of diversity is dramatically different. Here, GOMEA LT exhibits a markedly different convergence behavior compared to GOMEA RT and GOMEA RTd . Due to the high selection pressure of GOM, the pattern of nodes with a positive contribution to the fitness quickly spread in the population, and the interdependencies among those nodes are captured by mutual information. Because the LT is built from this information the hierarchical crossover masks are made to mix individuals according to these patterns. Therefore, there is a mutual, reinforced convergence of genotype and the structure of the LT itself in GOMEA LT , which results in the rapid loss of diversity. This can be clearly seen in the DU map. It should be noted that this type of diversity loss is desirable as it is the result of extremely effective mixing behavior as a result of the right patterns being present in the population and being correctly modeled by the LT. Indeed, the fast convergence of GOMEA LT does not compromise the success rate when compared with GOMEA RT and GOMEA RTd . Rather, using the LT results in the second-best performance. This means that, almost half of the times (0.4 success rate), the right patterns are present in the population and are correctly modeled by the FOS. We hypothesize that either the correct information is present in the population, and a perfect solution is quickly found, or the wrong information is modeled and GOMEA LT quickly converges to a suboptimal solution. If a larger population size were to be chosen, the performance of GOMEA LT will increase because it enables more robust learning of the salient linkage information, and the consequent propagation of the correct patterns of nodes. This result was experimentally shown on different problems by adopting a framework of multiple interleaved runs with increasing population size (and tree height) [68] . Here, we consider only a single population size however. By looking at the DU map of GOMEA LT , we can see that, on average, the population almost completely converges in roughly the first quarter of the evolution. The improvement that can be achieved during the subsequent generations is likely to be minimal. This is a key insight to improve the performance of GOMEA LT : given the same budget (i.e., evaluations or time), it is better to use a bigger population size for less generations."]}
{"intro": ["We examine the effects of our sensemaking translucence interface in a laboratory study in which pairs of remote participants role-played detectives collaborating to solve a serial killer crime [2] . Half of the pairs used the sensemaking translucence interface and the other half used a standard interface [22] used in previous studies [19, 21] . As we will show, pairs using the sensemaking translucence interface were significantly better than those using the standard interface at uncovering pertinent clues and identifying the serial killer. However, participants viewed the sensemaking translucence interface as less valuable than the standard interface in terms of helping them focus their attention, develop hypotheses, or collaborate with their partners. The findings suggest that designers of collaborative analysis interfaces may want to incorporate real-time feedback to users about how the features of the interface are beneficial to their sensemaking processes."], "relatedWork": [], "rq": ["Study HypothesesHowever, sensemaking translucence may also come with costs. Analysts may feel compelled to share preliminary thoughts, and read their partners' emergent hypotheses. This may increase the cognitive demand of the crimesolving task. On the other hand, by reducing the need for explicit verbal sharing of information, our interface may reduce the time and effort required for the task [14, 47] . There is also a potential for the suspect visualization to be distracting. Since the direction of impact is unclear, we pose a research question:"]}
{"intro": [], "relatedWork": [], "rq": ["Conclusion & Future Work Conclusion & Future WorkWe have presented two different approaches to address learning and practice in pointing device evaluations. We think longitudinal designs are a must in such studies. Learning or practice effects clearly are not easy to come by and only longitudinal designs provide the flexibility to distinguish between learning of the device and the task. However, more research is needed regarding useful transfer and retention tasks that are able to address this issue. Longitudinal designs could also give more insight into usage strategies and how these might change over time. Besides, other influencing factors such as the motivation of users have to be considered. Since input device experiments rely on very basic tasks, they are currently very dependent on a controlled environment. Thereby, they often lack ecologic validity and so future research should also investigate how to evaluate input devices in the field using longitudinal designs (e.g. in combination with diaries [10] ). Finally, the overall goal should be to develop a framework of longitudinal evaluation methods in HCI. While the social sciences have developed a methodological framework for longitudinal research during the last decades, distinguishing between several different approaches and stating which type of research questions demand which kind of approach or method and the appropriate analysis method [e.g. 11], such a framework is still missing for human-computer studies."]}
{"intro": ["Several previous authors have done preliminary investigations into the structure of language representations:\u00d6stling and Tiedemann (2017), Malaviya, Neubig, and Littell (2017) , and Johnson et al. (2017) in the context of language modeling and machine translation, all of them using multilingual data. In this work we follow up on the findings of Rabinovich, Ordan, and Wintner (2017) , who, by using language representations consisting of manually specified feature vectors, find that the structure of a language representation space is approximately preserved by translation. However, their analysis only stretches as far as finding a correlation between their language representations and genetic distance, even though the latter is correlated to several other factors. We apply a multilingual language model to this problem, and evaluate the learned representations against a set of three language properties: (i) genetic distance (families), (ii) a novel measure of syntactic similarity (structural), and (iii) distance of language communities (geographical). We investigate: the Europarl corpus (Koehn, 2005) . They employ a feature-engineering approach to predict source languages and learn an Indo-European (IE) family tree using their language representations. Crucially, they posit that the relationships found between their representations encode the genetic relationships between languages. They use features based on sequences of POS tags, function words and cohesive markers. We significantly expand on this work by comparing three language similarity measures ( \u00a74). By doing this, we offer a stronger explanation of what language representations really represent. Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate."], "relatedWork": [], "rq": ["IntroductionSeveral previous authors have done preliminary investigations into the structure of language representations:\u00d6stling and Tiedemann (2017), Malaviya, Neubig, and Littell (2017) , and Johnson et al. (2017) in the context of language modeling and machine translation, all of them using multilingual data. In this work we follow up on the findings of Rabinovich, Ordan, and Wintner (2017) , who, by using language representations consisting of manually specified feature vectors, find that the structure of a language representation space is approximately preserved by translation. However, their analysis only stretches as far as finding a correlation between their language representations and genetic distance, even though the latter is correlated to several other factors. We apply a multilingual language model to this problem, and evaluate the learned representations against a set of three language properties: (i) genetic distance (families), (ii) a novel measure of syntactic similarity (structural), and (iii) distance of language communities (geographical). We investigate: the Europarl corpus (Koehn, 2005) . They employ a feature-engineering approach to predict source languages and learn an Indo-European (IE) family tree using their language representations. Crucially, they posit that the relationships found between their representations encode the genetic relationships between languages. They use features based on sequences of POS tags, function words and cohesive markers. We significantly expand on this work by comparing three language similarity measures ( \u00a74). By doing this, we offer a stronger explanation of what language representations really represent. Figure 2 illustrates the data and problem we consider in this paper. We are given a set of English goldstandard translations from the official languages of the European Union, based on speeches from the European Parliament. 1 We wish to learn language representations based on this data, and investigate the linguistic relationships which hold between the resulting representations (RQ2). For this to make sense, it is important to abstract away from the surface forms of the translations as, e.g., speakers from certain regions will tend to talk about the same issues. We therefore introduce several levels of abstraction: i) training on 1 This is the exact same data as used by Rabinovich et al. (2017) , originating from Europarl (Koehn, 2005). multilingual language model using a 2-layer LSTM, with the modification that each time-step includes a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate.", "Genetic DistanceFollowing Rabinovich et al. (2017), we use phylogenetic trees from Serva and Petroni (2008) hod illustrates the data and problem we consider per. We are given a set of English goldtranslations from the official languages of pean Union, based on speeches from the Parliament. 1 We wish to learn language ations based on this data, and investigate the relationships which hold between the resultsentations (RQ2). For this to make sense, it nt to abstract away from the surface forms of ations as, e.g., speakers from certain regions to talk about the same issues. We therefore several levels of abstraction: i) training on s the exact same data as used by Rabinovich et al. inating from Europarl (Koehn, 2005) . a representation of the language at hand. That is to say, each input to their LSTM is represented both by a character representation, c, and a language representation, l 2L. Since the set of language representations L is updated during training, the resulting representations encode linguistic properties of the languages. Whereas\u00d6stling and Tiedemann (2017) model hundreds of languages, we model only English -however, we redefine L to be the set of source languages from which our translations originate."]}
{"intro": ["The use of tabletops for co-located collaborative search is an ongoing topic in HCI research [18] . Tabletops can offer diverse benefits and potentials for collaborative search such as a closer face-to-face collaboration and more equitable working style [22] , an increased awareness and better group work experience [1] , and a horizontal form-factor whose affordances are well-suited to follow-up activities (e.g. sorting, sensemaking, making a purchasing decision) [1, 18] . However, other potentials of tabletops for search are still unexplored, e.g. the use of \"hybrid surfaces\" like [14, 25] that use tangible interaction with physical props in combination with multi-touch [15] . Except a single design study for video search [11] , such hybrid tabletop interaction has not been used in search scenarios yet. Furthermore, in the light of the popularity of tabletops (e.g. Microsoft Surface) in showrooms or flagship stores, it is surprising that no prior research has focused on the obvious task of collaborative search for products in a retail environment. In this paper we therefore present Facet-Streams (Figure 1 ), a novel design for collaborative faceted product search. It uses a hybrid interactive surface that combines information visualization techniques (a filter/flow metaphor [28] ) with tangible and multi-touch interaction to materialize collaborative search on a tabletop. Thereby, unlike in most previous work, our notion of search does not mean to populate an empty workspace with the results from a keyword search. Instead we mean a process of faceted collaborative filtering of a product catalog until the amount of results is sufficiently small to review and decide [10] . Furthermore, in a retail environment like a flagship store a \"good\" customer experience with \"soft\" factors such as fun, innovative design and social experience is often valued over \"hard\" factors such as task completion times and rates. Our work has therefore been guided by three research questions: (Q1) Does our design turn collaborative product search into a fun and social experience with increased group awareness? (Q2) Can we support the great variety of different search strategies and collaboration styles in different teams with a simple but flexible design? (Q3) Can we harness the expressive power of facets and Boolean logic without exposing users to complex formal notations? In the following, we discuss related work and the specifics of our context of use. Then, we introduce Facet-Streams and the underlying design rationale. We describe two user studies and discuss their results in terms of user experience, collaboration styles, and awareness. We conclude by summarizing our results and discussing them with respect to our research questions."], "relatedWork": ["(1) Morris et al. provide a comprehensive overview and analysis of tabletop search systems along dimensions such as search input, collaboration style and application domain [18] : Regarding search input, Facet-Streams is the first approach that uses hybrid surfaces with tangible and touch interaction. All previous approaches entirely rely on touch, mouse, or keyboard input without making use of any physical props as tangible user interface elements. Regarding the collaboration style, Facet-Streams is similar to FourBySix Search [9] , Cambiera [13] , and WeSearch [19] which all support seamless transitions between tightlycoupled collaboration and loosely-coupled parallel work. However, unlike these applications, Facet-Streams does not use keyword search for Web, document, or multimedia retrieval but uses a visual and tangible query language for faceted search. Thus, Facet-Streams shares commonalities with TeamSearch that also creates a faceted search experience based on Boolean-style AND queries on tagged photo collections [17] . Like TeamSearch, we use circular widgets to specify categorical criteria (or facets) but aim at a far greater query expressivity with arbitrary numbers and logical combinations of such widgets including AND and OR. Furthermore, we do not restrict users to only formulate either personal queries or collective queries. Instead we want to enable them to develop multiple personal and collective queries in parallel and to freely shift criteria between them for maximum flexibility in strategies and collaboration styles. A further fundamental difference between previous work and our design is the employed notion of search. Except TeamSearch and PDH [24] all systems in [18] increasingly populate the collaborative workspace with the results of keyword searches. Thereby search has the notion of adding result sets to the shared workspace. In contrast, we want to follow a faceted search approach [20] where search means narrowing down the entirety of products in the workspace to the desired subset. Thus the focus of collaboration in Facet-Streams lies on the formulation and logical combination of the desired facets of a product (e.g. \"price < 100\") before reviewing individual results. This is opposed to related work where collaboration is focused on reviewing and relating results after search. The only systems in [18] following a similar faceted approach have either limited expressivity (TeamSearch) or force users to only navigate a single facet at a time (PDH).", "(2) Ullmer et al. were the first to suggest physically constrained tokens to manipulate database queries and result visualizations [27] . Two kinds of physical tokens (knobs and range sliders) served as tangible input controllers that are put into slots next to a display. Although enabling some basic Boolean logic between the range sliders, the overall expressivity was limited to assigning database fields to the axes of a scatter plot and altering parameters of predefined queries. Nonetheless, this design inspired many further designs of tangible queries (e.g. for facilitating search for children [6] ) or Blackwell et al.'s Query by Argument (QBA) system [3] . QBA enables groups to manifest the course of an argument in spatial configurations of \"statement tokens\", i.e., RFID-tagged cards as place-holders for contributions to the discussion. Each token carries a reference to a virtual information item that contains the contribution (e.g. relevant text passages). By spatially configuring the statement tokens during discussion, the group provides continuous relevance feedback to an information retrieval system that continuously evaluates the spatial structure to adjust its ranking mechanisms. As a result, the system suggests related material on a peripheral screen. QBA's approach to use spatial configurations of tokens to materialize the (chrono-)logical order of an argument during a collaborative process has been inspirational for our use of a network of tokens for faceted search. However, we want to provide users with tokens for precise filtering and immediate feedback. QBA is targeted at working invisibly in the background to gradually adjust its ranking without the same need for precision and immediacy.", "(3) While being of great practical value for search, Boolean AND and OR are concepts difficult to grasp and they contradict the linguistic sense of \"and\" and \"or\" in our natural language use [4] . Therefore many attempts have been made to visualize these concepts in visual query languages and interfaces: Today's faceted search on ecommerce Web sites [20] and faceted visualizations such as [5, 16] allow to formulate the equivalent of sophisticated Boolean queries by taking a series of small, simple navigation steps. However, these designs are for single users only and do not provide a random access to all intermediate steps in navigation history. This hampers their use for co-located collaborative search where all criteria of multiple parallel queries must be accessible for iterative refinement at all times. Young et al.'s filter/flow metaphor [28] achieves this by visualizing a Boolean query as a sequence of logically linked nodes that carry the criteria. However, it only permits a single query per workspace and its simple layout uses too much screen estate for tabletops. FindFlow [8] expands this metaphor into a 2D plane with a more efficient use of screen estate, but lacks parallel queries and the Boolean OR for specifying complex criteria (e.g. \"either the hotel has a good restaurant OR I want to have a small kitchen in my room.\"). It also conflicts with the size limitations and legibility around a tabletop. This is also true for DataMeadow that uses a filter/flow metaphor for connecting data, filters, and visualizations for visual analytics [7] . Similarly LARK uses a filter/flow metaphor for managing multiple-coordinated views for collaborative visual analytics on large multi-touch displays or tabletops [26] . However, in contrast to Facet-Streams, both systems are not used for Boolean search and they also do not employ any tangible user interface elements. "], "rq": ["INTRODUCTIONThe use of tabletops for co-located collaborative search is an ongoing topic in HCI research [18] . Tabletops can offer diverse benefits and potentials for collaborative search such as a closer face-to-face collaboration and more equitable working style [22] , an increased awareness and better group work experience [1] , and a horizontal form-factor whose affordances are well-suited to follow-up activities (e.g. sorting, sensemaking, making a purchasing decision) [1, 18] . However, other potentials of tabletops for search are still unexplored, e.g. the use of \"hybrid surfaces\" like [14, 25] that use tangible interaction with physical props in combination with multi-touch [15] . Except a single design study for video search [11] , such hybrid tabletop interaction has not been used in search scenarios yet. Furthermore, in the light of the popularity of tabletops (e.g. Microsoft Surface) in showrooms or flagship stores, it is surprising that no prior research has focused on the obvious task of collaborative search for products in a retail environment. In this paper we therefore present Facet-Streams (Figure 1 ), a novel design for collaborative faceted product search. It uses a hybrid interactive surface that combines information visualization techniques (a filter/flow metaphor [28] ) with tangible and multi-touch interaction to materialize collaborative search on a tabletop. Thereby, unlike in most previous work, our notion of search does not mean to populate an empty workspace with the results from a keyword search. Instead we mean a process of faceted collaborative filtering of a product catalog until the amount of results is sufficiently small to review and decide [10] . Furthermore, in a retail environment like a flagship store a \"good\" customer experience with \"soft\" factors such as fun, innovative design and social experience is often valued over \"hard\" factors such as task completion times and rates. Our work has therefore been guided by three research questions: (Q1) Does our design turn collaborative product search into a fun and social experience with increased group awareness? (Q2) Can we support the great variety of different search strategies and collaboration styles in different teams with a simple but flexible design? (Q3) Can we harness the expressive power of facets and Boolean logic without exposing users to complex formal notations? In the following, we discuss related work and the specifics of our context of use. Then, we introduce Facet-Streams and the underlying design rationale. We describe two user studies and discuss their results in terms of user experience, collaboration styles, and awareness. We conclude by summarizing our results and discussing them with respect to our research questions."]}
{"intro": [], "relatedWork": [], "rq": ["INTERMEDIATE-LEVEL KNOWLEDGEAccording to H\u00f6\u00f6k and L\u00f6wgren [36] \"In the HCI field, the dominant approach to knowledge construction is to design innovative interaction schemes and to evaluate them empirically through more or less rigorous use studies.\" This can then be seen has having two major aims (i) to present the actual artefacts developed and (ii) to contribute to a more general understanding of HCI. Our assumption is that the same reasoning can be applied to CCI being a subfield of HCI. Further, H\u00f6\u00f6k and L\u00f6wgren argue that HCI research mainly produces knowledge at the level of instances and theories as illustrated in Figure 1 . Artefact-centered papers can be said to present instances, which may or may not be based on theories. However, the idea behind the notion of intermediate-level knowledge is that the space in-between the instances and the theories is non-empty and can be filled with knowledge constructs that are more general than the particular instances but have a different scope and purpose than general level theories [35, 36, 42] . Some examples are Patterns, Guidelines, Methods and Tools, Strong Concepts, Bridging Concepts, and Annotated Portfolios. While guidelines and methods are well-established notions within both HCI and CCI, other intermediate-level knowledge forms like strong and bridging concepts were introduced rather recently. We therefore provide an overview of Strong concepts, Bridging concepts and Annotated portfolios since they are the most relevant forms of intermediate-level knowledge for approaching our third research question."]}
{"intro": ["Growing interest in computer-supported group work [10] has motivated research on developing communication and collaboration technologies that support many aspects of group work: fostering informal interaction (e.g., [2] ), creating awareness of colleagues' presence and behavior (e.g., [44] ), and supporting shared tasks such as writing or meeting (e.g., [48] ). An implicit assumption in this work is that simply providing these tools is sufficient to improve computer-mediated collaboration. However, unless teams are given guidance on the basics of effective collaboration skills, the tools may offer little benefit [42] . For instance, giving a design team a screen sharing tool in order to help them come up with a single design proposal will not necessarily help them to develop the consensus-building skills needed to complete the assignment collaboratively.", "Teamwork can be a powerful tool for learning [41, 43] and for accomplishing tasks [20] . However, if people are expected to successfully work in teams, they should be given tools to adopt appropriate interpersonal skills to overcome challenges and to achieve effective team processes and outcomes [5, 35] . These skills are often taught by providing team members with feedback on their own behavior, along with guidance on how certain changes in their behavior might improve group outcomes [34] . Our high-level research goal, therefore, is to understand how collaboration technology can illuminate social processes and behaviors within teams, allowing members to reflect and learn to become better collaborators."], "relatedWork": [], "rq": ["Experience of Feedback DesignsTogether with the results for RQ3, these responses suggest that our users experienced the fish visualization as engaging and even enchanting (as defined by [32] ), as they offered an experience of being \"caught up and carried away\". However, users did feel that the playfulness designed into the fish sacrificed ease of use, glanceability, and peripherality, which are important in the task-related setting for which GroupMeter is designed for. They liked the unobtrusiveness of the bars, but also criticized their lack of excitement.", "DISCUSSIONWith respect to RQ2, our results show that people changed their behavior when seeing feedback. They spent more time agreeing with each other, less time discussing the brainstormed ideas when seeing the fish, and drastically decreased their disagreement when seeing the bars. The fact that GroupMeter was able to elicit changes in communication behavior is especially encouraging given previous research suggesting that people's choice of words in conversation is largely spontaneous, unintentional, and uncontrolled [9, 39] . In our study, participants were nonetheless able to effect changes in their communication patterns (e.g., agreeing) in both visualization conditions. Our results thus demonstrate the power of automated linguistic analysis for teamwork feedback in stimulating reflection on and change in behavior. This leaves open the question, however, of which specific behavioral changes are desirable. We did not explicitly pose normative guidelines, although length of bar and distance of fish from the center possibly implied that more agreements were preferred. This, in addition to the mere presence of the feedback guiding participants toward self-focused awareness, might have led them to conform more to the group [13] . If fewer agreements and more discussion are favorable behaviors [21, 27] , integrating appropriate guidelines for effective teamwork into the design is an important future direction."]}
{"intro": ["The flow of knowledge requires an effective knowledge management (KM) strategy and the mobilisation, integration, sharing, and application of tacit and explicit knowledge in a dynamic manner. However, most KM frameworks lay an emphasis on managing explicit knowledge by focussing on the processes of capture, storage, retrieval, transfer and application (Argote and Ingram 2000; Sunassee and Sewry 2002; Dyba 2003; Arling and Chun 2011) . Tacit knowledge, on the other hand, needs the key mechanisms of interaction and feedback for effective sharing and use (Polanyi 1967; Nonaka and Takeuchi 1995; Kreiner 2002; Xue et al. 2011; Margaryan et al. 2011) . Within a dynamic and holistic knowledge approach, the existing and created tacit and explicit knowledge are mobilised and integrated, and made available to collaborative team members. The need therefore exists for a KM framework which addresses the requirements to facilitate the exchange and application of tacit knowledge, in addition to explicit knowledge. The paper addresses this gap by presenting a model that makes tacit and explicit knowledge available for organisational practices and routines through the supporting mechanisms of interaction and feedback. Specifically, the paper investigated the research question of how knowledge generated during development activities can be leveraged and effectively applied to ensure long-term sustainability. The developed model makes available and accessible dynamic tacit and explicit knowledge that is applied for effective decision-making and problem-solving, and provides the long-term and continuous perspective for sustainable development and improved environmental impact. The proposed model was validated during a case study conducted at one of the world's leading software organisation which currently employs more than 250,000 individuals Dalcher 2010, 2013) ."], "relatedWork": ["Many organisational operations are considered straight forward processes of planned, monitored, and controlled activities in a disciplined, orderly and methodical way. Dalcher (2003a, b) argues that a control perspective offers short-term focus with a limited emphasis on growth, improvement or the long-term accumulation of knowledge, reflection, experience or wisdom. Shifting attention towards a knowledge-based economy, emphasises continuous discovery and the creation, integration and application of knowledge. Knowledge creation, and its integration, can be viewed as collective processes of constructing, articulating and redefining shared beliefs and mental models through social interaction that help manage complex tasks and activities during collaboration, (Grant 1996; Huang 2000; Chang et al. 2012 ). However, Huang et al. (2001) argue that current conceptualisation of how knowledge is integrated and made available within the context of coordinating specialised expertise and tasks remains limited. It is therefore important to explore the dynamics of knowledge integration while performing collaborative activities such as decision-making which further generate ideas through collective input.", "Further, effective knowledge flows are critical for interaction and sustaining knowledge integration. Briggs et al. (2003) report on the value of facilitating interaction and accomplishing organisational tasks, and how in the case of inter-organisational collaboration, knowledge flows support significantly complex tasks when goals are to be accomplished by teams whose members do not share culture, communication and coordination processes. Gladstein (1984) , Hackman (1987), and McGrath (1984) argue that performance is a result of the interactions and dynamics among team members, and Argote and Ingram (2000) state that the utilisation of knowledge embedded within a team's interactions and tasks is the key to achieving better performance. Several researchers have investigated the importance of team work as members with diverse skills, knowledge, experiences, and expertise are required to work together to resolve the issues or problems encountered during project execution. However, a focus on how knowledge flows and supports collaboration and knowledge integration appears to be limited."], "rq": ["IntroductionThe flow of knowledge requires an effective knowledge management (KM) strategy and the mobilisation, integration, sharing, and application of tacit and explicit knowledge in a dynamic manner. However, most KM frameworks lay an emphasis on managing explicit knowledge by focussing on the processes of capture, storage, retrieval, transfer and application (Argote and Ingram 2000; Sunassee and Sewry 2002; Dyba 2003; Arling and Chun 2011) . Tacit knowledge, on the other hand, needs the key mechanisms of interaction and feedback for effective sharing and use (Polanyi 1967; Nonaka and Takeuchi 1995; Kreiner 2002; Xue et al. 2011; Margaryan et al. 2011) . Within a dynamic and holistic knowledge approach, the existing and created tacit and explicit knowledge are mobilised and integrated, and made available to collaborative team members. The need therefore exists for a KM framework which addresses the requirements to facilitate the exchange and application of tacit knowledge, in addition to explicit knowledge. The paper addresses this gap by presenting a model that makes tacit and explicit knowledge available for organisational practices and routines through the supporting mechanisms of interaction and feedback. Specifically, the paper investigated the research question of how knowledge generated during development activities can be leveraged and effectively applied to ensure long-term sustainability. The developed model makes available and accessible dynamic tacit and explicit knowledge that is applied for effective decision-making and problem-solving, and provides the long-term and continuous perspective for sustainable development and improved environmental impact. The proposed model was validated during a case study conducted at one of the world's leading software organisation which currently employs more than 250,000 individuals Dalcher 2010, 2013) ."]}
{"intro": [], "relatedWork": [], "rq": ["A. Research QuestionsThe main goal of this study is summarize and characterize the existing proposals in the academic literature of architectural evaluations of SoS, according to the types of evaluations and QA's mostly addressed by them. To achieve this goal, we defined the following research questions: [14] , We articulated the search string with keywords derived from the research questions. However, since the study is a systematic mapping, we followed Petersen's suggestion [13] [6] and used only Population and Intervention.", "V. RESULTS OF THE MAPPING2) RQ2: Which quality attributes are addressed by the techniques?: We considered the QA's used for proposals validation, as well as those that could be addressed outside the validation but were mentioned by the authors. Consequently the mostly addressed QA's are: Performance (11/22 \u2248 50%), flexibility (6/22 \u2248 27%), robustness (6/22 \u2248 27%), and reliability (4/22 \u2248 18%) (see Figure 4) . Notice that operational QA's predominate over other types related with postdeployment, like maintainability, evolvability, sustainability and others. This is really interesting because in the SoS context the constituent systems must be reconfigured to collaborate and address a common goal; however, QA's related to those properties are addressed to a lesser extent. An example of systems (that can be considered as constituent systems) that claim for these QA's are legacy and SCADA systems, which 1 [18]- [39] 3) RQ3: Which kinds of validation are used by the studies reporting techniques?: Most proposals offer no empirical validation: 10/22 (\u2248 45%) were illustrated with an example, whilst just 8/22 (\u2248 36%) were validated with a case study (see Figure 3) . It is curious to note that quantitative proposals preferred validation via illustrations, although the proposal [23] validated through experiment was also in this category (see Figure 4) . Finally, is interesting remark that the proposal that used an experiment as a validation method focused only on one QA. within them, 18/22 are quantitative, mostly using techniques related to meta-heuristics and fuzzy logic. These types of techniques have mainly been developed to evaluate the interaction of sets of systems according to the needs and expectations of stakeholders. In the same line, but to a lesser degree, were techniques that use formal models to corroborate the behavior of the design of the architecture. On the other hand, four hybrid techniques were found, of which only one comes from the software engineering community: a mixture of two well-known techniques, ATAM and MTW, but for which no validation was found. Respect to QA's, most approaches are operational, like performance, reliability, etc. Flexibility, despite being a nonoperational attribute, was highly considered according to the evolutionary nature of SoS; however, proposals that address it from a qualitative perspective were not found. We aimed to identify existing proposals to evaluate architectures and to determine which QA's were most addressed. To this end, we designed and executed an SLM which yield articles since 2005 to 2017, with an increasing number of publications per year (see Figure 5 ). However, considering the related works mentioned in Section III, we consider that there is a gap between the most QA's addressed by the proposals found in this SLM, and the key QA's identified recently."]}
{"intro": ["Although father involvement is growing, societal norms lag in a variety of ways (e.g., \"mommy\" groups, parenting advertisements targeted towards mothers, etc.). Furthermore, mothers are still doing more overall work inside and outside of the home [54] . This problem was highlighted in Hochschild and Machung's seminal book, The Second Shift, which described the shiftwork mothers took on related to homemaking and childrearing, in addition to their first shift in the workplace [34] . However, in the book, they also share a story of a father finding himself out of place when taking his child to the park because all of the other caretakers in the park are either stay-at-home moms (SAHMs) or female domestic workers [34] .", "Recent literature has described how parents go online to find social support and to overcome judgment they might experience in their offline environments [5, 6, 23, 37, 62] . However, much of that work has focused on mothers rather than fathers. Research that has addressed the role of fathers in particular examines how two SAHDs use social media, but the sample in the study was small [5] . We build on that work here to understand how SAHDs use social media related to their roles and identities as SAHDs. We explore the following research questions:"], "relatedWork": [], "rq": ["INTRODUCTIONRecent literature has described how parents go online to find social support and to overcome judgment they might experience in their offline environments [5, 6, 23, 37, 62] . However, much of that work has focused on mothers rather than fathers. Research that has addressed the role of fathers in particular examines how two SAHDs use social media, but the sample in the study was small [5] . We build on that work here to understand how SAHDs use social media related to their roles and identities as SAHDs. We explore the following research questions:"]}
{"intro": ["However, in the physical world, Chile is becoming more centralized everyday [15] , and it is not clear if the virtual population present in Twitter reflects or is representative of the physical population, and to what degree this virtual population is also centralized. A common saying is \"Santiago is not Chile\" 3 , referring to the fact that the capital is not representative of the country, yet media outlets concentrate in Santiago and government policies are tailored at the needs of Santiago. Given the climatic, geographical and cultural diversity of Chile, centralism is a serious problem. One example is that the law that dictates minimum housing requirements is the same for all regions of the country [1] , in spite of the extreme weather differences between northern and southern regions.", "Previous literature regarding this subject states that more populated cities are over-represented in Twitter, while less populated cities are under-represented [11] . Because of overrepresentation of populated places, in addition to the lack of balance in physical population distribution in Chile, content from or about non largely populated locations is lost in the timeline of tweets. In addition, it is hard to find local content, as the only salient ways to find content are to click on trending topics, which by definition are biased towards more populated cities, and by searching. But this implies that the information seeker already knows what to look for, if an information need effectively exists. As such, there is no current way to explore a geographically diverse timeline: users have the responsibility to follow diverse accounts. However, current interfaces do not allow users to see a diversity of tweets according to any criteria. Since Twitter recommends users (the \"who to follow\" functionality) and tweets (the \"discover\" tab) based on account connections and activity, users who do not have diverse connections will not receive diverse recommendations. Motivated by the situation described, in this paper we propose a methodology to address the following research questions:"], "relatedWork": ["There is no clear answer to the question what is Twitter? [9] . However, a wide spectrum of research areas have seen it from different perspectives. One of them is the geographical span of networks: previous work has found that, the stronger the network (defined in terms of reciprocity in connections, 1-way and 2-way interactions by mentioning others), the lower is its geographical span [13] . In terms of discussion, local events have more dense networks of discussion than global events, and central individuals in the network are also located centrally in the physical world [17] . A demographic study of user accounts from the the U.S.A. concluded that populated cities are over-represented, while less populated cities are under-represented in Twitter [11] ."], "rq": ["INTRODUCTIONPrevious literature regarding this subject states that more populated cities are over-represented in Twitter, while less populated cities are under-represented [11] . Because of overrepresentation of populated places, in addition to the lack of balance in physical population distribution in Chile, content from or about non largely populated locations is lost in the timeline of tweets. In addition, it is hard to find local content, as the only salient ways to find content are to click on trending topics, which by definition are biased towards more populated cities, and by searching. But this implies that the information seeker already knows what to look for, if an information need effectively exists. As such, there is no current way to explore a geographically diverse timeline: users have the responsibility to follow diverse accounts. However, current interfaces do not allow users to see a diversity of tweets according to any criteria. Since Twitter recommends users (the \"who to follow\" functionality) and tweets (the \"discover\" tab) based on account connections and activity, users who do not have diverse connections will not receive diverse recommendations. Motivated by the situation described, in this paper we propose a methodology to address the following research questions:"]}
{"intro": ["Recommender systems are increasingly used in various on-line application domains, such as e-commerce and technology-enhanced learning. They provide users with personalized items based on a variety of information, including preferences, on-line history, and demographics. Traditionally, most researchers focused on the accuracy of recommender systems by proposing various algorithms to increase precision and recall. However, recent studies have shown that the overall user experience of recommender systems is affected by many factors beyond accuracy [15] in some domains, such as diversity and serendipity for the music recommenders. Recommender systems aim at helping users explore new items of interest [18] . Diversity is important because users will not be satisfied with recommendations that are overly similar to what they have consumed previously [19] . Moreover, diversity enables comparison among recommendations, thereby increasing the confidence in making a choice [8, 25] ."], "relatedWork": [], "rq": ["DISCUSSIONWe aimed to investigate the effects of two individual traits on user's perception of diversity. We categorized the participants into two groups according to their MS and VM scores. The results indicate that for both users with high MS and users with high VM, their perceptions of diversity for recommendations in ComBub are significantly higher than the perceived diversity in SimBub, notwithstanding that the actual diversity of recommendations in ComBub is consistent with that in SimBub. Therefore, we could confirm the hypotheses H2, H3 and answer the research question RQ2 by finding the positive effects of MS and VM on perceived diversity. Of note, for those who have low MS, ComBub led to a significantly lower perception of diversity than SimBub. Thus, the additional audio features shown in ComBub do not seem to supply real benefits for users with low MS in terms of perception of diversity. Even the additional features in a visualization may result in Session: Personalized Recommender Systems IV & Intelligent User Interfaces UMAP'18, July 8-11, 2018, Singapore higher cognitive load and a negative impact on user experience [4] . A similarly reversed result was not found for users with low VM. The correlation analysis results show a significantly positive correlation between individual traits (MS, VM) and the perceived diversity in visualization ComBub; however, no such a significant correlation was found for SimBub. This result implies that an advanced visualization interface like ComBub allows experts to leverage their attribute knowledge to perceive higher diversity. In contrast, novices seem to prefer a simple interface that does not require intimate attribute knowledge such as the meaning of each audio features introduced in ComBub."]}
{"intro": [], "relatedWork": [], "rq": ["Research DataTo answer our research questions, we used two datasets: (1) user comments posted on SPIEGEL Online 1 (SPON) and (2) the \"One Million Posts\" (OMP) corpus [56] . We selected the SPON news page for two reasons. First, SPON is the most-read online German newspaper according to Alexa.com [2] . Second, the topics covered are diverse and structured in articles, forums, and comments. We collected a comprehensive sample of published user comments from 01-01-2000 to 28-02-2017 with their respective metadata and all archived articles and forums. The data collection took one week and we did not notice any changes of forum features between old and new forums. Our sample comprises 11,276,843 comments (with title, text, timestamp, username, department, and quoted comments if available), 515,522 articles (with title, introduction, text, date, and partly author names), and 181,399 forums (with title and department). Most SPON articles are signed by an acronym to state the author, while the acronyms are assigned to full names in the imprint. However, we could only identify the full author names for 16% of the news articles as many assignments were missing. Additionally, we used the partly annotated comments of OMP, a dataset that consists of 11,773 labeled and one million unlabeled German online user comments posted on DER STANDARD, an Austrian newspaper website. The authors define the annotation category \"feedback\" as: \"Sometimes users ask questions or give feedback to the author of the article or the newspaper in general, which may require a reply/reaction\" [56] . This description is equivalent to our meta-comment definition."]}
{"intro": ["The perspective of looking at software architecture as a set of architecture decisions is widely recognized [1] , [2] . However, architecture decisions are often not explicitly documented in practice but reside in the architect's mind as tacit knowledge and are only implicit in the models that the architect creates [2] , [3] ; even though explicit capturing and documentation of architecture decisions has been associated with a multitude of benefits, such as avoiding knowledge vaporization, supporting change impact estimation, increasing system understanding, improving knowledge sharing, and facilitating architecture evaluation [2] , [3] , [4] . This gap between industry practice and the benefits demonstrated by research persists, although researchers have proposed many approaches to incorporate the documentation of architecture decisions in architecture practice [5] .", "However, multiple studies identified that these tools are often only applicable during certain stages of the architecting process and have been designed to only support a particular activity or only work under certain conditions [6] , [7] , [8] . In addition, van Heesch et al. [9] argued that the proposed approaches, i.e. decision templates, decision models, and annotations, \"do not frame all concerns of all stakeholders in an adequate and useful manner\". This makes the produced documentation only suitable for a small subset of stakeholders. Furthermore, existing decision documentation approaches do not integrate well with existing architecture documentation approaches (e.g. models, diagrams, views) [9] ."], "relatedWork": [], "rq": ["B. Sharing and Reusing DecisionsWe have seen in RQ1 that the preparation of a decision requires a lot of effort. This is aligned with the findings from van Heesch and Avgeriou [21] on the reasoning process of professional architects. Despite the effort that is being spent on these activities, we found that decisions are not shared within the company but are only consumed within the context of a project. However, reusing existing decisions as a basis for upcoming decisions could significantly reduce the effort of collecting and researching information. Of course, decisions cannot be applied without a critical reflection since the contexts of projects differ. Nevertheless, decisions can be used as a source of inspiration and information (e.g. \"Which alternatives were considered?\" or \"How where they evaluated?\"). Another aspect of sharing is the ability to make decisions available for stakeholders by exporting stakeholder-specific reports. We found that this is an important use-case for architects, since decisions are often used for communication with stakeholders, e.g. during review meetings.", "C. Perceived Benefits, Efficiency and FlexibilityRQ2 showed that there is only limited time available for documentation. This is supported by the study conducted by Tang et al. [19] , who identified time-pressure as one of the main barriers for documenting decisions. However, not enough time for documenting decisions can be seen as an indicator that the architects did not see enough direct benefits to make decision documentation a higher priority task. Although, all participants reported long-term benefits of decision documentation, such as avoiding knowledge vaporization, the immediate benefits of documenting decisions were not that evident. Architects only mentioned direct benefits in those projects, in which decisions were immediately used for reviews or where the project focussed on extending the knowledgebase of the company, e.g. creating architecture prototypes of emerging technologies."]}
{"intro": ["Verbal irony and sarcasm are a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate 1990) , such as to break their pattern of expectation. For the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered utterances in isolation (Davidov, Tsur, and Rappoport 2010; Gonz\u00e1lez-Ib\u00e1\u00f1ez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Riloff et al. 2013; Maynard and Greenwood 2014; Ghosh Guo, and Muresan 2015; Joshi, Sharma, and Bhattacharyya 2015; Ghosh and Veale 2016; Joshi et al. 2016b ). In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014) . Thus, to detect the speaker's sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context. Consider the Twitter conversation example in Table 1 . Without the context of userA's statement, the sarcastic intent of userB's response might not be detected."], "relatedWork": ["Existing work in computational models for sarcasm detection addresses a variety of different tasks. These include, primarily, classifying sarcastic vs. non-sarcastic utterances using various lexical and pragmatic features (Gonz\u00e1lez-Ib\u00e1\u00f1ez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Ghosh and Veale 2016; Joshi et al. 2016b; Muresan et al. 2016) , rules and text-patterns (Veale and Hao 2010) , specific hashtags (Maynard and Greenwood 2014) as well as semi-supervised approach (Davidov, Tsur, and Rappoport 2010) . Researchers have also examined different characteristics of sarcasm, such as sarcasm detection as a sense-disambiguation problem (Ghosh, Guo, and Muresan 2015) and sarcasm as a contrast between a positive sentiment and negative situation (Riloff et al. 2013; Joshi, Sharma, and Bhattacharyya 2015) . Apart from linguistically motivated contextual knowledge, cognitive features, such as eye-tracking information, are also used in sarcasm detection (Mishra et al. 2016 ). Schifanella et al. (2016) propose a multimodal approach, where textual and visual features are combined for sarcasm detection. Some studies present approaches for sarcasm detection in languages other than English. For example, Pt\u00e1\u010dek, Habernal, and Hong (2014) use various n-grams, including unigrams, bigrams, and trigrams, and a set of language-independent features, such as punctuation marks, emoticons, quotes, capitalized words, and character n-gram features, to identify sarcasm in Czech tweets. Similarly, Liu et al. (2014) introduce POS sequences and homophony features to detect sarcasm from Chinese utterances. Bharti, Babu, and Jena (2017) compared tweets written in Hindi to news context for irony identification. Most of these approaches have considered utterances in isolation. However, even humans have difficulty sometimes in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014) . Recently, an increasing number of researchers have started using contextual information for irony and sarcasm detection. The term context loosely refers to any information that is available beyond the utterance itself (Joshi, Bhattacharyya, and Carman 2017) . There are two major research directionsauthor context and conversation context-and we briefly discuss them here."], "rq": ["Use of Numbers.Use of Rhetorical Questions. We also found that sarcastic utterances that use rhetorical questions (RQ), especially in discussion forums (e.g., IAC v2 ) are hard to identify. Oraby et al. (2016) hypothesized that sarcastic utterances of RQ type are of the following structure: They contain questions in the middle of a post that are followed by a statement. Because many discussion posts are long and might include multiple questions, question marks are not very strong indicators for RQ. et al. (2014) showed that by providing additional conversation context, humans could identify sarcastic utterances that they were unable to identify without the context. However, it will be useful to understand whether a specific part of the conversation context triggers the sarcastic reply. To begin to address this issue, we conducted a qualitative study to understand (a) whether human annotators can identify parts of context that trigger the sarcastic reply and (b) if attention weights can signal similar information. For (a) we designed a crowdsourcing experiment (Crowdsourcing Experiment 1 in Section 6.1), and for (b) we looked at the attention weights of the LSTM networks (Section 6.2)."]}
{"intro": ["The design and evaluation of technologies intended to support recovery or management in relation to chronic health conditions has become a significant strand of HCI research. Much of this prior work has been structured around the design of prototype technologies which have been evaluated through relatively small-scale, controlled deployments. However, Blandford [8] has argued for research that engages with health technologies in a realistic context of use, and prior experience suggests that the study of deployments \"in the wild\" will reveal complex and unexpected phenomena that can only emerge in naturalistic settings [7] . Some health technologies have reached a sufficient level of maturity that they are being deployed on a wide-scale by national health services, and this should provide a broad range of opportunities for research studies that consider usage in naturalistic settings. In the case of chronic health conditions, these might need to consider engagement across a multi-year period; how to study such engagements effectively is then a challenging question."], "relatedWork": [], "rq": ["Supporting effective disengagementsWorkshop discussions also hint at a need for users to be able to reflect on their experience so as to obtain maximum benefit, with reflection potentially happening years after the engagement. Partly, this then requires organizational structures that ensure that access to records of interaction can be maintained, so that individuals can return to interfaces so as to revisit and reflect on content. There is, however, an interesting research question of whether technology can specifically support personal reflection. This may touch on recent HCI work around digital souvenirs of experience [15] , the design of which has been explicitly motivated in terms of the support that can be provided for reflection on experiences."]}
{"intro": ["Here as a first step, we focus on Airbnb hosts in the United States as a case study. This is based on two reasons. First, although Airbnb is a pioneering example of the sharing economy, there have been few empirical studies about its service providers. Second, currently US is Airbnb's largest market [17] , and extensive SES and other characteristics data are available, allowing us to approach our questions systematically. Our research questions are important due to several theoretical and practical reasons. First, participation in the sharing economy may be more appealing to people with lower-income than to richer ones, as it may provide potential revenue stream. Several studies have already pointed out that monetary compensation is one of the main motivations for participation in the sharing economy [16, 20, 26] . Second, many sharing economy platforms greatly reduce the costs associated with joining the markets. For instance, they have provided a system to make the payment safe, have verified registered customers, and have made it easy for would-be workers to join the platform. Therefore, they may attract workers from across the income spectrum. Third, there are still entry barriers to participate, such as having an underutilized room located in a neighborhood where wouldbe guests are willing come, and having the ability to deal with issues involved in managing listings. Therefore, certain groups of individuals may not afford becoming a host. Fourth, heterogeneous housing characteristics, such as location and decoration, together with the existence of racial discrimination on Airbnb as demonstrated recently [10, 11] , may make some listings more profitable, while others not, which may affect the joining of Airbnb. On the practical side, Airbnb has repeatedly claimed that it has been employed mainly by lower-income residents as their revenue supplement, which is yet to be verified. To answer our research questions, we use data crawled from Airbnb between May and August, 2016 and employ standard regression methods to control for other characteristics, such as housing and attractiveness. Our modeling results show that after controlling for demographic, housing, and attractiveness characteristics, income has a strong negative effect on the participation in Airbnb; areas with lower median household income tend to have more hosts. We also find that education is another influential factor; areas where there are a larger portion of residents with higher education degrees are associated with more hosts. However, when we consider the performance of listings, as measured by the number of newly received reviews, we find that income has a positive effect on entire-home listings, even after controlling for listing-and host-level characteristics. This means that entire-home listings located in areas with higher income tend to have more new reviews. Our findings therefore may suggest the disadvantage of SES-disadvantaged areas and the advantage of SES-advantaged areas."], "relatedWork": [], "rq": ["INTRODUCTIONHere as a first step, we focus on Airbnb hosts in the United States as a case study. This is based on two reasons. First, although Airbnb is a pioneering example of the sharing economy, there have been few empirical studies about its service providers. Second, currently US is Airbnb's largest market [17] , and extensive SES and other characteristics data are available, allowing us to approach our questions systematically. Our research questions are important due to several theoretical and practical reasons. First, participation in the sharing economy may be more appealing to people with lower-income than to richer ones, as it may provide potential revenue stream. Several studies have already pointed out that monetary compensation is one of the main motivations for participation in the sharing economy [16, 20, 26] . Second, many sharing economy platforms greatly reduce the costs associated with joining the markets. For instance, they have provided a system to make the payment safe, have verified registered customers, and have made it easy for would-be workers to join the platform. Therefore, they may attract workers from across the income spectrum. Third, there are still entry barriers to participate, such as having an underutilized room located in a neighborhood where wouldbe guests are willing come, and having the ability to deal with issues involved in managing listings. Therefore, certain groups of individuals may not afford becoming a host. Fourth, heterogeneous housing characteristics, such as location and decoration, together with the existence of racial discrimination on Airbnb as demonstrated recently [10, 11] , may make some listings more profitable, while others not, which may affect the joining of Airbnb. On the practical side, Airbnb has repeatedly claimed that it has been employed mainly by lower-income residents as their revenue supplement, which is yet to be verified. To answer our research questions, we use data crawled from Airbnb between May and August, 2016 and employ standard regression methods to control for other characteristics, such as housing and attractiveness. Our modeling results show that after controlling for demographic, housing, and attractiveness characteristics, income has a strong negative effect on the participation in Airbnb; areas with lower median household income tend to have more hosts. We also find that education is another influential factor; areas where there are a larger portion of residents with higher education degrees are associated with more hosts. However, when we consider the performance of listings, as measured by the number of newly received reviews, we find that income has a positive effect on entire-home listings, even after controlling for listing-and host-level characteristics. This means that entire-home listings located in areas with higher income tend to have more new reviews. Our findings therefore may suggest the disadvantage of SES-disadvantaged areas and the advantage of SES-advantaged areas."]}
{"intro": ["The study presented was entangled in the last phase of a research project on supporting burglary prevention advisors in their daily work. In the burglary prevention (BP) scenario, police trained in technical security visit residents at their homes to advise them on how to secure their properties against burglary (Giesbrecht et al. 2015; Comes and Schwabe 2016a, b) . The public mandate of police crime prevention units includes, among others, promoting the implementation of crime prevention measures and enabling communities to prevent burglary cases from happening. BP advisors act upon this task; however, they often lack systematic training for it. Depending on their career, they rely on an introductory hands-on training, general police officer schooling, exposure to burglary or crime cases in their previous appointments (e.g., during patrol or investigator duties), as well as experience from previous advisory encounters, and their technical expertise. Prone to influence by the complex nature of interpersonal communication, they differ significantly in how they motivate or enable their advisees and, as this study unveils, how they shape their task during this communication. Some focus on transferring the message: Bdon't be afraid!^, whereas others exaggerate stories from criminal statistics. After the rollout of the SmartProtector, a tablet-based tool designed according to persuasive technology guidelines to support a range of persuasive practices, the differences between the various advisors emerged. Each advisor favoured a stockpile of routines (stories, arguments, explanations, etc.) which were activated based on their preconceptions and observations about the advisee, the local or situational circumstances and the private perception of the advisor's task. This study makes clear that behaviours that were originally considered a mundane part of a conversation (e.g., a story from the neighbourhood), could be recognized as an essential and routinized persuasive device. The SmartProtector was appropriated as far as it could be meaningfully applied in the routines. This sheds new light on the persuasive aspect of the work: it grows out of a range of conversational routines and is not like a debate with explicit arguments or targeted behaviours. Consequently, supporting persuasion is less about extending the persuasive arsenal with technology but rather about equipping IT with a meaning that fits the stories, explanations, and narratives the advisors used to provide, and about affording new behaviours that may turn into routines. Overall, this study proposes the picture of persuasive practices as routines originating as strong stereotypes, as well as the advisor's opinion towards ongoing organizational discourses and tensions. Transforming those practices with IT requires consideration of multiple cues about the situation and its background rather than an optimistic assumption about the improvisational character of practices. The current study arrived at those insights by pursuing the following research questions:"], "relatedWork": [], "rq": ["IntroductionThe study presented was entangled in the last phase of a research project on supporting burglary prevention advisors in their daily work. In the burglary prevention (BP) scenario, police trained in technical security visit residents at their homes to advise them on how to secure their properties against burglary (Giesbrecht et al. 2015; Comes and Schwabe 2016a, b) . The public mandate of police crime prevention units includes, among others, promoting the implementation of crime prevention measures and enabling communities to prevent burglary cases from happening. BP advisors act upon this task; however, they often lack systematic training for it. Depending on their career, they rely on an introductory hands-on training, general police officer schooling, exposure to burglary or crime cases in their previous appointments (e.g., during patrol or investigator duties), as well as experience from previous advisory encounters, and their technical expertise. Prone to influence by the complex nature of interpersonal communication, they differ significantly in how they motivate or enable their advisees and, as this study unveils, how they shape their task during this communication. Some focus on transferring the message: Bdon't be afraid!^, whereas others exaggerate stories from criminal statistics. After the rollout of the SmartProtector, a tablet-based tool designed according to persuasive technology guidelines to support a range of persuasive practices, the differences between the various advisors emerged. Each advisor favoured a stockpile of routines (stories, arguments, explanations, etc.) which were activated based on their preconceptions and observations about the advisee, the local or situational circumstances and the private perception of the advisor's task. This study makes clear that behaviours that were originally considered a mundane part of a conversation (e.g., a story from the neighbourhood), could be recognized as an essential and routinized persuasive device. The SmartProtector was appropriated as far as it could be meaningfully applied in the routines. This sheds new light on the persuasive aspect of the work: it grows out of a range of conversational routines and is not like a debate with explicit arguments or targeted behaviours. Consequently, supporting persuasion is less about extending the persuasive arsenal with technology but rather about equipping IT with a meaning that fits the stories, explanations, and narratives the advisors used to provide, and about affording new behaviours that may turn into routines. Overall, this study proposes the picture of persuasive practices as routines originating as strong stereotypes, as well as the advisor's opinion towards ongoing organizational discourses and tensions. Transforming those practices with IT requires consideration of multiple cues about the situation and its background rather than an optimistic assumption about the improvisational character of practices. The current study arrived at those insights by pursuing the following research questions:", "Supporting advisory services and advisory practicesCurrently, advisory services are undergoing an intensive phase of transformation. The changes are driven by technology entering the encounters, expectations of the customers, as well as by the easy, on-line access to information, which was previously available only to experts (Schwabe and Nussbaumer 2009; Dolata and Schwabe 2017b) . Due to this transformation, the main focus of advisory services moves from information provision and recommendation to joint problem solving (Dolata and Schwabe 2017b) . This is reflected in the emergence of new advisory practices (Dolata and Schwabe 2017c, a; Fischer et al. 2017) . For instance, energy advisors involve the advisee during the analysis of specific, individual data for providing a suitable recommendation rather than generic suggestions (Fischer et al. 2017) . Similarly, bank advisors try to learn and document many aspects of an advisee's life, while asking typical questions, to offer an individualized rather than a standard, off-the-shelf package (Kilic et al. 2017) . The advisory practices encompass situative, improvisational elements (Fischer et al. 2014 (Fischer et al. , 2017 , as well as routinized behaviours and conversational strategies (Dolata and Schwabe 2017a, c; Kilic et al. 2017) . For example, in a BP advisory service, an advisor may routinely suggest a specific rim door lock with a bolt and present its working mechanism with a specimen; however this behavior will emerge if the door explicitly requires such a lock or the advisor notices that the advisee uses a (potentially insecure) chain lock (Dolata et al. 2016) . Given the fact that the advisory practices combine routinized and situative behaviour, and considering the ongoing transformation of advisory services, this field offers a range of relevant research questions: What is the main point of an advisory service if not the information transfer between an expert and a layperson? How can IT support new practices that outgrow previously existing routines? How should IT take account of the situative character or situative activation of some practices?"]}
{"intro": [], "relatedWork": [], "rq": ["DRIVING simulators are widely used for various purposes in driver training, traffic research, and automotive development [1] . When using a driving simulator, one does so to target some driving-related objectives (e.g., driver learning out-comes, traffic research questions, and vehicle design decisions) that involve the human driver as a crucial component, but where the use of a real vehicle for attaining the objectives is deemed unsafe, too costly, or otherwise ineffective. The objectives themselves, however, will typically remain focused on real driving, i.e., the use of the simulator is motivated by the assumption that the results will validly transfer to real vehicles and real traffic."]}
{"intro": [], "relatedWork": [], "rq": ["(E)xpectation, (I)ntensity, (P)ower, and (V)alenceboth classifier and human correlations on two test recordings, averaged over three raters. Overall, our results show that obtaining a high correlation between various human raters for the audio-visual Solid SAL data is indeed challenging. To mitigate a similar problem, Nicolaou et al. [76] proposed a method for achieving high interrater agreement and segmenting the continuous sequences into shorter clips. However, extending this method for automatic, dimensional, and continuous emotion detection remains a challenge. Moreover, during annotation, the human raters were exposed to audio-visual Solid SAL data without explicitly being instructed to pay attention to any particular cue (e.g., head gestures) or modality (e.g., audio or video). Therefore, it is not possible to conclude which cues or modalities were dominant for which ratings. In general, these issues still remain as open research questions in the field [74] , [75] ."]}
{"intro": [], "relatedWork": [], "rq": ["Problem Formulation and IdentifiabilityHowever, if the true parameters were (r, p, q) = (0.1, 0.2, 0.3) with proportions of BOWs being 0.4%, 19.8%, 8.1%, respectively, it is easy to verify that the system would have multiple valid solutions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and (0.1180, 0.1841, 0.3030) . In general, ifp(x) is known from the training BOW corpus, when can we guarantee to uniquely recover the bigram LM \u03b8? This is the question of identifiability, which means the transition matrix \u03b8 satisfying (1) exists and is unique. Identifiability is related to finding unique solutions of a system of polynomial equations since (1) is such a system in the elements of \u03b8. The details are beyond the scope of this paper, but applying the technique in (Basu and Boston, 2000) , it is possible to show that for W = 3 (including d ) we need longer documents (|x| \u2265 5) to ensure identifiability. The identifiability of more general cases is still an open research question."]}
{"intro": [], "relatedWork": [], "rq": ["Test materialsAn annotated evaluation resource would be needed to measure the robustness of a parser against human judgments. It would, however, be a daunting task to annotate noisy texts and their corresponding correct counterparts for all four of the parsers. We therefore made the simplifying assumption, like (Bigert et al. 2005) , that a parser is robust if it is able to produce a similar analysis for a correct sentence and a noisy version of the same sentence. Our assumption is that if a parser is able to do this, it will be able to perform in a robust way when it is confronted by noisy inputs. By making this assumption, we were therefore able to perform evaluations by using unannotated texts. It is clear that as the level of noise in the inputs increases, the performance of a system degrades correspondingly. The extent to which this occurs can be measured by increasing the number of mistakes in the input sentences and observing the effect that this has on its performance. In order to investigate the effect of increasing the amount of distortion in the input, and to answer our second research question, we constructed a test corpus which contained sentences with error-free sentences and their noisy counterparts with one or more spelling errors. Our test set had three error levels: each of the noisy sentences contained between one to three misspelled words. We started the test set construction by selecting 19 sentences from a public domain web page. We then altered one, two or three words per test sentence and this gave us a total of 443 test sentences -255 with one error and 94 with two and three errors respectively. The length of each of these sentences was between 5 and 36 words and the average length was 16.32 words per sentence. We then introduced misspellings manually into the sentences by deleting, adding and swapping characters, permitting only one edit operation per word. We based character additions on the keyboard proximity of letters in order to simulate errors in naturally-occurring texts. Since our purpose was not to evaluate robustness on structurally distorted sentences, we only permitted alterations that did not create an acceptable (valid) word."]}
{"intro": ["However, the snowballing research of user interface design has, until now, largely left aside the study of how visual elements in user interface design elicit emotional experiences. Current research of experiencing visual user interface designs, in the research area of visual aesthetics in humancomputer interaction, has mainly focused on the overall impression of visual user interfaces (e.g., [5] [6] [7] ), as a means of enhancing user experience with aesthetic pleasantness (e.g., [8] ). In addition, more detailed approaches have focused, for example, on typography [9] and on high-level attributes (e.g., [10] ). These high-level attributes include, for example, unity and prototypicality [11] , novelty [12] , and typicality and novelty [13] . Therefore, research of visual aesthetics in human-computer interaction lacks knowledge of emotional responses in experiencing low-level attributes, that is, visual elements (e.g., [14] ), such as color, size, and balance [15] . In this paper, appraisal theory of emotion [16] [17] [18] is utilized to elaborate the relationship of emotional user experience and visual elements. Therefore, this study adopts an interactionist approach to human-technology experience; that is, it does not merely focus on either user interface design properties or users' impressions and preferences of visual user interface designs, but analyses screen-based visual elements and their appraised dimensions together. Screen-design based approach focuses on detecting the visual properties of design components and their spatial organization in user interfaces [19] , which affect user experience. How these identified visual elements are appraised is the key to understanding their role in human-computer interaction and in web page design."], "relatedWork": [], "rq": ["Visual Elements and the Emotion ProcessOf the all possible appraisal dimensions, perhaps the most frequently used scheme for describing emotion is the combination of valence and arousal [40] . Valence refers to the pleasantness of the experience and arousal to how much the emotion is associated with activation. For example, feeling calm is pleasant but not an active emotion, while feeling energetic is a pleasant and active emotion. Sadness is a negative and deactivating emotion, and anger is a negative and arousing emotion. While these two are not the only relevant dimensions of emotional experience, they are often the most salient and can be used for rich descriptions of emotion in human-computer interaction [41] . Regarding emotional experiences of pictorial representations, for instance, works of art can differ in their potential to cause arousal. Works of art which possess ability to evoke high arousal are most likely perceived as dramatic and dynamic, and works of art with low potential on eliciting arousal are generally perceived as static and harmonious [46] . This paper focuses on studying the most salient visual elements in user interfaces and their relation to emotions attributed and elicited by them from the perspective of future programmers, in order to provide usable insight for the evaluation and design of visual elements in user interfaces that promote user experience and thus to benefit usercentered visual user interface design. This study focuses on the following research questions: What are the most salient visual elements in web pages from programmers' point of view? What kind of emotions do the salient visual elements elicit? How are the salient visual elements evaluated in the appraisal process? Advances in Human-Computer Interaction were allowed to return the templates anonymously without information that could be used to identify them, because the data collection was organized as a part of a university course. Thus, the reported age and gender information is from 40 participants, while ten participants answered without identification information. However, the average age of the 40 participants did not differ much from the average age of the participants in the course: the average age of the participants in the course was 25.0 years (SD = 5.8 and range ."]}
{"intro": [], "relatedWork": [], "rq": ["Place Analysis (RQ2)Of all the 1,323 check-ins, 626 (47.3%) were at private places, and the rest (52.7%) at public places (Table 2) . Of the 626 check-ins at private venues, 62% were reported from their own home, 30% from their friends' home, while the rest (8%) occurred at either their workplace or other private venues (e.g., student hostel, someone else's home while baby-sitting, etc.). A large number of check-ins at private venues might be due to two factors: a) the majority of participants (83%) reported living with their parents, and b) spending a night outside is relatively costly given the demographics and income earning status of participants. After manually browsing through some of the videos taken in private places, we found that some videos indeed show young people in large family homes (with a large living room and kitchen), but also studios and For check-ins at public places, 30% were at bars, followed by 27% at PBS (including public parks, lakeside, etc.) as shown in Figure 3b . Restaurants and travel each contributed around 10% of all check-ins. We observe that a significant portion of check-ins happened at PBS, which suggests that youth spend a considerable amount of time hanging out in these spaces away from mainstream nightlife areas. This provides support for the qualitative work done with youth in the US context [2] . The PBS videos are specially interesting as they are unfiltered. Videos in dark parks or squares where people hang out, and video taken on streets outside commercial venues, are commonly found in our data. These places also provide support for qualitative work on the practices of Swiss youth in these venues [8] . Note that both Lausanne and Zurich have a scenic lakeside used often for recreational activities. Overall, these findings reflect that the study indeed captured different patterns of participants' nightlife behavior. Figure 3d . In the inset of the same figure, we plot the overall 4SQ check-in distribution (i.e., without any temporal filtering). For both the temporally filtered and overall distribution, food places receive the most number of checkins, which is in contrast with our study findings. Similar to previous work [5] , we observe that places visited during night are more represented in our crowdsourcing study compared to Foursquare e.g., events category did not contain a single check-in for temporally filtered 4SQ data. For some of the categories, the check-in distribution of our study (Figure 3b ) is similar to the temporally filtered 4SQ check-ins; however it is significantly different with respect to the overall 4SQ check-in distribution. These findings point towards limitations of social media in terms of representativeness and temporal resolution at least in the context of Switzerland [45] ."]}
{"intro": ["Kruger [7] provides a detailed survey of robots being used in assembly lines. Several collaborative robots have been developed for work environments [3, 10] ; however, till date there have been no mobile robots which work with humans on automotive assembly lines. Our work is aimed at developing a mobile robot which can work along side humans in automotive assembly lines. Here, we highlight the key research challenges faced in developing this system along with obtained solutions and open research questions."], "relatedWork": [], "rq": ["INTRODUCTIONKruger [7] provides a detailed survey of robots being used in assembly lines. Several collaborative robots have been developed for work environments [3, 10] ; however, till date there have been no mobile robots which work with humans on automotive assembly lines. Our work is aimed at developing a mobile robot which can work along side humans in automotive assembly lines. Here, we highlight the key research challenges faced in developing this system along with obtained solutions and open research questions."]}
{"intro": ["As summarized in Table 1 , predictive physics simulation is not a new concept, and it has been increasingly utilized by recent artificial intelligence (AI) and animation research (e.g., [11, 32] ). However, research has focused on nonplayer characters or enabling simple high-level control of a simulated avatar, e.g., making a bipedal character walk in a desired direction. This has obvious applications in action games such as God of War [27] or Uncharted [20] , which presently need vast quantities of animation data to enable such high-level control. However, there are also various games such as Angry Birds [25] and QWOP [2] , where the player directly controls simulations on a low level. In such games, predictive simulation seems less explored. This observation prompts our primary research question: What novel possibilities and challenges emerge from combining predictive simulation with low-level direct control of simulated characters?"], "relatedWork": [], "rq": ["INTRODUCTIONAs summarized in Table 1 , predictive physics simulation is not a new concept, and it has been increasingly utilized by recent artificial intelligence (AI) and animation research (e.g., [11, 32] ). However, research has focused on nonplayer characters or enabling simple high-level control of a simulated avatar, e.g., making a bipedal character walk in a desired direction. This has obvious applications in action games such as God of War [27] or Uncharted [20] , which presently need vast quantities of animation data to enable such high-level control. However, there are also various games such as Angry Birds [25] and QWOP [2] , where the player directly controls simulations on a low level. In such games, predictive simulation seems less explored. This observation prompts our primary research question: What novel possibilities and challenges emerge from combining predictive simulation with low-level direct control of simulated characters?"]}
{"intro": ["Measuring the impact of a workplace building layout on faceto-face communication is an important step, not only to validate architects' objectives, but also to enable the evaluation and reconsideration of traditional design principles. Studies in architectural design, such as the work of Thomas Allen [1] , consider how organizational structure and spatial configuration of work environments combined to influence communication between employees. However, these studies suffer from a crucial shortcoming: they lack reliable means of measuring face-to-face interactions in the workplace. Traditional approaches to evaluating the use of spaces in buildings rely on ethnographic studies where observers track employees over a period of time, or on self reports and surveys. Both approaches can deliver biased results, either because participants adapt their behavior when they know they are being observed [27] , or because they tend to offer socially desirable responses to surveys [3] . Furthermore, studying the impact of a building's layout on social behavior is challenging considering the large number of variables that can affect such behavior. For example, different types of organizational structure may affect social behavior more significantly than space layout."], "relatedWork": [], "rq": ["Research Question 1:However, informal or unplanned meetings between people separated horizontally into different subgroups of the formal organizational structure may be more affected by space [11] , which would present problems for communication for inspiration. In investigating our next research question, we proceed to test whether this is the case."]}
{"intro": [], "relatedWork": [], "rq": ["DiscussionIn answering our research questions about the effects of personalized scaffolding, we found some evidence for the fact that effective learning goal recommendations can scaffold self-directed learning and increase task performance. Recommending learning goals, either adapted to the users' task through the domain model (fixed scaffolding), or in addition adapted to the users' skills by taking into account the user model (personalized scaffolding), resulted in statistically significant effects in relation to the control condition for two of the dependent variables (task performance and perceived support). For both dependent variables, not only were the results of statistic significance, but also highly significant from a practical perspective. As to the performance, participants in the experimental conditions solved three times as many exercises correctly as in the control condition. In terms of perceived support, the difference between the means on the measurement scale was larger than 5.8 points which equates to a mean difference of over one point on a four point Likert scale. An important question is why the two experimental conditions did not differ with regard to these dependent variables. Clearly, we were not able to produce as strong effects with the recommendations based on the user model as human tutors were able to (e.g. in [12] ). Of course, a human tutor might be more sensitive to the exact feedback to give, tutors also serve additional functions (like motivational support), and in the Azevedo case, the tutor was also asked to only provide procedural and strategic scaffolding. When looking at number of learning goals selected in our study, however, there was a small but significant effect in that personalized scaffolding led to a smaller number of selections than the other two conditions. We take this as promising evidence that personalization was successful (as it reduced search), but that it did not produce effects on task performance or perceived support."]}
{"intro": ["companions [11, 12] . As the capability of artificial intelligence increases, this trend is likely to continue. However, one of the greatest challenges to introducing social robots into new domains is the time-consuming, tedious nature of designing interaction behaviors for all the scenarios the robot may encounter.", "The research question we sought to answer with this work is whether it is possible to build a memory system for human-robot interaction in a data-driven way. To our knowledge, there have not been any previous data-driven, behavior-learning systems designed and evaluated for the purpose of modeling memories in human-robot interaction. The most similar work to our proposed method is context-sensitive recurrent neural architectures [13] [14] [15] . But, they have only been applied to unimodal interaction data (dialogs) without automatic speech recognition errors, which are common in HRI. Furthermore, we go a step further than training a testing a model (Section 6) by first analyzing the types of memory-dependent behaviors that occur in conversation that helps to resolve ambiguous customer questions. However, the topics modeled there are at a course level, and only the most recent topic is considered in deciding the android's action. However, our system can respond to specific actions that occur anywhere in the interaction history."], "relatedWork": [], "rq": ["INTRODUCTIONThe research question we sought to answer with this work is whether it is possible to build a memory system for human-robot interaction in a data-driven way. To our knowledge, there have not been any previous data-driven, behavior-learning systems designed and evaluated for the purpose of modeling memories in human-robot interaction. The most similar work to our proposed method is context-sensitive recurrent neural architectures [13] [14] [15] . But, they have only been applied to unimodal interaction data (dialogs) without automatic speech recognition errors, which are common in HRI. Furthermore, we go a step further than training a testing a model (Section 6) by first analyzing the types of memory-dependent behaviors that occur in conversation that helps to resolve ambiguous customer questions. However, the topics modeled there are at a course level, and only the most recent topic is considered in deciding the android's action. However, our system can respond to specific actions that occur anywhere in the interaction history."]}
{"intro": [], "relatedWork": ["Meta-learning and algorithm selection have been studied extensively for supervised learning (Brazdil et al. 2003; Thornton et al. 2013 ), but much less for clustering. There is some work on building meta-learning systems that recommend clustering algorithms (Souto et al. 2008; Ferrari and de Castro 2015) . However, these systems do not take hyperparameter selection into account, or any form of supervision. More related to ours is the work of Caruana et al. (2006) . They generate a large number of clusterings using K-means and spectral clustering, and cluster these clusterings. This meta-clustering is presented to the user as a dendrogram. Here, we also generate a set of clusterings, but afterwards we select from that set the most suitable clustering based on pairwise constraints. The only other work, to our knowledge, that has explored the use of pairwise constraints for algorithm selection is that by Adam and Blockeel (2015) . They define a meta-feature based on constraints, and use this feature to predict whether EM or spectral clustering will perform better for a dataset. While their meta-feature attempts to capture one specific property of the desired clusters, i.e. whether they overlap, our approach is more general and allows selection between any clustering algorithms."], "rq": ["Question Q5: evaluating the clusterings on internal criteriaIn this section, we will investigate the trade-off between the ARI and two internal measures: the silhouette index (SI) (Rousseeuw 1987 ) and the density-based cluster validation (DBCV) score , both of which were also used in answering the previous research questions. The SI was chosen as it is well-known, and the extensive studies by Arbelaitz et al. (2013) and Vendramin et al. (2010) identify it as one of the best performing measures. The DBCV score was chosen as it is one of the few internal measures that does not have a spherical bias, and instead is based on the within-and between-cluster density connectedness of clusters. Although it does not have a spherical bias, the DBCV score comes with its own limitations; for example, it is strongly influenced by noise, and biased towards imbalanced clusterings (Van Craenendonck and Blockeel 2015) . Both of them range in [\u22121, 1], with higher values being better. Figure 4 shows how well the semi-supervised methods score on the internal measures for six datasets. In most cases, COBS performs comparable to its competitors. A notable exception is the parkinsons dataset, for which FOSC-OpticsDend produces clusterings that score significantly higher on both the DBCV score. Interestingly, the ARI of these clusterings is near zero. For parkinsons, the clusterings with the highest ARI score low on the internal measures. This, however, does not necessarily imply that the clustering does not identify any inherent structure (although this can be the case), it only means that it does not identify structure as it is defined by the silhouette score (i.e. spherical structure) or the DBCV score (i.e. density structure)."]}
{"intro": [], "relatedWork": ["In light of this, Wixon [16] argued that usability methods should be discussed as they are used in practise, rather than from an academic viewpoint. He further states that the usability evaluation methods are discussed in academia mostly in terms of qualities that are not relevant in practise. For example, usability tests are judged depending on how many flaws can be found with how many participants. However, in the practise, it is more important whether the method encourages participation, buy in, and collaboration by the development team. He calls for more case studies in real engineering, corporate, and political settings on how real products have been developed. Metastudies of such case studies can reveal more general findings. An example of a case study in practise is given by F\u00f8lstad et al. [17] . An important finding was that HCI practitioners tended not to evaluate their practice with regard to its impact on the development team and project leader."], "rq": ["Research MethodWe applied grounded theory [21] to analyse our data. Grounded theory is a systematic qualitative research methodology in the social sciences that emphasizes generation of theory from data gathered in the process of conducting research. Due to the small sample size and choice of projects studied, we cannot provide a theory. However, we used this systematic approach to handle the richness of the qualitative data we gathered. The main steps in the analysis were coding, memo writing, axial coding, sorting, and writing of findings. After each interview, we conducted open coding, meaning that we indexed the whole interview with codes as they came to mind, independently of previous findings. Memo writing served to condense the findings and identify the first results. Then, concepts and categories emerged from the codes. In the middle and at the end of the interviewing phase we conducted axial coding. In axial coding we looked through all interviews and codes and recoded all interviews such that the emerging categories were well covered. At the end of the process, we had 175 codes and 9 families. Some codes were too specific and did not belong to any family. The codes were then sorted graphically to establish which codes belong together and how they influence each other. We do not show the codes for spatial reasons. We describe the results in the main part of the paper, see Section 6, on the ground of three main categories: work context (research question 2), motivation, and work practice (research question 1) of each role. Then we indicate the strategies that employees used to overcome the obstacles in their daily work (research question 3). In the end we analyse the impact of the different factors on user-centred design."]}
{"intro": ["One possible way to solve this problem is to share the videos with public speaking experts and get subjective feedback. Challenges include identifying those experts and paying for their time. Another possibility is to obtain cheap micro-level annotations in the cloud using crowdsourcing [26] . However, crowdsourcing poses an inherent threat to the speaker's privacy. Speakers may not feel comfortable about real people viewing their videos and judging their speaking performance and body language. In this paper, we address this challenge by developing a fully automated framework that allows users to obtain feedback while being in complete control of their data."], "relatedWork": [], "rq": ["RESULTSIn the second research question, we want to evaluate the efficacy of the algorithm from human perspective. On this regard, we require annotations on the accuracy of each timeinstances. However, this is a huge task as there are as many as 50 time-instances for each pattern and 5 patterns per video. It was impractical to ask the participants to annotate the accuracy for each instance in the video. We solve this problem by recruiting thirty workers in the Mechanical Turk 2 website. In order to ensure high quality in the answers, we accept the turkers who completed at-least 1000 tasks with 99% acceptance rate. In addition, we perform a qualifying round; where we manually selected 30 turkers based on their performance on annotating a ground truth. These filtering and qualification round techniques were performed in light to the work of Mitra et al. [20] ."]}
{"intro": ["In the first few years of CENS, the focus was on developing smart dust technologies. \"Smart dust\" refers to intelligent static sensors that can be deployed by the hundreds or even thousands (Embedded Everywhere 2001; Warneke et al. 2001) . Smart dust sensors, as originally envisioned, would be scattered around the field site, with the sensor devices themselves blending unobtrusively into the background. The goal of smart dust systems was for the sensor and network to do all of the work -the sensing, the reasoning, and the adapting -without the presence or assistance of human researchers. Static smart dust remains the most common conception of sensors used in environmental research in the popular press (see for example, Lohr 2010) . Within CENS, however, the vision for environmental sensing systems changed dramatically from smart dust-like systems to systems that emphasized mobility, flexibility, and human participation in the sensing process. Why did such a change in direction occur? In this paper, we trace how this shift involved a confluence of topics important to science studies: the challenges of infrastructure development, the difficulties of interdisciplinary collaboration, and the situated nature of research practices."], "relatedWork": [], "rq": ["DiscussionMoving from static to dynamic sensing involved a process of unearthing the infrastructure, both conceptually and, in some cases, literally. By pulling the sensors out of the ground (or wherever their static installations were located) and providing ways to move them through the environment, CENS research shifted from static to dynamic infrastructure development. We contrast our characterization of CENS researchers as unearthing their infrastructure with Bowker's (1994) notion of \"infrastructural inversion.\" Infrastructural inversion is a methodological move that involves examining the infrastructural changes that preceded or accompanied a particular claim. CENS researchers adjusted their views about the components and configurations of their sensor technologies because of the difficulties encountered in reliably deploying static sensor networks. It would be a stretch, however, to say that CENS researchers themselves performed an infrastructural inversion because the changes in assumptions and technologies that occurred in CENS were the result of years of iterative technology development, testing, and refinement with particular scientific research questions as goals. It is certainly true that CENS technology development, both the processes and the products, changed significantly over the life of the Center. If infrastructural inversion does not encapsulate that change, another concept, or set of concepts is required."]}
{"intro": [], "relatedWork": [], "rq": ["Objectives, rationale and research questionsQuestions like these mark out the domain CSR projects are expected to explore. They provide the rationale for the structure of the CSR research agenda depicted below (cf., Figure 1 ). This structure takes account of a dictum ascribed to Max Planck: \"Knowledge has to precede application\" (Gruss, 2002) . However, it also takes account of the fact that research with a view to supporting engineering must not be confined to an ivory tower. Rather, it should be motivated by and cater to real needs, in line with the strategic goals of (public) European research funding. The two boxes at the bottom of the above diagram represent a wide range of possible scenarios that may provide this motivation, and also guide and validate the research effort proper. In fact, most CSR projects are committed, one way or another, to some concrete application scenario pertaining to one of these boxes."]}
{"intro": ["Over the last 20-30 years children have become important users of technologies, such as the Internet, smart phones, and iPads. However, in 2002 Druin [5] wrote that 'a child's role in the design of new technology has historically been minimized'. According to Druin this was caused by several factors, such as getting access to children who attend school all day, existing power structures, biases and assumptions between adults and children, and young children's difficulty in verbalizing their thoughts. Since then, much has happened, especially through journals and conferences dedicated to this topic, such as the International Journal of Child Computer Interaction and the Interaction Design and Children conference. However, while clear advances have been made for typically developing children, it may be less common to involve developmentally diverse children in design. In this paper we present a literature review of how this group of developmentally diverse children has been included in the design process, focusing on their role and the methods and techniques used. More specifically, we aim to answer the following research questions:"], "relatedWork": [], "rq": ["INTRODUCTIONOver the last 20-30 years children have become important users of technologies, such as the Internet, smart phones, and iPads. However, in 2002 Druin [5] wrote that 'a child's role in the design of new technology has historically been minimized'. According to Druin this was caused by several factors, such as getting access to children who attend school all day, existing power structures, biases and assumptions between adults and children, and young children's difficulty in verbalizing their thoughts. Since then, much has happened, especially through journals and conferences dedicated to this topic, such as the International Journal of Child Computer Interaction and the Interaction Design and Children conference. However, while clear advances have been made for typically developing children, it may be less common to involve developmentally diverse children in design. In this paper we present a literature review of how this group of developmentally diverse children has been included in the design process, focusing on their role and the methods and techniques used. More specifically, we aim to answer the following research questions:"]}
{"intro": [], "relatedWork": [], "rq": ["Contrasting the Knowledge Bases (RQ1)Related work suggests that with adequate support the output of non-professionals can be improved to match that of the professionals in many cases [5, 21, 28] . In our study, the best treatments by non-professionals were assessed very similarly to the best by professionals. We find this remarkable, given how the professionals' treatments were carefully curated by a group of seasoned LBP professionals with decades of practical knowledge from the field. The low-performing treatments by non-professionals, however, were clearly assessed lower than the worst professionals' treatments (Table 4) .", "Contrasting the Knowledge Bases (RQ1)Crowdsourcing literature suggests that laymen's contributions, when augmented and filtered in novel ways, can often replace the need for experts [27] . In our case, however, the professionals were quick to point that the treatments by non-professionals need to be scientifically validated before being even considered to be included as treatment suggestions in any kind of national authoritative guidelines. This type of manoeuvring between medical liability and practical help has been also identified earlier as one of the key challenges in medical peer-support systems [15] . There must exist a clear division between official treatments by clinical professionals and our crowdsourced treatment database accessible via Back Pain Workshop.", "Contrasting the Knowledge Bases (RQ1)In addition to simply providing \"good treatments\", the nonprofessionals' knowledge base is valuable in other ways as well. Based on an informal content analysis, the treatments in the non-professionals' knowledge base were not as medically exactly articulated in nature as in the professionals' one. They also at times included typos, bad grammar, and upper-case text. However, textual nonprofessional descriptions on which treatments seem to work can be used by professionals in learning about the realworld experiences of potential patients [20] ."]}
{"intro": ["Assessing another person's interruptibility prior to interaction with them is a natural human behaviour [24, 48] that is generally easily handled by the human brain. However, creating such capability in the context of a machine, so that there is harmonious synchronicity with human behaviour, is a significant challenge that has important ramifications for the demands placed upon a user. Historically, interruptibility has been studied in static task-oriented environments such as offices, using desktop computers (e.g., [20, 12, 26, 41] ), or in controlled laboratory simulations (e.g., [16, 4, 39] )."], "relatedWork": [], "rq": ["Personalisation vs composite modelsAs machine learning involves the use of at least one of these approaches, all works highlighted in Table 6 provide a basis for addressing this research question. However, we note that the number of comparative works is limiting (e.g. Pejovic and Musolesi [43] compare learning from a combined set of unordered cases and personalised ordered cases). Additionally there has been little empirical investigation into a framework for facilitating a hybrid approach to training data, where any complexity and accuracy trade-offs could be improved upon."]}
{"intro": ["The user-centred design paradigm requires interfaces to minimize cognitive complexity (Thomas and John 2009) . This also affects the choice of vocabulary to be used in the interface. In general, the research advocates using simple vocabulary which requires little processing from the user (Thomas and John 2009 ). However, a collaborative application for expert-layperson encounter has two users: the expert may prefer professional vocabulary, while the layperson may have problems understanding it. In other words, the user preferences concerning vocabulary will differ between the expert and the layperson. There are arguments going each direction. It has been shown, that expert and layperson understand each other better if the expert adapts layperson's lexicon (Bromme et al. 2005b) . Adapting a simple vocabulary reduces the negative effects of Bthe course of expertise^, i.e., the too optimistic experts' assumptions about laypersons' knowledge (Hinds 1999; Jucks and Bromme 2007) . This line of reasoning supports the use of simple vocabulary in collaborative interfaces. However, there are strong arguments for using expert vocabulary too. Primarily, confronting a novice with professional termini enhances their literacy in the specific domain, which in turn rises the efficacy of the layperson (Ozer and Bandura 1990; Bandura 1997; Yaniv and Kleinberger 2000; Topol 2015) . As a consequence, the layperson can better interact with other stakeholders in this domain and, thus, become independent from the expert. A collaborative interface can support this development by introducing expert vocabulary in its interface to be taken up by the collaboration partners. But what if the use of expert vocabulary undermines the mutual understanding? This research explores how the use of professional vocabulary in a collaborative application designed for expert-layperson encounter affects the interaction."], "relatedWork": [], "rq": ["Collaboration as negotiating and sharing meaningAgreement on word meaning and reference is a complex process (Clark 1992; Clark and Wilkes-Gibbs 1986) . First, the involved parties implicitly identify differences and lack of agreement if it comes to a meaning or a reference. Then, they locate the disagreement in a series of turns. And, finally, they engage in a recursive process which ends up with a mutual acceptance of a word's meaning (Clark and WilkesGibbs 1986) . This shows that establishing a meaning of a word is a highly collaborative process which involves communication and coordination, as well as negotiation if such is necessary (Clark 1996; Clark and Wilkes-Gibbs 1986) . As explained, those processes can become more complicated if an external actor or IT proposing its terminology gets introduced into the setting -the number of potential candidate references and meanings grow which makes the identification, location and negotiation of a disagreement more complex. However, visual support can also make conversational coordination more efficient (Brennan 2005) . IT, if carefully designed, can even take the role of a boundary object (Star 2010; Star and Griesemer 1989) , i.e., a computer can provide room for flexible interpretations, such that each group of users sees them as coherent with their own environment, while keeping a fixed and unambiguous core. While collaborative technology has been proposed as a boundary object in multiple scenarios (Henderson 1991; Lee 2007; Star 2010) , it is necessary to delineate the original concept from the notion of technology proposed in this manuscript. A boundary object allows members of different groups to read different meanings specific to their needs from the same material (Henderson 1991 ) -the meanings they identify do not need to be identical, but form a common denominator such that work on the individual or collaborative tasks can be continued (Lee 2007) . However, this can exactly pose danger to the effect of a service encounter: if an advisee accepts his own, inadequate vision of a concept which is not equal with the advisor's one, and does not negotiate it, he may fail in the implementation phase. In other words, while research has identified potential of technology for sharing core meaning across groups of users, no conclusive answer can be given on whether IT use will make vocabulary transfer and entrainment easier (due to its boundary character) or harder (due to its role as a third collaborator with its own lexicon). Thus, the research questions we ask in the current study are neither trivial nor irrelevant."]}
{"intro": ["However, the robot is also faced with another, often more critical burden of conveying its intent [3] , e.g. which of the two bottles it is going to pick up to clean in Fig.1 . In robotics and animation, this is often achieved by legible motion, that is intent-expressive -it enables the inference of intentions [4] , it is \"readable\" [5] , \"anticipatory\" [6] , or \"understandable\" [7] .", "The writing domain, however, clear distinguishes the two. The word legibility, traditionally an attribute of written text [11] , refers to the quality of being easy to read. When we write legibly, we try consciously, and with some effort, to make our writing clear and readable to someone else, like in Fig.1(top,  right) . The word predictability, on the other hand, refers to the quality of matching expectation. When we write predictably, we fall back to old habits, and write with minimal effort, as in Fig.1(top, left) ."], "relatedWork": [], "rq": ["B. Connection to Psychologyobserver were willing to provide examples of what they ct, the robot could learn how to act via Learning from onstration [24] - [26] or Inverse Reinforcement Learning [29] . Doing so in a high-dimensional space, however, is an active area of research. cond, the robot must find a trajectory that minimizes C. is tractable in low-dimensional spaces, or if C is convex. e efficient trajectory optimization techniques do exist for dimensional spaces and non-convex costs [30] , they are ct to local minima, and how to alleviate this issue in ice remains an open research question [31] , [32] ."]}
{"intro": [], "relatedWork": [], "rq": ["Correlation of CAF in writing abilityThe main (quantitative) finding obtained with regard to the second research question is that every CAF domain improved at the group level based on stanines. CAF components for writing ability were also highly correlated with each other suggesting that they are interconnected. Particularly, fluency is the most strongly correlated with proficiency. This finding is consistent with the one obtained by Larsen-Freeman (2006) who reported an increase in every CAF domain as proficiency improved. It is also partly consistent with Robinson's (2001 Robinson's ( , 2005 cognition hypothesis which states that it is not only possible but also natural that complexity and accuracy receive concurrent attention from the learner. However, contrary to Robinson who states that fluency develops separately, this study found that fluency increased in tandem with complexity and accuracy."]}
{"intro": ["Apart from the product and process approach, which has been widely used to teach writing, the genre-based approach (GBA) has also been proposed. There are three schools or models of genre each focusing on different concepts: English for Specific Purposes (ESP), Systematic Functional Linguistics (SFL), and the New Rhetoric (NR). However, this study focuses only on the Systematic Functional Linguistics (henceforth SFL) because the concept and details of the SFL genre meet the content of the target course adopted in this study. A review of the literature reveals that numerous studies have employed the GBA (SFL) to equip Thai tertiary students with needed writing skills (Krisnachinda, 2006; Kongpetch, 2006; Chaisiri, 2010) . These studies have reported that the students' writing ability has been improved and they also have developed positive attitudes toward the GBA. However, most research in Thailand has been conducted with English-major students who usually have quite good command of English language skills. This study investigates the writing achievement of undergraduate engineering students for whom the writing skill is also necessary not only for their academic but also their professional success. Beer (2005) argues that over 40% of the work time of engineers is spent on writing and ranks the ability to write as the most important skill in engineers' success. For the Thai context, teaching engineering students is seen as a challenge because many of them have poor English background knowledge. Most of them study only a few English courses in the first and second academic years, as evidenced in the findings of Wattanasakulpusakorn (1996) , who found that undergraduate engineering students had limited writing ability and were unable to pass writing exams. Thus, it was interesting to employ the GBA to determine if this particular approach could help improve the engineering students' writing ability. Therefore, the present study aims at answering the following research questions:"], "relatedWork": [], "rq": ["INTRODUCTIONApart from the product and process approach, which has been widely used to teach writing, the genre-based approach (GBA) has also been proposed. There are three schools or models of genre each focusing on different concepts: English for Specific Purposes (ESP), Systematic Functional Linguistics (SFL), and the New Rhetoric (NR). However, this study focuses only on the Systematic Functional Linguistics (henceforth SFL) because the concept and details of the SFL genre meet the content of the target course adopted in this study. A review of the literature reveals that numerous studies have employed the GBA (SFL) to equip Thai tertiary students with needed writing skills (Krisnachinda, 2006; Kongpetch, 2006; Chaisiri, 2010) . These studies have reported that the students' writing ability has been improved and they also have developed positive attitudes toward the GBA. However, most research in Thailand has been conducted with English-major students who usually have quite good command of English language skills. This study investigates the writing achievement of undergraduate engineering students for whom the writing skill is also necessary not only for their academic but also their professional success. Beer (2005) argues that over 40% of the work time of engineers is spent on writing and ranks the ability to write as the most important skill in engineers' success. For the Thai context, teaching engineering students is seen as a challenge because many of them have poor English background knowledge. Most of them study only a few English courses in the first and second academic years, as evidenced in the findings of Wattanasakulpusakorn (1996) , who found that undergraduate engineering students had limited writing ability and were unable to pass writing exams. Thus, it was interesting to employ the GBA to determine if this particular approach could help improve the engineering students' writing ability. Therefore, the present study aims at answering the following research questions:"]}
{"intro": ["However, subsequent developments have led to increasing skepticism. For example, after significant accuracy problems [24] and scientific criticism [95] , Flu and Dengue Trends folded in the summer of 2015 [154] . and scattered, as are experimental contexts. Further, we suspect many additional studies yielded negative results and were not published. That is, measuring disease using internet data works some of the time, but not always. We ask: when does it work and how does it work?"], "relatedWork": [], "rq": ["Forecasting doesn't work (RQ3)1-week and 2-week forecasts perform poorly (r 2 = 0.41 and 0.51, respectively). In addition to greater noise, the forecasts show a tendency for spikes (e.g., January 2014 in Figure 6b ) and an \"echo\" effect of the same length as the forecast horizon. We speculate that the latter is due to training data that includes summer periods when phase does not matter; when applied to times near the peak when it does, the model cannot compensate. These results contrast with prior work. For example, we reported several situations with high correlations between Wikipedia article traffic and official data under a forecasting offset [57] . However, this work had no independent test set; i.e., it reported training set correlations, which are very likely to be higher than test set correlations. Similarly, Bardak & Tan [13] reported linear regression models whose performance was best at a 5-day forecast. This work tested a large number of algorithms and parameterizations and used internal crossvalidation rather than a test set that strictly followed the training, the latter being a more realistic setting."]}
{"intro": [], "relatedWork": [], "rq": ["How do we work with found data?The study we present in this paper asks and answers our empirical research question about the extent to which bot-bot reverts in Wikipedia constitute bot-bot conflict. However, in this paper, we also frame our approach to answering this particular empirical question as an epistemological issue in computational social science: when working with large-scale \"found data\" [36] of the traces users leave behind when interacting on a platform, how do we best operationalize culturally-specific concepts like conflict in a way that aligns with the particular context in which those traces were made? In other words, how do we integrate the rich, thick descriptions of qualitative contextual inquiry in the practice of computational social science to arrive at a more holistic analysis? How can a research team both dive deep into the complexity of particular cases and scale up to the massive size of a dataset like those found in contemporary social computing platforms?", "Mathbot's disambiguation links.This can be considered a task conflict between the two bots in Hinds & Bailey's typology, although there was no conflict at all between the two bots' developers. Unlike the conflict over Mathbot, there was no fundamental disagreement about whether these references ought to appear or not that was fought through bots. Both bots were developed to cooperate with each other and their developers fully agreed with each other about what tasks ought to be done and how, but the bug caused CyberBot II to change its task from fixing references to removing them. When the issue was raised to CyberBot II's developer by multiple Wikipedians, the developer quickly disabled the bot, investigated, identified the bug, fixed it, re-enabled the bot, and wrote a short note explaining what went wrong. 17 7 COMMENT CODING 7.1 Overview The previous section gave qualitative descriptions of different kinds of bot-bot revert events, giving enough of the local context to understand to what extent they are or are not conflict. On its own, these cases are sufficient counter-examples to critique the assumption that bot-bot reverts necessarily indicate conflict. These cases also gave us a set of different kinds of bot-bot revert events, which we had classified as various kinds of bot-bot conflict or non-conflict. However, we still did not have an answer to our empirical research question: How many of the bot-bot reverts in our dataset are instances of genuine conflict like the one between AnomieBot and CyberBot II, and how many were cases of routine, even collaborative work like fixing double redirects? As we had hundreds of thousands of revert events, manually reviewing and investigating them all ourselves was not feasible. Because the work requires substantial trace literacy of the Wikipedian community, crowdsourcing approaches would also be lacking."]}
{"intro": ["We evaluate two classes of computational models for logical metonymy. The classes represent the two main current approaches in lexical semantics: probabilistic and distributional models. Probabilistic models view the interpretation as the assignment of values to random variables. Their advantage is that they provide a straightforward way to include context, by simply including additional random variables. However, practical estimation of complex models typically involves independence assumptions, which may or may not be appropriate, and such models only take first-order co-occurrence into account 1 . In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem. Distributional models typically do not require any independence assumptions, and include second-order co-occurrences. At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010) ."], "relatedWork": [], "rq": ["IntroductionWe evaluate two classes of computational models for logical metonymy. The classes represent the two main current approaches in lexical semantics: probabilistic and distributional models. Probabilistic models view the interpretation as the assignment of values to random variables. Their advantage is that they provide a straightforward way to include context, by simply including additional random variables. However, practical estimation of complex models typically involves independence assumptions, which may or may not be appropriate, and such models only take first-order co-occurrence into account 1 . In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem. Distributional models typically do not require any independence assumptions, and include second-order co-occurrences. At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010) ."]}
{"intro": ["There are many forms of caregiving that are often undervalued and unseen within society. Caregivers of many kinds share similar burdens and experiences, such as prioritizing others' well being above their own, feelings of guilt for taking time for self-care, and care-related stress [12, 32, 81, 82] . However, caregivers are also a diverse population juggling both their own unique needs and the unique needs of those they care for. In particular, the number of informal caregivers who are caring for family members with Alzheimer's Disease (AD) is rising dramatically in the United console) and in which a myriad of user activities may serve as input (e.g., step count while grocery shopping, vacuuming etc). Use of a mobile platform, such as a smartphone, enables the creation of more accessible interventions by removing locational and temporal constraints often present with console-based exergames [92] . With pervasive exergames, the activities that people do throughout their day (and in varied settings) serve as input to the game experience [25, 77] . These games have a great potential to more effectively address barriers to PA that caregivers face, such as the unpredictability and frequency of caregiving responsibilities, as they offer and encourage a wide range of PA experiences [49, 98] . However, work is needed that characterizes how pervasive exergames can be made accessible, useful, and engaging for the AD caregiver population, given the significant challenges that they face.", "Additionally, research has explored how technological innovations can support various caregiver populations with their caregiver responsibilities [37] , and the potential for technology-mediated social support amongst caregivers (e.g., through online support communities [87] ). However, despite the fact that caregivers' needs are interdependent [12] , prior work has typically explored the design of technologies that address discrete caregiver needs (e.g., self-care through PA or increased social connectedness). Therefore, research is needed to examine the specific barriers that AD caregivers face to wellness, and how these barriers can be addressed in concert. Our work seeks to address this research gap through a formative study exploring how pervasive games can jointly address barriers to PA and social connectedness in the AD caregiver population. Taking this approach allows us to explore the broader question of how technology can simultaneously address multiple caregiver needs; such an approach has the potential to be an effective and efficient means of enabling impactful engagements with health technologies in a population that is significantly overburdened. Our work is guided by the following research questions:"], "relatedWork": [], "rq": ["INTRODUCTIONAdditionally, research has explored how technological innovations can support various caregiver populations with their caregiver responsibilities [37] , and the potential for technology-mediated social support amongst caregivers (e.g., through online support communities [87] ). However, despite the fact that caregivers' needs are interdependent [12] , prior work has typically explored the design of technologies that address discrete caregiver needs (e.g., self-care through PA or increased social connectedness). Therefore, research is needed to examine the specific barriers that AD caregivers face to wellness, and how these barriers can be addressed in concert. Our work seeks to address this research gap through a formative study exploring how pervasive games can jointly address barriers to PA and social connectedness in the AD caregiver population. Taking this approach allows us to explore the broader question of how technology can simultaneously address multiple caregiver needs; such an approach has the potential to be an effective and efficient means of enabling impactful engagements with health technologies in a population that is significantly overburdened. Our work is guided by the following research questions:", "Designing for Minimized InteractionsBeyond goal-setting an important area of health technology research is nurturing app engagement. Prior work has demonstrated that health apps often have high drop-off rates, with users quickly abandoning them [24, 26] . In response to this trend, one of the primary threads of research within health technology research has been exploring how to increase interaction with health systems. This goal has been sought in part because consistent engagement with the technology is seen as necessary to give users sufficient exposure to the technology to induce change. However, our work suggests that a different design orientation and research question is needed for the AD caregiver population, that is: what is the minimal level of engagement needed to support change? Framed differently, there is a need for work that examines the sweet spot of minimizing user interaction and maximizing benefits."]}
{"intro": [], "relatedWork": [], "rq": ["What content attracts new participants?We explore these research questions using nine months of data from a Facebook page titled \"Valor por Michoac\u00e1n SDR 1 \"(VXM), which translates as \"Courage for Michoac\u00e1n.\" This page was created to inform people of \"unsafe situations\" or \"situaciones de riesgo\" (SDR) in the state of Michoac\u00e1n. This type of activity has been reported in previous work that examined how residents of communities afflicted by violence used Twitter to form alert networks and help one another identify potential danger on the streets [29] . However, shortly after its creation, VXM began to focus more on reporting about the activities of an armed group in the region, the self-defense forces [12] ."]}
{"intro": [], "relatedWork": [], "rq": ["DISCUSSION AND CONCLUSIONSIt is worth noting some differences between document retrieval using a bag of words and frame retrieval using a bag of visual words: 1) because visual features overlap in the image, some spatial information is implicitly preserved (i.e., randomly shuffling bits of the image around will almost certainly change the bag-of-visual-words description). This is in contrast to the bag-of-words representation of text, where all spatial information between words (e.g., the word order or proximity) is discarded. 2) An image query typically contains many more visual words than a text query, as can be seen in Fig. 8 , a query region of a reasonable size may contain 30-100 visual words. However, since the visual words are a result of (imperfect) detection and also might be occluded in other views, only a proportion of the visual words may be expected to match between the query region and target image. This differs from the web-search case, where a query is treated as a conjunction, and all words should match in order to retrieve a document or web page. 3) Internet search engines exploit cues such as the link structure of the Web [6] and web page popularity (the number of visitors over some period of time) to compute a static rank [31] of web pages. This query independent rank provides a general indicator of a quality of a web page and enables more efficient and in some cases more accurate retrieval. For example, the inverted file index can be ordered by the static rank, allowing the retrieval algorithm to access the high-quality documents first. An interesting research question would be to develop an analog to static ranking for video collections."]}
{"intro": ["Recent research in neuroscience successfully began to analyze the connectivity data using graph theoretical methods [20] and statistics [43] . However, visualization systems can provide significant insights for the discovery of unforeseen structural correlation patterns, in particular across several datasets. For example, comparing patterns of functional connectivity and anatomical connectivity pre-and post-removal of parts of brain tissue may help neuroscientists to understand how the brain rewires itself to restore its function. Although statistical and graph theoretical methods are available for such analysis, visualization systems featuring connectivity comparison tools can provide significant insights for the discovery of unanticipated correlation patterns. Visual graph comparison, thus, can be an essential tool for comprehensive brain connectivity analysis."], "relatedWork": [], "rq": ["Weighted Graph Visualization and ComparisonComparison of weighted graphs is an open research question. However, various comparison techniques are proposed for unweighted graphs in a number of domains for problems related to characterization of metabolic pathways [8, 36] , business process models [2] and software evolution [11] ."]}
{"intro": [], "relatedWork": [], "rq": ["MethodologyAs the data was a kind of document, they would be included as qualitative data (Creswell, 2013) . In this paper however data collection was also in form of quantitative, as an instrument called AntConc v 3.4.3w would be used to draw information. Later in the discussion, the interpretation would be provided firstly by quantitative data to answer the first research question. The quantitative analysis employed chi-square test by Microsoft Excel 2010. Qualitative interpretation, which discourse analysis was used to extract further analysis, was then delivered to support the quantitative findings (Creswell, 2004) ."]}
{"intro": [], "relatedWork": ["In healthcare, a multitude of neuropsychological tests have been developed and used by clinicians for assessing cognition. Mini Mental State Examination (MMSE) [46] , Montreal Cognitive Assessment (MoCA) [29] and Addenbrooke's Cognitive Examination-III (ACE-III) [15] are widely used for cognitive impairment screening and monitoring in clinical settings. Currently, these assessment tools are typically paper-based and not designed for self-administration [15] . Thus, it is infeasible to run the tests frequently to monitor changes in cognitive functions over time due to learning effects [46] , costs and resource requirements around availability of qualified clinical staff to administer them. These can adversely affect the ability of clinicians to detect the early signs of decline in cognitive functions, potentially delaying diagnosis and treatment as well as undermining the effectiveness of medication or other interventions [37] . These limitations have led to a call for alternative approaches. Taking advantage of computerised assessments may provide more precise test results, better control of stimulus presentation and ease of administration to assess cognitive abilities. One of the most widely used computerised tools for cognitive measurements in clinical research is the Cambridge Neuropsychological Test Automated Battery (CANTAB). The CANTAB system includes various tests measuring a range of cognitive functions, e.g. executive function, attention, memory and decision making [55] . It has been extensively used to assess cognitive functions in older people [32] , athletes with exposure to repeated brain injuries [7] , for paediatric neuropsychological assessment [25] , HIV dementia patients [34] and alcohol drinkers [14] . However, trained clinical personnel are required for protocol administration of CANTAB.", "\"Serious games\" have recently gained increasing research attention for their potential to improve sustained participation in continual assessment and therapy by incorporating elements of fun and user engagement in their design [10, 11, 26, 27, 47] . Serious games are those designed for some additional purpose beyond pure entertainment, such as: training, marketing, communicating, assessing and/or enhancing cognitive and physical health [33] . For instance, a tabletop-gaming platform in the Eldergames project was developed to improve cognitive functions in older adults. Their findings showed that their interactive tabletop games were wellaccepted with regard to usability and reported to create a positive experience [11] . However, this approach requires space for equipment setup and is thus not feasible for running in a large-scale experiment. In contrast, the ubiquitous computing power of modern mobile devices offers promising solutions for data collection and processing for cognitive assessment and monitoring outside clinical settings. Mobile versions of serious games have been developed to simulate common daily activities such as cooking [27] and supermarket shopping [54] , in order to assess and help improve cognitive functions among people with mild cognitive impairment (MCI). To complete tasks in the game scenarios, a multitude of cognitive processes were involved, e.g. object recognition, attention, visual search, memory and executive functions. By comparing in-game task performance and classic cognitive assessments, e.g. MMSE and TMT [45] , their findings demonstrated significant correlations between variables in the games and results from standard cognitive measures. Unlike the simulation-based designs that artificially represent real-world scenarios in such games, replicating a popular casual game Whack-a-Mole presents more game-like attributes and reduces the feelings of being tested [47] . A Go/No-Go discrimination task has also been incorporated into serious games to measure cognitive inhibition. The significant correlations between median response time and cognitive test scores suggested that this in-game feature could be used as a predictor for cognitive status. A recent systematic review found that the use of gamified tasks can help improve drop-out rates in longitudinal studies including a reduction in test anxiety [26] . Because of their entertaining nature, these game-based assessments were reportedly well received by the users even in older adults. In spite of common misperceptions, a systematic review reported that older adults enjoyed video games and benefited from game-based cognitive intervention [20] . This was supported by a recent report demonstrating that 23 per cent of the U.S. gamers were 50 years and older [41] . Hence, these studies have emphasised the potential of serious games as highly engaging cognitive assessments to monitor changes in cognition outside of a clinical environment for populations with cognitive disorder across age groups.", "It is important to note that all these studies have explored hand movement in non-time-dependent tasks, such as handwriting. However, gameplay hand movement is closely related to user reactions on game stimuli. The characteristics of touch gestures in games are highly dependent on the time the user perceives stimuli in the game and the limited time they have to perform a specific gesture. Therefore the shape, speed and length of a gesture can be different, depending on the time it takes to perceive a game trigger. For example, a slow response time in identifying a game object that a player needs to interact with (e.g. in \"Fruit Ninja\" spotting a fruit that is about to move out of the screen) could result in a faster and more erratic gesture in order to complete the gesture in the reduced time available. Previous studies have shown that mental fatigue [21] and age [8] adversely affect the speed of processing resulting in slower reaction time. This means that in certain games, faster and more erratic gestures could be an indicator of slower response time to visual stimuli, and therefore indicative of cognitive decline."], "rq": ["Addressing Research Questions7.1.1 RQ1: Are the swipe length and shape of touch gestures related to changes in cognitive performance? and RQ2: Is the speed of touch gestures related to changes in cognitive performance? We found that overall swipe speed features were positively correlated with TMTA and TMTB in Tetris and Candy Crush, while overall swipe speed features in Fruit Ninja and Candy Crush were positively correlated with RESIN. These results imply that increases in swipe speed were associated with decreases in performance on these cognitive functions (visual search, mental flexibility and response inhibition). This finding demonstrates a clear diversion from the studies on traditional handwriting movement [28, 36, 44] . Indeed, in handwriting studies, higher hand movement speed is correlated with increase in cognitive performance. However, we consider that the seemingly contradictory finding can be explained by the significant differences in user intention, between handwriting and gameplay. Based on gameplay observations, we noted that fast and erratic gestures in games like Fruit Ninja, tend to occur when players narrowly miss certain objects that they are expected to interact with, within a limited time frame. We therefore hypothesise that the temporal nature of game interaction can lead to more erratic and fast gestures, as the cognitive abilities of the players decline. The positive correlation between the directness index of swipes and response inhibition ability as well as the swipe patterns idiosyncratic to Fruit Ninja provide evidence to support this explanation. It is consistent with the negative correlation we identify between VISP and swipe speed in Fruit Ninja. The results seem to suggest that these fast swipes, demonstrate decreases in visuospatial abilities, and are indeed rushed movements."]}
{"intro": [], "relatedWork": [], "rq": ["RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsHinduja and Patchin (2010) have reported that like traditional bullying, cyberbullying includes \"being ignored, disrespected, picked on, or otherwise hassled\" [12] (p. 208). However, when newer technological features are used to debase people, such as spreading rumors, stalking, or threatening, cyberbullying is more harmful and dangerous than traditional bullying. With the rapid changes in technology in recent years, it is important to revisit the effects of newer technology (e.g., apps) on cyberbullying.", "RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsJacobs et al. [13] conducted focus-groups with cyber victims aged 12 to 15 and developed a coding scheme for cyberbullying. They found: a) common forms and consequences of victimizations; b) victims' perceptions and attitudes towards cyberbullying; c) reasons for cyberbullying; and d) reactions after being cyberbullied. However, the authors report that most findings are explained by the previous literature, and they did not hold follow-up individual interviews so that participants could share their experiences more privately or deeply.", "RQ1) What effect does the increasing prevalence of image and video content, compared to text, in messaging appsRecent efforts in the HCI literature have focused on mechanisms to counter cyberbullying. Sutherland et al. [25] studied users' readiness for reporting cyberbullying to authorities as a function of severity of bullying in animated scenarios. Fan et al. [10] designed a social media app to foster positive online behavior and prevent cyberbullying. Ashktorab and Vitak's [2] participatory design study identified effective cyberbullying interventions. Our research, however, focuses on understanding the current state of cyberbullying, given the emergence of features that were not explored in past research [19] . Design interventions are believed to be more likely to succeed if built on a strong understanding of how the features of these new apps affect cyberbullying."]}
{"intro": [], "relatedWork": ["In this section, we overview the current landscape of automated testing and input generation tools for Android, discussing limitations of these approaches while illustrating CRASHSCOPE'S novelty in context. Several approaches for detecting and reproducing crashes are available in literature [30] - [32] , [39] , [40] , [43] , [51] , [52] , [63] , [64] , [69] , [74] , [78] , [80] - [82] ; however, we forgo discussion of these approaches, as they are not presented in the context of mobile apps, and hence do not consider the unique associated challenges."], "rq": ["IV. EMPIRICAL STUDY 1: CRASH DETECTION CAPABILITYThe goal of our first study is to evaluate the effectiveness of CRASHSCOPE at discovering crashes in Android apps as compared to state-of-the-art approaches for testing mobile apps. The quality focus of this first study concerns the fault detection capabilities of CRASHSCOPE in terms of locating crashes. The context of this study consists of 61 open-source Android apps previously used to evaluate automated testing approaches in [29] , as well as five approaches for automated input generation (listed in Table II) . We investigated the following research questions (RQs): A. Methodology In order to compare CRASHSCOPE against other stateof-the-art automated input generation tools for Android, we utilized a subset of subject apps and tools available in the Androtest testing suite [8] , [29] . We chose to perform this study on a subset of the tools offered by the Androtest artifact due to runtime issues, namely, some tools would not run consistently on the set of provided subject apps (e.g., the tools would launch an emulator but not the app), causing inconsistent results we chose to exclude. However, when contacted, the authors of the tool were helpful in supporting us. We believe the tools tested against constitute a diverse representation of the publicly available Android testing tools. The Androtest suite contains 68 subject applications for testing; however, when recompiling the applications to run the tools and extract the apps from the VM to run with CRASHSCOPE, seven of the subject apps failed to compile with the instrumentation necessary to gather code-coverage results. Therefore, each tool in the suite was allowed to run for one hour for each of the remaining 61 subject apps, five times, whereas we ran all 12 combinations of the CRASHSCOPE strategies once on each of these apps. It is worth noting that the execution of tools in the Androtest suite (except for Android monkey) can not be controlled by a criteria such as maximum number of events."]}
{"intro": [], "relatedWork": [], "rq": ["ACCOMMODATION & CONTEXT UDPATE.Immediately related to this question is the mounting theoretical and experimental evidence that triggers may be \"hard\" or \"soft\" depending of the possibility of accommodation (Abusch, 2010; cf. Jayez et al, 2014) and the extent to which accommodation is possible (Beaver & Zeevat, 2012) , which brings us to the third research question that explores whether different triggers may exhibit different online processing behaviors. Anaphoric triggers such as pronouns and also/too are found to be hard to accommodate, whereas factives and aspectual predicates are found to accommodate easily (Schwarz, 2014) . This distinction between hard and soft triggers raises the questions of how context update may work when considered in light of the memory retrieval process. Accommodation of a presupposition can be thought of as a recovery strategy that is initiated when memory retrieval fails. It could be that the distinction between hard and soft triggers is directly related to memory retrieval mechanisms. For instance, a direct-access, cue-based retrieval mechanism may be best suited for hard triggers during the processing of presuppositions since these require exhaustive search of context which would be burdensome for a search-based mechanism. However, the situation is less clear for soft triggers. The fact that they are easier to accommodate and perhaps even non-anaphoric in nature suggests that the consequences of triggering an accommodation process due to an accidental memory retrieval failure are less severe. This suggests that the constraints on possible memory mechanisms may be looser for soft triggers than for hard ones, suggesting several empirically testable possibilities."]}
{"intro": ["On the one hand, the purposefully design trustless mining protocol [6] [41] [46] does not require a third-party entity to authorize transactions but merely miners' consensus, which in turn supports people's trust in blockchain [41] [57] . On the other hand, the emerging social organization of mining practices brings forward issues of trust among miners such as the risk of 51% attack [15] [19] or of selfish miners [16] [25] [47] [55] , explored mostly within the security research area. Relevant HCI works on blockchain and its trust related issues have started to emerge [33] [48] [49] . We agree with the argument that blockchain offers a unique perspective to explore trust as its characteristics contrast with the centralized, regulated, and nonanonymous traditional transaction systems which have informed the existing HCI models of trust [23] [45] . However, apart from modeling-based security research on mining, we know little about miners' practices from their first-person perspective, and how the specific blockchain's characteristics impact on their trust. To address this gap, we report on interviews with 20 Bitcoin blockchain miners about their mining practices and related trust challenges, in order to explore the following research questions: 1. Which are miners' motivations for bitcoin mining? 2. Which are Bitcoin blockchain's' characteristics impacting on miners' trust and its dimensions? 3. Which is the social organization of mining practices: are there different approaches and types of miners? 4. Which are the main trust challenges and how do people attempt to mitigate them?"], "relatedWork": [], "rq": ["IntroductionOn the one hand, the purposefully design trustless mining protocol [6] [41] [46] does not require a third-party entity to authorize transactions but merely miners' consensus, which in turn supports people's trust in blockchain [41] [57] . On the other hand, the emerging social organization of mining practices brings forward issues of trust among miners such as the risk of 51% attack [15] [19] or of selfish miners [16] [25] [47] [55] , explored mostly within the security research area. Relevant HCI works on blockchain and its trust related issues have started to emerge [33] [48] [49] . We agree with the argument that blockchain offers a unique perspective to explore trust as its characteristics contrast with the centralized, regulated, and nonanonymous traditional transaction systems which have informed the existing HCI models of trust [23] [45] . However, apart from modeling-based security research on mining, we know little about miners' practices from their first-person perspective, and how the specific blockchain's characteristics impact on their trust. To address this gap, we report on interviews with 20 Bitcoin blockchain miners about their mining practices and related trust challenges, in order to explore the following research questions: 1. Which are miners' motivations for bitcoin mining? 2. Which are Bitcoin blockchain's' characteristics impacting on miners' trust and its dimensions? 3. Which is the social organization of mining practices: are there different approaches and types of miners? 4. Which are the main trust challenges and how do people attempt to mitigate them?"]}
{"intro": [], "relatedWork": ["There is long-standing research on emotion assessment from physiological signals [1] , [22] , [23] , [24] , [14] , [25] . Among these studies, few of them achieved notable results using video stimuli. Lisetti and Nasoz used physiological response to recognize emotion in response to movie scenes [14] . The movie scenes elicited six emotions, namely, sadness, amusement, fear, anger, frustration, and surprise. They achieved a high recognition rate of 84 percent for the recognition of these six emotions. However, the classification was based on the analysis of the signals in response to preselected segments in the shown video known to be related to highly emotional events.", "Takahashi [15] recorded EEG and peripheral physiological signals from 12 participants. He then classified the responses to emotional videos into five classes, namely, joy, sadness, disgust, fear, and relax. He achieved the accuracy of 41.7 percent using EEG signals. However, the feature level fusion (FLF) of EEG signals and peripheral physiological signals failed to improve the classification accuracy.", "Eye gaze and pupillary responses have been used extensively to measure attention. However, we are not aware of research on how emotions affect eye gaze while watching videos; therefore, the eye gaze itself has not been used for emotion recognition. The pupillary response is the measurement of pupil diameter over time. Pupils can dilate or constrict in response to light, cognitive, attentional, and emotional stimuli [27] , [28] . Gao et al. [29] showed the significance of using pupillary reflex for emotion assessment after reducing the light effect using a real-time feedback."], "rq": ["CONCLUSIONSThis paper showed the performance of an interparticipant emotion recognition tagging approach using participants' EEG signals, gaze distance and pupillary response as affective feedbacks. The feasibility of an approach to recognize emotion in response to videos is shown. Although the results were based on a fairly small video data set due to experimental limitations, the promising accuracy can be scalable to more samples from a larger population. The improved performance using multimodal fusion techniques leads to the conclusion that by adding other modalities, such as facial expressions, accuracy as well as robustness should further improve. Results from our previous studies [3] showed that there is a significant difference between peoples' emotional self-assessments in response to videos. However, there usually exists one most popular emotional tag for which there is significant agreement in a population. This \"most popular emotion\" has been shown to be detectable with monitoring users' bodily responses. Moreover, the population tags give the retrieval system a higher chance of success in a given population. We have shown that it is possible to design an accurate and user-independent classification protocol to recognize emotions from pupillary reflex, EEG signals in response to video content. Moreover, we have shown that for the video data set utilized, the nonverbal affective cues can replace affective self-report with comparable emotion recognition performance and no requisite of direct user inputs. We can thus answer positively to our two research questions. Thierry Pun received the electrical engineer diploma in 1979. He received the PhD degree in image processing for the development of a visual prosthesis for the blind in 1982 from the Swiss Federal Institute of Technology, Lausanne. He is head of the Computer Vision and Multimedia Laboratory, Computer Science Department, University of Geneva, Switzerland. He was a visiting fellow from 1982 to 1985 at the National Institutes of Health, Bethesda, Maryland. After being a CERN fellow from 1985 to 1986 in Geneva, Switzerland, he joined the University of Geneva in 1986, where he is currently a full professor in the Computer Science Department. He has authored or coauthored about 300 full papers as well as eight patents. His current research interests, related to affective computing and multimodal interaction, concern: physiological signals analysis for emotion assessment and braincomputer interaction, multimodal interfaces for blind users, data hiding, multimedia information retrieval systems. He is a member of the IEEE."]}
{"intro": ["Over 70% of people who smoke are in frequent contact with physicians, nurse practitioners, counselors, therapists, and other clinicians [59] . We henceforth use the term providers to refer to this collection of practitioners. Strategies have been recommended for in-person smoking cessation counseling to tailor to specific needs of individuals [43, 59] . However, primary care physicians and residents are known to face common barriers such as lack of training, time, and resources to support smoking cessation and conflicts in priorities while managing other health conditions [15, 39] . Physicians are thus recommended to redirect clients to dedicated tobacco cessation counsellors and/or telephone based quitline counselors for specialized counseling on quitting smoking [39] ."], "relatedWork": [], "rq": ["Designing individualized technology to quit smokingNeeds and perspectives of providers, who are also important stakeholders in the treatment of nicotine addiction, are relatively less studied to inform design of technology. Recent independent survey studies in the US [42] and Australia [67] showed providers have primarily positive attitudes towards potential use of technology for smoking cessation support. Both providers and clients preferred features in applications that allow users to track their progress, personalize, match, and adapt to changing interests and needs of clients, and help manage withdrawal symptoms and medication needs for nicotine addiction [27, 42] . However, majority of providers did not consider current apps to be effective for smoking cessation [42] . Research is needed to further incorporate perspectives and expertise of providers on how these needs can be addressed through designing technology for individualized support. Understanding strategies and barriers to in-person counseling can provide insights into opportunities for technology to build upon these strategies and address challenges in practice. To develop an in-depth empirical understanding of provider practices and inform design for individualized needs, we investigated the following research questions: "]}
{"intro": ["Identification and categorization of registers and genres has a long history. Early studies on specialized languages centered on the role of registers following Firth's (1957) concept of lexical collocation. Firth's theory of text cohesiveness and register types is based on the idea that text register and text cohesiveness can be determined by the distribution of the words in a text and their combinations. Later work in register analysis followed Halliday (1978) in which linguists identified special registers on the basis of lexical aspects, which were considered sufficient in themselves in order to distinguish specific registers. While these early register studies generally focused on isolated words and their frequency within texts, they did not consider how registers compared to each other in their respective differences. Recently, however, many linguists have begun to focus on the study of registers from a comparative perspective known as register variation (Biber 1988; Conrad & Biber 2001) . While much of this current work is located under the paradigm of multi-dimensional variation analysis (Biber 1988 ) and considers the co-occurrence of syntactic constructions, only some of the research considers how lexical items can be used to analyze register variation (e.g.. Biber, Conrad & Cortes 2004) . Of specific importance to this study is the analysis of variation based on shared bigrams, or the co-occurrence of words across corpora, to categorize texts into different dimensions of register variation."], "relatedWork": ["Though this overview is far from complete, it does illustrate that n-gram analyses are a useful tool in corpus linguistic analyses. Thus, n-gram analysis was selected as an approach to register analysis for this study. However, in a similar fashion to Peng et al. (2003) , and following the computational linguistic standards of Jurafsky and Martin (2000) , the current study only considers bigrams. This is because unigrams do not capture enough of the syntactic and semantic context and larger n-grams, such as trigrams and quadgrams, create sparse data problems. Bigrams, on the other hand tap into both the paradigmatic and syntagmatic features of the text and, in addition to being extremely simple to compute, they have been found to be effective in many computational applications to include text categorization."], "rq": ["4.\uf6dc CorporaThe Map Task Corpus is a tasked-based corpus that is the linguistic product of a cooperative task involving two participants. The Instruction Givers have a marked route on their map and give directions to the Instruction Followers who have no route. The maps are not identical, which elicits unscripted problem solving dialog. Because of the domain, we predicted a strong spatial predisposition in the lexical and syntactic collocations of this dialog. However, to ensure that any Map Task Corpus findings were based purely on their spatiality, another task-based corpus was selected to include in the analysis: the TRAINS Corpus. This corpus is based on the routing and scheduling of freight trains. The corpus shares with Map Task its basis as a task based corpus, but it is more temporal and directional in nature than the spatial Map Task Corpus. A selection of non-task based spoken dialogs were also included in the analysis. Following the methodology of Biber (1988) , the spoken dialogs found in the London Lund Corpus were included in the analysis and broken up into six different speech situations: spontaneous speech, prepared speech, face to face conversations, telephone conversations, interviews, and broadcast speech. Our primary purpose in including the LLC was to use it as a means to compare natural dialogs and task-based dialogs. As such, we were also interested in the possibility that bigrams might be powerful enough to delimit natural dialogs from task-based dialogs in a factor analysis based on the idea that task-based dialogs were more instructional in nature and depended on a more controlled lexical domain. Because there has been ample attention given to dialectical differences between American and British spoken dialects (Biber 1987; Helt 2001) , we also included American spoken dialogs. These included the Santa Barbara Corpus and the Switchboard Corpus. The Santa Barbara Corpus is a collection of natural speech recordings taken from people across the United States. The Switchboard Corpus, on the other hand, is a collection of about 2,400 two-sided random topic telephone conversations taken from 543 speakers from all areas of the United States. Since one of Biber's (1988) primary research questions was the delineation of spoken and written dimensions, we also included the LOB and the Brown corpora. The Brown corpus is a collection of written American texts published in 1961. It comprises 500 text samples of about 2,000 words each and totals about one million words. Each text sample is categorized into one of fifteen registers including religion, science, fiction, humor, and press reports. The LOB Corpus is a direct replication of the Brown Corpus, but is based on 1961 text samples taken from British written sources. Based on the work of Biber (1988) and Louwerse et al. (2004) it was thought that the written corpora would be distinguished from the spoken corpora as a result of the written texts being more integrated and less fragmented and involved. A brief description of the corpus used in this investigation is found in Table 1 ."]}
{"intro": ["A Patient Reported Outcome Measure is any report of the status of a patient's health condition that comes directly from the patient, without interpretation of the patient's response by a clinician or anyone else [5] . PROMs are regularly acquired by healthcare professionals through administering questionnaires to patients. The necessity for hospitals to acquire PROMs is based on the need for providing evidence of performing value-based health care. Up till recently PROM data were mainly collected with paper-and-pencil methods, and since a few years Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. HRI '18 Companion, March 5-8, 2018 e-health solutions such as apps on tablets or smartphones are used, where answer options can be selected by touch buttons. However, elderly people often do not have the e-health literacy for using these devices [6] , or find the technology difficult to use due to their disabilities and chronic diseases. A social robot which can conduct a verbal dialogue, supported by gestures and an answer display, would not require e-health literacy from the patient. There is evidence that such social robots are perceived as more supportive than e-health solutions such as an interactive tablet [2] . Our research question was therefore twofold: 1) how to design a dialogue, using a social robot with a screen, for acquiring PROMs by direct verbal and visual communication, and 2) what is the resulting usability in terms of user satisfaction, effectiveness and efficiency?"], "relatedWork": [], "rq": ["INTRODUCTIONA Patient Reported Outcome Measure is any report of the status of a patient's health condition that comes directly from the patient, without interpretation of the patient's response by a clinician or anyone else [5] . PROMs are regularly acquired by healthcare professionals through administering questionnaires to patients. The necessity for hospitals to acquire PROMs is based on the need for providing evidence of performing value-based health care. Up till recently PROM data were mainly collected with paper-and-pencil methods, and since a few years Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. HRI '18 Companion, March 5-8, 2018 e-health solutions such as apps on tablets or smartphones are used, where answer options can be selected by touch buttons. However, elderly people often do not have the e-health literacy for using these devices [6] , or find the technology difficult to use due to their disabilities and chronic diseases. A social robot which can conduct a verbal dialogue, supported by gestures and an answer display, would not require e-health literacy from the patient. There is evidence that such social robots are perceived as more supportive than e-health solutions such as an interactive tablet [2] . Our research question was therefore twofold: 1) how to design a dialogue, using a social robot with a screen, for acquiring PROMs by direct verbal and visual communication, and 2) what is the resulting usability in terms of user satisfaction, effectiveness and efficiency?"]}
{"intro": ["Native Americans comprise an exceptional class of citizenship within the U.S. While many Native Americans are voting members of tribal nations, they are also eligible to vote in local, state, and national elections. However, the historically agonistic relationship between the U.S. federal government and Native American nations, has discouraged Native American individuals from engaging with electoral politics in the U.S. [25, 49, 57] . Moreover, Indian Country 1 , which is associated with some of the largest Native American voting blocs, suffers from a lack of communications infrastructure 2 , limiting Natve American individuals' potential for political engagement through digital means. To demonstrate the critical need for Internet infrastructure in Indian Country, it is necessary to understand the discursive qualities and data characteristics of political content disseminated across Internet Protocol (IP) networks. Indeed, the U.S. Government Accountability Office has recently issued a statement outlining the need for data surrounding tribal Internet access [23] ."], "relatedWork": ["A large body of work has explored information and communication technologies (ICTs) and political engagement, and it is clear that the Internet enables new grassroots movements to quickly materialize and operate for a period of time [20, 21, 6, 7, 19] . The Mexican Zapatista movement of the 1990s provides a prime example of the success social movement organizations (SMOs) can achieve by networking over the Internet [20] . Prior studies have commented on the balkanization that occurs in political social networks on Twitter, where actors divide into affiliate networks, reducing exposure to opposing viewpoints [26] . However, for marginalized social groups, sharing political viewpoints within affiliate networks can become a source of in-group validation and motivation for political mobilization [12, 56, 31] . While social media platforms can empower marginalized groups, limited Internet access and connectivity continues to trouble Indian Country. According to the Federal Communications Commission (FCC), fewer than 15% of people living on tribal land have broadband access [18] . As is typical for infrastructurepoor, rural areas, many reservations depend on wireless networks to extend residential broadband access [2, 16] . In some communities, this is accomplished through a combination of wireless backhaul links connecting homes to the Internet over Wi-Fi [51, 48] or TV whitespaces [58] ; in others, residents rely on cellular network coverage to access the Internet from home [24, 34] ; in still others, people must travel what can be tens of miles in order to reach the nearest access point, typically located in private businesses bordering tribal land or in tribal libraries and media centers [34] . The dependence on wireless technology leads to connection opportunities that are either limited because of their financial expense (in the case of data subscriptions), attenuated performance over long distance operations (in the case of microwave and satellite), or excessive time requirements (in the case of opportunistic transactions made from a municipal cellular or Wi-Fi hotspot)."], "rq": ["Bandwidth CharacteristicsWe address RQ3 in light of Indian Country's infrastructural limitations described in the Introduction and Related Works sections. We investigate the impact media richness has on the propagation of individual tweets in the Native American advocates data set. We argue that all tweets are essentially bulletins that enable asynchronous interaction between the poster and audience. However, the richness of individual tweets can vary considerably depending on the presence, type, and size of media embedded in the tweet. Of the 5,172 unique tweets we observe in the Native American advocates data set, 1.7% contain embedded video content, 35.8% contain embedded photo content, and 62.5% do not contain any embedded content. Per Daft and Lengel's definition, we consider tweets with embedded media to be richer than those that lack embedded media [14, 35] . Moreover, we consider tweets with embedded videos or GIFs to be richer than tweets with embedded photos based on the fact that such media offers the \"simultaneous transmission of multiple information cues\" [35] . Similarly, we consider tweets with embedded videos to be richer than tweets with embedded GIFs, as the audio component lends the expression of a greater \"variety of languages\" [35] ."]}
{"intro": ["Mobile phones have become truly ubiquitous and have permeated each and every walk of life; they accompany us throughout the day, from first thing in the morning to last thing at night. However, in conjunction with the meteoric rise in mobile device ownership, researchers have begun to highlight a number of growing pains such as the impact on social order when using technology during collocated interactions [30, 40, 44, 52] . Given the increased prevalence of mobile devices, an arguably important yet understudied research question we seek to address in this paper is how do individuals conduct the interactional work of interleaving mobile device use with their ongoing conversation?", "Existing work within the HCI community has highlighted both how mobile devices provide eminent support for distributed interactions and the power of mobile devices to assist in connecting non-collocated individuals [19, 27] . Moreover, mobile devices have been purposed for use in collaborative task situations amongst collocated groups [24, 35, 39] . However, in spite of considerable progress in mobile technology, critical voices from academic [53] to popular [55] writers have pointed out the ways in which mobile devices may isolate people from one another in social situations. On the other hand, socio-technical studies have shown people are skilled at interleaving mobile device use and social interaction, for example in a living room setting [47] and in a collaborative photo-taking setting [16] ."], "relatedWork": [], "rq": ["INTRODUCTIONMobile phones have become truly ubiquitous and have permeated each and every walk of life; they accompany us throughout the day, from first thing in the morning to last thing at night. However, in conjunction with the meteoric rise in mobile device ownership, researchers have begun to highlight a number of growing pains such as the impact on social order when using technology during collocated interactions [30, 40, 44, 52] . Given the increased prevalence of mobile devices, an arguably important yet understudied research question we seek to address in this paper is how do individuals conduct the interactional work of interleaving mobile device use with their ongoing conversation?"]}
{"intro": ["We investigate this research question on the example of discriminative training for patent translation, using the algorithm for multi-task learning with 1 / 2 regularization presented by Simianer et al. (2012) . We compare multi-task learning on \"natural\" tasks given by IPC sections to multitask learning on \"random\" tasks given by random shards and to baseline models trained on independent tasks and pooled tasks. We find that both versions of multi-task learning improve over independent or pooled training. However, differences between multi-task learning on IPC tasks and random tasks are small. This points to a more general regularization effect of multi-task learning and indicates a broad applicability of multi-task learning techniques. Another advantage of the 1 / 2 reg-ularization technique of Simianer et al. (2012) is a considerable efficiency gain due to parallelization and iterative feature selection that makes the algorithm suitable for big data applications and for large-scale training with millions of sparse features. Last but not least, our best result for multitask learning improves by nearly 2 BLEU points over the standard MERT baseline."], "relatedWork": ["In this paper we apply the multi-task learning technique of Simianer et al. (2012) to tasks defined as IPC sections and to random tasks. Their algorithm can be seen as a weight-based backward feature elimination variant of Obozinski et al. (2010) 's gradient-based forward feature selection algorithm for 1 / 2 regularization. The latter approach is related to the general methodology of using block norms to select entire groups of features jointly. For example, such groups can be defined as non-overlapping subsets of features (Yuan and Lin, 2006) , or as hierarchical groups of features (Zhao et al., 2009 ), or they can be grouped by the general structure of the prediction problem (Martins et al., 2011) . However, these approaches are concerned with grouping features within a single prediction problem whereas multitask learning adds an orthogonal layer of multiple task-specific prediction problems. By virtue of averaging selected weights after each epoch, the algorithm of Simianer et al. (2012) is related to McDonald et al. (2010) 's iterative mixing procedure. This algorithm is itself related to the bagging procedure of Breiman (1996) , if random shards are considered from the perspective of random samples. In both cases averaging helps to reduce the variance of the per-sample classifiers."], "rq": ["IntroductionWe investigate this research question on the example of discriminative training for patent translation, using the algorithm for multi-task learning with 1 / 2 regularization presented by Simianer et al. (2012) . We compare multi-task learning on \"natural\" tasks given by IPC sections to multitask learning on \"random\" tasks given by random shards and to baseline models trained on independent tasks and pooled tasks. We find that both versions of multi-task learning improve over independent or pooled training. However, differences between multi-task learning on IPC tasks and random tasks are small. This points to a more general regularization effect of multi-task learning and indicates a broad applicability of multi-task learning techniques. Another advantage of the 1 / 2 reg-ularization technique of Simianer et al. (2012) is a considerable efficiency gain due to parallelization and iterative feature selection that makes the algorithm suitable for big data applications and for large-scale training with millions of sparse features. Last but not least, our best result for multitask learning improves by nearly 2 BLEU points over the standard MERT baseline.", "ConclusionOur research question regarding the superiority of \"natural\" or \"random\" tasks was shown to be undetermined for the application of patent translation. The obvious question for future work is if and how a task division can be found that improves multi-task learning over our current results. Such an investigation will have to explore various similarity metrics and clustering techniques for IPC sub-classes (W\u00e4schle and Riezler, 2012a) , e.g., for the goal of optimizing clustering with respect to the ratio of between-cluster to within-cluster similarity for a given metric. However, the final criterion for the usefulness of a clustering is necessarily application specific (von Luxburg et al., 2012) , in our case specific to patent translation performance. Nevertheless, we hope that the presented and future work will prove useful and generalizable for related multi-task learning scenarios."]}
{"intro": ["The internet connects diverse communities globally, with English as the dominant language of communication. As there is an exponential growth in the number of internet users from different languages and cultures, various social media networking sites have emerged to connect them through online communication for different purposes. Although English is primarily used as a medium of communication virtually, such as in online chat and e-mails, there are other forums for regional and local communication where normally languages other than English are used. On these forums, people often mix English language with their regional languages for effective communication; however, this results in code switching of different types. The research shows that this phenomenon has been largely ignored in sociolinguistics studies of computer-mediated communication (Siebenhaar, 2006) , therefore, the current study focuses on this issue in Saudi Arabian context.", "One of the central issues in bilingualism research is code-switching, which is the alternative use by bilinguals of two or more languages in the same conversation. Under this general term, different forms of bilingual behaviors are subsumed. Sometimes switching occurs between the turns of different speakers in the conversation, sometimes between utterances within a single turn, and sometimes even within a single utterance (Hudson, 1980) . However, in today\"s technological era, the question arises if code-switching on the internet occurs in the same way as face-to-face communication. It appears as there is no sufficient evidence in the Saudi context which could show the different functions of code switching in online communication, therefore, the current study examines the bilingual world on the internet and aims to answer the following research questions to fill that research gap."], "relatedWork": [], "rq": ["IntroductionOne of the central issues in bilingualism research is code-switching, which is the alternative use by bilinguals of two or more languages in the same conversation. Under this general term, different forms of bilingual behaviors are subsumed. Sometimes switching occurs between the turns of different speakers in the conversation, sometimes between utterances within a single turn, and sometimes even within a single utterance (Hudson, 1980) . However, in today\"s technological era, the question arises if code-switching on the internet occurs in the same way as face-to-face communication. It appears as there is no sufficient evidence in the Saudi context which could show the different functions of code switching in online communication, therefore, the current study examines the bilingual world on the internet and aims to answer the following research questions to fill that research gap."]}
{"intro": ["We understand that this negative perception is also due to the misconceptions or limited knowledge of the public. Certainly, their knowledge is driven by print and digital media. A recent survey of 1078 participants conducted in the UK investigated awareness of RAI. The results showed that 85% claimed to have heard of AI. However, it highlighted that most participants had a distorted understanding of AI [12] . The rationale for this limited knowledge lies in the complexity of forming public opinion, particularly in the cognitive miser theory [34] . The theory suggests that people form opinions and attitudes with incomplete information using cognitive shortcuts or heuristics mostly gleaned from mass media [34, 40] . However, research shows that negative perceptions can be reduced by giving exposure or experience with the technology. For instance, in the education domain, a study conducted with teachers from five EU countries to understand their views on the use of robots in education showed that teachers found robots to be a disruptive technology [41] . Although, other researchers highlighted that teachers' views were derived from limited understanding or no experience of robots [1, 52] . Importantly, they conducted studies and showed a transformation of perception after limited and longterm exposure to a robot in the classroom [1, 2, 52] ."], "relatedWork": [], "rq": ["FINDINGS, DISCUSSION AND FUTURE WORKRQ3 -Future Jobs: Older peoples' opinions about humans doing hazardous work were not affected by our exhibit whereas opinions of younger people were. Our hypothesis, H3 was partially accepted as we did expose one effect of our exhibit on participants' opinions. For one question scene, C, for all generations except BabyB+, the ratings of humans doing the task (QOp1) were lower for those exposed to the exhibit than for those not exposed. i.e. they thought that it was less likely that humans would be doing the task in 10 to 15 years' time. Scene C showed an \"Analog sensor read\". It could well be that most of our participants felt that it was likely this fairly simple task would be within the capability of robots soon and so humans would not be doing it, whereas the other scenes showed tasks that were viewed as less straight-forward by our participants. The fact that, for the other scenes, opinions were not significantly affected by exposure could mean we lacked enough data. However, we could interpret this as either a) our interactive exhibit strategy of presenting a balanced view, aimed at visitors seeking new information activating the Scientific Literacy Model [5] , was not effective as it only had a partial effect on opinions, or b) experiencing our exhibit mostly confirmed pre-existing opinions about the future of RAI in hazardous environments."]}
{"intro": ["In this paper, we focus on RPS pain synthesis, and clinician recognition. Clinicians need to both visually (and verbally) assess the amount of pain patients are experiencing in order to make treatment decisions. However, research suggests that clinicians are likely to both underestimate the intensity of patients' self-reported pain [47] , and they are also likely to overall interpret facial expressions with less accuracy than laypersons [21] . Some researchers have hypothesized this is due to the decline in empathy as clinicians progress through their training [27] . Unfortunately, ignoring patients' emotional needs can lead to incorrect diagnostic decisions, poorer health outcomes, and reduced patient satisfaction [70, 30] .", "Fortunately, training can be effective in improving clinicians' ability to detect patient facial expressions [23, 11] . Thus, RPSs may prove to be an excellent training tool to facilitate this learning process. Certainly this effect has been shown to be true in the virtual space (c.f. [20] ); however, it is difficult to use virtual simulations in situ at the bedside, where clinicians must visually assess patient faces while simultaneously conduct physical exams [29, 41] . Furthermore, augmented reality and projected displays are unfortunately ill-suited to most hospital simulation domains due to space and power issues [22] , and can cause physical barriers between the patient and clinician. Thus, perhaps physically-embodied, expressive RPS systems may be an effective educational intervention for patient facial expression decoding."], "relatedWork": ["Facial expressions of pain are important nonverbal communication signals, particularly in healthcare [25, 47] . Communicating pain via facial expressions is important because it signals to clinicians that a patient might need medical attention. While selfreporting is the main method of assessing pain, there are issues with this method. The most critical issue is that self-report cannot be used for children or patients with communication challenges such as patients with cognitive impairments or unconscious patients. In such cases, an observer must visually assess patients' pain. However, there are differences between how clinicians and how patients conceptualize pain (clinicians are substantially impaired at pain perception), which leads to problems when making diagnostic decisions for these patients [47, 66, 2, 39] ."], "rq": ["Pilot studyZhang et al. designed an activity for simulating different natural expressions including anger and disgust in their participants [68] . Therefore, the naturally stimulated anger videos had very low intensity compared to an acted dataset such as the one we used in our previous work [41] . Naturalistic anger source videos were hard to recognize due to their very low intensity. However, since our research questions were focused on pain, we decided to continue with the study and include anger and disgust in addition to pain to make sure we do not ask participants to only watch and label only one type of expression (pain).", "DISCUSSIONWe found support for our first research question, which supports earlier findings in the literature, that clinicians are less accurate than laypersons in decoding pain [21, 23, 43, 35, 27, 17] . Our work showed that this effect is found regardless of simulation embodiment (robot or virtual character), which suggests even the novelty of the embodiment is not sufficient to incur improved decoding skills. However, since training has been shown to help mitigate these clinical habituation effects, expressive, interactive RPSs may be useful in the medical education curriculum to be used alongside procedural skills training. For example, rather than having infrequent and contextless patient empathy training sessions [58] , clinical learners could be continually learning pain decoding skills as they practice other critical care skills on patient simulators (e.g., performing physical exams, assessing patient state, etc.)."]}
{"intro": ["At academic settings in particular, summarizing skills or techniques are essential to academic success. Students are usually required to produce study summaries, to complete various types of summary assignments, and to complete tasks that call for the incorporation of a written source material in term papers or any other similar presentation (Johns & Mayes, 1990; Kirkland & Saunders, 1991; Nunan, 2003; Brown, 2004) . However, this is not an activity to be expected to be carried out with the same degree of success by all language learners. After all, some more proficient language learners outperform the others and some others, with less background and practice in these skills, lag behind."], "relatedWork": [], "rq": ["III. DESCRIPTIVE AND INFERENTIAL STATISTICSThe differences between the ESP2 and PSP2 thus confirmed one more procedure was required to prove on which set did the subject group outperformed the other, on the ESP2 or on the PSP2.This demanded applying the T-test to determine the statistical difference between the means on the two paired sets of scores. Once the T-test was applied to the two paired sample groups (Tables 2-d and 2 -e, below), it was discovered that the difference in means between the two sets (ESP2 vs. PSP2) were 16.85 vs. 19 .42 with significance levels of 0.048 and 0.049. This indicates that there is a significant difference in performance between ESP2 vs. PSP2. Thus, as far as these particular sets of groups are concerned, the second research question as to the selection of the topic(s) is answered in favor of the subject groups in PSP2. However, in order to see whether or not the differences among the groups are statistically significant, the one-way ANOVA procedure was run. The results of the ANOVA procedure for the ESP3 and PSP3 are given below in Tables  3-3 According to Table 3 -b, the results of ANOVA procedure on mean differences of ESP3 post-test as reflected in the F-ratio of 5.074 and the significance level of 0.002 (sig<%5) indicate that because the level of statistical significance is lower than %5,it can logically be argued that there is a statistically significant difference among the subject groups. To locate where the differences between the groups lie, the post-hoc Tukey's HSD was applied resulting in Table 3 -c. A similar examination of the results of ANOVA on the PSP3 post-test with F-ratio of 5.809 and the statistical significance of 0.001 belonging to the means of the groups reveal that in this case because the significance level is lower than %5, it can be asserted that there is a statistically significant difference among the subject groups of PSP3. To locate the differences between the groups, the post-hoc Tukey's HSD was applied which resulted into Table 3 -c'."]}
{"intro": ["We perform most manipulations with our hands. However, the objects that we manipulate are often not positioned optimally, so that it may be uncomfortable or strainful to reach Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. UIST '18, October 14-17, 2018 them; in particular if the manipulation takes a fair amount of time. This is especially true for overhead interaction. Fortunately, virtual reality (VR) allows us to shift virtual hands to the overhead target, while our physical (real) hands remain at a lower and much more comfortable position. Such a shift, or translation, of the virtual hand space (VHS) has previously been considered for bringing distant targets into reach. For instance, the Go-Go interaction technique [37] provides a nonlinear transfer function that allows the user to reach distant objects with his virtual hand by simply extending his physical arm. More recently, Erg-O [33] describes a nonlinear mapping by which the interaction space is subdivided into tetrahedrons, within which the position of the virtual hands is dynamically adjusted to make virtual targets more easily accessible. These two techniques demonstrate the considerable potential of such an approach. However, three aspects in particular are not considered in previous work.", "Third, one of the most important aspects of interaction in VR, besides task performance, is that the users actually perceive their virtual hands as part of their body. This perception is commonly called the body ownership illusion or Virtual Hand Illusion (VHI) and leads to a more natural interaction, a stronger feeling of plausibility and presence [23] , and arguably also a lower cognitive load. The work mentioned above did not explore the feeling of body ownership regarding the shifted virtual hands. However, in other noninteractive experiments, the body ownership illusion was reported to break when the artificial hand was located too far away from the physical hand [22, 29, 34] . It remains for us to explore how the proposed VHS transformations affect the illusion of \"owning\" a virtual hand (in the sense of it being part of one's body) during interaction in immersive VR.", "In this paper, we address the three issues listed above. Accordingly, we present Ownershift, an interaction technique for reducing the strain of prolonged overhead tasks in VR. The technique enables users to reach quickly for any target with a 1:1 mapping, which is beneficial for very fast movements (see Fig 1, A) . However, if the overhead reaching pose is maintained for extended periods of time, the hand space is shifted slowly; the virtual hand moves upward slowly, gently guiding the user to gradually lower his own hand in order to maintain alignment with the target (Fig 1, B) . This leads to a less physically strained pose for prolonged interaction ( Fig. 1, C) . With our work, we contribute (1) an interaction technique that effectively reduces fatigue during overhead interaction in VR, (2) an evaluation of this technique in a user study comparing it with an instant shift and unaided overhead interaction, (3) evidence of the body ownership illusion, despite a large vertical offset between the real and the virtual hand."], "relatedWork": [], "rq": ["DISCUSSIONFrom our findings, we conclude that a vertical shift (65 cm) of the VHS can effectively reduce fatigue during overhead interaction (H1), while maintaining the illusion of owning the virtual hand (H2). Performance is only slightly affected by this shift (less than 4% decrease) and the user feels in control of the virtual hand (H3). The decrease in task performance could be addressed by increasing the size of the moving target; with a target of 3 cm radius, participants would have stayed within the target's bounds 90% of the time (in the Ownershift condition, 90% of measured errors were smaller than 0.03). Additionally, it stands to reason that training could improve task performance [19] . As interpreted by Kohli et al. [26] , in a warped virtual space, users are less precise at touching targets, but precise enough. However, it remains to be explored how a gradual shift might affect more delicate manipulations (e.g., drawing a straight horizontal line). We find that participants performed equally well with the gradual shift in the Ownershift technique, as when interacting with an instantly shifted virtual hand, which has earlier been shown to be more effective than retargeting through interpolation [19] . The qualitative results further suggest that a gradual shift is preferable, since it was described as much less noticeable and less disruptive. Furthermore, it did not require a mental recalibration phase, since the initial 1:1 mapping in Ownershift allows initial ballistic movements toward the target (RQ1). This is in line with earlier work [19] , which found that the instant shift was disorienting, when it meant that participants first had to look around to locate the hand, before being able to start interacting. Importantly, we found no evidence that task performance was diminished, while the VHS was gradually being shifted (RQ2)."]}
{"intro": ["Traditionally, personality assessments rely on survey questionnaires to quantify the five trait dimensions for each individual based on answers to a series of questions about dispositional tendencies in affect, cognition, and behavior [34, 35] . However, the past decade of research has revealed that digital traces of behavior created from the use of digital media technologies (e.g., social media, smartphones) can be analyzed to infer an individual's standing on these five trait dimensions of personality [38, 56] ."], "relatedWork": [], "rq": ["Personality Models in Multi-Cultural Settings (RQ1)Previous work [9, 10] has extensively used Call data for inferring personality traits, showing its predictive power especially for Agreeableness and Extraversion. In our models, however, Call features did not turn out to be predictive for these sociability-relevant traits. This might be due to a recent and considerable shift to Internet based call and messenger services such as FaceTime, WhatsApp, Viber and similar mobile apps 6 ."]}
{"intro": [], "relatedWork": [], "rq": ["Feature selection for K-means1. ACC and NMI illustrate that KFDRL performs better than other feature selection algorithms on most of the datasets. 2. The traditional dimensionality reduction algorithm LapScore, which is a modified for Laplacian Eigenmaps (LE), recognizes and utilizes manifold structure embedded in high dimensional data by graph Laplacian without learning mechanism. The results reported in Tables 3 and 4 show that the joint-framework or two-step algorithms can perform better than traditional and one-step algorithms. 3. In comparison to other algorithms, KFDRL demonstrated superiorities. First, it simultaneously benefits from manifold and discriminative information, because the objective function proposed in KFDRL combines manifold learning and discriminative regression learning. Second, the constraints imposed on the scaled assignment cluster matrix H and on the transformation matrix W can help it to find physically meaningful features (a forced constraint imposed to the scaled assignment cluster matrix H and the transformation matrix W can fit the physical meaning). The best results obtained with feature selection for each dataset have better performance than the clustering results obtained on the original datasets. It indicates that feature selection can not only reduce the size of date to increase the computation speed, but also efficiently remove the redundant and noisy information demonstrating the great significance of data pre-processing. 4. KFDRL cannot show a better performance than the other algorithms in a few cases. For example, it shows that JELSR performs better than KFDRL with ACC metric on Sonar dataset. Moreover, it shows that KFDRL performs worse than all features selected and DFSC with NMI metric on the Isolet dataset, as well as JELSR and LSPE on the Umist dataset. It is noted that these methods mentioned above are all single-step methods, which describe their better performance. We explain several reasons for the worse performance of KFDRL. First, Sonar and Isolet are both voice data. The advantage of KFDRL can be identified in the task of clustering image data because the embedded dimension and the number of classes are considered to be identical (Cai et al. 2008 (Cai et al. , 2011 Liu et al. 2012) , i.e. c m. However, it cannot have better results on the Sonar and Isolet datasets. We believe the embedding learning employed in JELSR and LSPE on the Umist dataset has superiority over other techniques. As reported in Tables 3 and 4 an algorithm resulting in a better performance of clustering different datasets than others does not exist. Hence, the best algorithm still needs to be chosen based on the nature of the dataset to be clustered and finding an algorithm best for clustering all the different datasets will be an open research question for future. 5. In general, KFDRL performs better than JELSR on most of the datasets, which indicates that the manifold learning alone is not efficient in many cases. Nonetheless, JELSR outperforms KFDRL on the Sonar and Umist dataset, which may be a result of the inherently discriminant structure of the datasets. Hence, a discriminative based algorithm cannot perform as well as it does in the case of non-discriminant datasets. It is worth reminding that KFDRL includes discriminative and manifold learning whereas JELSR is based on manifold learning only."]}
{"intro": ["Because of this period of in-the-field learning, when an intelligent agent's reasoning causes it to perform incorrectly or unexpectedly, only the end user is in a position to better personalize-or more accurately, to debug-the agent's flawed reasoning. Debugging, in this context, refers to mindfully and purposely adjusting the agent's reasoning (after its initial training) so that it more closely matches the user's expectations. Recent research has made inroads into supporting this type of functionality [1, 11, 14, 16] . Debugging, however, can be difficult for even trained software developers-helping end users do so, when they lack knowledge of either software engineering or machine learning, is no trivial task."], "relatedWork": [], "rq": ["Cost/BenefitIn theory, a sound mental model enables a person to reason effectively about their best course of action in a given situation [10] . Thus, we expected participants with sounder mental models (the With-scaffolding participants, according to the RQ1 results) to debug more effectively than those with less sound models. For example, knowing that the recommender could be steered more effectively by using unique, highly specific words (e.g., \"Merseybeat\") rather than broad, common descriptors (e.g., \"oldies\") should have helped such participants debug the agent's reasoning more effectively than participants who did not understand this. Surprisingly, when using participants' perceptions of cost/benefit as a surrogate for effectiveness, the soundness of participants' mental models showed little impact on this measure of debugging effectiveness. However, mental model transformation was tied with cost/benefit: participants who most improved the soundness of their mental models reported that the effort of debugging was significantly more worthwhile than participants whose mental models improved less, or not at all ( Table 2 , row 1 & Figure 4A )."]}
{"intro": ["Thus, this research is aimed to portray students' perceptions on their teacher's roles in EFL classroom by using social semiotic approach. The participants are the 11 th grade students in a state senior high school in Banjarmasin. The students express their perceptions on teachers' role through drawings. Clarebout et al (2007) argues that drawings can be used to identify nuances and ambivalences within a person's belief system, indicating they would be useful when studying pupil conceptions. This research strategy is increasingly being used to probe students' feelings about how they teachers accomplish their role in the classroom. This research is going to answer a single research question: How do students perceive their teacher's roles in the EFL classroom viewed from their drawings? 2. LITERATURE REVIEW 2.1 TEACHERS' ROLES To achieve an effective teaching-learning process, teachers ought to know their roles in the classroom. In the process of transferring knowledge, teachers are expected to play particular roles. However, the implementation of the role itself depends on the situation of the educational institution, the students, and the subject matter we are required to teach, and so on. Considering those various situations, teachers are required to play more than one role. According to Harmer (2007) , there are five roles of the teachers: (1) teacher as a controller, (2) Teacher as a prompter, (3) teacher as participant, (4) teacher as resource, and (5) teacher as tutor. Furthermore, those five roles are becoming the primary issue in this research. The writer intends to investigate the realization of those five roles by analyzing students' perception through their drawings 2.2 SOCIAL SEMIOTIC APPROACH In this study, the semiotic sign which will be analyzed is students' drawing. Images show not the thing actually seen but its representations in human consciousness. Turkcan (2013) proposes that semiotic approach has increasingly gained importance so that people, who live in a visual bombardment today, can analyze the codes included in visual culture and understand the form of visual communication. Halliday (1978) states that social semiotic theory provides the basis for the study of semiotic resources other than language (e.g. images, architecture, music, mathematical symbolism, gesture, clothing). Moreover, Aiello (2006) argues that the main aim of social semiotics is to look systematically at how textual strategies are deployed to make certain meanings. He also adds that social semiotics concentrates on practices of meaning-making and considers how we make meaning using various semiotic resources, modes and their affordances. Then, what is developed by Kress and van Leeuwen's (1996) and Van Leeuwen and Jewitt (2001) is how we can use social semiotics as an analytical tool to access these meanings by drawing on the contexts and cultures of its production and how we as individuals assign meanings to our texts through our prior knowledge, exposure and use of semiotic resources and modal affordance in our communicative practices. Kress and Van Leeuwen (1996) have extended this idea to images, using a slightly different terminology: representational meaning, interactive meaning, and compositional meaning."], "relatedWork": [], "rq": ["INTRODUCTIONThus, this research is aimed to portray students' perceptions on their teacher's roles in EFL classroom by using social semiotic approach. The participants are the 11 th grade students in a state senior high school in Banjarmasin. The students express their perceptions on teachers' role through drawings. Clarebout et al (2007) argues that drawings can be used to identify nuances and ambivalences within a person's belief system, indicating they would be useful when studying pupil conceptions. This research strategy is increasingly being used to probe students' feelings about how they teachers accomplish their role in the classroom. This research is going to answer a single research question: How do students perceive their teacher's roles in the EFL classroom viewed from their drawings? 2. LITERATURE REVIEW 2.1 TEACHERS' ROLES To achieve an effective teaching-learning process, teachers ought to know their roles in the classroom. In the process of transferring knowledge, teachers are expected to play particular roles. However, the implementation of the role itself depends on the situation of the educational institution, the students, and the subject matter we are required to teach, and so on. Considering those various situations, teachers are required to play more than one role. According to Harmer (2007) , there are five roles of the teachers: (1) teacher as a controller, (2) Teacher as a prompter, (3) teacher as participant, (4) teacher as resource, and (5) teacher as tutor. Furthermore, those five roles are becoming the primary issue in this research. The writer intends to investigate the realization of those five roles by analyzing students' perception through their drawings 2.2 SOCIAL SEMIOTIC APPROACH In this study, the semiotic sign which will be analyzed is students' drawing. Images show not the thing actually seen but its representations in human consciousness. Turkcan (2013) proposes that semiotic approach has increasingly gained importance so that people, who live in a visual bombardment today, can analyze the codes included in visual culture and understand the form of visual communication. Halliday (1978) states that social semiotic theory provides the basis for the study of semiotic resources other than language (e.g. images, architecture, music, mathematical symbolism, gesture, clothing). Moreover, Aiello (2006) argues that the main aim of social semiotics is to look systematically at how textual strategies are deployed to make certain meanings. He also adds that social semiotics concentrates on practices of meaning-making and considers how we make meaning using various semiotic resources, modes and their affordances. Then, what is developed by Kress and van Leeuwen's (1996) and Van Leeuwen and Jewitt (2001) is how we can use social semiotics as an analytical tool to access these meanings by drawing on the contexts and cultures of its production and how we as individuals assign meanings to our texts through our prior knowledge, exposure and use of semiotic resources and modal affordance in our communicative practices. Kress and Van Leeuwen (1996) have extended this idea to images, using a slightly different terminology: representational meaning, interactive meaning, and compositional meaning."]}
{"intro": ["Prior research on leadership has typically defined it as an individual trait or individual role [12, 13] . These studies have typically focused on how one individual influences the behavior of other team members to help meet team objectives [57] . However, many problems addressed by virtual teams are often complex and require specialized knowledge to solve that no single individual possesses [45] . In these situations it is difficult to have one person in charge of the overall team. This problem is only exacerbated when these teams work at a distance [2, 3, 35] . As a result, there has been a shift away from traditional hierarchical leadership structures to a more decentralized or shared leadership [13, 40] .", "Shared leadership is the distribution of leadership among team members and is characterized by the sharing of leadership roles [12, 13] . Shared leadership, however, offers the possibility for every team member to be included in team decisions which potentially promises more inclusion and better team experiences. This paper seeks to build on prior literature examining shared leadership in virtual teams [5, 13] . The research question this study attempts to address is: \"How does shared leadership impact the identification, satisfaction and actual performance of virtual teams?\" This study has three goals: 1) To examine the impacts of shared leadership on an individual's willingness to identify with their team in light of their race and gender. Team identification has been found to be an important process variable for the success of virtual teams [9, 45] . However, little research has been done to understand the conditions that facilitate team identification when these teams are racially and gender diverse. 2) Investigate the impact of shared leadership on team satisfaction. Virtual teams are often criticized for being overly sterile and too task focused [44] . As work becomes both more virtual and collaborative, understanding what factors facilitate a positive team work environment becomes increasingly important 3) Examine the impact of shared leadership on the performance of virtual teams. Prior research has only posited positive impacts between shared leadership and virtual team performance [5, 13] ; however, there are reasons to believe that shared leadership could potentially decrease the performance of virtual teams."], "relatedWork": [], "rq": ["INTRODUCTIONShared leadership is the distribution of leadership among team members and is characterized by the sharing of leadership roles [12, 13] . Shared leadership, however, offers the possibility for every team member to be included in team decisions which potentially promises more inclusion and better team experiences. This paper seeks to build on prior literature examining shared leadership in virtual teams [5, 13] . The research question this study attempts to address is: \"How does shared leadership impact the identification, satisfaction and actual performance of virtual teams?\" This study has three goals: 1) To examine the impacts of shared leadership on an individual's willingness to identify with their team in light of their race and gender. Team identification has been found to be an important process variable for the success of virtual teams [9, 45] . However, little research has been done to understand the conditions that facilitate team identification when these teams are racially and gender diverse. 2) Investigate the impact of shared leadership on team satisfaction. Virtual teams are often criticized for being overly sterile and too task focused [44] . As work becomes both more virtual and collaborative, understanding what factors facilitate a positive team work environment becomes increasingly important 3) Examine the impact of shared leadership on the performance of virtual teams. Prior research has only posited positive impacts between shared leadership and virtual team performance [5, 13] ; however, there are reasons to believe that shared leadership could potentially decrease the performance of virtual teams."]}
{"intro": ["In contrast, no support for the \"what is beautiful is usable\" hypothesis was found by Th\u00fcring and Mahlke [18] as well as by Tuch et al. [10] . In two of three experiments in which the usability and the aesthetics of a portable digital audio player were varied, Th\u00fcring and Mahlke [18] did not find an effect either (however, a \"trend\" with a value < .10 in the second experiment) of manipulated aesthetics on perceived usability or of manipulated usability on perceived visual aesthetics. However, usability affected the overall judgment of the audio player in both experiments and manipulated aesthetics in one of them. In a recent experiment, Tuch et al. [10] found neither an effect of manipulated aesthetics nor an effect of manipulated usability of an online shop on perceived usability at the preuse phase. Even after interacting with the online shop, interface aesthetics had no influence on users' perception of its usability. However, perceived aesthetics and a subcomponent of hedonic quality (hedonic quality identity, AttrakDiff questionnaire [19] ) were not only affected by interfaceaesthetics but also, and according to Ben-Bassat et al. [14] and Lee and Koubek [15] , by interface usability. The authors concluded that the \"what is beautiful is usable\" notion can be reversed to a \"what is usable is beautiful\" under certain circumstances [10] , characterizing a further variant of the interplay between aesthetics and usability."], "relatedWork": [], "rq": ["DiscussionConcerning the global evaluation of the mobile phone prototype, the present results revealed significant effects both of manipulated usability and manipulated aesthetics on perceived appeal, with comparable medium to large effect sizes (usability: 2 = .078; aesthetics: 2 = .100). Both factors explained a similar amount of variance in perceived appeal. Therefore, our results suggest that perceived appeal, serving for a global evaluation of noninstrumental quality [21] , depends on aesthetics and usability more or less to the same extent. This finding corresponds to a result of Lee and Koubek [15] , who found that manipulated aesthetics and usability both significantly affect user preference, which in turn might be determined by perceived appeal. Regarding the \"indirect\" effect of aesthetics on perceived usability (research questions 3.1 and 3.2), we found that goodness completely mediates the relationship between perceived beauty and perceived usability (H3.1) but not between manipulated aesthetics and perceived usability (Hypothesis 3.2). Consequently, neither perceived beauty nor manipulated aesthetics seem to be directly linked to perceived usability. However, the former finding (H3.1) corresponds to the inference rule of evaluative consistency, which implies an indirect effect of (perceived) beauty on pragmatic quality, as stated by Hassenzahl and Monk [4] as well as by van Schaik et al. [13, page 17] . Following van Schaik et al. [13] , this kind of inference results in a highly beauty-driven overall evaluation (\"What is beautiful is good\" and \"I like it, it must be good on all attributes\"), which may include the judgment of usability. Concerning the argument of van Schaik et al. [13] , it is noteworthy to mention that the relation between perceived beauty and perceived usability is based only on variance due to subjects. Consequently, we may assume that the mediation effect solely depends on an individual evaluative process and therefore might be better characterized as \"What is perceived as beautiful is perceived as good\" and \"What is perceived as good is perceived as usable, \" independent of whether a system is really ugly or beautiful (in the sense of manipulated aesthetics). On the contrary, in the current study, the mediation analysis for manipulated aesthetics, goodness, and perceived usability (Hypothesis 3.2) did not reveal an effect, because manipulated aesthetics and perceived usability were not correlated. Regarding this finding, we may conclude that there is neither a direct nor an indirect link between aesthetics and perceived usability, or, in other words: people judge the usability of a system independently of whether it is ugly or beautiful (in the sense of manipulated aesthetics)."]}
{"intro": ["Based on the number of studies and validation data, the most notable examples of such receptive tests are the Vocabulary Levels Test (VLT, Nation, 1983; Schmitt, Schmitt and Clapham, 2001) , the Vocabulary Size Test (VST, Nation, 2006; Nation & Beglar, 2007) and the so called yes/no tests, such as Meara and Milton's X-Lex (2003) . These ostensibly uncomplicated tests have been employed in SLA to explore learners' vocabulary knowledge to allow for, inter alia, monitoring of progress and allocating students to different proficiency groups in the case of placement tests. The problem with these tests, however, is that although they measure lexical knowledge to some extent, the results cannot necessarily be compared across cultures, learning contexts and individual language backgrounds, which in turn makes it difficult to standardize these tests. As Cobb rightly argues, 'vocabulary tests can focus on either language or the learner ' (2000, p. 300) and in most research to date, the focus has been mainly on the language, disregarding other factors that can influence lexical knowledge. Recently, it has been argued, however, that due to the effect of prior lexical knowledge on additional language learning, the conceptualization and operationalization of a multifaceted lexical knowledge posits several challenges and perhaps this is the reason why measuring vocabulary size in multiple languages and exploring the relationship and interconnection between different languages is still an understudied area.", "There is a growing amount of evidence that the difference between monolinguals, bilinguals and multilinguals can be fundamental from a language acquisition perspective. For instance Bialystok, Craik and Luk (2008) found that bilinguals from a variety of language backgrounds can even outperform their monolingual counterparts on letter fluency and word naming tasks due to their executive control. Barac and Bialystok (2011) tested different bilingual groups and a monolingual group on verbal and non-verbal tasks. Their conclusion indicates that on the executive control tasks bilinguals outperformed monolinguals, regardless of language background, however on the receptive and productive vocabulary tasks only Spanish-English bilinguals showed an advantage due to the similarity between the languages. Employing a different research paradigm, Molnar (2008) found that Hungarian-Romanian bilinguals outperformed the Hungarian monolingual group on the VLT, however the Romanian monolingual group obtained significantly higher scores than both other groups due to their high proficiency in the typologically closer language. This illustrates Treffers-Daller's (2011) claim about the importance of obtaining detailed information of participants' lexicon, and as this paper attempts to demonstrate, since background languages have a fundamental impact on the mental lexicon of multilingual language learners, prior to establishing the relationship between vocabulary size and other competences, the understanding of vocabulary size in the case of multiple languages and the interaction between all these languages is highly pressing.", "The present paper therefore sets out to explore vocabulary knowledge, with a focus on the learner and their language background. In particular, it investigates the connection between vocabulary sizes in different background languages in the case of Hungarian (L1) native speakers living in Romania (L2) and studying English (L3) at university level. Similarly to Molnar (2010) and Szabo (forthcoming) a widely used receptive, meaning recognition vocabulary test was employed to measure lexical knowledge at different levels and cognate recognition in not just the target language (L3), but the typologically closer language (L2) as well. By contrast to these two studies however, the present study opted for the VST, as its format also allowed for an online measure that records response time. Furthermore, there are only a handful of studies that explore lexical access or reaction time on yes/no vocabulary size tests (e.g. Miralpeix and Meara, 2014; Pellicer-S\u00e1nchez and Schmitt, 2012) or on multiple-choice tests (e.g. Harrington, 2006; Laufer and Nation, 2001; Tanabe, 2016) , thus it became of interest whether vocabulary size is also a good indication of lexical access at different proficiency levels. Another point of interest was related to the cognate facilitation effect. While Garc\u00eda (1991) found that learners could only take advantage of their cognate knowledge after explicit instruction on cognates, Hall (2002) , Molnar (2010) , and Szabo (forthcoming) found that learners might take advantage of lexical similarities between languages intuitively. Since we can corroborate from several studies that cognates have a facilitative effect as they are processed faster and more easily than control words (e.g. Cobb, 2000; Helms-Park and Perhan, 2016) , and this concurs with Elgort's assertion that cognates are recognized, translated and acquired faster and more accurately by bilinguals ' (2013, p. 255) , accounting for them in lexical tests seems essential."], "relatedWork": [], "rq": ["Lexical access (RT): size and frequency effects (RQ3)Following Laufer and Nation (2001) and based on Tanabe (2016) , the participants have been equally divided into two groups (High and Low groups) using the mediansplit on their ENVST scores in order to establish whether the group with a larger vocabulary size also showed faster accessibility in the tested foreign languages. Table  3 illustrates just this. It becomes obvious from the mean scores and paired t-tests that there is a statistically significant difference between the two groups' L3 (t(26) = 9.10, p < .001) and L2 vocabulary scores (t(26) = 2.87, p < .01). Furthermore, in support of size effect, the t-tests also indicate that students with a larger vocabulary also had a faster response time in English (t(26) = -2.03, p = .04), but not Romanian t(26) = -1.14, p = .26). The reason for this probably lies in the fact that the more proficient the group is in a language, the less variability can be detected in their RT scores. Figure 4 furthermore shows that there is a slight difference between the response times to cognates and non-cognates. In order to test whether the difference overall between response time to correct answers on cognates and non-cognates is significant, t-tests were conducted. In the case of the ENVST, t = -.689, p = 0.491, R 2 = .016 (cognate M = 12.43 and non-cognate M = 12.64), which indicates that there is no difference between the means. However in the case of the Romanian test (t = -2.23, p = 0.03, R 2 = .048, cognate M = 9.61 and non-cognate M = 10.12) and the Hungarian test (t = -2.27, p = 0.02, R 2 = .039, cognate M = 6.27 and non-cognate M = 6.54) there is a significant difference between the means, indicating that even in the case of multiple choice tests cognates are likely to be recognised faster than non-cognate items, albeit the advantage of cognates is not as accentuated as in yes/no studies."]}
{"intro": ["However, despite their current prominence, it is not clear that commodity cluster technology will remain cost-effective when scaled to the PetaFLOPs range. New issues -such as power, cooling, and physical infrastructure -are becoming increasingly important. Additional issues, such as bandwidth to primary and secondary storage, are emerging as significant challenges. (This point is expanded upon in section 2; section 2 of [3] outlines a similar argument.)"], "relatedWork": ["Almost since the Field-Programmable Gate Array was introduced in 1985, researchers have contemplated ways of using FPGAs to build high performance CustomComputing Machines (CCMs). Several have been used in clusters or direct predecessors of clusters. Early researchers used large collections of FPGAs because individual FPGAs had relatively few resources; hence multiple FPGAs were required to implement a substantial design. Typical examples include the Splash-2 attached processor [2] and \"The Virtual Computer\" [5] . Although the goal of these systems was to provide a fast co-processor, a kernel of the idea of \"clusters of interacting systems\" is in these designs. The Splash-2 boards had a configurable interconnection network between the FPGAs and one model for programming these boards was to use a variant of data parallel C [11] . In this model, the processing elements are not independent systems; however, clearly this was approaching a cluster of communicating systems.", "Other related projects have been built. For example, in 2003 AFRL Rome built a 48-node cluster of PCs with an off-the-shelf FPGA board on the PCI bus [22] . However, this is different from the Adaptable Computing Clusterwhere the FPGA was in the network data path -and very different from the current project where the Platform FPGA is the node."], "rq": ["Commodity Cluster SolutionsMulti-core processors will allow the on-chip rateof-computation to continue to grow exponentially with Moore's Law. However, as just mentioned, this does not address the issue of primary memory speed. This so called \"Memory Wall\" was predicted many years ago [37] but microprocessors have been able to avoid the the growing disparity by using ever-larger on-chip caches. Thus it possible that some clever research will emerge that again postpones the bottleneck issue; however to date the only solution has been to create ever-larger data caches. There are other unanswered research questions as well, such as how hundreds cores on a single chip will communicate (on-chip and across the nodes of cluster). The point is not to say that multi-chip processors have insurmountable problems. Rather just to note that the technology is not without risk."]}
{"intro": ["In recent years, a great deal of research has been conducted into the design and implementation of tabletop interactive systems; for example, the works of ; Rogers et al. (2004) ; Shen et al. (2003) ; Shen et al (2004) ; Streitz et al. France, [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] (1999); Streitz, et al. (2002) ; and Wu and Balakrishnan (2003) . Especially exciting about this domain is that a tabletop application can be targeted to simultaneous use by multiple users seated around the table. This organization, however, creates a problem unique to the domain: how should on-display objects be oriented?", "Research has been conducted into how participants in non-computer based, tablecentered collaborative tasks make use of orientation (Tang 1991; Kruger et al. 2003) . These investigations have shown that users prefer a straight-on orientation for reading text, and orient objects towards themselves, others, or in-line with shared artifacts to ease reading in different circumstances. They have discovered, however, that in a collaborative setting, a straight-on orientation toward the reader is not always desired or exercised. In fact, orientation is employed as a tool to aide in interaction with other users. Despite this, it seems that designers of collaborative research systems have, in general, opted to attempt to orient text towards the reader, as seen in Bruijn et al. (2001) , Rekimoto et al. (1999) , Shen et al. (2003) , Shen et al. (2004) , and Streitz et al. (1999) . Thus, there is a tension between a desire to allow for the use of orientation as an aid to collaboration, and the designers' assumption that users need to have textual elements oriented towards them. Absent from this previous work is a thorough investigation into the parameters for determining when a solution to the text orientation problem should be applied in the context of tabletop groupware. Although users seem to prefer a \"normal\" orientation of text (Kruger et al. 2003) , and studies in the psychology literature in non-tabletop situations with constrained user head and body movement by Tinker (1972) and Norman (1984, 1985) indicate that readability is compromised when text is oriented, is it possible that there are circumstances where it might be appropriate to ignore this preference in favour of a less preferred orientation that may aide collaboration in other ways? Is orientation so critical for text readability that design elements must be sacrificed in order for it to be addressed? Without empirical data quantifying the extents to which readability is compromised in less-preferred orientations on tabletops, it is difficult to make informed choices when confronted with these tradeoffs. Our present work provides such empirical data via two experiments that examine the effect of text orientation on the performance of tasks common to tabletop collaborative groupware. Based on the results, we hope to provide insights for system designers as to when the issue of text orientation takes precedence, and when it can be safely ignored. Although very infrequently done in the field of human-computer interaction, replication and extension of experimental work is an important aspect of research. Our work also contributes in this regard by re-examining and extending the studies of readability of text orientation by Tinker (1972) and Koriat and Norman (1985) to tasks relevant to the new domain of tabletop displays, and where users' head and body movements are unconstrained."], "relatedWork": [], "rq": ["Human Ability to Read Text at Various OrientationsThe work of both groups seems to confirm the intuitive notion that reading orientation should be a key concern to designers of tabletop systems. However: their work is not directly applicable to tabletop collaborative groupware research. First, rather than allow participants free and natural movement during the experiments, the position and orientation of their heads was constrained. Second, in both sets of experiments, readability was determined by how quickly non-conforming strings were identified. In the Tinker (1972) study, this was done at the semantic level, as participants were required to find the word in paragraphs that spoiled their meaning. In the Koriat and Norman (1985) experiment, this was done at the syntactic level, as the study consisted of the presentation to the participant a series of strings of characters, which subjects were required to identify as either words or non-words. Though this experimental design enabled them to answer their research questions, we note that the identification of non-conforming or gibberish strings is not directly applicable to real user interface scenarios. In most applications, textual artifacts consist of either common words or domain terms that might be expected by the user. This assumption might aide in the reading of text at varying orientations, and so should be considered when evaluating user performance of reading at varying orientations."]}
{"intro": [], "relatedWork": [], "rq": ["Trust in CMCEarly research questioned the efficacy of text-based CMC environments as vehicles through which trust could be established, suggesting that they did not provide enough nonverbal social cues to foster trust development [2] . More recent studies have challenged this view and demonstrated that that while trust may develop more swiftly in face-toface settings, individuals can reach similar levels of trust in text-based CMC environments when given enough time Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CSCW'08, November 8-12, 2008 , San Diego, California, USA. Copyright 2008 ACM 978-1-60558-007-4/08/11...$5.00. [18] . However, the vast majority of research on trust development in CMC focuses primarily on outcome measures of trust. Only recently has work begun to examine the processes by which groups develop trust. One mechanism through which we believe trust may be achieved is through the use of linguistic mimicry."]}
{"intro": ["Preventive maintenance still remains as a standard approach for a huge number of manufacturers [6] . This means that maintenance is carried out after a specific time or after a certain number of process steps have been completed by a piece of equipment or machine. However, this method does not allow for the actual condition of the changed parts and can result in relatively high costs, as components may be replaced while still in good working condition. On the other hand, having malfunctions due to components wearing out before their scheduled maintenance is a costly option [7] .", "There is already a huge number of scientific papers concerning the application of data analytics and related technologies in maintenance [11] [12] [13] . However, there is no research effort reported to systematically review the current state of this new wave of industrial revolution. Therefore, the purpose of this paper is to find and cluster those relevant to maintenance use in a production context. This aim of the paper is to provide a systematic literature review, within data analytics covering the maintenance of production systems. More specifically, seven research questions are listed below. They provide a more appropriate answer as to the purpose of this paper:"], "relatedWork": [], "rq": ["IntroductionThere is already a huge number of scientific papers concerning the application of data analytics and related technologies in maintenance [11] [12] [13] . However, there is no research effort reported to systematically review the current state of this new wave of industrial revolution. Therefore, the purpose of this paper is to find and cluster those relevant to maintenance use in a production context. This aim of the paper is to provide a systematic literature review, within data analytics covering the maintenance of production systems. More specifically, seven research questions are listed below. They provide a more appropriate answer as to the purpose of this paper:"]}
{"intro": ["While most HCI research on memory technologies has focused on episodic memory impairments such as those associated with dementia, less work has focused on building technologies for autobiographical memory impairments to help people living with other mental disorders such as depression. Memory impairments in depression are however fundamentally different; their effect is felt not through the loss of episodic memories, but rather difficulties in the retrieval of episodic memories through higher levels of autobiographical memories such as general events and lifetime periods. Another distinct body of work explored computerized interventions for depression such as online Cognitive Behavioral Therapy (CBT). Such interventions address memory impairment, not as the main focus, but rather through a subset of psycho-educational materials concerned with negative thinking patterns, and tools for tracking mood [9, 10, 17] , which provide limited support for the distinctive autobiographical memory impairments associated with depression. Our work aims to bridge these two strands of work to contribute to the design of novel classes of technologies that specifically address memory impairments in depression. We argue that understanding the specific memory impairments in depression, and the new range of challenges they pose, offers a rich opportunity to extend HCI research on memory technologies in new directions. This paper is an initial step towards exploring this space and focuses on the following research questions: 1. How are memory impairments in depression addressed through tailored interventions used in clinical and neuropsychological practice? 2. What is the role of materials in these memory interventions and how are they employed in therapeutic practice? 3. How can therapeutic memory interventions for depression inform the design of novel memory technologies?"], "relatedWork": [], "rq": ["INTRODUCTIONWhile most HCI research on memory technologies has focused on episodic memory impairments such as those associated with dementia, less work has focused on building technologies for autobiographical memory impairments to help people living with other mental disorders such as depression. Memory impairments in depression are however fundamentally different; their effect is felt not through the loss of episodic memories, but rather difficulties in the retrieval of episodic memories through higher levels of autobiographical memories such as general events and lifetime periods. Another distinct body of work explored computerized interventions for depression such as online Cognitive Behavioral Therapy (CBT). Such interventions address memory impairment, not as the main focus, but rather through a subset of psycho-educational materials concerned with negative thinking patterns, and tools for tracking mood [9, 10, 17] , which provide limited support for the distinctive autobiographical memory impairments associated with depression. Our work aims to bridge these two strands of work to contribute to the design of novel classes of technologies that specifically address memory impairments in depression. We argue that understanding the specific memory impairments in depression, and the new range of challenges they pose, offers a rich opportunity to extend HCI research on memory technologies in new directions. This paper is an initial step towards exploring this space and focuses on the following research questions: 1. How are memory impairments in depression addressed through tailored interventions used in clinical and neuropsychological practice? 2. What is the role of materials in these memory interventions and how are they employed in therapeutic practice? 3. How can therapeutic memory interventions for depression inform the design of novel memory technologies?"]}
{"intro": [], "relatedWork": [], "rq": ["E-LearningTo put it shortly, previous research has generally provided us with some practical and theoretical insights to understand e-leaning instruction (e.g., Alsultanny, 2006; K\u0131l\u0131\u00e7kaya & Krajka, 2010) . As the above review suggests, incidental lexical knowledge certainly makes a significant contribution to L2 learners' lexical knowledge. However, little is known about L2 lexical knowledge in situated learning situation. Therefore, this study aimed to investigate the effectiveness of computer-assisted instruction on intermediate L2 learners' vocabulary achievement. Hence, the following research questions were addressed: "]}
{"intro": [], "relatedWork": [], "rq": ["Results and DiscussionHowever, the findings of the current study differ from the study by Jafari and Shokrpour (2012) , which showed high (M=3.72) usage of support reading strategies. These rather contradictory results may be due to different style of data collection carried out by the authors. Jafari and Shokrpour (2012) distributed the questionnaires after they administered a reading comprehension test. The respondents were instructed to answer the questionnaires based on the strategies they used when answering the test. These differences may have influenced the findings of the studies. The fourth research questions sought to investigate the usage of global reading strategies (GLOB). Table 3 reveals that global reading strategies are the least frequently used strategies with overall mean of 3.30. The results were consistent with both Jafari and Shokrpour (2012) and Magogwe (2013) studies, in which both studies showed moderate usage of global reading strategies with means of 3.14 and 3.42, respectively. The findings from the three studies suggested that global reading strategies were the least prevalent strategies used by the students. A possible explanation for this might be that the students were not exposed to listed strategies. Another possible explanation for this is that the students might think that the strategies were not important when reading academic texts. The last research question sought to identify five least frequently used strategies by the students to read academic texts. Table 4 shows that most students were not familiar with global reading strategies, which explains why it had the lowest overall mean as compared with two other strategies. The findings reveal that the students might not be aware of global reading strategies. Another possible explanation is that the students might not have been explicitly introduced to these strategies. Global reading strategies are important in assisting students to comprehend their academic texts. If the students were not exposed and thought the strategies were not important for them, it is imperative for the language instructors to introduce the strategies to the students and put emphasis on the importance of the strategies. "]}
{"intro": [], "relatedWork": [], "rq": ["Previous Work on MultithreadingAn open research question is whether software-based multithreading can successfully tolerate more modest forms of latency, such as the remote latencies in hardware DSMs (e.g., the SGI Origin [7] ). To implement software-based multithreading, we need two software mechanisms: (i) the ability to switch between threads; and (ii) a mechanism for knowing when to trigger thread switches. The former mechanism is clearly feasible, since software can save and restore all thread-specific state (e.g, registers, the program counter, any condition codes, etc.). The latter mechanism, however, had been lacking in the past, since there was no way for software to directly observe and react to cache misses in a sufficiently lightweight fashion. (Note that the signal handler mechanism used to trigger thread switches in software DSMs is not applicable to cache misses, since it is too costly and can only react to page-level access violations.) Fortunately, a mechanism which provides this functionality was recently proposed: informing memory operations [4] ."]}
{"intro": ["Our paper discusses the present-future gap (see Figure 1 ) that is particularly relevant in the evaluation of prototypes. Prototypes are, by definition, pieces of technology created for shaping and learning about possible futures and how they could be changed with technology. The construction of a prototype in itself, even without any empirical study, helps designers learn about the design problem [61] . However, empirical evaluation adds something further: ability to learn about its capacity to change the world. Hence, evaluations are about what the prototype might become. Although prototypes may embody research hypotheses that are future-oriented [75] , only evaluations render them empirically researchable.", "It is especially the boldest, most future-looking visions of HCI that need to pay attention to this issue. For an illustrative example, consider the first implementations of ubiquitous computing systems in classrooms, in the Classroom 2000 project [1] . Here, we have the benefit of retrospection, since similar systems are now in use; however, in the 1990s, university students were not using phones, tablet computers, and laptops in the classroom. To learn about the impact of their new tablet-like note-taking application, the researchers evaluated it in circumstances that to some extent correspond with the way computers are used in classrooms in the 2010s. The studies did reveal some benefits that have since been documented, such as its usefulness for those students who do not normally take notes. However, because the prototypes were single-purpose devices, the Classroom 2000 work did not uncover the major riddle that mobile, networked computers pose for present-day education: distraction and avoidance of notetaking when materials are available online. Had early studies warned about this possible future, perhaps we would have fewer issues with ubiquitous technology in education.", "Our focus is particularly on issues related to scientific validity. We discuss the construction of research questions and hypotheses, threats to validity, replicability, and separation between researchers and participants. This setting of scope allows us to address most of the empirical research published in the HCI field [13, 32] . However, it does not address all prominent research interests in HCI. For example, in the 'third paradigm' of HCI [20, 21, 74] , an evaluation may aim at knowledge that is pluralistic, situated, and not necessarily valid in a traditional sense, to inspire new design ideas. Stakeholders may assume active roles that go beyond those of 'user' or 'participant'. However, as we argue, the present-future gap stretches our assumed 'scientific' notion of validity, because prototypes entail value-imbued assumptions about possible or desirable futures."], "relatedWork": [], "rq": ["INTRODUCTIONOur focus is particularly on issues related to scientific validity. We discuss the construction of research questions and hypotheses, threats to validity, replicability, and separation between researchers and participants. This setting of scope allows us to address most of the empirical research published in the HCI field [13, 32] . However, it does not address all prominent research interests in HCI. For example, in the 'third paradigm' of HCI [20, 21, 74] , an evaluation may aim at knowledge that is pluralistic, situated, and not necessarily valid in a traditional sense, to inspire new design ideas. Stakeholders may assume active roles that go beyond those of 'user' or 'participant'. However, as we argue, the present-future gap stretches our assumed 'scientific' notion of validity, because prototypes entail value-imbued assumptions about possible or desirable futures."]}
{"intro": [], "relatedWork": [], "rq": ["MCS ApplicationsMany MCS applications target a particular research question rather than general usage. For example, StudentLife collects information about university students' mental health, academic performance, and behavioral trends [38] . In contrast, MyExperience was one of the first general-purpose MCS applications; however, it runs on Windows Mobile devices (1% of current market share) 3 and requires knowledge of an XML-like scripting language for configuration [17] . The sensing capabilities of AndWellness are limited to location and activity type, and the app is confined to Android and requires XML knowledge for configuration [19] . Survalitics aims to be a general-purpose survey instrument, but it is confined to Android and requires manual modification of Java code [27] . Reporter is a commercial app ($3.99 per installation) for iOS that deploys scheduled surveys. 4 AWARE is similar to Sensus in many respects, but the iOS version currently requires manual compilation and deployment of source code [15] . Funf offers a comprehensive set of sensors and non-technical configuration; however, it lacks scheduled and sensor-triggered surveys, and it is only available for Android [1] . Paco allows researchers to design mobile sensing plans for iOS and Android devices without programming knowledge; 5 however, sensing in Paco is limited primarily to app-and call-based events, and this limitation carries over to surveys, which can only be triggered on the basis of these events."]}
{"intro": [], "relatedWork": [], "rq": ["Evaluation of the SDS.There are alternative measures that measure similar constructs, such as the Credibility scales by McCroskey and Teven [54] . However, the METI instrument differs from these in that it explicitly focuses on epistemic trust, that is, whether the target of the evaluations is a trustworthy source for the knowledge that the user seeks. This was desirable for our research question."]}
{"intro": ["The Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT) has put pressure on English teachers to seek better ways to promote English communication skills in their students. Productive vocabulary knowledge is indispensable for both oral and written communication. However, it is difficult to picture students being engaged in real communication in many English classes in Japan owing to lack of opportunities to use their English knowledge in meaningful form-focused input and output. Because of limited class time allocated to vocabulary instruction in many English classrooms in Japan, students take control of vocabulary learning. They traditionally memorize word meanings by using lists, written and oral drills, and flashcards with little opportunity to use the words in authentic contexts. Beginners specifically tend to memorize word meanings and spellings by looking at a wordlist without paying attention to other aspects of word knowledge such as syntactic features and pronunciation of words, while they feel a difficulty in using the learned words in writing and speaking [1] ."], "relatedWork": [], "rq": ["III. RESULTS AND DISCUSSIONTable \u2164 includes the questionnaire. All participants were allowed to write their perception of the task freely. Their responses are classified into eight categories. With respect to the second research question, the results of the questionnaire revealed that 20% of the participants perceived the task as difficult, while 15% of them perceived it as enjoyable or found importance in the usage of language. However, the results of a separate questionnaire regarding the difficulty of the task indicated that approximately 70% of the participants perceived the task level to be of an appropriate difficulty level. Most of them used the mini dictionary in confirming the meaning of words during the discussion and their group work was supportive with good feedback from peers. Furthermore, the participants seemed to perceive three factors for the effectiveness of the task. Eighteen percent of the students found it effective for practicing vocabulary, 8% of them mentioned spelling, and 13% of them mentioned listening practice. Some students referred to comprehension. Ten percent of the participants mentioned that this task helped them understand the text and 10% of them perceived the task efficient for hypothesis testing. In sum, the participants seemed to perceive the dictogloss activity as effective in promoting skills related to the productive knowledge of English. Interestingly, no one mentioned grammatical aspects of language learning. Nabei [23] points out that students perceive teacher talk as one of the most useful learning sources. The participants in the current study might have attended more to vocabulary since the teacher focused on their vocabulary knowledge during class discussions."]}
{"intro": [], "relatedWork": [], "rq": ["A. Quantitative AnalysisOften times, comparative L2 research tend to interpret L2 learners' deficiency in the target language as a failure in nativelikeness. However, as Cook (2015) pointed out, we should focus on \"the reasons why L2 users create novel ways of thinking rather than in their putative deficiency compared to monolingual native speakers\" (p. 157) [19] . Therefore, in addition to providing quantitative evidence on the general tendency of \"how\" Chinese speakers' perform comparing with native speakers, the second research question addressed the \"why\" question by elucidating prominent features in Chinese speakers' production of motion verbal phrases."]}
{"intro": ["The high-level research question in this work is whether cycling and serpentining -as two perspectives of reexamining top-N list -improve user experience. However, we are not trying to optimize a particular user experience. We recognize that different experiences may require different approaches. A situation where a site recommends a single item cannot benefit from serpentining. A user who treats the top-N list as a \"to-do\" list, taking the top item each time, would not be served well by cycling. Rather, we want to see how these manipulations relate to user experience in the hopes of guiding designers in adopting them, or offering them to users. Similarly to the finding from Ziegler's work [32] that users are willing to accept a certain loss of accuracy in order to have more diverse recommendations, we expect that the perceived accuracy of recommendations may get reduced because of the manipulation; however, we test whether the accuracy reduction may be preferred in exchange for the exposure to a broader and \"fresher\" set of items."], "relatedWork": ["However, there is a need for more systematic study of recommender models' and algorithms' effects on user perceptions and experience. Change in recommendations is a good thing when users perceive more freshness and less boredom, but also could be confusing when changes are highly unexpected or overly dramatic. In other words, several psychological factors (that may not be directly observable) may be involved in user's decision making and, therefore, a systematic user-centered approach is needed to evaluate their potential involvement. Users' exposure to recommendations can also be studied by analyzing user actions, following the approaches and ideas from the science of persuasion and marketing. As Trellis's [30] work showed, advertising exposure has a nonlinear effect, in other words, repetitive exposure is necessary but has diminishing gain. Their and others' results [18] suggested that two to three ad exposures might be optimum. As discussed by Petty and Cacioppo [21] and also suggested by their results, repeating a persuasive communication tends to first increase and then decrease agreement. They proposed a two-stage attitude modification process: repetition enhances a person's ability to process a message in the first stage, and tedium and reactance are elicited by excessive exposures in the second stage. Similarly, this two-stage process might also apply in recommendations. Although CARS [1] adapt recommendations based on users' contextual state, i.e., based on time, mood, or companion(s), there might be contexts that are hard to measure and very sparse data about them is available for each individual user. Therefore, repetitive recommendations may increase the chances that users process the recommendations in relevant contexts. In addition, we study user-perceived boredom and freshness associated with the dynamics through surveys. There is not much research on changing recommendations based on users' past exposure to recommended items. One thread of related research is CTR (Click-Through Rate) estimation in information retrieval [2] , where documents with many exposures but no positive feedback from users are downgraded because their estimated CTRs become lower. Recommender systems can also utilize indirect feedback, such as clicks, which would be treated as an implicit preference signal [9] . In other words, when focusing on implicit users' feedback in response to displayed recommendations, a recommender can be designed to achieve similar dynamics. We do not use CTR as the primary evaluation approach in our work, because the system studied is not targeted at generating click-throughs, but rather at helping users have better experience with in exploring and finding movies (as measured in a much more holistic, comprehensive manner). Moreover, in our system users can see and rate movies without clicking through to detail pages, so informativeness of clicks as a primary evaluation measure may be limited. However, we do keep track of clicks as one of several indicators of user activities and engagement with the system.", "Recommender systems can be evaluated with offline metrics and online field experiments. Offline metrics sometimes make assumptions about online environments. One such important assumption is that the recommendation value decays going from the top to the bottom of a recommended item list. The nDCG [10] and weighted recall (or Breese's score [4] ) evaluation metrics, for example, assume exponential decay. We propose to test this assumption, because it may not be optimal to display all best 1 https://movielens.org recommendations at the same time. List-wise optimization have been shown to improve recommendations [28] , which suggests that an optimal list may not be the same as a collection of individually optimal items. Also, it has been shown that, in addition to accuracy, many other properties of a recommender are important aspects of user satisfaction [22, 19, 20, 32, 11] , such as diversity, novelty, etc. Pu et al. [22] proposed a user-centric evaluation framework for recommender systems with state-of-art survey designs [23] . Knijnenburg et al. [12] proposed a comprehensive framework taking into account both objective system measurements and subjective user perceptions to explain user experience. We directly apply this framework in evaluating our manipulations. Particularly, they postulate six components and their causal relationships -objective system aspects (OSA), subjective system aspects (SSA), user experience (EXP), user interactions or activities (INT), situational characteristics (SC) and personal characteristics (PC) --according to Theories of Reasoned Action (TRA) [7] . We use and model the former four components through methods of recording and analyzing user activities and survey responses here. This framework relies strongly on asking users their subjective experience through survey questions. In many examples of this type of studies, users typically interact one time with a system and then evaluate its performance. However, in our current study users can interact with a system over time, i.e., over several sessions. Because of this, we vary the moment of presenting the survey questions, to see if querying the user experience might affect how users interact with the system."], "rq": ["INTRODUCTIONThe high-level research question in this work is whether cycling and serpentining -as two perspectives of reexamining top-N list -improve user experience. However, we are not trying to optimize a particular user experience. We recognize that different experiences may require different approaches. A situation where a site recommends a single item cannot benefit from serpentining. A user who treats the top-N list as a \"to-do\" list, taking the top item each time, would not be served well by cycling. Rather, we want to see how these manipulations relate to user experience in the hopes of guiding designers in adopting them, or offering them to users. Similarly to the finding from Ziegler's work [32] that users are willing to accept a certain loss of accuracy in order to have more diverse recommendations, we expect that the perceived accuracy of recommendations may get reduced because of the manipulation; however, we test whether the accuracy reduction may be preferred in exchange for the exposure to a broader and \"fresher\" set of items."]}
{"intro": [], "relatedWork": [], "rq": ["HRI as an Intrinsically Interdisciplinary FieldHRI is closely related to HCI. Both disciplines address the problem of human engagement with interactive systems (e.g., with computer systems, smartphones, or robots). Initially considered as an interdisciplinary subject, contemporary HCI is a research discipline in its own right, with an established community, conferences and scientific journals, and specific paradigms, approaches, and methods. HCI study programs exist at undergraduate, master, and PhD levels, with dedicated syllabi and textbooks. Professorships in HCI are routinely appointed, although HCI scholars are often embedded within computer science (or similar) departments. Altogether, these are concrete indicators of a disciplinary field, although they do not yet indicate an institutionalized discipline. However, understanding the historical evolution of HCI requires familiarity with the challenges that the discipline has faced and still partly faces (e.g., B\u00f8dker, 2006 B\u00f8dker, , 2015 Harrison, Tatar, & Sengers, 2007; Rogers, 2012; Shackel, 2009) . If HRI will follow a similar evolution into a cohesive discipline or will rather develop as a collection of research questions that have to be addressed by several independent disciplines remains an open question (Weiss, 2012) ."]}
{"intro": ["As with any research involving human subjects, research by way of an app store deployment needs to be conducted ethically. However, as there is often not a direct relationship or contact between researcher and subject, this is not straightforward. In particular, it is difficult to ensure that all users understand that they are in a research study and that their autonomy in deciding to take part and to freely withdraw is supported. According to guidelines created by McMillan et al. (2013) for app store deployments, consent can and should be gained through in-app mechanisms. However, as McMillan et al. explain, the inevitable uncertainties over whether users have read and understood the study information and are in a position to give consent need to be counter balanced with respect and care for privacy."], "relatedWork": [], "rq": ["DebriefingSomewhat problematic is that there is currently no 'end' to the data collection beyond users deleting the app. Unless all users abandon the app, we will at some point have to end data collection and ought to inform users when doing this. It may also be responsible to inform users of our findings from the study, particularly if the app is found not to support positive change. Either way, it is likely be more responsible to end logging rather than disable or withdraw the app itself. An ending remains a design task and debriefing is enmeshed with this. Grimpe et al. (2014) argue research should not just anticipate outcomes, but look forward to and navigate potential lines of technology adoption. With Quped we may anticipate answers to our research question, but we also need to think about the purposes beyond behaviour change that this app may be put to. Anticipation ought to look beyond the results of the study. We must recognise that users are accruing data with this app, which may be valuable to them and belongs to them as much (or more so) than us. This means that we should not suddenly disable the app or cause people to lose data. Our current attitude to this is that the app will remain available if the study ends and that logging will be turned off. However, it might be that in the future we provide the ability to export data out of the app."]}
{"intro": [], "relatedWork": [], "rq": ["DISCUSSIONIn the present study, we investigated (1) whether older adults' experience of interacting with robots would affect their explicit attitudes and/or acceptance toward robots compared to younger adults; (2) whether state curiosity toward robots differs between the two age groups; and (3) whether older adults implicitly hold more negative attitudes toward robots than younger adults. To address the first research question, we examined whether there exists an age difference in pretest-posttest explicit attitude ratings. We found age-related differences in TAM-PEU scores. Surprisingly, perceived ease of use toward the assistive robot was stronger in older adults than in younger adults, even after interacting with the robot. This might seem to be contradictory to the IAT results, suggesting that older adults more strongly associated robots with negative attributes. One possible explanation is that younger adults are more experienced with new technologies such as the voice control function. Their standard of \"perceived ease of use\" might, therefore, be higher than older adults'. For example, if the assistive robot could not respond to voice commands immediately, younger adults might become annoyed due to the long response latency. Then they might think that the usability of the assistive robot was not good. On the other hand, older adults might be less familiar with new technologies. Thus, once they found that they could communicate with the assistive robot with voice commands, even if the tasks were as simple as searching news on the Internet and it might take a while for the assistive robot to complete this task, it was still sufficient for older adults to be satisfied because they did not have to control the robot via the \"complicated\" graphical user interface. Previous studies suggested that technology experience influenced robot acceptance across age groups [20] . Perceived enjoyment also enhanced robot acceptance in older adults [10] . For older adults with lower technology familiarity, positive experience might be a key factor to the enhancement of robot acceptance. However, we did not observe other age-related differences in other explicit measures of NARS, TAM, and STAI, suggesting that the explicit attitudes and acceptance toward the assistive robot for the older and the younger participants were similar in many other aspects.", "DISCUSSIONFor our second research question, we first compared younger and older adults' state curiosity toward robots and found a significant difference: older adults appeared to be less curious about the robot than younger adults. We then examined the role of personal association and found a significant moderating effect. Our findings support our hypothesis that older adults were less curious than younger adults in general; yet, this age difference was moderated by personal association. In other words, for topics that older adults perceived as a higher level of personal meaning and relatedness, they would invest up to the same level of curiosity in the topic as would younger adults. At first glance, this finding on state curiosity may seem to be in conflict with the findings of our other explicit attitudinal measures (i.e., we found age difference in state curiosity but not in other acceptance measures except the TAM PEU subscale). However, the nature of curiosity is slightly different from mere acceptance in the sense that curiosity captures the willingness to invest energy and resources to obtain information. Hence, it may be seen as one step further from simple likes or dislikes, which may explain why the result differs from the other explicit attitudinal measures. According to the selective engagement theory [21] , people would become more selective in how they allocate their cognitive resources with age. It could be related to a well-supported fact that the older adults are less willing to learn technology via trail-and-error compared to other training methods such as reading a printed manual [40] . The Diffusion of Innovation theory [41] points out that trialability or testability are keys for potential adopters to evaluate an innovation, and it would predict that older adults are less likely to be early adopters of new technology (e.g., robots) compared to younger adults because older adults might be less willing to actually use it. However, the selection would be based on personal association. However, previous studies did not test this hypothesis in the context of willingness to allocate time and effort for technology-related information seeking. Thus, the present study offers some insights into the usefulness of the selective engagement theory in the context of technology-related curiosity. This part of the study has the limitation of measuring personal association with a single item. Also, this study cannot provide evidence for causation. Future studies should consider including a more comprehensive scale to capture curiosity or directly manipulate one's personal association for a technological product in an experiment. Despite these limitations, this finding is particularly interesting because it suggests the role of personal association on curiosity, and this has vast implications for future robotic designs, adult education, and other services to older adults. Echoing the findings of other explicit attitude measures, we recommend future designers of services dedicated for older adults to include some elements of personal association/relevance (i.e., personalized familiarity or relatedness)."]}
{"intro": ["Some authors have recently claimed that concern traces [4, 14] improve predictability of software instabilities [6] . A concern usually spans a number of scattered architectural decisions and, possibly, is associated with multiple requirements [3, 4, 14] . For instance, concern traces allow describing which elements in architectural artefacts were influenced by a stakeholder's concern. However, all studies analysing the value of concern traces focused on implementation artefacts [4, 7] rather than on early software development artefacts. This paper presents a first exploratory evaluation on the effectiveness of concern traceability for assessing architecture stability. More specifically, we address the following research questions:"], "relatedWork": [], "rq": ["IntroductionSome authors have recently claimed that concern traces [4, 14] improve predictability of software instabilities [6] . A concern usually spans a number of scattered architectural decisions and, possibly, is associated with multiple requirements [3, 4, 14] . For instance, concern traces allow describing which elements in architectural artefacts were influenced by a stakeholder's concern. However, all studies analysing the value of concern traces focused on implementation artefacts [4, 7] rather than on early software development artefacts. This paper presents a first exploratory evaluation on the effectiveness of concern traceability for assessing architecture stability. More specifically, we address the following research questions:"]}
{"intro": ["These cognitive challenges have been addressed by current research. In the psychology research literature, the cognitive capabilities of individuals and the role of the physical environment thereupon are discussed extensively [cf. 27, 28, 31] . Research on memory aid systems highlights suitable technical support to recall information [cf. [11, 15, 16, 21] . However these current research discussions address mostly the individual level or single-user scenarios, and insights on the cognitive aspects in mobile collaborations and how the individuals' cognitive capabilities influence their collaborative behavior are scarce. To provide appropriate support, it is important to understand the role of the OOS-OOM phenomenon in mobile collaborations. This brings us to our first research question: RQ1: How does forgetting influence mobile counseling sessions?", "The cognitive aspects in collaborations are broadly discussed in research on computer supported collaborative work (CSCW). Researchers provide high-level insights on the concept of collaborative memory [2, 24] and extensively discuss how to support collaboration partners to create and use a collaborative memory [1, 2, 19] . However, current research focuses mostly on stationary scenarios, and insights on the novel challenges and their influence on group members' cognitive capabilities and the creation of a collabora-tive memory are lacking. Current research on mobile memory aid systems addresses the individual's cognitive capability, and presents diverse solutions to facilitate memory cue creation [15, 16, 21] . However, research insights on suitable support accounting for multiple users are scarce. Both the collaborative situation of the clientconsultant relationship and the mobile setting give rise to new challenges for developing suitable support systems. We thus asked ourselves how an appropriately designed \"memory aid\" could support actors to create and use a collaborative memory in their mobile collaboration. After reviewing related literature on psychology research, CSCWresearch and memory aid systems, as well as discussing different design options, it was \"working with pictures\" that turned out to be a surprisingly useful concept for responding to the characteristics of mobile collaborations and to bring information back into the minds of the actors. Thus, in this study we further pursue the second research question"], "relatedWork": ["The actors' cognitive processes not only comprise the individual's internal mental processes, but also their interactions with others as well as with the physical artifacts in the surrounding environment. In this context, the changing environmental context can influence individual's cognitive capabilities considerably. In psychology research literature, researchers discuss extensively the individual's cognitive processes and the role and effects of the environmental context on an individual's cognitive capabilities [cf. 3, 8, 10, 27, 28, 31] . Studies report on different effects (e.g., environmental reinstatement [28] , where changing the physical environment causes individuals to remember fewer memories) and propose measures to reduce the resulting context-dependent forgetting (e.g., multiple-learningcontext-technique [27] , where individuals learn information in different environmental contexts rather than one). Whereas these studies provide useful insights on how to enhance a human's cognitive capabilities, they mainly discuss them on the individual level, when reporting on several actors and how they influence each other's memories, discussions remain mostly conceptual. These interactions between actors can enhance individual's cognitive capability, e.g., by either cross-cueing [9] , where information from others trigger memories of an individual that s/he would not have remembered alone, or impair it, e.g., by the \"outshining\"-effect [28] , where an individual's memories are \"outshone\" by easier-to-access information. However, current research studies barely provide insights how corresponding effects influence actors' behaviors in collaborations.", "Current research on counseling support systems investigates consultants' and clients' collaborative work practices from different perspectives. Novak [17] describes the hampering effects of information overload in shared problem solving, where actors' ability to make decisions deteriorates due to the presence of too much information. Novak and Schwabe [18] highlight the impact of sticky information that is bound to a specific location (physically, mentally) aggravating the exchange of information. Nussbaumer et al. [20] discuss the information asymmetry between consultants (as experts) and clients (as laypersons), and report on the negative effects, thwarting collaborative interactions. Schmidt-Rauch and Nussbaumer [25] give insights into actors' collaborative task of co-creating the value of counseling and show how to design appropriate support to help them becoming more equal co-creators. In their solutions, these researchers give insights into collaborative work practices in counseling collaborations, showing how technical support systems should be designed to support users appropriately. However, they rarely consider the individuals' cognitive aspects in storing and recalling information, and do not discuss how they influence the individual's collaborative behavior. Furthermore, they focus mostly on stationary scenarios, disregarding the actors' physical environment and its role and effects within the consultant-client collaboration."], "rq": ["INTRODUCTIONThese cognitive challenges have been addressed by current research. In the psychology research literature, the cognitive capabilities of individuals and the role of the physical environment thereupon are discussed extensively [cf. 27, 28, 31] . Research on memory aid systems highlights suitable technical support to recall information [cf. [11, 15, 16, 21] . However these current research discussions address mostly the individual level or single-user scenarios, and insights on the cognitive aspects in mobile collaborations and how the individuals' cognitive capabilities influence their collaborative behavior are scarce. To provide appropriate support, it is important to understand the role of the OOS-OOM phenomenon in mobile collaborations. This brings us to our first research question: RQ1: How does forgetting influence mobile counseling sessions?", "INTRODUCTIONThe cognitive aspects in collaborations are broadly discussed in research on computer supported collaborative work (CSCW). Researchers provide high-level insights on the concept of collaborative memory [2, 24] and extensively discuss how to support collaboration partners to create and use a collaborative memory [1, 2, 19] . However, current research focuses mostly on stationary scenarios, and insights on the novel challenges and their influence on group members' cognitive capabilities and the creation of a collabora-tive memory are lacking. Current research on mobile memory aid systems addresses the individual's cognitive capability, and presents diverse solutions to facilitate memory cue creation [15, 16, 21] . However, research insights on suitable support accounting for multiple users are scarce. Both the collaborative situation of the clientconsultant relationship and the mobile setting give rise to new challenges for developing suitable support systems. We thus asked ourselves how an appropriately designed \"memory aid\" could support actors to create and use a collaborative memory in their mobile collaboration. After reviewing related literature on psychology research, CSCWresearch and memory aid systems, as well as discussing different design options, it was \"working with pictures\" that turned out to be a surprisingly useful concept for responding to the characteristics of mobile collaborations and to bring information back into the minds of the actors. Thus, in this study we further pursue the second research question"]}
{"intro": [], "relatedWork": [], "rq": ["Geostatistical ModelingWhile autocorrelation was ignored in HCI and related fields for many years, this is increasingly no longer the case. However, the methods that have been used to control for autocorrelation in HCI thus far -spatial error and spatial lag models -do not capture the spatial relationship between the demographics in one area and the Pok\u00e9Stop density nearby. Spatial Durbin models, an emerging best practice in the geostatistics literature, do capture this type of dependence, which is fundamental to our analysis. As cross-region relationships between dependent and independent variables like those in our analysis (more formally, \"exogenous spatial relationships\") are quite common in spatial data studied in HCI, spatial Durbin models will likely prove useful for HCI research questions outside the context of this paper. Overviews of spatial Durbin modeling can be found in Yang et al. [64] and Elhorst [7] ."]}
{"intro": [], "relatedWork": [], "rq": ["Results and DiscussionTo address RQ1 we compare the performance of RMUB against that of the other baselines. We observe that the RMUB method clearly outperforms the UB, UIR and URM baselines for P@5, nDCG@5 and nDCG@10, in both MovieLens 100K (see Table 1a ) and 1M (Table 1b) . Its performance in terms of P@50, however, is similar to some of the baselines, showing that our method is able to rank higher than such baselines interesting items for the user, at least until some reasonable cut-off, which in this case seems to be 50. Moreover, since this method takes two parameters (k and \u03bb), we analyse now its performance sensitivity. Due to space constraints, we only explore in Figure 2 the neighbourhood size k, but we include the performance for two values of \u03bb -the optimal (\u03bb = 0.1) and neutral (\u03bb = 0.5) configurations -where a negligible difference is obtained. We can also notice in the same figure that the baseline NC+P obtains a much better performance than RMUB, consistently with results reported in [2] . Table 1 also shows that the coverage results for the NC+P baseline are better than for RMUB in their optimal settings. We further observed (we omit the detailed results here for the sake of space) that the coverage of NC+P decreases with larger k's (as reported in [2] ), whereas the coverage of RMUB increases, but at the expense of losing precision. All in all, our answer to RQ1 is that relevance models as a standalone method for neighbour selection are useful but not optimal."]}
{"intro": [], "relatedWork": [], "rq": ["RQ1: What is the effect of asking for contributions?In this research, we seek to deepen our understanding of several key questions related to the design of precision crowdsourcing requests. First of all, fundamentally we are interested in whether our requests are effective at eliciting contributions. The core idea of precision crowdsourcing is to turn information consumers into contributors through a series of interventions. We are motivated to study not only the immediate response to precision crowdsourcing requests, but also the impact on long-term behaviors. As shown by Masli et al. [20] , \"techniques that manipulate users into participating and contributing information may succeed in the short-term but might cause long-term harm, because users tend to recognize the manipulations and may consider them unfair\". Looking at the positive side, previous research also shows that entry barriers and other opportunities for members to make community-specific investments can increase users' commitment to the system [14] . In this paper, we study two categories of long term behaviors: long-term commitment and long-term contribution. We are interested to know whether asking users to contribute information increases or decreases their usage of the system, or has no significant effect at all. Further, asking may achieve users' compliance as expected, however may affect users' voluntary contribution out of the requests when we stopping prompting."]}
{"intro": ["We identify that container-like understandings of urban tourism space [51] , as represented in the tourist-historic city model [12] [13] [14] , resemble the traditional framing of space as a \"natural fact\" [64] in the CSCW community. Both approaches, however, are lacking satisfying explanations of how tourism space can emerge and develop in residential neighborhoods. The tourist-historic city model solely takes into account tourism-related facilities and infrastructure as defining elements of tourism space. Such structures are often non-existing in new urban tourism areas like Kreuzk\u00f6lln, one of our case-study neighborhoods. Still, this neighborhood without any major sights is becoming a tourism hotspot [33, 55] . Against this background, we follow a constructionist understanding of urban tourism space [51, 68, 136] . We argue that tourism space, like tourist sights [84] , is socially constructed through representations [99, 102] and performances [15, [45] [46] [47] 74] . That means we no longer regard tourism facilities and infrastructure as the central elements defining tourism space. Instead, people are the major agents who transform places and landscapes into tourist destinations. They attach meanings and values to places and objects [37, 102] , produce written, oral, or pictorial representations of them, and thus contribute to the discourse of how places or objects are to be perceived. Finally, following the performative turn in tourism studies [74, 75] , we argue that places need to be enacted through \"bodily performances\" [15, 75] . Practices, such as picture taking or collectively \"gazing\" [122] upon a building, are necessary to enact places in a touristic manner [76] . Taking these theoretical considerations into account, we analyze how two different Berlin neighborhoods, Kreuzk\u00f6lln and City West, are socially constructed in Airbnb listings. The following three questions guided our research: RQ1: How are the two neighborhoods Kreuzk\u00f6lln and City West constructed as tourism spaces in Airbnb listings? RQ2: How does the space construction differ between these two neighborhoods? RQ3: How do the neighborhood descriptions differ between Airbnb hosts and the destination management and marketing organization (DMO)?"], "relatedWork": ["Understanding space and place on a conceptual level is a central goal of geography. Much empirical work, however, deals with natural or cultural phenomena happening in distinct places. Considering the nature of space just became popular again after the \"spatial turn\" [116] in social sciences from the late 1960s onwards [117] . This development induced debates on the nature of space and place in various disciplines, including the CSCW research community [41, 50, 64] . In 1996, Harrison and Dourish [64] and in 2006 Dourish [41] broadly discussed differences and similarities between the concepts of space and place and resulting implications for CSCW researchers. An important outcome of their considerations, to which we relate our research, is the assumption that digital technologies, such as online sharing platforms, influence the way people encounter and appropriate urban space."], "rq": ["INTRODUCTIONWe identify that container-like understandings of urban tourism space [51] , as represented in the tourist-historic city model [12] [13] [14] , resemble the traditional framing of space as a \"natural fact\" [64] in the CSCW community. Both approaches, however, are lacking satisfying explanations of how tourism space can emerge and develop in residential neighborhoods. The tourist-historic city model solely takes into account tourism-related facilities and infrastructure as defining elements of tourism space. Such structures are often non-existing in new urban tourism areas like Kreuzk\u00f6lln, one of our case-study neighborhoods. Still, this neighborhood without any major sights is becoming a tourism hotspot [33, 55] . Against this background, we follow a constructionist understanding of urban tourism space [51, 68, 136] . We argue that tourism space, like tourist sights [84] , is socially constructed through representations [99, 102] and performances [15, [45] [46] [47] 74] . That means we no longer regard tourism facilities and infrastructure as the central elements defining tourism space. Instead, people are the major agents who transform places and landscapes into tourist destinations. They attach meanings and values to places and objects [37, 102] , produce written, oral, or pictorial representations of them, and thus contribute to the discourse of how places or objects are to be perceived. Finally, following the performative turn in tourism studies [74, 75] , we argue that places need to be enacted through \"bodily performances\" [15, 75] . Practices, such as picture taking or collectively \"gazing\" [122] upon a building, are necessary to enact places in a touristic manner [76] . Taking these theoretical considerations into account, we analyze how two different Berlin neighborhoods, Kreuzk\u00f6lln and City West, are socially constructed in Airbnb listings. The following three questions guided our research: RQ1: How are the two neighborhoods Kreuzk\u00f6lln and City West constructed as tourism spaces in Airbnb listings? RQ2: How does the space construction differ between these two neighborhoods? RQ3: How do the neighborhood descriptions differ between Airbnb hosts and the destination management and marketing organization (DMO)?"]}
{"intro": [], "relatedWork": [], "rq": ["ResultsBesides, a first analysis of the data exhibits many outliers, in particular for the one-command listeners. To understand the presence of these outliers, we manually scrutiny some of them and their change history. We observe that some of these outliers are GUI listeners which size has been reduced over the commits. For instance, we identified outliers that contained multiple GUI commands before commits that reduced them as one-or two-command listeners. Such listeners distort the analysis of the results by considering listeners that have been large, as one-or two-command listeners. We thus removed those outliers from the data set, since outliers removal, when justified, may bring benefits to the data analysis [25] . We compute the box plot statistics to identify and then remove the outliers. Figure 2 depicts the number of fault fixes per LoC (i.e., FIX) of the analyzed GUI listeners. We observe an increase of the fault fixes per LoC when CMD \u2265 3. These results are detailed in Table 1 . The mean value of FIX constantly increases over CMD. Because these data follow a monotonic relationship, we use the Spearman's rank-order correlation coefficient to assess the correlation between the number of fault fixes per LoC and the number of GUI commands in GUI listeners [25] . We also use a 95 % confidence level (i.e., p-value<0.05). This test exhibits a low correlation (0.4438) statistically significant with a p-value of 2.2 \u00d7 10 \u221216 . Regarding RQ1, on the basis of these results we can conclude that the number of GUI commands per GUI listeners does not have a strong negative impact on fault-proneness of the GUI listener code. This result is surprising regarding the global increase that can be observed in Figure 2 . One possible explanation is that the mean of the number of bugs per LoC slowly increases over the number of commands as shown in the first row of Table 1 . On the contrary, the range of the box plots of Figure 2 strongly increases with 3-command listeners. This means that the 3+-command data sets are more variable than for the 1-and 2-command data sets. Figure 3 depicts the number of commits per LoC (i.e., COM-MIT) of the analyzed GUI listeners. These results are also detailed in Table 1 . We observe that COMMIT does not constantly increases over CMD. This observation is assessed by the absence of correlation between these two variables (0.0570), even if this result is not statistically significant with a p-value of 0.111. We can, however, observe in Figure 3 an increase of COMMIT for the three-command listeners. Regarding RQ2, on the basis of these results we can conclude that there is no evidence of a relationship between the number of GUI commands per GUI listeners and the change-proneness of the GUI listener code."]}
{"intro": [], "relatedWork": [], "rq": ["METHODOLOGYThe method described henceforth was influenced and heavily indebted to the exemplary review into Sustainable HCI conducted by DiSalvo et al (2010) [25] and the previous research [26, 41] which inspired their approach to reviewing and framing research questions. However, the focus in this review is shifted towards interpersonal rather than environmental relations. Of crucial importance is how the HCI community conceptualises prosociality and the scope of human agency within the economy. The review process began by constructing a relevant corpus of papers from which specific examples could then be selected. The search term \"Prosocial HCI\" was used in the ACM Guide to Computing Literature with all results noted and added to the corpus. Any papers that were cited in each corpus paper were then also examined to see if they fit within the original scope and were added where suitable. The criteria for determining suitability were twofold. Firstly, it was asked whether each work had an explicit goal which was related to prosocial economic relations."]}
{"intro": ["There are two implicit assumptions in the above research question, which are intuitive but not always confirmed by prior studies. The first assumption is that profiles length positively affects user utility. Some works show that profile length of new users is positively correlated to the accuracy of recommendations in term of user utility [12] [13] [4] . However, this result cannot be easily generalized, as its supporting experiments are limited to item-based collaborative algorithms, and accuracy is measured only in terms of error metrics: RMSE [13] and MAE [4] . Moreover, [28] finds that the correlation between profile length and utility is not always present, but it depends on the elicitation strategy adopted. These studies instill some doubts on the general assumption that a longer profile corresponds to more accurate recommendations. We may wonder, for example, to what extent we can claim that the fallout of a content-based recommender algorithm improves with the profile length."], "relatedWork": ["Similar results are outlined in [29] , where experiments show that more elicited ratings do not necessarily imply more perceived effort. However, these findings have a different motivation with respect to [27] and [28] . Users in [29] perceive a low effort with poor quality recommender algorithms, even in the case of a very long elicitation process, as they feel the need to provide more ratings to the RS in order to improve quality."], "rq": ["INTRODUCTIONThere are two implicit assumptions in the above research question, which are intuitive but not always confirmed by prior studies. The first assumption is that profiles length positively affects user utility. Some works show that profile length of new users is positively correlated to the accuracy of recommendations in term of user utility [12] [13] [4] . However, this result cannot be easily generalized, as its supporting experiments are limited to item-based collaborative algorithms, and accuracy is measured only in terms of error metrics: RMSE [13] and MAE [4] . Moreover, [28] finds that the correlation between profile length and utility is not always present, but it depends on the elicitation strategy adopted. These studies instill some doubts on the general assumption that a longer profile corresponds to more accurate recommendations. We may wonder, for example, to what extent we can claim that the fallout of a content-based recommender algorithm improves with the profile length."]}
{"intro": ["Telehealth systems are increasingly gaining attention from health consumers and health professionals due to their promise to reduce healthcare costs and to make effective use of clinical resources. However, there are issues revolving around the underlying concepts of current telehealth applications, which are designed to manage diseases instead of preventing them [17] . They help to keep patients' conditions stable, but do not motivate them to improve their health. These applications do not promote self-efficacy and are expensive due to the heavy reliance on clinicians to interpret data and make decisions. Many existing systems cannot be extended by third parties, entail additional costs when adding new functionalities, and do not address the social and psychological needs of the patient. Such shortcomings motivate the development of novel online telehealth platforms, which provide inexpensive and ubiquitous care via mainstream sensing devices [17, 34] .", "Telehealth systems are commonly used for managing acute and chronic conditions where regular monitoring of vital signs (e.g. heart rate and weight) is crucial. However, requirements for general health support are different and include education, monitoring, psychological support (motivation), and adding social components for patient support. In previous work, we demonstrated the advantages of leveraging consumer-level devices such as off-theshelf computers and motion-sensing input devices to make telehealth more accessible and affordable [1] . In this paper, we systematically categorise and analyse a range of sensing devices for healthcare, particularly their capability to extend the functionalities of telehealth systems. We evaluate each device based on its technology, pros and cons of its usage in healthcare, and findings from related studies. We try to answer the following research questions: 1) \"how can off-the-shelf consumer-level sensing devices be categorised for healthcare applications?\", and 2) \"how have these devices already been leveraged in healthcare?\" Section 2 reviews and analyses common consumer-level sensing devices. Section 3 presents results of the analysis and discusses how these devices can be leveraged in telehealth systems and Section 4 concludes the paper."], "relatedWork": [], "rq": ["INTRODUCTIONTelehealth systems are commonly used for managing acute and chronic conditions where regular monitoring of vital signs (e.g. heart rate and weight) is crucial. However, requirements for general health support are different and include education, monitoring, psychological support (motivation), and adding social components for patient support. In previous work, we demonstrated the advantages of leveraging consumer-level devices such as off-theshelf computers and motion-sensing input devices to make telehealth more accessible and affordable [1] . In this paper, we systematically categorise and analyse a range of sensing devices for healthcare, particularly their capability to extend the functionalities of telehealth systems. We evaluate each device based on its technology, pros and cons of its usage in healthcare, and findings from related studies. We try to answer the following research questions: 1) \"how can off-the-shelf consumer-level sensing devices be categorised for healthcare applications?\", and 2) \"how have these devices already been leveraged in healthcare?\" Section 2 reviews and analyses common consumer-level sensing devices. Section 3 presents results of the analysis and discusses how these devices can be leveraged in telehealth systems and Section 4 concludes the paper."]}
{"intro": [], "relatedWork": [], "rq": ["Trade-Off between Dynamic Programming and Feature LocalityIt is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005) . There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task."]}
{"intro": ["The current state of the art in natural language processing calls for components Callable of sophisticated deep semantic analysis, modular representation of resources, and re-usability of those resources across different NLP applications. Furthermore, it has been demonstrated that the sheer diversity of interactions between distinct kinds of linguistic information is guaranteed to defeat any staged approach to generation/understandiug that successively maps between levels of representation [1] . One strategy for addressing these problems is to stratify resources so that inter-stratal mappings are simplified. This is aimed at allowiug high-level information to apply as early in analysis ms possible at a minimal cost. A number of current processing architectures call for such a design. The stratification technique is also one way of ensuring modularity and improved reusability. However, one important problem with almost all existing linguistic resources is that the interstratal mappings between, for example, strings and semantics, are anything but simple. This,is because the standard syntax-semantics-pragmatics modularization under-stratifies by imposing fewer distinctious than are necessary."], "relatedWork": [], "rq": ["4value oi the Beta attribute of tile top-most chooser. After classifying the input structure according to tile features it contains (already yielding a paxticular subtype of CHOOSER), the type of the topmost node of the input structure is then recursively expanded. Expansion is performed by rewriting all embedded types through unifying substitution of their definitio~ls until no filrther rewriting is possible (i.e., until all types are ground types). Expansion terminates with a conq)lcte description coralmtible with the input partial description and with the definitions ill the feature type system representing \"all the linguistic strata defined. In the general case, we will end up with not just sac description, lint rather with a set which is then to be interpreted as a disjunction of possil~le solutions to the initial problem. The complete structure which is the result of the interprctation of the semantic specification (given under the nero feltture) is given in Figure 3 HPSG, does provide cxtensive detail at this level. Now, due to the strict modularity enforced in our translation, it is possible to explore combinations of approaches and, moreover, to combine descriptions from a theory like HPSG with the kind of descriptions employed in Systemic Linguistics or its Computational instautlations. This has been shown to be possible il~ a simple experiment carried out by Martin Emelc where an existing HPSG grammar was taken and the semantics of that grammar (a simple situation semantics-informed frame-like representation) was rewritten to give tim syntagmatic categories and structures of the SFG. This makes it possible to describe the information ot)tained front the two approaches within a single executable declarative specification. Her('., however, our utain concern has been with making available the higher-levels of specification, and so wc will abstract away front the string to syutagmatic structure component of the mapl)iug and take as the 'input' specification the lowest level ofinforroation obtained from the SFG, as shown at)ove. Therefore, we proceed by putting this specification in tile syn slot of the IthNK-CHOOSER relation. Tcrnl rewriting applies to construct the sere side of tim relation and also to complete tile syn sl)ecification. The result is again tile COml)lete specification of the set of constraints titat describe the structure, which is again the structure shown in Figure 3 . This is precisely the same linguistic-sign that was produced as a result of \"generation\", starting froln the imrc semautic part of the descril)tion - descriptions beiug developed within the PENMAN, KOMET and POLYGLOSS projects. We have shown that systemic-functional grammars and semantics can easily be converted to the TFS formalism. This has produced a fragment that can both generate and analyse. Furthermore, the analysis achieved with our experimental fragment supports the mapping from surface representation to deep semantic levels of representation that are far removed from the contingencies of surface syntax. These represeutatimts also preserve breadth, in that the semantic distinctions necessary for generation concerning 'pragmatic' information such as textual organization and interpersonal communicative goals are also recovered. It is especially imt)ortaut that all of these diverse levels have now been made accessible for analysis within a system where there is only one representational formalism and only one interpretational device operating on the representations. This paper has described and motivated the basis for a host of important further research questions, some of which we are now following up. For example, the fragment we have illustrated here is very small: the problem of handling large lattices needs to be addressed both on implementational aud theoretical levels. A fldl specification of the grammar component of PENMAN alone as we describe it here would involve tens, possibly hundreds, of thousands of types: this ueeds to be supported by sufficiently powerful and robust implementations. But on the theorcticai level, there are also further nmdularities within the SFG account that we have not yet utilized to constrain term explosions due to forming cross-products across sublattices: two areas here clearly present themselves ---stronger modularization according to the paradigmatic/syntagmatic dimensiou and according to functional regions in the grammar [4] , which already provide a meta-level of organization across sublattices that remains unused. A fnrther area is a closer study of the similarities and differeuces between, e.g., the information of the SFG and the HPSG modules --it is to be expected that there is currently duplication which could be more effectively distributed, perhaps providing a more effective TFS translation. Finally, the availability of a representation of some systenfic-functional grammars in a standard formalism should further facilitate comparison and evaluation of the grammatic',d description with respect to other current cmnputational accounts of grammar: it should be more straightforward to identify the distinctive features and claims of the approach, thus opening the door to an easier exchange of information and analyses. Further, performing the TFS translation for the entire PENMAN grammar would provide an effective test of the TFS formalism (and its implementation) overall since there are no comparable grammars (i.e., paradigmatic feature based without a phrase structure skeleton) of this size available elsewhere."]}
{"intro": [], "relatedWork": [], "rq": ["RQ1a: Search Engine Bias Quantification FrameworkFor instance, the last row of Table 1 computes OB(q, r) at rank r = 5 with respect to the situation shown in Figure 1 . Note that the bias score s 2 of the top-ranked item i 2 is given the highest weight, followed by the bias score s 4 of the second-ranked item i 4 , and so on. This follows the intuition Session: Politics, Party, Policy, & Participation CSCW 2017, February 25-March 1, 2017, Portland, OR, USA that bias in the higher ranked items are likely to influence the user more than bias in the lower ranked items. 2 Ranking Bias: The ranking bias is intended to capture the additional bias introduced by the ranking system, over the bias that was already present in the set of relevant items (i.e., relevant to q) input to the ranking system. If possible, ranking bias could be measured by auditing the exact ranking system being deployed by the search engines. However, for any commercial search engine deployed in the real-world, it is infeasible to know the internal details of the ranking system. Hence, we view the ranking system as a 'black box' where we only observe the inputs and outputs."]}
{"intro": [], "relatedWork": [], "rq": ["PROCESS: SCAFFOLDING AN ITERATIVE AND REFLECTIVE PROCESSTo support the process of children engaging as protagonists, we employed a process model that could support this particular stance throughout the entire design process. The model was based on six main activities. The circular model illustrates design as an iterative process, since all design outcomes eventually lead to new research questions and new problem framings. Moreover, iterations occur within and between each of the activities and with increaasing experience, eventually crisscrossing effects are created that use the model as a framework for navigating through one's own design project. The model contains six main design activities, each including several sub-activities: (1) the design brief, for framing a complex challenge and planning the design process; (2) field studies, to explore and research the context and users; (3) ideation, for the creative development of ideas using various techniques and materials; (4) fabrication, for mock-up and prototyping of concepts using digital technologies; (5) argumentation, for testing a design concept or product and reflecting on the design moves and arguments of the process; and (6) reflection, for reflecting on the learning outcome-or design competence-developed through the entire design process (see Fig. 1 ). For a description of the process model, see Smith et al. [34] . We recognize that our process model is one among several suitable design models to illustrate iterative design processes. However, this particular model was designed to embed certain characteristics that support children as protagonists throughout the design process. First, the design model supported an ongoing interplay between divergent and convergent thinking and doing. Divergence and convergence notoriously compel the children to open up their design process and take in new perspectives and subsequently deselect (potentially important) aspects in their efforts to reach a meaningful design solution. The interplay between divergence and convergence demands a high level of commitment, agency, and determination. These in turn sustain the children's role as protagonists in the process. Second, the design model incorporates argumentation and reflection as the closing activities of each iteration. By positioning reflection as the outcome of the design activity, the design process closely ties its aim to the objective of developing a reflecting stance toward technology and design, as described in the \"child as protagonist\" perspective. Third, the design model does not prescribe either specific actions or project measures. It merely indicates how the design process develops. This is to encourage the children to be mindful about their process, their collaboration, and their choices during the process, rather than focusing solely on the tangible outcome. There are no formal instructions in the model, indicating that there is no correct way for children to proceed through the stages of project framing, research, ideation, and fabrication to the final stages of argumentation and reflection. This lack of authority in the model transfers a high level of self-efficacy and agency to the children. They have to stay in charge of the design process while they gradually explore the design brief and activities provided by the design experts."]}
{"intro": ["As the use of drones becomes increasingly prevalent, research on effective communication with humans and HRI for social robots is necessary in order to facilitate the acceptance of drones in everyday environments. We refer to these drones as social drones. Gongora and Gonzalez-Jimenez [5] examined the technology for surveying drone maneuvers using GPS, and Cho et al. [6] examined the aspect of usability, considering the approachability of drone control to the general public. However, there has been a lack of research on service evaluations based on the perceptions of the humans (users) that interact with drones. Therefore, we consider what variables must be considered in human-drone interactions and what effects these variables have on service evaluations.", "Furthermore, as social robots, drones face a robotontological issue, namely, safety. Dautenhahn and his colleagues [13] studied human comfort while interacting with a social robot. They thought that feelings of safety with a robot would be impossible to study and instead user comfort should be the focus. In this paper, perceived safety includes 2 Advances in Human-Computer Interaction both of these meanings and we constructed a variable that could influence the level of satisfaction in social drone services. Little by little, academic focus has shifted to human perceptions of social drones. Cauchard and her colleagues [14] studied drones as a type of social computing that features an affective factor. However, this is merely a starting point in the study of social drones. In this paper, we will explore the relationship between user satisfaction and two fundamental issues: drone control conditions and perceived safety. Thus, in order to empirically investigate this relationship, the present study examines the following research question. [15] define an avatar or an agent as follows: \"a perceptible digital representation whose behaviors reflect those executed, typically in real time, by a specific human being.\" These terms are often encountered when using a computer application or playing a game. Examples include the clipper in MS office, Siri on an iPhone, and various avatars featured in the game Second Life. Depending on who controls these characters (a human versus the system), there can be different perceptions or service evaluations. Lim and Reeves [9] studied the engagement of avatars and agents, part of the game experience, and found that playing a game with avatars showed improved engagement over the use of agents. Concerning general interactions, Cauchard and her colleagues [14] studied the social evaluations during interactions with digital human representations and observed that there is a difference in social evaluations of avatar and agent environments when digital human representations were made to smile, one of the social cues of interaction. Namely, there was a tendency toward a negative evaluation of the smile of a digital human, which is an agent."], "relatedWork": [], "rq": ["IntroductionFurthermore, as social robots, drones face a robotontological issue, namely, safety. Dautenhahn and his colleagues [13] studied human comfort while interacting with a social robot. They thought that feelings of safety with a robot would be impossible to study and instead user comfort should be the focus. In this paper, perceived safety includes 2 Advances in Human-Computer Interaction both of these meanings and we constructed a variable that could influence the level of satisfaction in social drone services. Little by little, academic focus has shifted to human perceptions of social drones. Cauchard and her colleagues [14] studied drones as a type of social computing that features an affective factor. However, this is merely a starting point in the study of social drones. In this paper, we will explore the relationship between user satisfaction and two fundamental issues: drone control conditions and perceived safety. Thus, in order to empirically investigate this relationship, the present study examines the following research question. [15] define an avatar or an agent as follows: \"a perceptible digital representation whose behaviors reflect those executed, typically in real time, by a specific human being.\" These terms are often encountered when using a computer application or playing a game. Examples include the clipper in MS office, Siri on an iPhone, and various avatars featured in the game Second Life. Depending on who controls these characters (a human versus the system), there can be different perceptions or service evaluations. Lim and Reeves [9] studied the engagement of avatars and agents, part of the game experience, and found that playing a game with avatars showed improved engagement over the use of agents. Concerning general interactions, Cauchard and her colleagues [14] studied the social evaluations during interactions with digital human representations and observed that there is a difference in social evaluations of avatar and agent environments when digital human representations were made to smile, one of the social cues of interaction. Namely, there was a tendency toward a negative evaluation of the smile of a digital human, which is an agent."]}
{"intro": [], "relatedWork": [], "rq": ["RQ4. How is sleep debt related to mood?A meta-analysis of laboratory sleep studies found that sleep deprivation significantly negatively impacts mood [42] . However, in contrast, a study of college students who were sleep deprived for 24 hours showed no significant changes in mood, such as negative mood states of anger and anxiety [43] . The authors explain this to the notion that 24 hours of sleep deprivation is long enough to affect fatigue but not long enough to impact mood. A large cross-sectional survey study of college students though did find a relationship between self-reported sleep disturbances and negative mood [28] . Because sleep loss over 24 hours did not affect mood, we expect that longer accumulated sleep could affect mood. Therefore in this research question we examine sleep debt."]}
{"intro": ["As HCI has responded to broader societal challenges such as sustainability [16] and healthy living [2, 7, 15 ] the field has increasingly explored how technologies might be used to promote behavioural change. Working in this area, the focus of this paper is on the participatory design of a range of technologies to motivate exercise for people recovering from a stroke at home. Over a three-year time frame, we have worked closely with clinicians and patients to understand the stroke experience and how we might meet the varying needs of our participants. Here we present an account of the development of distinct solutions to motivate post-stroke rehabilitation exercises for four individuals who wished to recover upper limb functionality after stroke, and who volunteered to participate in our project. Following participatory design sessions in their homes, we have deployed four prototypes for periods ranging from four weeks to seven months. All participants needed to do rehabilitative exercise regularly at home, without professional support. However, adherence to programmes of rehabilitation at home is poor [20] perhaps because rehabilitative exercise can be boring and difficult to do. In response to these challenges, the research question driving this work was whether we could improve participants' adherence to a rehabilitation schedule by developing technologies that tapped into their individual motivations, but which are supportive of broader care goals, i.e., bridging between the domestic life of individuals recovering from stroke and their clinical care programmes."], "relatedWork": [], "rq": ["INTRODUCTIONAs HCI has responded to broader societal challenges such as sustainability [16] and healthy living [2, 7, 15 ] the field has increasingly explored how technologies might be used to promote behavioural change. Working in this area, the focus of this paper is on the participatory design of a range of technologies to motivate exercise for people recovering from a stroke at home. Over a three-year time frame, we have worked closely with clinicians and patients to understand the stroke experience and how we might meet the varying needs of our participants. Here we present an account of the development of distinct solutions to motivate post-stroke rehabilitation exercises for four individuals who wished to recover upper limb functionality after stroke, and who volunteered to participate in our project. Following participatory design sessions in their homes, we have deployed four prototypes for periods ranging from four weeks to seven months. All participants needed to do rehabilitative exercise regularly at home, without professional support. However, adherence to programmes of rehabilitation at home is poor [20] perhaps because rehabilitative exercise can be boring and difficult to do. In response to these challenges, the research question driving this work was whether we could improve participants' adherence to a rehabilitation schedule by developing technologies that tapped into their individual motivations, but which are supportive of broader care goals, i.e., bridging between the domestic life of individuals recovering from stroke and their clinical care programmes."]}
{"intro": ["Recent years have seen a growth in the learning games market (also referred to as educational/serious games, games-based learning or games for learning), projected to reach $4.8M within the next two years [4] . The inclusion of these games within formal education has become more commonplace with many games prioritising the teaching of curriculum subjects such as literacy and numeracy [6] [7] [8] . The promise to support or even enhance learning through games has increased designers' responsibility to elucidate their design rationale. However, games researchers have often expressed concerns as to whether learning games effectively marry good game design and pedagogy [9] [10] [11] .", "Feedback plays a powerful role in raising achievement above and beyond other instructional interventions [19] . Games, it has been argued, are particularly apt in delivering in-the-moment feedback [20, 21] , and are \"feedback-rich environments that can provide many, often subtle, cues about player status\" [22] . Previous research has begun to recognise the need to examine how feedback is designed in learning games [23] . This work has often, however, treated feedback at a high-level, for instance simply identifying the presence of appropriate feedback [8, [24] [25] [26] , thus excluding a deeper analysis of how this feedback has been designed. This paper seeks to understand and evaluate how feedback is currently represented in learning games for early learners through the critical case of the reading domain. A review of the empirical literature on feedback and learning games is used to inform a framework for content analysis that is subsequently applied to five popular early learning games for reading. Taking a theoretical and critical lens, we scrutinise the types of feedback present in these games to propose new opportunities for game design and research."], "relatedWork": [], "rq": ["Need to Support Learning Mechanics as well as ContentIn contrast to the uniform inclusion of effective teaching principles for reading in all of the games, with the exception of Nessy, the remaining four games reflected less effort in supporting learning of the game mechanics (gameplay mechanics; Table 3 ). Typically in games the player develops an understanding of the game play schema through experiencing failures at various points in the game and then trying again [50] . However, within learning games it is difficult to separate failure due to the game mechanic or failure due to a gap in understanding the learning content. Previous work has shown when children experience breakdowns during learning games they may need support with both the learning content and with working out the game mechanics [51] . This need for support has been found to increase in pace with the complexity of game mechanics [47, 52] . Plass et al. [50] recommend in learning game design the choice of game mechanics should not introduce these unnecessary confounds. Whilst the reviewed games mainly utilised more familiar multiple choice mechanics, given the young learner group we argue that they will still need opportunities to become familiar with the broader game play schema prior to focusing on new learning content. The most appropriate form for this support remains an open research question."]}
{"intro": ["In recent years, due to an increased use of ubiquitous and wearable technologies as well as social networks, the everyday-life of the individuals is now tightly bounded with the digital life: an increasingly large fraction of what we say and do, from taking a picture to buying a good, from visiting a place to meeting a friend, leaves a digital trace that tells something about our lives [13] . Unprecedented capabilities in collecting user and context data, together with technical and theoretical advancements in computer science give novel opportunities for User Modeling: a User Model (UM) could now exploit information related to many aspects of the user (from her medical records to her food behavior, from her physiological parameters to her psychological states, etc.), creating a sort of total, holistic representation of an individual [4, 17] . The increased Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. complexity of UM would then enable novel forms of personalized services, which may be delivered anywhere at any time, potentially impacting every domain of people' daily life [5] . In particular, new forms of recommendations might be able to give suggestions on an aspect in a specific domain starting from data coming from multiple, and maybe apparently unrelated, contexts. This would enable a sort of holistic recommendation, that is to say, a recommendation built on the ground of a holistic representation of the user's needs, interests, knowledge, preferences [5] . Such a representation is defined through the collection of data from diverse data sources and by reasoning over them in order to populate the different facets describing the person. As an example, the interests of the users may be inferred on the ground of the places she visits, the topics she discusses or the opinions she expresses on social networks or through the reviews she writes. The notion of holistic recommendation can be seen as an evolution ofcontext-aware (recommender) systems [1] , which have a long tradition in creating algorithms considering different variables about the user in order to provide (possibly just-in-time) recommendations. For instance, they can mine social networks to automatically infer context features [18, 21] . Other works focused on providing real-time dynamic recommendations [23] . Probably the field where real-time recommendations have been most used is tourism [2] . Holistic recommendations are based on a \"complete\" representation of the user. Such systems could capture every aspect of the user pertaining to the different spheres of her life, even over very long periods of time, and handle their changes, according to a lifelong user modeling vision [12] . In principle, the user could be allowed to explore her holistic user model, which could further scaffold processes of self-reflection. Moreover, holistic recommenders could be exploited to make forecasts on the user's goals, behavior and preferences on the basis of past and current trends in her data. For example, we can think of smart adaptive systems able to predict what would be useful for users, by simulating the future evolution of their data and setting the right goals to be reached based on such predictions. Then, they could provide recommendations triggered by the user's current condition, suggesting which kinds of actions and changes the user should put in place to meet the set goals. Finally, holistic recommenders can provide suggestions in every domain and context of the user's everyday life, becoming a sort of pervasive personal advisor. However, even if extremely accurate, how and when such suggestions are delivered may arise potential issues, e.g. interrupting the user's activities, or being out of context and socially inappropriate (e.g. the user does not want to have recommendations about the diet to follow, which could be seen by her friends, when she is out with them). In this perspective, privacy surely becomes a fundamental concern. How can we define a holistic user model, provide holistic recommendations and ensure, at the same time, the privacy issues that may arise from the collection of data potentially coming from the whole individual's life?"], "relatedWork": [], "rq": ["PRIVACY ISSUES IN HOLISTIC RECOMMENDATIONAlthough holistic recommendation may provide undeniable advantages to end users, they use large amounts of potentially sensitive/private data (e.g., health data, location data, sexual preferences, and so on). Moreover, models and algorithms trained by using such behavioural data may leverage discriminating patterns, e.g., genderor ethnicity-based decisions that results in recommending two completely different products to two different persons (e.g., with different gender or ethnicity), sharing exactly the same behaviour. In addition, it is important to trace provenance and proof of the data in the user and context model, in the perspective to make the models explainable and scrutable by the user [24] and give her the possibility to know how the data in the models are gathered and inferred. Consequently, an ethical holistic recommender system algorithm or model must be fair, explainable and privacy-preserving. Fairness. With the recent advances in artificial intelligence and machine learning and the resulting concerns for their consequences on human freedom and rights, in the last decade, many research groups have addressed fairness issues [9, 11] . The topic, however, has been only superficially addressed in the recommender systems community [9] , even though this is a crucial aspect of holistic recommendation, due to the heterogeneity and complexity of data sources. Consequently, many research questions are still far from being solved or even discussed, in some cases. First, how widespread is the problem of algorithmic bias in recommender systems? How to address the complexity of holistic recommender systems in an efficient way without affecting the accuracy of the recommendations too much? How to detect discrimination in the underlying algorithms? How to regularize them in order to dismiss potentially discriminative patterns and decisions?"]}
{"intro": ["Before focusing on San Francisco, a quick definition of terms is in order, namely, ethnicity and the relatedly contested term, race. As numerous scholars have argued (e.g., Fishman 1989; Fought 2006; Eckert 2008 2; 3 ; Becker & Coggshall 2009) 2; 3 , these forms of categorization are just as socially constructed as any other categories (e.g., gender or class), and the ways in which this construction occurs will have implications for the analysis of language in a given community. Ethnic categories, which typically reflect shared aspects of identity such as cultural and religious heritage and migration history, are often conflated in American discourse with racial categories. Race, however, is problematically constructed with greater reference to perceived physical similarities such as skin color or facial features, which can vary widely within ethnic groups. That said, even the US Census refers to ethnic differences such as Race, only using the category of ethnicity to refer to persons of Hispanic or Latino heritage. Although this article relies on some 'racial' categories when referencing demographic sources such as the US Census, it advocates a theoretical focus on the local construction of ethnic identities and their boundaries, in other words, the ways in which ethnicity is conceived, created, and challenged by San Franciscans in their day-to-day lives (Eckert 2008a) ."], "relatedWork": [], "rq": ["Mexican Americans and being Chicano in San FranciscoThe third analysis of San Francisco English, by Moonwomon (1992) , focused specifically on the low vowel system, analyzing phonetic shift in the vowels of TRAP, LOT, and THOUGHT. This study, however, eliminated ethnicity as part of the analysis through its methodological design. By confining her speaker sample to white women, Moonwomon investigated research questions particularly relevant to that demographic, considering variation in socioeconomic class as it is realized between speakers of comparable sex and ethnicity. At the same time, Moonwomon's choice to define her participant sample in this way implied that ethnic variation (and gender or sex variation as well) has such a potentially strong impact on the analysis of San Francisco English that it was beyond the scope of her study. Her analysis did confirm that the sound changes found among younger speakers in the Hinton et al. (1987) study (of which she was a part) were in progress within this sample, specifically that TRAP was raising and fronting before nasals, lowering and backing elsewhere, and that LOT and THOUGHT were moving toward merger. Furthermore, Moonwomon found differences between middle-class and working class women with respect to attitudes toward ethnic shift in San Francisco, with some of the working class participants expressing greater resistance to the increased Asian and specifically Chinese presence in their neighborhood. However, Moonwomon did not draw specific connections between the low vowel production patterns of these women and their orientations toward ethnicity, although she did find more advanced (i.e., more raised before nasals) productions of the TRAP vowel for the middle-class speakers. As Bucholtz' (2007, 10 inter alia) work in a Bay Area high school suggests, discourse about race often stands in for discourse about class, and analyses of class ideologies in the San Francisco area are inseparable from the local construction of ethnicity, particularly since the early 1990s."]}
{"intro": [], "relatedWork": [], "rq": ["MotivationHowever, Karamanis (2003) pointed out that many metrics of coherence can be derived from the claims of Centering, all of which could be used for the type of text structuring assumed in this paper. Hence, a general methodology for identifying which of these metrics represent the most promising candidates for text structuring is required, so that at least some of them can be compared empirically. This is the second research question that this paper addresses, building upon previous work on corpus-based evaluations of Centering, and particularly the methods used by Poesio et al. (2004) . We use the gnome corpus (Poesio et al., 2004) as the domain of our experiments because it is reliably annotated with features relevant to Centering and contains the genre that we are mainly interested in."]}
{"intro": ["Formative feedback is a critical aspect of project-based design courses because it helps students assess and improve their in-progress work [11] . Without it, students learn less effectively and the quality of their project solutions suffers [14, 35] . However, instructors struggle to deliver timely, personalized feedback due to increasing student demand for design courses coupled with significant long-term resource constraints faced by many public institutions [2] .", "Researchers have explored two approaches for scaling feedback generation in project-based design courses. The first engages classroom peers in feedback exchange [19] . Peer feedback has learning benefits for the recipient and the provider [8, 28] , and there is a shared context to ground the exchange [6] . However, peer feedback can be narrow since students are learning the same course material and typically share similar demographics [11] . Peer feedback can also be affected by friendship and competition between the students [36] , and the increased workload it imposes on them [22] . These issues could affect the quality of the feedback and students' willingness to act on it. Further, students depend on instructors to orchestrate the exchange.", "The second approach engages online crowds external to the classroom. Students receive feedback from people in social networks, online communities, and task markets. External crowds may include more authentic, specialized audiences that would be hard to access in class. This might increase students' perceived value of and willingness to act on the feedback. Students can also gather feedback from external crowds as often as their time and resources allow. Planning for feedback aids self-regulation skills, which is critical for career readiness [43] . However, feedback from such crowds can be noisy due to the lack of a shared context [5] , among many factors, and requires social or financial capital [41] ."], "relatedWork": [], "rq": ["Content Focuses on Assessment and Analysis (RQ2)However, these categories may be less suitable for the early stage. At this stage, students may benefit most from reflecting on how they approached the design, how the design compares to other solutions, and how it fits in a broader social and cultural context [37] . Feedback in the categories brainstorming (3%), comparison (3%), processoriented (2%), free association (2%), and identity-invoking (<1%) would best speak to those issues, but these categories were rarely addressed by any of the four sources."]}
{"intro": [], "relatedWork": [], "rq": ["CONCLUSION AND FUTURE WORKPurchase incidence is a key pillar of every customer choice decision [7, 3] . Yet, so far it has only received marginal attention in the context of recommender systems. However, as commercial recommendations aim at influencing customer choices [2] , purchase incidence should be incorporated when a recommendation is made. For this reason, we have analyzed customer data from an in-store recommendation system at a brick-and-mortar grocery retailer that provides customers with tailored price promotions. Our findings support the view that recommender systems should not only focus on the product or service to recommend but also on the timing of the recommendation [12] . Regarding the first research question we find that in many fast moving categories, there exists an optimal point in time for a recommendation which maximizes its acceptance among customers. In particular, we show that considering this optimum and categoryspecific individual inter-purchase times, has a positive effect on the success of recommendations, increasing precision by up to 6 %. Thus, answering the third research question, incorporating this additional information improves recommender systems. Moreover, we find that the optimal point in time for a recommendation precedes the average category inter-purchase time for most categories, indicating accelerated category purchases. Finally, regarding the second research question, we find that recommendations can influence individual inter-purchase times."]}
{"intro": ["Based on an observation in the fourth semester students of English department at IKIP Mataram who programmed Writing subject, it was found that the disability of majority students to present correct topics for their paragraph which are often in complete sentences rather than phrases. The students' paragraph topic statements are not appropriately stated yet; they are mostly not in response to the topics proposed. The students' supporting sentences are not clearly elaborated to explain further about the topic statements. And, the concluding sentences are not precisely stated to end writing their paragraphs. Next to them, the use of punctuation which signed the order of the sentence was also inappropriate. However, these difficulties can be tolerated since it is a productive skill that is more complicated that it seems at first, and often seems to be the most difficult of the skills since it has a number of micro skills such as using orthography correctly, spelling, and punctuation (Orwig in Armana, 2011) .", "Ideally a good paragraph is that it considers a topic to be discussed. The paragraph topic directs discussion on what ideas which are going to be further explored in a paragraph. The topic of discussion is commonly stated in a phrase and not in a complete sentence. Next to them, a good paragraph is composed of three main components; topic sentence, supporting sentence, and the concluding sentence. Then the question is that how these difficulties can be minimized or even eliminated within writing learning process that students have better paragraph writing ability. This simple question can be optionally solved by applying Process based Approach as the approach to teach students in learning to write English paragraph. Process based Approach is a teaching of process triggering students to create their own ideas step-by-step to result in a comprehensive organization of paragraph. Therefore, to provide for an appropriate approach in teaching writing will be able to lead students into having higher sense of creativity in learning to write English, particularly in learning to write English paragraph. Process based approach, based on the elaboration on the teaching approach, is claimed to be an effective teaching approach to pump up students' creativity in learning to write. Seeing the phenomena above, the researchers have in depth interest to research a process based approach to teaching writing entitled:\"Process based Approach towards Students' Writing English Paragraph Ability\". The use of process based approach is expected to be effective steps for students in the efforts of creatively developing their English paragraph writing ability. The main research question of the study is: Is Process based Approach effective towards students' English paragraph writing ability at the fourth semester students of English Department at IKIP Mataram in the academic year of 2016/2017? Oshima (2006: 2) defines paragraph as a group of related sentences that discuss one (and usually only one) main idea. A paragraph can be as short as one sentence or as long as ten sentences. The number of sentences is unimportant, however, the paragraph should be long enough to develop the main idea clearly. Further, Irawati, (2013: 5) states that paragraph is the basic unit of composition. Paragraph usually consists of several sentences, they are topic sentence, supporting or developing sentence and concluding sentence. Thus, it can be concluded that paragraph is a unit of composition consisting of several related sentences; topic sentence, supporting sentence, and concluding sentence which are coherently composed and usually developing one main idea."], "relatedWork": [], "rq": ["IntroductionIdeally a good paragraph is that it considers a topic to be discussed. The paragraph topic directs discussion on what ideas which are going to be further explored in a paragraph. The topic of discussion is commonly stated in a phrase and not in a complete sentence. Next to them, a good paragraph is composed of three main components; topic sentence, supporting sentence, and the concluding sentence. Then the question is that how these difficulties can be minimized or even eliminated within writing learning process that students have better paragraph writing ability. This simple question can be optionally solved by applying Process based Approach as the approach to teach students in learning to write English paragraph. Process based Approach is a teaching of process triggering students to create their own ideas step-by-step to result in a comprehensive organization of paragraph. Therefore, to provide for an appropriate approach in teaching writing will be able to lead students into having higher sense of creativity in learning to write English, particularly in learning to write English paragraph. Process based approach, based on the elaboration on the teaching approach, is claimed to be an effective teaching approach to pump up students' creativity in learning to write. Seeing the phenomena above, the researchers have in depth interest to research a process based approach to teaching writing entitled:\"Process based Approach towards Students' Writing English Paragraph Ability\". The use of process based approach is expected to be effective steps for students in the efforts of creatively developing their English paragraph writing ability. The main research question of the study is: Is Process based Approach effective towards students' English paragraph writing ability at the fourth semester students of English Department at IKIP Mataram in the academic year of 2016/2017? Oshima (2006: 2) defines paragraph as a group of related sentences that discuss one (and usually only one) main idea. A paragraph can be as short as one sentence or as long as ten sentences. The number of sentences is unimportant, however, the paragraph should be long enough to develop the main idea clearly. Further, Irawati, (2013: 5) states that paragraph is the basic unit of composition. Paragraph usually consists of several sentences, they are topic sentence, supporting or developing sentence and concluding sentence. Thus, it can be concluded that paragraph is a unit of composition consisting of several related sentences; topic sentence, supporting sentence, and concluding sentence which are coherently composed and usually developing one main idea."]}
{"intro": [], "relatedWork": ["The nature of sensemaking activities and the cognitive processes involved have been the subject of established work [28, 41] , with recent studies focusing on how groups establish common ground [12, 13, 21, 56] , uncover hidden knowledge [14, 22] , and engage with large analyses [20, 27, 37, 38, 40] . In terms of general sensemaking, the data-frame model [28] describes several key macrocogintive processes relevant to our own investigation, including connecting data to a frame (an explanation reflecting person's compiled experience), reframing, elaborating, questioning and comparing frames. The more recent work detail additional behavioural and analytical processes observed in collaborative settings. Our work is orthogonal, focusing on the role of expertise. In terms of analysis methods, our work is similar to [23] who performed an insight-based user study to understand how analysts reach insight during visual exploration. Our focus, however, is on collaborative model exploration, in particular for trade-off analysis."], "rq": ["We looked at frequency and order of scenario types.When it comes to the setup, although we used a multitouch interactive surface throughout the study, predominantly a single domain expert led most of the interaction in each case study session. The rest of the participants discussed seated or standing, but refrained from interacting. This finding is supported by other studies on collaboration around interactive displays [47, 53] . We note that although most participants did not interact directly with the display, they were however actively involved in the exploration. For example, they proposed new research questions, requested to see particular views or to refine existing criteria. We also observed at least two instances in UC1 where domain experts explored the Pareto front in their own laptops.", "Subjective User FeedbackTo the best of our knowledge, there are no other studies that looked at collaborative model exploration in real-world settings. In terms of results, we found similar processes to those described in general sensemaking literature [28, 41] . Our contribution here, however, is in identifying why these processes tend to occur (e.g., storytelling to recap), and when they occur (e.g., storytelling periodically for large groups, or at the end of big chunks of exploration for smaller groups). Alignment is a particular process to model exploration, also described in [11] . However, our work considers more complex models, multiple computational stages and co-located expertise. We discuss next seven key findings from our study, relating them to our initial research questions, and comment on the applicability of our methods to other domains:", "Subjective User Feedback1. Exploration as Multiple Linked Analysis Scenarios [Q1] We observed that exploration is split into mini-exploration scenarios. Trade-off analysis starts with a preliminary exploration often leading to a focused research question. The remainder of the exploration is characterised by the nonlinear interleaving of new and refined hypotheses and research questions [41] , operating on a variety of exploration objects. Those scenarios denote a shift in the research questions and hypotheses set out by experts, which often result in change of focus in the model or data space. A parallel can be drawn between our approach and the data-frame model described in [28] . For instance, their \"reframing\" maps to our new scenario, and \"elaborating the frame\" to our refine scenario. In our case, however, re-framing revealed itself to happen specifically when participants shift their research questions and hypotheses. Furthermore, we provide a more fine grained analysis of the exploration, by crossing high level categories of interest (e.g., insight, expertise), with exploration objects (correlation, exploration method)."]}
{"intro": [], "relatedWork": ["In robotics, the delay inherent to control loops can have a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous robots. Visual servoing of a robot, for example, can be sensitive to the delays introduced through image acquisition and processing [9] . Similarly, delays in proprioception can produce instabilities during dynamic motion generation. In [2] , a dynamically smooth controller has been proposed that can deal with delay in proprioceptive readings. However, the approach assumes constant and known time-delay. A major milestone in robot control with time-delay was the ROTEX experiment [8] . Here, extended Kalman filters and graphical representation were used to estimate the state of objects in space, thereby enabling sensor-based long-range teleoperation. How to effectively deal with such communication delays has been a central research question in robotic tele-operation. Delays in robot control loops are not limited to sensor measurements only. A prominent approach for dealing with actuation delays is the Smith Predictor [17] . The Smith Predictor assumes a model of the plant, e.g. robot system, and can become unstable in the presence of model inaccuracies. A different approach has been proposed in [3] . A neural network was first trained to predict the state of mobile robots based on positions, orientations, and the previously issued action commands. The decision making process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in our paper follows a similar line of thought. However, instead of predicting specific states of the robot, we are interested in predicting the delay occurring at different parts of the control loop."], "rq": ["II. RELATED WORKIn robotics, the delay inherent to control loops can have a detrimental impact on system performance. This is particularly true for sensor-based control used in autonomous robots. Visual servoing of a robot, for example, can be sensitive to the delays introduced through image acquisition and processing [9] . Similarly, delays in proprioception can produce instabilities during dynamic motion generation. In [2] , a dynamically smooth controller has been proposed that can deal with delay in proprioceptive readings. However, the approach assumes constant and known time-delay. A major milestone in robot control with time-delay was the ROTEX experiment [8] . Here, extended Kalman filters and graphical representation were used to estimate the state of objects in space, thereby enabling sensor-based long-range teleoperation. How to effectively deal with such communication delays has been a central research question in robotic tele-operation. Delays in robot control loops are not limited to sensor measurements only. A prominent approach for dealing with actuation delays is the Smith Predictor [17] . The Smith Predictor assumes a model of the plant, e.g. robot system, and can become unstable in the presence of model inaccuracies. A different approach has been proposed in [3] . A neural network was first trained to predict the state of mobile robots based on positions, orientations, and the previously issued action commands. The decision making process was, then, based on predicted states instead of perceived states, e.g. sensor readings. The approach presented in our paper follows a similar line of thought. However, instead of predicting specific states of the robot, we are interested in predicting the delay occurring at different parts of the control loop."]}
{"intro": ["The replication crisis in psychology has prompted psychologists to engage in a great deal of soul-searching and improvement of scientific practice. We support all movement to amend sub-optimal practices such as ''p-hacking'' and to bring to light all experimenter degrees of freedom preventing principled theory-driven research (Simmons et al. 2011 ). However, we find it noteworthy that all of this soulsearching has largely remained in the realm of methodological cleanliness within the current paradigm, specifically with regard to is assumptions about measurement and analysis. It has broached less often the question of what theoretical premises prompt the original expectation that-under optimal methodological conditionsmeasured behaviors can be replicated at all? Biological, chemical, and physical sciences alike have had to grapple with the empirical quandary that even so-called simple one-dimensional recursive systems can become, with nothing but a linear increase in a single parameter, mathematically unpredictable (Hofstadter 1981) . Psychologists appear to expect replicability out of the complex biological systems pressing the response buttons in our experiments. Looking strictly at the high dimensionality of the human, cognitive systems and contrasting that with the lowdimensionality of potentially unpredictable systems, we offer the possibility that this expectation of replicability might benefit from some narrower bounds."], "relatedWork": [], "rq": ["Interaction-Dominant DynamicsHowever, not all research questions can be addressed in within-participant designs (e.g., research where learning effects during first exposure are important, applied research-for example on psychotherapy), not all manipulations can be operationalized in terms of continuous incremental changes (e.g., research employing complex action-sequences, research contrasting qualitatively different situations), and not all studies can be conducted in a way to collect a sufficient number of data points to conduct fractal analysis (e.g., questionnaire research, observational research, research employing invasive measurement procedures\u2026). For some of these cases, effectively relying on the CDD logic (at least its measurement and analysis part) might be inevitable. For other cases, some work arounds might be to run different experimental conditions back-to-back, and continue measuring across the period where one task end and the new task (or experimental condition begins). The transition between two experimental conditions can be similarly informative as to whether these two conditions are comparable, or lead to a re-organization of the cognitive system (Wallot 2014) ."]}
{"intro": [], "relatedWork": [], "rq": ["Main findings:VII. THREATS TO VALIDITY In 2015, Petersen et al. [16] created a checklist for objectively assessing the quality of systematic mapping studies. In this context a score can be computed as the ratio of the number of actions taken in a study in comparison to the total number of actions in the checklist. In our case we achieve a score of 65%, far higher than most systematic studies in the literature, which have a distribution with a median of 33% and 48% as the absolute maximum value. As always, however, threats to validity are unavoidable. The following reports on the main threats to validity to our study and how we mitigated them. External validity. The most severe external threat of our study consists in the fact that our primary studies are not representative of the state of the art on architecting microservices. As a solution, we applied a search strategy consisting of both automatic search and backward-forward snowballing on the selected studies in combination. Also, we considered only peer-reviewed papers and excluded the so-called grey literature (e.g., white papers, editorials, etc.); nevertheless, this potential bias did not impact our study significantly since considered papers have undergone a rigorous peer-reviewed process, which is a well-established requirement for high quality publications. We also applied well-defined and previously validated inclusion and exclusion criteria, which we refined iteratively by considering the pilot studies of our review. Internal validity. We rigorously defined the research protocol of our study and we iteratively defined our classification framework by rigorously applying the keywording process. The syntheses of the collected data have been performed by applying well-assessed descriptive statistics. During the horizontal analysis we made a sanity test of the extracted data by cross-analyzing parameters of the classification framework. Construct validity. We mitigated this potential bias by automatically searching the studies on multiple data sources, independently of publishers' policies or business concerns; also we are reasonably confident about the construction of the search string since the terms used are very general and suited to our research questions; the automatic search has been complemented with snowballing. Also, we rigorously selected the potentially relevant studies according to welldocumented inclusion and exclusion criteria. This selection stage was performed by one researcher and, as suggested by [20] , a random sample of potentially relevant studies was identified and the inter-researcher agreement was ensured. Conclusion validity. We rigorously defined and iteratively refined our classification framework, so that we could reduce potential biases during the data extraction process. In so doing we also have the guarantee that the data extraction process was aligned with our research questions. More in general, we mitigated potential threats to conclusion validity by applying the best practices coming from three different guidelines on systematic studies [7, 16, 20] . We applied those best practices in each phase of our study and we documented each phase in a publicly available research protocol, thus making our study easy to be replicated by other researchers."]}
{"intro": ["The respective AGLA/E modules taught at North-West University introduce students to a number of interrelated skills and a set of knowledge to assist them in becoming academically literate and make overt what is usually covert, i.e. enhancing epistemological access. Developing academic vocabulary in particular is one of the aims of these modules, which is achieved by presenting the AWL to students. However, the AWL covers only about 10% of a running text whereas higher education second language (L2) and foreign language (FL) students need at least a size of 4,000-5,000 word families (cf. Laufer and Ravenhorst-Kalovski 2010, and Nation 2006) . While firstyear students at North-West University, as in any other higher education institution, could be expected to master the AWL (Nation 1990 ), they need a vocabulary size large enough to allow them to follow lectures and to read, and respond to, academic textbooks. However, the exact vocabulary size of these students remains unknown. Furthermore, the available literature indicates that vocabulary size predicts reading comprehension, which is an important component of academic literacy (cf. Nation 2001 Nation , 2006 . It remains to be seen, however, whether this relationship holds for overall academic literacy, particularly since adequate levels of academic literacy are important to persist and prosper with studies in higher education (Carstens 2013 , Van de Poel and Van Dyk 2014 , Van Dyk and Van de Poel 2013 . Students' academic performance is seemingly mediocre despite different efforts to address their inadequate levels of preparedness. In many cases, no tangible results of these efforts are evident. Although this is a global issue, South Africa is a case in point where a recent report by the Council on Higher Education (2011) has shown that approximately 49% of registered students for a three-year degree programme managed to graduate only after five years. It is our opinion that this gap (a possible relationship between academic literacy and vocabulary size) should be bridged, which the present study sets out to do. It focuses on students who study through the medium of English but who are L2 users of English (henceforth ESL students). It tests first-year ESL students' vocabulary size in relation to their academic literacy level in an attempt to answer the following questions:"], "relatedWork": [], "rq": ["The relationship between academic literacy and vocabulary size at word bandsIn order to gain more insights into the link between vocabulary size and academic literacy (related to the second research question), the relationship between vocabulary size and academic literacy was explored further by mapping each frequency word-band score onto TALL levels. The aim was to examine the relationship between vocabulary size and academic literacy at the level of word bands, and to find out which frequency word bands were completely mastered by the students. This was achieved by running a one-way ANOVA at each of the frequency word bands. The mean scores and standard deviations are presented in Table 4, while  Table 5 presents the ANOVA results. Table 4 , scores at each word band vary from one level to another, which entails that the same levels identified by the TALL are also identified by the VLT scores. Furthermore, on average, the mean scores achieved out of 30 are satisfactory -we found a mean score of 28.05 at the AWL, 29.03 at the 2,000-word, 28.20 at the 3,000-word, and 25.26 at the 5,000-word bands. These scores were weighed against Schmitt et al.'s (2001) cut-off point. According to Schmitt et al. (2001) , a vocabulary band is mastered if the score at that band is at least 24 out of 30. Considering the above scores, it appears that a huge majority of the participants has achieved the suggested threshold. Note, however, that as many as 11.30% of the students (i.e. 39 students out of 345 participants) did not reach the minimum required score."]}
{"intro": ["Without this feedback, the player experience may not be optimal and players may switch to an alternative game. Usually user experience is evaluated after there is a working prototype implemented which it is ready for beta testing [2] . However, in the early stages of development prototypes can take the form of game sketches and thus, for some testing, a fully functional prototype may not be necessary. Time constraints and budgetary limitations often influence the fidelity of the prototype being developed.", "In order to evaluate a prototype there are two main evaluation methods: inspection based and user testing. The most widely researched inspection method is the heuristic evaluation method developed by Nielsen and Molich [5] ; in recent years bespoke heuristic sets have emerged for evaluating games [6] [7] [8] . In user testing, people from the target user group interact with the prototype or product. During this interaction, their behavior and experiences are collected using a variety of techniques including observations [9] and think aloud [10] . However, when evaluating prototypes the results can be influenced by the fidelity effect associated with the form of the prototype. In a study examining usability, many of the problems were not reported in the lowfidelity version [11] as they were associated with the functionality of the device. In another study it was concluded that users appeared to over compensate for deficiencies in aesthetics in lowfidelity prototypes [12] . When evaluating prototypes of games designed for children, understanding the effect fidelity has on the results, is clearly desirable.", "When a game is aimed at children, user testing is a credible option to evaluate usability and user experience. However, many traditional adult evaluation methods are ineffective when used with children [39] and so adaptations to evaluation methods are necessary. The behavior of the evaluator may affect the children's performance, as might other factors, including the decoration of the room and the observational equipment being deployed [13] ."], "relatedWork": [], "rq": ["DiscussionThe first question aimed to discover whether the initial expectations of children would be lower when presented with a low-fidelity prototype. The mean scores before the children played the game were similar with the Screen version having the highest mean. It was expected that the iPad version would have the highest score but this did not prove to be the case. In a study examining aesthetics in prototypes with adults, users appeared to compensate for deficiencies in aesthetic design by overrating the aesthetic qualities of reduced fidelity prototypes [12] . The issue of overcompensation might have occurred with the children being over enthusiastic when rating, for example the screen version, when rating the game with the Smileyometer; additionally it has been shown in other studies that children are generous in their evaluations of software [42] , but this is generally associated with younger children. Although the age range of the children was 7-9 these were balanced within each groups, see Table 5 for average age of the children in each group. The second research question aimed to establish if there is any difference, depending on fidelity, between children's ratings of their overall experience of playing the game. The results of the Smileyometer after the children had played the game suggest that the low-fidelity sketch version is similar to the high-fidelity iPad version, whilst the screen version was lower. However the Again Again table showed that only 58% of the children who played the sketched version stated that they wished to play the game again. This is compared to 69% for the iPad and 75% for the screen version. All three versions were favored by the children in so far as there were no children stating that they did not want to play the game again on the iPad, only one did not wish to play on the screen and just two did not want to play on the sketch version. It would appear as though the game experience can be predicted through analyzing lower fidelity prototypes (within the constraints of this game genre) using the two tools within the Fun Toolkit."]}
{"intro": ["Chiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. However, due to lack of linguistic knowledge, Chiang's HPB model contains only one type of non-terminal symbol X, often making it difficult to select the most appropriate translation rules. 1 One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima'an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable.", "Different from the soft constraint modeling adopted in (Chan et al., 2007; Marton and Resnik, 2008; Shen et al., 2009; He et al., 2010; Huang et al., 2010; Gao et al., 2011) , our approach encodes syntactic information in translation rules. However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model. Our approach maintains the advantages of Chiang's HPB model while at the same time incorporating head information and flexible reordering in a derivation in a natural way. Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang's HPB as well as a SAMT-style refined version of HPB."], "relatedWork": [], "rq": ["IntroductionChiang's hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (Chiang, 2005; Chiang, 2007) and has been widely adopted in statistical machine translation (SMT). Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion. However, due to lack of linguistic knowledge, Chiang's HPB model contains only one type of non-terminal symbol X, often making it difficult to select the most appropriate translation rules. 1 One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: Zollmann and Venugopal (2006) (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction. Almaghout et al. (2011) employ CCGbased supertags. Mylonakis and Sima'an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable."]}
{"intro": ["Users often represent the last line of defense between attackers and organizations. User responses to security warnings is thus a critical aspect of behavioral security [5, 26, 34] . A major inhibitor of the effectiveness of security warnings is habituation: diminished attention due to frequent exposure to warnings [36] . Through this process-also known as warning blindness [53] or fatigue [1] -users' attention to warnings can attenuate to the point where they hardly see the warning any longer. Although this problem is widely recognized [e.g., 3, 6, 21, 32, 38, 46] , a major limitation of past studies that examine habituation is that they used cross-sectional (i.e., single point in time) experimental designs. However, habituation is fundamentally a neurobiological phenomenon that develops over time [40] . Thus, past research on habituation to security warnings has provided only a snapshot of a dynamic problem. Our first research question is therefore:"], "relatedWork": [], "rq": ["INTRODUCTIONUsers often represent the last line of defense between attackers and organizations. User responses to security warnings is thus a critical aspect of behavioral security [5, 26, 34] . A major inhibitor of the effectiveness of security warnings is habituation: diminished attention due to frequent exposure to warnings [36] . Through this process-also known as warning blindness [53] or fatigue [1] -users' attention to warnings can attenuate to the point where they hardly see the warning any longer. Although this problem is widely recognized [e.g., 3, 6, 21, 32, 38, 46] , a major limitation of past studies that examine habituation is that they used cross-sectional (i.e., single point in time) experimental designs. However, habituation is fundamentally a neurobiological phenomenon that develops over time [40] . Thus, past research on habituation to security warnings has provided only a snapshot of a dynamic problem. Our first research question is therefore:"]}
